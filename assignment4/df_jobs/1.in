['Category:Electronic documents', '699814', '[[Category:Documents]]\n[[Category:Digital media]]\n[[Category:Information retrieval]]\n[[Category:Electronic publishing]]']
['Comprehensive Model of Information Seeking', '45206870', 'The \'\'\'Comprehensive Model of Information Seeking\'\'\', or CMIS, is a theoretical construct designed to predict how people will seek information.  It was first developed by J. David Johnson and has been utilized by a variety of disciplines including [[Library and Information Science]] and [[Health Communication]].\n\nThe CMIS has been empirically tested in health and organizational contexts<ref>Johnson, J. D., & Meischke, H. (1993). Cancer-related channel selection:  An extensionfor a sample of women who have had a mammogram. Women & Health, 20, 31-44.; Johnson, J. D., Donohue, W. A., Atkin, C. K., & Johnson, S. H. (1995). A comprehensive model of information seeking: Tests focusing on a technical organization. \nScience Communication, 16, 274-303.</ref> The CMIS has inherent strengths for studying how people react to health problems such as cancer.<ref name="auto">Johnson, J. D., Andrews, J. E. & Allard, S. (2001). A Model for Understanding and Affecting Genetics Information Seeking. Library and Information Science Research 23(4): 335-349.</ref> The CMIS specifies \'\'antecedents\'\' that explain why people become information seekers, \'\'information carrier characteristics\'\' that shape how people go about looking for information, and \'\'information seeking actions\'\' that reflect the nature of the search itself.\n\n==Design==\n\n[[File:Diagram of the Comprehensive Model of Information Seeking.jpg|thumb|right|The Comprehensive Model of Information Seeking]]\nThe CMIS has been quantitatively tested and performs well when it comes to health information seeking behaviors (HISB).<ref name="auto"/> There are three main schemas in the CMIS. These are:  Antecedents, information field, and information seeking actions.  The antecedents are those factors that determine how an information consumer will receive the information.  Those factors are:  Demographics, personal experience, salience, and beliefs.  These factors are fluid and can change during the health information seeking process.  The second schema is the information fields that consist of characteristics and utilities.  This schema is concerned with the channels and carriers of information.  A person’s understanding is developed through the information field.  The third schema involves the transformational processes and measured by the consumer’s understanding of the messages received through the information field.  The final schema involves information seeking actions.  This is what the consumer does as a result of the first two schemas through information seeking.  There are three major dimensions:  the scope, depth, and method of information seeking.<ref name="auto"/>\n\n==Antecedents==\nThe CMIS antecedents—demographics, personal experience, salience, and beliefs—are factors that determine an individual\'s natural predisposition to search for information from particular information carriers. Certain types of health information seeking can be triggered by an individual\'s degree of personal experience with disease.<ref>Johnson, J. D. (1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref> In the CMIS framework, two personal relevance factors, salience and beliefs, are seen as the primary determinants in translating a perceived gap into an active search for information. Salience refers to the personal significance of health information to the individual, such as perceptions of risk to one\'s health, which are likely to result in information seeking action. However, people also may be motivated to gather information to determine the implications of health events for themselves and/or others related to their future activities, a factor directly related to the rapidly growing field of genetics. An individual\'s beliefs about the nature of a particular disease, its impacts, and level of control, all directly relate to self-efficacy, one of our key variables, and one that plays an important role in information seeking and people\'s more general pattern of actions related to health.<ref>Johnson, J. D.(1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref>\n\n==Information Carrier Characteristics==\n\nThe information carrier characteristics are drawn from a model of Media Exposure and Appraisal (MEA) that has been tested on a variety of information carriers, including both sources and channels, and in a variety of cultural settings. Following the MEA, the CMIS focuses on editorial tone, communication potential, and utility. In the CMIS, characteristics are composed of editorial tone, which reflects an audience member\'s perception of credibility, while communication potential relates to issues of style and comprehensiveness. Utility relates the characteristics of a medium directly to the needs of an individual, and shares much with the uses and gratifications perspectives. For example, is the information contained in the medium relevant, topical, and important for the individual\'s purposes? In general, utility is very important for health information seeking.<ref name="auto"/>\n\n==Information Seeking Actions==\n\nThere are several types of information seeking actions that can result from the impetus provided by the factors identified by the CMIS. For example, search behavior can be characterized by its extent, or the number of activities carried out, which has two components: scope, the number of alternatives investigated; and, depth, the number of dimensions of an alternative investigated. There is also the method of the search, or channel, as another major dimension of the search.  For instance, an individual might choose the method of consulting a telephone information service, decide to have a narrow scope by only asking questions about smoking cessation clinics, but investigate every recommendation in detail, thus increasing the depth of the search.<ref name="auto"/>\n\n==Stages in the CMIS==\n\nA key concept from the CMIS is the notion of “stages,” or “cancer involvement”.  According to the CMIS, an individual may be at one of four stages regarding a cancer threat, and thereby have differing information needs and behaviors.\n\nThe first stage, \'\'Casual\'\', is characterized by a general lack of concern or interest. At this stage, individuals are not purposive in their search for cancer-related information; rather, their search is accidental and aimless, even apathetic.\n \nThe second stage is \'\'Purposive-Placid\'\'. This is characterized by the question, “What can I do to prevent cancer?” Individuals here might have some passing interest in cancer or genetic information, but are generally still not affected or directly concerned.\n\nThe third stage is \'\'Purposive-Clustered\'\'. Here, an individual will be in closer proximity to cancer. This is the point at which a person is motivated to look for practical information that will address the specific problem. For example, a first-degree relative of a recently diagnosed breast cancer patient may seek genetic screening or [[BRCA mutation|BRCA]] 1/2 testing. The person could clearly benefit from such information- seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes.\n\nThe fourth stage, \'\'Directed\'\', includes individuals who have been diagnosed as having cancer. Such individuals need knowledge for making informed decisions about treatment and management of the disease.<ref name="auto"/>\n\n== References ==\n{{Research help|Med}}\n{{Reflist}}\n\n\n\n[[Category:Communication]]\n[[Category:Information retrieval]]\n[[Category:Health sciences]]']
['Category:Information retrieval techniques', '46965346', '[[Category:Information retrieval]]']
['Category:Knowledge representation', '796635', "<!--''Main article : [[Knowledge representation and reasoning]]''-->\n{{Cat main|Knowledge representation}}\n\nSignificant articles:\n* [[Library classification]]\n* [[Ontology (computer science)]]\n* [[Semantic network]]\n{{Category TOC}}\n\n{{Commons cat|Knowledge representation}}\n\n[[Category:Artificial intelligence]]\n[[Category:Information science]]\n[[Category:Knowledge engineering]]\n[[Category:Programming paradigms]]\n[[Category:Reasoning]]\n[[Category:Scientific modeling]]\n[[Category:Information retrieval]]"]
['Category:Query languages', '911721', 'This [[Wikipedia:Category|category]] lists those [[domain-specific programming language]]s targeted at performing [[database]] [[query language|queries]].\n\n[[Category:Domain-specific programming languages]]\n[[Category:Data management]]\n[[Category:Databases]]']
['Enterprise Objects Framework', '59561', 'The \'\'\'Enterprise Objects Framework\'\'\', or more commonly simply \'\'\'EOF\'\'\', was introduced by [[NeXT]] in 1994 as a pioneering [[object-relational mapping]] product for its [[NeXTSTEP]] and [[OpenStep]] development platforms. EOF abstracts the process of interacting with a [[relational database]], mapping database rows to [[Java (programming language)|Java]] or [[Objective-C]] [[Object (computer science)|objects]]. This largely relieves developers from writing low-level [[SQL]] code. EOF enjoyed some niche success in the mid-1990s among financial institutions who were attracted to the rapid application development advantages of NeXT\'s object-oriented platform. Since [[Apple Inc]]\'s merger with NeXT in 1996, EOF has evolved into a fully integrated part of [[WebObjects]], an application server also originally from NeXT.\n\n== History ==\nIn the early 1990s [[NeXT]] Computer recognized that connecting to databases was essential to most businesses and yet also potentially complex.  Every data source has a different data-access language (or [[Application programming interface|API]]), driving up the costs to learn and use each vendor\'s product. The NeXT engineers wanted to apply the advantages of [[object-oriented programming]], by getting objects to "talk" to relational databases. As the two technologies are very different, the solution was to create an abstraction layer, insulating developers from writing the low-level procedural code ([[SQL]]) specific to each data source.\n\nThe first attempt came in 1992 with the release of Database Kit (DBKit), which wrapped an object-oriented framework around any database. Unfortunately, [[NEXTSTEP]] at the time was not powerful enough and DBKit had serious design flaws.\n\nNeXT\'s second attempt came in 1994 with the Enterprise Objects Framework (EOF) version 1, a [[Rewrite (programming)|complete rewrite]] that was far more modular and [[OpenStep]] compatible. EOF 1.0 was the first product released by [[NeXT]] using the Foundation Kit and introduced autoreleased objects to the developer community. The development team at the time was only four people: Jack Greenfield, Rich Williamson, Linus Upson and Dan Willhite. EOF 2.0, released in late 1995, further refined the architecture, introducing the editing context. At that point, the development team consisted of Dan Willhite, [[Craig Federighi]], Eric Noyau and Charly Kleissner.\n\nEOF achieved a modest level of popularity in the financial programming community in the mid-1990s, but it would come into its own with the emergence of the [[World Wide Web]] and the concept of [[web application]]s. It was clear that EOF could help companies plug their legacy databases into the Web without any rewriting of that data. With the addition of frameworks to do state management, load balancing and dynamic HTML generation, NeXT was able to launch the first object-oriented Web application server, [[WebObjects]], in 1996, with EOF at its core.\n\nIn 2000, Apple Inc. (which had merged with NeXT) officially dropped EOF as a standalone product, meaning that developers would be unable to use it to create desktop applications for the forthcoming [[macOS|Mac OS X]].  It would, however, continue to be an integral part of a major new release of WebObjects.  WebObjects 5, released in 2001, was significant for the fact that its frameworks had been ported from their native [[Objective-C]] programming language to the [[Java (programming language)|Java]] language.  Critics of this change argue that most of the power of EOF was a side effect of its Objective-C roots, and that EOF lost the beauty or simplicity it once had.  Third-party tools, such as [[EOGenerator]], help fill the deficiencies introduced by Java (mainly due to the loss of [[Objective-C#Categories|categories]]).\n\nThe Objective-C code base was re-introduced with some modifications to desktop application developers as [[Core Data]], part of Apple\'s [[Cocoa (API)|Cocoa API]], with the release of [[Mac OS X Tiger]] in April 2005.\n\n==How EOF works==\n\nEnterprise Objects provides tools and frameworks for object-relational mapping. The technology specializes in providing mechanisms to retrieve data from various data sources, such as relational databases via JDBC and JNDI directories, and mechanisms to commit data back to those data sources. These mechanisms are designed in a layered, abstract approach that allows developers to think about data retrieval and commitment at a higher level than a specific data source or data source vendor.\n\n<!--  Commented out because image was deleted: [[Image:EoModeler.png|frame|right|EOModeler application icon (Mac OS X)]] -->Central to this mapping is a model file (an "EOModel") that you build with a visual tool &mdash; either EOModeler, or the EOModeler plug-in to [[Xcode]]. The mapping works as follows:\n\n* Database tables are mapped to classes.\n* Database columns are mapped to class attributes.\n* Database rows are mapped to objects (or class instances).\n\nYou can build data models based on existing data sources or you can build data models from scratch, which you then use to create data structures (tables, columns, joins) in a data source. The result is that database records can be transposed into Java objects.\n\nThe advantage of using data models is that applications are isolated from the idiosyncrasies of the data sources they access. This separation of an application\'s business logic from database logic allows developers to change the database an application accesses without needing to change the application.\n\nEOF provides a level of database transparency not seen in other tools and allows the same model to be used to access different vendor databases and even allows relationships across different vendor databases without changing source code.  \n\nIts power comes from exposing the underlying data sources as managed graphs of persistent objects.  In simple terms, this means that it organizes the application\'s model layer into a set of defined in-memory data objects.  It then tracks changes to these objects and can reverse those changes on demand, such as when a user performs an undo command.  Then, when it is time to save changes to the application\'s data, it archives the objects to the underlying data sources.\n\n===Using Inheritance===\n\nIn designing Enterprise Objects developers can leverage the object-oriented feature known as [[Inheritance (computer science)|inheritance]]. A Customer object and an Employee object, for example, might both inherit certain characteristics from a more generic Person object, such as name, address, and phone number. While this kind of thinking is inherent in object-oriented design, relational databases have no explicit support for inheritance. However, using Enterprise Objects, you can build data models that reflect object hierarchies. That is, you can design database tables to support inheritance by also designing enterprise objects that map to multiple tables or particular views of a database table.\n\n==What is an Enterprise Object (EO)? ==\n\nAn Enterprise Object is analogous to what is often known in object-oriented programming as a [[Business object (computer science)|business object]] &mdash; a class which models a physical or [[conceptual object]] in the business domain (e.g. a customer, an order, an item, etc.). What makes an EO different from other objects is that its instance data maps to a data store. Typically, an enterprise object contains key-value pairs that represent a row in a relational database. The key is basically the column name, and the value is what was in that row in the database. So it can be said that an EO\'s properties persist beyond the life of any particular running application.\n\nMore precisely, an Enterprise Object is an instance of a class that implements the com.webobjects.eocontrol.EOEnterpriseObject interface.\n\nAn Enterprise Object has a corresponding model (called an EOModel) that defines the mapping between the class\'s object model and the database schema. However, an enterprise object doesn\'t explicitly know about its model. This level of abstraction means that database vendors can be switched without it affecting the developer\'s code. This gives Enterprise Objects a high degree of reusability.\n\n== EOF and Core Data ==\n\nDespite their common origins, the two technologies diverged, with each technology retaining a subset of the features of the original Objective-C code base, while adding some new features.\n\n=== Features Supported Only by EOF ===\n\nEOF supports custom SQL; shared editing contexts; nested editing contexts; and pre-fetching and batch faulting of relationships, all features of the original Objective-C implementation not supported by Core Data.  Core Data also does not provide the equivalent of an EOModelGroup—the NSManagedObjectModel class provides methods for merging models from existing models, and for retrieving merged models from bundles.\n\n=== Features Supported Only by Core Data ===\n\nCore Data supports fetched properties; multiple configurations within a managed object model; local stores; and store aggregation (the data for a given entity may be spread across multiple stores); customization and localization of property names and validation warnings; and the use of predicates for property validation.  These features of the original Objective-C implementation are not supported by the Java implementation.\n\n== External links ==\n* [http://www.linuxjournal.com/article.php?sid=7101&mode=thread&order=0&thold=0 article in linuxjournal about GDL2]\n\n[[Category:Data management]]\n[[Category:NeXT]]\n[[Category:Apple Inc. software]]']
['Match report', '2575602', "{{Unreferenced|date=May 2009}}\nIn [[metadata]], a '''match report''' is a report that compares two distinct [[data dictionary|data dictionaries]] and creates a list of the [[data element]]s that have been identified as [[Semantic equivalence|semantically equivalent]].\n\n== Use of match reports ==\n\nMatch reports are critical for systems that wish to automatically exchange data such as intelligent software agents.  If one computer system is requesting a report from a remote system that uses a distinct data dictionary and all of the data elements on the report manifest are included in the '''match report''' the report request can be executed.\n\nMatch reports are useful if data dictionaries use a metadata tagging system such as the [[UDEF]].\n\n==See also==\n*[[Data dictionary]]\n*[[Data warehouse]]\n*[[Metadata]]\n*[[Semantic equivalence]]\n*[[Universal Data Element Framework]]\n\n[[Category:Knowledge representation]]\n[[Category:Data management]]\n[[Category:Technical communication]]\n[[Category:Metadata]]\n{{compu-stub}}"]
['Control break', '3584856', "{{unreferenced|date=August 2016}}\n{{dablink|Not to be confused with 'Control-Break' displayed in MS-DOS when cancelling an ongoing task by pressing [[break key|Ctrl+Break]] key combination.}}\nIn [[computer programming]] a '''control break''' is a change in the value of one of the [[Key field|key]]s on which a file is sorted which requires some extra processing.  For example, with an input file sorted by post code, the number of items found in each postal district might need to be printed on a report, and a heading shown for the next district.  Quite often there is a hierarchy of nested control breaks in a program, e.g. streets within districts within areas, with the need for a grand total at the end. [[Structured programming]] techniques have been developed to ensure correct processing of control breaks in languages such as [[COBOL]] and to ensure that conditions such as empty input files and [[sequence error]]s are handled properly.\n\nWith [[fourth generation language]]s such as [[SQL]], the programming language should handle most of the details of control breaks automatically.\n\n[[Category:Conditional constructs]]\n[[Category:Data management]]"]
['World Wide Molecular Matrix', '4205544', '{{Refimprove|date=December 2010}}\n{{No footnotes|date=December 2010}}\n\nThe \'\'\'World Wide Molecular Matrix\'\'\' (\'\'\'WWMM\'\'\') is an electronic [[Disciplinary repository|repository]] for unpublished chemical [[data]]. First proposed in 2002 by [[Peter Murray-Rust]] and his colleagues in the [[chemistry]] department at the [[University of Cambridge]] in the [[United Kingdom]], WWMM provides a free, easily searchable [[database]] for information about thousands of complicated [[molecules]], data that would otherwise remain inaccessible to [[scientists]].\n\nMurray-Rust, a chemical [[cheminformatics|informatics]] specialist, has estimated that 80% of the results produced by chemists around the world is never published in [[scientific journals]]. Most of this data is not ground-breaking, yet it could conceivably be of use to scientists doing related projects—if they could access it. The WWMM was proposed as a solution to this problem. It would house the results of experiments on over 100,000 molecules in [[physical chemistry]], [[organic chemistry]], [[biochemistry]] and medicinal chemistry.\n\nIn other scientific fields, the need for a similar depository to house inaccessible information could be more acute. In a presentation at the "[[CERN]] Workshop on Innovations in Scholarly Communications ([[Open Archives Initiative|OAI4]])", Murray-Rust said that chemistry actually leads other fields in published data. He estimated that as much as 99% of the data in some scientific fields never reaches publication.{{Citation needed|date=December 2010|reason=This is not found in external link, OAI4 - Paine Ellsworth, ed.}}\n\nAlthough scientific in nature, the WWMM is part of the broader [[Open Archives Initiative|open archives]] and [[open source]] movements, pushes to make more and more information freely available to any user via the [[Internet]] or [[www|World Wide Web]]. In his [[CERN]] presentation, Murray-Rust stated that the WWMM was a "response to the expense of [scientific] journals", and he asked the rhetorical question, "Can we win the war to make data open, or will it be absorbed into the [[publishing]] and pseudo-publishing world?" Murray-Rust and his colleagues are also responsible for the development of the Chemical Mark-up Language ([[Chemical Markup Language|CML]]), a variant of [[XML]] intended for [[chemists]].\n\n==See also==\n* [[Open Archives Initiative|The open archives initiative (OAI)]]\n* [[Informatics (academic field)|The science of Informatics]]\n* [[Chemical Markup Language|Chemical Mark-up language (CML)]]\n\n==External links==\n*[http://www.ch.cam.ac.uk/person/pm286 The home page of Dr. Peter Murray-Rust at the University of Cambridge]\n*[http://www.escience.cam.ac.uk/projects/mi/mi_call.html The Cambridge Center for molecular informatics]\n*[http://www.nesc.ac.uk/events/ahm2003/AHMCD/pdf/157.pdf An outline of the WWMM]\n*[http://oai4.web.cern.ch/OAI4/ CERN Workshop on Innovations in Scholarly Communication (OAI4)]{{verify source|type=application to WWMM|date=December 2010}}\n\n[[Category:Data management]]']
['Content format', '7428842', '[[File:Pcm.svg|200px|thumb|right|Graphical representations of electrical data: analog audio content format (red), 4-bit digital pulse code modulated content format (blue).]][[File:Mi Fu-On Calligraphy.jpg|200px|thumb|right|Chinese calligraphy written in a language content format by [[Song Dynasty]] (A.D. 1051-1108) poet [[Mi Fu]].]][[File:12345678901-2-23456 barcode UPC(A).svg|200px|thumb|A series of numbers encoded in a Universal Product Code digital numeric content format.]]A \'\'\'content format\'\'\' is an [[encoding|encoded]] format for converting a specific type of [[data]] to displayable [[information]]. [[Content (media and publishing)|Content]] formats are used in [[recording]] and [[Telecommunication|transmission]] to prepare data for [[Information processing|observation]] or [[interpreting|interpretation]].<ref>Bob Boiko, \'\'Content Management Bible,\'\' Nov 2004 pp:79, 240, 830</ref><ref>[[Ann Rockley]], \'\'Managing Enterprise Content: A Unified Content Strategy,\'\' Oct 2002 pp:269, 320, 516</ref> This includes both [[Analog signal|analog]] and [[digitizing|digitized]] content. Content formats may be recorded and read by either natural or manufactured tools and mechanisms.\n\nIn addition to converting data to information, a content format may include the [[encryption]] and/or [[Scrambler|scrambling]] of that information.<ref>Jessica Keyes, \'\'Technology Trendlines,\'\' Jul  1995 pp:201</ref> Multiple content formats may be contained within a single section of a [[storage medium]] (e.g. [[Multitrack recording|track]], [[disk sector]], [[computer file]], [[document]], [[page (paper)|page]], [[Column (typography)|column]]) or transmitted via a single [[Channel (communications)|channel]] (e.g. [[wire]], [[carrier wave]]) of a [[transmission medium]]. With [[multimedia]], multiple tracks containing multiple content formats are presented simultaneously. Content formats may either be recorded in secondary signal processing methods such as a software container format (e.g. [[digital audio]], [[digital video]]) or recorded in the primary format (e.g. [[spectrogram]], [[pictogram]]). \n\nObservable data is often known as [[raw data]], or raw content.<ref>Oge Marques and Borko Furht, \'\'Content-Based Image and Video Retrieval,\'\' April 2002 pp:15</ref> A primary raw content format may be directly [[information processing|observable]] (e.g. [[image]], [[sound]], [[Motion (physics)|motion]], [[Odor|smell]], [[Haptic perception|sensation]]) or [[physics|physical]] data which only requires hardware to display it, such as a [[phonograph]]ic [[Gramophone needle|needle]] and [[diaphragm (acoustics)|diaphragm]] or a [[Image projector|projector]] [[List of light sources|lamp]] and [[magnifying glass]].\n\nThere has been a countless number of content formats throughout history. The following are examples of some common content formats and content format categories (covering: sensory experience, model, and language used for encoding information):\n{| width="65%"\n|- valign=top\n|width="50%"|\n*Audio data encoding<ref>David Austerberry, \'\'The Technology of Video and Audio Streaming, Second Edition,\'\' Sep 2004 pp: 328</ref>\n**[[Audio coding format]]\n**[[Analog signal|Analog]] [[Audio frequency|audio]] data\n**[[Stereophonic sound]] formats\n**[[Digital audio]] data\n**[[Synthesizer]] [[Music sequencer|sequences]]\n*Visual data encoding\n**[[Art techniques and materials|Hand rendering materials]]\n**[[Film speed]] formats\n**[[Pixel]] [[coordinates]] data\n**[[Color space]] data\n**[[Vector graphics|Vector graphic]] [[coordinates]]/[[dimensions]]\n**[[Texture mapping]] formats\n**[[3D display]] formats\n**[[Holographic]] formats\n**[[Display resolution]] formatting\n*[[Motion graphics]] encoding\n**[[Video coding format]]\n**[[Frame rate]] data\n**[[Video]] data<ref>M. Ghanbari, \'\'Standard Codecs: Image Compression to Advanced Video Coding,\'\' Jun 2003 pp:364</ref>\n**[[Computer animation]] formats\n*Instruction encoding\n**[[Musical notation]]\n**[[Computer language]]\n**[[Traffic signals]]\n|width="50%"|\n*[[Language|Natural languages formats]]\n**[[Writing system]]s\n**[[Phonetic]]\n**[[Sign language]]s\n*[[Signal (electronics)|Communication signaling formats]]\n*[[Code]] formats\n*Expert language formats\n**[[Graphic organizer]]\n**[[Statistical model]]\n**[[Table of elements#Standard periodic table|Table of elements]]\n**[[DNA sequence]]\n**[[Human anatomy]]\n**[[Biometrics|Biometric data]]\n**[[Chemical formula]]s\n**[[Aroma compound]]\n**[[Psychoactive drug#Psychoactive drug chart|Drug chart]]\n**[[Electromagnetic spectrum]]\n**[[Time standard]]\n**[[Numerical weather prediction]]\n**[[Capital asset pricing model]]\n**[[Measures of national income and output|National income and output]]\n**[[Celestial coordinate system]]\n**[[APP-6a|Military mapping]]\n**[[Geographic information system]]\n**[[Interstate Highway System]]\n|}\n\n==See also==\n*[[Communication]]\n*[[Representation (arts)]]\n*[[Modulation|Content carrier signals]]\n*[[Multiplexing|Content multiplexing format]]\n*[[Transmission (telecommunications)|Content transmission]]\n*[[Wireless|Wireless content transmission]]\n*Data storage device\n*[[Recording format]]\n*[[Encoder]]\n*[[Analog television]]: [[NTSC]], [[PAL]] and [[SECAM]]\n*[[Information mapping]]\n\n==References==\n{{reflist}}\n\n[[Category:Communication]]\n[[Category:Media technology]]\n[[Category:Data management]]\n[[Category:Recording]]\n[[Category:Film and video technology]]\n[[Category:Sound production technology]]\n\n\n{{library-stub}}']
['Workflow engine', '7711975', "A '''workflow engine''' is a [[software application]] that manages business processes. It is a key component in [[workflow technology]] and typically makes use of a [[database server]].\n\nA workflow engine manages and monitors the state of activities in a [[workflow]], such as the processing and approval of a loan application form, and determines which new activity to transition to according to defined processes (workflows).<ref>http://docs.oracle.com/cd/B13789_01/workflow.101/b10286/wfapi.htm</ref> The actions may be anything from saving an application form in a [[document management system]] to sending a reminder e-mail to users or escalating overdue items to management. A workflow engine facilitates the flow of information, tasks, and events. Workflow engines may also be referred to as Workflow Orchestration Engines.<ref>http://pic.dhe.ibm.com/infocenter/tivihelp/v48r1/index.jsp?topic=%2Fcom.ibm.sco.doc_2.2%2Fenablement%2Fworkfloworchestration.html</ref>\n\nWorkflow engines mainly have three functions:\n*\tVerification of the current status: Check whether the command is valid in executing a task.\n*\tDetermine the authority of users: Check if the current user is permitted to execute the task.\n*\tExecuting condition script: After passing the previous two steps, the workflow engine begins to evaluate the condition script in which the two processes are carried out, if the condition is true, workflow engine execute the task, and if the execution successfully completes, it returns the success, if not, it reports the error to trigger and roll back the change.<ref>The Workflow Engine Model. [http://msdn.microsoft.com/en-us/library/aa188337%28office.10%29.aspx  The Workflow Engine Model] Accessed 1 Dec. 2010.</ref>\n\nA workflow engine is a core technique for task allocation software, such as [[business process management]], in which the workflow engine allocates tasks to different executors while communicating data among participants. A workflow engine can execute any arbitrary sequence of steps, for example, a healthcare data analysis.<ref name=hf2010>{{Cite journal | last1 = Huser | first1 = V. | last2 = Rasmussen | first2 = L. V. | last3 = Oberg | first3 = R. | last4 = Starren | first4 = J. B. | title = Implementation of workflow engine technology to deliver basic clinical decision support functionality | doi = 10.1186/1471-2288-11-43 | journal = BMC Medical Research Methodology | volume = 11 | pages = 43 | year = 2011 | pmid = 21477364 | pmc = 3079703}}</ref>\n\n== See also ==\n*[[Business rules engine]]\n*[[Business rule management system]]\n*[[Comparison of BPEL engines]]\n*[[Inference engine]]\n*[[Java Rules Engine API]]\n*[[Rete algorithm]]\n*[[Ripple down rules]]\n*[[Semantic reasoner]]\n*[[BPEL|Business Process Execution Language]]\n*[[Production system (computer science)|Production system]]\n*[[Workflow management system]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:Servers (computing)]]\n[[Category:Workflow technology]]\n[[Category:Workflow software]]"]
['Content Engineering', '7829016', "{{Unreferenced|date=May 2009}}\n'''Content Engineering''' is a term applied to an engineering speciality dealing with the issues around the use of [[Content (media and publishing)|content]] in computer-facilitated environments.  Content production, [[content management]], content modelling, content conversion, and content use and repurposing are all areas involving this speciality.  It is not a speciality with wide industry recognition and is often performed on an ad hoc basis by members of software development or content production staff, but is beginning to be recognized as a necessary function in any complex content-centric project involving both content production as well as software system development.\n\nContent engineering tends to bridge the gap between groups involved in the production of content ([[Publishing]] and [[Editing|Editorial staff]], [[Marketing]], [[Sales]], [[Human resources|HR]]) and more technologically oriented departments such as [[Software Development]], or [[Information technology|IT]] that put this content to use in web or other software-based environments, and requires an understanding of the issues and processes of both sides.\n\nTypically, Content Engineering involves extensive use of Embedded,  [[XML]] technologies, XML being the most widespread language for representing structured content. [[Content_management_system|Content Management Systems]] are often key technology used in this practice though frequently Content Engineering fills the gap where no formal CMS has been put into place.\n\n[[Category:Data management]]"]
['Holos', '1423557', '{{multiple issues|\n{{notability|date=May 2010}}\n{{Unreferenced|date=May 2010}}\n{{Peacock|date=September 2012}}\n}}\n\n\'\'\'Holos\'\'\' is an influential [[OLAP]] (Online Analytical Processing) product of the 1990s. Developed by Holistic Systems in 1987, the product remained in use until around 2004.\n\n==Conception==\nThe Holos product succeeded an older generation of mainframe products such as [[System-W]]. It was the first to use an industry standard [[SQL]] database (as opposed to a proprietary one), and also the first to use the new GUI PC for the user interface.{{citation needed|date=September 2012}} In physically separating the \'\'number crunching\'\' from the user interface, the product was immediately client/server, although the term didn\'t come into use until some time later. In fact the process was described as cooperative processing at the time as client/server was not a current term at that time. The client/server model used for Holos was initially for a very "light" client as it was not clear at that time (1986/7) that PCs were going to be so commonplace and most were still running MS-DOS.\n\nIn fact it was technically possible to run the system using "dumb" terminal with reduced functionality in early versions although save for in Holistic\'s test environment this was rarely if ever done. In time due to the increased popularly of PCs and their increased power and the available of a stable and more functional version of Microsoft Windows additional functionality was added to the client end mostly in the form of development aids. In addition to data services, the Holos Server supplied business logic and calculation services. It also provided complementary services to the Holos Client which meant the internal processing associated with the report writer, worksheet, etc., was distributed between the two components.\n\n==Architecture==\nThe core of the Holos Server was a [[business intelligence]] (BI) [[virtual machine]]. The Holos Language (HL) was compiled into a soft instruction code, and executed in this virtual machine (similar in concept to Java in more modern systems). The virtual machine was fully fault-tolerant, using structured [[exception handling]] internally, and provided a debugger interface. The debugger was machine-level until quite late on, after which it also supported source-level access.\n\nOLAP data was handled as a core data type of HL, with specific syntax to accommodate multidimensional data concepts, and complete programmatic freedom to explore and utilise the data. This made it very different from the industry trend of query-based OLAP and SQL engines. On the upside, it allowed amazing flexibility in the applications to which it could be applied. On the downside, it mean that 3-tier configurations were never successfully implemented since the processing had to be close to the data itself. This hindered large-scale deployment to many clients, and the use of OLAP data from other vendors. In reality, its own data access times were probably some of the fastest around—at the individual cell level; they had to be in order to be practical. However, when fetching back bulk data for non-cooperating clients, or data from other vendors, the queries could not be optimised as a whole. Its own data access used a machine-wide shared memory cache.\n\n==Language==\nThe Holos Language was a very broad language in that it covered a wide range of statements and concepts, including the reporting system, business rules, OLAP data, SQL data (using the Embedded SQL syntax within the hosting HL), device properties, analysis, forecasting, and data mining. It even supported elements to enable self-documentation and self-verification. Placing all these areas on a common footing, and allowing them to co-operate by sharing data, events, etc., was key to the number of possibilities that resulted. For instance, the report writer supported input as well as output, plus interactive graphics, and a comprehensive event mechanism to pass back information about the viewed data to event handlers. Also, reports and data were separate entities, thus allowing the same report to be applied to different data as long as it was described by similar meta-data. This meant that when terms like [[Enterprise Information System|EIS]] and [[Management information systems|MIS]] were first coined, the industry norm was "slideshows", i.e. pre-programmed transitions between views, whereas Holos provided data-driven drill-down, i.e. no pre-programmed views or links. The transitions could be made dependent upon the data values and trends, in conjunction with the available business logic.\n\n==OLAP Storage==\nHolos Server provided an array of different, but compatible, storage mechanisms for its multi-cube architecture: memory, disk, SQL. It was therefore the first product to provide "hybrid OLAP" ([[HOLAP]]). It provided a very versatile mechanism for joining cubes, irrespective of their storage technology, dimensionality, or meta-data, and this was eventually given a [[US patent]] (called COA—Compound OLAP Architecture {{US patent|6289352}}{{US patent|6490593}}). One novel aspect of this was a \'stack\' feature that allowed read/write cubes to be stacked over read-only cubes. Read operations to the overall virtual cube then visited both \'racks\' (top first, and then the bottom), whereas write operations only affected the top. The resulting valve-like mechanism found many applications in data sharing, what-if forecasting, and aggregation of slow SQL-based data. Since the overhead of the joining was small, it was not uncommon to have stacks 7 levels deep, and joining terabytes of real OLAP data. Around about V8.5, Holos Server implemented a hierarchical lock manager, allowing nesting of fine and coarse-grain OLAP locks, and full transaction control.\n\n==Business Rules==\nThe business logic supported full cross-dimensional calculations, automatic ordering of rules using static data-flow analysis, and the identification and solution of simultaneous equations. The rules treated all dimensions in an orthogonal fashion. The aggregation process did not distinguish between simple summation or average calculations, and more complex non-commutative calculations. Both could be applied to any dimension member. The process allowed aggregation levels (i.e. those calculation levels starting with base data (level 0) and proceeding up to the overall grand total) to be individually pre-stored or left to be calculated on demand.\n\n==Holos Client==\nThe Holos Client was both a design and delivery vehicle, and this made it quite large. Around about 2000, the Holos Language was made object-oriented (HL++) with a view to allowing the replacement of the Holos Client with a custom Java or VB product. However, the company were never sold on this, and so the project was abandoned.\n\nOne of the biggest failures was not to provide a thin-client interface to the Holos Server, and this must have contributed to the product\'s demise. Although an [[HTML]] toolkit was sold, it was clumsy and restricted. By the time a real thin-client mechanism was developed, it was far too late and it never got to market.\n\n==Deployment==\nBefore its demise, the Holos Server product ran under Windows NT (Intel and Alpha), VMS (VAX and Alpha), plus about 10 flavours of UNIX, and accessed over half-a-dozen different SQL databases. It was also ported to several different locales, including Japanese.\n\n==Company==\n{{main|Crystal Decisions}}\nHolistic Systems was purchased by the hardware company [[Seagate Technology]] in 1996. Along with other companies such as [[Crystal Services]], it was used to create a new subsidiary company called [[Seagate Software]]. Only Holistic and Crystal remained, and Seagate Software was renamed to [[Crystal Decisions]]. Holistic and Crystal had very different sales models. The average sale for the Holos Product in the United States was in excess of $250,000 and was sold primarily to Fortune 500 companies by a direct sales force. The Crystal sales model was based upon a "shrink wrapped" product Crystal Reports sold primarily through resellers. As Crystal was acquired prior to Holistic the senior management in the sales and marketing arena were mostly drawn from that organisation. They felt that all the product range should be sold through third parties and over a period of time dismantled the direct sales force culmination in a significant drop in sales for the Holos Product. Subsequently after some in-fighting and argument over product strategy, the main Holos development team finally started to leave around 2000, and Crystal Decisions was finally taken over by [[Business Objects (company)|Business Objects]] in 2004. Following the takeover, support for Holos was outsourced to [[Raspberry Software]], which was set up by former employees of Crystal Decisions.\n\n[[Category:Data management]]\n[[Category:Online analytical processing]]']
['CA Gen', '965842', '\'\'\'CA Gen\'\'\' is a [[Computer Aided Software Engineering]] (CASE) application development environment marketed by [[CA Technologies]]. Gen was previously known as \'\'\'IEF\'\'\' (\'\'\'Information Engineering Facility\'\'\'), \'\'\'Composer by IEF\'\'\', \'\'\'Composer\'\'\', \'\'\'COOL:Gen\'\'\', \'\'\'Advantage:Gen\'\'\' and \'\'\'AllFusion Gen\'\'\'.\n\nThe toolset originally supported the [[information engineering]] methodology developed by [[Clive Finkelstein]], [[James Martin (author)|James Martin]] and others in the early 1980s. Early versions supported IBM\'s [[IBM DB2|DB2]] database, [[IBM 3270|3270]] \'block mode\' screens and generated [[COBOL]] code.\n\nIn the intervening years the toolset has been expanded to support additional development techniques such as [[Component-based software engineering|component-based development]]; creation of [[Client–server model|client/server]] and [[web application]]s and generation of [[C (programming language)|C]], [[Java (programming language)|Java]] and [[C Sharp (programming language)|C#]]. In addition, other platforms are now supported such as many variants of *ix-like Operating Systems (AIX, HP-UX, Solaris, Linux) as well as Windows.\n\nIts range of supported database technologies have widened to include [[Oracle Database|ORACLE]], [[Microsoft SQL Server]], [[ODBC]], [[Java Database Connectivity|JDBC]] as well as the original DB2.\n\nThe toolset is fully integrated - objects identified during analysis carry forward into design without redefinition. All information is stored in a repository (central encyclopedia). The encyclopedia allows for large team development - controlling access so that multiple developers may not change the same object simultaneously.<ref>https://communities.ca.com/web/ca-gen-edge-global-user-community/wiki/-/wiki/EDGE+User+Group+CA+Gen+Wiki/What+is+CA+Gen?&#p_36</ref>\n\n==Overview==\nIt was initially produced by [[Texas Instruments]], with input from [[James Martin (author)|James Martin]] and his consultancy firm James Martin Associates, and was based on the Information Engineering Methodology (IEM). The first version was launched in 1987.\n\nIEF became popular among large government departments and public utilities. It initially supported a [[CICS]]/COBOL/DB2 target environment.  However, it now supports a wider range of relational databases and operating systems. IEF was intended to shield the developer from the complexities of building complete multi-tier cross-platform applications.\n\nIn 1995, Texas Instruments decided to change their marketing focus for the product. Part of this change included a new name - "Composer".\n\nBy 1996, IEF had become a popular tool. However, it was criticized by some IT professionals for being too restrictive, as well as for having a high per-workstation cost ($15K USD). But it is claimed that IEF reduces development time and costs by removing complexity and allowing rapid development of large scale enterprise transaction processing systems.\n\nIn 1997, Composer had another change of branding, Texas Instruments sold the [[Texas Instruments Software]] division, including the Composer rights, to [[Sterling Software]]. Sterling software changed the well known name "Information Engineering Facility" to "COOL:Gen". COOL was an acronym for "Common Object Oriented Language" - despite the fact that there was little [[Object-oriented programming|object orientation]] in the product.\n\nIn 2000, Sterling Software was acquired by [[Computer Associates]] (now CA). CA has rebranded the product three times to date and the product is still used widely today. Under CA, recent releases of the tool added support for the CA-[[DATACOM/DB|Datacom]] DBMS, the Linux operating system, C# code generation and [[ASP.NET]] web clients. The current version is known as CA Gen - version 8 being released in May 2010, with support for customised web services, and more of the toolset being based around the [[Eclipse (software)|Eclipse framework]].\n\nThere are a variety of "add-on" tools available for CA Gen, including Project Phoenix from Jumar - a collection of software tools and services focused on the modernisation and re-platforming of existing/legacy CA Gen applications to new environments,<ref>[http://www.jumar-solutions.com/ Jumar]</ref> GuardIEn - a [[Configuration Management]] and Developer Productivity Suite,<ref>[http://www.iet.co.uk IET Ltd]</ref> QAT Wizard,<ref>[http://www.qat.com/qat_wizard.asp QAT Wizard]</ref> an interview style wizard that takes advantage of the meta model in Gen, products for multi-platform application reporting and XML/SOAP enabling of Gen applications.,<ref>[http://www.canamsoftware.com/ Canam Software Labs]</ref> and developer productivity tools such as Access Gen, APMConnect, QA Console and Upgrade Console from Response Systems <ref>[http://www.response-systems.com Response Systems]</ref>\nRecently CA GEN has released its latest version 8.5.\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.uk.capgemini.com/public-sector/tax-welfare/regenerate Capgemini REGENERATE offering] - Support, Update, Migrate\n* [http://www.edgeusergroup.org EDGE User Group] - the user group for CA Gen\n* [http://www.edgeusergroup.org/wiki CA Gen Wiki] - sponsored by the EDGE User Group\n* [http://www.gentalk.biz gentalk.biz] - CA Gen Blog - inactive\n* [http://www.qat.com QAT Global] - CA Gen Services and Training Provider (USA)\n* [http://www.iet.co.uk IET] - CA Gen Product and Services Provider (UK)\n* [http://www.jumar-solutions.com/ Jumar Solutions] - CA Gen Product and Services Provider (UK)\n* [http://www.response-systems.com/ Response Systems] - CA Gen Product and Services Provider (UK)\n* [http://www.facet.com.au/ Facet Consulting] - CA Gen Services Provider (Australia)\n* [http://www.canamsoftware.com/ Canam Software Labs, Inc.] - CA Gen Product and Service Provider (Canada)\n\n[[Category:Computer-aided software engineering tools]]\n[[Category:Data management]]\n[[Category:CA Technologies]]\n\n\nEdited by: Sambit Mishra']
['Dynamic knowledge repository', '1004008', '{{refimprove|date=August 2007}}\n\nThe \'\'\'dynamic knowledge repository\'\'\' (\'\'\'DKR\'\'\') is a concept developed by [[Douglas C. Engelbart]] as a primary strategic focus for allowing humans to address complex problems.{{when|date=September 2011}} Doug has proposed that a DKR will enable us to develop a collective [[IQ]] greater than any individual\'s IQ. References and discussion of Engelbart\'s DKR concept are available at the [[Doug Engelbart Institute]].<ref>{{cite web | url=http://www.dougengelbart.org/about/dkrs.html|title=About Dynamic Knowledge Repositories – an introduction | accessdate=September 15, 2011 | author=Christina Engelbart}}</ref>\n\n==Definition==\nA knowledge repository is a computerized system that systematically captures, organizes and categorizes an organization\'s knowledge. The repository can be searched and data can be quickly retrieved.\n\nThe effective knowledge repositories include factual, conceptual, procedural and meta-cognitive techniques. The key features of knowledge repositories include communication forums.\n\nA knowledge repository can take many forms to "contain" the knowledge it holds. A customer database is a knowledge repository of customer information and insights – or electronic explicit knowledge. A Library is a knowledge repository of books – physical explicit knowledge. A community of experts is a knowledge repository of tacit knowledge or experience. The nature of the repository only changes to contain/manage the type of knowledge it holds. A repository (as opposed to an archive) is designed to get knowledge out. It should therefore have some rules of structure, classification, taxonomy, record management, etc., to facilitate user engagement.\n\n==References==\n{{Reflist|2}}\n\n== External links==\n* [http://dougengelbart.org/ Doug Engelbart Institute]\n\n[[Category:Knowledge representation]]\n[[Category:Data management]]\n\n\n{{compu-storage-stub}}']
['Ontology-based data integration', '11476249', '\'\'\'Ontology-based data integration\'\'\' involves the use of [[ontology (computer science)|ontology]](s) to effectively combine data or information from multiple heterogeneous sources.<ref name="wache">{{cite conference |author1=H. Wache |author2=T. Vögele |author3=U. Visser |author4=H. Stuckenschmidt |author5=G. Schuster |author6=H. Neumann |author7=S. Hübner | title=Ontology-Based Integration of Information A Survey of Existing Approaches | year=2001 | citeseerx = 10.1.1.142.4390 }}</ref> It is one of the multiple [[data integration]] approaches and may be classified as Global-As-View (GAV).<ref name="refone">{{cite conference | author=Maurizio Lenzerini | title=Data Integration:  A Theoretical Perspective | year=2002 | pages=243–246 | url=http://www.dis.uniroma1.it/~lenzerin/homepagine/talks/TutorialPODS02.pdf }}</ref> The effectiveness of ontology based data integration is closely tied to the consistency and expressivity of the ontology used in the integration process.\n\n==Background==\nData from multiple sources are characterized by multiple types of heterogeneity. The following hierarchy is often used:<ref name="sheth">{{cite book | author=A.P. Sheth | title = Changing Focus on Interoperability in Information Systems: From System, Syntax, Structure to Semantics | booktitle=Interoperating Geographic Information Systems. M. F. Goodchild, M. J. Egenhofer, R. Fegeas, and C. A. Kottman (eds.), Kluwer Academic Publishers | year=1999 | pages=5–30 | url=http://lsdis.cs.uga.edu/library/download/S98-changing.pdf}}</ref><ref>[http://daks.ucdavis.edu/~ludaesch/Paper/AHM02/tutorial5.html AHM02 Tutorial 5: Data Integration and Mediation; Contributors: B. Ludaescher, I. Altintas, A. Gupta, M. Martone, R. Marciano, X. Qian]</ref>\n* [[Syntactic heterogeneity]]: is a result of differences in representation format of data\n* Schematic or [[structural heterogeneity]]: the native model or structure to store data differ in data sources leading to structural heterogeneity. Schematic heterogeneity that particularly appears in structured databases is also an aspect of structural heterogeneity.<ref name="sheth"/>\n* [[Semantic heterogeneity]]: differences in interpretation of the \'meaning\' of data are source of semantic heterogeneity\n* [[System heterogeneity]]: use of different [[operating system]], hardware platforms lead to system heterogeneity\n\n[[ontology (computer science)|Ontologies]], as formal models of representation with explicitly defined concepts and named relationships linking them, are used to address the issue of [[semantic heterogeneity]] in data sources. In domains like [[bioinformatics]] and [[biomedicine]], the rapid development, adoption and public availability of ontologies [http://www.bioontology.org/repositories.html#obo] has made it possible for the [[data integration]] community to leverage them for [[semantic integration]] of data and information.\n\n==The role of ontologies==\n\nOntologies enable the unambiguous identification of entities in heterogeneous information systems and assertion of applicable named relationships that connect these entities together. Specifically, ontologies play the following roles:\n\n* Content Explication<ref name="wache"/>\nThe ontology enables accurate interpretation of data from multiple sources through the explicit definition of terms and relationships in the ontology.\n\n* Query Model<ref name="wache"/>\nIn some systems like SIMS,<ref name="arens">{{cite conference |author1=Y. Arens |author2=C. Hsu |author3=C.A. Knoblock | title=Query Processing in sims information mediator | year=1996 | url=http://www.isi.edu/integration/papers/arens98-agents.pdf}}</ref> the query is formulated using the ontology as a global query schema.\n\n* Verification<ref name="wache"/>\nThe ontology verifies the mappings used to integrate data from multiple sources. These mappings may either be user specified or generated by a system.\n\n===Approaches using ontologies for data integration===\nThere are three main architectures that are implemented in ontology-based data integration applications,<ref name="wache"/> namely, \n;Single ontology approach: A single ontology is used as a global reference model in the system. This is the simplest approach as it can be simulated by other approaches.<ref name="wache"/> SIMS<ref name="arens"/>  a prominent example of this approach.  The Structured Knowledge Source Integration component of [[Cyc|Research Cyc]] is another prominent example of this approach.<ref>http://www.cyc.com/content/semantic-knowledge-source-integration</ref><ref>http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/2299</ref> (Title = Harnessing Cyc to Answer Clinical Researchers\' Ad Hoc Queries)\n\n;Multiple ontologies: Multiple ontologies, each modeling an individual data source, are used in combination for integration. Though, this approach is more flexible than the single ontology approach, it requires creation of mappings between the multiple ontologies. Ontology mapping is a challenging issue and is focus of large number of research efforts in [[computer science]] [http://www.ontologymatching.org/]. The OBSERVER system<ref name="mena">{{cite conference |author1=E. Mena |author2=V. Kashyap |author3=A. Sheth |author4=A. Illarramendi | title=OBSERVER: An Approach for Query Processing in Global Information Systems based on Interoperation across Pre-existing Ontologies | year=1996 | url=http://dit.unitn.it/~p2p/RelatedWork/Matching/MKSI96.pdf}}</ref> is an example of this approach.\n\n;Hybrid approaches: The hybrid approach involves the use of multiple ontologies that subscribe to a common, top-level vocabulary.<ref name="goh">{{cite conference | author=Cheng Hian Goh | title=Representing and Reasoning about Semantic Conflicts in Heterogeneous Information Systems | year=1997 | url=http://context2.mit.edu/coin/publications/goh-thesis/goh-thesis.pdf}}</ref>  The top-level vocabulary defines the basic terms of the domain. Thus, the hybrid approach makes it easier to use multiple ontologies for integration in presence of the common vocabulary.\n\n==See also==\n* [[Data mapping]]\n* [[Enterprise application integration]]\n* [[Enterprise information integration]]\n* [[Ontology mapping]]\n* [[Schema matching]]\n\n==References==\n\n<references/>\n\n==External links==\n*[http://sid.cps.unizar.es/OBSERVER/ OBSERVER home page]\n*[http://www.cyc.com/content/semantic-knowledge-source-integration Cyc Semantic Knowledge Source Integration (SKSI)]\n\n[[Category:Ontology (information science)]]\n[[Category:Data management]]']
['Category:Data structures', '691150', "{{catdiffuse}}\n\n{{Commons cat|Data structures}}\n{{Category see also|Data types}}\n\nIn [[computer science]], a '''[[data structure]]''' is a way of storing [[data]] in a computer so that it can be used efficiently. Often a carefully chosen data structure will allow a more efficient [[algorithm]] to be used. The choice of the data structure must begin from the choice of an [[abstract data structure]].\n\n{{Cat main|Data structures}}\n\n== See also ==\n\n* [[:Category:Abstract data types]]\n* [[:Category:Hashing]]\n* [[:Category:Computer file formats]]\n\n[[Category:Algorithms and data structures]]\n[[Category:Computer data|Structures]]\n[[Category:Data management|Structures]]\n[[Category:Computer programming]]"]
['Virtual facility', '13444144', "{{Advert|article|date=May 2013}}\n[[Image:VirtualFacility.jpg|thumb|250px|A Virtual Facility snapshot created with 6SigmaDC software.|right]]\nA '''Virtual Facility''' (VF) is a highly realistic digital representation of a [[data center]] (primarily). The term virtual in Virtual Facility refers to the use of the word as in [[Virtual reality|Virtual Reality]] rather than the abstraction of computer resources as in [[platform virtualization]]. The VF mirrors the characteristics of the physical facility over time and allows modeling all relevant characteristics of a physical data center with a high degree of precision. \n\n==VF Model includes==\n\n* Three-dimensional physical facility layout\n* Network connectivity of facility equipment\n* Full inventory of facility equipment, including electronics and electrical systems such as [[Power distribution unit|Power Distribution Units]] (PDU’s) and [[Uninterruptible power supplies|Uninterruptible Power Supplies]] (UPS’s)\n* Full air conditioning system (ACU’s) and controls within the room\n\nThe term Virtual Facility was introduced by Future Facilities, a data centre design consultancy focused on delivering Design and Operational solutions to address the emerging environmental problems facing the modern Mission Critical Facility (MCF). The concept is in essence a convergence of the fields of [[Virtual reality|Virtual Reality]] (VR), [[Computer simulation|Computer Simulation]] and [[Expert systems|Expert Systems]], applied to the specific domain of facilities.\n\nThe VF type of computer simulation allows detailed analysis and prototyping of air flow in the data center by making use of [[Computational fluid dynamics|Computational Fluid Dynamics]] (CFD) techniques. This in turn allows the air flow and temperatures of the facility to be analyzed visually ([[Scientific visualization|Scientific Visualisation]]) and numerically to study and predict what will happen in the real facility. The importance of scientific methods in design of mission critical facilities has become a necessity, since the performance gains predicted by [[Moore’s law|Moore's Law]] go hand in hand with a rise in power and heat dissipated by equipment. Rules of thumb have proven to be no longer adequate.\n\n==VF design purposes==\n\n* Green field design\n* Asset management\n* Troubleshooting existing data centers\n* Making existing data centers more resilient\n* Making existing data centers more energy efficient\n* Cost prediction\n* Staff training\n* Capacity planning\n* Load growth management\n \nThe VF is now being employed by many large organizations as a way of virtually assessing a situation before having to spend huge sums of money trying to solve a problem in the real facility.\n\nIt is essential to know whether adding new equipment or changing equipment will cause a logistical or thermal problem.  The VF allows the designer or operator to assess the best course of action and gives in depth understanding on unintuive behaviours.\n\n==References==\n{{reflist}}\n* {{Citation\n  | last = Seymour\n  | first = Mark\n  | title = Virtual Data Centre Design. A blueprint for success\n  | url=http://www.futurefacilities.com/newsarticles/articles/commerzbankarticlesummerZDTjournal.pdf\n  | accessdate = 2007-09-26}}\n\n[[Category:Data management]]"]
['Reference data', '15349103', "{{For|use in finance|Reference data (financial markets)}}\n'''Reference data''' are [[data]] that define the set of permissible values to be used by other [[data field]]s. Reference data gain in value when they are widely re-used and widely referenced. Typically, they do not change overly much in terms of definition, apart from occasional revisions. Reference data are often defined by standards organizations, such as country codes as defined in [[ISO 3166-1]].<ref>{{Cite web|title = IBM Redbooks {{!}} Reference Data Management|url = http://www.redbooks.ibm.com/abstracts/tips1016.html|website = www.redbooks.ibm.com|date = 2013-05-16|accessdate = 2015-12-09|language = en}}</ref><ref>{{Cite web|title = Reference Data Management and Master Data: Are they Related ?   (Oracle Master Data Management)|url = https://blogs.oracle.com/mdm/entry/reference_data_management_and_master|website = blogs.oracle.com|accessdate = 2015-12-09}}</ref>\n\nExamples of reference data include:\n* [[Units of measurement]]\n* [[Country code]]s\n* Corporate codes\n* [[conversion of units|Fixed conversion rates]] e.g., [[weight]], [[temperature]], and [[length]]\n* [[Calendar]] structure and constraints\n\n==Differences with master data==\nReference data should be distinguished from [[master data]], which represent key business entities such as customers and materials in all their necessary detail (e.g., for customers: number, name, address, and date of account creation). In contrast, reference data usually consist only of a list of permissible values and attached textual descriptions. A further difference between reference data and master data is that a change to the reference data values may require an associated change in business process to support the change; a change in master data will always be managed as part of existing business processes. For example, adding a new customer or sales product is part of the standard business process. However, adding a new product classification (e.g. restricted sales item) or a new customer type (e.g. gold level customer) will result in a modification to the business processes to manage those items.\n\n==References==\n<references />\n\n==Further reading==\n* {{Book reference|title = Managing Reference Data in Enterprise Databases|last = Chisholm|first = Malcolm|publisher = Morgan Kaufmann Publishers|year = 2001|isbn = 1558606971|location = |pages = }}\n* {{Book reference|title = Master Data Management for SaaS Applications|last = Whei-Jen|first = Chen|publisher = IBM Redbooks|year = 2014|isbn = 978-0738440040|location = |pages = }}\n* {{Book reference|title = Master Data Management and Data Governance|last = Berson|first = Alex|publisher = McGraw-Hill Osborne Media|year = 2011|isbn = 978-0071744584|location = |pages = }}\n\n==See also==\n* [[Master Data]]\n* [[Data modeling]]\n* [[Master Data Management]]\n* [[Enterprise bookmarking]]\n* [[Data architecture]]\n* [[Transaction data]]\n* [[Code_(metadata)]]\n\n== External links ==\n* [https://msdn.microsoft.com/en-us/library/hh213066.aspx Microsoft MSDN, Reference Data Services in DQS, 2012]\n\n[[Category:Data management]]"]
['Information architecture', '185945', '{{Information science}}\n\'\'\'Information architecture\'\'\' (\'\'\'IA\'\'\') is the structural design of shared [[information]] environments; the art and science of organizing and labelling [[website]]s, [[intranet]]s, [[online communities]] and [[software]] to support usability and findability; and an emerging [[community of practice]] focused on bringing principles of [[design]] and [[architecture]] to the digital landscape.<ref name = "What">{{Cite journal | title = What is IA? | publisher = Information Architecture Institute | url = http://www.iainstitute.org/documents/learn/What_is_IA.pdf | format = [[PDF]] | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}.</ref>  Typically, it involves a [[Scientific modelling|model]] or [[concept]] of [[information]] that is used and applied to activities which require explicit details of complex [[information system]]s. These activities include [[library]] systems and [[database]] development.\n\nInformation architecture is considered to have been founded by [[Richard Saul Wurman]].<ref name = "Richard Saul Wurman, Cooper-Hewitt">{{cite web|title=Richard Saul Wurman awarded for Lifetime Achievement|url=http://wurman.com/rsw/|publisher=Smithsonian Cooper-Hewitt, National Design Museum|accessdate=19 April 2014}}</ref> Today there is a growing network of active IA specialists who constitute the [[Information Architecture Institute]].<ref>{{Cite journal | title = Join the IA Network | publisher = Information Architecture Institute | url = http://www.iainstitute.org/en/network/ | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}.</ref>\n\n==Definition==\n\'\'Information architecture\'\' has somewhat different meanings in different branches of [[Information system|IS]] or [[Information technology|IT]]:\n# The structural design of shared information environments.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}\n# The art and science of organizing and labeling web sites, intranets, online communities, and software to support [[findability]] and [[usability]].<ref name="What"/><ref>Morville&nbsp;&amp; Rosenfeld (2007). p.&nbsp;4. "The art and science of shaping information products and experienced to support usability and findability."</ref>\n# An emerging [[community of practice]] focused on bringing principles of design and architecture to the digital landscape.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}<ref>Resmini, A. & Rosati, L. (2012). A Brief History of Information Architecture. \'\'Journal of Information Architecture\'\'. Vol. 3, No. 2. [Available at http://journalofia.org/volume3/issue2/03-resmini/]. Originally published in Resmini, A. & Rosati L. (2011). \'\'Pervasive Information Architecture\'\'. Morgan Kauffman. (Edited by the authors).</ref>\n# The combination of organization, labeling, search and navigation systems within websites and intranets.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}\n# Extracting required parameters/data of Engineering Designs in the process of creating a knowledge-base linking different systems and standards.  \n# A subset of [[data architecture]] where usable data (a.k.a. information) is constructed in and designed or arranged in a fashion most useful or empirically holistic to the users of this data.\n# The practice of organizing the information / content / functionality of a web site so that it presents the best user experience it can, with information and services being easily usable and findable (as applied to web design and development).<ref>{{Cite web|url=https://developer.mozilla.org/en-US/docs/Glossary/Information_architecture|title=Information Architecture|last=|first=|date=|website=|publisher=Mozilla Developer Network|access-date=}}</ref>\n\n=== Debate ===\nThe difficulty in establishing a common definition for "information architecture" arises partly from the term\'s existence in multiple fields.  In the field of [[systems design]], for example, information architecture is a component of [[enterprise architecture]] that deals with the information component when describing the structure of an enterprise.\n\nWhile the definition of information architecture is relatively well-established in the field of systems design, it is much more debatable within the context of online information systems (i.e., websites). Andrew Dillon refers to the latter as the "big IA–little IA debate".<ref>{{Cite journal | last = Dillon | first = A | year = 2002 | title = Information Architecture in JASIST: Just where did we come from? | journal = Journal of the American Society for Information Science and Technology | volume = 53 | pages = 821–23 | issue = 10 | doi = 10.1002/asi.10090 | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}.</ref> In the little IA view, information architecture is essentially the application of [[information science]] to [[web design]] which considers, for example, issues of classification and information retrieval. In the big IA view, information architecture involves more than just the organization of a website; it also factors in [[user experience]], thereby considering [[usability]] issues of [[information design]].\n\n==Information architect==\nAbout the term \'\'\'information architect\'\'\' [[Richard Saul Wurman]] wrote: "I mean architect as used in the words \'\'architect of foreign policy\'\'. I mean architect as in the creating of systemic, structural, and orderly principles to make something work — the thoughtful making of either artifact, or idea, or policy that informs because it is clear."<ref>Wurman, "Introduction", in: \'\'Information Architects\'\' (1997). p. 16.</ref>\n\n==Notable people in information architecture==\n\n===Pioneers===\n*[[Richard Saul Wurman]]\n*[[Peter Morville]]\n*[[Louis Rosenfeld]]\n\n===First generation===\n*Jorge Arango\n*[[Jesse James Garrett]]\n*[[Adam Greenfield]]\n*[[Peter Merholz]]\n*[[Eric Reiss]]\n*[[Donna Spencer]]\n*[[Christina Wodtke]]\n\n===Second generation===\n*[[Abby Covert]]\n*[[Andrew Hinton]]\n*[[Dan Klyn]]\n*[[Andrea Resmini]]\n\n===Influencers===\n*[[David Weinberger]]\n\n== See also ==\n{{Div col}}\n* [[Applications architecture]]\n* [[Card sorting]]\n* [[Chief experience officer]]\n* [[Content management]]\n* [[Content strategy]]\n* [[Controlled vocabulary]]\n* [[Data management]]\n* [[Data presentation architecture]]\n* [[Digital humanities]]\n* [[Ecological interface design]]\n* [[Enterprise information security architecture]]\n* [[Faceted classification]]\n* [[Human factors and ergonomics]]\n* [[Informatics]]\n* [[Interaction design]]\n* [[Process architecture]]\n* [[Site map]]\n* [[Social information architecture]]\n* [[Tree testing]]\n* [[User experience design]]\n* {{section link|Visualization (graphics)|Knowledge visualization}}\n* [[Wayfinding]]\n* [[Web graph]]\n* [[Web literacy]] (Infrastructure)\n{{Div col end}}\n\n== References ==\n{{reflist}}\n\n== Bibliography ==\n* {{Cite book | editor-last1 = Wurman | editor-first1 = Richard Saul | editor1-link = Richard Saul Wurman | isbn = 1-888-00138-0 | url = http://www.amazon.com/dp/1888001380 | year = 1997 | title = Information Architects | edition = 1st | publisher = Graphis Inc. }}\n* {{Cite book | last2 = Rosenfeld | first2 = Louis | author2-link = Lou Rosenfeld | last1 = Morville | first1 = Peter | author1-link = Peter Morville | isbn = 0-596-52734-9 | url = https://books.google.com/books?id=2d2Ry2hZc2MC&printsec=frontcover&dq=information+architecture#v=onepage&q&f=false | year = 2007 | title = Information architecture for the World Wide Web | edition = 3rd|publisher = O\'Reilly & Associates | place = Sebastopol, CA | ref=harv}}\n* {{Cite book | last1 = Brown | first1 = Peter  | isbn = 0-471-48679-5 | url = http://www.amazon.com/dp/0471486795 | year = 2003 | title = Information Architecture with XML | edition = 1st | publisher = John Wiley & Sons Ltd. }}\n* {{Cite book | last1 = Wodtke | first1 = Christina | author1-link = Christina Wodtke | isbn = 0-321-60080-0 | url = https://books.google.com/books?id=Tp40QFGCU2sC | year = 2009 | title = Information Architecture - Blueprints for the Web | edition = 2nd | publisher = New Riders }}\n* {{Cite book | last1 = Resmini | first1 = Andrea | last2 = Rosati | first2 = Luca | isbn = 0-123-82094-4 | url = https://books.google.com/books?id=ntWc13nSiNkC | year = 2011 | title = Pervasive Information Architecture - Designing Cross-channel User Experiences | edition = 1st | publisher = Morgan Kauffman }}\n\n== Further reading ==\n* {{cite book|author1=Wei Ding|author2=Xia Lin|title=Information Architecture: The Design and Integration of Information Spaces|url=https://books.google.com/books?id=-wy3RhKoWWQC|date= 15 May 2009 | publisher=Morgan & Claypool |isbn=978-1-59829-959-5}}\n* {{cite book|author1=Sue Batley|title=Information Architecture for Information Professionals|url=https://books.google.com/books?id=6g0PAQAAMAAJ|date=January 2007| publisher=Woodhead Publishing |isbn=978-1-84334-233-5}}\n* {{cite book|author1=Earl Morrogh|title=Information Architecture: An Emerging 21st Century Profession\n|url=https://books.google.com/books?id=JzlmQgAACAAJ&dq|year=2003| publisher=Prentice Hall |isbn=9780130967466}}\n* {{cite book|author1=Peter Van Dijck|title=Information Architecture for Designers: Structuring Websites for Business Success |url=https://books.google.com/books?id=Wy2sb0r_udYC&dq|date=August 1, 2003| publisher=Rotovision|isbn=9782880467319}}\n* {{cite book|author1=Alan Gilchrist|author2=Barry Mahon|title=Information Architecture: Designing Information Environments for Purpose|url=https://books.google.com/books?id=akxqAAAAMAAJ&q|year=2004| publisher=Facet|isbn=9781856044875}}\n\n{{Semantic Web}}\n\n[[Category:Data management]]\n[[Category:Enterprise architecture]]\n[[Category:Information architects]]\n[[Category:Information governance]]\n[[Category:Information science]]\n[[Category:Information technology management]]\n[[Category:Information technology]]\n[[Category:Records management]]\n[[Category:Technical communication]]\n[[Category:Information architecture| ]]']
['Schema crosswalk', '18048026', 'A \'\'\'schema crosswalk\'\'\' is a table that shows equivalent elements (or "fields") in more than one [[database schema]]. It maps the elements in one schema to the equivalent elements in another schema.\n\nCrosswalk tables are often employed within or in parallel to [[enterprise systems]], especially when multiple systems are interfaced or when the system includes [[legacy system]] data. In the context of Interfaces, they function as a sort of internal [[Extract, Transform, Load|ETL]] mechanism.\n\nFor example, this is a [[metadata]] crosswalk from [[MARC standards|MARC]] to [[Dublin Core]]:\n\n<center>\n{| class="wikitable"\n|-\n! MARC field\n! \n! Dublin Core element\n|-\n| $260c (Date of publication, distribution, etc.)\n| →\n| Date.Created\n|-\n| 522 (Geographic Coverage Note)\n| →\n| Coverage.Spatial\n|-\n| $300a (Physical Description)\n| →\n| Format.Extent\n|}\n</center>\n\nCrosswalks show people where to put the data from one scheme into a different scheme. They are often used by libraries, archives, museums, and other cultural institutions to translate data to or from [[MARC standards|MARC]], [[Dublin Core]], [[Text Encoding Initiative|TEI]], and other metadata schemes.  For example, say an archive has a MARC record in their catalog describing a manuscript.  If the archive makes a digital copy of that manuscript and wants to display it on the web along with the information from the catalog, it will have to translate the data from the MARC catalog record into a different format such as [[Metadata Object Description Schema|MODS]] that is viewable in a webpage.  Because MARC has different fields than MODS, decisions must be made about where to put the data into MODS. This type of "translating" from one format to another is often called "metadata mapping" or "field mapping," and is related to "[[data mapping]]", and "[[Semantic mapper|semantic mapping]]".\n\nCrosswalks also have several technical capabilities.  They help databases using different metadata schemes to share information. They help metadata harvesters create union catalogs.  They enable search engines to search multiple databases simultaneously with a single query.\n\n== Challenges for crosswalks ==\n\nOne of the biggest challenges for crosswalks is that no two metadata schemes are 100% equivalent.  One scheme may have a field that doesn\'t exist in another scheme, or it may have a field that is split into two different fields in another scheme; this is why you often lose data when mapping from a complex scheme to a simpler one.  For example, when mapping from MARC to Simple Dublin Core, you lose the distinction between types of titles:\n\n<center>\n{| class="wikitable"\n|-\n! MARC field\n! \n! Dublin Core element\n|-\n| 210 Abbreviated Title\n| →\n| Title\n|-\n| 222 Key Title\n| →\n| Title\n|-\n| 240 Uniform Title\n| → \n| Title\n|-\n| 242 Translated Title\n| →\n| Title\n|-\n| 245 Title Statement\n| →\n| Title\n|-\n| 246 Variant Title\n| →\n| Title\n|}\n</center>\n\nSimple Dublin Core only has one single "Title" element so all of the different types of MARC titles get lumped together without any further distinctions.  This is called "many-to-one" mapping. This is also why, once you\'ve translated these titles into Simple Dublin Core you can\'t translate them back into MARC.  Once they\'re Simple Dublin Core you\'ve lost the MARC information about what types of titles they are so when you map from Simple Dublin Core back to MARC, all the data in the "Title" element maps to the basic MARC 245 Title Statement field.<ref>[http://www.loc.gov/marc/dccross.html "Dublin Core to MARC Crosswalk,"] Network Development and MARC Standards Office, Library of Congress</ref>\n\n<center>\n{| class="wikitable"\n|-\n! Dublin Core element\n!\n! MARC field\n|-\n| Title\n| →\n| 245 Title Statement\n|-\n| Title\n| →\n| 245 Title Statement\n|-\n| Title\n| →\n| 245 Title Statement\n|-\n| Title\n| →\n| 245 Title Statement\n|-\n| Title\n| →\n| 245 Title Statement\n|-\n| Title\n| →\n| 245 Title Statement\n|}\n</center>\n\nThis is why crosswalks are said to be "lateral" (one-way) mappings from one scheme to another.  Separate crosswalks would be required to map from scheme A to scheme B and from scheme B to scheme A.<ref>{{Cite book|title=Metadata fundamentals for all librarians|last=Caplan|first=Priscilla|publisher=American Library Association|year=2003|isbn=0838908470|location=Chicago|pages=39|quote=|via=}}</ref>\n\n===Difficulties in mapping===\nOther mapping problems arise when:\n\n*One scheme has one element that needs to be split up with different parts of it placed in multiple other elements in the second scheme ("one-to-many" mapping)\n*One scheme allows an element to be repeated more than once while another only allows that element to appear once with multiple terms in it\n*Schemes have different data formats (e.g. \'\'John Doe\'\' or \'\'Doe, John\'\')\n*An element in one scheme is indexed but the equivalent element in the other scheme is not\n*Schemes may use different controlled vocabularies\n*Schemes change their standards over time\n\nSome of these problems are simply not fixable. As Karen Coyle says in "\'\'Crosswalking Citation Metadata: The University of California\'s Experience,\'\'"\n\n<blockquote>"The more metadata experience we have, the more it becomes clear that metadata perfection is not attainable, and anyone who attempts it will be sorely disappointed.  When metadata is crosswalked between two or more unrelated sources, there will be data elements that cannot be reconciled in an ideal manner.  The key to a successful metadata crosswalk is intelligent flexibility.  It is essential to focus on the important goals and be willing to compromise in order to reach a practical conclusion to projects."<ref><u>in</u> "Metadata in Practice" Diane I. Hillmann and Elaine L. Westbrooks, eds., American Library Association, Chicago, 2004, p. 91.</ref></blockquote>\n\n==Examples==\n\nMARC to Dublin Core (Library of Congress) \nhttp://loc.gov/marc/marc2dc.html\n\nDublin Core to MARC21 (Library of Congress) \nhttp://www.loc.gov/marc/dccross.html\n\nDublin Core to UNIMARC (UKOLN)\nhttp://www.ukoln.ac.uk/metadata/interoperability/dc_unimarc.html\n\nTEI to and from MARC\nhttp://purl.oclc.org/NET/teiinlibraries\n\nFGDC to USMARC (Alexandria) \nhttp://www.alexandria.ucsb.edu/public-documents/metadata/fgdc2marc.html\n\nONIX to MARC21 (LC) \nhttp://www.loc.gov/marc/onix2marc.html\n\nVRA to MARC (Indiana University) \nhttp://php.indiana.edu/%7Efryp/marcmap.html\n\nMetadata Mappings (MIT Library)\nhttp://web.archive.org/web/20080720134522/http://libraries.mit.edu/guides/subjects/metadata/mappings.html\n\nMapping Between Metadata formats (UKOLN) \nhttp://www.ukoln.ac.uk/metadata/interoperability/\n\nInternational Metadata Standard Mappings (Academia Sinica) \nhttp://www.sinica.edu.tw/%7Emetadata/standard/mapping-foreign_eng.htm\n\nJATS to MARC\nhttp://webservices.itcs.umich.edu/mediawiki/jats/index.php/JATS-to-MARC_mapping\n\n== See also ==\n* [[Meta tag]]\n* [[Metadata]]\n* [[Database]]\n\n==References==\n {{Reflist}}\n\n==External links==\n* [http://www.oclc.org/research/researchworks/schematrans/default.htm "Metadata Crosswalk Depository" (SchemaTrans)](OCLC)\n* [http://www.ukoln.ac.uk/metadata/interoperability "Mapping Between Metadata Formats"] (UKOLN)\n* [http://www.getty.edu/research/conducting_research/standards/intrometadata/path.html "Crosswalks the Path to Universal Access?"] (Getty)\n* [http://www.dlib.org/dlib/june06/chan/06chan.html "Metadata Interoperability and Standardization - A Study of Methodology Part I"] (D-Lib)\n\n[[Category:Data management]]\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]\n[[Category:Metadata]]\n[[Category:Technical communication]]']
['Enterprise data management', '2654483', "{{notability|date=July 2015}}\n{{No footnotes|date=November 2009}}\n'''Enterprise Data Management''' ('''EDM''') is:\n\n#A concept – referring to the ability of an organization to precisely define, easily integrate and effectively retrieve data for both internal applications and external communication.\n#A business objective – focused on the creation of accurate, consistent and transparent content.  EDM emphasizes data precision, granularity and meaning and is concerned with how the content is integrated into [[business application]]s as well as how it is passed along from one [[business process]] to another.\n\nEDM arose to address circumstances where users within organizations independently source, model, manage and [[Computer data storage|store data]].  Uncoordinated approaches by various segments of the organization can result in [[data conflict]]s and quality inconsistencies – lowering the trustworthiness of the data as it is used for operations and [[Business reporting|reporting]]. \n\nThe goal of EDM is trust and confidence in data assets.  Its components are:\n\n==Strategy and governance==\nEDM requires a strategic approach to choosing the right processes, technologies and resources (i.e. data owners, governance, stewardship, [[data analyst]]s and [[data architect]]s).  EDM is a challenge for organizations because it requires alignment among multiple stakeholders (including IT, operations, finance, strategy and [[end-user]]s) and relates to an area (creation and use of common data) that has not traditionally had a clear “owner.”  \n\nThe governance challenge can be a big obstacle to the implementation of an effective EDM because of the difficulties associated with providing a [[business case]] on the benefits of data management.  The core of the challenge is due to the fact that data quality has no intrinsic value.  It is an enabler of other processes and the true benefits of effective data management are systematic and intertwined with other processes.  This makes it hard to quantify all the downstream implications or upstream improvements.  \n\nThe difficulties associated with quantification of EDM benefits translate into challenges with the positioning of EDM as an organizational priority.  Achieving organizational alignment on the importance of data management (as well as managing data as an ongoing area of focus) is the domain of [[governance]].  In recent years the establishment of an EDM and the EDM governance practice has become commonplace despite these difficulties. \n\n==Program implementation==\nImplementation of an EDM program encompasses many processes – all of which need to be coordinated throughout the organization and managed while maintaining operational continuity.  Below are some of the major components of EDM implementation that should be given consideration:\n\n===Stakeholder requirements===\nEDM requires alignment among multiple stakeholders (at the right level of authority) who all need to understand and support the EDM objectives.  EDM begins with a thorough understanding of the requirements of the end users and the organization as a whole.  Managing stakeholder requirements is a critical, and ongoing, process based in an understanding of [[workflow]], data dependencies and the tolerance of the organization for operational disruption.  Many organizations use formal processes such as [[service level agreement]]s to specify requirements and establish EDM program objectives.\n\n===Policies and procedures===\nEffective EDM usually includes the creation, documentation and enforcement of operating policies and procedures associated with [[change management]], (i.e. [[data model]], business [[glossary]], master data shared domains, [[data cleansing]] and [[data normalization|normalization]]), data [[stewardship]], security constraints and dependency rules.  In many cases, these policies and procedures are documented for the first time as part of the EDM initiative.\n\n===Data definitions and tagging===\n\nOne of the core challenges associated with EDM is the ability to compare data that is obtained from multiple internal and external sources.  In many circumstances, these sources use inconsistent terms and definitions to describe the data content itself – making it hard to compare data, hard to automate business processes, hard to feed complex applications and hard to exchange data.  This frequently results in a difficult process of [[data mapping]] and cross-referencing.  Normalization of all the terms and definitions at the data attribute level is referred to as the [[metadata]] component of EDM and is an essential prerequisite for effective data management.\n\n===Platform requirements===\n\nEven though EDM is fundamentally a data content challenge, there is a core technology dimension that must be addressed.  Organizations need to have a functional storage platform, a comprehensive data model and a robust messaging infrastructure.  They must be able to integrate data into applications and deal with the challenges of the existing (i.e. legacy) technology infrastructure.  Building the platform or partnering with an established technology provider on how the data gets stored and integrated into business applications is an essential component of the EDM process.\n\nEnterprise data management as an essential business requirement has emerged as a priority for many organizations.  The objective is confidence and trust in data as the glue that holds business strategy together.\n\n==See also==\n* [[Master data management]]\n* [[Master Data]]\n\n==References==\n{{Reflist}}\n\n;General\n* Enterprise Data Management Council http://www.edmcouncil.org\n* [http://www.thegoldensource.com/files/EDM_Finextra_Report_Final.pdf Issues in Enterprise Data Management: A Survey Report, 12/06]\n\n[[Category:Data management]]\n[[Category:Product lifecycle management]]"]
['Conference on Innovative Data Systems Research', '21047573', "{{multiple issues|\n{{notability|Events|date=September 2011}}\n{{primary sources|date=September 2011}}\n}}\n\n{{Infobox Academic Conference\n | history = 2002–\n | discipline = [[Database]]\n | abbreviation = CIDR\n | publisher = CIDR Conference\n | country= [[United States]]\n | frequency = biennial\n}}\nThe '''Conference on Innovative Data Systems Research''' ('''CIDR''') is a biennial [[computer science]] conference focused on research into new techniques for [[data management]]. It was started in 2002 by [[Michael Stonebraker]], [[Jim Gray (computer scientist)|Jim Gray]], and [[David DeWitt]], and is held at the [[Asilomar Conference Grounds]] in [[Pacific Grove, California]].\n\nCIDR focuses on presenting work that is more speculative, radical, or provocative than what is typically accepted by the traditional database research conferences (such as the [[International Conference on Very Large Data Bases]] (VLDB) and the [[ACM SIGMOD Conference]]).\n\n==See also==\n* [[International Conference on Very Large Data Bases]] (VLDB)\n* [[ACM SIGMOD Conference]]\n\n==External links==\n* [http://www-db.cs.wisc.edu/cidr/ CIDR website]\n\n[[Category:Data management]]\n[[Category:Computer science conferences]]\n\n\n{{database-stub}}\n{{compu-conference-stub}}"]
['Write–read conflict', '217741', "In [[computer science]], in the field of [[database]]s, '''write–read conflict''', also known as '''reading uncommitted data''', is a computational anomaly associated with interleaved execution of transactions.\n\nGiven a schedule S\n\n:<math>S = \\begin{bmatrix}\nT1 & T2 \\\\\nR(A) &  \\\\\nW(A) & \\\\\n & R(A) \\\\\n & W(A)\\\\\n & R(B) \\\\\n & W(B) \\\\\n & Com. \\\\\nR(B) & \\\\\nW(B) & \\\\\nCom. & \\end{bmatrix}</math>\n\nT2 could read a database object A, modified by T1 which hasn't committed. This is a '''''dirty read'''''.\n\nT1 may write some value into A which makes the database inconsistent.  It is possible that interleaved execution can expose this inconsistency and lead to inconsistent final database state, violating [[ACID]] rules.\n\n[[Strict two-phase locking|Strict 2PL]] overcomes this inconsistency by locking T2 out from performing a Read/Write on A.  Note however that [[Strict two-phase locking|Strict 2PL]] can have a number of drawbacks, such as the possibility of [[deadlock]]s.\n\n== See also ==\n\n* [[Concurrency control]]\n* [[Read–write conflict]]\n* [[Write–write conflict]]\n\n==References==\n{{reflist}}\n{{Unreferenced|date=August 2009}}\n\n{{DEFAULTSORT:Write-Read Conflict}}\n[[Category:Data management]]\n[[Category:Transaction processing]]"]
['Isolation (database systems)', '325521', '{{Refimprove|date=January 2009}}\nIn [[database]] systems, \'\'\'isolation\'\'\' determines how transaction integrity is visible to other users and systems. For example, when a user is creating a Purchase Order and has created the header, but not the Purchase Order lines, is the header available for other systems/users, carrying out [[Concurrency (computer science)|concurrent]] operations (such as a report on Purchase Orders), to see?\n\nA lower isolation level increases the ability of many users to access data at the same time, but increases the number of concurrency effects (such as dirty reads or lost updates) users might encounter. Conversely, a higher isolation level reduces the types of concurrency effects that users may encounter, but requires more system resources and increases the chances that one transaction will block another.<ref>"Isolation Levels in the Database Engine", Technet, Microsoft, http://technet.microsoft.com/en-us/library/ms189122(v=SQL.105).aspx</ref>\n\nIsolation is typically defined at database level as a property that defines how/when the changes made by one operation become visible to other. On older systems, it may be implemented systemically, for example through the use of temporary tables. In two-tier systems, a Transaction Processing (TP) manager is required to maintain isolation. In n-tier systems (such as multiple websites attempting to book the last seat on a flight), a combination of stored procedures and transaction management is required to commit the booking and send confirmation to the customer.<ref>"The Architecture of Transaction Processing Systems", Chapter 23, Evolution of Processing Systems, Department of Computer Science, Stony Brook University, retrieved 20 March 2014, http://www.cs.sunysb.edu/~liu/cse315/23.pdf</ref>\n\nIsolation is one of the [[ACID]] (Atomicity, Consistency, Isolation, Durability) properties.\n\n==Concurrency control==\n[[Concurrency control]] comprises the underlying mechanisms in a [[DBMS]] which handles isolation and guarantees related correctness. It is heavily utilized by the database and storage engines (see above) both to guarantee the correct execution of concurrent transactions, and (different mechanisms) the correctness of other DBMS processes. The transaction-related mechanisms typically constrain the database data access operations\' timing ([[Schedule (computer science)|transaction schedules]]) to certain orders characterized as the [[serializability]] and [[recoverability]] schedule properties. Constraining database access operation execution typically means reduced performance (rates of execution), and thus concurrency control mechanisms are typically designed to provide the best performance possible under the constraints. Often, when possible without harming correctness, the serializability property is compromised for better performance. However, recoverability cannot be compromised, since such typically results in a quick database integrity violation.\n\n[[Two-phase locking]] is the most common transaction concurrency control method in DBMSs, used to provide both serializability and recoverability for correctness. In order to access a database object a transaction first needs to acquire a [[Lock (database)|lock]] for this object. Depending on the access operation type (e.g., reading or writing an object) and on the lock type, acquiring the lock may be blocked and postponed, if another transaction is holding a lock for that object.\n\n==Isolation levels==\nOf the four [[ACID]] properties in a [[Database management system|DBMS]] (Database Management System), the isolation property is the one most often relaxed.  When attempting to maintain the highest level of isolation, a DBMS usually acquires [[Lock (database)|locks]] on data or implements [[multiversion concurrency control]], which may result in a loss of [[concurrency (computer science)|concurrency]].  This requires adding logic for the [[software application|application]] to function correctly.\n\nMost DBMSs offer a number of \'\'transaction isolation levels\'\', which control the degree of locking that occurs when selecting data. For many database applications, the majority of database transactions can be constructed to avoid requiring high isolation levels (e.g. SERIALIZABLE level), thus reducing the locking overhead for the system.  The programmer must carefully analyze database access code to ensure that any relaxation of isolation does not cause software bugs that are difficult to find. Conversely, if higher isolation levels are used, the possibility of [[deadlock]] is increased, which also requires careful analysis and programming techniques to avoid.\n\nThe isolation levels defined by the [[American National Standards Institute|ANSI]]/[[International Organization for Standardization|ISO]] [[SQL]] standard are listed as follows.\n\n===Serializable===\nThis is the \'\'highest\'\' isolation level.\n\nWith a lock-based [[concurrency control]] DBMS implementation, [[serializability]] requires read and write locks (acquired on selected data) to be released at the end of the transaction.  Also \'\'range-locks\'\' must be acquired when a [[Select (SQL)|SELECT]] query uses a ranged \'\'WHERE\'\' clause, especially to avoid the \'\'phantom reads\'\' phenomenon (see below).\n\nWhen using non-lock based concurrency control, no locks are acquired; however, if the system detects a \'\'write collision\'\' among several concurrent transactions, only one of them is allowed to commit.  See \'\'[[snapshot isolation]]\'\' for more details on this topic.\n\nFrom : (Second Informal Review Draft) ISO/IEC 9075:1992, Database Language SQL- July 30, 1992:\n\'\'The execution of concurrent SQL-transactions at isolation level SERIALIZABLE is guaranteed to be serializable. A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins.\'\'\n\n===Repeatable reads===\nIn this isolation level, a lock-based [[concurrency control]] DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction.  However, \'\'range-locks\'\' are not managed, so \'\'\'\'\'[[Isolation (database systems)#Phantom reads|phantom reads]]\'\'\'\'\' can occur.\n\nAlso, write skew is possible when one transaction updates column to some color whereas competing transactions updates the same column to some other color(s). In serial execution of the transactions, you should end up with the whole column unicolored whereas repeatable read admits a mixture of colors.<ref>[https://wiki.postgresql.org/wiki/SSI#Simple_Write_Skew Postgresql wiki - SSI]</ref>\n\n===Read committed===\nIn this isolation level, a lock-based [[concurrency control]] DBMS implementation keeps write locks (acquired on selected data) until the end of the transaction, but read locks are released as soon as the [[Select (SQL)|SELECT]] operation is performed (so the \'\'non-repeatable reads\'\' phenomenon can occur in this isolation level, as discussed below). As in the previous level, \'\'range-locks\'\' are not managed.\n\nPutting it in simpler words, read committed is an isolation level that guarantees that any data read is committed at the moment it is read. It simply restricts the reader from seeing any intermediate, uncommitted, \'dirty\' read. It makes no promise whatsoever that if the transaction re-issues the read, it will find the same data; data is free to change after it is read.\n\n===Read uncommitted===\nThis is the \'\'lowest\'\' isolation level. In this level, \'\'\'\'\'[[Isolation (database systems)#Dirty reads|dirty reads]]\'\'\'\'\' are allowed, so one transaction may see \'\'not-yet-committed\'\' changes made by other transactions.\n\nSince each isolation level is stronger than those below, in that no higher isolation level allows an action forbidden by a lower one, the standard permits a DBMS to run a transaction at an isolation level stronger than that requested (e.g., a "Read committed" transaction may actually be performed at a "Repeatable read" isolation level).\n\n==Default isolation level==\nThe \'\'default isolation level\'\' of different [[Database management system|DBMS]]\'s varies quite widely. Most databases that feature transactions allow the user to set any isolation level. Some DBMS\'s also require additional syntax when performing a SELECT statement to acquire locks (e.g. \'\'SELECT ... FOR UPDATE\'\' to acquire exclusive write locks on accessed rows).\n\nHowever, the definitions above have been criticized as being ambiguous, and as not accurately reflecting the isolation provided by many databases:\n\n:This paper shows a number of weaknesses in the anomaly approach to defining isolation levels. The three ANSI phenomena are ambiguous, and even in their loosest interpretations do not exclude some anomalous behavior ... This leads to some counter-intuitive results. In particular, lock-based isolation levels have different characteristics than their ANSI equivalents. This is disconcerting because commercial database systems typically use locking implementations. Additionally, the ANSI phenomena do not distinguish between a number of types of isolation level behavior that are popular in commercial systems.<ref name="sql-isolation">\n{{cite web\n| url = http://www.cs.umb.edu/~poneil/iso.pdf\n| title = A Critique of ANSI SQL Isolation Levels\n| accessdate = 29 July 2012 }}\n</ref>\n\nThere are also other criticisms concerning ANSI SQL\'s isolation definition, in that it encourages implementors to do "bad things":\n\n:... it relies in subtle ways on an assumption that a locking schema is used for concurrency control, as opposed to an optimistic or multi-version concurrency scheme. This implies that the proposed semantics are \'\'ill-defined\'\'.<ref>{{cite web\n| accessdate = 2010-03-09\n| publisher = DataStax\n| location = www.DataStax.com\n| title = Customer testimonials (SimpleGeo, CLOUDSTOCK 2010)\n| author = salesforce\n| date = 2010-12-06\n| url = https://www.youtube.com/v/7J61pPG9j90?version=3\n| quote = (see above at about 13:30 minutes of the webcast!)}}</ref>\n\n==Read phenomena==\nThe ANSI/ISO standard SQL 92 refers to three different \'\'read phenomena\'\' when Transaction 1 reads data that Transaction 2 might have changed.\n\nIn the following examples, two transactions take place. In the first, Query 1 is performed. Then, in the second transaction, Query 2 is performed and committed. Finally, in the first transaction, Query 1 is performed again.\n\nThe queries use the following data table:\n\n{|class="wikitable"\n|+ users\n! id !! name !! age\n|-\n| 1  || Joe  || 20\n|-\n| 2  || Jill || 25\n|}\n\n===Dirty reads===\nA \'\'dirty read\'\' (aka \'\'uncommitted dependency\'\') occurs when a transaction is allowed to read data from a row that has been modified by another running transaction and not yet committed.\n\nDirty reads work similarly to [[Isolation (database systems)#Non-repeatable reads|non-repeatable reads]]; however, the second transaction would not need to be committed for the first query to return a different result. The only thing that may be prevented in the READ UNCOMMITTED isolation level is updates appearing out of order in the results; that is, earlier updates will always appear in a result set before later updates.\n\nIn our example, Transaction 2 changes a row, but does not commit the changes.  Transaction 1 then reads the uncommitted data.  Now if Transaction 2 rolls back its changes (already read by Transaction 1) or updates different changes to the database, then the view of the data may be wrong in the records of Transaction 1.\n\n{|style="font-size: 94%;"\n|-\n! Transaction 1\n! Transaction 2\n|-\n|<source lang="sql">\n/* Query 1 */\nSELECT age FROM users WHERE id = 1;\n/* will read 20 */\n</source>\n|\n|-\n|\n|<source lang="sql">\n/* Query 2 */\nUPDATE users SET age = 21 WHERE id = 1;\n/* No commit here */\n</source>\n|-\n|<source lang="sql">\n/* Query 1 */\nSELECT age FROM users WHERE id = 1;\n/* will read 21 */\n</source>\n|\n|-\n|\n|<source lang="sql">\nROLLBACK; /* lock-based DIRTY READ */\n</source>\n|}\n\nBut in this case no row exists that has an id of 1 and an age of 21.\n\n===Non-repeatable reads===\nA \'\'non-repeatable read\'\' occurs, when during the course of a transaction, a row is retrieved twice and the values within the row differ between reads.\n\n\'\'Non-repeatable reads\'\' phenomenon may occur in a lock-based concurrency control method when read locks are not acquired when performing a [[Select (SQL)|SELECT]], or when the acquired locks on affected rows are released as soon as the SELECT operation is performed.  Under the [[multiversion concurrency control]] method, \'\'non-repeatable reads\'\' may occur when the requirement that a transaction affected by a [[commit conflict]] must roll back is relaxed.\n\n{|style="font-size: 94%;"\n|-\n! Transaction 1\n! Transaction 2\n|-\n|<source lang="sql">\n/* Query 1 */\nSELECT * FROM users WHERE id = 1;\n</source>\n|\n|-\n|\n|<source lang="sql">\n/* Query 2 */\nUPDATE users SET age = 21 WHERE id = 1;\nCOMMIT; /* in multiversion concurrency\n   control, or lock-based READ COMMITTED */\n</source>\n|-\n|<source lang="sql">\n/* Query 1 */\nSELECT * FROM users WHERE id = 1;\nCOMMIT; /* lock-based REPEATABLE READ */\n</source>\n|}\n\nIn this example, Transaction 2 commits successfully, which means that its changes to the row with id 1 should become visible. However, Transaction 1 has already seen a different value for \'\'age\'\' in that row. At the SERIALIZABLE and REPEATABLE READ isolation levels, the DBMS must return the old value for the second SELECT. At READ COMMITTED and READ UNCOMMITTED, the DBMS may return the updated value; this is a non-repeatable read.\n\nThere are two basic strategies used to prevent non-repeatable reads. The first is to delay the execution of Transaction 2 until Transaction 1 has committed or rolled back. This method is used when locking is used, and produces the serial [[Schedule (computer science)|schedule]] \'\'\'T1, T2\'\'\'. A serial schedule exhibits \'\'repeatable reads\'\' behaviour.\n\nIn the other strategy, as used in \'\'[[multiversion concurrency control]]\'\', Transaction 2 is permitted to commit first, which provides for better concurrency. However, Transaction 1, which commenced prior to Transaction 2, must continue to operate on a past version of the database&nbsp;— a snapshot of the moment it was started. When Transaction 1 eventually tries to commit, the DBMS checks if the result of committing Transaction 1 would be equivalent to the schedule \'\'\'T1, T2\'\'\'. If it is, then Transaction 1 can proceed. If it cannot be seen to be equivalent, however, Transaction 1 must roll back with a serialization failure.\n\nUsing a lock-based concurrency control method, at the REPEATABLE READ isolation mode, the row with ID = 1 would be locked, thus blocking Query 2 until the first transaction was committed or rolled back. In READ COMMITTED mode, the second time Query 1 was executed, the age would have changed.\n\nUnder multiversion concurrency control, at the SERIALIZABLE isolation level, both SELECT queries see a snapshot of the database taken at the start of Transaction 1. Therefore, they return the same data. However, if Transaction 1 then attempted to UPDATE that row as well, a serialization failure would occur and Transaction 1 would be forced to roll back.\n\nAt the READ COMMITTED isolation level, each query sees a snapshot of the database taken at the start of each query. Therefore, they each see different data for the updated row. No serialization failure is possible in this mode (because no promise of serializability is made), and Transaction 1 will not have to be retried.\n\n===Phantom reads===\nA \'\'phantom read\'\' occurs when, in the course of a transaction, two identical queries are executed, and the collection of rows returned by the second query is different from the first.\n\nThis can occur when \'\'[[range locks]]\'\' are not acquired on performing a \'\'[[Select (SQL)|SELECT]] ... WHERE\'\' operation.\nThe \'\'phantom reads\'\' anomaly is a special case of \'\'Non-repeatable reads\'\' when Transaction 1 repeats a ranged \'\'SELECT ... WHERE\'\' query and, between both operations, Transaction 2 creates (i.e. [[INSERT]]) new rows (in the target table) which fulfill that \'\'WHERE\'\' clause.\n\n{|style="font-size: 95%;"\n|-\n! Transaction 1\n! Transaction 2\n|-\n|<source lang="sql">\n/* Query 1 */\nSELECT * FROM users\nWHERE age BETWEEN 10 AND 30;\n</source>\n|\n|-\n|\n|<source lang="sql">\n/* Query 2 */\nINSERT INTO users(id,name,age) VALUES ( 3, \'Bob\', 27 );\nCOMMIT;\n</source>\n|-\n|<source lang="sql">\n/* Query 1 */\nSELECT * FROM users\nWHERE age BETWEEN 10 AND 30;\nCOMMIT;\n</source>\n|\n|}\n\nNote that Transaction 1 executed the same query twice. If the highest level of isolation were maintained, the same set of rows should be returned both times, and indeed that is what is mandated to occur in a database operating at the SQL SERIALIZABLE isolation level. However, at the lesser isolation levels, a different set of rows may be returned the second time.\n\nIn the SERIALIZABLE isolation mode, Query 1 would result in all records with age in the range 10 to 30 being locked, thus Query 2 would block until the first transaction was committed. In REPEATABLE READ mode, the range would not be locked, allowing the record to be inserted and the second execution of Query 1 to include the new row in its results.\n\n==Isolation Levels, Read Phenomena and Locks==\n\n===Isolation Levels vs Read Phenomena===\n{|class="wikitable"\n! Isolation level !! Dirty reads !! Non-repeatable reads !! Phantoms\n|-\n| Read Uncommitted  || may occur || may occur || may occur\n|-\n| Read Committed  || - || may occur || may occur\n|-\n| Repeatable Read || - || - || may occur\n|-\n| Serializable || - || - || -\n|}\n\nAnomaly Serializable is not the same as Serializable. That is, it is necessary, but not sufficient that a Serializable schedule should be free of all three phenomena types.<ref name="sql-isolation"/>\n\n"may occur" means that the isolation level suffers that phenomenon, while "-" means that it does not suffer it.\n\n===Isolation Levels vs Lock Duration ===\n{{Citation needed|reason=This is a still a mess. Write operations always place locks until commit, whatever isolation level is used. Queries (selects/reads) never place any kind of lock under Read Uncommitted isolation level. Locks placed by read operations (the only operations that are affected by the isolation level used) should not be referred as Read and Range, but as Data Locks and Predicate Locks. Write operations also place Data and Predicate locks, always until commit, whatever isolation level is used.|date=January 2014}}\n\nIn lock-based concurrency control, isolation level determines the duration that locks are held.<br>  \'\'\'"C"\'\'\' - Denotes that locks are held until the transaction commits.<br>\n\'\'\'"S"\'\'\' - Denotes that the locks are held only during the currently executing statement.  Note that if locks are released after a statement, the underlying data could be changed by another transaction before the current transaction commits, thus creating a violation.\n\n{|class="wikitable"\n! Isolation level !! Write Operation !! Read Operation !! Range Operation (...where...)\n|-\n| Read Uncommitted  || S || S || S\n|-\n| Read Committed  || C || S || S\n|-\n| Repeatable Read || C || C || S\n|-\n| Serializable || C || C || C\n|}\n\n==See also==\n* [[Atomicity (database systems)|Atomicity]]\n* [[Consistency (database systems)|Consistency]]\n* [[Durability (database systems)|Durability]]\n* [[Lock (database)]]\n* [[Optimistic concurrency control]]\n* [[Relational Database Management System]]\n* [[Snapshot isolation]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://docs.oracle.com/cd/B12037_01/server.101/b10743/toc.htm  Oracle® Database Concepts], [http://docs.oracle.com/cd/B12037_01/server.101/b10743/consist.htm#sthref1919 chapter 13 Data Concurrency and Consistency, Preventable Phenomena and Transaction Isolation Levels]\n* [http://docs.oracle.com/cd/B19306_01/server.102/b14200/toc.htm Oracle® Database SQL Reference], [http://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_10.htm#i2068385 chapter 19 SQL Statements: SAVEPOINT to UPDATE], [http://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_10005.htm#i2067247 SET TRANSACTION]\n<!-- representations in api: java -->\n* in [[Java Database Connectivity|JDBC]]: [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#field_summary Connection constant fields], [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#getTransactionIsolation() Connection.getTransactionIsolation()], [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#setTransactionIsolation(int) Connection.setTransactionIsolation(int)]\n* in [[Spring Framework]]: [http://static.springsource.org/spring/docs/current/javadoc-api/org/springframework/transaction/annotation/Transactional.html @Transactional], [http://static.springsource.org/spring/docs/current/javadoc-api/org/springframework/transaction/annotation/Isolation.html Isolation]\n<!-- representations in api: .NET_Framework -->\n* [http://www.bailis.org/blog/when-is-acid-acid-rarely/ P.Bailis. When is "ACID" ACID? Rarely]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Isolation (Database Systems)}}\n[[Category:Data management]]\n[[Category:Transaction processing]]']
['Data dictionary', '645139', '{{Distinguish|Dictionary (data structure)}}\n{{Use dmy dates|date=July 2013}}\nA \'\'\'data dictionary\'\'\', or [[metadata repository]], as defined in the \'\'IBM Dictionary of Computing\'\', is a "centralized repository of information about data such as meaning, relationships to other data, origin, usage, and format."<ref>ACM, [http://portal.acm.org/citation.cfm?id=541721 IBM Dictionary of Computing], 10th edition, 1993</ref> The term can have one of several closely related meanings pertaining to [[database]]s and [[database management system]]s (DBMS):\n\n* A [[document]] describing a database or collection of databases\n* An integral [[software component|component]] of a [[Database management system|DBMS]] that is required to determine its structure\n* A piece of [[middleware]] that extends or supplants the native data dictionary of a DBMS\n\n==Documentation==\nThe terms \'\'data dictionary\'\' and \'\'data repository\'\' indicate a more general software utility than a catalogue. A \'\'catalogue\'\' is closely coupled with the DBMS software. It provides the information stored in it to the user and the DBA, but it is mainly accessed by the various software modules of the DBMS itself, such as [[Data definition language|DDL]] and [[Data manipulation language|DML]] compilers, the query optimiser, the transaction processor, report generators, and the constraint enforcer. On the other hand, a \'\'data dictionary\'\' is a data structure that stores [[metadata]], i.e., (structured) data about information. The software package for a stand-alone data dictionary or data repository may interact with the software modules of the DBMS, but it is mainly used by the designers, users and administrators of a computer system for information resource management. These systems maintain information on system hardware and software configuration, documentation, application and users as well as other information relevant to system administration.<ref>Ramez Elmasri, Shamkant B. Navathe: \'\'Fundamentals of Database Systems\'\', 3rd. ed. sect. 17.5, p. 582</ref>\n\nIf a data dictionary system is used only by the designers, users, and administrators and not by the DBMS Software, it is called a \'\'passive data dictionary.\'\' Otherwise, it is called an \'\'active data dictionary\'\' or \'\'data dictionary.\'\'  When a passive data dictionary is updated, it is done so manually and independently from any changes to a DBMS (database) structure. With an active data dictionary, the dictionary is updated first and changes occur in the DBMS automatically as a result.\n\nDatabase [[User (computing)|users]] and [[Application software|application]] developers can benefit from an authoritative data dictionary document that catalogs the organization, contents, and conventions of one or more databases.<ref>TechTarget, \'\'SearchSOA\'\', [http://searchsoa.techtarget.com/sDefinition/0,,sid26_gci211896,00.html What is a data dictionary?]</ref> This typically includes the names and descriptions of various [[Table (database)|tables]] (records or Entities) and their contents ([[Column (database)|fields]]) plus additional details, like the [[Data type|type]] and length of each [[data element]].  Another important piece of information that a data dictionary can provide is the relationship between Tables.  This is sometimes referred to in Entity-Relationship diagrams, or if using Set descriptors, identifying which Sets database Tables participate in.\n\nIn an active data dictionary constraints may be placed upon the underlying data.  For instance, a Range may be imposed on the value of numeric data in a data element (field), or a Record in a Table may be FORCED to participate in a set relationship with another Record-Type.  Additionally, a distributed DBMS may have certain location specifics described within its active data dictionary (e.g. where Tables are physically located).\n\nThe data dictionary consists of record types (tables) created in the database by systems generated command files, tailored for each supported back-end DBMS. Command files contain SQL Statements for CREATE TABLE, CREATE UNIQUE INDEX, ALTER TABLE (for referential integrity), etc., using the specific statement required by that type of database.\n\nThere is no universal standard as to the level of detail in such a document.\n\n==Middleware==\nIn the construction of database applications, it can be useful to introduce an additional layer of data dictionary software, i.e. [[middleware]], which communicates with the underlying DBMS data dictionary. Such a "high-level" data dictionary may offer additional features and a degree of flexibility that goes beyond the limitations of the native "low-level" data dictionary, whose primary purpose is to support the basic functions of the DBMS, not the requirements of a typical application. For example, a high-level data dictionary can provide alternative [[entity-relationship model]]s tailored to suit different applications that share a common database.<ref>U.S. Patent 4774661, [http://www.freepatentsonline.com/4774661.html Database management system with active data dictionary], 19 November 1985, AT&T</ref> Extensions to the data dictionary also can assist in [[query optimization]] against [[distributed database]]s.<ref>U.S. Patent 4769772, [http://www.freepatentsonline.com/4769772.html Automated query optimization method using both global and parallel local optimizations for materialization access planning for distributed databases], 28 February 1985, Honeywell Bull</ref>  Additionally, DBA functions are often automated using restructuring tools that are tightly coupled to an active data dictionary.\n\n[[Software framework]]s aimed at [[rapid application development]] sometimes include high-level data dictionary facilities, which can substantially reduce the amount of programming required to build [[Menu (computing)|menus]], [[Form (programming)|forms]], reports, and other components of a database application, including the database itself. For example, PHPLens includes a [[PHP]] [[class library]] to automate the creation of tables, indexes, and [[foreign key]] constraints [[Portability (software)|portably]] for multiple databases.<ref>PHPLens, [http://phplens.com/lens/adodb/docs-datadict.htm ADOdb Data Dictionary Library for PHP]</ref> Another PHP-based data dictionary, part of the RADICORE toolkit, automatically generates program [[Object (computer science)|objects]], [[Scripting language|scripts]], and SQL code for menus and forms with [[data validation]] and complex [[join (SQL)|joins]].<ref>RADICORE, [http://www.radicore.org/viewarticle.php?article_id=5 What is a Data Dictionary?]</ref> For the [[ASP.NET]] environment, [[Base One International|Base One\'s]] data dictionary provides cross-DBMS facilities for automated database creation, data validation, performance enhancement ([[Cache (computing)|caching]] and index utilization), [[application security]], and extended [[data type]]s.<ref>Base One International Corp., [http://www.boic.com/b1ddic.htm Base One Data Dictionary]</ref>  [[Visual DataFlex]] features<ref>VISUAL DATAFLEX,[http://www.visualdataflex.com/features.asp?pageid=1030 features]</ref> provides the ability to use DataDictionaries as class files to form  middle layer between the user interface and the underlying database.   The intent is to create standardized rules to maintain data integrity and enforce business rules throughout one or more related applications.\n\n==Platform-specific examples==\nDevelopers use a \'\'data description specification\'\' (\'\'DDS\'\') to describe data attributes in file descriptions that are external to the application program that processes the data, in the context of an [[IBM System i]].<ref>{{cite web |url=http://publib.boulder.ibm.com/infocenter/iseries/v5r3/topic/dds/rbafpddsmain.htm |title=DDS documentation for IBM System i V5R3}}</ref>\n\n==See also==\n*[[Data hierarchy]]\n*[[Data modeling]]\n*[[Database schema]]\n*[[ISO/IEC 11179]]\n*[[Metadata registry]]\n*[[Semantic spectrum]]\n*[[Vocabulary OneSource]]\n*[[Metadata repository]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n{{Commons category|Data dictionary}}\n*Yourdon, \'\'Structured Analysis Wiki\'\', [http://yourdon.com/strucanalysis/wiki/index.php?title=Chapter_10 Data Dictionaries]\n\n{{Data warehouse}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Data Dictionary}}\n[[Category:Data management]]\n[[Category:Data modeling]]\n[[Category:Knowledge representation]]\n[[Category:Metadata]]']
['Modular concurrency control', '24906259', '#REDIRECT [[Global concurrency control]]\n\nModular concurrency control\n\n[[Category:Data management]]\n[[Category:Databases]]\n[[Category:Transaction processing]]\n[[Category:Concurrency control]]']
['Data center', '579730', '{{Refimprove|date=July 2015}}\n[[File:NetworkOperations.jpg|thumb|right|An operation engineer overseeing a network operations control room of a data center]]\n\nA \'\'\'data center\'\'\' is a facility used to house computer systems and associated components, such as [[telecommunication]]s and [[computer data storage|storage systems]]. It generally includes redundant or backup [[power supply|power supplies]], redundant data communications connections, environmental controls (e.g., air conditioning, fire suppression) and various security devices. Large data centers are industrial scale operations using as much electricity as a small town.<ref name=NYT92212>{{cite news|title=Power, Pollution and the Internet|url=http://www.nytimes.com/2012/09/23/technology/data-centers-waste-vast-amounts-of-energy-belying-industry-image.html|accessdate=2012-09-25|newspaper=The New York Times|date=September 22, 2012|author=James Glanz}}</ref><ref name="ReferenceDC2">"[http://www.academia.edu/6982393/Power_Management_Techniques_for_Data_Centers_A_Survey Power Management Techniques for Data Centers: A Survey]", 2014.</ref>\n\n==History==\n[[File:Indiana University Data Center - P1100134.JPG|thumb|[[Indiana University]] Data Center. [[Bloomington, Indiana]]]]\n{{Copypaste|section|url=http://www.rackspace.com/blog/datacenter-evolution-1960-to-2000/|date=August 2014}}\n{{Unreferenced section|date=August 2014}}\nData centers have their roots in the huge computer rooms of the early ages{{when|date=September 2015}} of the computing industry. Early computer systems, complex to operate and maintain, required a special environment in which to operate. Many cables were necessary to connect all the components, and methods to accommodate and organize these were devised such as standard [[19-inch rack|racks]] to mount equipment, [[raised floor]]s, and [[cable tray]]s (installed overhead or under the elevated floor). A single mainframe required a great deal of power, and had to be cooled to avoid overheating. Security became important&nbsp;– computers were expensive, and were often used for [[military]] purposes. Basic design-guidelines for controlling access to the computer room were therefore devised.\n\nDuring the boom of the microcomputer industry, and especially during the 1980s, users started to deploy computers everywhere, in many cases with little or no care about operating requirements. However, as information technology (IT) operations started to grow in complexity, organizations grew aware of the need to control IT resources. The advent of [[Unix]] from the early 1970s led to the subsequent proliferation of freely available [[Linux]]-compatible [[personal computer|PC]] operating-systems during the 1990s. These were called "[[Server (computing)|servers]]", as [[timesharing]] operating systems like Unix rely heavily on the [[client-server model]] to facilitate sharing unique resources between multiple users. The availability of inexpensive [[Networking hardware|networking]] equipment, coupled with new standards for network [[structured cabling]], made it possible to use a hierarchical design that put the servers in a specific room inside the company. The use of the term "data center", as applied to specially designed computer rooms, started to gain popular recognition about this time.{{citation needed|date=September 2015}}\n\nThe boom of data centers came during the [[dot-com bubble]] of 1997–2000. [[Company|Companies]] needed fast Internet connectivity and non-stop operation to deploy systems and to establish a presence on the Internet. Installing such equipment was not viable for many smaller companies. Many companies started building very large facilities, called \'\'\'Internet data centers\'\'\' (IDCs), which provide [[customer|commercial client]]s with a range of solutions for systems deployment and operation. New technologies and practices were designed to handle the scale and the operational requirements of such large-scale operations. These practices eventually migrated toward the private data centers, and were adopted largely because of their practical results. Data centers for cloud computing are called \'\'\'cloud data centers\'\'\' (CDCs). But nowadays, the division of these terms has almost disappeared and they are being integrated into a term "data center".\n\nWith an increase in the uptake of [[cloud computing]], business and government organizations scrutinize data centers to a higher degree in areas such as security, availability, environmental impact and adherence to standards. Standards documents from accredited [[professional]] groups, such as the [[Telecommunications Industry Association]], specify the requirements for data-center design. Well-known operational metrics for [[data availability|data-center availability]] can serve to evaluate the [[Business Impact Analysis|commercial impact]] of a disruption. Development continues in operational practice, and also in environmentally-friendly data-center design. Data centers typically cost a lot to build and to maintain.{{citation needed|date=September 2015}}\n\n==Requirements for modern data centers==\n[[File:Datacenter-telecom.jpg|thumb|left|Racks of telecommunications equipment in part of a data center]]\n{{Copypaste|section|url=https://global.ihs.com/doc_detail.cfm?&rid=TIA&input_doc_number=TIA-942&item_s_key=00414811&item_key_date=860905&input_doc_number=TIA-942&input_doc_title=#abstract|date=August 2014}}\nIT operations are a crucial aspect of most organizational operations around the world. One of the main concerns is \'\'\'business continuity\'\'\'; companies rely on their information systems to run their operations. If a system becomes unavailable, company operations may be impaired or stopped completely. It is necessary to provide a reliable infrastructure for IT operations, in order to minimize any chance of disruption. Information security is also a concern, and for this reason a data center has to offer a secure environment which minimizes the chances of a security breach. A data center must therefore keep high standards for assuring the integrity and functionality of its hosted computer environment. This is accomplished through redundancy of mechanical cooling and power systems (including emergency backup power generators)serving the data center along with fiber optic cables.\n\nThe [[Telecommunications Industry Association]]\'s Telecommunications Infrastructure Standard for Data Centers<ref>[http://www.tia-942.org TIA-942 Telecommunications Infrastructure Standard for Data Centers]</ref> specifies the minimum requirements for telecommunications infrastructure of data centers and computer rooms including single tenant enterprise data centers and multi-tenant Internet hosting data centers. The topology proposed in this document is intended to be applicable to any size data center.<ref>{{cite web|url=http://www.tiaonline.org/standards/ |title=Archived copy |accessdate=2011-11-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20111106042758/http://www.tiaonline.org/standards/ |archivedate=2011-11-06 |df= }}</ref>\n\nTelcordia GR-3160, \'\'NEBS Requirements for Telecommunications Data Center Equipment and Spaces\'\',<ref>[http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&DOCUMENT=GR-3160& GR-3160, NEBS Requirements for Telecommunications Data Center Equipment and Spaces]</ref> provides guidelines for data center spaces within telecommunications networks, and environmental requirements for the equipment intended for installation in those spaces. These criteria were developed jointly by Telcordia and industry representatives. They may be applied to data center spaces housing data processing or Information Technology (IT) equipment. The equipment may be used to:\n* Operate and manage a carrier\'s telecommunication network\n* Provide data center based applications directly to the carrier\'s customers\n* Provide hosted applications for a third party to provide services to their customers\n* Provide a combination of these and similar data center applications\n\nEffective data center operation requires a balanced investment in both the facility and the housed equipment. The first step is to establish a baseline facility environment suitable for equipment installation. Standardization and modularity can yield savings and efficiencies in the design and construction of telecommunications data centers.\n\nStandardization means integrated building and equipment engineering. Modularity has the benefits of scalability and easier growth, even when planning forecasts are less than optimal. For these reasons, telecommunications data centers should be planned in repetitive building blocks of equipment, and associated power and support (conditioning) equipment when practical. The use of dedicated centralized systems requires more accurate forecasts of future needs to prevent expensive over construction, or perhaps worse&nbsp;— under construction that fails to meet future needs.\n\nThe "lights-out" data center, also known as a darkened or a [[dark data]] center, is a data center that, ideally, has all but eliminated the need for direct access by personnel, except under extraordinary circumstances. Because of the lack of need for staff to enter the data center, it can be operated without lighting. All of the devices are accessed and managed by remote systems, with automation programs used to perform unattended operations. In addition to the energy savings, reduction in staffing costs and the ability to locate the site further from population centers, implementing a lights-out data center reduces the threat of malicious attacks upon the infrastructure.<ref>{{cite book | first=Victor | last=Kasacavage | year=2002 | page=227 | title=Complete book of remote access: connectivity and security | series=The Auerbach Best Practices Series | publisher=CRC Press | isbn=0-8493-1253-1\n}}</ref><ref>{{cite book |author1=Burkey, Roxanne E. |author2=Breakfield, Charles V. | year=2000 | title=Designing a total data solution: technology, implementation and deployment | page=24 | series=Auerbach Best Practices | publisher=CRC Press | isbn=0-8493-0893-3 }}</ref>\n\nThere is a trend to modernize data centers in order to take advantage of the performance and energy efficiency increases of newer IT equipment and capabilities, such as [[cloud computing]]. This process is also known as data center transformation.<ref name="mspmentor.net">Mukhar, Nicholas. "HP Updates Data Center Transformation Solutions," August 17, 2011 [http://www.mspmentor.net/2011/08/17/hp-updates-data-transformation-solutions/]</ref>\n\nOrganizations are experiencing rapid IT growth but their data centers are aging. Industry research company [[International Data Corporation]] (IDC) puts the average age of a data center at nine years old.<ref name="mspmentor.net"/> [[Gartner]], another research company says data centers older than seven years are obsolete.<ref>{{cite web|url=http://www.forbes.com/2010/03/12/cloud-computing-ibm-technology-cio-network-data-centers.html |title=Sperling, Ed. "Next-Generation Data Centers," Forbes, March 15. 2010 |publisher=Forbes.com |date= |accessdate=2013-08-30}}</ref>\n\nIn May 2011, data center research organization [[Uptime Institute]] reported that 36 percent of the large companies it surveyed expect to exhaust IT capacity within the next 18 months.<ref>Niccolai, James. "Data Centers Turn to Outsourcing to Meet Capacity Needs," CIO.com, May 10, 2011 [http://www.cio.com/article/681897/Data_Centers_Turn_to_Outsourcing_to_Meet_Capacity_Needs]</ref>\n\nData center transformation takes a step-by-step approach through integrated projects carried out over time. This differs from a traditional method of data center upgrades that takes a serial and siloed approach.<ref>Tang, Helen. "Three Signs it\'s time to transform your data center," August 3, 2010, Data Center Knowledge [http://www.datacenterknowledge.com/archives/2010/08/03/three-signs-it%E2%80%99s-time-to-transform-your-data-center/]</ref> The typical projects within a data center transformation initiative include standardization/consolidation, virtualization, [[automation]] and security.\n* Standardization/consolidation: The purpose of this project is to reduce the number of data centers a large organization may have. This project also helps to reduce the number of hardware, software platforms, tools and processes within a data center. Organizations replace aging data center equipment with newer ones that provide increased capacity and performance. Computing, networking and management platforms are standardized so they are easier to manage.<ref name="datacenterknowledge.com">Miller, Rich. "Complexity: Growing Data Center Challenge," Data Center Knowledge, May 16, 2007\n[http://www.datacenterknowledge.com/archives/2007/05/16/complexity-growing-data-center-challenge/]</ref>\n* Virtualize: There is a trend to use IT virtualization technologies to replace or consolidate multiple data center equipment, such as servers. Virtualization helps to lower capital and operational expenses,<ref>Sims, David. "Carousel\'s Expert Walks Through Major Benefits of Virtualization," TMC Net, July 6, 2010\n[http://virtualization.tmcnet.com/topics/virtualization/articles/193652-carousels-expert-walks-through-major-benefits-virtualization.htm]</ref> and reduce energy consumption.<ref>Delahunty, Stephen. "The New urgency for Server Virtualization," InformationWeek, August 15, 2011. [http://www.informationweek.com/news/government/enterprise-architecture/231300585]</ref> Virtualization technologies are also used to create virtual desktops, which can then be hosted in data centers and rented out on a subscription basis.<ref>{{cite web|title=HVD: the cloud\'s silver lining|url=http://www.intrinsictechnology.co.uk/FileUploads/HVD_Whitepaper.pdf|publisher=Intrinsic Technology|accessdate=2012-08-30}}</ref>  Data released by investment bank Lazard Capital Markets reports that 48 percent of enterprise operations will be virtualized by 2012. Gartner views virtualization as a catalyst for modernization.<ref>Miller, Rich. "Gartner: Virtualization Disrupts Server Vendors," Data Center Knowledge, December 2, 2008 [http://www.datacenterknowledge.com/archives/2008/12/02/gartner-virtualization-disrupts-server-vendors/]</ref>\n* Automating: Data center automation involves automating tasks such as [[provisioning]], configuration, [[Patch (computing)|patching]], release management and compliance. As enterprises suffer from few skilled IT workers,<ref name="datacenterknowledge.com"/> automating tasks make data centers run more efficiently.\n* Securing: In modern data centers, the security of data on virtual systems is integrated with existing security of physical infrastructures.<ref>Ritter, Ted. Nemertes Research, "Securing the Data-Center Transformation Aligning Security and Data-Center Dynamics," [http://lippisreport.com/2011/05/securing-the-data-center-transformation-aligning-security-and-data-center-dynamics/]</ref> The security of a modern data center must take into account physical security, network security, and data and user security.\n\n==Carrier neutrality==\nToday many data centers are run by [[Internet service provider]]s solely for the purpose of hosting their own and third party [[Server (computing)|servers]].\n\nHowever traditionally data centers were either built for the sole use of one large company, or as [[carrier hotel]]s or [[Network-neutral data center]]s.\n\nThese facilities enable interconnection of carriers and act as regional fiber hubs serving local business in addition to hosting content [[Server (computing)|servers]].\n\n==Data center tiers==\n<!-- linked from [[data availability]] -->\nThe [[Telecommunications Industry Association]] is a trade association accredited by ANSI (American National Standards Institute). In 2005 it published [http://global.ihs.com/doc_detail.cfm?currency_code=USD&customer_id=2125452B2C0A&oshid=2125452B2C0A&shopping_cart_id=292558332C4A2020495A4D3B200A&country_code=US&lang_code=ENGL&item_s_key=00414811&item_key_date=940819&input_doc_number=TIA-942&input_doc_title= ANSI/TIA-942], Telecommunications Infrastructure Standard for Data Centers, which defined four levels (called tiers) of data centers in a thorough, quantifiable manner. TIA-942 was amended in 2008 and again in 2010. \'\'TIA-942:Data Center Standards Overview\'\' describes the requirements for the data center infrastructure. The simplest is a Tier 1 data center, which is basically a [[server room]], following basic guidelines for the installation of computer systems. The most stringent level is a Tier 4 data center, which is designed to host mission critical computer systems, with fully redundant subsystems and compartmentalized security zones controlled by [[biometric]] access controls methods. Another consideration is the placement of the data center in a subterranean context, for data security as well as environmental considerations such as cooling requirements.<ref>A ConnectKentucky article mentioning Stone Mountain Data Center Complex {{cite web|title=Global Data Corp. to Use Old Mine for Ultra-Secure Data Storage Facility|url=http://connectkentucky.org/_documents/connected_fall_FINAL.pdf|format=PDF|publisher=ConnectKentucky|accessdate=2007-11-01|date=2007-11-01}}</ref>\n\nThe German Datacenter star audit program uses an auditing process to certify 5 levels of "gratification" that affect Data Center criticality.\n\nIndependent from the ANSI/TIA-942 standard, the [[Uptime Institute]], a think tank and professional-services organization based in [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]], has defined its own four levels. The levels describe the availability of data from the hardware at a location. The higher the tier, the greater the availability. The levels are:\n<ref>A document from the Uptime Institute describing the different tiers (click through the download page) {{cite web\n |title=Data Center Site Infrastructure Tier Standard: Topology \n |url=http://uptimeinstitute.org/index.php?option=com_docman&task=doc_download&gid=82 \n |format=PDF \n |publisher=Uptime Institute \n |accessdate=2010-02-13 \n |date=2010-02-13 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20100613072610/http://uptimeinstitute.org/index.php?option=com_docman&task=doc_download&gid=82 \n |archivedate=2010-06-13 \n |df= \n}}</ref>\n<ref>The rating guidelines from the Uptime Institute {{cite web\n |title=Data Center Site Infrastructure Tier Standard: Topology \n |url=http://professionalservices.uptimeinstitute.com/UIPS_PDF/TierStandard.pdf \n |format=PDF \n |publisher=Uptime Institute \n |accessdate=2010-02-13 \n |date=2010-02-13 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20091007121511/http://professionalservices.uptimeinstitute.com:80/UIPS_PDF/TierStandard.pdf \n |archivedate=2009-10-07 \n |df= \n}}</ref>\n\n{| class="wikitable"\n|-\n! Tier Level\n! Requirements\n|-\n! 1\n|\n* Single non-redundant distribution path serving the IT equipment\n* Non-redundant capacity components\n* Basic site infrastructure with expected availability of 99.671%\n|-\n! 2\n|\n* Meets or exceeds all Tier 1 requirements\n* Redundant site infrastructure capacity components with expected availability of 99.741%\n|-\n! 3\n|\n* Meets or exceeds all Tier 2 requirements\n* Multiple independent distribution paths serving the IT equipment\n* All IT equipment must be dual-powered and fully compatible with the topology of a site\'s architecture\n* Concurrently maintainable site infrastructure with expected availability of 99.982%\n|-\n! 4\n|\n* Meets or exceeds all Tier 3 requirements\n* All cooling equipment is independently dual-powered, including chillers and heating, ventilating and air-conditioning (HVAC) systems\n* Fault-tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99.995%\n|}\n\nThe difference between 99.671%, 99.741%, 99.982%, and 99.995%, while seemingly nominal, could be significant depending on the application.\n\nWhilst no down-time is ideal, the tier system allows for unavailability of services as listed below over a period of one year (525,600 minutes):\n* Tier 1 (99.671%) status would allow 1729.224 minutes or 28.817 hours\n* Tier 2 (99.741%) status would allow 1361.304 minutes or 22.688 hours\n* Tier 3 (99.982%) status would allow 94.608 minutes or 1.5768 hours\n* Tier 4 (99.995%) status would allow 26.28 minutes or 0.438 hours\n\nThe Uptime Institute also classifies the tiers in different categories: design documents, constructed facility, operational sustainability<ref name="uptimeinstitute">{{cite web|url=http://uptimeinstitute.com/TierCertification/|title=Uptime Institute - Tier Certification|publisher=uptimeinstitute.com|accessdate=2014-08-27}}</ref>\n\n==Design considerations==\n[[File:Rack001.jpg|thumb|right|A typical server rack, commonly seen in [[colocation center|colocation]]]]\nA data center can occupy one room of a building, one or more floors, or an entire building. Most of the equipment is often in the form of servers mounted in [[19 inch rack]] cabinets, which are usually placed in single rows forming corridors (so-called aisles) between them. This allows people access to the front and rear of each cabinet. Servers differ greatly in size from [[Rack unit|1U servers]] to large freestanding storage silos which occupy many square feet of floor space. Some equipment such as [[mainframe computer]]s and [[computer storage|storage]] devices are often as big as the racks themselves, and are placed alongside them. Very large data centers may use [[intermodal container|shipping containers]] packed with 1,000 or more servers each;<ref>{{cite web|url=https://www.youtube.com/watch?v=zRwPSFpLX8I|title=Google Container Datacenter Tour (video)}}</ref> when repairs or upgrades are needed, whole containers are replaced (rather than repairing individual servers).<ref>{{cite web| title=Walking the talk: Microsoft builds first major container-based data center| url=http://www.computerworld.com/action/article.do?command=viewArticleBasic&articleId=9075519| archiveurl=https://web.archive.org/web/20080612193106/http://www.computerworld.com/action/article.do?command=viewArticleBasic&articleId=9075519| archivedate=2008-06-12| accessdate=2008-09-22}}</ref>\n\nLocal building codes may govern the minimum ceiling heights.\n\n===Design programming===\nDesign programming, also known as architectural programming, is the process of researching and making decisions to identify the scope of a design project.<ref>Cherry, Edith. "Architectural Programming: Introduction", Whole Building Design Guide, Sept. 2, 2009</ref> Other than the architecture of the building itself there are three elements to design programming for data centers: facility topology design (space planning), engineering infrastructure design (mechanical systems such as cooling and electrical systems including power) and technology infrastructure design (cable plant). Each will be influenced by performance assessments and modelling to identify gaps pertaining to the owner\'s performance wishes of the facility over time.\n\nVarious vendors who provide data center design services define the steps of data center design slightly differently, but all address the same basic aspects as given below.\n\n===Modeling criteria===\nModeling criteria are used to develop future-state scenarios for space, power, cooling, and costs in the data center.<ref>Mullins, Robert. "Romonet Offers Predictive Modelling Tool For Data Center Planning", Network Computing, June 29, 2011 [http://www.networkcomputing.com/data-center/231000669]</ref> The aim is to create a master plan with parameters such as number, size, location, topology, IT floor system layouts, and power and cooling technology and configurations. The purpose of this is to allow for efficient use of the existing mechanical and electrical systems and also growth in the existing data center without the need for developing new buildings and further upgrading of incoming power supply.\n\n===Design recommendations===\nDesign recommendations/plans generally follow the modelling criteria phase. The optimal technology infrastructure is identified and planning criteria are developed, such as critical power capacities, overall data center power requirements using an agreed upon PUE (power utilization efficiency), mechanical cooling capacities, kilowatts per cabinet, raised floor space, and the resiliency level for the facility.\n\n===Conceptual design===\nConceptual designs embody the design recommendations or plans and should take into account "what-if" scenarios to ensure all operational outcomes are met in order to future-proof the facility. Conceptual floor layouts should be driven by IT performance requirements as well as lifecycle costs associated with IT demand, energy efficiency, cost efficiency and availability. Future-proofing will also include expansion capabilities, often provided in modern data centers through modular designs.  These allow for more raised floor space to be fitted out in the data center whilst utilising the existing major electrical plant of the facility.\n\n===Detailed design===\nDetailed design is undertaken once the appropriate conceptual design is determined, typically including a proof of concept. The detailed design phase should include the detailed architectural, structural, mechanical and electrical information and specification of the facility.  At this stage development of facility schematics and construction documents as well as schematics and performance specification and specific detailing of all technology infrastructure, detailed IT infrastructure design and IT infrastructure documentation are produced.\n\n===Mechanical engineering infrastructure designs===\n[[File:CRAC Cabinets 2.jpg|thumb|CRAC Air Handler]]\nMechanical engineering infrastructure design addresses mechanical systems involved in maintaining the interior environment of a data center, such as heating, ventilation and air conditioning (HVAC); humidification and dehumidification equipment; pressurization; and so on.<ref name="nxtbook.com">Jew, Jonathan. "BICSI Data Center Standard: A Resource for Today\'s Data Center Operators and Designers," BICSI News Magazine, May/June 2010, page 28. [http://www.nxtbook.com/nxtbooks/bicsi/news_20100506/#/26]</ref>\nThis stage of the design process should be aimed at saving space and costs, while ensuring business and reliability objectives are met as well as achieving PUE and green requirements.<ref>Data Center Energy Management: Best Practices Checklist: Mechanical, Lawrence Berkeley National Laboratory [http://hightech.lbl.gov/dctraining/strategies/mam.html]</ref> Modern designs include modularizing and scaling IT loads, and making sure capital spending on the building construction is optimized.\n\n===Electrical engineering infrastructure design===\nElectrical Engineering infrastructure design is focused on designing electrical configurations that accommodate various reliability requirements and data center sizes. Aspects may include utility service planning; distribution, switching and bypass from power sources; uninterruptable power source (UPS) systems; and more.<ref name="nxtbook.com"/>\n\nThese designs should dovetail to energy standards and best practices while also meeting business objectives. Electrical configurations should be optimized and operationally compatible with the data center user\'s capabilities. Modern electrical design is modular and scalable,<ref>Clark, Jeff. "Hedging Your Data Center Power", The Data Center Journal, Oct. 5, 2011. [http://www.datacenterjournal.com/design/hedging-your-data-center-power/]</ref> and is available for low and medium voltage requirements as well as DC (direct current).\n\n===Technology infrastructure design===\n[[File:Under Floor Cable Runs Tee.jpg|thumb|Under Floor Cable Runs]]\nTechnology infrastructure design addresses the telecommunications cabling systems that run throughout data centers. There are cabling systems for all data center environments, including horizontal cabling, voice, modem, and facsimile telecommunications services, premises switching equipment, computer and telecommunications management connections, keyboard/video/mouse connections and data communications.<ref>Jew, Jonathan. "BICSI Data Center Standard: A Resource for Today\'s Data Center Operators and Designers," BICSI News Magazine, May/June 2010, page 30. [http://www.nxtbook.com/nxtbooks/bicsi/news_20100506/#/26]</ref> Wide area, local area, and storage area networks should link with other building signaling systems (e.g. fire, security, power, HVAC, EMS).\n\n===Availability expectations===\nThe higher the availability needs of a data center, the higher the capital and operational costs of building and managing it. Business needs should dictate the level of availability required and should be evaluated based on characterization of the criticality of IT systems estimated cost analyses from modeled scenarios. In other words, how can an appropriate level of availability best be met by design criteria to avoid financial and operational risks as a result of downtime?\nIf the estimated cost of downtime within a specified time unit exceeds the amortized capital costs and operational expenses, a higher level of availability should be factored into the data center design. If the cost of avoiding downtime greatly exceeds the cost of downtime itself, a lower level of availability should be factored into the design.<ref>Clark, Jeffrey. "The Price of Data Center Availability—How much availability do you need?", Oct. 12, 2011, The Data Center Journal [http://www.datacenterjournal.com/home/news/languages/item/2792-the-price-of-data-center-availability]</ref>\n\n===Site selection===\nAspects such as proximity to available power grids, telecommunications infrastructure, networking services, transportation lines and emergency services can affect costs, risk, security and other factors to be taken into consideration for data center design.   Whilst a wide array of location factors are taken into account (e.g. flight paths, neighbouring uses, geological risks) access to suitable available power is often the longest lead time item. Location affects data center design also because the climatic conditions dictate what cooling technologies should be deployed. In turn this impacts uptime and the costs associated with cooling.<ref>Tucci, Linda. "Five tips on selecting a data center location", May 7, 2008, SearchCIO.com [http://searchcio.techtarget.com/news/1312614/Five-tips-on-selecting-a-data-center-location]</ref> For example, the topology and the cost of managing a data center in a warm, humid climate will vary greatly from managing one in a cool, dry climate.\n\n===Modularity and flexibility===\n[[File:Cabinet Asile.jpg|thumb|Cabinet aisle in a data center]]\n{{main article|Modular data center}}\n\nModularity and flexibility are key elements in allowing for a data center to grow and change over time. Data center modules are pre-engineered, standardized building blocks that can be easily configured and moved as needed.<ref>Niles, Susan. "Standardization and Modularity in Data Center Physical Infrastructure," 2011, Schneider Electric, page 4. [http://www.apcmedia.com/salestools/VAVR-626VPD_R1_EN.pdf]</ref>\n\nA modular data center may consist of data center equipment contained within shipping containers or similar portable containers.<ref>Pitchaikani, Bala. "Strategies for the Containerized Data Center," DataCenterKnowledge.com, Sept. 8, 2011. [http://www.datacenterknowledge.com/archives/2011/09/08/strategies-for-the-containerized-data-center/]</ref> But it can also be described as a design style in which components of the data center are prefabricated and standardized so that they can be constructed, moved or added to quickly as needs change.<ref>Niccolai, James. "HP says prefab data center cuts costs in half," InfoWorld, July 27, 2010. [http://www.infoworld.com/d/green-it/hp-says-prefab-data-center-cuts-costs-in-half-837?page=0,0]</ref>\n\n===Environmental control===\n{{main article|Data center environmental control}}\nThe physical environment of a data center is rigorously controlled.\n[[Air conditioning]] is used to control the temperature and humidity in the data center. [[ASHRAE]]\'s "Thermal Guidelines for Data Processing Environments"<ref>{{cite book|title=Thermal Guidelines for Data Processing Environments|year=2012|publisher=American Society of Heating, Refrigerating and Air-Conditioning Engineers|isbn=978-1936504-33-6|author=ASHRAE Technical Committee 9.9, Mission Critical Facilities, Technology Spaces and Electronic Equipment|edition=3}}</ref> recommends a temperature range of {{convert|18|–|27|C|F}}, a dew point range of {{convert|5|–|15|C|F}}, and a relative humidity between 40% to 60% for data center environments.<ref name=ServersCheck>{{Cite web| title = Best Practices for data center monitoring and server room monitoring  | url=https://serverscheck.com/sensors/temperature_best_practices.asp | author = ServersCheck | accessdate = 2016-10-07}}</ref>  The temperature in a data center will naturally rise because the electrical power used heats the air. Unless the heat is removed, the ambient temperature will rise, resulting in electronic equipment malfunction. By controlling the air temperature, the server components at the board level are kept within the manufacturer\'s specified temperature/humidity range. Air conditioning systems help control [[humidity]] by cooling the return space air below the [[dew point]]. Too much humidity, and water may begin to [[condensation|condense]] on internal components. In case of a dry atmosphere, ancillary humidification systems may add water vapor if the humidity is too low, which can result in [[electrostatics|static electricity]] discharge problems which may damage components. Subterranean data centers may keep computer equipment cool while expending less energy than conventional designs.\n\nModern data centers try to use economizer cooling, where they use outside air to keep the data center cool. At least one data center (located in [[Upstate New York]]) will cool servers using outside air during the winter. They do not use chillers/air conditioners, which creates potential energy savings in the millions.<ref>{{cite news| url=http://www.reuters.com/article/pressRelease/idUS141369+14-Sep-2009+PRN20090914 | work=Reuters | title=tw telecom and NYSERDA Announce Co-location Expansion | date=2009-09-14}}</ref>  Increasingly [http://www.datacenterdynamics.com/focus/archive/2013/09/air-air-combat-indirect-air-cooling-wars-0 indirect air cooling] is being deployed in data centers globally which has the advantage of more efficient cooling which lowers power consumption costs in the data center.\n\nTelcordia [http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&DOCUMENT=GR-2930& GR-2930, \'\'NEBS: Raised Floor Generic Requirements for Network and Data Centers\'\'], presents generic engineering requirements for raised floors that fall within the strict NEBS guidelines.\n\nThere are many types of commercially available floors that offer a wide range of structural strength and loading capabilities, depending on component construction and the materials used. The general types of [[raised floor]]s include stringer, stringerless,  and structural platforms, all of which are discussed in detail in GR-2930 and summarized below.\n* \'\'\'\'\'Stringered raised floors\'\'\'\'\' - This type of raised floor generally consists of a vertical array of steel pedestal assemblies (each assembly is made up of a steel base plate, tubular upright, and a head) uniformly spaced on two-foot centers and mechanically fastened to the concrete floor. The steel pedestal head has a stud that is inserted into the pedestal upright and the overall height is adjustable with a leveling nut on the welded stud of the pedestal head.\n* \'\'\'\'\'Stringerless raised floors\'\'\'\'\' - One non-earthquake type of raised floor generally consists of an array of pedestals that provide the necessary height for routing cables and also serve to support each corner of the floor panels. With this type of floor, there may or may not be provisioning to mechanically fasten the floor panels to the pedestals. This stringerless type of system (having no mechanical attachments between the pedestal heads) provides maximum accessibility to the space under the floor. However, stringerless floors are significantly weaker than stringered raised floors in supporting lateral loads and are not recommended.\n* \'\'\'\'\'Structural platforms\'\'\'\'\' - One type of structural platform consists of members constructed of steel angles or channels that are welded or bolted together to form an integrated platform for supporting equipment. This design permits equipment to be fastened directly to the platform without the need for toggle bars or supplemental bracing. Structural platforms may or may not contain panels or stringers.\n\nData centers typically have [[raised floor]]ing made up of {{convert|60|cm|ft|abbr=on|0}} removable square tiles. The trend is towards {{convert|80|-|100|cm|in|abbr=on}} void to cater for better and uniform air distribution. These provide a [[plenum space|plenum]] for air to circulate below the floor, as part of the air conditioning system, as well as providing space for power cabling.\n\n====Metal whiskers====\nRaised floors and other metal structures such as cable trays and ventilation ducts have caused many problems with [[zinc whiskers]] in the past, and likely are still present in many data centers. This happens when microscopic metallic filaments form on metals such as zinc or tin that protect many metal structures and electronic components from corrosion. Maintenance on a raised floor or installing of cable etc. can dislodge the whiskers, which enter the airflow and may short circuit server components or power supplies, sometimes through a high current metal vapor [[plasma arc]]. This phenomenon is not unique to data centers, and has also caused catastrophic failures of satellites and military hardware.<ref>{{cite web|title=NASA - metal whiskers research|url=http://nepp.nasa.gov/whisker/other_whisker/index.htm|publisher=NASA|accessdate=2011-08-01}}</ref>\n\n===Electrical power===\n\n[[File:Datacenter Backup Batteries.jpg|thumb|right|A bank of batteries in a large data center, used to provide power until diesel generators can start]]\n\nBackup power consists of one or more [[uninterruptible power supply|uninterruptible power supplies]], battery banks, and/or [[Diesel generator|diesel]] / [[gas turbine]] generators.<ref>Detailed explanation of UPS topologies {{cite web|url=http://www.emersonnetworkpower.com/en-US/Brands/Liebert/Documents/White%20Papers/Evaluating%20the%20Economic%20Impact%20of%20UPS%20Technology.pdf |format=PDF |title=EVALUATING THE ECONOMIC IMPACT OF UPS TECHNOLOGY |deadurl=yes |archiveurl=https://web.archive.org/web/20101122074817/http://emersonnetworkpower.com/en-US/Brands/Liebert/Documents/White%20Papers/Evaluating%20the%20Economic%20Impact%20of%20UPS%20Technology.pdf |archivedate=2010-11-22 |df= }}</ref>\n\nTo prevent [[single point of failure|single points of failure]], all elements of the electrical systems, including backup systems, are typically fully duplicated, and critical servers are connected to both the "A-side" and "B-side" power feeds. This arrangement is often made to achieve [[N+1 redundancy]] in the systems. [[Transfer switch#Static transfer switch|Static transfer switches]] are sometimes used to ensure instantaneous switchover from one supply to the other in the event of a power failure.\n\n===Low-voltage cable routing===\nData cabling is typically routed through overhead [[cable tray]]s in modern data centers. But some{{Who|date=May 2012}} are still recommending under raised floor cabling for security reasons and to consider the addition of cooling systems above the racks in case this enhancement is necessary. Smaller/less expensive data centers without raised flooring may use anti-static tiles for a flooring surface. Computer cabinets are often organized into a [[Data center environmental control#Aisle containment|hot aisle]] arrangement to maximize airflow efficiency.\n\n===Fire protection===\n[[File:FM200 Three.jpg|thumb|[[FM200]] Fire Suppression Tanks]]\nData centers feature [[fire protection]] systems, including [[passive fire protection|passive]] and [[Active Design]] elements, as well as implementation of [[fire prevention]] programs in operations. [[Smoke detectors]] are usually installed to provide early warning of a fire at its incipient stage. This allows investigation, interruption of power, and manual fire suppression using hand held fire extinguishers before the fire grows to a large size. An [[active fire protection]] system, such as a [[fire sprinkler system]] or a [[clean agent]] fire suppression gaseous system, is often provided to control a full scale fire if it develops. High sensitivity smoke detectors, such as [[aspirating smoke detector]]s, activating [[clean agent]] fire suppression gaseous systems activate earlier than fire sprinklers.\n\n* Sprinklers = structure protection and building life safety.\n* Clean agents = business continuity and asset protection.\n* No water = no collateral damage or clean up.\n\nPassive fire protection elements include the installation of [[Firewall (construction)|fire walls]] around the data center, so a fire can be restricted to a portion of the facility for a limited time in the event of the failure of the active fire protection systems. Fire wall penetrations into the server room, such as cable penetrations, coolant line penetrations and air ducts, must be provided with fire rated penetration assemblies, such as [[fire stop]]ping.\n\n===Security===\nPhysical security also plays a large role with data centers. Physical access to the site is usually restricted to selected personnel, with controls including a layered security system often starting with fencing, [[bollard]]s and [[mantrap (access control)|mantraps]].<ref>{{cite web|author=Sarah D. Scalet |url=http://www.csoonline.com/article/220665 |title=19 Ways to Build Physical Security Into a Data Center |publisher=Csoonline.com |date=2005-11-01 |accessdate=2013-08-30}}</ref> [[Video camera]] surveillance and permanent [[security guard]]s are almost always present if the data center is large or contains sensitive information on any of the systems within. The use of finger print recognition [[mantrap (snare)|mantrap]]s is starting to be commonplace.\n\n==Energy use==\n[[File:Google Data Center, The Dalles.jpg|thumb|[[Google Data Centers|Google Data Center]], [[The Dalles, Oregon]]]]\n{{main article|IT energy management}}\n\nEnergy use is a central issue for data centers. Power draw for data centers ranges from a few kW for a rack of servers in a closet to several tens of MW for large facilities. Some facilities have power densities more than 100 times that of a typical office building.<ref>{{cite web|url=http://www1.eere.energy.gov/femp/program/dc_energy_consumption.html|title=Data Center Energy Consumption Trends|publisher=U.S. Department of Energy|accessdate=2010-06-10}}</ref> For higher power density facilities, electricity costs are a dominant [[operating expense]] and account for over 10% of the [[total cost of ownership]] (TCO) of a data center.<ref>J Koomey, C. Belady, M. Patterson, A. Santos, K.D. Lange. [http://www.intel.com/assets/pdf/general/servertrendsreleasecomplete-v25.pdf Assessing Trends Over Time in Performance, Costs, and Energy Use for Servers] Released on the web August 17th, 2009.</ref> By 2012 the cost of power for the data center is expected to exceed the cost of the original capital investment.<ref>{{cite web|url=http://www1.eere.energy.gov/femp/pdfs/data_center_qsguide.pdf |title=Quick Start Guide to Increase Data Center Energy Efficiency |publisher=U.S. Department of Energy |accessdate=2010-06-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20101122035456/http://www1.eere.energy.gov:80/femp/pdfs/data_center_qsguide.pdf |archivedate=2010-11-22 |df= }}</ref>\n\n===Greenhouse gas emissions===\nIn 2007 the entire [[information and communication technologies]] or ICT sector was estimated to be responsible for roughly 2% of global [[Greenhouse gas|carbon emissions]] with data centers accounting for 14% of the ICT footprint.<ref name="smart1">{{cite web|url=http://www.smart2020.org/_assets/files/03_Smart2020Report_lo_res.pdf |title=Smart 2020: Enabling the low carbon economy in the information age |publisher=The Climate Group for the Global e-Sustainability Initiative |accessdate=2008-05-11 |deadurl=yes |archiveurl=https://web.archive.org/web/20110728032834/http://www.smart2020.org/_assets/files/03_Smart2020Report_lo_res.pdf |archivedate=2011-07-28 |df= }}</ref> The US EPA estimates that servers and data centers are responsible for up to 1.5% of the total US electricity consumption,<ref name="energystar1">{{cite web|url=http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf|title=Report to Congress on Server and Data Center Energy Efficiency|publisher=U.S. Environmental Protection Agency ENERGY STAR Program}}</ref> or roughly .5% of US GHG emissions,<ref>A calculation of data center electricity burden cited in the [http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf Report to Congress on Server and Data Center Energy Efficiency] and electricity generation contributions to green house gas emissions published by the EPA in the [http://epa.gov/climatechange/emissions/downloads10/US-GHG-Inventory-2010_ExecutiveSummary.pdf Greenhouse Gas Emissions Inventory Report]. Retrieved 2010-06-08.</ref>  for 2007. Given a business as usual scenario greenhouse gas emissions from data centers is projected to more than double from 2007 levels by 2020.<ref name="smart1"/>\n\nSiting is one of the factors that affect the energy consumption and environmental effects of a datacenter. In areas where climate favors cooling and lots of renewable electricity is available the environmental effects will be more moderate. Thus countries with favorable conditions, such as: Canada,<ref>[http://www.theglobeandmail.com/report-on-business/canada-called-prime-real-estate-for-massive-data-computers/article2071677/ Canada Called Prime Real Estate for Massive Data Computers - Globe & Mail] Retrieved June 29, 2011.</ref> Finland,<ref>[http://datacenter-siting.weebly.com/ Finland - First Choice for Siting Your Cloud Computing Data Center.]. Retrieved 4 August 2010.</ref> Sweden,<ref>{{cite web|url=http://www.stockholmbusinessregion.se/templates/page____41724.aspx?epslanguage=EN|title=Stockholm sets sights on data center customers|accessdate=4 August 2010|archiveurl=https://web.archive.org/web/20100819190918/http://www.stockholmbusinessregion.se/templates/page____41724.aspx?epslanguage=EN|archivedate=19 August 2010}}</ref> Norway <ref>[http://www.innovasjonnorge.no/en/start-page/invest-in-norway/industries/datacenters/ In a world of rapidly increasing carbon emissions from the ICT industry, Norway offers a sustainable solution] Retrieved 1 March 2016.</ref> and Switzerland,<ref>[http://www.greenbiz.com/news/2010/06/30/swiss-carbon-neutral-servers-hit-cloud Swiss Carbon-Neutral Servers Hit the Cloud.]. Retrieved 4 August 2010.</ref> are trying to attract cloud computing data centers.\n\nIn an 18-month investigation by scholars at Rice University\'s Baker Institute for Public Policy in Houston and the Institute for Sustainable and Applied Infodynamics in Singapore, data center-related emissions will more than triple by 2020.\n<ref>{{Cite news\n |author=Katrice R. Jalbuena \n |title=Green business news. \n |quote= \n |publisher=EcoSeed \n |date=October 15, 2010 \n |pages= \n |url=http://ecoseed.org/en/business-article-list/article/1-business/8219-i-t-industry-risks-output-cut-in-low-carbon-economy \n |accessdate=2010-11-11 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20160618081417/http://ecoseed.org/en/business-article-list/article/1-business/8219-i-t-industry-risks-output-cut-in-low-carbon-economy \n |archivedate=2016-06-18 \n |df= \n}}</ref>\n\n===Energy efficiency===\nThe most commonly used metric to determine the energy efficiency of a data center is [[power usage effectiveness]], or PUE. This simple ratio is the total power entering the data center divided by the power used by the IT equipment.\n\n:<math> \\mathrm{PUE}  =  {\\mbox{Total Facility Power} \\over \\mbox{IT Equipment Power}} </math>\n\nTotal facility power consists of power used by IT equipment plus any overhead power consumed by anything that is not considered a computing or data communication device (i.e. cooling, lighting, etc.). An ideal PUE is 1.0 for the hypothetical situation of zero overhead power. The average data center in the US has a PUE of 2.0,<ref name="energystar1"/> meaning that the facility uses two watts of total power (overhead + IT equipment) for every watt delivered to IT equipment. State-of-the-art data center energy efficiency is estimated to be roughly 1.2.<ref>{{cite web|url=https://microsite.accenture.com/svlgreport/Documents/pdf/SVLG_Report.pdf|title=Data Center Energy Forecast|publisher=Silicon Valley Leadership Group}}</ref> Some large data center operators like [[Microsoft]] and [[Yahoo!]] have published projections of PUE for facilities in development; [[Google]] publishes quarterly actual efficiency performance from data centers in operation.<ref>{{cite web|url=https://www.google.com/about/datacenters/efficiency/internal/|title=Efficiency: How we do it – Data centers|publisher=Google|accessdate=2015-01-19}}</ref>\n\nThe [[U.S. Environmental Protection Agency]] has an [[Energy Star]] rating for standalone or large data centers. To qualify for the ecolabel, a data center must be within the top quartile of energy efficiency of all reported facilities.<ref>Commentary on introduction of Energy Star for Data Centers {{cite web|title=Introducing EPA ENERGY STAR for Data Centers |url=http://www.emerson.com/edc/post/2010/06/15/Introducing-EPA-ENERGY-STARc2ae-for-Data-Centers.aspx |format=Web site |publisher=Jack Pouchet |accessdate=2010-09-27 |date=2010-09-27 |deadurl=yes |archiveurl=https://web.archive.org/web/20100925210539/http://emerson.com/edc/post/2010/06/15/Introducing-EPA-ENERGY-STARc2ae-for-Data-Centers.aspx |archivedate=2010-09-25 |df= }}</ref>\n\nEuropean Union also has a similar initiative: EU Code of Conduct for Data Centres<ref>{{cite web|url=http://iet.jrc.ec.europa.eu/energyefficiency/ict-codes-conduct/data-centres-energy-efficiency |title=EU Code of Conduct for Data Centres |publisher=iet.jrc.ec.europa.eu |date= |accessdate=2013-08-30 }}</ref>\n\n===Energy use analysis===\nOften, the first step toward curbing energy use in a data center is to understand how energy is being used in the data center. Multiple types of analysis exist to measure data center energy use. Aspects measured include not just energy used by IT equipment itself, but also by the data center facility equipment, such as chillers and fans.<ref>Sweeney, Jim. "Reducing Data Center Power and Energy Consumption: Saving Money and \'Going Green,\' " GTSI Solutions, pages 2–3. [http://www.gtsi.com/cms/documents/white-papers/green-it.pdf]</ref>\n\n===Power and cooling analysis===\nPower is the largest recurring cost to the user of a data center.<ref name=DRJ_Choosing>{{Citation\n | title = Choosing a Data Center\n | url = http://www.atlantic.net/images/pdf/choosing_a_data_center.pdf\n | publisher = Disaster Recovery Journal\n | year = 2009\n | author = Cosmano, Joe\n | accessdate = 2012-07-21\n}}</ref>  A power and cooling analysis, also referred to as a thermal assessment, measures the relative temperatures in specific areas as well as the capacity of the cooling systems to handle specific ambient temperatures.<ref>Needle, David. "HP\'s Green Data Center Portfolio Keeps Growing," InternetNews, July 25, 2007. [http://www.internetnews.com/xSP/article.php/3690651/HPs+Green+Data+Center+Portfolio+Keeps+Growing.htm]</ref> A power and cooling analysis can help to identify hot spots, over-cooled areas that can handle greater power use density, the breakpoint of equipment loading, the effectiveness of a raised-floor strategy, and optimal equipment positioning (such as AC units) to balance temperatures across the data center. Power cooling density is a measure of how much square footage the center can cool at maximum capacity.<ref name=Inc_Howtochoose>{{Citation\n | title = How to Choose a Data Center\n | url = http://www.inc.com/guides/2010/11/how-to-choose-a-data-center_pagen_2.html\n | year = 2010\n | author = Inc. staff\n | accessdate = 2012-07-21\n}}</ref>\n\n===Energy efficiency analysis===\nAn energy efficiency analysis measures the energy use of data center IT and facilities equipment. A typical energy efficiency analysis measures factors such as a data center\'s power use effectiveness (PUE) against industry standards, identifies mechanical and electrical sources of inefficiency, and identifies air-management metrics.<ref>Siranosian, Kathryn. "HP Shows Companies How to Integrate Energy Management and Carbon Reduction," TriplePundit, April 5, 2011. [http://www.triplepundit.com/2011/04/hp-launches-program-companies-integrate-manage-energy-carbon-reduction-strategies/]</ref>\n\n===Computational fluid dynamics (CFD) analysis===\n{{main article|Computational fluid dynamics}}\n\nThis type of analysis uses sophisticated tools and techniques to understand the unique thermal conditions present in each data center—predicting the temperature, airflow, and pressure behavior of a data center to assess performance and energy consumption, using numerical modeling.<ref>Bullock, Michael. "Computation Fluid Dynamics - Hot topic at Data Center World," Transitional Data Services, March 18, 2010. [http://blog.transitionaldata.com/aggregate/bid/37840/Seeing-the-Invisible-Data-Center-with-CFD-Modeling-Software] {{webarchive |url=https://web.archive.org/web/20120103183406/http://blog.transitionaldata.com/aggregate/bid/37840/Seeing-the-Invisible-Data-Center-with-CFD-Modeling-Software |date=January 3, 2012 }}</ref> By predicting the effects of these environmental conditions, CFD analysis in the data center can be used to predict the impact of high-density racks mixed with low-density racks<ref>Bouley, Dennis (editor). "Impact of Virtualization on Data Center Physical Infrastructure," The Green grid, 2010. [http://www.thegreengrid.org/~/media/WhitePapers/White_Paper_27_Impact_of_Virtualization_Data_On_Center_Physical_Infrastructure_020210.pdf?lang=en]</ref> and the onward impact on cooling resources, poor infrastructure management practices and AC failure of AC shutdown for scheduled maintenance.\n\n===Thermal zone mapping===\nThermal zone mapping uses sensors and computer modeling to create a three-dimensional image of the hot and cool zones in a data center.<ref>Fontecchio, Mark. "HP Thermal Zone Mapping plots data center hot spots," SearchDataCenter, July 25, 2007. [http://searchdatacenter.techtarget.com/news/1265634/HP-Thermal-Zone-Mapping-plots-data-center-hot-spots]</ref>\n\nThis information can help to identify optimal positioning of data center equipment. For example, critical servers might be placed in a cool zone that is serviced by redundant AC units.\n\n===Green data centers===\n[[File:Magazin Vauban E.jpg|thumb| This water-cooled data center in the [[Independent Port of Strasbourg|Port of Strasbourg]], France claims the attribute \'\'green\'\'.]]\nData centers use a lot of power, consumed by two main usages: the power required to run the actual equipment and then the power required to cool the equipment. The first category is addressed by designing computers and storage systems that are increasingly power-efficient.<ref name="ReferenceDC2"/> To bring down cooling costs data center designers try to use natural ways to cool the equipment. Many data centers are located near good fiber connectivity, power grid connections and also people-concentrations to manage the equipment, but there are also circumstances where the data center can be miles away from the users and don\'t need a lot of local management. Examples of this are the \'mass\' data centers like Google or Facebook: these DC\'s are built around many standardized servers and storage-arrays and the actual users of the systems are located all around the world. After the initial build of a data center staff numbers required to keep it running are often relatively low: especially data centers that provide mass-storage or computing power which don\'t need to be near population centers.Data centers in arctic locations where outside air provides all cooling are getting more popular as cooling and electricity are the two main variable cost components.<ref>{{cite web|url=http://www.gizmag.com/fjord-cooled-data-center/20938/|title=Fjord-cooled DC in Norway claims to be greenest|access-date=23 December 2011}}</ref>\n\n==Network infrastructure==\n[[File:Paris servers DSC00190.jpg|thumb|left|An example of "rack mounted" servers]]\nCommunications in data centers today are most often based on [[computer network|networks]] running the [[Internet protocol|IP]] [[protocol (computing)|protocol]] suite. Data centers contain a set of [[Router (computing)|routers]] and [[Network switch|switches]] that transport traffic between the servers and to the outside world. [[Redundancy (engineering)|Redundancy]] of the Internet connection is often provided by using two or more upstream service providers (see [[Multihoming]]).\n\nSome of the servers at the data center are used for running the basic Internet and [[intranet]] services needed by internal users in the organization, e.g., e-mail servers, [[proxy server]]s, and [[Domain Name System|DNS]] servers.\n\nNetwork security elements are also usually deployed: [[firewall (networking)|firewalls]], [[VPN]] [[Gateway (computer networking)|gateways]], [[intrusion detection system]]s, etc. Also common are monitoring systems for the network and some of the applications. Additional off site monitoring systems are also typical, in case of a failure of communications inside the data center.\n\n==Data center infrastructure management==\n[[Data center infrastructure management]] (DCIM) is the integration of information technology (IT) and facility management disciplines to centralize monitoring, management and intelligent capacity planning of a data center\'s critical systems. Achieved through the implementation of specialized software, hardware and sensors, DCIM enables common, real-time monitoring and management platform for all interdependent systems across IT and facility infrastructures.\n\nDepending on the type of implementation, DCIM products can help data center managers identify and eliminate sources of risk to increase availability of critical IT systems. DCIM products also can be used to identify interdependencies between facility and IT infrastructures to alert the facility manager to gaps in system redundancy, and provide dynamic, holistic benchmarks on power consumption and efficiency to measure the effectiveness of "green IT" initiatives.\n\nIt\'s important to measure and understand data center efficiency metrics.  A lot of the discussion in this area has focused on energy issues, but other metrics beyond the PUE can give a more detailed picture of the data center operations. Server, storage, and staff utilization metrics can contribute to a more complete view of an enterprise data center. In many cases, disc capacity goes unused and in many instances the organizations run their servers at 20% utilization or less.<ref>{{cite web|url=http://content.dell.com/us/en/enterprise/d/large-business/measure-data-center-efficiency.aspx |title=Measuring Data Center Efficiency: Easier Said Than Done |publisher=Dell.com |accessdate=2012-06-25 |deadurl=yes |archiveurl=https://web.archive.org/web/20101027083349/http://content.dell.com:80/us/en/enterprise/d/large-business/measure-data-center-efficiency.aspx |archivedate=2010-10-27 |df= }}</ref> More effective automation tools can also improve the number of servers or virtual machines that a single admin can handle.\n\nDCIM providers are increasingly linking with [[computational fluid dynamics]] providers to predict complex airflow patterns in the data center. The CFD component is necessary to quantify the impact of planned future changes on cooling resilience, capacity and efficiency.<ref name="gartner">{{cite web|url=http://www.gartner.com/it-glossary/computational-fluid-dynamic-cfd-analysis|title=Computational-Fluid-Dynamic (CFD) Analysis &#124; Gartner IT Glossary|publisher=gartner.com|accessdate=2014-08-27}}</ref>\n\n==Managing the capacity of a data center==\n{{unreferenced section|date=August 2016}}\n[[File:Capacity of a datacenter - Life Cycle.jpg|thumbnail|left|Capacity of a datacenter - Life Cycle]]\nSeveral parameters may limit the capacity of a data center. For long term usage, the main limitations will be available area, then available power. In the first stage of its life cycle, a data center will see its occupied space growing more rapidly than consumed energy. With constant densification of new IT technologies, the need in energy is going to become dominant, equaling then overcoming the need in area (second then third phase of cycle). The development and multiplication of connected objects, the needs in storage and data treatment lead to the necessity of data centers to grow more and more rapidly. It is therefore important to define a data center strategy before being cornered. The decision, conception and building cycle lasts several years. Therefore, it is imperative to initiate this strategic consideration when the data center reaches about 50% of its power capacity. Maximum occupation of a data center needs to be stabilized around 85%, be it in power or occupied area. Resources thus managed will allow a rotation zone for managing hardware replacement and will allow temporary cohabitation of old and new generations. In the case where this limit would be overcrossed durably, it would not be possible to proceed to material replacements, which would invariably lead to smothering the information system. The data center is a resource in its own right of the information system, with its own constraints of time and management (life span of 25 years), it therefore needs to be taken into consideration in the framework of the SI midterm planning (between 3 and 5 years).\n\n==Applications==\n[[File:IBMPortableModularDataCenter.jpg|thumb|right|A 40-foot [[Portable Modular Data Center]]]]\n\nThe main purpose of a data center is running the IT systems applications that handle the core business and operational data of the organization. Such systems may be proprietary and developed internally by the organization, or bought from [[enterprise software]] vendors. Such common applications are [[Enterprise resource planning|ERP]] and [[Customer relationship management|CRM]] systems.\n\nA data center may be concerned with just [[operations architecture]] or it may provide other services as well.\n\nOften these applications will be composed of multiple hosts, each running a single component. Common components of such applications are [[database]]s, [[file server]]s, [[application server]]s, [[middleware]], and various others.\n\nData centers are also used for off site backups. Companies may subscribe to backup services provided by a data center. This is often used in conjunction with [[Tape drive|backup tapes]]. Backups can be taken off servers locally on to tapes. However, tapes stored on site pose a security threat and are also susceptible to fire and flooding. Larger companies may also send their backups off site for added security. This can be done by backing up to a data center. Encrypted backups can be sent over the Internet to another data center where they can be stored securely.\n\nFor quick deployment or [[disaster recovery]], several large hardware vendors have developed mobile/modular solutions that can be installed and made operational in very short time. Companies such as\n[[File:Edge Night 02.jpg|thumb|A modular data center connected to the power grid at a utility substation]]\n* [[Cisco Systems]],<ref>{{cite web|title=Info and video about Cisco\'s solution |url=http://www.datacenterknowledge.com/archives/2008/May/15/ciscos_mobile_emergency_data_center.html |publisher=Datacentreknowledge |accessdate=2008-05-11 |date=May 15, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20080519213241/http://www.datacenterknowledge.com:80/archives/2008/May/15/ciscos_mobile_emergency_data_center.html |archivedate=2008-05-19 |df= }}</ref>\n* [[Sun Microsystems]] ([[Sun Modular Datacenter]]),<ref>{{cite web|url=http://www.sun.com/products/sunmd/s20/specifications.jsp|archiveurl=https://web.archive.org/web/20080513090300/http://www.sun.com/products/sunmd/s20/specifications.jsp|archivedate=2008-05-13|title=Technical specs of Sun\'s Blackbox|accessdate=2008-05-11}}</ref><ref>And English Wiki article on [[Sun Modular Datacenter|Sun\'s modular datacentre]]</ref>\n* [[Groupe Bull|Bull]] (mobull),<ref>{{cite web|title=Mobull Plug and Boot Datacenter|url=http://www.bull.com/extreme-computing/mobull.html|publisher=Bull|first=Daniel|last=Kidger|accessdate=2011-05-24}}</ref>\n* [[IBM]] ([[Portable Modular Data Center]]),\n* [[Schneider-Electric]] ([[Portable Modular Data Center]]),\n* [[Hewlett-Packard|HP]] ([[HP Performance Optimized Datacenter|Performance Optimized Datacenter]]),<ref>{{cite web|url=http://h18004.www1.hp.com/products/servers/solutions/datacentersolutions/pod/index.html |title=HP Performance Optimized Datacenter (POD) 20c and 40c - Product Overview |publisher=H18004.www1.hp.com |date= |accessdate=2013-08-30}}</ref>\n* [[Huawei]] (Container Data Center Solution),<ref>{{cite web|title=Huawei\'s Container Data Center Solution|url=http://www.huawei.com/ilink/enenterprise/download/HW_143893|publisher=Huawei|accessdate=2014-05-17}}\n</ref> and\n* [[Google]] ([[Google Modular Data Center]]) have developed systems that could be used for this purpose.<ref>{{cite web|url=http://www.crn.com/hardware/208403225 |publisher=ChannelWeb |accessdate=2008-05-11 |title=IBM\'s Project Big Green Takes Second Step |first=Brian |last=Kraemer |date=June 11, 2008 |deadurl=yes |archiveurl=https://web.archive.org/web/20080611114732/http://www.crn.com:80/hardware/208403225 |archivedate=2008-06-11 |df= }}</ref><ref>{{cite web|url=http://hightech.lbl.gov/documents/data_centers/modular-dc-procurement-guide.pdf |title=Modular/Container Data Centers Procurement Guide: Optimizing for Energy Efficiency and Quick Deployment |format=PDF |date= |accessdate=2013-08-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20130531191212/http://hightech.lbl.gov/documents/data_centers/modular-dc-procurement-guide.pdf |archivedate=2013-05-31 |df= }}</ref>\n* BASELAYER has a patent on the software defined modular data center.<ref>{{Citation|title = System and method of providing computer resources|url = http://www.google.com/patents/US8434804|date = May 7, 2013|accessdate = 2016-02-24|first = George|last = Slessman}}</ref><ref>{{Cite web|title = Modular Data Center Firm IO to Split Into Two Companies|url = http://www.datacenterknowledge.com/archives/2014/12/02/modular-data-center-firm-io-to-split-into-two-companies/|website = Data Center Knowledge|access-date = 2016-02-24|language = en-US}}</ref>\n\n==US wholesale and retail colocation providers==\nAccording to Synergy Research Group, "the scale of the wholesale colocation market in the United States is very significant relative to the retail market, with Q3 wholesale revenues reaching almost $700 million. [[Digital Realty]] Trust is the wholesale market leader, followed at a distance by [[DuPont Fabros]]." Synergy Research also describes the US colocation market as the most mature and well-developed in the world," based on revenue and the continued adoption of cloud infrastructure services.\n* Contains estimates from Synergy Research Group.<ref name="srgresearch">{{cite web|url=https://www.srgresearch.com/articles/mature-us-colocation-market-led-equinix-and-centurylink-savvis|title=Mature US Colocation Market Led by Equinix and CenturyLink-Savvis &#124; Synergy Research Group|author=Synergy Research Group, Reno, NV|publisher=srgresearch.com|accessdate=2014-08-27}}</ref>\n\n{| class="wikitable sortable"\n|-\n!Rank !! Company Name !! US Market Share\n|-\n!1\n| Various Providers || 34%\n|-\n!2\n| [[Equinix]] || 18%\n|-\n!3\n| [[CenturyLink-Savvis]] || 8%\n|-\n!4\n| [[SunGard]] || 5%\n|-\n!5\n| [[AT&T]] || 5%\n|-\n!6\n| [[Verizon]] || 5%\n|-\n!7\n| Telx || 4%\n|-\n!8\n| CyrusOne || 4%\n|-\n!9\n| [[Level 3 Communications]] || 3%\n|-\n!10\n| [[Internap]] || 2%\n|}\n\n==See also==\n{{columns-list|colwidth=20em|\n* [[Central apparatus room]]\n* [[Colocation center]]\n* [[Data center infrastructure management]]\n* [[Disaster recovery]]\n* [[Dynamic Infrastructure]]\n* [[Electrical network]]\n* [[HVAC]]\n* [[Internet exchange point]]\n* [[Internet hosting service]]\n* [[Modular data center]]\n* [[Neher–McGrath]]\n* [[Network operations center]]\n* [[Open Compute Project]], by [[Facebook]]\n* [[Peering]]\n* [[Server farm]]\n* [[Server room]]\n* [[Server Room Environment Monitoring System]]\n* [[Server sprawl]]\n* [[Sun Modular Datacenter]]\n* [[Telecommunications network]]\n* [[Utah Data Center]]\n* [[Web hosting service]]\n* [[Anderson Powerpole]] connector\n}}\n\n==References==\n{{reflist|colwidth=30em}}\n\n==External links==\n{{Commons category|Data centers}}\n{{wikibooks|The Design and Organization of Data Centers}}\n{{wiktionary}}\n* [http://hightech.lbl.gov/datacenters.html Lawrence Berkeley Lab] - Research, development, demonstration, and deployment of energy-efficient technologies and practices for data centers\n* [http://hightech.lbl.gov/dc-powering/faq.html DC Power For Data Centers Of The Future] - FAQ: 380VDC testing and demonstration at a Sun data center.\n* [http://www.dccompendium.com/ DC Compendium] - Repository and compendium of data centers globally.\n* [http://media.wix.com/ugd/fb8983_e929404b24874e4fa7a8279f1cda58f8.pdf White Paper] - Property Taxes: The New Challenge for Data Centers\n\n{{Authority control}}\n{{Cloud computing}}\n\n{{DEFAULTSORT:Data Center}}\n[[Category:Computer networking]]\n[[Category:Applications of distributed computing]]\n[[Category:Cloud storage]]\n[[Category:Data management]]\n[[Category:Distributed data storage]]\n[[Category:Distributed data storage systems]]\n[[Category:Servers (computing)]]\n[[Category:Data centers| ]]']
['Database-centric architecture', '13783336', '\'\'\'Database-centric Architecture\'\'\' or \'\'\'data-centric architecture\'\'\' has several distinct meanings, generally relating to [[software architecture]]s in which [[database]]s play a crucial role. Often this description is meant to contrast the design to an alternative approach. For example, the characterization of an architecture as "database-centric" may mean any combination of the following:\n\n* using a standard, general-purpose [[relational database management system]], as opposed to customized in-[[Memory (computers)|memory]] or [[Computer file|file]]-based [[data structures]] and [[access method]]s. With the evolution of sophisticated [[Database management system|DBMS]] software, much of which is either free or included with the [[operating system]], application developers have become increasingly reliant on standard database tools, especially for the sake of [[rapid application development]].\n* using dynamic, [[Table (database)|table]]-driven logic, as opposed to logic embodied in previously [[compiled]] [[Computer program|program]]s. The use of table-driven logic, i.e. behavior that is heavily dictated by the contents of a database, allows programs to be simpler and more flexible. This capability is a central feature of [[dynamic programming language]]s. See also [[control table]]s for tables that are normally coded and embedded within programs as [[data structures]] (i.e. not compiled statements) but could equally be read in from a [[flat file]], [[database]] or even retrieved from a [[spreadsheet]].\n* using [[stored procedure]]s that run on [[database server]]s, as opposed to greater reliance on logic running in middle-tier [[application server]]s in a [[multi-tier architecture]]. The extent to which [[business logic]] should be placed at the back-end versus another tier is a subject of ongoing debate. For example, Toon Koppelaars presents a detailed analysis of alternative [[Oracle Database|Oracle-based]] architectures that vary in the placement of business logic, concluding that a database-centric approach has practical advantages from the standpoint of ease of development and maintainability.<ref>[https://web.archive.org/web/20060525094651/http://www.oracle.com/technology/pub/articles/odtug_award.pdf] A Database-centric approach to J2EE Application Development</ref>\n* using a shared database as the basis for communicating between [[Parallel computing|parallel processes]] in [[distributed computing]] applications, as opposed to direct [[inter-process communication]] via [[message passing]] functions and [[message-oriented middleware]]. A potential benefit of database-centric architecture in [[distributed application]]s is that it simplifies the design by utilizing DBMS-provided [[transaction processing]] and [[Index (database)|indexing]] to achieve a high degree of reliability, performance, and capacity.<ref>{{Citation |author=Lind P, Alm M |title=A database-centric virtual chemistry system |journal=J Chem Inf Model |volume=46 |issue=3 |pages=1034–9 |year=2006 |pmid=16711722 |doi=10.1021/ci050360b |postscript=. }}</ref> For example, [[Base One]] describes a database-centric distributed computing architecture for [[Grid computing|grid]] and [[Computer cluster|cluster]] computing, and explains how this design provides enhanced security, fault-tolerance, and [[scalability]].<ref>[http://www.boic.com/dbgrid.htm Database-Centric Grid and Cluster Computing]</ref>\n* an overall enterprise architecture that favors shared data models<ref>{{Cite news|url=http://tdan.com/the-data-centric-revolution/18780|title=The Data Centric Revolution|newspaper=TDAN.com|access-date=2017-01-09}}</ref> over allowing each application to have its own, idiosyncratic data model. \n\n==See also==\n*[[Control table]]s\n*[[:Category:Data-centric programming languages|Data-centric programming languages]]\n*The [[data-driven programming]] paradigm, which makes the information used in a system the primary design driver.\n*See the [http://datacentricmanifesto.org/ datacentricmanifesto.org] \n\n==References==\n{{Reflist}}\n\n{{Database}}\n\n{{DEFAULTSORT:Database-Centric Architecture}}\n[[Category:Software architecture]]\n[[Category:Data management]]\n[[Category:Distributed computing architecture]]']
['Category:Structured storage', '25067249', '{{Cat main|Structured storage}}\n\n[[Category:Data management]]']
['Synthetic data', '25270778', '{{Citation style|date=May 2014}}\'\'\'Synthetic data\'\'\' are "any production data applicable to a given situation that are not obtained by direct measurement" according to the McGraw-Hill Dictionary of Scientific and Technical Terms;<ref name="McGraw">Synthetic data. (n.d.). \'\'McGraw-Hill Dictionary of Scientific and Technical Terms\'\'. Retrieved November 29, 2009, from Answers.com Web site: [http://www.answers.com/topic/synthetic-data]</ref> where Craig S. Mullins, an expert in data management, defines production data as "information that is persistently stored and used by professionals to conduct business processes.".<ref name="Mullins">Mullins, Craig S. (2009, February 5). \'\'What is Production Data?\'\' Message posted to http://www.neon.com/blog/blogs/cmullins/archive/2009/02/05/What-is-Production-Data_3F00_.aspx</ref>\n\nThe creation of synthetic data is an involved process of data [[Anonymity|anonymization]]; that is to say that synthetic data is a [[subset]] of anonymized data.<ref name="MachanavajjhalaEtAl">{{Cite journal\n  | title = Privacy: Theory meets Practice on the Map\n  | journal = 2008 IEEE 24th International Conference on Data Engineering\n  | doi = 10.1109/ICDE.2008.4497436\n  | pages = 277–286\n  | year = 2008\n  | last1 = MacHanavajjhala\n  | first1 = Ashwin\n  | last2 = Kifer\n  | first2 = Daniel\n  | last3 = Abowd\n  | first3 = John\n  | last4 = Gehrke\n  | first4 = Johannes\n  | last5 = Vilhuber\n  | first5 = Lars}}</ref> Synthetic data is used in a variety of fields as a filter for information that would otherwise compromise the [[confidentiality]] of particular aspects of the data. Many times the particular aspects come about in the form of human information (i.e. name, home address, [[IP address]], telephone number, social security number, credit card number, etc.).\n\n== Usefulness ==\n\nSynthetic data are generated to meet specific needs or certain conditions that may not be found in the original, real data.  This can be useful when designing any type of system because the synthetic data are used as a simulation or as a theoretical value, situation, etc.  This allows us to take into account unexpected results and have a basic solution or remedy, if the results prove to be unsatisfactory. Synthetic data are often generated to represent the authentic data and allows a baseline to be set.<ref name="Barse">Barse, E.L., Kvarnström, H., & Jonsson, E. (2003). \'\'Synthesizing test data for fraud detection systems.\'\' Manuscript submitted for publication, Department of Computer Engineering, Chalmbers University of Technology, Göteborg, Sweden. Retrieved from http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1254343&isnumber=28060</ref> Another use of synthetic data is to protect privacy and confidentiality of authentic data. As stated previously, synthetic data is used in testing and creating many different types of systems; below is a quote from the abstract of an article that describes a software that generates synthetic data for testing fraud detection systems that further explains its use and importance.\n"This enables us to create realistic behavior profiles for users and attackers. The data is used to train the [[fraud]] detection system itself, thus creating the necessary adaptation of the system to a specific environment."<ref name="Barse"/>\n\n==History==\nThe history of the generation of synthetic data dates back to 1993. In 1993, the idea of original fully synthetic data was created by [[Donald Rubin|Rubin]].<ref name="Rubin1993">{{Cite journal\n  | authorlink = Rubin, Donald B.\n  | title = Discussion: Statistical Disclosure Limitation\n  | journal = Journal of Official Statistics\n  | volume = 9\n  | pages = 461–468\n  | year = 1993}}\n</ref> Rubin originally designed this to synthesize the Decennial Census long form responses for the short form households. He then released samples that did not include any actual long form records - in this he preserved anonymity of the household.<ref name="Abowd">\n{{Cite web\n  | last = Abowd\n  | first = John M.\n  | title = Confidentiality Protection of Social Science Micro Data: Synthetic Data and Related Methods. [Powerpoint slides]\n  | url=http://www.idre.ucla.edu/events/PPT/2006_01_30_abowd_UCLA_synthetic_data_presentation.ppt\n  | accessdate = 17 February 2011 }} \n</ref> Later that year, the idea of original partially synthetic data was created by Little. Little used this idea to synthesize the sensitive values on the public use file.<ref name="Little">{{Cite journal\n  | authorlink = Little, Rod\n  | title = Statistical Analysis of Masked Data\n  | journal = Journal of Official Statistics\n  | volume = 9\n  | pages = 407–426\n  | year = 1993}}\n</ref>\n\nIn 1994, [[Stephen Fienberg|Fienberg]] came up with the idea of critical refinement, in which he used a parametric posterior predictive distribution (instead of a Bayes bootstrap) to do the sampling.<ref name="Abowd"/> Later, other important contributors to the development of synthetic data generation are [[Trivellore Raghunathan|Raghunathan]], [[Jerry Reiter|Reiter]], [[Donald Rubin|Rubin]], [[John M. Abowd|Abowd]], [[Jim Woodcock|Woodcock]]. Collectively they came up with a solution for how to treat partially synthetic data with missing data. Similarly they came up with the technique of Sequential Regression Multivariate [[Imputation (statistics)|Imputation]].<ref name="Abowd"/>\n\n==Applications==\nSynthetic data are used in the process of [[data mining]].  Testing and training [[fraud]] detection systems, confidentiality systems and any type of system is devised using synthetic data. As described previously, synthetic data may seem as just a compilation of “made up” data, but there are specific algorithms and generators that are designed to create realistic data.<ref name="Deng">Deng, R. (2002). \'\'Information and Communications Security\'\'. Proceedings of the 4th International Conference, ICICS 2002 Singapore, December 2002. Retrieved from https://books.google.com/books?id=6mod7enQa8cC&pg=PA265&dq=%22synthetic+data%22#v=onepage&q=%22synthetic%20data%22&f=false</ref> This synthetic data assists in teaching a system how to react to certain situations or criteria. Researcher doing [[clinical trials]] or any other research may generate synthetic data to aid in creating a baseline for future studies and testing.  For example, intrusion detection software is tested using synthetic data. This data is a representation of the authentic data and may include intrusion instances that are not found in the authentic data. The synthetic data allows the software to recognize these situations and react accordingly. If synthetic data was not used, the software would only be trained to react to the situations provided by the authentic data and it may not recognize another type of intrusion.<ref name="Barse"/>\n\nSynthetic data is also used to protect the [[privacy]] and [[confidentiality]] of a set of data. Real data contains personal/private/confidential information that a programmer, software creator or research project may not want to be disclosed.<ref name="Abowd2">Abowd, J.M., & Lane, J. (2004). \'\'New Approaches to Confidentiality Protection: Synthetic Data, Remote Access and Research Data Centers\'\'. Manuscript submitted for publication, Cornell Institute for Social and Economic Research (CISER), Cornell University, Ithica, New York. Retrieved from http://www.springerlink.com/content/27nud7qx09qurg3p/fulltext.pdf</ref> Synthetic data holds no personal information and cannot be traced back to any individual; therefore, the use of synthetic data reduces confidentiality and privacy issues.\n\n==Calculations==\nResearchers test the framework on synthetic data, which is "the only source of ground truth on which they can objectively assess the performance of their [[algorithm]]s".<sup>10</sup>\n\n"Synthetic data can be generated with random orientations and positions."<sup>8</sup>  Datasets can be get fairly complicated. A more complicated dataset can be generated by using a synthesizer build.  To create a synthesizer build, first use the original data to create a model or equation that fits the data the best. This model or equation will be called a synthesizer build. This build can be used to generate more data.<sup>9</sup>\n\nConstructing a synthesizer build involves constructing a [[statistical model]].  In a [[linear regression]] line example, the original data can be plotted, and a best fit [[linear regression|linear line]] can be created from the data.  This [[linear regression|line]] is a synthesizer created from the original data.  The next step will be generating more synthetic data from the synthesizer build or from this linear line equation.  In this way, the new data can be used for studies and research, and it protects the [[confidentiality]] of the original data.<sup>9</sup>\n\nDavid Jensen from the Knowledge Discovery Laboratory mentioned how to generate synthetic data in his "Proximity 4.3 Tutorial" chapter 6: "Researchers frequently need to explore the effects of certain data characteristics on their [[data model]]." To help construct [[data set|datasets]] exhibiting specific properties, such as [[autocorrelation|auto-correlation]] or degree disparity, proximity can generate synthetic data having one of several types of graph structure<sup>10</sup>:[[random graph]]s that is generated by some [[random process]];[[lattice graph]]s having a ring structure;[[lattice graph]]s having a grid structure, etc.\nIn all cases, the data generation process follows the same process:\n1.\tGenerate the empty [[Graph (data structure)|graph structure]].\n2.\tGenerate [[Attribute-value system|attribute values]] based on user-supplied prior probabilities.\n\nSince the [[Attribute-value system|attribute values]] of one object may depend on the [[Attribute-value system|attribute values]] of related objects, the attribute generation process assigns values collectively.<sup>10</sup>\n\n==References==\n{{Reflist}}\n* Wang, A, Qiu, T, & Shao, L. (2009). \'\'A Simple Method of Radial Distortion Correction with Centre of Distortion Estimation\'\'. 35. Retrieved from http://www.springerlink.com/content/8180144q56t30314/fulltext.pdf\n* Duncan, G. (2006). \'\'Statistical confidentiality: Is Synthetic Data the Answer?\'\' Retrieved from http://www.idre.ucla.edu/events/PPT/2006_02_13_duncan_Synthetic_Data.ppt\n* Jensen, D. (2004). \'\'Proximity 4.3 Tutorial Chapter 6.\'\' Retrieved from http://kdl.cs.umass.edu/proximity/documentation/tutorial/ch06s09.html\n* Jackson, C, Murphy, R, & Kovaˇcevic´, J. (2009). \'\'Intelligent Acquisition and Learning of Fluorescence Microscope Data Models.\'\' 18(9), Retrieved from http://www.andrew.cmu.edu/user/jelenak/Repository/08_JacksonMK.pdf\n* {{cite book| author=Adam Coates and Blake Carpenter and Carl Case and Sanjeev Satheesh and Bipin Suresh and Tao Wang and David J. Wu and Andrew Y. Ng| chapter=Text Detection and Character Recognition in Scene Images with Unsupervised Feature Learning| title=ICDAR| year=2011| pages=440–445| accessdate=13 May 2014}}\n\n==External links==\n* The "DataGenerator" a model-based synthetic data generator: http://finraos.github.io/DataGenerator/\n* The \'\'datgen\'\' synthetic data generator: http://www.datasetgenerator.com\n* Fienberg, S. E. (1994). "Conflicts between the needs for access to statistical information and demands for confidentiality," Journal of Official Statistics 10, 115–132.\n* Little, R (1993). "Statistical Analysis of Masked Data," Journal of Official Statistics, 9, 407-426.\n* Raghunathan, T.E., Reiter, J.P., and Rubin, D.B. (2003). "Multiple Imputation for Statistical Disclosure Limitation," Journal of Official Statistics, 19, 1-16.\n* Reiter, J.P. (2004). "Simultaneous Use of Multiple Imputation for Missing Data and Disclosure Limitation," Survey Methodology, 30, 235-242.\n\n \t\n{{FOLDOC}}\n{{Statistics}}\n\n[[Category:Data]]\n[[Category:Computer data]]\n[[Category:Data management]]']
['Uniform information representation', '1610862', "{{unreferenced|date=December 2009}}\n\n'''Uniform information representation'''  allows information from several realms or disciplines to be displayed and worked with as if it came from the same realm or discipline.  It takes information from a number of sources, which may have used different methodologies and metrics in their data collection, and builds a single large collection of information, where some records may be more complete than others across all fields of data\n\nUniform information representation is particularly important in the fields of [[Enterprise Information Integration]] (EII) and [[Electronic Data Interchange]] (EDI), where different departments of a large organization may have collected information for different purposes, with different labels and units, until one department realized that data already collected by those other departments could be re-purposed for their own needs—saving the enterprise the effort and cost of re-collecting the same information.\n\n{{DEFAULTSORT:Uniform Information Representation}}\n[[Category:Data management]]\n\n{{Comp-sci-stub}}"]
['Point-in-time recovery', '4576703', "{{Redirect|PITR|other uses|Pitr (disambiguation){{!}}Pitr}}\n{{Unreferenced stub|auto=yes|date=December 2009}}\n\n'''Point-in-time recovery''' ('''PITR''') in the context of [[computer]]s involves systems whereby an administrator can restore or recover a set of data or a particular setting from a time in the past. Note for example [[Windows XP]]'s capability to restore operating-system settings from a past date (before data corruption occurred, for example).  [[Time Machine (OS X)|Time Machine]] for Mac OS X provides another example of point-in-time recovery.\n\nOnce PITR logging starts for a PITR-capable [[database]], a [[database administrator]] can restore that database from [[backup]]s to the state that it had at any time since.\n\n==External links==\n* [http://blog.ganneff.de/blog/2008/02/15/postgresql-continuous-archivin.html PostgreSQL Continuous Archiving and Point-In-Time Recovery (PITR) blog/article]\n* [http://dev.mysql.com/doc/refman/5.5/en/point-in-time-recovery.html MySQL 5.5 Point in Time Recovery]\n\n{{DEFAULTSORT:Point-In-Time Recovery}}\n[[Category:Data management]]\n\n\n{{Compu-stub}}"]
['Content migration', '26350658', '{{Multiple issues|\n{{primary sources|date=March 2011}}\n{{cleanup|date=March 2011}}\n}}\n\n\'\'\'Content Migration\'\'\' is the process of moving information stored on a [[Web content management system]]  (CMS), [[Digital asset management]] (DAM), [[Document management system]] (DMS), or flat HTML based system to a new system. Flat HTML content can entail HTML files, [[Active Server Pages]] (ASP), [[JavaServer Pages]] (JSP), [[PHP]], or content stored in some type of [[HTML]]/[[JavaScript]] based system and can be either static or dynamic content.   \n\nContent Migrations can solve a number of issues ranging from:\n* Consolidation from one or more CMS systems into one system to allow for more centralized control, governance of content, and better   Knowledge    management and sharing.\n* Reorganizing content due to mergers and acquisitions to assimilate as much content from the source systems for a unified look and feel.\n* Converting content that has grown organically either in a CMS or Flat HTML and standardizing the formatting so standards can be applied for a unified branding of the content.\n\nThere are many ways to access the content stored in a CMS.  Depending on the CMS vendor they offer either an  [[Application programming interface]] (API), [[Web services]], rebuilding a record by writing [[SQL]] queries, [[XML]] exports, or through the web interface.\n\n# The API<ref name="refname1"/> requires a developer to read and understand how to interact with the source CMS’s API layer then develop an application that extracts the content and stores it in a database, XML file, or Excel. Once the content is extracted the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.\n# Most CMSs use a database to store and associate content so if no API exists the SQL programmer must reverse engineer the table structure.  Once the structure is reverse engineered, very complex SQL queries are written to pull all the content from multiple tables into an intermediate table or into some type of [[Comma-separated values]] (CSV) or XML file.   Once the developer has the files or database the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.\n# XML export creates XML files of the content stored in a CMS but after the files are exported they need to be altered to fit the new scheme of the target CMS system.  This is typically done by a developer by writing some code to do the transformation.\n# HTML files, JSP, ASP, PHP, or other application server file formats are the most difficult.  The  structure for Flat HTML files are based on a culmination of  folder structure, HTML file structure, and image locations.  In the early days of content migration, the developer had to use programming languages to parse the html files and save it as structured database, XML or CSV. Typically PERL, JAVA, C++, or C# were used because of the regular expression handling capability.  JSP, ASP, PHP, ColdFusion, and other Application Server technologies usually rely on server side includes to help simplify development but makes it very difficult to migrate content because the content is not assembled until the user looks at it in their web browser.  This makes is very difficult to look at the files and extract the content from the file structure.\n# Web Scraping allows users to access most of the content directly from the Web User Interface.  Since a web interface is visual (this is the point of a CMS) some Web Scrapers leverage the UI to extract content and place it into a structure like a Database, XML, or CSV formats.  All CMSs, DAMs, and DMSs use  web interfaces so extracting the content for one or many source sites is basically the same process.  In some cases it is possible to push the content into the new CMS using the web interface but some CMSs use JAVA applets, or Active X Control which are not supported by most web scrapers.  In that case the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.\n\'\'\'The basic content migration flow\'\'\'\n\n1. Obtain an inventory of the content.<br />\n2. Obtain an inventory of Binary content like Images, PDFs, CSS files, Office Docs, Flash, and any binary objects.<br />\n3. Find any broken links in the content or content resources.<br />\n4. Determine the Menu Structure of the Content.<br />\n5. Find the parent/sibling connection to the content so the links to other content and resources are not broken when moving them.<br />\n6. Extract the Resources from the pages and store them into a Database or File structure.  Store the reference in a database or a File.<br />\n7. Extract the HTML content from the site and store locally.<br />\n8. Upload the resources to the new CMS either by using the API or the web interface and store the new location in a Database or XML.<br />\n9. Transform the HTML to meet the new CMSs standards and reconnect any resources.<br />\n10. Upload the transformed content into the new system.\n\n== References ==\n<references>\n<ref name="refname1">[http://msdn.microsoft.com/en-us/library/ms453426.aspx What the Content Migration APIs Are Not]</ref>\n</references>\n\n==External links==\n* [http://www.cmswire.com/cms/web-publishing/no-small-task-migrating-content-to-a-new-cms-002437.php No Small Task: Migrating Content to a New CMS]\n\n[[Category:Data management]]']
['Data migration', '1135408', "{{More footnotes|date=February 2013}}\n'''Data migration''' is the process of [[data transfer|transferring]] [[data]] between [[computer data storage|computer storage]] types or [[file format]]s. It is a key consideration for any system implementation, upgrade, or consolidation. Data migration is usually performed programmatically to achieve an ''automated migration'', freeing up human resources from tedious tasks. Data migration occurs for a variety of reasons, including server or storage equipment replacements, maintenance or upgrades, [[Software modernization|application migration]], website consolidation and [[data center]] relocation.<ref>Janssen C, Data migration, http://www.techopedia.com/definition/1180/data-migration (retrieved 12 August 2013)</ref>\n\nTo achieve an effective data migration procedure, data on the old system is [[data mapping|mapped]] to the new system utilising a design for [[data extraction]] and [[data loading]]. The design relates old [[data format]]s to the new system's formats and requirements. Programmatic data migration may involve many phases but it minimally includes ''data extraction'' where data is read from the old system and ''data loading'' where data is written to the new system.\n\nAfter loading into the new system, results are subjected to [[data verification]] to determine whether data was accurately translated, is complete, and supports processes in the new system. During verification, there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous [[data loss]].\n\nAutomated and manual data cleaning is commonly performed in migration to improve [[data quality]], eliminate [[data duplication|redundant]] or obsolete information, and match the requirements of the new system.\n\nData migration phases (design, [[extract, transform, load|extraction]], [[data cleansing|cleansing]], load, verification) for applications of moderate to high complexity are commonly repeated several times before the new system is deployed.\n\n==Categories==\n\nData is stored on various media in [[Computer file|files]] or [[databases]], and is generated and consumed by [[software applications]] which in turn support [[business processes]]. The need to transfer and convert data can be driven by multiple business requirements and the approach taken to the migration depends on those requirements. Four major migration categories are proposed on this basis.\n\n===Storage migration===\nA business may choose to rationalize the physical media to take advantage of more efficient storage technologies. This will result in having to move physical blocks of data from one tape or disk to another, often using [[Storage virtualization|virtualization]] techniques. The data format and content itself will not usually be changed in the process and can normally be achieved with minimal or no impact to the layers above.\n\n===Database migration===\n{{main article|Schema migration}}\nSimilarly, it may be necessary to move from one [[database]] vendor to another, or to upgrade the version of database software being used. The latter case is less likely to require a physical data migration, but this can happen with major upgrades. In these cases a physical transformation process may be required since the underlying data format can change significantly. This may or may not affect behavior in the applications layer, depending largely on whether the data manipulation language or protocol has changed – but modern applications are written to be agnostic to the database technology so that a change from [[Sybase]], [[MySQL]], [[IBM DB2|DB2]] or [[Microsoft SQL Server|SQL Server]] to [[Oracle Database|Oracle]] should only require a testing cycle to be confident that both functional and non-functional performance has not been adversely affected.\n\n===Application migration===\nChanging application vendor – for instance a new [[Customer relationship management|CRM]] or [[Enterprise resource planning|ERP]] platform – will inevitably involve substantial transformation as almost every application or suite operates on its own specific data model and also interacts with other applications and systems within the [[enterprise application integration]] environment. Furthermore, to allow the application to be sold to the widest possible market, commercial off-the-shelf packages are generally configured for each customer using [[metadata]]. [[Application programming interfaces]] (APIs) may be supplied by vendors to protect the [[data integrity|integrity of the data]] they have to handle.\n\n===Business process migration===\n[[Business processes]] operate through a combination of human and application systems actions, often orchestrated by [[business process management]] tools. When these change they can require the movement of data from one store, database or application to another to reflect the changes to the organization and information about customers, products and operations. Examples of such migration drivers are mergers and acquisitions, business optimization and reorganization to attack new markets or respond to competitive threat.\n\nThe first two categories of migration are usually routine operational activities that the IT department takes care of without the involvement of the rest of the business. The last two categories directly affect the operational users of processes and applications, are necessarily complex, and delivering them without significant business downtime can be challenging. A highly adaptive approach, concurrent synchronization, a business-oriented audit capability and clear visibility of the migration for stakeholders are likely to be key requirements in such migrations.\n\n===Project versus process===\nThere is a difference between data migration and [[data integration]] activities. Data migration is a project by means of which data will be moved or copied from one environment to another, and removed or decommissioned in the source. During the migration (which can take place over months or even years), data can flow in multiple directions, and there may be multiple migrations taking place simultaneously. The [[Extract, Transform, Load]] actions will be necessary, although the means of achieving these may not be those traditionally associated with the [[Extract, Transform, Load|ETL]] acronym.\n\nData integration, by contrast, is a permanent part of the IT architecture, and is responsible for the way data flows between the various applications and data stores - and is a process rather than a project activity. Standard ETL technologies designed to supply data from operational systems to data warehouses would fit within the latter category.\n\n== Migration as a form of digital preservation ==\nMigration, which focuses on the digital object itself, is the act of transferring, or rewriting data from an out-of-date medium to a current medium and has for many years been considered the only viable approach to long-term preservation of digital objects.<ref>{{cite journal|author1=van der Hoeven, Jeffrey|author2= Bram Lohman|author3=Remco Verdegem|title=Emulation for Digital Preservation in Practice: The Results|journal=The International Journal of Digital Curation|volume=2|issue=2|year=2007|pages=123-132|url=http://www.ijdc.net/index.php/ijdc/article/view/50|doi=10.2218/ijdc.v2i2.35}}</ref> Reproducing brittle newspapers onto [[microform|microfilm]] is an example of such migration.\n\n=== Disadvantages ===\n* Migration addresses the possible obsolescence of the data carrier, but does not address the fact that certain technologies which run the data may be abandoned altogether, leaving migration useless.\n* Time-consuming – migration is a continual process, which must be repeated every time a medium reaches obsolescence, for all data objects stored on a certain media.\n* Costly - an institution must purchase additional data storage media at each migration.<ref>{{cite journal|author=Muira, Gregory|title=Pushing the Boundaries of Traditional Heritage Policy: maintaining long-term access to multimedia content|journal=IFLA Journal|volume=33|year=2007|pages=323-326|url=http://www.ifla.org/files/assets/hq/publications/ifla-journal/ifla-journal-4-2007.pdf}}</ref>\n\nAs a result of the disadvantages listed above, technology professionals have begun to develop alternatives to migration, such as [[emulator|emulation]].\n\n==See also==\n* [[Data conversion]]\n* [[Data transformation]]\n* [[Extract, transform, load]]\n* [[System migration]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* {{dmoz|Computers/Software/Databases/Data_Warehousing/Extraction_and_Transformation|Data Migration}}\n\n{{Authority control}}\n\n[[Category:Data management]]"]
['Data stream management system', '26760516', '{{Use dmy dates|date=July 2013}}\n\nA \'\'\'Data stream management system\'\'\' (DSMS) is a computer program to manage continuous data streams. It is similar to a [[database management system]] (DBMS), which is, however, designed for static data in conventional databases. A DSMS also offers a flexible query processing so that the information need can be expressed using queries. However, in contrast to a DBMS, a DSMS executes a \'\'continuous query\'\' that is not only performed once, but is permanently installed. Therefore, the query is continuously executed until it is explicitly uninstalled. Since most DSMS are data-driven, a continuous query produces new results as long as new data arrive at the system. This basic concept is similar to [[Complex event processing]] so that both technologies are partially coalescing.\n\n== Functional principle ==\n\nOne of the most important features of a DSMS is the possibility to handle potentially infinite and rapidly changing data streams by offering flexible processing at the same time, although there are only limited resources such as main memory. The following table provides various principles of DSMS and compares them to traditional DBMS.\n\n{| border="1"\n! Database management system (DBMS)\n! Data stream management system (DSMS)\n|-\n|Persistent data (relations)\n|volatile data streams\n|-\n|Random access\n|Sequential access\n|-\n|One-time queries \n|Continuous queries\n|-\n|(theoretically) unlimited secondary storage\n|limited main memory\n|-\n|Only the current state is relevant\n|Consideration of the order of the input\n|-\n|relatively low update rate\n|potentially extremely high update rate\n|-\n|Little or no time requirements\n|Real-time requirements\n|-\n|Assumes exact data \n|Assumes outdated/inaccurate data\n|-\n|Plannable query processing\n|Variable data arrival and data characteristics\n|}\n\n== Processing and streaming models ==\nOne of the biggest challenges for a DSMS is to handle potentially infinite data streams using a fixed amount of memory and no random access to the data. There are different approaches to limit the amount of data in one pass, which can be divided into two classes. For the one hand, there are compression techniques that try to summarize the data and for the other hand there are window techniques that try to portion the data into (finite) parts.\n\n=== Synopses ===\nThe idea behind compression techniques is to maintain only a synopsis of the data, but not all (raw) data points of the data stream. The algorithms range from selecting random data points called sampling to summarization using histograms, wavelets or sketching. One simple example of a compression is the continuous calculation of an average. Instead of memorizing each data point, the synopsis only holds the sum and the number of items. The average can be calculated by dividing the sum by the number. However, it should be mentioned that synopses cannot reflect the data accurately. Thus, a processing that is based on synopses may produce inaccurate results.\n\n=== Windows ===\nInstead of using synopses to compress the characteristics of the whole data streams, window techniques only look on a portion of the data. This approach is motivated by the idea that only the most recent data are relevant. Therefore, a window continuously cuts out a part of the data stream, e.g. the last ten data stream elements, and only considers these elements during the processing. There are different kinds of such windows like sliding windows that are similar to [[FIFO (computing and electronics)|FIFO]] lists or tumbling windows that cut out disjoint parts. Furthermore, the windows can also be differentiated into element-based windows, e.g., to consider the last ten elements, or time-based windows, e.g., to consider the last ten seconds of data. There are also different approaches to implementing windows. There are, for example, approaches that use timestamps or time intervals for system-wide windows or buffer-based windows for each single processing step. Sliding-window query processing is also suitable to being implemented in parallel processors by exploiting parallelism between different windows and/or within each window extent.<ref>{{cite journal|last1=De Matteis|first1=Tiziano|last2=Mencagli|first2=Gabriele|title=Parallel Patterns for Window-Based Stateful Operators on Data Streams: An Algorithmic Skeleton Approach|journal=International Journal of Parallel Programming|date=25 March 2016|doi=10.1007/s10766-016-0413-x}}</ref>\n\n== Query Processing ==\nSince there are a lot of prototypes, there is no standardized architecture. However, most DSMS are based on the [[Information retrieval|query]] processing in DBMS by using declarative languages to express queries, which are translated into a plan of operators. These plans can be optimized and executed. A query processing often consists of the following steps.\n\n=== Formulation of continuous queries ===\nThe formulation of queries is mostly done using declarative languages like [[SQL]] in DBMS. Since there are no standardized query languages to express continuous queries, there are a lot of languages and variations. However, most of them are based on [[SQL]], such as the [[Continuous Query Language]] (CQL), [[StreamSQL]] and [[Event stream processing|EPL]]. There are also graphical approaches where each processing step is a box and the processing flow is expressed by arrows between the boxes.\n\nThe language strongly depends on the processing model. For example, if windows are used for the processing, the definition of a window has to be expressed. In [[StreamSQL]], a query with a sliding window for the last 10 elements looks like follows:\n<source lang="sql">\nSELECT AVG(price) FROM examplestream [SIZE 10 ADVANCE 1 TUPLES] WHERE value > 100.0\n</source>\nThis stream continuously calculates the average value of "price" of the last 10 tuples, but only considers those tuples whose prices are greater than 100.0.\n\nIn the next step, the declarative query is translated into a logical query plan. A query plan is a directed graph where the nodes are operators and the edges describe the processing flow. Each operator in the query plan encapsulates the semantic of a specific operation, such as filtering or aggregation. In DSMSs that process relational data streams, the operators are equal or similar to the operators of the [[Relational algebra]], so that there are operators for selection, projection, join, and set operations. This operator concept allows the very flexible and versatile processing of a DSMS.\n\n=== Optimization of queries ===\nThe logical query plan can be optimized, which strongly depends on the streaming model. The basic concepts for optimizing continuous queries are equal to those from [[Query optimizer|database systems]]. If there are relational data streams and the logical query plan is based on relational operators from the [[Relational algebra]], a query optimizer can use the algebraic equivalences to optimize the plan. These may be, for example, to push selection operators down to the sources, because they are not so computationally intensive like join operators.\n\nFurthermore, there are also cost-based optimization techniques like in DBMS, where a query plan with the lowest costs is chosen from different equivalent query plans. One example is to choose the order of two successive join operators. In DBMS this decision is mostly done by certain statistics of the involved databases. But, since the data of a data streams is unknown in advance, there are no such statistics in a DSMS. However, it is possible to observe a data stream for a certain time to obtain some statistics. Using these statistics, the query can also be optimized later. So, in contrast to a DBMS, some DSMS allows to optimize the query even during runtime. Therefore, a DSMS needs some plan migration strategies to replace a running query plan with a new one.\n\n=== Transformation of queries ===\nSince a logical operator is only responsible for the semantics of an operation but does not consist of any algorithms, the logical query plan must be transformed into an executable counterpart. This is called a physical query plan. The distinction between a logical and a physical operator plan allows more than one implementation for the same logical operator. The join, for example, is logically the same, although it can be implemented by different algorithms like a [[Nested loop join]] or a [[Sort-merge join]]. Notice, these algorithms also strongly depend on the used stream and processing model.\nFinally, the query is available as a physical query plan.\n\n=== Execution of queries ===\nSince the physical query plan consists of executable algorithms, it can be directly executed. For this, the physical query plan is installed into the system. The bottom of the graph (of the query plan) is connected to the incoming sources, which can be everything like connectors to sensors. The top of the graph is connected to the outgoing sinks, which may be for example a visualization. Since most DSMSs are data-driven, a query is executed by pushing the incoming data elements from the source through the query plan to the sink. Each time when a data element passes an operator, the operator performs its specific operation on the data element and forwards the result to all successive operators.\n\n== Data Stream Management Systems ==\n* [http://www.sqlstream.com/stream-processing/ SQLstream]\n* [http://www-db.stanford.edu/stream STREAM] <ref name="StandfordStream">[http://ilpubs.stanford.edu:8090/641/ Arasu, A., et. al. \'\'STREAM: The Stanford Data Stream Management System.\'\'  Technical Report. 2004, Stanford InfoLab.]</ref>\n* [http://www.cs.brown.edu/research/aurora/ AURORA],<ref name="aurora">{{cite conference | author = Abadi | title = Aurora: A Data Stream Management System | conference = SIGMOD 2003 | citeseerx = 10.1.1.67.8671 |display-authors=etal}}</ref> [http://www.streambase.com/ StreamBase Systems, Inc.]\n* [http://telegraph.cs.berkeley.edu/telegraphcq/ TelegraphCQ] <ref name="telegraphcq">[http://www.cs.berkeley.edu/~franklin/Papers/TCQcidr03.pdf Chandrasekaran, S. et al, "TelegraphCQ: Continuous Dataflow Processing for an Uncertain World." CIDR 2003.]</ref>\n* [http://research.cs.wisc.edu/niagara/ NiagaraCQ],<ref name="niagaracq">[http://www.cs.wisc.edu/niagara/papers/NiagaraCQ.pdf Chen, J. et al, "NiagaraCQ: A Scalable Continuous Query System for Internet Databases." SIGMOD 2000.]</ref> \n* [http://wwwdb.inf.tu-dresden.de/research-projects/closed-projects/qstream/ QStream]\n* [http://dbs.mathematik.uni-marburg.de/Home/Research/Projects/PIPES PIPES], [http://www.softwareag.com/de/products/wm/events/overview/default.asp webMethods Business Events]\n* [http://www-db.in.tum.de/research/projects/StreamGlobe/index.shtml StreamGlobe]\n* [http://odysseus.informatik.uni-oldenburg.de/ Odysseus]\n* [http://www.microsoft.com/sqlserver/en/us/solutions-technologies/business-intelligence/streaming-data.aspx StreamInsight]\n* [http://www-01.ibm.com/software/data/infosphere/streams/ InfoSphere Streams]\n* [http://www.sas.com/en_us/software/data-management/event-stream-processing.html SAS Event Stream Processing Engine]\n* [http://go.sap.com/uk/product/data-mgmt/complex-event-processing.html  SAP Event Stream Processor]\n* [https://www.pipelinedb.com/ Pipeline DB]\n\n== See also ==\n* [[Complex Event Processing]]\n* [[Event stream processing]]\n* [[Relational data stream management system]]\n\n== References ==\n{{Reflist}}\n* {{Cite book\n|last=Aggarwal\n|first=Charu C.\n|authorlink=\n|year=2007\n|title=Data Streams: Models and Algorithms\n|publisher=Springer\n|location=New York\n|id=\n|isbn=978-0-387-47534-9\n}}\n* {{Cite book\n|first1=Lukasz\n|last1=Golab\n|first2=M. Tamer\n|last2=Özsu\n|authorlink=\n|year=2010\n|title=Data Stream Management\n|publisher=Morgan and Claypool\n|location=Waterloo, USA\n|id=\n|isbn=978-1-608-45272-9\n}}\n\n==External links==\n*[http://www.pam2004.org/papers/113.pdf Using Data Stream Management Systems for Traffic Analysis: A Case Study, last visited 2013-01-10]\n*[http://infolab.stanford.edu/stream/ STREAM: Stanford Stream Data Manager, last visited 2013-01-10]\n*[http://datalab.cs.pdx.edu/niagara/ NiagaraST: A Research Data Stream Management System at Portland State University, last visited 2013-01-10]\n*[http://odysseus.informatik.uni-oldenburg.de Odysseus: An open source Java based framework for Data Stream Management Systems, last visited 2013-01-10]\n*[http://home.dei.polimi.it/margara/papers/survey.pdf Processing Flows of Information: From Data Stream to Complex Event Processing] - Survey article on Data Stream and Complex Event Processing Systems, last visited 2013-01-10\n*[http://www.streambase.com/developers/docs/latest/streamsql/index.html StreamSQL reference, last visited 2013-01-10]\n*[https://web.archive.org/web/20140706215458/http://www.sqlstream.com/stream-processing-with-sql/ Stream processing with SQL] - Introduction to streaming data management with SQL\n\n[[Category:Data management]]']
['Semantic integration', '4381551', '{{Unreferenced|date=October 2013}}\n\'\'\'Semantic integration\'\'\' is the process of interrelating information from diverse sources, for example calendars and to do lists, email archives, presence information (physical, psychological, and social), documents of all sorts, contacts (including [[social graph]]s), search results, and advertising and marketing relevance derived from them. In this regard, [[semantics]] focuses on the organization of and action upon [[information]] by acting as an intermediary between heterogeneous data sources, which may conflict not only by structure but also context or value.\n\n==Applications and methods==\n\nIn [[enterprise application integration]] (EAI), semantic integration can facilitate or even automate the communication between computer systems using [[metadata publishing]]. Metadata publishing potentially offers the ability to automatically link [[ontology (computer science)|ontologies]]. One approach to (semi-)automated ontology mapping requires the definition of a semantic distance or its inverse, [[semantic similarity]] and appropriate rules. Other approaches include so-called \'\'lexical methods\'\', as well as methodologies that rely on exploiting the structures of the ontologies.  For explicitly stating similarity/equality, there exist special properties or relationships in most ontology languages. [[Web Ontology Language|OWL]], for example has "owl:equivalentClass", "owl:equivalentProperty" and "owl:sameAs".\n\nEventually system designs may see the advent of composable architectures where published semantic-based interfaces are joined together to enable new and meaningful capabilities{{Citation needed|date=February 2014}}. These could predominately be described by means of design-time declarative specifications, that could ultimately be rendered and executed at run-time{{Citation needed|date=February 2014}}.\n\nSemantic integration can also be used to facilitate design-time activities of interface design and mapping. In this model, semantics are only explicitly applied to design and the run-time systems work at the [[syntax]] level{{Citation needed|date=February 2014}}. This  "early semantic binding" approach can improve overall system performance while retaining the benefits of semantic driven design{{Citation needed|date=February 2014}}.\n\n==Examples==\n\nThe [[Pacific Symposium on Biocomputing]] has been a venue for the popularization of the ontology mapping task in the biomedical domain, and a number of papers on the subject can be found in its proceedings.\n\n==See also==\n* [[Data integration]]\n* [[Dataspaces]]\n* [[Enterprise integration]]\n* [[Ontology-based data integration]]\n* [[Ontology matching]]\n* [[Semantic heterogeneity]]\n* [[Semantic translation]]\n* [[Semantic unification]]\n\n== References ==\n{{Reflist|2}}\n\n==External links==\n*[https://web.archive.org/web/20070811204850/http://zapthink.com/report.html?id=ZapFlash-08082003 Semantic Integration: Loosely Coupling the Meaning of Data]\n*[http://drops.dagstuhl.de/opus/volltexte/2005/40/ Ontology Mapping: The State of the Art] (2005 paper)\n*[http://arxiv.org/ftp/arxiv/papers/0901/0901.4934.pdf 2010 paper by Carl Hewitt]\n*[http://wwwhome.portavita.nl/~yeb/ooi.pdf OpenCyc to Oracle Interface]\n\n{{Semantic Web}}\n\n{{DEFAULTSORT:Semantic Integration}}\n[[Category:Ontology (information science)]]\n[[Category:Data management]]\n[[Category:Semantics]]\n[[Category:Bioinformatics]]']
['Signed overpunch', '16284001', '{{Refimprove|date=March 2008}}\nA \'\'\'signed overpunch\'\'\' is a code used to store the [[Sign (mathematics)|sign]] of a number by changing the last digit. It is used in [[COBOL]], especially when using [[EBCDIC]]. Its purpose is to save a character that would otherwise be used by the sign digit.<ref name="EncycArk">{{Cite web|url=http://www.3480-3590-data-conversion.com/article-signed-fields.html |title=Tech Talk, COBOL Tutorials, EBCDIC to ASCII Conversion of Signed Fields |accessdate=2008-03-15}}</ref>  The code is derived from the [[punched card#IBM 80-column punched card formats and character codes|Hollerith Punched Card Code]], where both a digit and a sign can be entered in the same card column.\n\n==The codes==\n{| class="wikitable" style="text-align:center"\n! Code !! Digit !! Sign\n|-\n| } || 0 || &minus;\n|-\n| J || 1 || &minus;\n|-\n| K || 2 || &minus;\n|-\n| L || 3 || &minus;\n|-\n| M || 4 || &minus;\n|-\n| N || 5 || &minus;\n|-\n| O || 6 || &minus;\n|-\n| P || 7 || &minus;\n|-\n| Q || 8 || &minus;\n|-\n| R || 9 || &minus;\n|-\n| { || 0 || +\n|-\n| A || 1 || +\n|-\n| B || 2 || +\n|-\n| C || 3 || +\n|-\n| D || 4 || +\n|-\n| E || 5 || +\n|-\n| F || 6 || +\n|-\n| G || 7 || +\n|-\n| H || 8 || +\n|-\n| I || 9 || +\n|}\n\n==Examples==\n10} is -100<BR>\n45A is 451\n\nDecimal points are usually implied and not explicitly stated in the text. Using numbers with two decimal digits:\n\n1000} is -100.00\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Signed Overpunch}}\n[[Category:Computer programming]]\n[[Category:Punched card]]\n[[Category:Data management]]\n[[Category:History of software]]']
['Learning object', '18126', 'A \'\'\'learning object\'\'\' is "a collection of content items, practice items, and assessment items that are combined based on a single learning objective".<ref>{{citation|last=Cisco Systems|title=Reusable information object strategy|url=http://www.cisco.com/warp/public/779/ibs/solutions/learning/whitepapers/el_cisco_rio.pdf}}</ref>  The term is credited to Wayne Hodgins, and dates from a working group in 1994 bearing the name.<ref>{{citation|last=Gerard|first=R.W.|title="Shaping the mind: Computers in education", In N. A. Sciences, Applied Science and Technological Progress|url=https://books.google.com/books?id=BTcrAAAAYAAJ|year=1967}}</ref> The concept encompassed by \'Learning Objects\' is known by numerous other terms, including: content objects, chunks, educational objects, information objects, intelligent objects, knowledge bits, knowledge objects, learning components, media objects, reusable curriculum components, nuggets, reusable information objects, reusable learning objects, testable reusable units of cognition, training components, and units of learning.\n\nThe core idea of the use of learning objects is characterized by the following: discoverability, reusability, and interoperability. To support discoverability, learning objects are described by Learning Object Metadata, formalized as IEEE 1484.12 [[Learning object metadata]].<ref>[http://129.115.100.158/txlor/docs/IEEE_LOM_1484_12_1_v1_Final_Draft.pdf IEEE 1484.12 Learning Object Metadata]</ref> To support reusability, the IMS Consortium proposed a series of specifications such as the IMS [[Content package]]. And to support interoperability, the U.S. military\'s [[Advanced Distributed Learning]] organization created the [[Sharable Content Object Reference Model]].<ref>[http://legacy.adlnet.gov/Technologies/scorm/SCORMSDocuments/2004%204th%20Edition/Overview.aspx SCORM 2004 4th Edition Version 1.1 Overview]</ref> Learning objects were designed in order to reduce the cost of learning, standardize learning content, and to enable the use and reuse of learning content by learning management systems.<ref>[http://www.irrodl.org/index.php/irrodl/article/view/32/378|Stephen Downes|Learning Objects: Resources For Distance Education Worldwide|The International Review of Research in Open and Distributed Learning|Volume 2 Number 1 2001|Athabasca University Press]</ref>\n\n==Definitions==\nThe [[Institute of Electrical and Electronics Engineers]] (IEEE) defines a learning object as "any entity, digital or non-digital, that may be used for learning, education or training".<ref>{{Harvnb|Learning Technology Standards Committee|2002|p=45}}</ref>\n\nChiappe defined Learning Objects as: "A digital self-contained and reusable entity, with a clear educational purpose, with at least three internal and editable components: content, learning activities and elements of context. The learning objects must have an external structure of information to facilitate their identification, storage and retrieval: the metadata."<ref>{{Harvnb|Chiappe|Segovia|Rincon|2007|p=8}}.</ref>\n\nThe following definitions focus on the relation between learning object and digital media.  RLO-CETL, a British inter-university Learning Objects Center, defines "reusable learning objects" as "web-based interactive chunks of e-learning designed to explain a stand-alone learning objective".<ref>{{citation|chapter= Learning Objects|url=http://www.rlo-cetl.ac.uk/joomla/index.php?option=com_content&task=view&id=235&Itemid=28|title=RLO-CETL: Reusable Learning Objects|accessdate=2008-04-29}}.</ref> Daniel Rehak and Robin Mason define it as "a digitized entity which can be used, reused or referenced during technology supported learning".<ref>http://129.115.100.158/txlor/docs/IEEE_LOM_1484_12_1_v1_Final_Draft.pdf</ref><ref>{{Harvnb|Rehak|Mason|2003|p=}}</ref>\n\nAdapting a definition from the Wisconsin Online Resource Center, Robert J. Beck suggests that learning objects have the following key characteristics:\n\n* Learning objects are a new way of thinking about learning content. Traditionally, content comes in a several hour chunk.  Learning objects are much smaller units of learning, typically ranging from 2 minutes to 15 minutes.\n* Are self-contained – each learning object can be taken independently\n* Are reusable – a single learning object may be used in multiple contexts for multiple purposes\n* Can be aggregated – learning objects can be grouped into larger collections of content, including traditional course structures\n* Are tagged with metadata – every learning object has descriptive information allowing it to be easily found by a search<ref name="beck">{{citation|last=Beck|first=Robert J.|chapter=What Are Learning Objects?|url=http://www4.uwm.edu/cie/learning_objects.cfm?gid=56 | title=Learning Objects|publisher=Center for International Education, University of Wisconsin-Milwaukee|accessdate=2008-04-29}}</ref>\n\n== Components ==\nThe following is a list of some of the types of information that may be included in a learning object and its metadata:\n* General Course Descriptive Data, including: course identifiers, language of content (English, Spanish, etc.), subject area (Maths, Reading, etc.), descriptive text, descriptive keywords\n* Life Cycle, including: version, status\n* Instructional Content, including: text, web pages, images, sound, video\n* Glossary of Terms, including: terms, definition, acronyms\n* Quizzes and Assessments, including: questions, answers\n* Rights, including: cost, copyrights, restrictions on Use\n* Relationships to Other Courses, including prerequisite courses\n* Educational Level, including: grade level, age range, typical learning time, and difficulty. [IEEE 1484.12.1:2002]\n*Typology as defined by Churchill (2007): presentation, practice, simulation, conceptual models, information, and contextual representation <ref name="Churchill">Churchill, D. (2007). Towards a useful classification of learning objects. \'\'Educational Technology Research & Development, 55(5)\'\', 479-497.</ref>\n\n==Metadata==\nOne of the key issues in using learning objects is their identification by search engines or content management systems.{{Citation needed|date=April 2008}}  This is usually facilitated by assigning descriptive [[learning object metadata]]. Just as a book in a library has a record in the [[card catalog]], learning objects must also be tagged with metadata.  The most important pieces of metadata typically associated with a learning object include:\n# \'\'\'objective:\'\'\' The educational objective the learning object is instructing\n# \'\'\'prerequisites:\'\'\' The list of skills (typically represented as objectives) which the learner must know before viewing the learning object\n# \'\'\'topic:\'\'\' Typically represented in a taxonomy, the topic the learning object is instructing\n# \'\'\'interactivity:\'\'\' The [[Interaction Model]] of the learning object.\n# \'\'\'technology requirements:\'\'\' The required system requirements to view the learning object.\n\n==Mutability==\n\nA mutated learning object is, according to Michael Shaw, a learning object that has been "re-purposed and/or re-engineered, changed or simply re-used in some way different from its original intended design". Shaw also introduces the term "contextual learning object", to describe a learning object that has been "designed to have specific meaning and purpose to an intended learner".<ref>{{Harvnb|Shaw|2003}}</ref>\n\n==Portability==\nBefore any institution invests a great deal of time and energy into building high-quality e-learning content (which can cost over $10,000 per classroom hour),<ref>Rumble, Greville. 2001. The Cost and Costing of Networked Learning. Journal of Asynchronous Learning Networks, Volume 5, Issue 2.</ref> it needs to consider how this content can be easily loaded into a [[Learning Management System]]. It is possible for example, to package learning objects with [[SCORM]] specification and load it in [[Moodle]] Learning Management System or [[Desire2Learn]] Learning Environment.\n\nIf all of the properties of a course can be precisely defined in a common format, the content can be serialized into a standard format such as [[XML]] and loaded into other systems.  When it is considered that some e-learning courses need to include video, mathematical equations using [[MathML]], chemistry equations using [[Chemical Markup Language|CML]] and other complex structures, the issues become very complex, especially if the systems needs to understand and validate each structure and then place it correctly in a database.{{Citation needed|date=April 2008}}\n\n==Criticism==\nIn 2001, David Wiley criticized learning object theory in his paper, [https://web.archive.org/web/20041019162710/http:/rclt.usu.edu/whitepapers/paradox.html The Reusability Paradox] which is [http://www.darcynorman.net/2003/08/21/addressing-the-reusability-paradox/ summarized by D\'Arcy Norman] as, \'\'If a learning object is useful in a particular context, by definition it is not reusable in a different context. If a learning object is reusable in many contexts, it isn’t particularly useful in any.\'\' \nIn [http://www.learningspaces.org/papers/objections.html Three Objections to Learning Objects and E-learning Standards], Norm Friesen, Canada Research Chair in E-Learning Practices at Thompson Rivers University, points out that the word \'\'neutrality\'\' in itself implies \'\'a state or position that is antithetical ... to pedagogy and teaching.\'\'\n\n== See also ==\n* [[Intelligent tutoring system]]\n* [[North Carolina Learning Object Repository (NCLOR)]]\n* [[Serious games]]\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n*{{citation|last=Beck|first=Robert J.|title="What Are Learning Objects?", Learning Objects, Center for International Education, University of Wisconsin-Milwaukee, |url= http://www4.uwm.edu/cie/learning_objects.cfm?gid=56 |year= 2009|accessdate= 2009-10-23}}.\n*{{citation|last=Learning Technology Standards Committee|title=Draft Standard for Learning Object Metadata. IEEE Standard 1484.12.1|place=New York|publisher=Institute of Electrical and Electronics Engineers|year=2002|url=http://ltsc.ieee.org/wg12/files/LOM_1484_12_1_v1_Final_Draft.pdf| format=PDF| accessdate=2008-04-29}}.\n*{{citation|last1=Rehak |first1=Daniel R.|first2=Robin |last2=Mason|chapter=Engaging with the Learning Object Economy|editor-first=Allison|editor-last=Littlejohn|title=Reusing Online Resources: A Sustainable Approach to E-Learning|place= London|publisher= Kogan Page| year=2003| pages=22–30| isbn=978-0-7494-3949-1}}.\n*{{citation|last=Shaw|first=Michael|chapter=(Contextual and Mutated) Learning Objects in the Context of Design, Learning and (Re)Use| url=http://www.shawmultimedia.com/edtech_oct_03.html|title=Teaching and Learning with Technology|date=October 2003|accessdate=2008-04-29}}\n*{{citation|first1=Andrés Chiappe |last1=Laverde |first2=Yasbley Segovia |last2=Cifuentes |first3=Helda Yadira Rincón |last3=Rodríguez|chapter=Toward an instructional design model based on learning objects|editor-first=Springer|editor-last=Boston|title=Educational Technology Research and Development |place=Boston|year= 2007|pages=671–81|publisher=Springer US |doi=10.1007/s11423-007-9059-0 |issue=6 |volume=55 |issn=1042-1629|id=(Print) {{ISSN|1556-6501}} (Online) |url= http://www.springerlink.com/content/u84w63873vq77h2h/?p=41be7fbeef9648ee9b554f1835112005&pi=6|accessdate=2008-08-21}} Spanish Draf: [http://andreschiappe.blogspot.com/2007/09/que-es-un-objeto-de-aprendizaje-what-is.html \'\'Blog de Andrés Chiappe - Objetos de Aprendizaje\'\'].\n*{{citation|last=Northrup|first=Pamela|title=Learning Objects for Instruction: Design and Evaluation |place=USA|publisher=Information Science Publishing |year=2007| format=Book}}.\n*{{citation|last1=Hunt|first1=John P.|last2=Bernard|first2=Robert|title="An XML-based information architecture for learning content", IBM developerWorks, |url= http://www.ibm.com/developerworks/xml/library/x-dita9a |year= 2005|accessdate= 2005-08-05}}.\n*Churchill, D. (2007). Towards a useful classification of learning objects.  \'\'Educational Technology Research & Development, 55(5)\'\', 479-497.\nInnayah: Creating An Audio Script with Learning Object, unpublished, 2013.\n\n==External links==\n* The [http://www4.uwm.edu/cie/learning_objects.cfm?gid=55 Learning Objects] at Milwaukee\'s Center for International Education.\n\n{{DEFAULTSORT:Learning Object}}\n[[Category:Data management]]\n[[Category:Educational materials]]\n[[Category:Educational technology]]']
['Change data capture', '3557729', '{{No footnotes|date=March 2016}}\nIn [[database]]s, \'\'\'change data capture\'\'\' (CDC) is a set of software [[Design pattern (computer science)|design patterns]] used to determine (and track) the data that has changed so that action can be taken using the changed data. Also, Change data capture (CDC) is an approach to data integration that is based on the identification, capture and delivery of the changes made to enterprise data sources.\n\nCDC solutions occur most often in [[data warehouse|data-warehouse]] environments since capturing and preserving the state of data across time is one of the core functions of a data warehouse, but CDC can be utilized in any database or data repository system.\n\n== Methodology ==\n\nSystem developers can set up CDC mechanisms in a number of ways and in any one or a combination of system layers from application logic down to physical storage.\n\nIn a simplified CDC context, one computer system has data believed to have changed from a previous point in time, and a second computer system needs to take action based on that changed data.  The former is the source, the latter is the target.  It is possible that the source and target are the same system physically, but that would not change the design pattern logically.\n\nNot uncommonly, multiple CDC solutions can exist in a single system.\n\n=== Timestamps on rows ===\nTables whose changes must be captured may have a column that represents the time of \'\'\'last\'\'\' change.  Names such as LAST_UPDATE, etc. are common.  Any row in any table that has a timestamp in that column that is more recent than the last time data was captured is considered to have changed.\n\n=== Version Numbers on rows ===\nDatabase designers give tables whose changes must be captured a column that contains a version number.  Names such as VERSION_NUMBER, etc. are common.  When data in a row changes, its version number is updated to the current version.  A supporting construct such as a reference table with the current version in it is needed.  When a change capture occurs, all data with the latest version number is considered to have changed.  When the change capture is complete, the reference table is updated with a new version number.\n\nThree or four major techniques exist for doing CDC with version numbers, the above paragraph is just one.\n\n==== Use in Optimistic Locking ====\nVersion numbers can be useful with [[optimistic locking]] in ACID transactional or [[Relational database management system|relational database management systems (RDMBS)]]. For an example in read-then-update scenarios for [[Create, read, update and delete|CRUD]] applications in [[relational database management system]]s, a row is first read along with the state of its version number; in a separate transaction, a [[Update (SQL)|SQL UPDATE]] statement is executed along with an additional [[Where (SQL)|WHERE clause]] that includes the version number found from the initial read. If no record was updated, it usually means that the version numbers didn\'t match because some other action/transaction had already updated the row and consequently its version number. Several [[Object relational mapper|object relational mapping]] tools use this method to detect for optimistic locking scenarios (including [[Hibernate (Java)|Hibernate]]).\n\n=== Status indicators on rows ===\nThis technique can either supplement or complement timestamps and versioning. It can configure an alternative if, for example, a status column is set up on a table row indicating that the row has changed (e.g. a boolean column that, when set to true, indicates that the row has changed). Otherwise, it can act as a complement to the previous methods, indicating that a row, despite having a new version number or an earlier date, still shouldn\'t be updated on the target (for example, the data may require human validation).\n\n=== Time/Version/Status on rows ===\nThis approach combines the three previously discussed methods.  As noted, it is not uncommon to see multiple CDC solutions at work in a single system, however, the combination of time, version, and status provides a particularly powerful mechanism and programmers should utilize them as a trio where possible.  The three elements are not redundant or superfluous.  Using them together allows for such logic as, "Capture all data for version 2.1 that changed between 6/1/2005 12:00 a.m. and 7/1/2005 12:00 a.m. where the status code indicates it is ready for production."\n\n=== Triggers on tables ===\nMay include a [[observer pattern|publish/subscribe]] pattern to communicate the changed data to multiple targets.  In this approach, [[Database trigger|triggers]] log events that happen to the transactional table into another queue table that can later be "played back".  For example, imagine an Accounts table, when transactions are taken against this table, triggers would fire that would then store a history of the event or even the deltas into a separate queue table.  The queue table might have schema with the following fields: Id, TableName, RowId, TimeStamp, Operation.  The data inserted for our Account sample might be:  1, Accounts, 76, 11/02/2008 12:15am, Update.\nMore complicated designs might log the actual data that changed.  This queue table could then be "played back" to replicate the data from the source system to a target.\n\n[More discussion needed]\n\nAn example of this technique is the pattern known as the [[log trigger]].\n\n=== Event Programming ===\nCoding a change into an application at appropriate points is another method that can give  intelligent discernment that data changed.  Although this method involves programming vs. more easily implemented "dumb" triggers, it may provide more accurate and desirable CDC, such as only after a COMMIT, or only after certain columns changed to certain values - just what the target system is looking for.\n\n=== Log scanners on databases ===\nMost database management systems manage a [[transaction log]] that records changes made to the database contents and to [[metadata]]. By scanning and interpreting the contents of the database transaction log one can capture the changes made to the database in a non-intrusive manner.\n\nUsing transaction logs for change data capture offers a challenge in that the structure, contents and use of a transaction log is specific to a database management system. Unlike data access, no standard exists for transaction logs. Most database management systems do not document the internal format of their transaction logs, although some provide programmatic interfaces to their transaction logs (for example: Oracle, DB2, SQL/MP, SQL/MX and SQL Server 2008).\n\nOther challenges in using transaction logs for change data capture include:\n\n* Coordinating the reading of the transaction logs and the archiving of log files (database management software typically archives log files off-line on a regular basis).\n* Translation between physical storage formats that are recorded in the transaction logs and the logical formats typically expected by database users (e.g., some transaction logs save only minimal buffer differences that are not directly useful for change consumers).\n* Dealing with changes to the format of the transaction logs between versions of the database management system.\n* Eliminating uncommitted changes that the database wrote to the transaction log and later [[Rollback (data management)|rolled back]].\n* Dealing with changes to the metadata of tables in the database.\n\nCDC solutions based on transaction log files have distinct advantages that include:\n\n* minimal impact on the database (even more so if one uses [[log shipping]] to process the logs on a dedicated host).\n* no need for programmatic changes to the applications that use the database.\n* low [[Latency (engineering)|latency]] in acquiring changes.\n* [[data integrity|transactional integrity]]: log scanning can produce a change stream that replays the original transactions in the order they were committed. Such a change stream include changes made to all tables participating in the captured transaction.\n* no need to change the database schema\n\n== Confounding factors ==\nAs often occurs in complex domains, the final solution to a CDC problem  may have to balance many competing concerns.\n\n=== Unsuitable source systems ===\n\nChange data capture both increases in complexity and reduces in value if the source system saves [[metadata]] changes when the data itself is not modified.  For example, some [[Data model]]s track the user who last looked at but did not change the data in the same structure as the data.  This results in [[Noise (signal processing)|noise]] in the Change Data Capture.\n\n=== Tracking the capture ===\n\nActually tracking the changes depends on the data source.  If the data is being persisted in a modern [[database]] then Change Data Capture is a simple matter of permissions.  Two techniques are in common use:\n# Tracking changes using [[Database Trigger]]s\n# Reading the [[transaction log]] as, or shortly after, it is written.\n\nIf the data is not in a modern database, Change Data Capture becomes a programming challenge.\n\n=== Push versus pull ===\n* \'\'\'Push\'\'\': the source process creates a snapshot of changes within its own process and delivers rows downstream. The downstream process uses the snapshot, creates its own subset and delivers them to the next process.\n* \'\'\'Pull\'\'\': the target that is immediately downstream from the source, prepares a request for data from the source. The downstream target delivers the snapshot to the next target, as in the push model.\n\n=== Alternatives ===\n\nSometimes the [[Slowly changing dimension]] is used as a method, this is an example:\n[[File:Scd model.png|frame|center|Scd model]]\n\n==See also==\n\n* [[Slowly Changing Dimension]]\n* [[Referential integrity]]\n\n== References ==\n{{reflist}}\n\n==External links==\n* [https://github.com/linkedin/databus/wiki LinkedIn Databus]\n* [http://www.informaticacloud.com/images/whitepapers/data%20replication%20best%20practices.pdf Data Replication as a Service Best Practices]\n* [https://web.archive.org/web/20110902084451/http://www.attunity.com:80/attunity_stream Attunity Change Data Capture (CDC)]\n* [http://www-01.ibm.com/software/data/infosphere/change-data-capture/  IBM Infosphere CDC]\n* [https://web.archive.org/web/20060523023144/http://www.oracle.com:80/technology/oramag/oracle/03-nov/o63tech_bi.html Tutorial on setting up CDC in Oracle 9i]\n* [http://social.technet.microsoft.com/wiki/contents/articles/how-to-enable-sql-azure-change-data-capture.aspx Tutorial on setting up SQL Azure Change Data Capture]\n* [http://msdn2.microsoft.com/en-us/library/bb522489(SQL.100).aspx Details of the CDC facility included in Microsoft Sql Server 2008 Feb \'08 CTP]\n* [http://www.jumpmind.com/products/symmetricds/features SymmetricDS - Heterogeneous, Cross Platform CDC]\n* [http://www.gamma-soft.com/wp/index.php?page_id=30 Gamma-Soft CDC Technology]\n* [http://www.talend.com/download/talend-open-studio?qt-product_tos_download=3#qt-product_tos_download Talend]\n\n{{DEFAULTSORT:Change Data Capture}}\n[[Category:Computer data]]\n[[Category:Data management]]']
['Operational historian', '8829912', '{{Refimprove|date=October 2013}}\n{{Use British English|date=March 2014}}\n\'\'\'Operational historian\'\'\' refers to a database software application that logs or historizes time-based process data.<ref>{{cite web|title=A Practical Guide to Process Data Historians and Process Information Systems|url=http://www.tappi.org/Downloads/unsorted/UNTITLED---PCEI99339pdf.aspx|publisher=TAPPI|accessdate=14 September 2012|author=R. H. (Rick) Meeker, Jr.|format=PDF|date=13 January 1999}}</ref> Historian software is used to record trends and historical information about industrial processes for future reference. It captures plant management information about production status, performance monitoring, quality assurance, tracking and genealogy, and product delivery with enhanced data capture, data compression, and data presentation capabilities.<ref name=GlobalspecArticle  >{{cite web |url= http://www.globalspec.com/learnmore/industrial_engineering_software/industrial_controls_software/trending_historian_software |title= Globalspec Historian Article| accessdate=12 Jul 2012}}</ref>\n\nOperational historians are like enterprise historians but differ in that they are used by engineers on the plant floor rather than by business processes.{{citation needed|date=February 2015}} They are typically cheaper, lighter in weight, and easier to use and reconfigure than enterprise historians. Having an operational historian enables "at the source" analysis of the historical data that is not typically possible with enterprise historians.\nTypically, these applications offer two layers of data access: through a dedicated SDK(Standard Development Kit)(sometimes in two different flavours: full administration API(Application Programming Interface) and high-speed read/write API), as well as user front-end tools (for instance, administration panels, engineering consoles or portal-like web clients).\n\nBecause these applications are designed to fulfil specific operation time requirements, their marketing materials often indicate that these are real-time database systems.<ref name="example">[http://software.schneider-electric.com/products/wonderware/production-information-management/historian/ Wonderware Historian - Example of naming the operational historian the real-time database]</ref> However, since such performance measurements are often executed for atomic operations (especially write operations), not necessarily whole transactions, not all of the operational historians must be in fact real-time databases.\n\nUsual challenges the operational historians must address are as follows:\n* data collection from real-time external systems,\n* storage and archiving of very large volumes of data,\n* tag organisation (typically [[Time series database|time series]], where a single sample contains the information about the time stamp, the value and the sample quality),\n* basic data limit monitoring (alarms) and user prompts (messages),\n* performance of read and write operations.\n\n== Data access ==\nAs opposed to enterprise historians, the data access layer in the operational historian is designed to offer sophisticated data fetching modes without complex information analysis facilities. The following settings are typically available for data access operations:\n* Data scope (single point, history based on time range, history based on sample count),\n* Request modes (raw data, last-known value, aggregation, interpolation),\n* Sampling (single point, all points without sampling, all points with interval sampling),\n* Data omission (based on the sample quality, based on the sample value, based on the count).\n\nEven though the operational historians are rarely [[relational database management system]]s, they often offer [[SQL]]-based interfaces to query the database. In most of such implementations, the dialect does not follow the SQL standard in order to provide syntax for specifying data access operations parameters.\n\n== Notable software ==\n=== Commercial ===\n* [[ABB]] Decathlon History\n* [[Aspen Technology]] InfoPlus.21 <ref>{{Citation | url = http://www.aspentech.com/aspenONE_MES_Brochure.pdf | format = PDF | publisher = Aspen Technology, Inc. | title = aspenONE MES Brochure | page = 2 | year = 2014 | accessdate = 30 July 2014 }}</ref>\n* [[ENEA_AB|Enea]]\'s Polyhedra Historian, a module of [[Polyhedra DBMS]]<ref>{{Citation | url = http://developer.polyhedra.com/polyhedra-features/historian | publisher = Enea AB |  title = Handling time-series data in Polyhedra IMDB | date = 11 May 2012 | accessdate = 30 July 2014 }}</ref>\n* [[GE Intelligent Platforms]] Proficy Historian<ref>{{Citation | url = http://www.ge-ip.com/account/prepsend/file/Proficy_Historian_5-5.pdf | format = PDF | publisher = GE Intelligent Platforms, Inc. | title = Datasheet: Proficy Historian 5.5 | year = 2013 | accessdate = 30 July 2014 }}</ref>\n* [[Honeywell]] Uniformance PHD<ref>{{Citation | url = http://www.honeywellprocess.com/library/marketing/notes/uniformance-phd-pin.pdf | publisher = Honeywell International Inc. | title = Uniformance PHD Product Information Note | format = PDF | year = 2013 | accessdate = 30 July 2014 }}</ref>\n* [[Iconics ]] [http://www.iconics.com/Home/Products/Historians/Hyper-Historian.aspx Hyper Historian]\n* [[Inductive Automation]] [[Ignition SCADA#SQL Bridge| SQL Bridge]] module of [[Ignition SCADA]]<ref>{{Citation | url = http://inductiveautomation.com/scada-software/scada-modules/sqlbridge | publisher = Inductive Automation | title = High-Powered Data Acquisition | year = 2014 | accessdate = 30 July 2014 }}</ref>\n* [[National Instruments]] Citadel, used in [[LabVIEW]] DSC and other products<ref>{{Citation | url = http://www.ni.com/white-paper/6579/en/ | publisher = National Instruments Corp. |  title = Logging Data with National Instruments Citadel | date = 19 July 2012 | accessdate = 30 July 2014 }}</ref>\n* [[OSIsoft]] - PI System\n* [[Schneider Electric]] InStep Software [http://www.instepsoftware.com/instep-software-products/edna-enterprise-data-historian eDNA Real-Time Historian]\n* [[Schneider Electric]] Wonderware Historian<ref>{{Citation | url =  http://global.wonderware.com/EN/PDF%20Library/Datasheet_Wonderware_Historian.pdf | publisher = Invensys Systems, Inc. | title = Wonderware Historian Software Datasheet | format = PDF | year = 2013 | accessdate = 30 July 2014 }}</ref>\n* [[Yokogawa]] Exaquantum Historian<ref>{{Citation | url = http://www.yokogawa.com/eu/pims/pdf/BU%20GMSCS0102-02E%20Exaquantum%20Bulletin%2072ppi.pdf | publisher = Yokogawa Marex Limited |  title = Exaquantum delivers Production Excellence | format = PDF | year = 2013 | accessdate = 30 July 2014 }}</ref>\n* [[Jaaji Technologies]] inSis Historian<ref>{{Citation | url = http://www.jaajitech.com/Infoview | publisher = Jaaji Software Technologies Private Limited |  title = Find, View and Analyze your process data from everywhere | year = 2014 | accessdate = 30 July 2014 }}</ref>\n\n==See also==\n* [[Time series database]]\n* [[Relational database management system]]\n\n== References ==\n{{reflist}}\n\n[[Category:Data management]]']
['Storage model', '8288646', "{{Unreferenced|date=December 2006}}\nA '''storage model''' is a model that captures key ''physical'' aspects of data structure in a data store. \n\nOn the other hand, a [[data model]] is a model that captures key ''logical'' aspects of data structure in a database.\n\n\n\n[[Category:Data management]]\n\n\n{{Compu-storage-stub}}"]
['Category:Data warehousing', '5611387', '[[Category:Information technology management]]\n[[Category:Business intelligence]]\n[[Category:Data management|Warehousing]]']
['Social information architecture', '31377324', "{{orphan|date=April 2011}}\n'''Social information architecture''' is a sub-domain of [[information architecture]] which deals with the social aspects of conceptualizing, modeling and organizing information. Social Information Architecture, also known as Social iA <ref>[http://sweetinformationarchitecture.net/social-information-architecture/%20 Sweet Information Architecture]</ref> has become more relevant because of the rise of [[Social Media]] and [[Web 2.0]] in recent times.\n\n== Approach ==\nThere are different approaches to the explanation of Social iA. \n\n===A) The architecture model (internal space)===\n\nArchitects designing a physical community space, have to consider how the architecture will shape social interactions. A long hallway of offices creates an utterly different dynamic than desks with arranged in an open space. One might foster individuality, privacy, propriety; the other: collaboration, distraction, communalism.\n\nStill, physical spaces can be flexibly repurposed and worked around if the inhabitants desire a social dynamic not instantly afforded by the space. Office doors can be left open to invite easier interaction. Partitions can be raised between adjacent desks to limit distraction and increase privacy.\n\nThat’s physical architecture. The information architectures of online communities are far more deterministic and far less flexible. They literally define the social architecture by pre-specifying in immutable computer code what information you have access to, who you can talk to, where you can go. In the online world, information architecture = social architecture.<ref>http://www.steinbock.org/</ref>\n\n===B) The social dialogue and information model (external space)===\n\nAll  major brands use information architecture to market their products online, it is then commonly wrapped under the umbrella phrase 'digital strategy'. Information architecture used for strategic purposes encompasses brand [[SEO]], strategic placement of virals, social media presence etc. \n\nCharities, news outlets and social dialogue forums can make a much more specific use of the same tools for positive and  important social purposes. Social Information Architecture is perceived as the socially conscious wing of  commercial information architecture <ref>http://www.sweetinformationarchitecture.net</ref> and function to exchange information and ideas between people and groups. \n\nSocial iA can pick up on conflicting issues that are treated with misunderstanding between  cultures and leaves individuals and societies vulnerable to exploitation and manipulation. Since the net has such a far reach it is obvious to use it for meaningful and coordinated social dialogue. \n\nExample of such issues are faith, environment,  politics, climate change, war, injustice and other social challenges. Information architecture can  help create frameworks in which sharing information brings people together, inspires and encourages them to participate in a forward thinking and unfragmented way. One of its core activities is to spread messages that bring people from opposite sites of social  and cultural spectrums together and to confront uncomfortable subject head on.\n\n== How does social information architecture work? ==\nSocial iA utilizes a variety of [[Web2.0]] applications to filter relevant or valuable information and weave them in appropriate information repository or provide feedback to interesting channels. Social iA makes strategic use of Search Engines, Social Media, Google Algorithms, as well as websites, video & news channels. It ‘reads’ or 'listens' to social conversations and [[search engine]] queries and engages with the net actively to gather clues about the world’s pulse on the internet. It assesses data, social & political trends, and respond with targeted campaigns to give people ideas, as well as help people with making sense of information.\n\n== Principals ==\nDan Brown in his paper 8 Principals of Social Information Architecture <ref>[http://socialinformationarchitecture.org.uk/paper/8principal_infoarchi.pdf Eight Principles of Information Architecture], Dan Brown. Published in the Bulletin of the American Society for Information Science and Technology – August/September 2010 – Volume 36, Number 6</ref> enlists the following principals:<br />\n1. The principle of objects: Treat content as a living, breathing thing,\nwith a lifecycle, behaviors and attributes.<br />\n2. The principle of choices: Create pages that offer meaningful choices to users, keeping the range of choices available focused on a particular task. <br />\n3. The principle of disclosure: Show only enough information to help\npeople understand what kinds of information they’ll find as they dig\ndeeper. <br />\n4. The principle of exemplars: Describe the contents of categories by\nshowing examples of the contents.<br />\n5. The principle of front doors: Assume at least half of the website’s\nvisitors will come through some page other than the home page.<br />\n6. The principle of multiple classification: Offer users several different classification schemes to browse the site’s content.<br />\n7. The principle of focused navigation: Don’t mix apples and oranges\nin your navigation scheme.<br />\n8. The principle of growth: Assume the content you have today is a\nsmall fraction of the content you will have tomorrow.\n\n== What can social information architecture achieve? ==\nSocial information architecture has many potentials in terms of fostering social connections and how information is shared in social spaces on the web.\n\n== References==\n{{Reflist}}\n\n== See also ==\nWodtke, Christina and Govella, Austin '''Information Architecture: Blueprints for the Web''' (2009) Second Edition, Published by New Riders\n\n{{Semantic Web}}\n\n[[Category:Information architects|*Information architecture]]\n[[Category:World Wide Web]]\n[[Category:Data management]]\n[[Category:Information science]]\n[[Category:Information technology]]\n[[Category:Digital technology]]\n[[Category:New media]]"]
['Australian National Data Service', '31960097', "The '''Australian National Data Service''' (ANDS) was established in 2008 to help address the challenges of storing and managing Australia's research data, and making it discoverable and accessible for validation and reuse. It is a joint collaboration between [[Monash University]], [[The Australian National University]] and [[CSIRO]].\n\n==Background==\nANDS is funded by the [[Australian Department of Education]]. The funding has been provided through Australian Government's National Collaborative Research Infrastructure Strategy (NCRIS) as part of the Platforms for Collaboration Investment Plan.<ref>{{cite web|url=http://ncris.innovation.gov.au/Capabilities/Pages/PfC.aspx#ANDS |title=Platforms for Collaboration |accessdate=2011-06-02 |deadurl=yes |archiveurl=https://web.archive.org/web/20110702094824/http://ncris.innovation.gov.au:80/Capabilities/Pages/PfC.aspx |archivedate=2011-07-02 |df= }}</ref> The NCRIS roadmap emphasized the vital importance of eResearch Infrastructure to Australian future research competitiveness.<ref>{{cite web|title=eResearch Infrastructure |url=http://www.pfc.org.au/bin/view/Main |publisher=NCRIS |accessdate=25 June 2011 }}{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> In mid-2009 ANDS was further funded by the Education Investment Fund (EIF) for the establishment of the Australian Research Data Commons under the Australian Government’s Super Science Initiative.<ref>{{cite web|title=Super Science Initiative |url=http://www.innovation.gov.au/SCIENCE/RESEARCHINFRASTRUCTURE/Pages/SuperScience.aspx |publisher=DIISR |accessdate=25 June 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/20110601193908/http://www.innovation.gov.au/Science/ResearchInfrastructure/Pages/SuperScience.aspx |archivedate=1 June 2011 |df= }}</ref>\n\n==Research Data Australia==\n''Research Data Australia'' (formerly the ''ANDS Collections Registry'') is an online discovery service run by ANDS.<ref>[http://researchdata.ands.org.au/home/about Research Data Australia]</ref><ref>[http://www.ands.org.au/resource/registry.html ANDS Collections Registry] {{webarchive |url=https://web.archive.org/web/20140302113241/http://www.ands.org.au/resource/registry.html |date=March 2, 2014 }}</ref> It allows researchers to publicise the existence of their research data and enable prospective users of that data to find it.\n\n''Research Data Australia'' makes use of the [[ISO 2146]]-based [[RIF-CS]] metadata standard.<ref>[http://ands.org.au/guides/cpguide/cpgrifcs.html About RIF-CS]</ref>\n\n== External links ==\n* {{Official website|http://www.ands.org.au}}\n* [http://researchdata.ands.org.au Research Data Australia]\n\n==References==\n<references />\n\n[[Category:Data management]]\n\n\n{{Australia-org-stub}}"]
['Meta-data management', '3198327', '[[File:Linnaeus - Regnum Animale (1735).png|thumb|[[Linnaean taxonomy]], a metadata system used historically for grouping animals in zoos, first published in 1735]]\n[[File:6123034166 card catalog.jpg|thumb|[[Card catalog]] and digital media access point]]\n\'\'\'Meta-data management\'\'\' (also known as [[metadata]] management, without the hyphen) involves managing [[data]] about \'\'other data\'\', whereby this "other data" is generally referred to as \'\'content\'\' data. The term is used most often in relation to [[Digital media]], but older forms of metadata are catalogs, dictionaries, and taxonomies. For example, the [[Dewey Decimal Classification]] is a metadata management system for books developed in 1876 for libraries.\n\n==Metadata schema==\nMetadata management can be defined as the end-to-end process and governance framework for creating, controlling, enhancing, attributing, defining and managing a metadata schema, model or other structured aggregation system, either independently or within a repository and the associated supporting processes (often to enable the management of content). For web-based systems, [[Uniform Resource Locator|URL]]s, images, video etc. may be referenced from a triples table of object, attribute and value.\n==Scope==\nWith specific [[knowledge domain]]s, the boundaries of the metadata for each must be managed, since a general [[ontology]] is not useful to experts in one field whose language is knowledge-domain specific.\n==Metadata manager==\nIf one is in the process of making a knowledge management solution, creating a metadata schema and developing a system in which metadata is managed are very important. In such a project, a dedicated metadata manager may be appointed in order to maintain adherence to metadata and information management standards. {{Citation needed|date=January 2011}} This is a person who will be responsible for the metadata strategy, and possibly, the implementation. A metadata manager does not need to know about and be involved with everything concerning the solution, but it does help to have an understanding of as much of the process as possible to make sure a relevant schema is developed.\n==Metadata management over time==\n\nManaging the metadata in a knowledge management solution is an important step in a metadata strategy. It is part of the strategy to make sure that the metadata are complete, current and correct at any given time. Managing a metadata project is also about making sure that users of the system are aware of the possibilities allowed by a well-designed metadata system and how to maximize the benefits of metadata. Regularly monitoring the metadata to ensure that the schema remains relevant is advised.\n\n===Wikipedia metadata===\nWikipedia is a project that actively manages metadata for its articles and files. For example, volunteer editors carefully curate new biographical articles based on the notability (\'\'claim to fame\'\'), name, birth, and/or death dates.<ref>See the internal Wikipedia project on the English Wikipedia called [[Wikipedia:WikiProject Biography]]</ref> Similarly, volunteer editors carefully curate new architectural articles based on name, municipality, or [[geo coordinates]].<ref>See [[Wikipedia:WikiProject Architecture]]</ref> When new articles with a valid alternate spelling are added to Wikipedia that match up to existing articles based on metadata, these are then manually checked and if needed, tagged for merging.<ref>See [[Wikipedia:WikiProject Merge]]</ref> When new articles are added that are considered out of scope or otherwise unfit for Wikipedia, these are nominated for deletion.<ref>See [[Wikipedia:Articles for deletion]]</ref> To help keep track of metadata on Wikipedia, the new Wikimedia project [[Wikidata]] was established in 2012. Click on the pictures to view more metadata about these images:\n<gallery>\nFile:Sta-eulalia.jpg|This picture of the [[Barcelona Cathedral]] was uploaded to the English Wikipedia in 2003 to illustrate its Wikipedia article, and was transferred to [[Wikimedia Commons]] in 2007 so it could be used in other language versions of Wikipedia.\nFile:Article catedral pantalla estreta.png|This screenprint of the [[Catalan Wikipedia]] page on the cathedral features several photos including this one. The screenprint was uploaded to Wikimedia Commons in 2007 soon after the photo was available there, but [[:ca:Catedral de Barcelona|that article]] on the Catalan Wikipedia has since been expanded.\n</gallery>\n\n== See also ==\n* [[Data Defined Storage]] \n* [[Metadata discovery]]\n* [[Metadata publishing]]\n* [[Metadata registry]]\n* [[ISO/IEC 11179]]\n* [[Dublin core]]\n\n \n==References==\n{{reflist}}\n{{DEFAULTSORT:Meta-Data Management}}\n[[Category:Metadata]]\n[[Category:Data management]]']
['Contrast set learning', '34055690', '\'\'\'Contrast set learning\'\'\' is a form of [[association rule learning]] that seeks to identify meaningful differences between separate groups by reverse-engineering the key predictors that identify for each particular group. For example, given a set of attributes for a pool of students (labeled by degree type), a contrast set learner would identify the \'\'contrasting\'\' features between students seeking bachelor\'s degrees and those working toward PhD degrees.\n\n== Overview ==\n\nA common practice in [[data mining]] is to [[Statistical classification|classify]], to look at the attributes of an object or situation and make a guess at what category the observed item belongs to. As new evidence is examined (typically by feeding a \'\'training set\'\' to a learning [[algorithm]]), these guesses are reﬁned and improved. Contrast set learning works in the opposite direction. While classiﬁers read a collection of data and collect information that is used to place new data into a series of discrete categories, contrast set learning takes the category that an item belongs to and attempts to reverse engineer the statistical evidence that identifies an item as a member of a class. That is, contrast set learners seek rules associating attribute values with changes to the class distribution.<ref name="bay01">{{cite journal\n |author1=Stephen Bay |author2=Michael Pazzani | year = 2001\n | title = Detecting group differences: Mining contrast sets\n | journal = Data Mining and Knowledge Discovery\n | volume = 5\n | issue= 3\n | pages= 213–246\n | url= http://wotan.liu.edu/docis/lib/musl/rclis/dbl/dmiknd/(2001)5%253A3%253C213%253ADGDMCS%253E/www.isle.org%252F~sbay%252Fpapers%252Fstucco.dmkd.pdf\n }}\n</ref> They seek to identify the key predictors that contrast one classification from another.\n\nFor example, an aerospace engineer might record data on test launches of a new rocket. Measurements would be taken at regular intervals throughout the launch, noting factors such as the trajectory of the rocket, operating temperatures, external pressures, and so on. If the rocket launch fails after a number of successful tests, the engineer could use contrast set learning to distinguish between the successful and failed tests. A contrast set learner will produce a set of association rules that, when applied, will indicate the key predictors of each failed tests versus the successful ones (the temperature was too high, the wind pressure was too high, etc.).\n\nContrast set learning is a form of [[association rule learning]].<ref name="webb03">{{cite conference\n |author1=GI Webb |author2=S. Butler |author3=D. Newlands | year = 2003\n | title = On Detecting Differences Between Groups\n | conference = KDD\'03 Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n| url= http://portal.acm.org/citation.cfm?id=956781\n }}\n</ref> Association rule learners typically offer rules linking attributes commonly occurring together in a training set (for instance, people who are enrolled in four-year programs and take a full course load tend to also live near campus). Instead of ﬁnding rules that describe the current situation, contrast set learners seek rules that differ meaningfully in their distribution across groups (and thus, can be used as predictors for those groups).<ref name="bay99">{{cite conference\n |author1=Stephen Bay |author2=Michael Pazzani | year = 1999\n | title = Detecting change in categorical data: mining contrast sets\n | conference = KDD \'99 Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining\n }}\n</ref> For example, a contrast set learner could ask, “What are the key identifiers of a person with a bachelor\'s degree or a person with a PhD, and how do people with PhD\'s and bachelor’s degrees differ?”\n\nStandard [[Classification in machine learning|classifier]] algorithms, such as [[C4.5]], have no concept of class importance (that is, they do not know if a class is "good" or "bad"). Such learners cannot bias or filter their predictions towards certain desired classes. As the goal of contrast set learning is to discover meaningful differences between groups, it is useful to be able to target the learned rules towards certain classifications. Several contrast set learners, such as MINWAL<ref name="cai98">{{cite conference\n |author1=C.H. Cai |author2=A.W.C. Fu |author3=C.H. Cheng |author4=W.W. Kwong | year = 1998\n | title = Mining association rules with weighted items\n | conference = Proceedings of International Database Engineering and Applications Symposium (IDEAS 98)\n | url= http://appsrv.cse.cuhk.edu.hk/~kdd/assoc_rule/paper_chcai.pdf\n }}\n</ref> or the family of TAR algorithms,<ref name="hu03"/><ref name="burlet07">{{cite conference\n |author1=K. Gundy-Burlet |author2=J. Schumann |author3=T. Barrett |author4=T. Menzies | year = 2007\n | title = Parametric analysis of ANTARES re-entry guidance algorithms using advanced test generation and data analysis\n | conference = In 9th International Symposium on Artiﬁcial Intelligence, Robotics and Automation in Space\n }}\n</ref><ref name="gay10">{{cite journal\n |author1=Gregory Gay |author2=Tim Menzies |author3=Misty Davies |author4=Karen Gundy-Burlet | year = 2010\n | title = Automatically Finding the Control Variables for Complex System Behavior\n | journal = Automated Software Engineering\n | volume = 17\n | issue= 4\n | url= http://www.greggay.com/pdf/10tar3.pdf \n }}\n</ref> assign weights to each class in order to focus the learned theories toward outcomes that are of interest to a particular audience. Thus, contrast set learning can be though of as a form of weighted class learning.<ref name="menzies03">{{cite journal\n |author1=T. Menzies |author2=Y. Hu | year = 2003\n | title = Data Mining for Very Busy People\n | journal = IEEE Computer\n | volume = 36\n | issue= 11\n | pages= 22–29\n | url= http://menzies.us/pdf/03tar2.pdf\n | doi=10.1109/mc.2003.1244531\n }}\n</ref>\n\n=== Example: Supermarket Purchases ===\n\nThe differences between standard classification, association rule learning, and contrast set learning can be illustrated with a simple supermarket metaphor. In the following small dataset, each row is a supermarket transaction and each "1" indicates that the item was purchased (a "0" indicates that the item was not purchased):\n\n{| class="wikitable"\n|-\n! \'\'Hamburger\'\' !! \'\'Potatoes\'\' !! \'\'Foie Gras\'\' !! \'\'Onions\'\' !! \'\'Champagne\'\' !! \'\'Purpose of Purchases\'\'\n|-\n| 1 || 1 || 0 || 1 || 0 || Cookout\n|-\n| 1 || 1 || 0 || 1 || 0 || Cookout\n|-\n| 0 || 0 || 1 || 0 || 1 || Anniversary\n|-\n| 1 || 1 || 0 || 1 || 0 || Cookout\n|-\n| 1 || 1 || 0 || 0 || 1 || Frat Party\n|}\n\nGiven this data,\n* Association rule learning may discover that customers that buy onions and potatoes together are likely to also purchase hamburger meat.\n* Classification may discover that customers that bought onions, potatoes, and hamburger meats were purchasing items for a cookout.\n* Contrast set learning may discover that the major difference between customers shopping for a cookout and those shopping for an anniversary dinner are that customers acquiring items for a cookout purchase onions, potatoes, and hamburger meat (and \'\'do not purchase\'\' foie gras or champagne).\n\n== Treatment learning ==\n\n<!--[[File:TreatmentLearningExample.png|thumb|200px|Example of a treatment produced based on data collected while riding a bicycle. This treatment states that an optimal riding speed can be obtained while the hill slope is constrained to between −10 and 0% and the cadence is between 1.4 and 2.5. {{deletable image-caption|date=December 2011}}]]-->\n\nTreatment learning is a form of weighted contrast-set learning that takes a single \'\'desirable\'\' group and contrasts it against the remaining \'\'undesirable\'\' groups (the level of desirability is represented by weighted classes).<ref name="hu03">{{cite book\n | author = Y. Hu\n | year = 2003\n | title = Treatment learning: Implementation and application\n | type= Master\'s thesis\n | publisher=Department of Electrical Engineering, University of British Columbia\n }}\n</ref> The resulting "treatment" suggests a set of rules that, when applied, will lead to the desired outcome.\n\nTreatment learning differs from standard contrast set learning through the following constraints:\n* Rather than seeking the differences between all groups, treatment learning specifies a particular group to focus on, applies a weight to this desired grouping, and lumps the remaining groups into one "undesired" category.\n* Treatment learning has a stated focus on minimal theories. In practice, treatment are limited to a maximum of four constraints (i.e., rather than stating all of the reasons that a rocket differs from a skateboard, a treatment learner will state one to four major differences that predict for rockets at a high level of statistical significance).\n\nThis focus on simplicity is an important goal for treatment learners. Treatment learning seeks the \'\'smallest\'\' change that has the \'\'greatest\'\' impact on the class distribution.<ref name="menzies03"/>\n\nConceptually, treatment learners explore all possible subsets of the range of values for all attributes. Such a search is often infeasible in practice, so treatment learning often focuses instead on quickly pruning and ignoring attribute ranges that, when applied, lead to a class distribution where the desired class is in the minority.<ref name="gay10"/>\n\n=== Example: Boston housing data ===\n\nThe following example demonstrates the output of the treatment learner TAR3 on a dataset of housing data from the city of [[Boston]] (a nontrivial public dataset with over 500 examples). In this dataset, a number of factors are collected for each house, and each house is classified according to its quality (low, medium-low, medium-high, and high). The \'\'desired\'\' class is set to "high", and all other classes are lumped together as undesirable.\n\nThe output of the treatment learner is as follows:\n\n<code>\n Baseline class distribution:\n low: 29%\n medlow: 29%\n medhigh: 21%\n high: 21%\n\n Suggested Treatment: [PTRATIO=[12.6..16), RM=[6.7..9.78)]\n\n New class distribution:\n low: 0%\n medlow: 0%\n medhigh: 3%\n high: 97%\n</code>\n\nWith no applied treatments (rules), the desired class represents only 21% of the class distribution. However, if one filters the data set for houses with 6.7 to 9.78 rooms and a neighborhood parent-teacher ratio of 12.6 to 16, then 97% of the remaining examples fall into the desired class (high-quality houses).\n\n== Algorithms ==\n\nThere are a number of algorithms that perform contrast set learning. The following subsections describe two examples.\n\n=== STUCCO ===\n\nThe STUCCO contrast set learner<ref name="bay01"/><ref name="bay99"/> treats the task of learning from contrast sets as a [[Tree traversal|tree search]] problem where the root node of the tree is an empty contrast set. Children are added by specializing the set with additional items picked through a canonical ordering of attributes (to avoid visiting the same nodes twice). Children are formed by appending terms that follow all existing terms in a given ordering. The formed tree is searched in a breadth-first manner. Given the nodes at each level, the dataset is scanned and the support is counted for each group. Each node is then examined to determine if it is significant and large, if it should be pruned, and if new children should be generated. After all significant contrast sets are located, a post-processor selects a subset to show to the user - the low order, simpler results are shown first, followed by the higher order results which are "surprising and significantly different.<ref name="bay99"/>"\n\nThe support calculation comes from testing a null hypothesis that the contrast set support is equal across all groups (i.e., that contrast set support is \'\'independent of group membership\'\'). The support count for each group is a frequency value that can be analyzed in a contingency table where each row represents the truth value of the contrast set and each column variable indicates the group membership frequency. If there is a difference in proportions between the contrast set frequencies and those of the null hypothesis, the algorithm must then determine if the differences in proportions represent a relation between variables or if it can be attributed to random causes. This can be determined through a [[Chi-squared test|chi-square test]] comparing the observed frequency count to the expected count.\n\nNodes are pruned from the tree when all specializations of the node can never lead to a significant and large contrast set. The decision to prune is based on:\n* The minimum deviation size: The maximum difference between the support of any two groups bust be greater than a user-specified threshold.\n* Expected cell frequencies: The expected cell frequencies of a contingency table can only decrease as the contrast set is specialized. When these frequencies are too small, the validity of the chi-square test is violated.\n* <math>\\chi^2</math> bounds: An upper bound is kept on the distribution of a statistic calculated when the null hypothesis is true. Nodes are pruned when it is no longer possible to meet this cutoff.\n\n=== TAR3 ===\n\nThe TAR3<ref name="burlet07"/><ref name="schumann09">{{cite conference\n |author1=J. Schumann |author2=K. Gundy-Burlet |author3=C. Pasareanu |author4=T. Menzies |author5=A. Barrett | year = 2009\n | title = Software V&V support by parametric analysis of large software simulation systems\n | conference = Proceedings of the 2009 IEEE Aerospace Conference\n }}\n</ref> weighted contrast set learner is based on two fundamental concepts - the \'\'\'lift\'\'\' and \'\'\'support\'\'\' of a rule set.\n\nThe lift of a set of rules is the change that some decision makes to a set of examples after imposing that decision (i.e., how the class distribution shifts in response to the imposition of a rule). TAR3 seeks the smallest set of rules which induces the biggest changes in the sum of the weights attached to each class multiplied by the frequency at which each class occurs. The lift is calculated by dividing the score of the set in which the set of rules is imposed by the score of the baseline set (i.e., no rules are applied). Note that by reversing the lift scoring function, the TAR3 learner can also select for the remaining classes and reject the target class.\n\nIt is problematic to rely on the lift of a rule set alone. Incorrect or misleading data noise, if correlated with failing examples, may result in an overfitted rule set. Such an overfitted model may have a large lift score, but it does not accurately reﬂect the prevailing conditions within the dataset. To avoid overfitting, TAR3 utilizes a support threshold and rejects all rules that fall on the wrong side of this threshold. Given a target class, the support threshold is a user-supplied value (usually 0.2) which is compared to the ratio of the frequency of the target class when the rule set has been applied to the frequency of that class in the overall dataset. TAR3 rejects all sets of rules with support lower than this threshold.\n\nBy requiring both a high lift and a high support value, TAR3 not only returns ideal rule sets, but also favors smaller sets of rules. The fewer rules adopted, the more evidence that will exist supporting those rules.\n\nThe TAR3 algorithm only builds sets of rules from attribute value ranges with a high heuristic value. The algorithm determines which ranges to use by ﬁrst determining the lift score of each attribute’s value ranges. These individual scores are then sorted and converted into a cumulative probability distribution. TAR3 randomly selects values from this distribution, meaning that low-scoring ranges are unlikely to be selected. To build a candidate rule set, several ranges are selected and combined. These candidate rule sets are then scored and sorted. If no improvement is seen after a user-defined number of rounds, the algorithm terminates and returns the top-scoring rule sets.\n\n== References ==\n{{Reflist}}\n\n{{DEFAULTSORT:Contrast Set Learning}}\n[[Category:Data management]]\n[[Category:Data mining]]']
['DMAIC', '3733991', '{{sources|date=April 2012}}\n\'\'\'DMAIC\'\'\' (an acronym for \'\'Define, Measure, Analyze, Improve and Control\'\') (pronounced \'\'də-MAY-ick\'\') refers to a data-driven improvement cycle used for improving, optimizing and stabilizing business processes and designs. The DMAIC improvement cycle is the core tool used to drive [[Six Sigma]] projects. However, DMAIC is not exclusive to Six Sigma and can be used as the framework for other improvement applications.\n\n==Steps==\nDMAIC is an abbreviation of the five improvement steps it comprises: Define, Measure, Analyze, Improve and Control. All of the DMAIC process steps are required and always proceed in the given order.\n[[File:DMAICWebdingsII.png|thumbnail|right|400px|The five steps of DMAIC]]\n\n===Define===\nThe purpose of this step is to clearly articulate the business problem, goal, potential resources, project scope and high-level project timeline.  This information is typically captured within project charter document.  Write down what you currently know. Seek to clarify facts, set objectives and form the project team. Define the following:\n\n* A problem \n* The customer(s)\n* [[Voice of the customer]] (VOC) and  [[Critical to Quality]] (CTQs) — what are the critical process outputs?\n\n===Measure===\nThe purpose of this step is to objectively establish current baselines as the basis for improvement.  This is a data collection step, the purpose of which is to establish process performance baselines.  The performance metric baseline(s) from the Measure phase will be compared to the performance metric at the conclusion of the project to determine objectively whether significant improvement has been made.  The team decides on what should be measured and how to measure it. It is usual for teams to invest a lot of effort into assessing the suitability of the proposed measurement systems. Good data is at the heart of the DMAIC process:\n\n===Analyze===\nThe purpose of this step is to identify, validate and select root cause for elimination.  A large number of potential root causes (process inputs, X) of the project problem are identified via root cause analysis (for example a [[Ishikawa diagram|fishbone diagram]]).  The top 3-4 potential root causes are selected using multi-voting or other consensus tool for further validation.  A data collection plan is created and data are collected to establish the relative contribution of each root causes to the project metric, Y.  This process is repeated until "valid" root causes can be identified.  Within Six Sigma, often complex analysis tools are used. However, it is acceptable to use basic tools if these are appropriate.  Of the "validated" root causes, all or some can be\n\n* List and prioritize potential causes of the problem\n* Prioritize the root causes (key process inputs) to pursue in the Improve step\n* Identify how the process inputs (Xs) affect the process outputs (Ys).  Data are analyzed to understand the magnitude of contribution of each root cause, X, to the project metric, Y.  Statistical tests using p-values accompanied by Histograms, Pareto charts, and line plots are often used to do this.\n* Detailed process maps can be created to help pin-point where in the process the root causes reside, and what might be contributing to the occurrence.\n\n===Improve===\nThe purpose of this step is to identify, test and implement a solution to the problem; in part or in whole. This depends on the situation. Identify creative solutions to eliminate the key root causes in order to fix and prevent process problems. Use brainstorming or techniques like [[Six Thinking Hats]] and [[Random stimulus|Random Word]]. Some projects can utilize complex analysis tools like DOE ([[Design of Experiments]]), but try to focus on obvious solutions if these are apparent. However, the purpose of this step can also be to find solutions without implementing them.\n\n* Create\n* Focus on the simplest and easiest solutions\n* Test solutions using [[PDCA|Plan-Do-Check-Act]] (PDCA) cycle\n* Based on PDCA results, attempt to anticipate any avoidable risks associated with the "improvement" using [[Failure mode and effects analysis|FMEA]]\n* Create a detailed implementation plan\n* Deploy improvements\n\n===Control===\nThe purpose of this step is to sustain the gains.  Monitor the improvements to ensure continued and sustainable success. Create a control plan. Update documents, business process and training records as required.\n\nA [[Control chart]] can be useful during the Control stage to assess the stability of the improvements over time by serving as 1. a guide to continue monitoring the process and 2. provide a response plan for each of the measures being monitored in case the process becomes unstable.\n\n===Replicate and thank the teams===\nThis is additional to the standard DMAIC steps but it should be considered. Think about replicating the changes in other processes. Share your new knowledge within and outside of your organization.  It is very important to always provide positive morale support to team members in an effort to maximize the effectiveness of DMAIC.\n\nReplicating the improvements, sharing your success and thanking your team members helps build buy-in for future DMAIC or improvement initiatives.\n\n===Additional Steps===\nSome organizations add a \'\'\'\'\'R\'\'\'ecognize\'\' step at the beginning, which is to recognize the right problem to work on, thus yielding an RDMAIC methodology.<ref name="WebberWallace2006p43">{{cite book | first1=Larry | last1=Webber | first2=Michael | last2=Wallace | title=Quality Control for Dummies | url=https://books.google.com/books?id=9BWkxto2fcEC&pg=PA43 | accessdate=2012-05-16 | date=15 December 2006 | publisher=For Dummies | isbn=978-0-470-06909-7 | pages=42–43 }}</ref>\n\n==See also==\n*[[Design for Six Sigma|DFSS]]\n*[[Industrial engineering]]\n*[[Kaizen]]\n*[[PDCA]]\n*[[Six Sigma]]\n\n==References==\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:Six Sigma]]']
['Data warehouse', '7990', '{{multiple issues|\n{{Refimprove|date=February 2008}}\n{{Citation style|date=September 2013}}\n}}\n\n[[File:Data warehouse overview.JPG|thumb|200px|Data Warehouse Overview]]\n\nIn [[computing]], a \'\'\'data warehouse\'\'\' (\'\'\'DW\'\'\' or \'\'\'DWH\'\'\'), also known as an \'\'\'enterprise data warehouse\'\'\' (\'\'\'EDW\'\'\'), is a system used for [[Business reporting|reporting]] and [[data analysis]], and is considered a core component of [[business intelligence]].<ref>Dedić, N. and Stanier C., 2016., "An Evaluation of the Challenges of Multilingualism in Data Warehouse Development" in 18th International Conference on Enterprise Information Systems - ICEIS 2016, p. 196.</ref> DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.\n\nThe data stored in the warehouse is [[upload]]ed from the [[operational system]]s (such as marketing or sales). The data may pass through an [[operational data store]] for additional operations before it is used in the DW for reporting.\n\n==Types of systems==\n;[[Data mart]]:  A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area), hence they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data.<ref>{{cite web |url=http://docs.oracle.com/html/E10312_01/dm_concepts.htm |title=Data Mart Concepts |publisher=Oracle |year=2007}}</ref> Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.\n\n{| class="wikitable"\n|+ Difference between data warehouse and {{nowrap|data mart}}\n|-\n! Data warehouse\n! Data mart\n|-\n| enterprise-wide data\n| department-wide data\n|-\n| multiple subject areas\n| single subject area\n|-\n| difficult to build\n| easy to build\n|-\n| takes more time to build\n| less time to build\n|-\n| larger memory\n| limited memory\n|}\n\n\'\'\'Types of data marts\'\'\'\n* Dependent data mart\n* Independent data mart\n* Hybrid data mart\n;[[Online analytical processing]] (OLAP): OLAP is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effectiveness measure. OLAP applications are widely used by [[Data Mining]] techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day. The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing & Dicing.<ref name=dwh>{{cite web |url=https://intellipaat.com/tutorial/data-warehouse-tutorial/ |title=Data Warehousing Tutorial For Beginners |publisher=Intellipaat}}</ref>\n\n;[[Online transaction processing]] (OLTP): OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining [[data integrity]] in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually [[Third normal form|3NF]]).<ref>{{cite web |url=http://datawarehouse4u.info/OLTP-vs-OLAP.html |title=OLTP vs. OLAP |year=2009 |website=Datawarehouse4u.Info |quote=We can divide IT systems into transactional (OLTP) and analytical (OLAP). In general we can assume that OLTP systems provide source data to data warehouses, whereas OLAP systems help to analyze it.}}</ref> Normalization is the norm for data modeling techniques in this system.\n\n;Predictive analysis: Predictive analysis is about [[pattern recognition|finding]] and quantifying hidden patterns in the data using complex mathematical models that can be used to [[prediction|predict]] future outcomes.  Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for CRM ([[customer relationship management]]).\n\n==Software tools==\nThe typical extract-transform-load ([[Extract, transform, load|ETL]])-based data warehouse uses [[Staging (data)|staging]], [[data integration]], and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an [[operational data store]] (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a [[star schema]].  The access layer helps users retrieve data.<ref name=IJCA96Patil>{{cite journal |url=http://www.ijcaonline.org/proceedings/icwet/number9/2131-db195 |author1=Patil, Preeti S. |author2=Srikantha Rao |author3=Suryakant B. Patil |title=Optimization of Data Warehousing System: Simplification in Reporting and Analysis |work=IJCA Proceedings on International Conference and workshop on Emerging Trends in Technology (ICWET) |year=2011 |volume=9 |issue=6 |pages=33–37 |publisher=Foundation of Computer Science}}</ref>\n\nThis definition of the data warehouse focuses on data storage. The main source of the data is cleaned, transformed, catalogued and made available for use by managers and other business professionals for [[data mining]], [[OLAP|online analytical processing]], [[market research]] and [[decision support]].<ref>Marakas & O\'Brien 2009</ref> However, the means to retrieve and analyze data, to [[Extract, transform, load|extract, transform, and load]] data, and to manage the [[data dictionary]] are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes [[business intelligence tools]], tools to extract, transform, and load data into the repository, and tools to manage and retrieve [[metadata]].\n\n==Benefits==\nA data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to:\n* Integrate data from multiple sources into a single database and data model. Mere congregation of data to single database so a single query engine can be used to present data is an ODS.\n* Mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large, long running, analysis queries in transaction processing databases.\n* Maintain [[Provenance#Data provenance|data history]], even if the source transaction systems do not.\n* Integrate data from multiple source systems, enabling a central view across the enterprise. This benefit is always valuable, but particularly so when the organization has grown by merger.\n* Improve [[data quality]], by providing consistent codes and descriptions, flagging or even fixing bad data.\n* Present the organization\'s information consistently.\n* Provide a single common data model for all data of interest regardless of the data\'s source.\n* Restructure the data so that it makes sense to the business users.\n* Restructure the data so that it delivers excellent query performance, even for complex analytic queries, without impacting the [[operational system]]s.\n* Add value to operational business applications, notably [[customer relationship management]] (CRM) systems.\n*Make decision–support queries easier to write.\n*Optimized data warehouse architectures allow data scientists to organize and disambiguate repetitive data.<ref>{{Cite web|url=https://www.idera.com/resourcecentral/whitepapers/modern-data-architecture|title=Modern Data Architecture {{!}} IDERA|last=|first=|date=|website=www.idera.com|publisher=|access-date=2016-09-18}}</ref>\n\n==Generic environment==\n\nThe environment for data warehouses and marts includes the following:\n\n* Source systems that provide data to the warehouse or mart;\n* Data integration technology and processes that are needed to prepare the data for use;\n* Different architectures for storing data in an organization\'s data warehouse or data marts;\n*Different tools and applications for the variety of users;\n*Metadata, data quality, and governance processes must be in place to ensure that the warehouse or mart meets its purposes.\n\nIn regards to source systems listed above, Rainer{{clarify|reason=Who is Rainer?|date=December 2014}} states, "A common source for the data in data warehouses is the company\'s operational databases, which can be relational databases".<ref name=rainer2012>{{cite book|last=Rainer|first=R. Kelly|title=Introduction to Information Systems: Enabling and Transforming Business, 4th Edition (Kindle Edition)|date=2012-05-01|publisher=Wiley|pages=127, 128, 130, 131, 133}}</ref>\n\nRegarding data integration, Rainer states, "It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse".<ref name=rainer2012/>\n\nRainer discusses storing data in an organization’s data warehouse or data marts.<ref name=rainer2012 />\n\nMetadata are data about data. "IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures".<ref name=rainer2012 />\n\nToday, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers.<ref name=rainer2012 /> A "data warehouse" is a repository of historical data that are organized by subject to support decision makers in the organization.<ref name=rainer2012 /> Once data are stored in a data mart or warehouse, they can be accessed.\n\n==History==\nThe concept of data warehousing dates back to the late 1980s<ref>{{cite web|url=http://www.computerworld.com/databasetopics/data/story/0,10801,70102,00.html |title=The Story So Far |date=2002-04-15 |accessdate=2008-09-21 |deadurl=yes |archiveurl=https://web.archive.org/web/20080708182105/http://www.computerworld.com/databasetopics/data/story/0,10801,70102,00.html |archivedate=2008-07-08 |df= }}</ref> when IBM researchers Barry Devlin and Paul Murphy developed the "business data warehouse". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to [[decision support system|decision support environments]]. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as [[legacy system]]s), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from "[[data mart]]s" that were tailored for ready access by users.\n\nKey developments in early years of data warehousing were:\n* 1960s – [[General Mills]] and [[Dartmouth College]], in a joint research project, develop the terms \'\'dimensions\'\' and \'\'facts\'\'.<ref name="kimball16">Kimball 2002, pg. 16</ref>\n* 1970s – [[ACNielsen]] and IRI provide dimensional data marts for retail sales.<ref name="kimball16" />\n* 1970s – [[Bill Inmon]] begins to define and discuss the term: Data Warehouse.{{citation needed|date=June 2014}}\n* 1975 – [[Sperry Univac]] introduces [[MAPPER]] (MAintain, Prepare, and Produce Executive Reports) is a database management and reporting system that includes the world\'s first [[Fourth-generation programming language|4GL]]. First platform designed for building Information Centers (a forerunner of contemporary Enterprise [[Data Warehousing]] platforms)\n* 1983 – [[Teradata]] introduces a database management system specifically designed for decision support.\n* 1984 – [[Metaphor Computer Systems]], founded by [[David Liddle]] and Don Massaro, releases Data Interpretation System (DIS). DIS was a hardware/software package and GUI for business users to create a database management and analytic system.\n* 1988 – Barry Devlin and Paul Murphy publish the article \'\'An architecture for a business and information system\'\' where they introduce the term "business data warehouse".<ref>{{cite journal|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5387658|title=An architecture for a business and information system|journal=IBM Systems Journal | doi=10.1147/sj.271.0060|volume=27|pages=60–80}}</ref>\n* 1990 – Red Brick Systems, founded by [[Ralph Kimball]], introduces Red Brick Warehouse, a database management system specifically for data warehousing.\n* 1991 – Prism Solutions, founded by [[Bill Inmon]], introduces Prism Warehouse Manager, software for developing a data warehouse.\n* 1992 – [[Bill Inmon]] publishes the book \'\'Building the Data Warehouse\'\'.<ref>{{cite book|last=Inmon|first=Bill|title=Building the Data Warehouse|year=1992|publisher=Wiley|isbn=0-471-56960-7}}</ref>\n* 1995 – The Data Warehousing Institute, a for-profit organization that promotes data warehousing, is founded.\n* 1996 – [[Ralph Kimball]] publishes the book \'\'The Data Warehouse Toolkit\'\'.<ref name=":0">{{cite book|title=The Data Warehouse Toolkit|last=Kimball|first=Ralph|publisher=Wiley|year=2011|isbn=9780470149775|page=237}}</ref>\n* 2012 – [[Bill Inmon]] developed and made public technology known as "textual disambiguation". Textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format. Once raw text is passed through textual disambiguation, it can easily and efficiently be accessed and analyzed by standard business intelligence technology. Textual disambiguation is accomplished through the execution of textual ETL. Textual disambiguation is useful wherever raw text is found, such as in documents, Hadoop, email, and so forth.\n\n==Information storage==\n\n===Facts===\nA fact is a value or measurement, which represents a fact about the managed entity or system.\n\nFacts as reported by the reporting entity are said to be at raw level. E.g. in a mobile telephone system, if a BTS (base transceiver station) received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 \'\'\'facts\'\'\' or measurements to a management system:\n* tch_req_total = 1000\n* tch_req_success = 820\n* tch_req_fail = 180\n\nFacts at the raw level are further aggregated to higher levels in various [[Dimension (data warehouse)|dimensions]] to extract more service or business-relevant information from it. These are called aggregates or summaries or aggregated facts.\n\nFor instance, if there are 3 BTSs in a city, then the facts above can be aggregated from the BTS to the city level in the network dimension. For example:\n\n* <math>tch\\_req\\_success\\_city = tch\\_req\\_success\\_bts1 + tch\\_req\\_success\\_bts2 + tch\\_req\\_success\\_bts3</math>\n* <math>avg\\_tch\\_req\\_success\\_city = (tch\\_req\\_success\\_bts1 + tch\\_req\\_success\\_bts2 + tch\\_req\\_success\\_bts3) / 3</math>\n\n===Dimensional versus normalized approach for storage of data===\nThere are three or more leading approaches to storing data in a data warehouse&nbsp;— the most important approaches are the dimensional approach and the normalized approach.\n\nThe dimensional approach refers to [[Ralph Kimball]]’s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/[[star schema]]. The normalized approach, also called the [[Third normal form|3NF]] model (Third Normal Form) refers to Bill Inmon\'s approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.\n\nIn a [[Star schema|dimensional approach]], [[transaction data]] are partitioned into "facts", which are generally numeric transaction data, and "[[dimension (data warehouse)|dimensions]]", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the total price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.\n\nA key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly.<ref name=":0" /> Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization’s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus, this type of modeling technique is very useful for end-user queries in data warehouse.<ref name=dwh />\n\nThe main disadvantages of the dimensional approach are the following:\n# In order to maintain the integrity of facts and dimensions, loading the data warehouse with data from different operational systems is complicated.\n# It is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business.\n\nIn the normalized approach, the data in the data warehouse are stored following, to a degree, [[database normalization]] rules. Tables are grouped together by \'\'subject areas\'\' that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008){{Citation needed|date=November 2013}}.\nThe main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the [[data structure]] of the data warehouse.\n\nBoth normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as [[Database normalization#Normal forms|Normal Forms]]). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).\n\nIn \'\'Information-Driven Business\'\',<ref>{{cite book|last=Hillard|first=Robert|title=Information-Driven Business|year=2010|publisher=Wiley|isbn=978-0-470-62577-4}}</ref> Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of [[Entropy (information theory)|information entropy]] and usability in terms of the Small Worlds data transformation measure.<ref>{{cite web|url=http://mike2.openmethodology.org/wiki/Small_Worlds_Data_Transformation_Measure |title=Information Theory & Business Intelligence Strategy - Small Worlds Data Transformation Measure - MIKE2.0, the open source methodology for Information Development |publisher=Mike2.openmethodology.org |date= |accessdate=2013-06-14}}</ref>\n\n==Design methods==\n{{refimprove section|date=July 2015}}\n\n===Bottom-up design===\nIn the \'\'bottom-up\'\' approach, [[data mart]]s are first created to provide reporting and analytical capabilities for specific [[business process]]es. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of "the bus", a collection of [[Dimension (data warehouse)#Types|conformed dimension]]s and [[Facts (data warehouse)#Types|conformed fact]]s, which are dimensions that are shared (in a specific way) between facts in two or more data marts.<ref>{{Cite web|url=http://decisionworks.com/2003/09/the-bottom-up-misnomer/|title=The Bottom-Up Misnomer - DecisionWorks Consulting|website=DecisionWorks Consulting|language=en-US|access-date=2016-03-06}}</ref>\n\n===Top-down design===\nThe \'\'top-down\'\' approach is designed using a normalized enterprise [[data model]]. [[Data element|"Atomic" data]], that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.<ref name="ReferenceA">Gartner, Of Data Warehouses, Operational Data Stores, Data Marts and Data Outhouses, Dec 2005</ref>\n\n===Hybrid design===\nData warehouses (DW) often resemble the [[hub and spokes architecture]]. [[Legacy system]]s feeding the warehouse often include [[customer relationship management]] and [[enterprise resource planning]], generating large amounts of data. To consolidate these various data models, and facilitate the [[extract transform load]] process, data warehouses often make use of an [[operational data store]], the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW.\n\nThe DW database in a hybrid solution is kept on [[third normal form]] to eliminate [[data redundancy]]. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a [[master data management]] solution where operational, not static information could reside.\n\nThe [[Data Vault Modeling]] components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and [[star schema]]. The Data Vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The Data Vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.\n\n==Versus operational system==\nOperational systems are optimized for preservation of [[data integrity]] and speed of recording of business transactions through use of [[database normalization]] and an [[entity-relationship model]]. Operational system designers generally follow the [[Codd\'s 12 rules|Codd rules]] of [[database normalization]] in order to ensure data integrity. Codd defined five increasingly stringent rules of normalization. Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. [[Relational database]]s are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. Finally, in order to improve performance, older data are usually periodically purged from operational systems.\n\nData warehouses are optimized for analytic access patterns.  Analytic access patterns generally involve selecting specific fields and rarely if ever \'select *\' as is more common in operational databases.  Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a [[column-oriented DBMS]].  Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.\n\n==Evolution in organization use==\nThese terms refer to the level of sophistication of a data warehouse:\n\n; Offline operational data warehouse: Data warehouses in this stage of evolution are updated on a regular time cycle (usually daily, weekly or monthly) from the operational systems and the data is stored in an integrated reporting-oriented data\n; Offline data warehouse: Data warehouses at this stage are updated from data in the operational systems on a regular basis and the data warehouse data are stored in a data structure designed to facilitate reporting.\n; On time data warehouse: Online Integrated Data Warehousing represent the real time Data warehouses stage data in the warehouse is updated for every transaction performed on the source data\n; Integrated data warehouse: These data warehouses assemble data from different areas of business, so users can look up the information they need across other systems.<ref>{{cite web |url=http://www.tech-faq.com/data-warehouse.html |title=Data Warehouse }}</ref>\n\n==See also==\n{{colbegin|3}}\n*[[Accounting intelligence]]\n*[[Anchor modeling]]\n*[[Business intelligence]]\n*[[Business intelligence tools]]\n*[[Data integration]]\n*[[Data mart]]\n*[[Data mining]]\n*[[Data presentation architecture]]\n*[[Data scraping]]\n*[[Data warehouse appliance]]\n*[[Database management system]]\n*[[Decision support system]]\n*[[Data vault modeling]]\n*[[Executive information system]]\n*[[Extract, transform, load]]\n*[[Master data management]]\n*[[Online analytical processing]]\n*[[Online transaction processing]]\n*[[Operational data store]]\n*[[Semantic warehousing]]\n*[[Snowflake schema]]\n*[[Software as a service]]\n*[[Star schema]]\n*[[Slowly changing dimension]]\n*[[Data warehouse automation]]\n{{colend}}\n\n==References==\n{{Reflist|30em|<!--refs=\n<ref name=ahsan>\n{{Cite journal\n|last1=Abdullah\n|first1=Ahsan\n|title=Analysis of mealybug incidence on the cotton crop using ADSS-OLAP (Online Analytical Processing) tool, Volume 69, Issue 1\n|journal= Computers and Electronics in Agriculture\n|year=2009\n|pages=59–72\n|doi=10.1016/j.compag.2009.07.003\n|volume=69\n}}\n</ref>-->\n}}\n\n==Further reading==\n* Davenport, Thomas H. and Harris, Jeanne G. \'\'Competing on Analytics: The New Science of Winning\'\' (2007) Harvard Business School Press. ISBN 978-1-4221-0332-6\n* Ganczarski, Joe. \'\'Data Warehouse Implementations: Critical Implementation Factors Study\'\' (2009) [[VDM Verlag]] ISBN 3-639-18589-7 ISBN 978-3-639-18589-8\n* Kimball, Ralph and Ross, Margy. \'\'The Data Warehouse Toolkit\'\' Third Edition (2013) Wiley, ISBN 978-1-118-53080-1\n* Linstedt, Graziano, Hultgren. \'\'The Business of Data Vault Modeling\'\' Second Edition (2010) Dan linstedt, ISBN 978-1-4357-1914-9\n* William Inmon. \'\'Building the Data Warehouse\'\' (2005) John Wiley and Sons, ISBN 978-8-1265-0645-3\n\n==External links==\n* [http://www.kimballgroup.com/html/articles.html Ralph Kimball articles]\n* [http://www.ijcaonline.org/archives/number3/77-172 International Journal of Computer Applications]\n* [http://dwreview.com/DW_Overview.html Data Warehouse Introduction]\n\n{{data}}\n{{Data warehouse}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Data Warehouse}}\n[[Category:Business intelligence]]\n[[Category:Data management]]\n[[Category:Data warehousing| ]]\n[[Category:Information technology management]]']
['Disaster recovery plan', '6309764', '{{merge to|Business continuity planning|date=June 2015}}\nA \'\'\'disaster recovery plan\'\'\' (DRP) is a documented process or set of procedures to recover and protect a business [[Information_technology|IT]] infrastructure in the event of a [[disaster]].<ref name="5 tips">{{cite web |url=http://www.smallbusinesscomputing.com/News/ITManagement/5-tips-to-build-an-effective-disaster-recovery-plan.html |title=5 Tips to Build an Effective Disaster Recovery Plan |publisher=Small Business Computing |date=14 June 2012 |accessdate=9 August 2012 |first1=Bill |last1=Abram}}</ref>  Such a plan, ordinarily documented in written form, specifies  procedures an organization is to follow in the event of a disaster. It is "a comprehensive statement of consistent actions to be taken before, during and after a disaster."<ref name="DR journal">{{cite web |url=http://www.drj.com/new2dr/w2_002.htm |title=Disaster Recovery Planning Process |first1=Geoffrey H. |last1=Wold |work=Disaster Recovery Journal |series=Adapted from Volume 5 #1 | publisher=Disaster Recovery World |year=1997 |accessdate=8 August 2012}}</ref> The disaster could be [[Natural disaster|natural]], [[Environmental disaster|environmental]] or [[Anthropogenic hazard|man-made]]. Man-made disasters could be intentional (for example, an act of a terrorist) or unintentional (that is, accidental, such as the breakage of a man-made dam).\n\nGiven organizations\' increasing dependency on [[information technology]] to run their operations, a disaster recovery plan, sometimes erroneously called a [[Continuity of Operations]] Plan (COOP), is increasingly associated with the recovery of information technology data, assets, and facilities.\n\n==Objectives==\n\nOrganizations cannot always avoid disasters, but with careful planning the effects of a disaster can be minimized. The objective of a disaster recovery plan is to minimize downtime and data loss.<ref>[http://www.comp-soln.com/DRP_whitepaper.pdf \'\'An Overview of the Disaster Recovery Planning Process - From Start to Finish.\'\'] Comprehensive Consulting Solutions Inc.( "Disaster Recovey Planning, An Overview: White Paper." )March 1999. Retrieved 8 August 2012.</ref> The primary objective is to protect the organization in the event that all or part of its operations and/or computer services are rendered unusable. The plan minimizes the disruption of operations and ensures that some level of organizational stability and an orderly recovery after a disaster will prevail.<ref name="DR journal" />  Minimizing downtime and data loss is measured in terms of two concepts: the [[recovery time objective]] (RTO) and the [[recovery point objective]] (RPO).\n \nThe recovery time objective is the time within which a business process must be restored, after a [[disaster|major incident]] (MI) has occurred, in order to avoid unacceptable consequences associated with a break in [[business continuity]]. The recovery point objective (RPO) is the age of files that must be recovered from backup storage for normal operations to resume if a computer, system, or network goes down as a result of a MI. The RPO is expressed backwards in time (that is, into the past) starting from the instant at which the MI occurs, and can be specified in seconds, minutes, hours, or days.<ref>[http://whatis.techtarget.com/definition/recovery-point-objective-RPO \'\'Definition: Recovery point objective (RPO).\'\'] Retrieved 10 August 2012.</ref> The recovery point objective (RPO) is thus the maximum acceptable amount of data loss measured in time. It is the age of the files or data in backup storage required to resume normal operations after the MI.<ref>{{cite web |url=http://www.techopedia.com/definition/1032/recovery-point-objective-rpo |title=Recovery Point Objective (RPO): Definition - What does Recovery Point Objective (RPO) mean? |work=Techopedia |publisher=Janalta Interactive Inc. |year=2012 |accessdate=10 August 2012 }}</ref>\n\n[[File:Schematic ITSC and RTO, RPO, MI.jpg|frame|left|A DR plan illustrating the chronology of the \'\'\'{{color|#bd00e0|RPO}}\'\'\' and the \'\'\'{{color|#ff7f7c|RTO}}\'\'\' with respect to the \'\'\'{{color|#fe0000|MI}}\'\'\'.]]\n{{clear}}\n\n==Relationship to the Business Continuity Plan==\n\nAccording to the SANS institute, the [[Business continuity planning|Business Continuity Plan]] (BCP) is a comprehensive organizational plan that includes the disaster recovery plan. The Institute further states that a Business Continuity Plan (BCP) consists of the five component plans:<ref name="The Disaster Recovery Plan.">[http://www.sans.org/reading_room/whitepapers/recovery/disaster-recovery-plan_1164 \'\'The Disaster Recovery Plan.\'\'] Chad Bahan. GSEC Practical Assignment version 1.4b. SANS Institute InfoSec Reading Room. June 2003. Retrieved 24 August 2012.</ref>\n\n* Business Resumption Plan\n* Occupant Emergency Plan\n* Continuity of Operations Plan\n* Incident Management Plan\n* Disaster Recovery Plan\n\nThe Institute states that the first three plans (Business Resumption, Occupant Emergency, and Continuity of Operations Plans) do not deal with the IT infrastructure. They further state that the Incident Management Plan (IMP) does deal with the IT infrastructure, but since it establishes structure and procedures to address cyber attacks against an organization’s IT systems, it generally does not represent an agent for activating the Disaster Recovery Plan, leaving The Disaster Recovery Plan as the only BCP component of interest to IT.<ref name="The Disaster Recovery Plan."/>\n\n[[Disaster Recovery Institute]] International states that disaster recovery is the area of business continuity that deals with \'\'technology\'\' recovery as opposed to the recovery of business operations.<ref>https://www.drii.org/glossary.php</ref>\n\n==Benefits==\n\nLike every insurance plan, there are benefits that can be obtained from the drafting of a disaster recovery plan. Some of these benefits are:<ref name="DR journal" />\n\n* Providing a sense of security\n* Minimizing risk of delays\n* Guaranteeing the reliability of standby systems\n* Providing a standard for testing the plan\n* Minimizing decision-making during a disaster\n* Reducing potential legal liabilities\n* Lowering unnecessarily stressful work environment\n\n==Types of plans==\n\nThere is no one right type of disaster recovery plan,<ref name=MSU>{{cite web |url=http://www.drp.msu.edu/documentation/stepbystepguide.htm |publisher=Michigan State University |title=Disaster Recovery Planning - Step by Step Guide |accessdate=9 May 2014 }}</ref> nor is there a one-size-fits-all disaster recovery plan.<ref name="5 tips" /><ref name=MSU /> However, there are three basic strategies that feature in all disaster recovery plans: (1) preventive measures, (2) detective measures, and (3) corrective measures.<ref>{{cite web |url=http://emailarchivingandremotebackup.com/backup-disaster-recovery.html |title=Backup Disaster Recovery |publisher=Email Archiving and Remote Backup |year=2010 |accessdate=9 May 2014}}</ref> Preventive measures will try to prevent a disaster from occurring. These measures seek to identify and reduce risks. They are designed to mitigate or prevent an event from happening. These measures may include keeping data backed up and off site, using surge protectors, installing generators and conducting routine inspections. Detective measures are taken to discover the presence of any unwanted events within the IT infrastructure.  Their aim is to uncover new potential threats. They may detect or uncover unwanted events. These measures include installing fire alarms, using up-to-date antivirus software, holding employee training sessions, and installing server and [[network monitoring]] software.  Corrective measures are aimed to restore a system after a disaster or otherwise unwanted event takes place. These measures focus on fixing or restoring the systems after a disaster. Corrective measures may include keeping critical documents in the Disaster Recovery Plan or securing proper [[insurance policy|insurance policies]], after a "lessons learned" brainstorming session.<ref name="5 tips" /><ref>{{cite web|url=http://www.stonecrossingsolutions.com/technical-solutions/disaster-recovery/ |title=Disaster Recovery & Business Continuity Plans |publisher=Stone Crossing Solutions |date=2012 |accessdate=9 August 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20120823045007/http://www.stonecrossingsolutions.com/technical-solutions/disaster-recovery/ |archivedate=23 August 2012 |df= }}</ref>\n\nA disaster recovery plan must answer at least three basic questions: (1) what is its objective and purpose, (2) who will be the people or teams who will be responsible in case any disruptions happen, and (3) what will these people do (the procedures to be followed) when the disaster strikes.<ref>{{cite web|url=http://www.continuitycompliance.org/disaster-recovery-planning-on-virtual-and-cloud-platforms-survey-results-now-available/ |title=Disaster Recovery – Benefits of Getting Disaster Planning Software and Template and Contracting with Companies Offering Data Disaster Recovery Plans, Solutions and Services: Why Would You Need a Disaster Recovery Plan? |publisher=Continuity Compliance |date=7 June 2011 |accessdate=14 August 2012 |archivedate=9 May 2014 |archiveurl=http://www.webcitation.org/6PQaoed5G?url=http://www.continuitycompliance.org/disaster-recovery-planning-on-virtual-and-cloud-platforms-survey-results-now-available/ |deadurl=yes |df= }}</ref>\n\n==Types of disasters==\n[[Image:SH-60B helicopter flies over Sendai.jpg|thumb|right|200px|The tsunami that affected Japan in 2011, a type of natural disaster]]\n[[Image:UA Flight 175 hits WTC south tower 9-11 edit.jpeg|thumb|right|200px|September 11, 2001, in New York City, a type of man-made disaster: it caused pollution, loss of lives, property damage, and considerable [[data loss]]]]\n\nDisasters can be [[Natural disaster|natural]] or [[Anthropogenic hazard|man-made]]. Man-made disasters could be intentional (for example, sabotage or an act of [[terrorism]]) or unintentional (that is, accidental, such as the breakage of a man-made dam).  Disasters may encompass more than weather. They may involve Internet threats or take on other man-made manifestations such as theft.<ref name="5 tips" />\n\n===Natural disaster===\n{{Main article|Natural disaster}}\nA natural disaster is a major adverse event resulting from the earth\'s natural hazards. Examples of natural disasters are [[flood]]s, [[tsunami]]s, [[tornado]]es, [[hurricane|hurricanes/cyclones]], [[volcanic eruption]]s, [[earthquake]]s, [[heat wave]]s, and [[landslide]]s.  Other types of disasters include the more [[End time|cosmic]] scenario of an [[Impact event|asteroid hitting the Earth]].\n\n===Man-made disasters===\n{{Main article|Man-made disasters}}\nMan-made disasters are the consequence of technological or human hazards. Examples include [[stampede]]s, [[fire|urban fires]], [[industrial accident]]s, [[oil spill]]s, [[nuclear explosion]]s/[[nuclear radiation]] and acts of [[war]].  Other types of man-made disasters include the more cosmic scenarios of catastrophic [[global warming]], [[nuclear war]], and [[bioterrorism]].\n\nThe following table categorizes some disasters and notes first response initiatives. Note that whereas the sources of a disaster may be natural (for example, heavy rains) or man-made (for example, a broken dam), the results may be similar (flooding).<ref>[http://www.nten.org/sites/nten/files/Sample%20Disaster%20Recovery%20Plan.doc \'\'Business Continuity Planning (BCP): Sample Plan For Nonprofit Organizations.\'\'] {{wayback|url=http://www.nten.org/sites/nten/files/Sample%20Disaster%20Recovery%20Plan.doc |date=20100602065521 }} Pages 11-12. Retrieved 8 August 2012.</ref>\n\n{| class="wikitable"\n! rowspan="16" | Natural\n! colspan="3"  | Disaster\n|- bgcolor="#CCCCCC"\n!Example|| Profile || First Response\n|-\n|[[Avalanche]]||The sudden, drastic flow of snow down a slope, occurring when either natural triggers, such as loading from new snow or rain, or artificial triggers, such as explosives or backcountry skiers, overload the snowpack||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption\n|-\n|[[Blizzard]]||A severe snowstorm characterized by very strong winds and low temperatures||Power off all equipment; listen to blizzard advisories; Evacuate area, if unsafe; Assess damage\n|-\n|[[Earthquake]]||The shaking of the earth’s crust, caused by underground volcanic forces of breaking and shifting rock beneath the earth’s surface||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption\n|-\n|[[Fire|Fire (wild)]]||Fires that originate in uninhabited areas and which pose the risk to spread to inhabited areas||Attempt to suppress fire in early stages; Evacuate personnel on alarm, as necessary; Notify fire department; Shut off utilities; Monitor weather advisories\n|-\n|[[Flood]]||Flash flooding: Small creeks, gullies, dry streambeds, ravines, culverts or even low-lying areas flood quickly||Monitor flood advisories; Determine flood potential to facilities; Pre-stage emergency power generating equipment; Assess damage\n|-\n|[[Freezing Rain]]||Rain occurring when outside surface temperature is below freezing||Monitor weather advisories; Notify employees of business closure; home; Arrange for snow and ice removal\n|-\n|[[Heat wave]]||A prolonged period of excessively hot weather relative to the usual weather pattern of an area and relative to normal temperatures for the season||Listen to weather advisories; Power-off all servers after a graceful shutdown if there is imminent potential of power failure; Shut down main electric circuit usually located in the basement or the first floor\n|-\n|[[Hurricane]]||Heavy rains and high winds||Power off all equipment; listen to hurricane advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Do not use telephones, in the event of severe lightning; Assess damage\n|-\n|[[Landslide]]||Geological phenomenon which includes a range of ground movement, such as rock falls, deep failure of slopes and shallow debris flows||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption\n|-\n|[[Lightning strike]]||An electrical discharge caused by lightning, typically during thunderstorms||Power off all equipment; listen to hurricane advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Do not use telephones, in the event of severe lightning; Assess damage\n|-\n|[[Limnic eruption]]||The sudden eruption of carbon dioxide from deep lake water||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption\n|-\n|[[Tornado]]||Violent rotating columns of air which descent from severe thunderstorm cloud systems||Monitor tornado advisories; Power off equipment; Shut off utilities (power and gas); Assess damage once storm passes\n|-\n|[[Tsunami]]||A series of water waves caused by the displacement of a large volume of a body of water, typically an ocean or a large lake, usually caused by earthquakes, volcanic eruptions, underwater explosions, landslides, glacier calvings, meteorite impacts and other disturbances above or below water||Power off all equipment; listen to tsunami advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Assess damage\n|-\n|[[Volcanic eruption]]||The release of hot magma, volcanic ash and/or gases from a volcano||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption\n|-\n! rowspan="6" | Man-made\n|[[Bioterrorism]]||The intentional release or dissemination of biological agents as a means of coercion||Get information immediately from your [[Public Health]] officials via the news media as to the right course of action; If you think you have been exposed, quickly remove your clothing and wash off your skin; Also put on a [[HEPA]] to help prevent inhalation of the agent<ref>[http://answers.webmd.com/answers/1176206/what-should-i-do-if-there \'\'What should I do if there has been a bioterrorism attack?.\'\'] Edmond A. Hooker. WebMD. 9 October 2007. Retrieved 18 September 2012.</ref> \n|-\n|[[Civil unrest]]||A disturbance caused by a group of people that may include [[sit-in]]s and other forms of obstructions, riots, sabotage and other forms of crime, and which is intended to be a demonstration to the public and the government, but can escalate into general chaos||Contact local police or law enforcement<ref>[http://www.usfa.fema.gov/downloads/pdf/publications/fa-142.pdf \'\'Report of the Joint Fire/Police Task Force on Civil Unrest (FA-142): Recommendations for Organization and Operations During Civil Disturbance.\'\'] Page 55. FEMA. Retrieved 21 October 2012.</ref><ref>[http://www.xanaboo.com/BCP%20-%20Developing%20a%20Strategy%20to%20Minimize%20Risk%20and%20Maintain%20Operations.pdf \'\'Business Continuity Planning: Developing a Strategy to Minimize Risk and Maintain Operations.\'\'] {{wayback|url=http://www.xanaboo.com/BCP%20-%20Developing%20a%20Strategy%20to%20Minimize%20Risk%20and%20Maintain%20Operations.pdf |date=20140327234742 }} Adam Booher. Retrieved 19 September 2012.</ref> \n|-\n|[[Fire|Fire (urban)]]||Even with strict building fire codes, people still perish needlessly in fires||Attempt to suppress fire in early stages; Evacuate personnel on alarm, as necessary; Notify fire department; Shut off utilities; Monitor weather advisories\n|-\n|[[Hazardous material|Hazardous material spills]]||The escape of solids, liquids, or gases that can harm people, other living organisms, property or the environment, from their intended controlled environment such as a container.||Leave the area and call the local fire department for help.<ref>[http://www.tnema.org/public/hazmat.html \'\'Hazardous Materials.\'\'] {{wayback|url=http://www.tnema.org/public/hazmat.html |date=20121011150052 }} Tennessee Emergency Management Office. Retrieved 7 September 2012.</ref> If anyone was affected by the spill, call the your local Emergency Medical Services line<ref>[http://www.atsdr.cdc.gov/MHMI/index.asp \'\'Managing Hazardous Materials Incidents (MHMIs).\'\'] Center for Disease Control. Retrieved 7 September 2012.</ref>\n|-\n||[[Nuclear and radiation accidents|Nuclear and Radiation Accidents]]||An event involving significant release of radioactivity to the environment or a reactor core meltdown and which leads to major undesirable consequences to people, the environment, or the facility||Recognize that a CBRN incident has or may occur. Gather, assess and disseminate all available information to first responders. Establish an overview of the affected area. Provide and obtain regular updates to and from first responders.<ref>[http://www.nato.int/docu/cep/cep-cbrn-response-e.pdf \'\'Guidelines for First Response to a CBRN Incident.\'\'] Project on Minimum Standards and Non-Binding Guidelines for First Responders Regarding Planning, Training, Procedure and Equipment for Chemical, Biological, Radiological and Nuclear (CBRN) Incidents.] NATO. Emergency Management. Retrieved 21 October 2012.</ref>\n|-\n|[[Power Failure]]||Caused by summer or winter storms, lightning or construction equipment digging in the wrong location||Wait 5–10 minutes; Power-off all Servers after a graceful shutdown; Do not use telephones, in the event of severe lightning; Shut down main electric circuit usually located in the basement or the first floor\n|-\n|}\n\nIn the realm of information technology per se, disasters may also be the result of a computer security exploit. Some of these are: [[computer virus]]es, [[cyberattack]]s, [[denial-of-service attack]]s, [[hacker (computer security)|hacking]], and [[malware]] exploits. These are ordinarily attended to by [[information security]] experts.\n\n==Planning methodology==\n\nAccording to Geoffrey H. Wold of the Disaster Recovery Journal, the entire process involved in developing a Disaster Recovery Plan consists of 10 steps:<ref name="DR journal" />\n\n===Obtaining top management commitment===\nFor a disaster recovery plan to be successful, the central responsibility for the plan must reside on [[Management#Top-level managers|top management]]. Management is responsible for coordinating the disaster recovery plan and ensuring its effectiveness within the organization. It is also responsible for allocating adequate time and resources required in the development of an effective plan. Resources that management must allocate include both financial considerations and the effort of all personnel involved.\n\n===Establishing a planning committee===\nA [[plan]]ning [[committee]] is appointed to oversee the development and implementation of the plan. The planning committee includes representatives from all functional areas of the organization. Key committee members customarily include the operations manager and the data processing manager. The committee also defines the scope of the plan.\n\n===Performing a risk assessment===\nThe planning committee prepares a [[Probabilistic risk assessment|risk analysis]] and a [[business impact analysis]] (BIA) that includes a range of possible disasters, including natural, technical and human threats. Each functional area of the organization is analyzed to determine the potential consequence and impact associated with several disaster scenarios. The risk assessment process also evaluates the safety of critical documents and vital records. Traditionally, fire has posed the greatest threat to an organization. Intentional human destruction, however, should also be considered. A thorough plan provides for the “worst case” situation: destruction of the main building. It is important to assess the impacts and consequences resulting from loss of information and services. The planning committee also analyzes the costs related to minimizing the potential exposures.\n\n===Establishing priorities for processing and operations===\nAt this point, the critical needs of each department within the organization are evaluated in order to prioritize them. Establishing [[Wiktionary:priority|priorities]] is important because no organization possesses infinite resources and criteria must be set as to where to allocate resources first. Some of the areas often reviewed during the prioritization process are functional operations, key personnel and their functions, information flow, processing systems used, services provided, existing documentation, historical records, and the department\'s policies and procedures.\n\nProcessing and operations are analyzed to determine the maximum amount of time that the department and organization can operate without each critical system. This will later get mapped into the [[Recovery Time Objective]]. A critical system is defined as that which is part of a system or procedure necessary to continue operations should a department, computer center, main facility or a combination of these be destroyed or become inaccessible. A method used to determine the critical needs of a department is to document all the functions performed by each department. Once the primary functions have been identified, the operations and processes are then ranked in order of priority: essential, important and non-essential.\n\n===Determining recovery strategies===\nDuring this phase, the most practical alternatives for processing in case of a disaster are researched and evaluated. All aspects of the organization are considered, including [[Building|physical facilities]], [[computer hardware]] and [[software]], [[communications link]]s, [[data file]]s and [[database]]s, [[customer service]]s provided, user operations, the overall [[management information system]]s (MIS) structure, [[end-user]] systems, and any other processing operations.\n\nAlternatives, dependent upon the evaluation of the computer function, may include: [[hot site]]s, [[warm site]]s, [[cold site]]s, [[reciprocal agreement (disaster preparedness)|reciprocal agreements]], the provision of more than one data center, the installation and deployment of multiple computer system, duplication of service center, [[consortium]] arrangements, lease of equipment, and any combinations of the above.\n\nWritten [[Contract|agreements]] for the specific recovery alternatives selected are prepared, specifying contract duration, termination conditions, [[system testing]], [[cost]], any special security procedures, procedure for the notification of system changes, hours of operation, the specific hardware and other equipment required for processing, personnel requirements, definition of the circumstances constituting an [[emergency]], process to negotiate service extensions, guarantee of [[Computer compatibility|compatibility]], [[availability]], non-mainframe resource requirements, priorities, and other contractual issues.\n\n===Collecting data===\nIn this phase, data collection takes place. Among the recommended data gathering materials and documentation often included are\nvarious lists (employee backup position listing, critical telephone numbers list, master call list, master vendor list, notification checklist), inventories (communications equipment, documentation, office equipment, forms, [[insurance policy|insurance policies]], workgroup and data center computer hardware, [[microcomputer]] hardware and software, [[office supplies|office supply]], off-site storage location equipment, telephones, etc.), distribution register, software and data files backup/retention schedules, temporary location specifications, any other such other lists, materials, inventories and documentation. Pre-formatted forms are often used to facilitate the data gathering process.\n\n===Organizing and documenting a written plan===\nNext, an outline of the plan’s contents is prepared to guide the development of the detailed procedures. Top management reviews and approves the proposed plan. The outline can ultimately be used for the [[table of contents]] after final revision. Other four benefits of this approach are that (1) it helps to organize the detailed procedures, (2) identifies all major steps before the actual writing process begins, (3) identifies redundant procedures that only need to be written once, and (4) provides a [[plan|road map]] for developing the procedures.\n\nIt is often considered [[best practice]] to develop a standard format for the disaster recovery plan so as to facilitate the writing of detailed procedures and the documentation of other information to be included in the plan later. This helps ensure that the disaster plan follows a consistent format and allows for its ongoing future maintenance. [[Standardization]] is also important if more than one person is involved in writing the procedures.\n\nIt is during this phase that the actual written plan is developed in its entirety, including all detailed procedures to be used before, during, and after a disaster. The procedures include methods for maintaining and updating the plan to reflect any significant internal, external or systems changes. The procedures allow for a regular review of the plan by key personnel within the organization. The disaster recovery plan is structured using a team approach. Specific responsibilities are assigned to the appropriate team for each functional area of the organization. Teams responsible for administrative functions, [[building|facilities]], [[logistics]], user support, [[backup|computer backup]], restoration and other important areas in the organization are identified.\n\nThe structure of the contingency organization may not be the same as the existing organization chart. The contingency organization is usually structured with teams responsible for major functional areas such as administrative functions, facilities, logistics, user support, computer backup, restoration, and any other important area.\n\nThe [[management team]] is especially important because it coordinates the recovery process. The team assesses the disaster, activates the recovery plan, and contacts team managers. The management team also oversees, documents and monitors the recovery process. It is helpful when management team members are the final decision-makers in setting priorities, policies and procedures. Each team has specific responsibilities that are completed to ensure successful execution of the plan. The teams have an assigned manager and an alternate in case the team manager is not available. Other team members may also have specific assignments where possible.\n\n===Developing testing criteria and procedures===\nBest practices dictate that DR plans be thoroughly tested and evaluated on a regular basis (at least annually). Thorough DR plans include documentation with the procedures for testing the plan. The tests will provide the organization with the assurance that all necessary steps are included in the plan. Other reasons for testing include:\n* Determining the feasibility and compatibility of backup facilities and procedures.\n* Identifying areas in the plan that need modification.\n* Providing training to the team managers and team members.\n* Demonstrating the ability of the organization to recover.\n* Providing motivation for maintaining and updating the disaster recovery plan.\n\n===Testing the plan===\nAfter testing procedures have been completed, an initial "[[Dry run (testing)|dry run]]" of the plan is performed by conducting a structured walk-through test. The test will provide additional information regarding any further steps that may need to be included, changes in procedures that are not effective, and other appropriate adjustments. These may not become evident unless an actual dry-run test is performed. The plan is subsequently updated to correct any problems identified during the test. Initially, testing of the plan is done in sections and after normal business hours to minimize disruptions to the overall operations of the organization. As the plan is further polished, future tests occur during normal business hours.\n\nTypes of tests include: checklist tests, simulation tests, parallel tests, and full interruption tests.\n\n===Obtaining plan approval===\nOnce the disaster recovery plan has been written and tested, the plan is then submitted to management for approval. It is top management’s ultimate responsibility that the organization has a documented and tested plan. Management is responsible for (1) establishing the policies, procedures and responsibilities for comprehensive [[contingency plan]]ning, and (2) reviewing and approving the contingency plan annually, documenting such reviews in writing.\n\nOrganizations that receive information processing from [[service bureau]]s will, in addition, also need to (1) evaluate the adequacy of contingency plans for its service bureau, and (2)ensure that its contingency plan is compatible with its service bureau’s plan.\n\n==Caveats/controversies==\n\nDue to its high cost, disaster recovery plans are not without critics. [[Cormac Foster]] has identified five "common mistakes" organizations often make related to disaster recovery planning:<ref>[https://web.archive.org/web/20130116112225/http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx \'\'Five Mistakes That Can Kill a Disaster Recovery Plan. In archive.org\'\'] Cormac Foster. Dell Corporation. 25 October 2010. Retrieved 8 August 2012.</ref>\n\n===Lack of buy-in===\nOne factor is the perception by executive management that DR planning is "just another fake earthquake drill" or CEOs that fail to make DR planning and preparation a priority, are often significant contributors to the failure of a DR plan.\n\n===Incomplete RTOs and RPOs===\nAnother critical point is failure to include each and every important business process or a block of data. "Every item in your DR plan requires a Recovery Time Objective (RTO) defining maximum process downtime or a Recovery Point Objective (RPO) noting an acceptable restore point. Anything less creates ripples that can extend the disaster\'s impact." As an example, "payroll, accounting and the weekly customer newsletter may not be mission-critical in the first 24 hours, but left alone for several days, they can become more important than any of your initial problems."\n\n===Systems myopia===\nA third point of failure involves focusing only on DR without considering the larger business continuity needs: "Data and systems restoration after a disaster are essential, but every business process in your organization will need IT support, and that support requires planning and resources." As an example, corporate office space lost to a disaster can result in an instant pool of teleworkers which, in turn, can overload a company\'s [[VPN]] overnight, overwork the IT support staff at the blink of an eye and cause serious bottlenecks and monopolies with the dial-in PBX system.\n\n===Lax security===\nWhen there is a disaster, an organization\'s data and business processes become vulnerable. As such, security can be more important than the raw speed involved in a disaster recovery plan\'s RTO. The most critical consideration then becomes securing the new data pipelines: from new VPNs to the connection from offsite backup services. Another security concern includes documenting every step of the recovery process—something that is especially important in highly regulated industries, government agencies, or in disasters requiring post-mortem forensics. Locking down or remotely wiping lost handheld devices is also an area that may require addressing.\n\n===Outdated plans===\nAnother important aspect that is often overlooked involves the frequency with which DR Plans are updated. Yearly updates are recommended but some industries or organizations require more frequent updates because business processes evolve or because of quicker data growth. To stay relevant, disaster recovery plans should be an integral part of all [[business analysis]] processes, and should be revisited at every major corporate acquisition, at every new product launch and at every new system development milestone.\n\n==See also==\n* [[Disaster recovery]]\n* [[Business continuity planning]]\n* [[Federal Emergency Management Agency]]\n* [[Backup rotation scheme]]\n* [[Seven tiers of disaster recovery]]\n\n==References==\n{{reflist|2}}\n\n{{DEFAULTSORT:Disaster recovery plan}}\n[[Category:Disaster recovery]]\n[[Category:Data management]]\n[[Category:Backup]]\n[[Category:IT risk management]]\n[[Category:Planning]]']
['Data access', '1582494', '{{Refimprove|date=September 2014}}\n\'\'\'Data access\'\'\' typically refers to software and activities related to storing, retrieving, or acting on [[data]] housed in a [[database]] or other [[Information repository|repository]]. Two fundamental types of data access exist:\n\n# [[sequential access]] (as in [[Magnetic tape data storage|magnetic tape]], for example)\n# [[random access]] (as in indexed [[Digital media|media]])\n\nData access crucially involves [[authorization]] to access different data repositories. Data access can help distinguish the abilities of administrators and users. For example, administrators may have the ability to remove, edit and add data, while general users may not even have "read" rights if they lack access to particular information.\n\nHistorically, each repository (including each different database, [[file system]], etc.), might require the use of different [[Method (computer science)|methods]] and [[languages]], and many of these repositories stored their content in different and incompatible formats.\n\nOver the years standardized languages, methods, and formats, have developed to serve as interfaces between the often proprietary, and always idiosyncratic, specific languages and methods.  Such standards include [[SQL]] (1974- ), [[ODBC]] (ca 1990- ), [[JDBC]], [[XQuery API for Java|XQJ]], [[ADO.NET]], [[XML]], [[XQuery]], [[XPath]] (1999- ), and [[Web Services]].\n\nSome of these standards enable translation of data from [[unstructured data|unstructured]] (such as HTML or free-text files) to [[structured data|structured]] (such as [[XML]] or [[SQL]]).\n\nStructures such as [[connection string]]s and DBURLs<ref>\n{{cite web\n| url           = http://www.quickprogrammingtips.com/java/connecting-to-oracle-database-in-java.html\n| title         = Connecting to Oracle Database in Java\n| accessdate    = 2014-07-18\n| quote         = DBURL is of the form [...] jdbc:oracle:thin:@machinename:1521:databasename [...]\n}}\n</ref>\ncan attempt to standardise methods of [[Database connection|connecting to databases]].\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Data Access}}\n[[Category:Data management]]\n[[Category:Data access technologies| ]]\n\n\n{{Database-stub}}']
['XML database', '1442351', '{{multiple issues|\n{{refimprove|date=August 2011}}\n{{update|date=March 2015}}\n}}\n\nAn \'\'\'XML database\'\'\' is a [[data persistence]] software system that allows data to be specified, and sometimes stored, in [[XML]] format. This data can be [[XQuery|queried]], transformed, exported and returned to a calling system. XML databases are a flavor of [[document-oriented database]]s which are in turn a category of [[NoSQL]] database.\n\n== Rationale for XML in databases ==\nThere are a number of reasons to directly specify data in XML or other document formats such as JSON. For XML in particular, they include:<ref name=nicola2010>{{cite web|last1=Nicola|first1=Matthias|title=5 Reasons for Storing XML in a Database|url=http://nativexmldatabase.com/2010/09/28/5-reasons-for-storing-xml-in-a-database/|website=Native XML Database|accessdate=17 March 2015|date=28 September 2010}}</ref>\n<ref name=feldman2013>{{cite conference|last1=Feldman|first1=Damon|title=Moving from Relational Modeling to XML and MarkLogic Data Models|url=http://www.marklogic.com/resources/slides-moving-from-relational-modeling-to-xml-and-marklogic-data-models/resource_download/presentations/|conference=MarkLogic World|conferenceurl=http://world.marklogic.com/|date=11 April 2013|accessdate=17 March 2015}}</ref>\n* An enterprise may have a lot of XML in an existing standard format\n* Data may need to be exposed or ingested as XML, so using another format such as relational forces double-modeling of the data\n* XML is very well suited to sparse data, deeply nested data and mixed content (such as text with embedded markup tags)\n* XML is human readable whereas relational tables require expertise to access\n* Metadata is often available as XML\n* Semantic web data is available as RDF/XML\n\nSteve O\'Connell gives one reason for the use of XML in databases: the increasingly common use of XML for [[transport layer|data transport]], which has meant that "data is extracted from databases and put into XML documents and vice-versa".<ref name=oconnell2005>{{cite report|author=O\'Connell, Steve|work=Advanced Databases Course Notes|title="Section 9.2"|type=Syllabus|date=2005|publisher=[[University of Southampton]]|location=Southampton, England}}</ref>{{update inline|date=March 2015}} It may prove more efficient (in terms of conversion costs) and easier to store the data in XML format.  In content-based applications, the ability of the native XML database also minimizes the need for extraction or entry of metadata to support searching and navigation.\n\n== XML Enabled databases ==\nXML enabled databases typically offer one or more of the following approaches to storing XML within the traditional relational structure:\n#XML is stored into a CLOB ([[Character large object]])\n#XML is `shredded` into a series of Tables based on a Schema<ref name=oracle>{{cite book|title=Oracle XML DB Developer\'s Guide, 10\'\'g\'\' Release 2|date=August 2005|publisher=Oracle Corporation|chapter-url=http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb05sto.htm|accessdate=17 March 2015|chapter=XML Schema Storage and Query: Basic}}. Section [http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb05sto.htm#i1042421 Creating XMLType Tables and Columns Based on XML Schema]</ref>\n#XML is stored into a native XML Type as defined by ISO Standard 9075-14<ref name=iso9075-2011>{{cite web|title=ISO/IEC 9075-14:2011: Information technology -- Database languages -- SQL -- Part 14: XML-Related Specifications (SQL/XML)|url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=53686|publisher=[[International Organization for Standardization]]|accessdate=17 March 2015|date=2011}}</ref>\n\nRDBMS that support the ISO XML Type are:\n#IBM DB2 (pureXML<ref name=db2purexml>{{cite web|title=pureXML overview -- DB2 as an XML database|url=http://www-01.ibm.com/support/knowledgecenter/SSEPGG_10.1.0/com.ibm.db2.luw.xml.doc/doc/c0022308.html|website=IBM Knowledge Center|publisher=[[IBM]]|accessdate=17 March 2015}}</ref>)\n#Microsoft SQL Server<ref name=sqlserver2005>{{cite web|title=Using XML in SQL Server|url=https://msdn.microsoft.com/en-us/library/ms190936.aspx|website=Microsoft Developer Network|publisher=[[Microsoft Corporation]]|accessdate=17 March 2015}}</ref>\n#Oracle Database<ref name=oracle2>{{cite book|title=Oracle XML DB Developer\'s Guide, 10\'\'g\'\' Release 2|date=August 2005|publisher=Oracle Corporation|chapter-url=http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb04cre.htm|accessdate=17 March 2015|chapter=XMLType Operations}}</ref>\n#PostgreSQL<ref name=postgresql>{{cite book|title=PostgreSQL 9.0.19 Documentation|chapter-url=http://www.postgresql.org/docs/9.0/static/datatype-xml.html|accessdate=17 March 2015|chapter=8.13. XML Type}}</ref> <ref>[http://www.postgresql.org/docs/9.0/static/datatype-xml.html PostgreSQL - Data Types - XML Type]</ref>\n\nTypically an XML enabled database is best suited where the majority of data are non-XML. For datasets where the majority of data are XML, a [[#Native XML databases|native XML database]] is better suited.\n\n=== Example of XML Type Query in IBM DB2 SQL ===\n<source lang="sql">\nselect\n   id, vol, xmlquery(\'$j/name\', passing journal as "j") as name\nfrom\n   journals\nwhere \n   xmlexists(\'$j[licence="CreativeCommons"]\', passing journal as "j")\n</source>\n\n== Native XML databases ==\nThese databases are typically better when much of the data is in XML or other non-relational formats.{{fact|date=August 2015}}\n* [[BaseX]]\n* [[Berkeley DB]] XML Edition \n* [[eXist]]\n* [[MarkLogic Server]]\n* [[Qizx]]\n* [[Sedna (database)|Sedna]]\n\nAll the above databases uses XML as an interface to specify documents as tree structured data that may contain unstructured text, but on disk the data is stored as "optimized binary files." This makes query and retrieval faster. For MarkLogic it also allows XML and JSON to co-exist in one binary format.<ref>{{cite book|last1=Siegel|first1=Erik|last2=Retter|first2=Adam|title=eXist|date=December 2014|publisher=O\'Reilly & Associates|isbn=978-1-4493-3710-0|url=https://www.safaribooksonline.com/library/view/exist/9781449337094/ch04.html|accessdate=18 March 2015|chapter=4. Architecture}}</ref>\n\nKey features of native XML databases include:\n\n* Has an [[XML]] document as at least one fundamental unit of (logical) storage, just as a [[relational database]] has a row in a table as a fundamental unit of (logical) storage.\n* Need not have any particular underlying physical storage model. For example, NXDs can use optimized, proprietary storage formats. This is a key aspect of XML databases. Managing XML as large strings is inefficient due to the extra markup in XML. Compressing and indexing XML allows the illusion of directly accessing, querying and transforming XML while gaining the performance advantages of working with optimized binary tree structures.<ref name=kellogg2010>{{cite web|last1=Kellogg|first1=Dave|title=Yes, Virginia, MarkLogic is a NoSQL System|url=http://kellblog.com/2010/04/11/yes-virginia-marklogic-is-a-nosql-system/|website=Kellblog|accessdate=18 March 2015|date=11 April 2010}}</ref>\n\nThe standards for XML querying per W3C recommendation are [[XQuery]] 1.0 and XQuery 3.0.{{citation needed|date=March 2015}} XQuery includes [[XPath]] as a sub-language and XML itself is a valid sub-syntax of XQuery.\n\nIn addition to XPath, XML databases support [[XSLT]] as a method of transforming documents or query-results retrieved from the database. XSLT provides a [[declarative language]] written using an XML grammar. It aims to define a set of XPath [[Filter (software)|filter]]s that can transform documents (in part or in whole) into other formats including [[plain text]], XML, or [[HTML]].\n\nBut big picture, XML persistence describes only one format in the larger, faster moving [[NoSQL]] movement at this time. Many databases support XML plus other formats, even if XML is internally stored as an optimized, high-performance format and is a first-class citizen within the database. (see Google Trends Link above to see relative popularity of terms).\n\n=== Language features  ===\n{| class="wikitable sortable"\n|-\n! Name\n! License\n! Native Language\n! XQuery 3.0\n! XQuery Update\n! XQuery Full Text\n! EXPath Extensions\n! EXQuery Extensions\n! XSLT 2.0\n|-\n| BaseX\n| [[BSD License]]\n| Java\n| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}\n|-\n| eXist\n| [[LGPL|LGPL License]]\n| Java\n| {{Partial}} || {{Proprietary}} || {{Proprietary}} ||  {{No}} || {{Yes}} || {{Yes}}\n|-\n| MarkLogic Server\n| [[Commercial software|Commercial]]\n| C++\n| {{Partial}} ||  {{Proprietary}} || {{Proprietary}} || {{No}}  || {{No}} || {{Yes}}\n|-\n| Qizx\n| [[Commercial software|Commercial]]\n| Java\n| {{Yes}} || {{Yes}} || {{Yes}} || {{No}} || {{No}} || {{Yes}}\n|}\n\n=== Supported APIs ===\n{| class="wikitable sortable"\n|-\n! Name\n! [[XQuery API for Java|XQJ]]\n! XML:DB\n! [[Representational State Transfer|RESTful]]\n! RESTXQ\n! WebDAV\n|-\n| BaseX\n| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}\n|-\n| eXist\n| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}\n|-\n| MarkLogic Server\n| {{Yes}} || {{No}} || {{Yes}} || {{Yes}} || {{Yes}}\n|-\n| Qizx\n| {{No}} || {{No}} || {{Yes}} || {{No}} || {{No}}\n|-\n| Sedna\n| {{Yes}} || {{Yes}} || {{No}} || {{No}} || {{No}}\n|}\n\n== References ==\n{{reflist}}\n{{Refbegin}}\n\n== External links ==\n* [https://web.archive.org/web/20150906171257/http://db-engines.com/en/ranking/native+xml+dbms DB-Engines Ranking of Native XML DBMS] by popularity, updated monthly\n* [http://www.cfoster.net/articles/xmldb-business-case XML Databases - The Business Case, Charles Foster, June 2008] - Talks about the current state of Databases and data persistence, how the current Relational Database model is starting to crack at the seams and gives an insight into a strong alternative for today\'s requirements.\n* [http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-3717 An XML-based Database of Molecular Pathways (2005-06-02)] Speed / Performance comparisons of eXist, X-Hive, Sedna and Qizx/open\n* [https://web.archive.org/web/20070922082133/http://swing.felk.cvut.cz/index.php?option=com_docman&task=doc_view&gid=5&Itemid=62 XML Native Database Systems: Review of Sedna, Ozone, NeoCoreXMS] 2006\n* [http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/ic/&toc=comp/mags/ic/2005/02/w2toc.xml&DOI=10.1109/MIC.2005.48 XML Data Stores: Emerging Practices]\n* Bhargava, P.; Rajamani, H.; Thaker, S.; Agarwal, A. (2005) \'\'XML Enabled Relational Databases\'\', Texas, The University of Texas at Austin.\n* [https://web.archive.org/web/20070113224941/http://xmldb-org.sourceforge.net/ Initiative for XML Databases]\n* [http://www.rpbourret.com/xml/XMLAndDatabases.htm  XML and Databases, Ronald Bourret, September 2005]\n* [https://web.archive.org/web/20071011101718/http://cafe.elharo.com/xml/the-state-of-native-xml-databases/  The State of Native XML Databases, Elliotte Rusty Harold, August 13, 2007]\n* {{Official website|https://www.qualcomm.com/qizx|Qualcomm Qizx official website}}{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n{{Refend}}\n\n{{Database models}}\n{{Databases}}\n\n[[Category:XML]]\n[[Category:Data management]]\n[[Category:Data modeling]]\n[[Category:XML databases| ]]']
['Approximate inference', '39019965', "'''Approximate inference''' methods make it possible to learn realistic models from [[big data]] by trading off computation time for accuracy, when exact learning and [[inference]] are [[computationally intractable]].\n\n==Major methods classes ==\n\n*[[Variational Bayesian method]]s \n*[[Expectation propagation]]\n*[[Markov random field]]s \n*[[Bayesian network]]s\n**[[Variational message passing]]\n*loopy and generalized [[belief propagation]] \n<ref>{{cite journal|url=http://academic.research.microsoft.com/Paper/14666.aspx|title=Approximate Inference and Constrained Optimization|journal=Uncertainty in Artificial Intelligence - UAI|pages=313–320|year=2003}}</ref><ref>{{cite web|url=http://mlg.eng.cam.ac.uk/zoubin/approx.html|title=Approximate Inference|accessdate=2013-07-15}}</ref>\n\n==See also==\n*[[Statistical inference]]\n*[[fuzzy logic]]\n*[[data mining]]\n\n==References==\n{{reflist}}\n\n==External links==\n*{{cite web|url=http://videolectures.net/mlss09uk_minka_ai/|title=Machine Learning Summer School (MLSS), Cambridge 2009, Approximate Inference|author= Tom Minka, Microsoft Research|date=Nov 2, 2009|type=video lecture}}\n\n[[Category:Data management]]"]
['XSA', '15323614', "In computer science, '''XSA''' (better known as '''Cross-Server Attack''') is a networking security intrusion method which allows for a malicious client to compromise security over a website or service on a server by using implemented services on the server that may not be secure.\n\nIn general, XSA is demonstrated against websites, yet sometimes it is used in conjunction with other services located on the same server.\n\n== Basics ==\nXSA is a method that allows for a malicious client to use services that a remote server implements in order to attack another service on the same server or network.\n\nMost website hosting companies that offer hosting for large or even little amounts of separate websites are vulnerable to this method of attack, because of the amount of access services such as [[PHP]] and the webserver itself give to a client that allows the client to access other website configurations, files, passwords and the like.\n\n== History ==\n\nThe term 'XSA' was first coined by DeadlyData, a prominent [[Computer hacker]] during the early 2000s, over the voice communications software [[TeamSpeak]]. While he had not invented or pioneered this method of intrusion, he coined it as a shortened term to describe the act of performing Cross-Server Attacks (XSAs).\n\nIt was then used further in the community and now supports for most of the methods and subsets of the method that give both [[Computer hacker]] and malicious individuals the terminology to attack websites using software that is located on the same server.\n\n== See also ==\n{{Portal|Software Testing}}\n*[[SQL Injection]] (SQLi)\n*[[Cross-Site Scripting]] (XSS)\n*[[Cross-Site Request Forgery]] (CSRF)\n*[[Buffer Overflow]]\n\n{{DEFAULTSORT:XSA}}\n[[Category:Data management]]\n[[Category:Computer security exploits]]\n[[Category:Computer network security]]\n[[Category:World Wide Web]]\n[[Category:Web development]]"]
['Managed Memory Computing', '40119072', '{{multiple issues|\n{{Orphan|date=November 2013}}\n{{unreferenced|date=November 2013}}\n}}\n\n== This article pertains to a new technology used in Business Intelligence. Managed Memory Computing uses aggregated data for in-memory analytics   ==\n\nAggregated data cubes are the most effective form of storage of aggregated or summarized data for quick analysis. This technology is driven by [[Online Analytical Processing|Online Analytical Processing technology]]. Utilizing these data cubes involves intense disk I/O operations. This at times lowers the speed for users of data.\n\nConventional, [[In-Memory Processing|in-memory processing]] does not rely on stored and summarized or aggregated data but brings all the relevant data to the memory. This technology then utilizes intense processing and large amounts of memory to perform all calculations and aggregations while in memory.\n\nManaged Memory Computing blends the best of both methods, allowing users to define data cubes with per-structured and aggregated data, providing a logical business layer to users, and offering in-memory computation. These features make the response time for user interactions far superior and enable the most balanced approach between disk I/O and in-memory processing.\n\nThe hybrid approach of Managed Memory Computing provides analysis, dashboards, graphical interaction, ad hoc querying, presentation, and discussion driven analytic at blazing speeds, making the [[Business intelligence|Business Intelligence Tool]] ready for everything from an interactive session in the boardroom to a [[production planning]] meeting on the factory floor.\n\n== References ==\n\n[http://www.cioreview.in/magazine/ElegantJ-BI-Managed-Memory-Computing--Business-Intelligence-Redefined-CZSI499492332.html Introduction of Managed Memory Computing in CIOReview]\n\n[[Category:Business intelligence]]\n[[Category:Financial data analysis]]\n[[Category:Data management]]\n[[Category:Computer architecture]]']
['Datafication', '41731546', '\'\'\'Datafication\'\'\' is a modern technological trend turning many aspects of our life into computerised data <ref name="CukierMayer-Schoenberger2013">{{cite journal | last = Cukier | first =Kenneth | last2 = Mayer-Schoenberger | first2 = Viktor  | title =The Rise of Big Data | journal =Foreign Affairs | issue =May/June | pages = 28–40. | date =2013  | url = http://www.foreignaffairs.com/articles/139104/kenneth-neil-cukier-and-viktor-mayer-schoenberger/the-rise-of-big-data | accessdate = 24 January 2014}}</ref> and transforming this information into new forms of value. \n<ref name="SchuttOneil2014">\n{{cite book\n | last = O\'Neil | first =Cathy\n | last2 =Schutt\n | first2 = Rachel\n| title =Doing Data Science\n | publisher =O’Reilly Media\n | date =2013\n | pages =406\n  | isbn =978-1-4493-5865-5\n }}\n</ref>\nExamples of datafication as applied to social and communication media are how [[Twitter]] datafies stray thoughts or datafication of [[Human resource management|HR]] by [[LinkedIn]] and others.  Alternative examples are diverse and include aspects of the built environment, and design via engineering and or other tools that tie data to formal, functional or other physical media outcomes of which [[Formsolver]]<ref>https://www.formsolver.com</ref> is an example.\n\n[[File:Shape optimization for buildings by formsolver.jpg|thumbnail|right|Example: Datafication of the skin and form of a building to assist engineers, designers and architects determine the performance of particular building geometries. Example provided courtesy of Formsolver.com]]\n[[File:Emerging Shape Optimization Families for buildings by formsolver.jpg|thumbnail|right|Example: Shape families resulting from differing goals when data is used for the purposes of shape optimization. Example provided courtesy of Formsolver.com]]\n\n==See also==\n* [[Big data]]\n\n==References==\n{{Reflist}}\n\n[[Category:Information science]]\n[[Category:Technology forecasting]]\n[[Category:Data management]]\n[[Category:Big data]]\n[[Category:Information society]]\n\n\n{{Tech-stub}}']
['PhUSE', '43000868', "{{unreferenced|date=June 2014}}\n[[File:PhUSE Computational Science Symposium 2016 (26133831630).jpg|thumb|PhUSE Computational Science Symposium 2016]]\n'''PhUSE''', or '''Pharmaceutical Users Software Exchange''' is an independent, [[not-for-profit]] organization, that started in Europe, but now which serves as forum and global platform for [[clinical data management]], [[biostatistics]], and [[eClinical]] [[information technology]] professionals. It provides three collaboration platforms for members, a set online suits which implements worldwide collaboration tools, a [[wiki]], a repository of videos (PhUSE Tube), a [[blog]], a [[webforum]] and an [[archive]].\n\nPhUSE also publishes '''[[Pharmaceutical Programming]]''',  an [[academic journal]] focusing on programming for [[Drug regulation|drug regulation environments]] of the [[pharmaceutical]] industry, a quarterly [[newsletter]], '''PhUSE News'''. In addition, it organizes an annual [[Meeting|conference]].\n\n==External links==\n\n* [http://www.phuse.eu/ Official web site]\n* [http://www.phusewiki.org/wiki/index.php?title=PhUSE_Wiki PhUSE Wiki]\n* [http://www.phuse.eu/blog/blog.aspx Blog]\n* [http://www.phuse.eu/Society-Newsletters.aspx Newsletter]\n* [http://www.phuse.eu/forum.aspx Forum]\n* [http://www.phuse.eu/archive.aspx Archive]\n* [http://www.phuse.eu/phusetube.aspx PhUSE Tube]\n* [http://www.phuse.eu/publications.aspx Publications]\n\n[[Category:Pharmacy organizations]]\n[[Category:Regulation|Therapeutic goods]]\n[[Category:Biostatistics]]\n[[Category:Data management]]"]
['Personal, Inc.', '43509883', '{{Infobox company |\n| name=Personal, Inc.\n| logo=[[File:Personal, Inc. logo.png]]\n| type=[[Private company|Private]]\n| foundation=2009\n| location=[[Washington, D.C.]], US\n| industry=[[Internet]]\n| homepage={{URL|www.personal.com}}\n}}\n\n\'\'\'Personal\'\'\' (also referred to as Personal.com or Personal, Inc.) was a consumer [[Personal Data Service]] and [[identity management system]] for individuals to aggregate, manage and reuse their own data. It was re-launched in May 2016 as a collaborative data management and security solution for the workplace called TeamData.<ref>{{Cite web|url=http://www.prnewswire.com/news-releases/personalcom-becomes-teamdata-300275063.html|title=Personal.com Becomes TeamData|last=TeamData|website=www.prnewswire.com|access-date=2016-10-03}}</ref>\n\nPersonal\'s consumer products included: the Data Vault with Cloud Sync for secure management and sharing of data and documents between an individual and other individuals, companies, sites, apps and devices; and Data Imports to import information from third parties, including [[Social networking services|social media services]], companies and the [[United States Department of Education|U.S. Department of Education]], and  the Fill It App for automated completion of web and mobile forms, logins and checkouts.\n\nThe Personal platform supported user-centric [[DataPortability|data management and portability]] for over 1,200 different types (or fields) of structured, machine-readable, human-readable data. The platform also provided tools and APIs for developers and companies to integrate Fill It and the Data Vault into their websites and applications, primarily to give data back to their customers so they can autofill web and mobile forms.\n\n==History==\nPersonal was founded in 2009 in [[Washington, DC]] by the management team that built The Map Network, which was acquired by [[Nokia]]/[[Navteq|NAVTEQ]] in 2006.<ref name=acquisition>{{cite web |url=http://www.directionsmag.com/pressreleases/navteq-announces-agreement-to-acquire-the-map-network/110396 |title=NAVTEQ Announces Agreement to Acquire The Map Network |date=6 December 2006 |website=Directions Magazine |accessdate=21 August 2014}}</ref> Founded in 1999, The Map Network (previously called URHere.com) built the first platform for places and events to create and distribute digital online and mobile maps, location data and content. The Map Network served as the official mapping solution for over 100 cities and thousands of events and venues, from the NFL Super Bowl to the Democratic and Republican National Conventions to the Smithsonian Institution. It also produced the most-used interactive map of 9/11 relief and rescue efforts.<ref name=acquisition /><ref>{{cite web |url=http://www.prnewswire.com/news-releases/interactive-relief-and-rescue-map-aids-in-nyc-response-72052587.html |title=Interactive Relief and Rescue Map Aids in NYC Response |date=17 September 2001 |website=PR Newswire |accessdate=25 August 2014}}</ref><ref>{{cite web |url=http://spatialnews.geocomm.com/dailynews/2001/sep/11/ |title=The Geospatial Industry\'s Response to Terrorism |date=11 September 2001 |website=GeoCommunity |accessdate=26 August 2014}}</ref>\n\nCalled a “life management platform” by [[The Economist]]<ref>{{cite web |url=http://www.economist.com/blogs/babbage/2011/11/personal-data |title=A life-management platform? |last=L. |first=G. |date=17 November 2011 |website=The Economist |accessdate=8 August 2014}}</ref> and a “personal encrypted cloud service” by TIME for its user-centric approach to data,<ref name="time.com">{{cite web|url=http://time.com/3069834/how-to-take-control-of-your-personal-data/|title=How to Take Control of Your Personal Data|last=Stokes|first=Natasha|date=1 August 2014|website=Time Inc.|accessdate=8 August 2014}}</ref> the company has been associated with both the [[Infomediary]] model originated in 1999 by [[John Hagel III]] and Mark Singer, as well as the [[Vendor relationship management|vendor relationship management (VRM)]] model developed by Doc Searls. Personal closed $7.6m in funding in December 2010, including [[Steve Case]]’s Revolution Ventures, Grotech Ventures, [[Allen & Company]], [[Ted Leonsis]], Neil Ashe and [[Jonathan Miller (businessman)|Jonathan Miller]].<ref>{{cite web |url=http://techcrunch.com/2011/01/06/personal-raises-7m-from-steve-case-and-others-to-help-consumers-protect-their-digital-data/ |title=Personal Raises $7M From Steve Case And Others To Help Consumers Protect Their Digital Data |last=Rao |first=Leena |date=6 January 2011 |website=TechCrunch |accessdate=8 August 2014}}</ref>\n\nPersonal was early to embrace “small data,” which it defines as “big data for the benefit of individuals.”<ref>{{cite web |url=http://blog.personal.com/2012/03/the-era-of-small-data-begins/ |title=The Era of Small Data Begins |last=Green |first=Shane |date=6 March 2012 |website=Personal |accessdate=20 August 2014 }}</ref> The term “small data” may have been originally coined by [[Jeremie Miller]] of Sing.ly, who mentioned it in a talk at the Web 2.0 Summit in November 2011 and is cited in \'\'The Intention Economy\'\'.<ref>{{cite web |url=http://siliconprairienews.com/2011/11/watch-jeremie-miller-present-singly-at-the-web-2-0-summit/ |title=Watch Jeremie Miller present Singly at the Web 2.0 Summitt |first=Michael |date=9 November 2011 |website=Silicon Prairie News |accessdate=8 August 2014}}</ref> In 2011, Personal was a part of the first group of companies to join the [[Personal Data Ecosystem Consortium]]\'s \'\'Startup Circle.\'\'<ref>{{cite web |url=http://pde.cc/startup-circle/#2011 |title=Members of the PDEC Startup Circle |website=Personal Data Ecosystem Consortium |accessdate=20 August 2014}}</ref> A Small Data [[Meetup (website)|Meetup]] group has also formed in New York City, bringing together technology, legal and business experts to exchange ideas about user-centric and user-driven models for internet products and services.<ref>http://www.meetup.com/smalldata/</ref> Personal ultimately raised $24 million, including $4.5m from Bill Miller of [[Legg Mason]] and [[Esther Dyson]] of EDventures in October 2013.<ref>{{cite web |url=http://www.reuters.com/article/2013/10/15/idUS412005883920131015 |title=Personal raises $4.5 million to be the personal data vault we so desperately need |date=15 October 2013 |website=Reuters |accessdate=8 August 2014}}</ref>\n\n==Products and services ==\n\n===Overview===\nThe Personal Platform was a privacy- and security-by-design platform for individuals to manage and reuse their own data and information. The Fill It app was a 1-click form-filling solution for web and mobile logins, checkouts and forms, and the Data Vault app served as the main cloud-based repository for a user\'s data. Personal helped individuals take control and benefit from their information while knowing that the information in their Data Vault remained legally theirs and could not be used without their permission.<ref>{{Cite web|url=http://www.zdnet.com/article/intel-execs-on-big-data-and-privacy-its-a-balancing-act/|title=Intel execs on big data and privacy: It\'s a balancing act {{!}} ZDNet|last=King|first=Rachel|website=ZDNet|access-date=2016-10-03}}</ref>\n\n===Data Vault with Cloud Sync===\nPersonal spent two years building the Personal Platform before launching its Data Vault product in beta in November 2011. Following [[Privacy by Design]] principles, Personal only enabled users to see or share the sensitive data and all the files they stored in their Data Vault. Such information was encrypted, and could only be decrypted with a user’s password. Only users could choose and know their passwords to their vault because Personal did not store user passwords – and therefore could not reset them without deleting a user’s sensitive data and all files stored in their vault.<ref>{{cite web |url=http://www.ipc.on.ca/images/Resources/pbd-pde.pdf |title=Privacy by Design and the Emerging Personal Data Ecosystem |last=Cavoukian |first=Ann |last2=Green |first2=Shane |date=October 2012 |website=Office of the Information and Privacy Commissioner |accessdate=8 August 2014}}</ref> All Personal apps and services were linked to a user’s private Data Vault.\n\nThe Data Vault featured automatic synchronization of data and files added on any device logged into Personal. It also featured a “Secure Share” function that created a live, private network, allowing registered users to share access to data and files through an exchange of encrypted keys without the risk of transmitting the data or files through non-secure, direct means. It also allowed users to immediately update data across their own network and revoke access to it when they choose.\n\nPersonal launched its [[Android (operating system)|Android]] app on November 30, 2011.<ref name=mashable>{{cite web |url=http://mashable.com/2011/11/17/personal/ |title=Never Fill Out a Form Again? Personal Seeks to Be the Data Vault for Your Private Information |last=Parr |first=Ben |date=17 November 2011 |website=Mashable |accessdate=8 August 2014}}</ref><ref>{{cite web |url=https://www.personal.com/s/pages/news/personal-android-release/ |title=Personal Releases Android App for Its Private, Personal Network and Data Vault Service |date=30 November 2011 |website=Personal, Inc. |accessdate=8 August 2014}}</ref> The [[iOS]] Data Vault app was released on May 7, 2012.<ref>{{cite web |url=http://techcrunch.com/2012/05/07/personal-takes-its-secure-vault-for-all-of-your-private-digital-data-mobile-with-ios-app/ |title=Personal Takes Its Secure Vault For All Of Your Private, Digital Data Mobile WIth iOS App |last=Rao |first=Leena |date=7 May 2012 |website=TechCrunch |accessdate=8 August 2014}}</ref> Personal officially launched its [[Application programming interface|application programming interface (APIs)]] on October 2, 2012 at the Mashery Business of APIs Conference.<ref>{{cite web |url=https://www.personal.com/s/pages/news/personal-launches-personal-platform/ |title=Personal Launches \'Personal Platform at Business of APIs Conference, Opening APIs for Developers |date=2 October 2012 |website=Personal, Inc. |accessdate=8 August 2014}}</ref> A review by [[CNET]] highlighted the challenges of getting people to trust such a new service with their sensitive data and spending the time required entering enough data to make it useful.<ref>{{cite web |url=http://www.cnet.com/news/what-hump-personals-private-database-faces-challenges/ |title=What hump? Personal\'s private database faces challenges |last=Needleman |first=Rafe |date=30 November 2011 |website=CNET |accessdate=8 August 2014}}</ref>\n\n===Fill It App and Form Index===\nWhen the Data Vault was launched in November 2011, \'\'[[Mashable]]\'\' posed the question: “Never Fill Out a Form Again?”<ref name=mashable /> The [[World Economic Forum]] in its February 2013 report highlighted the possibility of saving 10 billion hours globally “and improv[ing] the delivery of public and private sector services” through automated form-filling tools, specifically citing Personal’s Fill It app.<ref>{{cite web |url=http://www3.weforum.org/docs/WEF_IT_UnlockingValuePersonalData_CollectionUsage_Report_2013.pdf |title=Unlocking the Value of Personal Data: From Collection to Usage |date=February 2013 |website=World Economic Forum |accessdate=8 August 2014}}</ref> In January 2013, Personal launched Fill It in beta as a web bookmarklet for automatic form-filling.<ref>{{cite web |url=http://www.digitaltrends.com/web/personal-coms-new-fill-it-feature-makes-quick-work-of-long-web-forms/#!bBp6iJ |title=PERSONAL.COM\'S NEW FILL IT APP MAKES QUICK WORK OF LONG ONLINE FORMS |last=Couts |first=Andrew |date=16 January 2013 |website=Digital Trends |accessdate=8 August 2014}}</ref>\n\nOn June 11, 2014, Personal released Fill It as a web extension and announced that it was publishing an index of over 140,000 1-click online forms at www.fillit.com.<ref name=extensionlaunch>{{cite web |url=http://tech.co/dc-based-startup-personal-launches-fill-it-for-quick-and-safe-auto-filling-on-online-forms-2014-06 |title=DC-Based Personal Launches Fill It for Quick and Safe Auto-Filling on Online Forms |last=Barba |first=Ronald |date=16 June 2014 |website=Tech Cocktail |accessdate=8 August 2014}}</ref> The company also announced that a mobile version of the product will launch later in the year. According to a story in \'\'[[Tech Cocktail]]\'\' about the launch, Personal’s “web extension and mobile app are able to support over 1,200 different types of reusable data, even enabling them to unlock more confidential information so they can complete longer forms, including patient registrations, job applications, event registrations, school admissions, insurance and bank applications, and government forms.”<ref name=extensionlaunch /> In November 2014, a mobile version of Fill It was launched that could autofill mobile forms using APIs.<ref>{{Cite web|url=http://tech.co/personal-launches-fill-it-mobile-2014-11|title=Personal Launches Fill It Mobile at #pii2014|date=2014-11-13|language=en-US|access-date=2016-10-03}}</ref>\n\nPersonal’s form portal ultimately indexed more than 500,000 forms with three components, which, together, allowed data to be captured and reused across any of the forms: (1) a form graph, which mapped individual form fields to the Personal ontology; (2) a semantic layer, which determined how data was required on a form (e.g. one field vs. three fields for a U.S. telephone number); and (3) a correlations graph, which helped individuals match their specific data to a form without looking at the data value (e.g. knowing which phone number is a mobile phone number, which address is a billing address, or that a person uses their middle name as a first name on most forms).<ref>{{cite web |url=http://tech.co/dc-startup-personal-university-data-privacy-security-2014-08 |title=Personal Launches "Personal University," a Video Series on Data Privacy and Security |last=Barba |first=Ronald |date=8 August 2014 |website=Tech Cocktail |accessdate=8 August 2014}}</ref>\n\n===Monetizing personal data===\nWith the [[initial public offering]] of [[Facebook]] in May 2012, there was media interest in the question of the monetary value of personal data and whether tools and services might emerge to help consumers monetize their own data. Personal was frequently cited as a company that could potentially offer such a service. Articles and pieces focusing on this subject have appeared in \'\'[[The New York Times]]\'\', \'\'[[AdWeek]]\'\', the \'\'[[MIT Technology Review]]\'\', and on \'\'[[CNN]]\'\' and \'\'[[National Public Radio]]\'\'.<ref>{{cite web |url=http://www.nytimes.com/2012/02/13/technology/start-ups-aim-to-help-users-put-a-price-on-their-personal-data.html?_r=0 |title=Start-Ups Seek to Help Users Put a Price on Their Personal Data |last=Brustein |first=Joshua |date=12 February 2012 |website=The New York Times |accessdate=8 August 2014}}</ref><ref>{{cite web |url=http://www.technologyreview.com/view/426235/is-personal-data-the-new-currency/ |title=Is Personal Data the New Currency? |last=Zax |first=David |date=30 November 2011 |website=MIT Technology Review |accessdate=8 August 2014}}</ref><ref>{{cite web |url=http://edition.cnn.com/2012/02/24/tech/web/owning-your-data-online |title=Manage (and make cash with?) your data online |last=Gross |first=Doug |date=27 February 2012 |website=CNN |accessdate=8 August 2014}}</ref> Company Co-founder and CEO Shane Green was quoted as saying that “the average American consumer would soon be able to realize over $1,000 per year” by granting limited, anonymous access to their data to marketers, but that figure was never supported by Green or the company.<ref>{{cite web |url=http://www.ft.com/cms/s/2/61c4c378-60bd-11e2-a31a-00144feab49a.html#axzz39ptB1It4 |title=Data mining offers rich seam |last=Palmer |first=Maija |date=18 February 2013 |website=Financial Times |accessdate=8 August 2014}}</ref>\n\n==Reception and Re-launch as TeamData==\nPersonal was the first online consumer-facing company to be named an Ambassador for [[Privacy by Design]] for its technical, business and legal commitments to providing users with control over the data they store in Personal’s service.<ref>{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/personal-com/ |title=Personal.com |website=Privacy by Design |accessdate=15 August 2014}}</ref><ref>{{cite web |url=http://www.privacybydesign.ca/content/uploads/2010/03/2011-10-24-Personal.com_.pdf |title=Personal and Privacy by Design |website=Privacy by Design |accessdate=20 August 2014}}</ref><ref>{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/joshua-p-galper/ |title=Joshua P. Galper |website=Privacy by Design |accessdate=20 August 2014}}</ref><ref>{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/shane-green/ |title=Shane Green |website=Privacy by Design |accessdate=20 August 2014}}</ref> The company received recognition for its user agreement, called the Owner Data Agreement,<ref>{{cite web |url=https://www.personal.com/owner-data-agreement/ |title=Owner Data Agreement |date=7 February 2014 |website=Personal, Inc. |accessdate=8 August 2014}}</ref> which acted like a reverse license agreement when data was shared between registered parties and emphasized that data ownership resides with the user. [[Doc Searls]] wrote in \'\'[[The Intention Economy: When Customers Take Charge]]\'\' that the Owner Data Agreement “had no precedent and modeled a new legal position, both for vendors and for intermediaries.”<ref>{{cite book |last=Searls |first=Doc |date=May 1, 2012 |title=The Intention Economy: When Customers Take Charge |publisher=Harvard Business Review Press |page=186 |isbn=978-1422158524 }}</ref> \n[[Fast Company (magazine)|Fast Company]] called the Data Vault “a tool that will simplify our lives.”<ref>{{cite web |url=http://www.fastcompany.com/1836521/personalcom-creates-online-vault-manage-all-your-data |title=PERSONAL.COM CREATES AN ONLINE VAULT TO MANAGE ALL YOUR DATA |last=Boyd |first=E.B. |date=7 May 2012 |website=Fast Company |accessdate=8 August 2014}}</ref> Personal has been included in case studies by Ctrl-Shift and Forrester regarding Personal Data Stores and Personal Identity Management.<ref>{{cite web |url=https://www.ctrl-shift.co.uk/index.php/research/product/64 |title=Personal Data Stores |website=Ctrl-Shift |accessdate=20 August 2014}}</ref><ref>{{cite web |url=http://blog.personal.com/uploads/2011/10/Forrester-Research-personal_identity_management.pdf |last=Khatibloo |first=Fatemeh |last2=Frankland |first2=Dave |last3=Maler |first3=Eve |last4=Smith |first4=Allison |date=30 September 2011 |title=Personal Identity Management |website=Forrester |accessdate=20 August 2014}}</ref>\n\nIn 2011, Personal received the Innovator Spotlight Award at Privacy Identity Innovation Conference (pii2011) and participated in the Technology Showcase at pii2012.<ref>{{cite web |url=http://www.prweb.com/releases/2011/5/prweb8484188.htm |last=Fonseca |first=Natalie |title=Personal and Passtouch Receive Innovator Spotlight Award at Privacy Identity Innovation Conference (pii2011) |website=PRWeb |accessdate=20 August 2014}}</ref><ref>{{cite web |url=https://www.privacyidentityinnovation.com/pii2012-seattle/pii2012-technology-showcase |title=pii2012 Technology Showcase |website=Privacy Identity Innovation |accessdate=20 August 2014}}</ref> In 2012, TechHive named Personal as one of the top five apps or web services of [[SXSW]].<ref>{{cite web |url=http://www.techhive.com/article/251744/hot_apps_and_web_services_of_sxsw.html |last=Sullivan |first=Mark |date=13 March 2012 |title=Hot Apps and Web Services of SXSW |accessdate=20 August 2014}}</ref> Personal won the 2013 Campus Technology Innovators Award with Lone Star College in July 2013.<ref>{{cite web |url=http://campustechnology.com/articles/2013/07/23/2013-innovators-awards.aspx |last=Raths |first=David |last2=Namahoe |first2=Kanoe |last3=Lloyd |first3=Meg |date=23 July 2013 |title=2013 Innovators Awards |website=Campus Technology |accessdate=20 August 2014}}</ref> Personal was included in a list of Executive Travel Magazine\'s favorite travel apps for 2013 in its May/June issue.<ref>{{citation |url=http://www.executivetravelmagazine.com/articles/ets-favorite-travel-apps-for-2013 |last=Null |first=Christopher |title=ET\'s Favorite Travel Apps of 2013 |archiveurl=https://web.archive.org/web/20131023184535/http://www.executivetravelmagazine.com/articles/ets-favorite-travel-apps-for-2013 |archivedate=2013-10-23 |accessdate=20 August 2014}}</ref>\nIn 2013, Personal was also included as part of NYU GovLab\'s Open Data 500 and was named by J. Walter Thompson as one of 100 things to watch for in 2014.<ref>{{cite web |url=http://www.opendata500.com/us/Personal-Inc/ |title=Personal, Inc. |website=Open Data 500 |accessdate=20 August 2014}}</ref><ref>{{cite web |url=http://www.jwtintelligence.com/2013/12/100-watch-2014/#axzz2qyBVCrMs |last=Mack |first=Ann |date=26 December 2013 |title=100 Things to Watch in 2014 |website=JWT Intelligence |accessdate=20 August 2014}}</ref> In 2015, the National Law Journal named Company Chief Policy Officer and General Counsel, Joshua P. Galper, as one of their 50 "Cybersecurity & Privacy Trailblazers."<ref>{{Cite web|url=http://pdfserver.amlaw.com/nlj/flipbook/Cybersecurity_Trailblazers_2015/Cyber_Security_Trailblazers_2015_Web.html#p%253D14%2523p=8|title=Cybersecurity_Trailblazers|website=pdfserver.amlaw.com|access-date=2016-10-03}}</ref>\n\nIn May 2016, Personal Co-Founder and CEO Shane Green announced the launch of TeamData with one of the other co-founders, Tarik Kurspahic, and new board chair [[Eric C. Anderson]]. TeamData focuses on the problem of securing and collaboratively managing data in the workplace, and is based on the technology and platform of Personal.<ref>{{Cite web|url=https://medium.com/@shanegreen/why-personal-com-graduated-to-teamdata-today-f75e0d539ba1#.yrkhukyec|title=Why Personal.com "graduated" to TeamData today|last=Green|first=Shane|date=2016-05-20|access-date=2016-10-03}}</ref> Onboardly included the new collaborative TeamData solution in its list of "Top 10 apps to keep your team on track" and as part of its Top 50 list of "all time best content marketing tools."<ref>{{Cite web|url=http://onboardly.com/content-marketing/all-time-best-tools-for-content-marketing-teams/|title=All Time Best Tools for Content Marketing Teams via @Onboardly|date=2016-04-07|language=en-US|access-date=2016-10-03}}</ref>\n\n==References==\n{{reflist|2}}\n\n==External links==\n*{{Official website}}\n**{{URL|www.fillit.com|Fill It homepage}}\n*{{ITunes Preview App|493536192|Personal}}\n*{{ITunes Preview App|910517122|Fill It}}\n*{{Google Play|com.personal.android|Personal}}\n*{{Google Play|com.personal.fillit|Fill It}}\n*[http://tech.co/tag/personal Personal] collected news and commentary at \'\'[[Tech Cocktail]]\'\'\n*{{Crunchbase|Personal|Personal}}\n\n[[Category:American websites]]\n[[Category:Android (operating system) software]]\n[[Category:Companies based in Washington, D.C.]]\n[[Category:Companies established in 2009]]\n[[Category:Data management]]\n[[Category:Data security]]\n[[Category:Internet companies of the United States]]\n[[Category:Internet properties established in 2009]]\n[[Category:IOS software]]']
['Systems of Engagement', '43443443', 'The concept of \'\'\'Systems of Engagement\'\'\' is attributed to [[Geoffrey Moore]], a business author of such books as Crossing the Chasm.<ref>Crossing the Chasm: Marketing and Selling High-tech Products to Mainstream Customers (1991, revised 1999 and 2014) – ISBN 0-06-051712-3</ref> In his paper for AIIM.org entitled: "Systems of Engagement and the Future of Enterprise IT" Moore states:\n“Amidst the texting and Twittering and Facebooking of a generation of digital natives, the fundamentals of next-generation communication and collaboration are being worked out. For them, it is clear, there is no going back. So at minimum, if you expect these folks to be your customers, your employees, and your citizens (and, frankly, where else could you look?), then you need to apply THEIR expectations to the next generation of enterprise IT systems....Systems of Engagement … will overlay and complement our deep investments in systems of record.”<ref>Moore, Geoffrey (2011). "Systems of Engagement and the Future of Enterprise IT". http://www.aiim.org/futurehistory</ref>{{cite news |last=Moore |first=Geoffrey |date=2011 |title=Systems of Engagement and the Future of Enterprise IT: A Sea Change in Enterprise IT |url=http://www.aiim.org/futurehistory |accessdate=October 7, 2014}}\nSince then Systems of Engagement has been adopted by organizations such as [http://blogs.forrester.com/category/systems_of_engagement Forrester Research], [[Hewlett-Packard|HP]], [[IBM]], [[AIIM]], and [http://www.avoka.com/blog/2013/07/deliver-a-system-of-engagement/ Avoka]. Forrester defines Systems of Engagement as follows: "Systems of engagement are different from the traditional systems of record that log transactions and keep the financial accounting in order: They focus on people, not processes....These new systems harness a perfect storm of mobile, social, cloud, and big data innovation to deliver apps and smart products directly in the context of the daily lives and real-time workflows of customers, partners, and employees.”<ref>http://blogs.forrester.com/ted_schadler/12-02-14-a_billion_smartphones_require_new_systems_of_engagement</ref> {{cite news |last=Schadler |first=Ted |date=February 14, 2012 |title=A Billion Smartphones Require New Systems Of Engagement |url=http://blogs.forrester.com/ted_schadler/12-02-14-a_billion_smartphones_require_new_systems_of_engagement |accessdate=October 7, 2014 }}\n\n==See also==\n* [[System of record]] — conventional enterprise systems designed to contain the authoritative data source for a given piece of information.\n==References==\n{{Reflist}}\n\n\n[[Category:Information systems]]\n[[Category:Data management]]\n\n\n{{compu-stub}}']
['Data management', '759312', '{{lead too short|date=October 2014}}\n\'\'\'Data management\'\'\' comprises all the [[List of academic disciplines|disciplines]] related to managing [[data]] as a valuable resource.\n\n== Overview ==\nThe official definition provided by [[DAMA]] International, the professional organization for those in the data management profession, is: "Data Resource Management is the development and execution of architectures, policies, practices and procedures that properly manage the full data lifecycle needs of an enterprise." This definition is fairly broad and encompasses a number of professions which may not have direct technical contact with lower-level aspects of data management, such as [[relational database]] management.\n\n[[File:The Data Lifecycle.jpg|thumb|The data lifecycle]]\n\nAlternatively, the definition provided in the DAMA Data Management Body of Knowledge (<ref>[https://technicspub.com/dmbok/ DAMA-DMBOK]</ref>) is:\n"Data management is the development, execution and supervision of plans, policies, programs and practices that control, protect, deliver and enhance the value of data and information assets."<ref>"DAMA-DMBOK Guide (Data Management Body of Knowledge) Introduction & Project Status" (Note: PDF no longer available online at https://www.dama.org, current version available for purchase)</ref>\n\nThe concept of "Data Management" arose in the 1980s as technology moved from sequential processing (first cards, then tape) to [[random access]] processing.  Since it was now technically possible to store a single fact in a single place and access that using random access disk, those suggesting that "Data Management" was more important than "[[Process Management]]" used arguments such as "a customer\'s home address is stored in 75 (or some other large number) places in our computer systems."  During this period, random access processing was not competitively fast, so those suggesting "Process Management" was more important than "Data Management" used batch processing time as their primary argument.  As applications moved into real-time, [[interactive]] applications, it became obvious to most practitioners that both management processes were important.  If the data was not well defined, the data would be mis-used in applications.  If the process wasn\'t well defined, it was impossible to meet user needs.\n\n==Corporate Data Quality Management==\n[[Corporate Data Quality Management]] (CDQM) is, according to the [[EFQM|European Foundation for Quality Management]] and the Competence Center Corporate Data Quality (CC CDQ, University of St. Gallen), the whole set of activities intended to improve corporate data quality (both reactive and preventive). Main premise of CDQM is the business relevance of high-quality corporate data. CDQM comprises with following activity areas:.<ref>[https://benchmarking.iwi.unisg.ch/Framework_for_CDQM.pdf EFQM ; IWI-HSG: EFQM Framework for Corporate Data Quality Management. Brussels : EFQM Press, 2011]</ref>\n* \'\'\'Strategy for Corporate Data Quality\'\'\': As CDQM is affected by various business drivers and requires involvement of multiple divisions in an  organization; it must be considered a company-wide endeavor.\n* \'\'\'Corporate Data Quality Controlling\'\'\': Effective CDQM requires compliance with standards, policies, and procedures. Compliance is monitored according to previously defined metrics and performance indicators and reported to stakeholders.\n* \'\'\'Corporate Data Quality Organization\'\'\': CDQM requires clear roles and responsibilities for the use of corporate data. The CDQM organization defines tasks and privileges for decision making for CDQM.\n* \'\'\'Corporate Data Quality Processes and Methods\'\'\': In order to handle corporate data properly and in a standardized way across the entire organization and to ensure corporate data quality, standard procedures and guidelines must be embedded in company’s daily processes.\n* \'\'\'Data Architecture for Corporate Data Quality\'\'\': The data architecture consists of the data object model - which comprises the unambiguous definition and the conceptual model of corporate data - and the data storage and distribution architecture.\n* \'\'\'Applications for Corporate Data Quality\'\'\': Software applications support the activities of Corporate Data Quality Management. Their use must be planned, monitored, managed and continuously improved.\n\n== Topics in Data Management ==\nTopics in Data Management, grouped by the DAMA DMBOK Framework,<ref>[http://dama-dach.org/dama-dmbok-functional-framework DAMA-DMBOK Functional Framework v3]</ref> include:\n{{colbegin|2}}\n# [[Data governance]]\n#* [[Data asset]]\n#* [[Data governance]]\n#* [[Data steward]]\n# Data Architecture, Analysis and Design\n#* [[Data analysis]]\n#* [[Data architecture]]\n#* [[Data modeling]]\n# Database Management\n#* [[Data maintenance]]\n#* [[Database administration]]\n#* [[Database management system]]\n# Data Security Management\n#* [[Data access]]\n#* [[Data erasure]]\n#* [[Data privacy]]\n#* [[Data security]]\n# Data Quality Management\n#* [[Data cleansing]]\n#* [[Data integrity]]\n#* [[Data enrichment]]\n#* [[Data quality]]\n#* [[Data quality assurance]]\n# Reference and Master Data Management\n#* [[Data integration]]\n#* [[Master data management]]\n#* [[Reference data]]\n# Data Warehousing and Business Intelligence Management\n#* [[Business intelligence]]\n#* [[Data mart]]\n#* [[Data mining]]\n#* Data movement ([[Extract, transform, load ]])\n#* [[Data warehouse]]\n# Document, Record and Content Management\n#* [[Document management system]]\n#* [[Records management]]\n# Meta Data Management\n#* [[Meta-data management]]\n#* [[Metadata]]\n#* [[Metadata discovery]]\n#* [[Metadata publishing]]\n#* [[Metadata registry]]\n# Contact Data Management\n#* [[Business continuity planning]]\n#* [[Marketing operations]]\n#* [[Customer data integration]]\n#* [[Identity management]]\n#* [[Identity theft]]\n#* [[Data theft]]\n#* [[ERP software]]\n#* [[CRM software]]\n#* [[Address (geography)]]\n#* [[Postal code]]\n#* [[Email address]]\n#* [[Telephone number]]\n{{colend}}\n\n==Body of Knowledge==\nThe DAMA Guide to the Data Management Body of Knowledge" (DAMA-DMBOK Guide), under the guidance of a new DAMA-DMBOK Editorial Board. This publication is available from April 5, 2009.\n\n==Usage==\n\nIn modern [[management fad|management usage]], one can easily discern a trend away from the term "data" in composite expressions to the term "[[information]]" or even "[[knowledge]]" when talking in a non-technical context. Thus there exists not only data management, but also [[information management]] and [[knowledge management]]. This is a misleading trend as it obscures that traditional data are managed or somehow [[data processing|processed]] on second looks.{{cn|date=June 2016}} The distinction between data and derived values can be seen in the [[information ladder]].{{cn|date=June 2016}} While data can exist as such, "information" and "knowledge" are always in the "eye" (or rather the brain) of the beholder and can only be measured in relative units.\n\nSeveral organisations have established a data management centre (DMC)<ref>\nFor example: {{cite book\n| last1                 = Kumar\n| first1                = Sangeeth\n| last2                 = Ramesh\n| first2                = Maneesha Vinodini\n| chapter               = Lightweight Management framework (LMF) for a Heterogeneous Wireless Network for Landslide Detection\n| editor1-last          = Meghanathan\n| editor1-first         = Natarajan\n| editor2-last          = Boumerdassi\n| editor2-first         = Selma\n| editor3-last          = Chaki\n| editor3-first         = Nabendu\n| editor4-last          = Nagamalai\n| editor4-first         = Dhinaharan\n| title                 = Recent Trends in Networks and Communications: International Conferences, NeCoM 2010, WiMoN 2010, WeST 2010,Chennai, India, July 23-25, 2010. Proceedings\n| url                   = https://books.google.com/books?id=8i5qCQAAQBAJ\n| series                = Communications in Computer and Information Science\n| volume                = 90\n| publisher             = Springer\n| publication-date      = 2010\n| page                  = 466\n| isbn                  = 9783642144936\n| accessdate            = 2016-06-16\n| quote                 = 4.4 Data Management Center (DMC)[:] The Data Management Center is the data center for all of the deployed cluster networks. Through the DMC, the LMF allows the user to list the services in any cluster member belonging to any cluster [...].\n}}\n</ref>\nfor their operations.\n\n==Integrated data management==\n\n\'\'\'Integrated data management\'\'\' (IDM) is a tools approach to facilitate data management and improve performance. IDM consists of an integrated, modular environment to manage enterprise application data, and optimize data-driven applications over its [[Information Lifecycle Management|lifetime]].<ref>[http://www.ibm.com/developerworks/data/library/techarticle/dm-0807hayes/?S_TACT=105AGX11&S_CMP=FP#ibm-content  Integrated Data Management: Managing data across its lifecycle] by Holly Hayes</ref><ref>[http://www.ibmsystemsmagmainframedigital.com/nxtbooks/ibmsystemsmag/mainframe_20090708/index.php#/34 Organizations thrive on Data] by Eric Naiburg</ref><ref>[http://download.boulder.ibm.com/ibmdl/pub/software/data/sw-library/data-management/optim/reports/fragmented.pdf Fragmented Management Across The Data Life Cycle Increases Cost And Risk] - A commissioned study conducted by Forrester Consulting on behalf of IBM October 2008</ref><ref>[http://publib.boulder.ibm.com/infocenter/idm/v2r1/index.jsp integrated IBM Data Management information center]</ref> IDM\'s purpose is to:\n*Produce enterprise-ready applications faster\n*Improve data access, speed iterative testing\n*Empower collaboration between architects, developers and DBAs\n*Consistently achieve service level targets\n*Automate and simplify operations\n*Provide contextual intelligence across the [[solution stack]]\n*Support business growth\n*Accommodate new initiatives without expanding infrastructure\n*Simplify application upgrades, consolidation and retirement\n*Facilitate alignment, consistency and governance\n*Define business policies and standards up front;  share, extend, and apply throughout the lifecycle\n\n==See also==\n{{colbegin|2}}\n* [[Open data]]\n* [[Information architecture]]\n* [[Information management]]\n* [[Enterprise architecture]]\n* [[Information design]]\n* [[Information system]]\n* [[Controlled vocabulary]]\n* [[Data curation]]\n* [[Data retention]]\n* [[Data governance]]\n* [[Data quality]]\n* [[Data modeling]]\n* [[Data management plan]]\n* [[Information lifecycle management]]\n* [[Computer data storage]]\n* [[Data proliferation]]\n* [[Digital preservation]]\n* [[Digital perpetuation]]\n* [[Document management]]\n* [[Enterprise content management]]\n* [[Hierarchical storage management]]\n* [[Information repository]]\n* [[Records management]]\n* [[System integration]]\n{{colend}}\n\n== References ==\n{{Reflist}}\n\n==External links==\n* {{dmoz|Computers/Software/Master_Data_Management/Articles/}}\n\n{{DEFAULTSORT:Data Management}}\n[[Category:Data management| ]]\n[[Category:Information technology management]]']
['SciDB', '38334530', "{{Infobox software\n| title       = SciDB\n| developer   = Paradigm4\n| publisher   =\n| genre       = Database management system\n| license     = [[Affero General Public License|AGPL]] v3<ref>[http://www.scidb.org/forum/viewtopic.php?f=16&t=364 Download & licensing]</ref>\n| first       =\n| last        =\n| coauthors   =\n| released    = 2008\n| website     = {{URL|http://www.paradigm4.com/}}\n}}\n{{Distinguish2|[http://scidb.sourceforge.net/ Scidb: Chess database]}}\n\n'''SciDB'''<ref>{{cite web | url = http://www.scidb.org/ | title=SciDB website}}</ref> is an array database designed for multidimensional data management and analytics common to scientific, geospatial, financial, and industrial applications.  It is developed by Paradigm4, co-founded by [[Michael Stonebraker]].\n\n==History and Characteristics==\n[[Michael Stonebraker]] co-created SciDB where, he claims, arrays are 100 or so times faster than a RDBMS on this class of problem.<ref>{{cite web | url = http://www.theregister.co.uk/2010/09/13/michael_stonebraker_interview | title=Stonebraker interview}}</ref> It is swapping rows and columns for mathematical arrays that put fewer restrictions on the data and can work in any number of dimensions unlike the conventionally widely used [[relational database management system]] model, in which each [[Relation (database)|relation]] supports only one dimension of records.\n\nAccording to a Strata Conference presentation on SciDB,<ref>{{cite web |url=http://strataconf.com/stratany2011/public/schedule/detail/21376 |title=Big Data and Big Analytics: SciDB is not Hadoop}}</ref> it natively supports:\n* An array data model for efficient storage and manipulation of larger-than-memory multi-dimensional arrays.\n* Data versioning and provenance to allow tracking results back to original supporting data.\n* What-if modeling, back-testing, and re-analysis.\n* Massive scale math on the arrays for linear algebra and analytics.\n* Uncertainty can be modeled by associating error-bars with data.\n* Efficient storage.\n\n==See also==\n<!-- Please do not list specific implementations here! -->\n* [[Comparison of object database management systems]]\n* [[Comparison of structured storage software]]\n\n== References ==\n{{Reflist|2}}\n\n==External links==\n* [http://www.theregister.co.uk/2010/09/13/michael_stonebraker_interview/ Michael Stonebraker interview]\n* [http://itknowledgeexchange.techtarget.com/soa-talk/stonebraker-sees-high-programming-overhead-for-nosql/ Stonebraker sees high programming overhead for NoSQL]\n\n[[Category:Data management]]\n[[Category:Distributed data stores]]\n[[Category:Document-oriented databases]]\n[[Category:Distributed computing architecture]]\n[[Category:Free database management systems]]\n[[Category:Structured storage]]\n[[Category:NoSQL]]\n[[Category:Software using the GNU AGPL license]]"]
['Data recovery hardware', '46219812', '{{more footnotes|date=March 2015}}\n\n\'\'\'Data recovery hardware\'\'\' was developed because [[data recovery software]] lacks the ability to deal with all lost or corrupted data files. Often the failures, such as media files with [[bad sectors]], [[firmware]] failures, PCB ([[Printed circuit board]]) failures, hard drive head failures, etc., cannot be fixed.\n\n==Bad sectors==\n\nThe two types of bad sectors are "physical" and "logical" bad sectors, or "hard" and "soft" bad sectors.<ref>{{cite web\n | url = http://www.howtogeek.com/173463/bad-sectors-explained-why-hard-drives-get-bad-sectors-and-what-you-can-do-about-it/\n | title = Bad Sectors Explained: Why Hard Drives Get Bad Sectors and What You Can Do About It\n | date = October 9, 2013| accessdate = March 26, 2015\n | author = Chris Hoffman  | website = How-To Geek, LLC\n}}</ref>\n\nWhen a disk has physical bad sectors, software cannot effectively offer soft reset, hard reset, power reset, error handling, read algorithm auto exchange nor skip sectors. If a disk with bad physical sectors is connected to a [[Personal computer|PC]], the condition would potentially not be detected.\n\nSoft bad sectors can potentially be fixed by either data recovery software or hardware, depending on the damaged condition. Some amount of bad sectors can be skipped using software, while a severely corrupted disk with a large area of bad sectors may potentially only be repaired.<ref>{{cite web\n | url = https://www.winxdvd.com/resource/repair-mp4-file-free.htm\n | title = How to Repair Corrupted MP4 Video File\n | date = July 3, 2015| accessdate = August 24, 2016\n | author = Estrella Garcia  | website = WinXDVD\n}}</ref>  Bad sectors are areas on the hard drive that cannot be read. Even new hardrives sometimes contain bad sectors. Since manufacturers intensely compete to cram more space into disks, systems operate close to the limit of that generation of technology.<ref>{{Cite web|url=https://www.grc.com/sr/faq.htm|title=GRC {{!}} SpinRite 6.0 FAQ - Frequently Asked Questions|website=www.grc.com|access-date=2016-10-20}}</ref>\n\n==Dead PCB==\n\nWhen the drive has dead PCB ([[Printed circuit board]]), users need to:\n* swap in a new PCB\n* put one donor [[Integrated circuit|chip]] and write by chip reader with matching ROM ([[Read-only memory]]) content\n* get the dead drive spinning\n\nWhen the drive has physical head damage, users need to open the drive in [[cleanroom]] environment and find donor heads or other donor components to swap.\n\n==Data recovery hardware types==\n*Disk image;\n*File extraction hardware;\n*Firmware repair hardware;\n*ROM chip reader;\n*Head and Platter Swap Tools (See [[Hard disk drive platter]]);\n*Spindle release hardware;\n*Other hardware\n\n== See also ==\n{{Portal|Computer security|Computing}}\n\n{{Div col||20em}}\n* [[Data recovery]]\n* [[Firmware]]\n* [[Bad sector]]\n* [[Disk image]]\n* [[Printed circuit board]]\n* [[Cleanroom]]\n* [[List of data recovery software]]\n* [[Comparison of file systems]]\n* [[Computer forensics]]\n* [[Continuous data protection]]\n* [[Data archaeology]]\n* [[Data loss]]\n* [[Error detection and correction]]\n* [[File carving]]\n* [[Hidden file and hidden directory]]\n* [[Knowledge extraction]]\n* [[Undeletion]]\n{{Div col end}}\n\n==Further reading==\n* Tanenbaum, A. & Woodhull, A. S. (1997). \'\'Operating Systems: Design And Implementation,\'\' 2nd ed. New York: Prentice Hall.\n* {{dmoz|Computers/Hardware/Storage/Data_Recovery/|Data recovery}}\n* {{cite web|url=https://www.grc.com/sr/faq.htm|title=GRC&nbsp;-&nbsp;SpinRite 6.0 FAQ - Frequently Asked Questions|publisher=}}\n\n==References==\n{{Reflist|30em}}\n\n{{DEFAULTSORT:Data Recovery}}\n[[Category:Data recovery|*]]\n[[Category:Computer data]]\n[[Category:Data management]]\n[[Category:Transaction processing]]\n[[Category:Hard disk software|*]]\n[[Category:Backup|Recovery]]']
['Author Name Disambiguation', '46474403', "{{Multiple issues|\n{{Underlinked|date=May 2016}}\n{{Orphan|date=May 2016}}\n{{refimprove|date=April 2015}}\n}}\n\n'''Author name disambiguation''' is a type of [[Record linkage]] that is applied to scholarly documents where the goal is to find all mentions of the same author and cluster them together. Authors of scholarly documents often share names which makes it hard to distinguish each author's work. Hence, author name disambiguation aims to find all publications that belong to a given author and distinguish them from publications of other authors who share the same name.\n\nThere are multiple reasons that cause author names to be ambiguous, among which: individuals may publish under multiple names for variety of reasons including different spelling, misspelling, name change due to marriage, or the use of middle names and initials.<ref>{{cite journal\n | authorlink = Smalheiser, Neil R and Torvik, Vetle I\n | title = Author name disambiguation\n | journal = [[Annual Review of Information Science and Technology]]\n | url = http://onlinelibrary.wiley.com/doi/10.1002/aris.2009.1440430113/full\n | accessdate = 2015-04-20\n | doi = 10.1002/aris.2009.1440430113\n}}</ref>\n\nTypical approach for author name disambiguation rely on information about the authors such as their affiliations, email addresses, year of publication, co-authors, topic information to distinguish between authors. These information can be used to learn a machine learning classifier that decides whether two mentions refer to the same author or not. Other approaches utilized heuristics to distinguish between authors.\n\n==References==\n{{Reflist}}\n\n[[Category:Metadata]]\n[[Category:Data management]]"]
['Information integration', '2714749', '{{Refimprove|date=September 2014}}\n\'\'\'Information integration\'\'\' (\'\'\'II\'\'\') (also called [[referential integrity]]) is the merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations. It is used in [[data mining]] and consolidation of data from unstructured or semi-structured resources. Typically, \'\'information integration\'\' refers to textual representations of knowledge but is sometimes applied to [[rich-media]] content. \'\'\'Information fusion\'\'\', which is a related term, involves the combination of information into a new set of information towards reducing redundancy and uncertainty.<ref name="dca">M. Haghighat, M. Abdel-Mottaleb, &  W. Alhalabi (2016). [http://dx.doi.org/10.1109/TIFS.2016.2569061 Discriminant Correlation Analysis: Real-Time Feature Level Fusion for Multimodal Biometric Recognition]. IEEE Transactions on Information Forensics and Security, 11(9), 1984-1996.</ref>\n\nExamples of [[Technology|technologies]] available to integrate information include [[data deduplication|deduplication]], and [[string metrics]] which allow the detection of similar text in different data sources by [[fuzzy string searching|fuzzy matching]]. A host of methods for these research areas are available such as those presented in the International Society of Information Fusion.\n\n==See also==\n* [[Data fusion]] (is a subset of Information integration)\n* [[Sensor fusion]]\n* [[Data integration]]\n* [[Image fusion]]\n\n==External links==\n* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]<ref name="dca"></ref>\n* [http://webcache.googleusercontent.com/search?q=cache:OrNCxOpaXAMJ:infolab.stanford.edu/pub/papers/integration-using-views.ps+information+integration&cd=5&hl=en&ct=clnk&gl=us&client=firefox-a Information Integration Using Logical View] LNCS 1997.\n* [http://www.isif.org/ International Society of Information Fusion]\n\n==Books==\n* Liggins, Martin E., David L. Hall, and James Llinas. Multisensor Data Fusion, Second Edition Theory and Practice (Multisensor Data Fusion). CRC, 2008. ISBN 978-1-4200-5308-1\n* David L. Hall, Sonya A. H. McMullen, Mathematical Techniques in Multisensor Data Fusion (2004), ISBN 1-58053-335-3\n* Springer, Information Fusion in Data Mining (2003), ISBN 3-540-00676-1\n* H. B. Mitchell, Multi-sensor Data Fusion – An Introduction (2007) Springer-Verlag, Berlin, ISBN 978-3-540-71463-7\n* S. Das, High-Level Data Fusion (2008), Artech House Publishers, Norwood, MA, ISBN 978-1-59693-281-4 and 1596932813\n* Erik P. Blasch, Eloi Bosse, and Dale A. Lambert, High-Level Information Fusion Management and System Design (2012), Artech House Publishers, Norwood, MA. ISBN 1608071510 | ISBN 978-1608071517\n\n==References==\n{{Reflist}}\n\n\n{{DEFAULTSORT:Information Integration}}\n[[Category:Data management]]\n\n[[ar:تكامل البيانات]]\n[[de:Informationsintegration]]']
['Data availability', '46877023', '{{cleanup reorganize|date=June 2015}}\nData availability<ref>http://searchstorage.techtarget.com/definition/data-availability</ref> is a term used by computer storage manufacturers and storage service providers (SSPs) to describe products and services that ensure that data continues to be available at a required level of performance in situations ranging from normal through "disastrous."\n\nAnytime a server loses power, for example, it has to reboot, recover data and repair corrupted data. The time it takes to recover, known as the mean time to recover (MTR), could be minutes, hours or days.<ref>http://blog.schneider-electric.com/datacenter/2012/10/12/understanding-data-center-reliability-availability-and-the-cost-of-downtime/</ref>\n\n==Data Center Standards==\nThe two organizations in the United States that publish data center standards are the [[Telecommunications Industry Association]] (TIA) and the [[Uptime Institute]].\n\n===TIA - Data Center Standards===\nSee wiki entry on [[TIA-942]].\n\n===Uptime Institute - Data Center Tier Standards===\n\'\'\'Tier I Requirements\'\'\'<ref>http://www.firstcomm.com/overview-of-data-center-availability-tiers/</ref>\n* Single non-redundant distribution path serving the IT equipment\n*  Non-redundant capacity components\n* Basic site infrastructure with expected availability of 99.671%\n\n\'\'\'Tier II Requirements\'\'\'\n* Meets or exceeds all Tier I requirements\n* Redundant site infrastructure capacity components with expected availability of 99.741%\n\n\'\'\'Tier III Requirements\'\'\'\n* Meets or exceeds all Tier I and Tier II requirements\n* Multiple independent distribution paths serving the IT equipment\n* All IT equipment must be dual-powered and fully compatible with the topology of a site’s architecture\n* Concurrently maintainable site infrastructure with expected availability of 99.982%\n\n\'\'\'Tier IV Requirements\'\'\'\n* Meets or exceeds all Tier I, Tier II and Tier III requirements\n* All cooling equipment is independently dual-powered, including chillers and heating, ventilating and air-conditioning (HVAC) systems\n* Fault-tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99.995%\n\nThe Uptime Institute’s tier system allows for the following minutes of downtime annually:\n* Tier I (99.671% minimum uptime) (1729 minutes maximum annual downtime)\n* Tier II (99.741% minimum uptime) (1361 minutes maximum annual downtime)\n* Tier III (99.982% minimum uptime) (95 minutes maximum annual downtime)\n* Tier IV (99.995% minimum uptime) (26 minutes maximum annual downtime)\n\n==See also==\n* Data Center Tiers [[Data center#Data center tiers|Data center tiers]]\n\n==References==\n{{reflist}}\n\n[[Category:Data management]]\n[[Category:Distributed data storage]]\n[[Category:Distributed data storage systems]]']
['Online transaction processing', '2329992', '\'\'\'Online transaction processing\'\'\', or \'\'\'OLTP\'\'\', is a class of [[information system]]s that facilitate and manage transaction-oriented applications, typically for data entry and retrieval [[transaction processing]].\n\nThe term is somewhat ambiguous; some understand a "transaction" in the context of computer or [[database transactions]], while others (such as the [[Transaction Processing Performance Council]]) define it in terms of business or [[financial transaction|commercial transactions]].<ref>[http://www.tpc.org/ Transaction Processing Performance Council website]</ref> OLTP has also been used to refer to processing in which the system responds immediately to user requests. An [[automated teller machine]] (ATM) for a bank is an example of a commercial transaction processing application. Online transaction processing applications are high throughput and insert or update-intensive in database management. These applications are used concurrently by hundreds of users. The key goals of OLTP applications are availability, speed, concurrency and recoverability.<ref>[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76992/ch3_eval.htm#2680 Application and System Performance Characteristics]</ref> Reduced paper trails and the faster, more accurate forecast for revenues and expenses are both examples of how OLTP makes things simpler for businesses. However, like many modern online information technology solutions, some systems require offline maintenance, which further affects the cost–benefit analysis of on line transaction processing system.\n\nOLTP is typically contrasted to [[Online analytical processing|OLAP]] (online analytical processing), which is generally characterized by much more complex queries, in a smaller volume, for the purpose of business intelligence or reporting rather than to process transactions. Whereas OLTP systems process all kinds of queries (read, insert, update and delete), OLAP is generally optimized for read only and might not even support other kinds of queries.\n\n==Overview==\nOLTP system is a popular data processing system in today\'s enterprises.  Some examples of OLTP systems include order entry, retail sales, and financial transaction systems.<ref>What is an OLTP System[http://docs.oracle.com/cd/E11882_01/server.112/e25523/part_oltp.htm]</ref>  On line transaction processing system increasingly requires support for transactions that span a network and may include more than one company. For this reason, modern on line transaction processing software use client or server processing and brokering software that allows transactions to run on different computer platforms in a network.\n\nIn large applications, efficient OLTP may depend on sophisticated transaction management software (such as [[CICS]]) and/or [[database]] optimization tactics to facilitate the processing of large numbers of concurrent updates to an OLTP-oriented database.\n\nFor even more demanding decentralized database systems, OLTP brokering programs can distribute transaction processing among multiple computers on a [[computer network|network]]. OLTP is often integrated into [[service-oriented architecture]] (SOA) and [[Web service]]s.\n\nOn line transaction processing (OLTP) involves gathering input information, processing the information and updating existing information to reflect the gathered and processed information. As of today, most organizations use a database management system to support OLTP. OLTP is carried in a client server system.\n\nOn line transaction process concerns about concurrency and atomicity.  Concurrency controls guarantee that two users accessing the same data in the database system will not be able to change that data or the user has to wait until the other user has finished processing, before changing that piece of data.  Atomicity controls guarantee that all the steps in transaction are completed successfully as a group. That is, if any steps between the transaction fail, all other steps must fail also.<ref>\n[http://technet.microsoft.com/en-us/library/ms187669(v=sql.105).aspx On line Transaction Processing vs. Decision Support]</ref>\n\n==Systems design==\nTo build an OLTP system, a designer must know that the large number of concurrent users does not interfere with the system\'s performance.  To increase the performance of OLTP system, designer must avoid the excessive use of indexes and clusters.\n\nThe following elements are crucial for the performance of OLTP systems:<ref>[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76992/ch3_eval.htm#2680 Application and System Performance Characteristics]</ref>\n*Rollback segments\n:Rollback segments are the portions of database that record the actions of transactions in the event that a transaction is rolled back.  Rollback segments provide read consistency, roll back transactions, and recover the database.<ref>[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76956/rollbak.htm Rollback]</ref>\n*Clusters\n:A cluster is a [[Database schema|schema]] that contains one or more tables that have one or more columns in common.  Clustering tables in database improves the performance of [[Join (SQL)|join]] operation.<ref>[http://www.iselfschooling.com/mc4articles/mc4cluster.htm cluster table]</ref>\n*Discrete transactions\n:All changes to the data are deferred until the transaction commits during a discrete transaction.  It can improve the performance of short, non-distributed transaction.<ref>[http://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/transac.htm Discrete Transactions]</ref>\n*[[Block (data storage)]] size\n:The data block size should be a multiple of the operating system\'s block size within the maximum limit to avoid unnecessary I/O.<ref>[http://docs.oracle.com/cd/B10500_01/server.920/a96524/c03block.htm Data Block]</ref>\n*[[Buffer cache]] size\n:To avoid unnecessary resource consumption, tune [[SQL]] statements to use the database buffer cache.<ref>[http://docs.oracle.com/cd/E16655_01/server.121/e15857/tune_buffer_cache.htm#TGDBA294 Database buffer Cache]</ref>\n*[[Dynamic allocation]] of space to tables and rollback segments\n*[[Transaction processing]] monitors and the multi-threaded server\n:A transaction processing monitor is used for coordination of services.  It is like an operating system and does the coordination at a high level of granularity and can span multiple computing devices.<ref>[http://c2.com/cgi/wiki?TransactionProcessingMonitor Transaction processing monitor]</ref>\n*[[Partition (database)]]\n:Partition increases performance for sites that have regular transactions while still maintain availability and security.<ref>[[Partition (database)|Partition]]</ref>\n*[[Database tuning]]\n:With database tuning, OLTP system can maximize its performance as efficiently and rapidly as possible.\n\n==Contrasted to==\n*[[Batch processing]]\n*[[Grid computing]]\n\n==See also==\n*[[On line analytical processing]] (OLAP)\n*[[Transaction processing]]\n*[[Database transaction]]\n\n==References==\n<references />\n\n==External links==\n{{Wiktionary|OLTP}}\n*[http://hstore.cs.brown.edu H-Store Project] (architectural and application shifts affecting OLTP performance)\n*[http://www.ibm.com/cics IBM CICS official website]\n*[http://www.tpc.org/ Transaction Processing Performance Council]\n*[http://dbms.knowledgehills.com/What-is-Online-Transaction-Processing-(OLTP)-Schema/a32p2 OLTP Schema]\n*[http://www.amazon.com/dp/1558601902 Transaction Processing: Concepts & Techniques Management]\n\n{{Databases}}\n\n{{DEFAULTSORT:On line Transaction Processing}}\n[[Category:Data management]]\n[[Category:Databases]]\n[[Category:Transaction processing]]']
['Document-oriented database', '15002414', '{{about|the software type|deployed applications of the software type|Full text database}}\n{{primary sources|date=May 2012}}\n\nA \'\'\'document-oriented database\'\'\', or \'\'\'document store\'\'\', is a [[computer program]] designed for storing, retrieving and managing document-oriented information, also known as [[Semi-structured model|semi-structured data]]. Document-oriented databases are one of the main categories of [[NoSQL]] databases, and the popularity of the term "document-oriented database" has grown<ref>[http://db-engines.com/en/ranking_categories DB-Engines Ranking per database model category]</ref> with the use of the term NoSQL itself. [[XML database]]s are a subclass of document-oriented databases that are optimized to work with [[XML]] documents. [[Graph databases]] are similar, but add another layer, the \'\'relationship\'\', which allows them to link documents for rapid traversal.\n\nDocument-oriented databases are inherently a subclass of the [[Key-value database|key-value store]], another NoSQL database concept. The difference lies in the way the data is processed; in a key-value store the data is considered to be inherently opaque to the database, whereas a document-oriented system relies on internal structure in the \'\'document\'\' in order to extract [[metadata]] that the database engine uses for further optimization. Although the difference is often moot due to tools in the systems,{{efn|To the point that document-oriented and key-value systems can often be interchanged in operation.}} conceptually the document-store is designed to offer a richer experience with modern programming techniques.\n\nDocument databases{{efn|And key-value stores in general.}} contrast strongly with the traditional [[relational database]] (RDB). Relational databases generally store data in separate \'\'tables\'\' that are defined by the programmer, and a single object may be spread across several tables. Document databases store all information for a given object in a single instance in the database, and every stored object can be different from every other. This makes mapping objects into the database a simple task, normally eliminating anything similar to an [[object-relational mapping]]. This makes document stores attractive for programming [[web application]]s, which are subject to continual change in place, and where speed of deployment is an important issue.\n\n== Documents ==\nThe central concept of a document-oriented database is the notion of a \'\'document\'\'. While each document-oriented database implementation differs on the details of this definition, in general, they all assume documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include [[XML]], [[YAML]], [[JSON]], and [[BSON]], as well as binary forms like PDF and Microsoft Office documents (MS Word, Excel, and so on).\n\nDocuments in a document store are roughly equivalent to the programming concept of an object. They are not required to adhere to a standard schema, nor will they have all the same sections, slots, parts, or keys. Generally, programs using objects have many different types of objects, and those objects often have many optional fields. Every object, even those of the same class, can look very different. Document stores are similar in that they allow different types of documents in a single store, allow the fields within them to be optional, and often allow them to be encoded using different encoding systems. For example, the following is a document, encoded in JSON:\n\n<syntaxhighlight lang="javascript">\n{\n    FirstName: "Bob", \n    Address: "5 Oak St.", \n    Hobby: "sailing"\n}\n</syntaxhighlight>\n\nA second document might be encoded in XML as:\n<syntaxhighlight lang="xml">\n  <contact>\n    <firstname>Bob</firstname>\n    <lastname>Smith</lastname>\n    <phone type="Cell">(123) 555-0178</phone>\n    <phone type="Work">(890) 555-0133</phone>\n    <address>\n      <type>Home</type>\n      <street1>123 Back St.</street1>\n      <city>Boys</city>\n      <state>AR</state>\n      <zip>32225</zip>\n      <country>US</country>\n    </address>\n  </contact>\n</syntaxhighlight>\n\nThese two documents share some structural elements with one another, but each also has unique elements. The structure and text and other data inside the document are usually referred to as the document\'s \'\'content\'\' and may be referenced via retrieval or editing methods, (see below). Unlike a relational database where every record contains the same fields, leaving unused fields empty; there are no empty \'fields\' in either document (record) in the above example. This approach allows new information to be added to some records without requiring that every other record in the database share the same structure.\n\nDocument databases typically provide for additional [[metadata]] to be associated with and stored along with the document content. That metadata may be related to facilities the datastore provides for organizing documents, providing security, or other implementation specific features.\n\n=== CRUD operations ===\nThe core operations a document-oriented database supports on documents are similar to other databases and while the terminology isn\'t perfectly standardized, most practitioners will recognize them as [[CRUD]]\n\n* Creation (or insertion)\n* Retrieval (or query, search, finds)\n* Update (or edit)\n* Deletion (or removal)\n\n=== Keys ===\nDocuments are addressed in the database via a unique \'\'key\'\' that represents that document. This key is a simple [[identifier]] (or ID), typically a [[String (computer science)|string]], a [[URI]], or a [[Path (computing)|path]]. The key can be used to retrieve the document from the database. Typically the database retains an [[Database index|index]] on the key to speed up document retrieval, and in some cases the key is required to create or insert the document into the database.\n\n=== Retrieval ===\nAnother defining characteristic of a document-oriented database is that, beyond the simple key-to-document lookup that can be used to retrieve a document, the database offers an API or query language that allows the user to retrieve documents based on content (or metadata). For example, you may want a query that retrieves all the documents with a certain field set to a certain value.  The set of query APIs or query language features available, as well as the expected performance of the queries, varies significantly from one implementation to another. Likewise, the specific set of indexing options and configuration that are available vary greatly by implementation.\n\nIt is here that the document store varies most from the key-value store. In theory, the values in a key-value store are opaque to the store, they are essentially black boxes. They may offer search systems similar to those of a document store, but may have less understanding about the organization of the content. Document stores use the metadata in the document to classify the content, allowing them, for instance, to understand that one series of digits is a phone number, and another is a postal code. This allows them to search on those types of data, for instance, all phone numbers containing 555, which would ignore the zip code 55555.\n\n=== Editing ===\nDocument databases typically provide some mechanism for updating or editing the content (or other metadata) of a document, either by allowing for replacement of the entire document, or individual structural pieces of the document.\n\n=== Organization ===\nDocument database implementations offer a variety of ways of organizing documents, including notions of\n\n* Collections: groups of documents, where depending on implementation, a document may be enforced to live inside one collection, or may be allowed to live in multiple collections\n* Tags and non-visible metadata: additional data outside the document content \n* Directory hierarchies: groups of documents organized in a tree-like structure, typically based on path or URI\n\nSometimes these organizational notions vary in how much they are logical vs physical, (e.g. on disk or in memory), representations.\n\n==Relationship to other databases ==\n\n=== Relationship to key-value stores ===\n\nA document-oriented database is a specialized  [[Key-value database|key-value store]], which itself is another NoSQL database category.  In a simple key-value store, the document content is opaque. A document-oriented database provides APIs or a query/update language that exposes the ability to query or update based on the internal structure in the \'\'document\'\'. This difference may be moot for users that do not need richer query, retrieval, or editing APIs that are typically provided by document databases. Modern key-value stores often include features for working with metadata, blurring the lines between document stores.\n\n=== Relationship to search engines ===\nSome search engines (aka [[information retrieval]]) systems like [[Elasticsearch]] provide enough of the core operations on documents to fit the definition of a document-oriented database.\n\n=== Relationship to relational databases ===\n\n{{cleanup|section|reason="Requires cleanup"|date=July 2016}}\n\nIn a relational database, data is first categorized into a number of predefined types, and \'\'tables\'\' are created to hold individual entries, or \'\'records\'\', of each type. The tables define the data within each record\'s \'\'fields\'\', meaning that every record in the table has the same overall form. The administrator also defines the \'\'relationships\'\' between the tables, and selects certain fields that they believe will be most commonly used for searching and defines \'\'indexes\'\' on them. A key concept in the relational design is that any data that may be repeated is normally placed in its own table, and if these instances are related to each other, a column is selected to group them together, the \'\'foreign key\'\'. This design is known as \'\'[[database normalization]]\'\'.<ref>{{cite web |url=https://support.microsoft.com/en-ca/kb/283878 |title=Description of the database normalization basics |website=Microsoft}}</ref>\n\nFor example, an address book application will generally need to store the contact name, an optional image, one or more phone numbers, one or more mailing addresses, and one or more email addresses. In a canonical relational database solution, tables would be created for each of these rows with predefined fields for each bit of data: the CONTACT table might include FIRST_NAME, LAST_NAME and IMAGE columns, while the PHONE_NUMBER table might include COUNTRY_CODE, AREA_CODE, PHONE_NUMBER and TYPE (home, work, etc.). The PHONE_NUMBER table also contains a foreign key column, "CONTACT_ID", which holds the unique ID number assigned to the contact when it was created. In order to recreate the original contact, the database engine uses the foreign keys to look for the related items across the group of tables and reconstruct the original data.\n\nIn contrast, in a document-oriented database there may be no internal structure that maps directly onto the concept of a table, and the fields and relationships generally don\'t exist as predefined concepts. Instead, all of the data for an object is placed in a single document, and stored in the database as a single entry. In the address book example, the document would contain the contact\'s name, image, and any contact info, all in a single record. That entry is accessed through its key, which allows the database to retrieve and return the document to the application. No additional work is needed to retrieve the related data; all of this is returned in a single object.\n\nA key difference between the document-oriented and relational models is that the data formats are not predefined in the document case. In most cases, any sort of document can be stored in any database, and those documents can change in type and form at any time. If one wishes to add a COUNTRY_FLAG to a CONTACT, this field can be added to new documents as they are inserted, this will have no effect on the database or the existing documents already stored. To aid retrieval of information from the database, document-oriented systems generally allow the administrator to provide \'\'hints\'\' to the database to look for certain types of information. These work in a similar fashion to indexes in the relational case. Most also offer the ability to add additional metadata outside of the content of the document itself, for instance, tagging entries as being part of an address book, which allows the programmer to retrieve related types of information, like "all the address book entries". This provides functionality similar to a table, but separates the concept (categories of data) from its physical implementation (tables).\n\nIn the classic normalized relational model, objects in the database are represented as separate rows of data with no inherent structure beyond that given to them as they are retrieved. This leads to problems when trying to translate programming objects to and from their associated database rows, a problem known as [[object-relational impedance mismatch]].<ref>{{cite web |url=http://www.agiledata.org/essays/impedanceMismatch.html |title=The Object-Relational Impedance Mismatch |first=Scott |last=Wambler |website=Agile Data}}</ref> Document stores more closely, or in some cases directly, map programming objects into the store. This eliminates the impedance mismatch problem, and is offered as one of the main advantages of the NoSQL approach.\n\n== Implementations ==\n{{main cat|Document-oriented databases}}\n\n{| class="wikitable sortable"\n|-\n! Name\n! Publisher\n! License\n! Languages supported\n! Notes\n! [[Representational State Transfer|RESTful]] API\n|-\n| [[BaseX]]\n| BaseX Team\n| {{free|[[BSD License]]}}\n| [[Java (programming language)|Java]], [[XQuery]]\n| Support for XML, JSON and binary formats; client-/server based architecture; concurrent structural and full-text searches and updates.\n| {{yes}}\n|-\n| [[InterSystems Caché|Caché]]\n| [[InterSystems]] Corporation\n| {{proprietary}}\n| [[Java (programming language)|Java]], [[C Sharp (programming language)|C#]], [[Node.js]]\n| Commonly used in Health, Business and Government applications.\n| {{yes}}\n|-\n| [[Cloudant]]\n| Cloudant, Inc.\n| {{proprietary}}\n| [[Erlang (programming language)|Erlang]], [[Java (programming language)|Java]], [[Scala (programming language)|Scala]], and [[C (programming language)|C]]\n| Distributed database service based on [[BigCouch]], the company\'s [[open source]] fork of the [[Apache Software Foundation|Apache]]-backed [[CouchDB]] project.  Uses JSON model.\n| {{yes}}\n|-\n| [[Clusterpoint|Clusterpoint Database]]\n| Clusterpoint Ltd.\n| {{proprietary}} with free download\n| [[JavaScript]], [[SQL]], [[PHP]], [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[C (programming language)|C]], [[C++]], \n| Distributed document-oriented XML / JSON database platform with [[ACID]]-compliant [[transaction processing|transactions]]; [[high-availability]] [[data replication]] and [[sharding]]; built-in [[full text search]] engine with [[relevance]] [[ranking]]; JS/SQL [[query language]]; [[Geographic information system|GIS]]; Available as pay-per-use [[cloud database|database as a service]] or as an on-premise free software download.<ref>[http://www.clusterpoint.com Document-oriented Database]. Clusterpoint. Retrieved on 2015-10-08.</ref>\n| {{yes}}\n|-\n| [[Couchbase Server]]\n| [[Couchbase, Inc.]]\n| {{free|[[Apache License]]}}\n| [[C]], [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[PHP]], [[SQL]], [[GoLang]], [[Spring Framework]], [[LINQ]]\n|Distributed NoSQL Document Database, JSON model and SQL based Query Language.\n| {{yes}}<ref>[http://www.couchbase.com/docs/ Documentation]. Couchbase. Retrieved on 2013-09-18.</ref>\n|-\n| [[CouchDB]]\n| [[Apache Software Foundation]]\n| {{free|[[Apache License]]}}\n| Any language that can make HTTP requests\n| JSON over REST/HTTP with [[Multi-Version Concurrency Control]] and limited [[ACID]] properties. Uses [[map (higher-order function)|map]] and [[fold (higher-order function)|reduce]] for views and queries.<ref>[http://couchdb.apache.org/docs/overview.html CouchDB Overview] {{webarchive |url=https://web.archive.org/web/20111020074113/http://couchdb.apache.org/docs/overview.html |date=October 20, 2011 }}</ref>\n| {{yes}}<ref>[http://wiki.apache.org/couchdb/HTTP_Document_API CouchDB Document API]</ref>\n|-\n| [[CrateIO]]\n| CRATE Technology GmbH\n| {{free|[[Apache License]]}}\n| [[Java (programming language)|Java]]\n| Use familiar SQL syntax for real time distributed queries across a cluster. Based on Lucene / Elasticsearch ecosystem with built-in support for binary objects (BLOBs).\n| {{yes}}<ref>{{cite web|url=https://crate.io/docs/stable/sql/rest.html |title=Archived copy |accessdate=2015-06-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20150622174526/https://crate.io/docs/stable/sql/rest.html |archivedate=2015-06-22 |df= }}</ref>\n|-\n|djondb\n|djondb.com\n|GNU GPL and Commercial\n|C, .Net, Java, Python, NodeJS, PHP.\n|Document Store with support to transactions.\n|{{No}}\n|-\n| [[DocumentDB]]\n| Microsoft\n| {{proprietary}}\n| [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[JavaScript]], [[SQL]]\n| Platform-as-a-Service offering, part of the [[Microsoft Azure]] platform.\n| {{yes}}\n|-\n|[[Elasticsearch]]\n|[[Shay Banon]]\n|{{free|[[Apache License]]}}\n|[[Java (programming language)|Java]]\n|[[JSON]], Search engine.\n|{{yes}}\n|-\n| [[eXist]]\n| eXist\n| {{free|[[LGPL]]}}\n| [[XQuery]], [[Java (programming language)|Java]]\n| XML over REST/HTTP, WebDAV, Lucene Fulltext search, binary data support, validation, versioning, clustering, triggers, URL rewriting, collections, ACLS, XQuery Update\n| {{yes}}<ref>[http://exist-db.org eXist-db Open Source Native XML Database]. Exist-db.org. Retrieved on 2013-09-18.</ref>\n|-\n| [[HyperDex]]\n| hyperdex.org\n| {{free|[[BSD License]]}}\n| [[C (programming language)|C]], [[C++]], [[Go (programming language)|Go]], [[Node.js]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]] \n| Support for [[JSON]] and binary documents.\n| {{no}}\n|-\n| [[Informix]]\n| IBM\n| Proprietary, with no-cost editions<ref>http://www.ibm.com/developerworks/data/library/techarticle/dm-0801doe/</ref>\n| Various (Compatible with MongoDB API)\n| RDBMS with JSON, replication, sharding and ACID compliance.\n| {{yes}}\n|-\n|[[Apache Jackrabbit|Jackrabbit]]\n|Apache Foundation\n|{{free|[[Apache License]]}}\n|[[Java (programming language)|Java]]\n|[[Java Content Repository]] implementation\n|{{dunno}}\n|-\n| [[Lotus Notes]] ([[IBM Lotus Domino]])\n| IBM\n| {{proprietary}}\n| [[LotusScript]], [[Java (programming language)|Java]], Lotus @Formula\n| [[MultiValue]]\n| {{yes}}\n|-\n| [[MarkLogic]]\n| MarkLogic Corporation\n| Free Developer license or Commercial<ref>http://developer.marklogic.com/licensing</ref>\n| [[REST]], [[Java (programming language)|Java]], [[JavaScript]], [[Node.js]], [[XQuery]], [[SPARQL]], [[XSLT]], [[C++]]\n| Distributed document-oriented database for JSON, XML, and [[Resource Description Framework|RDF triples]]. Built-in [[Full text search]], [[ACID]] transactions, [[High availability]] and [[Disaster recovery]], certified security.\n| {{yes}}\n|-\n| [[MongoDB]]\n| MongoDB, Inc\n| {{free|[[Affero General Public License|GNU AGPL v3.0]] for the DBMS, [[Apache 2 License]] for the client drivers}}<ref>[http://www.mongodb.org/about/licensing/ MongoDB Licensing]</ref>\n| [[C (programming language)|C]], [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[Perl]], [[PHP]], [[Python (programming language)|Python]], [[Node.js]], [[Ruby (programming language)|Ruby]], [[Scala (programming language)|Scala]] <ref>[http://docs.mongodb.org/ecosystem/drivers/community-supported-drivers/ Additional 30+ community MongoDB supported drivers]</ref>\n| Document database with replication and sharding, [[BSON]] store (binary format [[JSON]]).\n| {{yes}}<ref>[http://www.mongodb.org/display/DOCS/Http+Interface#HttpInterface-RESTInterfaces MongoDB REST Interfaces]</ref>\n|-\n| [[MUMPS]] Database\n| {{dunno}}\n| [[proprietary software|Proprietary]] and [[Affero General Public License|Affero GPL]]<ref>[http://sourceforge.net/projects/fis-gtm/ GTM MUMPS FOSS on SourceForge]</ref>\n| [[MUMPS]]\n| Commonly used in health applications.\n| {{dunno}}\n|-\n| [[ObjectDatabase++]]\n| Ekky Software\n| {{proprietary}}\n| [[C++]], [[C Sharp (programming language)|C#]], [[TScript]]\n| Binary Native C++ class structures\n| {{dunno}}\n|-\n| [[OrientDB]]\n| Orient Technologies\n| {{free|[[Apache License]]}}\n| [[Java (programming language)|Java]]\n| JSON over HTTP, SQL support, [[ACID]] transactions\n| {{yes}}\n|-\n| [[PostgreSQL]]\n| PostgreSQL\n| {{free | [http://www.postgresql.org/about/licence/ PostgreSQL Free License]}}\n| [[C (programming language)|C]]\n| HStore, JSON store (9.2+), JSON function (9.3+), HStore2 (9.4+), JSONB (9.4+)\n|{{no}}\n|-\n| [[Qizx]]\n| [[Qualcomm]]\n| [[Commercial software|Commercial]]\n| [[REST]], [[Java (programming language)|Java]], [[XQuery]], [[XSLT]], [[C (programming language)|C]], [[C++]], [[Python (programming language)|Python]]\n| Distributed document-oriented [[XML database]] with integrated [[full text search]]; support for [[JSON]], text, and binaries.\n|{{yes}}\n|-\n| [[RethinkDB]]\n| {{dunno}}\n| {{free|[[Affero General Public License|GNU AGPL]] for the DBMS, [[Apache 2 License]] for the client drivers}}\n| [[C++]], [[Python (programming language)|Python]], [[JavaScript]], [[Ruby (programming language)|Ruby]], [[Java (programming language)|Java]]\n| Distributed document-oriented [[JSON]] database with replication and sharding.\n|{{no}}\n|-\n| [[Rocket U2]]\n| Rocket Software\n| {{proprietary}}\n| {{dunno}}\n| UniData, UniVerse\n|{{yes}} (Beta)\n|-\n| [[Sedna (database)|Sedna]]\n|sedna.org\n|{{free|[[Apache License]]}}\n|[[C++]], [[XQuery]]\n|[[XML database]]\n|{{no}}\n|-\n| [[SimpleDB]]\n| Amazon\n| Proprietary online service\n|[[Erlang (programming language)|Erlang]]\n|\n|{{dunno}}\n|-\n| [[Solr]]\n| Apache\n|{{free|[[Apache License]]}}\n|[[Java (programming language)|Java]]\n|Search engine\n|{{yes}}\n|-\n| [[TokuMX]]\n|Tokutek\n|{{free|[[GNU Affero General Public License]]}}\n|[[C++]], [[C Sharp (programming language)|C#]], [[Go (Programming language)|Go]]\n|[[MongoDB]] with [[Fractal tree index|Fractal Tree indexing]]\n|{{dunno}}\n|-\n| [[Virtuoso Universal Server|OpenLink Virtuoso]]\n| [[OpenLink Software]]\n|GPLv2[1] and proprietary\n|[[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[SPARQL]]\n|[[Middleware]] and [[database engine]] hybrid\n|{{yes}}\n|}\n\n===XML database implementations===\n{{Further information|XML database}}\nMost XML databases are document-oriented databases.\n\n== See also ==\n* [[Database theory]]\n* [[Data hierarchy]]\n* [[Full text search]]\n* [[In-memory database]]\n* [[Internet Message Access Protocol]] (IMAP)\n* [[Machine-Readable Documents]]\n* [[NoSQL]]\n* [[Object database]]\n* [[Online database]]\n* [[Real time database]]\n* [[Relational database]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* Assaf Arkin. (2007, September 20). [https://web.archive.org/web/20080327222152/http://blog.labnotes.org:80/2007/09/20/read-consistency-dumb-databases-smart-services/ Read Consistency: Dumb Databases, Smart Services.]\n{{refend}}\n\n==External links==\n* [http://db-engines.com/en/ranking/document+store DB-Engines Ranking of Document Stores] by popularity, updated monthly\n\n{{Database models}}\n{{Databases}}\n\n[[Category:Document-oriented databases| ]]\n[[Category:Data management]]\n[[Category:Database management systems]]\n[[Category:Types of databases]]']
['NoSQL', '23968131', '{{Redirect|Structured storage|the Microsoft technology also known as structured storage|COM Structured Storage}}\nA \'\'\'NoSQL\'\'\' (originally referring to "non SQL", "non relational" or "not only SQL")<ref>http://nosql-database.org/ "NoSQL DEFINITION: Next Generation Databases mostly addressing some of the points: being non-relational, distributed, open-source and horizontally scalable"</ref> database provides a mechanism for [[Computer data storage|storage]] and [[data retrieval|retrieval]] of data which is modeled in means other than the tabular relations used in [[relational database]]s. Such databases have existed since the late 1960s, but did not obtain the "NoSQL" moniker until a surge of popularity in the early twenty-first century,{{r|leavitt}} triggered by the needs of [[Web 2.0]] companies such as [[Facebook]], [[Google]], and [[Amazon.com]].<ref>{{cite conference |title=History Repeats Itself: Sensible and NonsenSQL Aspects of the NoSQL Hoopla |first=C. |last=Mohan |conference=Proc. 16th Int\'l Conf. on Extending Database Technology |year=2013 |url=http://openproceedings.eu/2013/conf/edbt/Mohan13.pdf}}</ref><ref>http://www.eventbrite.com/e/nosql-meetup-tickets-341739151 "Dynamo clones and BigTables"</ref><ref>http://www.wired.com/2012/01/amazon-dynamodb/ "Amazon helped start the “NoSQL” movement."</ref> NoSQL databases are increasingly used in [[big data]] and [[real-time web]] applications.<ref>{{cite web|url= http://db-engines.com/en/blog_post/23 |title= RDBMS dominate the database market, but NoSQL systems are catching up |publisher= DB-Engines.com |date= 21 Nov 2013 |accessdate= 24 Nov 2013 }}</ref>   NoSQL systems are also sometimes called "Not only SQL" to emphasize that they may support [[SQL]]-like query languages.<ref>{{cite web|url=http://searchdatamanagement.techtarget.com/definition/NoSQL-Not-Only-SQL|title=NoSQL (Not Only SQL)|quote=NoSQL database, also called Not Only SQL}}</ref><ref>{{cite web | url = http://martinfowler.com/bliki/NosqlDefinition.html | title = NosqlDefinition | first = Martin | last = Fowler | authorlink = Martin Fowler | quote = many advocates of NoSQL say that it does not mean a "no" to SQL, rather it means Not Only SQL }}</ref>\n\nMotivations for this approach include: simplicity of design, simpler [[Horizontal scaling#Horizontal and vertical scaling|"horizontal" scaling]] to [[cluster computing|clusters]] of machines  (which is a problem for relational databases),<ref name="leavitt">{{cite journal |first=Neal |last=Leavitt |title=Will NoSQL Databases Live Up to Their Promise? |journal=[[IEEE Computer]] |year=2010 |url=http://www.leavcom.com/pdf/NoSQL.pdf}}</ref> and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.  Sometimes the data structures used by NoSQL databases are also viewed as "more flexible" than relational database tables.<ref>http://www.allthingsdistributed.com/2012/01/amazon-dynamodb.html "Customers like SimpleDB’s table interface and its flexible data model. Not having to update their schemas when their systems evolve makes life much easier"</ref>\n\nMany NoSQL stores compromise [[consistency (database systems)|consistency]] (in the sense of the [[CAP theorem]]) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance the lack of ability to perform ad-hoc JOINs across tables), lack of standardized interfaces, and huge previous investments in existing relational databases.<ref>{{cite web\n| url         = http://www.journalofcloudcomputing.com/content/pdf/2192-113X-2-22.pdf\n| title       = Data management in cloud environments: NoSQL and NewSQL data stores\n| first1 = K. | last1 = Grolinger | first2 = W. A. | last2 = Higashino | first3 = A. | last3 = Tiwari | first4 = M. A. M. | last4 = Capretz\n| date = 2013\n| publisher   = Aira, Springer\n| accessdate  = 8 Jan 2014\n}}\n</ref>\nMost NoSQL stores lack true [[ACID]] transactions, although a few databases, such as [[MarkLogic]], [[Aerospike database|Aerospike]], FairCom [[c-treeACE]], Google [[Spanner (database)|Spanner]] (though technically a [[NewSQL]] database), Symas [[Lightning Memory-Mapped Database|LMDB]], and [[OrientDB]] have made them central to their designs. (See [[#ACID and JOIN Support|ACID and JOIN Support]].)\n\nInstead, most NoSQL databases offer a concept of "eventual consistency" in which database changes are propagated to all nodes "eventually" (typically within milliseconds) so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads.<ref>https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads</ref>  Additionally, some NoSQL systems may exhibit lost writes and other forms of [[data loss]].<ref>Martin Zapletal: Large volume data analysis on the Typesafe Reactive Platform, ScalaDays 2015, [http://www.slideshare.net/MartinZapletal/zapletal-martinlargevolumedataanalytics Slides]</ref> Fortunately, some NoSQL systems provide concepts such as [[write-ahead logging]] to avoid data loss.<ref>http://www.dummies.com/how-to/content/10-nosql-misconceptions.html "NoSQL databases lose data" section</ref> For [[distributed transaction processing]] across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Even current relational databases "do not allow referential integrity constraints to span databases."<ref>https://iggyfernandez.wordpress.com/2013/07/28/no-to-sql-and-no-to-nosql/</ref> There are few systems that maintain both [[ACID]] transactions and [[X/Open XA]] standards for distributed transaction processing.\n\n== History ==\nThe term \'\'NoSQL\'\' was used by Carlo Strozzi in 1998 to name his lightweight, [[Strozzi NoSQL (RDBMS)|Strozzi NoSQL open-source relational database]] that did not expose the standard [[SQL|Structured Query Language]] (SQL) interface, but was still relational.<ref name=":0">{{cite web\n| url         = http://publications.lib.chalmers.se/records/fulltext/123839.pdf\n| title       = Investigating storage solutions for large data: A comparison of well performing and scalable data storage solutions for real time extraction and batch insertion of data\n| first       = Adam\n| last        = Lith\n| first2 = Jakob | last2 = Mattson\n| date        = 2010\n| publisher   = Department of Computer Science and Engineering, Chalmers University of Technology\n| location    = Göteborg\n| page        = 70\n| accessdate  = 12 May 2011\n| quote       = Carlo Strozzi first used the term NoSQL in 1998 as a name for his open source relational database that did not offer a SQL interface[...]\n}}\n</ref>  His NoSQL RDBMS is distinct from the circa-2009 general concept of NoSQL databases.  Strozzi suggests that, because the current NoSQL movement "departs from the relational model altogether, it should therefore have been called more appropriately \'NoREL\'",<ref>{{cite web|url=http://www.strozzi.it/cgi-bin/CSA/tw7/I/en_US/nosql/Home%20Page |title=NoSQL Relational Database Management System: Home Page |publisher=Strozzi.it |date=2 October 2007 |accessdate=29 March 2010}}</ref> referring to \'No Relational\'.\n\nJohan Oskarsson of [[Last.fm]] reintroduced the term \'\'NoSQL\'\' in early 2009 when he organized an event to discuss "open source [[distributed database|distributed, non relational databases]]".<ref>{{cite web|url= http://blog.sym-link.com/2009/05/12/nosql_2009.html |title= NoSQL 2009 |publisher= Blog.sym-link.com |date= 12 May 2009 |accessdate= 29 March 2010 }}</ref> The name attempted to label the emergence of an increasing number of non-relational, distributed data stores, including open source clones of Google\'s BigTable/MapReduce and Amazon\'s Dynamo. Most of the early NoSQL systems did not attempt to provide [[ACID|atomicity, consistency, isolation and durability]] guarantees, contrary to the prevailing practice among relational database systems.<ref>{{cite web|url= http://databases.about.com/od/specificproducts/a/acid.htm |title= The ACID Model| first = Mike | last = Chapple }}</ref>\n\nBased on 2014 revenue, the NoSQL market leaders are [[MarkLogic]], [[MongoDB]], and [[Datastax]].<ref>{{cite web|accessdate=2015-11-17|url=http://wikibon.com/hadoop-nosql-software-and-services-market-forecast-2013-2017/|title=Hadoop-NoSQL-rankings}}</ref> Based on 2015 popularity rankings, the most popular NoSQL databases are [[MongoDB]], [[Apache Cassandra]], and [[Redis]].<ref>{{cite web|accessdate=2015-07-31|url=http://db-engines.com/en/ranking|title=DB-Engines Ranking}}</ref>\n\n== Types and examples of NoSQL databases ==\nThere have been various approaches to classify NoSQL databases, each with different categories and subcategories, some of which overlap. What follows is a basic classification by data model, with examples:\n* \'\'\'[[Column (data store)|Column]]\'\'\': [[Accumulo]], [[Apache Cassandra|Cassandra]], [[Druid (open-source data store)|Druid]], [[HBase]], [[Vertica]], [[SAP HANA]]\n* \'\'\'[[Document-oriented database|Document]]\'\'\': [[Apache CouchDB]], [[Clusterpoint]], [[Couchbase]], [[DocumentDB]], [[HyperDex]], [[Lotus Notes|IBM Domino]], [[MarkLogic]], [[MongoDB]], [[OrientDB]], [[Qizx]], [[RethinkDB]]\n* \'\'\'[[Key-value store|Key-value]]\'\'\': [[Aerospike database|Aerospike]], [[Couchbase]], [[Dynamo (storage system)|Dynamo]], FairCom [[c-treeACE]], [[FoundationDB]], [[HyperDex]], [[MemcacheDB]], [[MUMPS]], [[Oracle NoSQL Database]], [[OrientDB]], [[Redis]], [[Riak]], [[Berkeley DB]]\n* \'\'\'[[Graph database|Graph]]\'\'\': [[AllegroGraph]], ArangoDB, [[InfiniteGraph]], [[Apache Giraph]], [[MarkLogic]], [[Neo4J]], [[OrientDB]], [[Virtuoso Universal Server|Virtuoso]], [[Stardog]]\n* \'\'\'[[Multi-model database|Multi-model]]\'\'\': Alchemy Database, ArangoDB, CortexDB, [[Couchbase]], [[FoundationDB]], [[MarkLogic]], [[OrientDB]]\n\nA more detailed classification is the following, based on one from Stephen Yen:<ref>{{cite web|url=https://dl.dropboxusercontent.com/u/2075876/nosql-steve-yen.pdf|format=PDF|title=NoSQL is a Horseless Carriage|last=Yen|first=Stephen|publisher=NorthScale|accessdate=2014-06-26}}.</ref>\n\n{| style="text-align: left;" class="wikitable sortable"\n|-\n! Type !! Examples of this type\n|-\n| Key-Value Cache || [[Oracle Coherence|Coherence]], [[IBM WebSphere eXtreme Scale|eXtreme Scale]], [[GigaSpaces]],  GemFire, [[Hazelcast]], [[Infinispan]], JBoss Cache, [[Memcached]], Repcached, [[Terracotta, Inc.|Terracotta]], [[Velocity (memory cache)|Velocity]]\n|-\n| Key-Value Store || Flare, Keyspace, RAMCloud, SchemaFree, [[Hyperdex]], [[Aerospike database|Aerospike]]\n|-\n| Key-Value Store (Eventually-Consistent) || DovetailDB, [[Oracle NoSQL Database]], [[Dynamo (storage system)|Dynamo]], [[Riak]], Dynomite, MotionDb, [[Voldemort (distributed data store)|Voldemort]], SubRecord\n|-\n| Key-Value Store (Ordered) || Actord, [[FoundationDB]], Lightcloud, [[Lightning Memory-Mapped Database|LMDB]], Luxio, [[MemcacheDB]],  NMDB, Scalaris, TokyoTyrant\n|-\n| Data-Structures Server || [[Redis]]\n|-\n| Tuple Store || [[Jini|Apache River]], Coord, [[GigaSpaces]]\n|-\n| Object Database || DB4O, [[Objectivity/DB]], [[Perst]], Shoal, [[Zope Object Database|ZopeDB]]\n|-\n| Document Store || [[Clusterpoint]], [[Couchbase]], [[CouchDB]], [[DocumentDB]], [[Lotus Notes|IBM Domino]], [[MarkLogic]], [[MongoDB]], [[Qizx]], [[RethinkDB]], [[XML database|XML-databases]]\n|-\n| [[Wide column store|Wide Column Store]] || [[BigTable]], [[Apache Cassandra|Cassandra]], [[Druid (open-source data store)|Druid]], [[Apache HBase|HBase]], [[Hypertable]], KAI, KDI, OpenNeptune, Qbase\n|}\n\n[[Correlation database]]s are model-independent, and instead of row-based or column-based storage, use value-based storage.\n\n=== Key-value store ===\n{{main|Key-value database}}\nKey-value (KV) stores use the [[associative array]] (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection.<ref>{{cite web\n| accessdate =1 January 2012\n| publisher = Stackexchange\n| location = http://dba.stackexchange.com/questions/607/what-is-a-key-value-store-database\n| title = Key Value stores and the NoSQL movement\n| author = Sandy\n| date = 14 January 2011\n| url = http://dba.stackexchange.com/a/619\n| quote = Key-value stores allow the application developer to store schema-less data. This data usually consists of a string that represents the key, and the actual data that is considered the value in the "key-value" relationship. The data itself is usually some kind of primitive of the programming language (a string, an integer, or an array) or an object that is being marshaled by the programming language\'s bindings to the key-value store. This structure replaces the need for a fixed data model and allows proper formatting.}}</ref><ref>{{cite web\n| accessdate =1 January 2012\n| publisher = Marc Seeger\n| location = http://blog.marc-seeger.de/2009/09/21/key-value-stores-a-practical-overview/\n| title = Key-Value Stores: a practical overview\n| first = Marc | last = Seeger\n| date = 21 September 2009\n| url = http://blog.marc-seeger.de/assets/papers/Ultra_Large_Sites_SS09-Seeger_Key_Value_Stores.pdf\n| quote = Key-value stores provide a high-performance alternative to relational database systems with respect to storing and accessing data. This paper provides a short overview of some of the currently available key-value stores and their interface to the Ruby programming language.}}</ref>\n\nThe key-value model is one of the simplest non-trivial data models, and richer data models are often implemented as an extension of it. The key-value model can be extended to a discretely ordered model that maintains keys in [[Lexicographical order|lexicographic order]]. This extension is computationally powerful, in that it can efficiently retrieve selective key \'\'ranges\'\'.<ref>{{cite web\n| accessdate =8 May 2014\n| publisher = Ilya Katsov\n| title = NoSQL Data Modeling Techniques \n| first = Ilya | last = Katsov\n| date = 1 March 2012\n| url = http://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/}}</ref>\n\nKey-value stores can use [[consistency model]]s ranging from [[eventual consistency]] to [[serializability]]. Some databases support ordering of keys. There are various hardware implementations, and some users maintain data in memory (RAM), while others employ [[solid-state drive]]s or [[hard disk drive|rotating disks]].\n\nExamples include [[Oracle NoSQL Database]], [[Redis]], and [[dbm]].\n\n=== Document store ===\n{{main|Document-oriented database|XML database}}\nThe central concept of a document store is the notion of a "document". While each document-oriented database implementation differs on the details of this definition, in general, they all assume that documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include XML, [[YAML]], and [[JSON]] as well as binary forms like [[BSON]].  Documents are addressed in the database via a unique \'\'key\'\' that represents that document. One of the other defining characteristics of a document-oriented database is that in addition to the key lookup performed by a key-value store, the database offers an API or query language that retrieves documents based on their contents.\n\nDifferent implementations offer different ways of organizing and/or grouping documents:\n* Collections\n* Tags\n* Non-visible metadata\n* Directory hierarchies\n\nCompared to relational databases, for example, collections could be considered analogous to tables and documents analogous to records. But they are different: every record in a table has the same sequence of fields, while documents in a collection may have fields that are completely different.\n\n=== Graph ===\n{{main|Graph database}}\n\nThis kind of database is designed for data whose relations are well represented as a [[graph (discrete mathematics)|graph]] consisting of elements interconnected with a finite number of relations between them. The type of data could be social relations, public transport links, road maps or network topologies.\n\n; Graph databases and their query language\n{| style="text-align: left;" class="wikitable sortable"\n ! Name !! Language(s) !! Notes\n |-\n | [[AllegroGraph]] || [[SPARQL]] || [[Resource Description Framework|RDF]] triple store\n |-\n | [[DEX (Graph database)|DEX/Sparksee]] || [[C++]], [[Java (programming language)|Java]], [[.NET Framework|.NET]], [[Python (programming language)|Python]] || [[Graph database]]\n |-\n | [[FlockDB]] || [[Scala (programming language)|Scala]] || [[Graph database]]\n |-\n | [[IBM DB2]] || [[SPARQL]] || [[Resource Description Framework|RDF]] triple store added in DB2 10\n |-\n | [[InfiniteGraph]] || [[Java (programming language)|Java]] || [[Graph database]]\n |-\n | [[MarkLogic]] || [[Java (programming language)|Java]], [[JavaScript]], [[SPARQL]], [[XQuery]] || Multi-model [[Document-oriented database|document database]] and [[Resource Description Framework|RDF]] triple store\n |-\n | [[Neo4j]] || [[Cypher Query Language|Cypher]] || [[Graph database]]\n |-\n | [[Ontotext|OWLIM]] || [[Java (programming language)|Java]], [[SPARQL|SPARQL 1.1]]|| [[Resource Description Framework|RDF]] triple store\n |-\n |-\n | [[Oracle Database|Oracle]] || [[SPARQL|SPARQL 1.1]] || [[Resource Description Framework|RDF]] triple store added in 11g\n |-\n | [[OrientDB]] || [[Java (programming language)|Java]], SQL || Multi-model [[Document-oriented database|document]] and [[graph database]]\n |-\n | [[sqrrl|Sqrrl Enterprise]] || [[Java (programming language)|Java]] || [[Graph database]]\n |-\n | [[Virtuoso Universal Server|OpenLink Virtuoso]] || [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[SPARQL]] || [[Middleware]] and [[database engine]] hybrid\n |-\n | [[Stardog]] || [[Java (programming language)|Java]], [[SPARQL]] || [[Graph database]]\n|}\n\n=== Object database ===\n{{main|Object database}}\n* [[db4o]]\n* [[Gemstone (database)|GemStone/S]]\n* [[InterSystems Caché]]\n* [[JADE (programming language)|JADE]]\n* [[ObjectDatabase++]]\n* [[ObjectDB]]\n* [[Objectivity/DB]]\n* [[ObjectStore]]\n* [[Odaba|ODABA]]\n* [[Perst]]\n* [[Virtuoso Universal Server|OpenLink Virtuoso]]\n* [[Versant Object Database]]\n* [[ZODB]]\n\n=== Tabular ===\n* [[Apache Accumulo]]\n* [[BigTable]]\n* [[HBase|Apache Hbase]]\n* [[Hypertable]]\n* [[Mnesia]]\n* [[Virtuoso Universal Server|OpenLink Virtuoso]]\n\n=== Tuple store ===\n* [[Apache River]]\n* [[GigaSpaces]]\n* [[Tarantool]]\n* [[TIBCO Software|TIBCO]] ActiveSpaces\n* [[Virtuoso Universal Server|OpenLink Virtuoso]]\n\n=== Triple/quad store (RDF) database ===\n{{main|Triplestore|Named graph}}\n* [[AllegroGraph]]\n* [[Jena (framework)|Apache JENA]] (It is a framework, not a database)\n* [[MarkLogic]]\n* [[Ontotext|Ontotext-OWLIM]]\n* [[Oracle NoSQL Database|Oracle NoSQL database]]\n* [[Virtuoso Universal Server]]\n* [[Stardog]]\n\n=== Hosted ===\n* [[Amazon DynamoDB]]\n* [[Amazon SimpleDB]]\n* [[Appengine|Datastore on Google Appengine]]\n* [[Clusterpoint|Clusterpoint database]]\n* [[Cloudant|Cloudant Data Layer (CouchDB)]]\n* [[Freebase (database)|Freebase]]\n* [[Microsoft Azure#Table Service|Microsoft Azure Tables]]<ref>http://azure.microsoft.com/en-gb/services/storage/tables/</ref>\n* [[DocumentDB|Microsoft Azure DocumentDB]]<ref>http://azure.microsoft.com/en-gb/services/documentdb/</ref>\n* [[Virtuoso Universal Server|OpenLink Virtuoso]]\n\n=== Multivalue databases ===\n* D3 [[Pick database]]\n* [[Extensible Storage Engine]] (ESE/NT)\n* [[InfinityDB]]\n* [[InterSystems Caché]]\n* jBASE [[Pick database]]\n* [[Northgate Information Solutions]] Reality, the original Pick/MV Database\n* [[OpenQM]]\n* Revelation Software\'s [[OpenInsight]]\n* [[Rocket U2]]\n\n=== Multimodel database ===\n\n* [[Couchbase]]\n* [[FoundationDB]]\n* [[MarkLogic]]\n* [[OrientDB]]\n\n== Performance ==\nBen Scofield rated different categories of NoSQL databases as follows:<ref>{{cite web|url=http://www.slideshare.net/bscofield/nosql-codemash-2010|title=NoSQL - Death to Relational Databases(?)|last=Scofield|first=Ben |date=2010-01-14|accessdate=2014-06-26}}</ref>\n\n{| style="text-align: left;" class="wikitable sortable"\n|-\n! Data Model !! Performance !! Scalability !! Flexibility !! Complexity !! Functionality\n|-\n| Key–Value Store ||  high || high || high || none || variable (none)\n|-\n| Column-Oriented Store || high || high || moderate || low || minimal\n|-\n| Document-Oriented Store || high || variable (high) || high || low || variable (low)\n|-\n| Graph Database || variable || variable || high || high || [[graph theory]]\n|-\n| Relational Database || variable || variable || low || moderate || [[relational algebra]]\n|}\n\nPerformance and scalability comparisons are sometimes done with the [[YCSB]] benchmark.\n\n{{see also|Comparison of structured storage software}}\n\n== Handling relational data ==\nSince most NoSQL databases lack ability for joins in queries, the [[database schema]] generally needs to be designed differently. There are three main techniques for handling relational data in a NoSQL database. (See table Join and ACID Support for NoSQL databases that support joins.)\n\n=== Multiple queries ===\nInstead of retrieving all the data with one query, it\'s common to do several queries to get the desired data. NoSQL queries are often faster than traditional SQL queries so the cost of having to do additional queries may be acceptable. If an excessive number of queries would be necessary, one of the other two approaches is more appropriate.\n\n=== Caching/replication/non-normalized data ===\nInstead of only storing foreign keys, it\'s common to store actual foreign values along with the model\'s data. For example, each blog comment might include the username in addition to a user id, thus providing easy access to the username without requiring another lookup. When a username changes however, this will now need to be changed in many places in the database. Thus this approach works better when reads are much more common than writes.<ref name="DataModeling-Couchbase.com_December_5_2014c">{{cite web |url=http://www.couchbase.com/sites/default/files/uploads/all/whitepapers/Couchbase_Whitepaper_Transitioning_Relational_to_NoSQL.pdf |title=Making the Shift from Relational to NoSQL\n |newspaper=Couchbase.com |accessdate= December 5, 2014}}</ref>\n\n=== Nesting data ===\nWith document databases like MongoDB it\'s common to put more data in a smaller number of collections. For example, in a blogging application, one might choose to store comments within the blog post document so that with a single retrieval one gets all the comments. Thus in this approach a single document contains all the data you need for a specific task.\n\n== ACID and JOIN Support ==\n\nIf a database is marked as supporting [[ACID]] or [[Join (SQL)|joins]], then the documentation for the database makes that claim. The degree to which the capability is fully supported in a manner similar to most SQL databases or the degree to which it meets the needs of a specific application is left up to the reader to assess.\n{| class="wikitable"\n|-\n! Database !! ACID !! Joins\n|-\n| [[Aerospike (company)|Aerospike]] || {{Yes}} || {{No}}\n|-\n| ArangoDB || {{Yes}} || {{Yes}}\n|-\n| [[CouchDB]] || {{Yes}} || {{Yes}}\n|-\n| [[c-treeACE]] || {{Yes}} || {{Yes}}\n|-\n| [[HyperDex]] || {{Yes}}{{refn|name=HyperDexAcid|group=nb|HyperDex currently offers ACID support via its Warp extension, which is a commercial add-on.}} || {{Yes}}\n|-\n| [[InfinityDB]] || {{Yes}} || {{No}}\n|-\n| [[Lightning Memory-Mapped Database|LMDB]] || {{Yes}} || {{No}}\n|-\n| [[MarkLogic]] || {{Yes}} || {{Yes}}{{refn|name=MarkLogicJoins|group=nb|Joins do not necessarily apply to document databases, but MarkLogic can do joins using semantics.<ref>http://www.gennet.com/big-data/cant-joins-marklogic-just-matter-semantics/</ref>}}\n|-\n| [[OrientDB]] || {{Yes}} || {{Yes}}\n\n|}\n\n{{reflist|group=nb}}\n\n== See also ==\n<!-- please do not list specific implementations here -->\n* [[CAP theorem]]\n* [[Comparison of object database management systems]]\n* [[Comparison of structured storage software]]\n* [[Correlation database]]\n* [[Distributed cache]]\n* [[Faceted search]]\n* [[MultiValue]] database\n* [[Multi-model database]]\n* [[Triplestore]]\n* [[Schema-agnostic databases]]\n\n== References ==\n{{Reflist|33em}}\n\n== Further reading ==\n*{{cite book\n | first1 = Pramod | last1 = Sadalage | first2 = Martin | last2 = Fowler | authorlink2 = Martin Fowler\n | date = 2012\n | title = NoSQL Distilled: A Brief Guide to the Emerging World of Polyglot Persistence\n | publisher = Addison-Wesley\n | isbn = 0-321-82662-0\n}}\n*{{cite book\n | first1 = Dan | last1 = McCreary | first2 = Ann | last2 =Kelly\n | date = 2013\n | title = Making Sense of NoSQL: A guide for managers and the rest of us\n | isbn = 9781617291074\n}}\n*{{cite book\n | first1 = Lena | last1 = Wiese \n | date = 2015\n | title =  Advanced Data Management for SQL, NoSQL, Cloud and Distributed Databases \n | publisher = DeGruyter/Oldenbourg\n | isbn = 978-3-11-044140-6\n}}\n* {{cite web| first = Christof | last = Strauch | date = 2012|title=NoSQL Databases|url=http://www.christof-strauch.de/nosqldbs.pdf}}\n* {{cite journal| last1 = Moniruzzaman | first1 = A. B. | last2 = Hossain | first2 = S. A. | date = 2013|title=NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison|arxiv=1307.0191}}\n* {{cite journal| first = Kai | last = Orend | date = 2013|title=Analysis and Classification of NoSQL Databases and Evaluation of their Ability to Replace an Object-relational Persistence Layer|citeseerx = 10.1.1.184.483 }}\n* {{cite web| first1 = Ganesh | last1 = Krishnan | first2 = Sarang | last2 = Kulkarni | first3 = Dharmesh Kirit | last3 = Dadbhawala | title=Method and system for versioned sharing, consolidating and reporting information|url=https://www.google.com/patents/US7383272?pg=PA1&dq=ganesh+krishnan&hl=en&sa=X}}\n\n== External links ==\n* {{cite web|url=http://www.christof-strauch.de/nosqldbs.pdf|title=NoSQL whitepaper| first = Christoph | last = Strauch|publisher=Hochschule der Medien|location = Stuttgart}}\n* {{cite web|url=http://nosql-database.org/|title=NoSQL database List| first = Stefan | last = Edlich}}\n* {{cite web|year=2010|url=http://www.infoq.com/articles/graph-nosql-neo4j|title=Graph Databases, NOSQL and Neo4j| first = Peter | last = Neubauer}}\n* {{cite web|year=2012|url=http://www.networkworld.com/article/2160905/tech-primers/a-vendor-independent-comparison-of-nosql-databases--cassandra--hbase--mongodb--riak.html|title=A vendor-independent comparison of NoSQL databases: Cassandra, HBase, MongoDB, Riak| first = Sergey | last = Bushik|publisher=NetworkWorld}}\n* {{cite web|year=2014|url=http://www.odbms.org/category/downloads/nosql-data-stores/nosql-data-stores-articles/|title=NoSQL Data Stores – Articles, Papers, Presentations|first = Roberto V. | last = Zicari|website=odbms.org}}\n{{Use dmy dates|date=February 2012}}\n{{Databases}}\n\n\n\n[[Category:NoSQL| ]]\n[[Category:Data management]]\n[[Category:Distributed data stores]]\n[[Category:Structured storage]]']
['Small Data: The Tiny Clues that Uncover Huge Trends', '49959657', "{{npov|date=May 2016}}\n\n{{Infobox book\n| name = Small Data: The Tiny Clues That Uncover Huge Trends\n| author = Martin Lindstrom\n| language = English\n| country = United States\n| publisher = St. Martins\n| isbn = 978-1250080684\n}}\nSmall Data: the Tiny Clues that Uncover Huge Trends is [[Martin Lindstrom]]'s seventh book. It chronicles his work as a branding expert, working with consumers across the world to better understand their behavior. The theory behind the book is that businesses can better create products and services based on observing consumer behavior in their homes, as opposed to relying solely on [[big data]].\n\n== Content ==\nThe book is based on a several year period of consumer studies for major corporations across the globe.<ref>{{Cite web|url=https://www.martinlindstrom.com/small-data/|title=Small Data - Martin Lindstrom - Bestselling Author|website=MartinLindstrom.com - Martin Lindstrom - Branding Expert & Consultant|language=en-US|access-date=2016-03-27}}</ref> It features case studies of the author's work interviewing consumers in their homes and using his observations to create hypotheses as to why they use products the way that they do.\n\n== Public Reception ==\n\nThe book was a New York Times Bestseller<ref>{{Cite web|url=http://www.nytimes.com/best-sellers-books/2016-03-13/advice-how-to-and-miscellaneous/list.html|title=Best Sellers - The New York Times|website=www.nytimes.com|access-date=2016-03-27}}</ref> upon release and was positively reviewed on several websites, Including [[Entrepreneur (magazine)|Entrepreneur]]<ref>{{Cite web|url=http://www.entrepreneur.com/article/271992|title=From an Elon Musk Bio to Malcolm Gladwell's 'Blink', These 9 Books Are Must-Reads|last=Agius|first=Aaron|website=Entrepreneur|access-date=2016-03-27}}</ref> and [[Forbes]]<ref>{{Cite web|url=http://www.forbes.com/sites/davidburkus/2016/01/10/16-must-read-business-books-for-2016/#4ce073648bae|title=16 Must-Read Business Books For 2016|website=Forbes|access-date=2016-03-27}}</ref>\n\n==References==\n{{Reflist}}\n\n\n\n[[Category:2016 books]]\n[[Category:2016 non-fiction books]]\n[[Category:Data management]]"]
['Research data archiving', '10022970', '\'\'\'Research data archiving\'\'\' is  the [[Computer_data_storage#Volatility|long-term storage]] of scholarly research [[data]], including  the natural sciences, social sciences, and life sciences. The various [[academic journals]] have differing policies regarding how much of their data and methods researchers are required to store in a public archive, and what is actually archived varies widely between different disciplines. Similarly, the major grant-giving institutions have varying attitudes towards public archival of data. In general, the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research. In recent years this approach has become increasingly strained as   research in some areas depends on large datasets which cannot easily be replicated independently.\n\nData archiving is more important in some fields than others.  In a few fields, all of the data necessary to replicate the work is already available in the journal article.  In drug development, a great deal of data is generated and must be archived so researchers can verify that the reports the drug companies publish accurately reflect the data.\n\nThe requirement of data archiving is a recent development in the [[history of science]].  It was made possible by advances in [[information technology]] allowing large amounts of data to be stored and accessed from central locations.  For example, the [[American Geophysical Union]] (AGU) adopted their first policy on data archiving in 1993, about three years after the beginning of the [[WWW]].<ref>”Policy on Referencing Data in and Archiving Data for AGU Publications” [http://www.agu.org/pubs/authors/policies/data_policy.shtml]</ref> This policy mandates that datasets cited in AGU papers must be archived by a recognised data center; it permits the creation of "data papers"; and it establishes AGU\'s role in maintaining data archives. But it makes no requirements on paper authors to archive their data.\n\nPrior to organized data archiving, researchers wanting to evaluate or replicate a paper would have to request data and methods information from the author.  The academic community expects authors to [[Data sharing (Science)|share supplemental data]].  This process was recognized as wasteful of time and energy and obtained mixed results.  Information could become lost or corrupted over the years.  In some cases, authors simply refuse to provide the information.\n\nThe need for data archiving and due diligence is greatly increased when the research deals with health issues or public policy formation.<ref>"The Case for Due Diligence When Empirical Research is Used in Policy Formation" by Bruce McCullough and Ross McKitrick. [http://economics.ca/2006/papers/0685.pdf]</ref><ref>[http://gking.harvard.edu/replication.shtml "Data Sharing and Replication" a website by Gary King]</ref>\n\n==Selected policies by journals==\n\n===\'\'The American Naturalist\'\'===\n{{quote|\'\'[[The American Naturalist]]\'\' requires authors to deposit the data associated with accepted papers in a public archive. For gene sequence data and phylogenetic trees, deposition in [[GenBank]] or [[TreeBASE]], respectively, is required. There are many possible archives that may suit a particular data set, including the [[Dryad (repository)|Dryad]] repository for ecological and evolutionary biology data. All accession numbers for GenBank, TreeBASE, and Dryad must be included in accepted manuscripts before they go to Production. If the data is deposited somewhere else, please provide a link. If the data is culled from published literature, please deposit the collated data in Dryad for the convenience of your readers. Any impediments to data sharing should be brought to the attention of the editors at the time of submission so that appropriate arrangements can be worked out.|JSTOR<ref>[http://www.jstor.org/page/journal/amernatu/forAuthor.html#data Supporting Data and Material]</ref>}}\n\n===\'\'Journal of Heredity\'\'===\n{{quote|The primary data underlying the conclusions of an article are critical to the verifiability and transparency of the scientific enterprise, and should be preserved in usable form for decades in the future. For this reason, \'\'Journal of Heredity\'\' requires that newly reported nucleotide or amino acid sequences, and structural coordinates, be submitted to appropriate public databases (e.g., GenBank; the [[EMBL Nucleotide Sequence Database]]; DNA Database of Japan; the [[Protein Data Bank]] ; and [[Swiss-Prot]]). Accession numbers must be included in the final version of the manuscript. For other forms of data (e.g., microsatellite genotypes, linkage maps, images), the Journal endorses the principles of the Joint Data Archiving Policy (JDAP) in encouraging all authors to archive primary datasets in an appropriate public archive, such as Dryad, TreeBASE, or the Knowledge Network for Biocomplexity. Authors are encouraged to make data publicly available at time of publication or, if the technology of the archive allows, opt to embargo access to the data for a period up to a year after publication.\n\nThe American Genetic Association also recognizes the vast investment of individual researchers in generating and curating large datasets. Consequently, we recommend that this investment be respected in secondary analyses or meta-analyses in a gracious collaborative spirit.|oxfordjournals.org<ref>[http://www.oxfordjournals.org/our_journals/jhered/for_authors/msprep_submission.html#4.%20DATA%20ARCHIVING%20POLICY Data archiving policy]</ref>}}\n\n===\'\'Molecular Ecology\'\'===\n{{quote|[[Molecular Ecology]] expects that data supporting the results in the paper should be archived in an appropriate public archive, such as GenBank, [[Gene Expression Omnibus]], TreeBASE, Dryad, the [[Knowledge Network for Biocomplexity]], your own institutional or funder repository, or as Supporting Information on the Molecular Ecology web site. Data are important products of the scientific enterprise, and they should be preserved and usable for decades in the future. Authors may elect to have the data publicly available at time of publication, or, if the technology of the archive allows, may opt to embargo access to the data for a period up to a year after publication. Exceptions may be granted at the discretion of the editor, especially for sensitive information such as human subject data or the location of endangered species.|Wiley<ref>[http://www.wiley.com/bw/submit.asp?ref=0962-1083&site=1 Policy on data archiving]</ref>}}\n\n===\'\'Nature\'\'===\n{{quote|Such material must be hosted on an accredited independent site (URL and accession numbers to be provided by the author), or sent to the \'\'Nature\'\' journal at submission, either uploaded via the journal\'s online submission service, or if the files are too large or in an unsuitable format for this purpose, on CD/DVD (five copies). Such material cannot solely be hosted on an author\'s personal or institutional web site.<ref>[http://www.nature.com/authors/editorial_policies/availability.html "Availability of Data and Materials: The Policy of Nature Magazine]</ref>\n\n\'\'Nature\'\' requires the reviewer to determine if all of the supplementary data and methods have been archived.  The policy advises reviewers to consider several questions, including: "Should the authors be asked to provide supplementary methods or data to accompany the paper online? (Such data might include source code for modelling studies, detailed experimental protocols or mathematical derivations.)|[[Nature (journal)|Nature]]<ref>{{cite web|title=Guide to Publication Policies of the Nature Journals|date=March 14, 2007|url=http://www.nature.com/authors/gta.pdf}}</ref>}}\n\n===\'\'Science\'\'===\n{{quote|\'\'Science\'\' supports the efforts of databases that aggregate published data for the use of the scientific community. Therefore, before publication, large data sets (including microarray data, protein or DNA sequences, and atomic coordinates or electron microscopy maps for macromolecular structures) must be deposited in an approved database and an accession number provided for inclusion in the published paper.<ref>[http://www.sciencemag.org/about/authors/prep/gen_info.dtl#datadep "General Policies of Science Magazine"]</ref>\n\n"Materials and methods" – \'\'Science\'\' now requests that, in general, authors place the bulk of their description of materials and methods online as supporting material, providing only as much methods description in the print manuscript as is necessary to follow the logic of the text. (Obviously, this restriction will not apply if the paper is fundamentally a study of a new method or technique.)|[[Science (journal)|Science]]<ref>[http://www.sciencemag.org/about/authors/prep/prep_online.dtl ”Preparing Your Supporting Online Material”]</ref>}}\n\n===Royal Society===\n{{quote|To allow others to verify and build on the work published in [[Royal Society]] journals, it is a condition of publication that authors make available the data, code and research materials supporting the results in the article.\nDatasets and code should be deposited in an appropriate, recognised, publicly available repository. Where no data-specific repository exists, authors should deposit their datasets in a general repository such as [[Dryad (repository)]] or [[Figshare]].\n|[[Royal Society]]<ref>[https://royalsociety.org/journals/ethics-policies/data-sharing-mining/ "Data sharing and mining"]</ref>}}\n\n==Policies by funding agencies==\nIn the United States, the [[National Science Foundation]] (NSF) has tightened requirements on data archiving.   Researchers seeking funding from NSF are now required to file a [[data management plan]] as a two-page supplement to the grant application.<ref>[http://news.sciencemag.org/scienceinsider/2010/05/nsf-to-ask-every-grant-applicant.html ”NSF to Ask Every Grant Applicant for Data Management Plan”]</ref>\n\nThe NSF [[Datanet]] initiative has resulted in funding of the \'\'\'Data Observation Network for Earth\'\'\' ([[DataONE]]) project, which will provide scientific data archiving for ecological and environmental data produced by scientists worldwide. DataONE\'s stated goal is to preserve and provide access to multi-scale, multi-discipline, and multi-national data. The community of users for DataONE includes scientists, ecosystem managers, policy makers, students, educators, and the public.\n\n==Data archives==\n\n===Natural sciences===\nThe following list refers to scientific data archives.\n* [[CISL Research Data Archive]]\n* [[Dryad (repository)|Dryad]]\n* [[ESO/ST-ECF Science Archive Facility]]\n* [http://www.ncdc.noaa.gov/paleo/treering.html International Tree-Ring Data Bank]\n* [http://www.icpsr.umich.edu Inter-university Consortium for Political and Social Research]\n* [http://knb.ecoinformatics.org Knowledge Network for Biocomplexity]\n* [[National Archive of Computerized Data on Aging]]\n* National Archive of Criminal Justice Data [http://www.icpsr.umich.edu/nacjd]\n* [[National Climatic Data Center]]\n* [[National Geophysical Data Center]]\n* [[National Snow and Ice Data Center]]\n* [[National Oceanographic Data Center]]\n* [http://daac.ornl.gov Oak Ridge National Laboratory Distributed Active Archive Center]\n* [[PANGAEA (data library)|Pangaea - Data Publisher for Earth & Environmental Science]]\n* [[World Data Center]]\n* [[DataONE]]\n\n===Social sciences===\n{{cleanup merge|Data archives|date=April 2016}}\n\n\'\'\'Data archives\'\'\' are professional institutions for the acquisition, preparation, preservation, and dissemination of social and behavioral data. The term is also sometimes used about natural science institutions (e.g., [[CISL Research Data Archive]], see [[Scientific data archiving]] and Borgman, 2007, p.&nbsp;18<ref>Borgman, Christine L. (2007).\'\'Scholarship in the digital age: information, infrastructure and the internet\'\'. Cambridge, MA: The MIT Press.</ref>), but here seems \'\'\'data centers\'\'\' to be the most used term. Data archives in the social sciences evolved in the 1950s and has been perceived as an international movement: \n\n<blockquote>By 1964 the International Social Science Council (ISSC) had sponsored a second conference on Social Science Data Archives and had a standing Committee on Social Science Data, both of which stimulated the data archives movement. By the beginning of the twenty-first century, most developed countries and some developing countries had organized formal and well-functioning national data archives. In addition, college and university campuses often have `data libraries\' that make data available to their faculty, staff, and students; most of these bear minimal archival responsibility, relying for that function on a national institution (Rockwell, 2001, p. 3227).<ref>Rockwell, R. C. (2001). Data Archives: International. IN: Smelser, N. J. & Baltes, P. B. (eds.) \'\'International Encyclopedia of the Social and Behavioral Sciences\'\' (vol. 5, pp. 3225- 3230). Amsterdam: Elsevier</ref></blockquote>\n\n* [[Registry of Research Data Repositories | re3data.org]] is a global registry of research data repository indexing data archives from all disciplines: http://www.re3data.org\n* CESSDA Members are data archives and other organisations that archive social science data and provide data for secondary use: http://www.cessda.net/about/members.html\n* Consortium of European Social Science Data Archives: http://www.cessda.org/\n* The Danish Data Archives: http://www.sa.dk/content/us/about_us ; specific page (only in Danish): http://www.sa.dk/dda/default.htm\n* Inter-university Consortium for Political and Social Research: http://www.icpsr.umich.edu/\n* The Roper Center for Public Opinion Research: http://www.ropercenter.uconn.edu\n* The Social Science Data Archive: http://dataarchives.ss.ucla.edu/\n* The NCAR Research Data Archive:  http://rda.ucar.edu\n\n===Life sciences===\n{{stub section|date=April 2016}}\n\n==See also==\n*[[Data archive]]\n\n==References==\n{{Reflist}}\n\n==Notes==\n* [[Registry of Research Data Repositories]] \'\'re3data.org\'\' [http://service.re3data.org/search/results?term=]\n* Statistical checklist required by \'\'Nature\'\' [http://www.nature.com/nature/authors/gta/Statistical_checklist.doc]\n* Policies of \'\'Proceedings of the National Academy of Sciences (U.S.)\'\' [http://www.pnas.org/misc/iforc.shtml#policies]\n* The US National Committee for CODATA [http://www7.nationalacademies.org/usnc-codata/Archiving.html]\n* The Role of Data and Program Code Archives in the Future of Economic Research  [http://research.stlouisfed.org/wp/2005/2005-014.pdf]\n* Data sharing and replication – Gary King website [http://gking.harvard.edu/replication.shtml]\n* The Case for Due Diligence When Empirical Research is Used in Policy Formation by McCullough and McKitrick [http://economics.ca/2006/papers/0685.pdf]\n* Thoughts on Refereed Journal Publication by Chuck Doswell [http://www.cimms.ou.edu/~doswell/pubreviews.html]\n* “How to encourage the right behaviour” An opinion piece published in \'\'Nature\'\',  March, 2002.[http://www.nature.com/nature/journal/v416/n6876/full/416001b.html]\n* [[NASA Astrophysics Data System]] [http://cdsads.u-strasbg.fr/]\n* [[Panton Principles]] for Open Data in Science, at Citizendium [http://en.citizendium.org/wiki/Panton_Principles]\n* [[Inter-university Consortium for Political and Social Research]] [http://www.icpsr.umich.edu]\n\n\n[[Category:Computer archives]]\n[[Category:Data management]]\n[[Category:Data publishing]]\n[[Category:Digital preservation]]\n[[Category:Information retrieval techniques]]\n[[Category:Knowledge representation]]\n[[Category:Structured storage]]']
['Physical data model', '1030540', '{{Refimprove|date=April 2008}}\n[[File:Physical Data Model Options.jpg|thumb|320px|Physical Data Model Options.<ref name="WH05">[http://georgewbush-whitehouse.archives.gov/omb/egov/documents/CRM.PDF FEA Consolidated Reference Model Document]. whitehouse.gov May 2005. p.91.  {{webarchive |url=https://web.archive.org/web/20100705040628/http://georgewbush-whitehouse.archives.gov/omb/egov/documents/CRM.PDF |date=July 5, 2010 }}</ref>]]\n\nA \'\'\'physical data model\'\'\' (or \'\'\'[[database design]]\'\'\') is a representation of a data design as implemented, or intended to be implemented, in a [[database management system]].  In the [[Project lifecycle | lifecycle of a project]] it typically derives from a [[logical data model]], though it may be [[reverse-engineer]]ed from a given [[database]] implementation.  A complete physical data model will include all the [[database artifact]]s required to create [[relationships between table]]s or to achieve performance goals, such as [[index (database)|index]]es, constraint definitions, linking tables, [[partitioned table]]s or [[cluster (computing)|cluster]]s.  Analysts can usually use a  physical data model to calculate storage estimates; it may include specific storage allocation details for a given database system.\n\n{{As of | 2012}} seven main databases dominate the commercial marketplace: [[Informix Dynamic Server|Informix]], [[Oracle Database|Oracle]], [[PostgreSQL|Postgres]], [[Microsoft SQL Server|SQL Server]], [[Sybase]], [[IBM DB2|DB2]] and [[MySQL]]. Other RDBMS systems tend either to be legacy databases or used within academia such as universities or further education colleges. Physical data models for each implementation would differ significantly, not least due to underlying [[operating system | operating-system]] requirements that may sit underneath them.  For example: SQL Server runs only on [[Microsoft Windows]] operating-systems, while Oracle and MySQL can run on Solaris, Linux and other UNIX-based operating-systems as well as on Windows. This means that the disk requirements, security requirements and many other aspects of a physical data model will be influenced by the RDBMS that a [[database administrator]] (or an organization) chooses to use.\n\n==Physical schema==\n\'\'Physical schema\'\' is a term used in [[data management]] to describe how [[data]] is to be represented and stored (files, indices, \'\'et al.\'\') in [[secondary storage]] using a particular [[database management system]] (DBMS) (e.g., Oracle RDBMS, Sybase SQL Server, etc.).\n\nIn the [[ANSI-SPARC Architecture|ANSI/SPARC Architecture]] [[three schema approach]], the \'\'internal schema\'\' is the view of data that involved data management technology.  This is as opposed to an \'\'external schema\'\' that reflects an individual\'s view of the data, or the \'\'[[conceptual schema]]\'\' that is the integration of a set of external schemas.\n\nSubsequently{{Citation needed|date=June 2012}} the internal schema was recognized to have two parts:\n\nThe [[logical schema]] was the way data were represented to conform to the constraints of a particular approach to database management.  At that time the choices were hierarchical and network.  Describing the logical schema, however, still did not describe how physically data would be stored on disk drives.  That is the domain of the \'\'physical schema\'\'.  Now logical schemas describe data in terms of relational \'\'tables and columns\'\', object-oriented \'\'classes\'\', and [[XML]] \'\'tags\'\'.\n\nA single set of tables, for example, can be implemented in numerous ways, up to and including an architecture where table rows are maintained on computers in different countries.\n\n==See also==\n*[[Database schema]]\n*[[Logical schema]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Physical Data Model}}\n[[Category:Data modeling]]\n[[Category:Data management]]\n[http://www.whitehouse.gov/sites/default/files/omb/assets/fea_docs/FEA_CRM_v23_Final_Oct_2007_Revised.pdf FEA Consolidated Reference Model Document] (whitehouse.gov) Oct 2007.\n\n[[ja:スキーマ (データベース)]]']
['Data exhaust', '51905821', "{{Multiple issues|\n{{Orphan|date=October 2016}}\n{{refimprove|date=October 2016}}\n}}\n\n'''Data exhaust''' refers to the trail of [[data]] left by the activities of an [[Internet]] user during his/her online activity. An enormous amount of often raw data are created. These data (which can take the form of [[Cookie (computing)|cookies]], temporary files, [[log file]]s etc.) can help to improve the online experience (for example through customized content). But they can also compromise privacy, as they offer a valuable insight into the user’s habits. It can be used to improve tracking trends and studying data exhaust also improves the user interface and the layout design. <ref name=techtarget>{{cite web|url=http://whatis.techtarget.com/definition/data-exhaust|title=What is data exhaust? - Definition from WhatIs.com|publisher=}}</ref>\n\nUnlike primary content, these data are not purposefully created by the user, who is often unaware of their very existence. A bank for example would consider as [[primary data]] information concerning the sums and parties of a transaction, whilst [[secondary data]] might include the percentage of transactions carried out at a [[cash machine]] instead of a real bank.<ref>{{cite web|url=http://www.pcworld.com/article/3069507/5-things-you-need-to-know-about-data-exhaust.html|title=5 things you need to know about data exhaust|publisher=}}</ref>\n\n==References==\n<references />\n\n\n[[Category:Data management]]\n\n{{internet-stub}}"]
['Data definition specification', '37621028', '{{technical|date=December 2012}}\nIn [[computing]], a \'\'\'data definition specification\'\'\' (DDS) is a guideline to ensure comprehensive and consistent data definition. It represents the attributes required to quantify data definition. A comprehensive data definition specification encompasses enterprise data, the hierarchy of [[data management]], prescribed guidance enforcement and criteria to determine compliance.\n\n==Overview==\nA data definition specification may be developed for any organization or specialized field, improving the quality of its products through consistency and transparency. It eliminates redundancy (since all contributing areas are referencing the same specification) and provides standardization, making it easier and more efficient to create, modify, verify, analyze and share information across the enterprise.<ref>Gouin, Deborah. & Corcoran, Charmane K. (2008).  Developing the MSU Enterprise Data Definition Standard.  Michigan State University Web site:  http://eis.msu.edu/uploads/---University%20EIS%20Working%20Committee%20Meetings/05%20August%202008/Enterprise%20Data%20Definition%20Standard%20Presentation082708.pdf</ref>\n   \nTo understand how a data definition specification works in an enterprise, we must look at the elements of a DDS. Writing data definitions, defining business terms (or rules) in the context of a particular environment, provides structure for an organization’s [[data architecture]]. In developing these definitions, the words used must be traceable to clearly defined data.\n\nA data definition specification may be used in the following activities to provide consistency and clarity between  departments supporting the activity:<ref name="datagovernance">Thomas, Gwen. (2008). Writing Enterprise-Quality Data Definitions: Tips for Creating Terms and Definitions. Data Governance Institute Web site: http://www.datagovernance.com/dgi_wp_writing_enterprise-quality_data_definitions.pdf</ref>\n*  [[Business intelligence]]\n*  [[Business process modeling]]\n*  Business rules management\n*  [[Data analysis]] and [[Data modeling|modeling]]\n*  [[Information architecture]]\n*  [[Metadata modeling]]\n*  Report generation\n\n== Criteria ==\nA data definition specification requires data definitions to be:\n* \'\'Atomic\'\' – singular, describing only one concept. Commonly used and ambiguous terms should be defined.<ref name="datagovernance" /> While a term refers to one concept, several words may be used in a term:\n:*File – A concept identifiable with one word\n:*File extension – A concept identifiable with more than one word \n* \'\'Traceable\'\' – Mapped to a specific data element. In business, a term may be traced to an entity (for example, a customer) or an attribute (such as a customer\'s name). A term may be a value in a [[data set]] (such as gender), or designate the data set itself. Traceability indicates relationships in the [[data hierarchy]].  \n* \'\'Consistent\'\' - Used in a standard [[Syntax (programming languages)|syntax]]; if used in a specific context, the context is noted\n* \'\'Accurate\'\' - Precise, correct and unambiguous, stating what the term is and is not<ref>International Organization for Standardization JTC1/SC32 Committee. (2004) ISO 11179-4. http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html.</ref>\n* \'\'Clear\'\' - Readily understood by the reader\n* \'\'Complete\'\' - With the term, its description and contextual references\n* \'\'Concise\'\' - To avoid circular references\n\n== Applications ==\n\n=== Enterprise data ===\nA data definition specification was produced by the [[Open Mobile Alliance]] to document charging data.<ref>{{cite web|url=http://www.openmobilealliance.org/Technical/release_program/docs/Charging_Data/V1_0-20110201-A/OMA-DDS-Charging_Data-V1_0-20110201-A.pdf|date=1 February 2011|website=Open Mobile Alliance|pages=6, 35|format=PDF|title=Charging Data|archiveurl=https://web.archive.org/web/20131006172727/http://technical.openmobilealliance.org/Technical/release_program/docs/Charging_Data/V1_0-20110201-A/OMA-DDS-Charging_Data-V1_0-20110201-A.pdf|archivedate=6 October 2013|accessdate=12 March 2014}}</ref> The document, the centralized catalog of data elements defined for interfaces, specifies the mapping of these data elements to protocol fields in the interfaces. Created for the exchange of financial data, Market Data Definition Language (MDDL) is an [[XML]] specification designed \n{{quote|to enable the interchange of information necessary to account, to analyze, and to trade financial instruments of the world\'s markets. It defines an XML-based interchange format and common data dictionary on the fields needed to describe: (1) financial instruments, (2) corporate events affecting value and tradability, and (3) market-related, economic and industrial indicators. The principal function of MDDL is to allow entities to exchange market data by standardizing formats and definitions. MDDL provides a common format for market data so that it can be efficiently passed from one processing system to another and provides a common understanding of market data content by standardizing terminology and by normalizing the relationships of various data elements to one another&nbsp;... From the user perspective, the goal of MDDL is to enable users to integrate data from multiple sources by standardizing both the input feeds used for data warehousing (i.e., define what\'s being provided by vendors) and the output methods by which client applications request the data (i.e., ensure compatibility on how to get data in and out of applications)."<ref>{{cite web|title=Market Data Definition Language (MDDL)|date=December 26, 2002|website=Cover Pages |url=http://xml.coverpages.org/mddl.html|archiveurl=https://web.archive.org/web/20131214075132/http://xml.coverpages.org/mddl.html|archivedate=December 14, 2013|accessdate=March 12, 2014}}</ref>}}\n\n=== Clinical submissions ===\nThe [[Clinical Data Interchange Standards Consortium]], a global, multidisciplinary, non-profit organization, has established standards to support the acquisition, exchange, submission and archiving of clinical research data and metadata. CDISC standards are vendor-neutral, platform-independent and freely available from the CDISC website. The Case Report Tabulation Data Definition Specification (define.xml) draft version 2.0, the oldest data definition specification, is part of the evolution from the 1999 FDA electronic submission (eSub) guidance and electronic Common Technical Document (eCTD) documents specifying that a document describing the content and structure of included data be included in a submission. Define.xml was developed to automate the review process by generating a machine-readable data-definition document. Define.xml has standardized submissions to the [[Food and Drug Administration]], reducing review times from over two years to several months.<ref>{{cite web|title=Define-XML|year=2012|website=Clinical Data Interchange Standards Consortium|url=http://www.cdisc.org/define-xml|archiveurl=https://web.archive.org/web/20131004232219/http://www.cdisc.org/define-xml|archivedate=October 4, 2013|accessdate=March 12, 2014}}</ref>\n\n=== Archival data ===\nA data definition specification is the foundation of [[metadata]] for [[scientific data archiving]]. The [[Metadata Encoding and Transmission Standard]] (METS) uses one principle of a DDS: consistent use of key terms to catalog digital objects for global use. The METS schema is a flexible mechanism for encoding descriptive, administrative and structural metadata for a [[digital library]] object and expressing complex links between metadata, and can provide a useful standard for the exchange of digital-library objects between repositories.<ref>Metadata Encoding & Transmission Standard (METS) Web site from The Library of Congress- Standards http://www.loc.gov/standards/mets/</ref>\n\nA similar effort is underway to preserve complex data associated with video-game archiving. Preserving Virtual Worlds attempted to address archival-format deficiencies, citing the lack of suitable documentation for interactive fiction and games at the [[bit]] level: specifically, the absence of "representation information" needed to map raw bits into higher-level data constructs.<ref>“Meta Data Schema Development” (2008) [http://pvw.illinois.edu/pvw/?page_id=25 Preserving Virtual Worlds website]</ref> Preserving Virtual Worlds 2 is a research project expanding on initial efforts in this field.<ref>Preserving Virtual Worlds 2, Researching best practices for videogame preservation. (2012). http://pvw.illinois.edu/pvw2/</ref>\n\n== See also ==\n* [[Clinical Data Interchange Standards Consortium]] (CDISC)\n* [[Data governance]]\n* [[ISO/IEC 11179]]\n* [[Metadata Encoding and Transmission Standard]] (METS)\n* [[OASIS (organization)|OASIS]]\n\n==References==\n{{reflist}}\n\n[[Category:Data management]]']
['Critical data studies', '51578025', '{{notability|date=December 2016}}\n{{Orphan|date=December 2016}}\n\n\'\'\'Critical data studies\'\'\' is the systematic study of data and its criticisms.<ref>Dalton, Craig, and Jim Thatcher, 2014.</ref> The field was named by scholars [[Craig Dalton]] and [[Jim Thatcher]] in their 2015 article titled "What does a critical data studies look like, and why do we care?" Interest has developed in this domain as a response to the emergence and reliance on \'[[big data]]\' in contemporary society.<ref>\'\'Ibid.\'\'</ref> Some of the other key scholars in this discipline include [[Rob Kitchen]] and [[Tracey P. Lauriault]].<ref>Kitchin, Rob, and Tracey P. Lauriault, 2014</ref><ref>Kitchin, Rob, 2014</ref> Scholars have attempted to make sense of data through different theoretical frameworks, some of which include analyzing data technically, ethically, politically/economically, temporally/spatially, and philosophically.<ref>Kitchin, Rob, 2014</ref> Some of the key academic journals related to critical data studies include the \'\'[[Journal of Big Data]]\'\' and \'\'[[Big Data and Society]]\'\'.\n\n==Why is a critical approach to data needed?==\n\nIn their article in which they coin the term \'critical data studies,\' Dalton and Thatcher also provide several justifications as to why data studies is a discipline worthy of a critical approach.<ref>Dalton, Craig, and Jim Thatcher, 2014.</ref> Firstly, \'big data\' is an important aspect of twenty-first century society, and the analysis of \'big data\' allows for a deeper understanding of what is happening and for what reasons.<ref>Dalton, Craig, and Jim Thatcher, 2014.</ref> Furthermore, big data as a technological tool and the information that it yields are not neutral, according to Dalton and Thatcher,<ref>\'\'Ibid.\'\'</ref> making it worthy of critical analysis in order to identify and address its biases. Building off this idea, another justification for a critical approach is that the relationship between big data and society is an important one, and therefore worthy of study.<ref>\'\'Ibid.\'\'</ref>  Dalton and Thatcher stress how the relationship is not an example of [[technological determinism]], but rather how big data can shape the lives of individuals. Big data technology can cause significant changes in society\'s structure and in the everyday lives of people,<ref>\'\'Ibid.\'\'</ref> and being a product of society, big data technology is worthy of sociological investigation.<ref>\'\'Ibid.\'\'</ref> Moreover, data sets are almost never completely raw, that is to say without any influences. Dalton and Thatcher describe how data are shaped by the vision or goals of a research team, and during the data collection process, certain things are quantified, stored, sorted and even discarded by the research team.<ref>\'\'Ibid.\'\'</ref> A critical approach is thus necessary in order to understand and reveal the intent behind the information being presented. Dalton and Thatcher also argue how data alone cannot speak for itself; in order to possess any concrete meaning, data must be accompanied by theoretical insight or be accompanied by alternative quantitative or qualitative research measures.<ref>\'\'Ibid.\'\'</ref> Dalton and Thatcher argue that if one were to only think of data in terms of its exploitative power, there is no possibility of using data for revolutionary, liberatory purposes.<ref>\'\'Ibid.\'\'</ref> Finally, Dalton and Thatcher propose that a critical approach in studying data allows for \'big data\' to be combined with older, \'small data,\' and thus create more thorough research, opening up more opportunities, questions and topics to be explored.<ref>\'\'Ibid.\'\'</ref>\n\n==Issues and Concerns for Critical Data Scholars==\n\nThe use of data in modern society brings about new ways of understanding and measuring the world, but also brings with it certain concerns or issues.<ref>Kitchin, Rob, 2014</ref> Data scholars attempt to bring some of these issues to light in their quest to be critical of data. Rob Kitchin identifies both technical and organizational issues of data, as well as some normative and ethical questions.<ref>\'\'Ibid.\'\'</ref> Technical and organization issues concerning data range from the scope of datasets, access to the data, the quality of the data, the integration of the data, the application of analytics and ecological fallacies, as well as the skills and organizational capabilities of the research team.<ref>\'\'Ibid.\'\'</ref> Some of the normative and ethical concerns addressed by Kitchin include surveillance through one\'s data (dataveillance), the privacy of one\'s data, the ownership of one\'s data, the security of one\'s data, anticipatory or corporate governance, and finally profiling individuals by their data.<ref>\'\'Ibid.\'\'</ref> All of these concerns must be taken into account by scholars of data in their objective to be critical.\n\n==References==\n{{reflist|24em}}\n\n==Sources==\n* Dalton, Craig, and Jim Thatcher. "What does a critical data studies look like, and why do we care? Seven points for a critical approach to ‘big data’." \'\'Society and Space open site\'\' (2014). Retrieved October 23, 2016.\n* Elkins, James R. "The Critical Thinking Movement: Alternating Currents in One Teacher\'s Thinking". \'\'myweb.wvnet.edu\'\'(1999). Retrieved 29 November 2016.\n* Kitchin, Rob. \'\'The data revolution: Big data, open data, data infrastructures and their consequences.\'\' Sage, 2014. Retrieved October 23, 2016.\n* Kitchin, Rob, and Tracey P. Lauriault. "Towards critical data studies: Charting and unpacking data assemblages and their work." (2014). Retrieved October 23, 2016.\n\n[[Category:Data management]]']
['Data recovery', '2160183', '{{Use dmy dates|date=June 2016}}\n{{Multiple issues|\n{{Refimprove|date=February 2012}}\n{{Manual|date=April 2016}}\n}}\n\nIn [[computing]], \'\'\'data recovery\'\'\' is a process of salvaging (retrieving) inaccessible, lost, corrupted, damaged or formatted data from [[secondary storage]], [[removable media]] or [[Computer file|files]], when the data stored in them cannot be accessed in a normal way. The data is most often salvaged from storage media such as internal or external [[hard disk drive]]s (HDDs), [[solid-state drive]]s (SSDs), [[USB flash drive]]s, [[Magnetic tape data storage|magnetic tapes]], [[CD]]s, [[DVD]]s, [[RAID]] subsystems, and other [[electronic devices]]. Recovery may be required due to physical damage to the storage devices or logical damage to the [[file system]] that prevents it from being [[Mount (computing)|mounted]] by the host [[operating system]] (OS).\n\nThe most common data recovery scenario involves an operating system failure, malfunction of a storage device, logical failure of storage devices, accidental damage or deletion, etc. (typically, on a single-drive, single-[[disk partition|partition]], single-OS system), in which case the ultimate goal is simply to copy all important files from the damaged media to another new drive. This can be easily accomplished using a [[Live CD]], many of which provide a means to [[Mount (computing)|mount]] the system drive and backup drives or removable media, and to move the files from the system drive to the backup media with a [[file manager]] or [[optical disc authoring software]]. Such cases can often be mitigated by [[disk partition]]ing and consistently storing valuable data files (or copies of them) on a different partition from the replaceable OS system files.\n\nAnother scenario involves a drive-level failure, such as a compromised [[file system]] or drive partition, or a [[hard disk drive failure]]. In any of these cases, the data is not easily read from the media devices. Depending on the situation, solutions involve repairing the logical file system, partition table or [[master boot record]],or updating the firmware or drive recovery techniques ranging from software-based recovery of corrupted data, hardware- and software-based recovery of damaged service areas (also known as the hard disk drive\'s "firmware"), to hardware replacement on a physically damaged drive which involves changes the parts of the damaged drive to make the data in a readable form and can be copied to a new drive. If a drive recovery is necessary, the drive itself has typically failed permanently, and the focus is rather on a one-time recovery, salvaging whatever data can be read.\n\nIn a third scenario, files have been accidentally "[[file deletion|deleted]]" from a storage medium by the users. Typically, the contents of deleted files are not removed immediately from the physical drive; instead, references to them in the directory structure are removed, and thereafter space they deleted data occupy is made available for later data overwriting. In the mind of [[end user]]s, deleted files cannot be discoverable through a standard file manager, but the deleted data still technically exists on the physical drive. In the meantime, the original file contents remain, often in a number of disconnected [[File system fragmentation|fragments]], and may be recoverable if not overwritten by other data files.\n\nThe term "data recovery" is also used in the context of [[Computer forensics|forensic]] applications or [[espionage]], where data which have been [[Encryption|encrypted]] or hidden, rather than damaged, are recovered. Sometimes data present in the computer gets encrypted or hidden due to reasons like virus attack which can only be recovered by some computer forensic experts. \n\n==Physical damage==\n{{See also|Data recovery hardware}}\n\nA wide variety of failures can cause physical damage to storage media, which may result from human errors and natural disasters. [[CD-ROM]]s can have their metallic substrate or dye layer scratched off; hard disks can suffer any of several mechanical failures, such as [[head crash]]es and failed motors; [[tape drive|tapes]] can simply break. Physical damage always causes at least some data loss, and in many cases the logical structures of the file system are damaged as well. Any logical damage must be dealt with before files can be salvaged from the failed media.\n\nMost physical damage cannot be repaired by end users. For example, opening a hard disk drive in a normal environment can allow airborne dust to settle on the platter and become caught between the platter and the [[read/write head]], causing new head crashes that further damage the platter and thus compromise the recovery process. Furthermore, end users generally do not have the hardware or technical expertise required to make these repairs. Consequently, data recovery companies are often employed to salvage important data with the more reputable ones using [[Cleanroom#Cleanroom classifications|class 100]] dust- and static-free [[cleanroom]]s.<ref>{{cite web|last=Vasconcelos|first=Pedro|title=DIY data recovery could mean "bye-bye"|url=http://blog.ontrackdatarecovery.co.uk/data-recovery-realities/diy-data-recovery-could-mean-bye-bye/|work=The Ontrack Data Recovery Blog|publisher=Kroll Ontrack UK|accessdate=23 May 2013}}</ref>\n\n===Recovery techniques===\nRecovering data from physically damaged hardware can involve multiple techniques. Some damage can be repaired by replacing parts in the hard disk. This alone may make the disk usable, but there may still be logical damage. A specialized disk-imaging procedure is used to recover every readable bit from the surface. Once this image is acquired and saved on a reliable medium, the image can be safely analyzed for logical damage and will possibly allow much of the original file system to be reconstructed.\n\n==== {{Anchor|SERVICE-AREA}}Hardware repair ====\n[[File:HD with toasty PCB.jpg|thumb|right|250px|Media that has suffered a catastrophic electronic failure requires data recovery in order to salvage its contents.]]\n\nA common misconception is that a damaged [[printed circuit board]] (PCB) may be simply replaced during recovery procedures by an identical PCB from a healthy drive. While this may work in rare circumstances on hard disk drives manufactured before 2003, it will not work on newer drives.  Electronics boards of modern drives usually contain drive-specific [[adaptation data]] required for accessing their system areas, so the related componentry needs to be either reprogrammed (if possible) or unsoldered and transferred between two electronics boards.<ref>{{cite web\n | url = http://www.donordrives.com/pcb-replacement-guide\n | title = Hard Drive Circuit Board Replacement Guide or How To Swap HDD PCB\n | accessdate = 27 May 2015\n | website = donordrives.com\n}}</ref><ref>{{cite web\n | url = http://www.pcb4you.com/pages/firmware-adaptation-service-rom-swap\n | archiveurl = https://web.archive.org/web/20130329021847/http://www.pcb4you.com/pages/firmware-adaptation-service-rom-swap\n | title = Firmware Adaptation Service - ROM Swap\n | accessdate = 27 May 2015 | archivedate = 29 March 2013\n | website = pcb4you.com\n}}</ref>\n\nEach hard disk drive has what is called a \'\'system area\'\' or \'\'service area\'\'; this portion of the drive, which is not directly accessible to the [[end user]], usually contains drive\'s [[firmware]] and adaptive data that helps the drive operate within normal parameters.<ref>{{cite web\n | url = http://www.recover.co.il/SA-cover/SA-cover.pdf\n | title = Hiding Data in Hard Drive\'s Service Areas\n | date = 14 February 2013 | accessdate = 23 January 2015\n | author = Ariel Berkman | website = recover.co.il\n | format = PDF\n}}</ref>  One function of the system area is to log defective sectors within the drive; essentially telling the drive where it can and cannot write data.\n\nThe sector lists are also stored on various chips attached to the PCB, and they are unique to each hard disk drive. If the data on the PCB do not match what is stored on the platter, then the drive will not calibrate properly.<ref>[https://web.archive.org/web/20130416232748/http://datarecoveryreport.com/#Swapping_PCB_Logic_Board#Swapping_PCB_Logic_Board Swapping Data Recovery Report]</ref> In most cases the drive heads will click because they are unable to find the data matching what is stored on the PCB.\n\n==Logical damage==\n{{See also|List of data recovery software}}\n[[Image:Data loss of image file.JPG|thumb|Result of a failed data recovery from a hard disk drive.]]\n\nThe term "logical damage" refers to situations in which the error is not a problem in the hardware and requires software-level solutions.\n\n===Corrupt partitions and file systems, media errors===\nIn some cases, data on a hard disk drive can be unreadable due to damage to the [[partition table]] or [[file system]], or to (intermittent) media errors. In the majority of these cases, at least a portion of the original data can be recovered by repairing the damaged partition table or file system using specialized data recovery software such as [[Testdisk]]; software like [[dd rescue]] can image media despite intermittent errors, and image raw data when there is partition table or file system damage. This type of data recovery can be performed by people without expertise in drive hardware, as it requires no special physical equipment or access to platters.\n\nSometimes data can be recovered using relatively simple methods and tools;<ref>[http://www.recover-computerdata.com/ Data Recovery Software]</ref> more serious cases can require expert intervention, particularly if parts of files are irrecoverable. [[File carving|Data carving]] is the recovery of parts of damaged files using knowledge of their structure.\n\n===Overwritten data===\n{{See also|Data erasure}}\n\nAfter data has been physically overwritten on a hard disk drive, it is generally assumed that the previous data are no longer possible to recover. In 1996, [[Peter Gutmann (computer scientist)|Peter Gutmann]], a computer scientist, presented a paper that suggested overwritten data could be recovered through the use of [[magnetic force microscope]].<ref>[http://www.cs.auckland.ac.nz/~pgut001/pubs/secure_del.html \'\'Secure Deletion of Data from Magnetic and Solid-State Memory\'\'], Peter Gutmann, Department of Computer Science, University of Auckland</ref> In 2001, he presented another paper on a similar topic.<ref>[http://www.cypherpunks.to/~peter/usenix01.pdf \'\'Data Remanence in Semiconductor Devices\'\'], Peter Gutmann, IBM T.J. Watson Research Center</ref>  To guard against this type of data recovery, Gutmann and Colin Plumb designed a method of irreversibly scrubbing data, known as the [[Gutmann method]] and used by several disk-scrubbing software packages.\n\nSubstantial criticism has followed, primarily dealing with the lack of any concrete examples of significant amounts of overwritten data being recovered.<ref>{{cite web | last = Feenberg | first = Daniel | title = Can Intelligence Agencies Read Overwritten Data? A response to Gutmann. | publisher = National Bureau of Economic Research | date = 14 May 2004 | url = http://www.nber.org/sys-admin/overwritten-data-guttman.html | accessdate = 21 May 2008}}</ref> Although Gutmann\'s theory may be correct, there is no practical evidence that overwritten data can be recovered, while research has shown to support that overwritten data cannot be recovered.{{specify|date=June 2013}}<ref>{{cite web|url=https://www.anti-forensics.com/disk-wiping-one-pass-is-enough/ |title=Disk Wiping –  One Pass is Enough |date=17 March 2009 |website=anti-forensics.com |deadurl=yes |archiveurl=https://web.archive.org/web/20120902011743/http://www.anti-forensics.com:80/disk-wiping-one-pass-is-enough |archivedate=2 September 2012 |df=dmy }}</ref><ref>{{cite web|url=https://www.anti-forensics.com/disk-wiping-one-pass-is-enough-part-2-this-time-with-screenshots/ |title=Disk Wiping –  One Pass is Enough –  Part 2 (this time with screenshots) |date=18 March 2009 |website=anti-forensics.com |deadurl=yes |archiveurl=https://web.archive.org/web/20121127130830/https://www.anti-forensics.com/disk-wiping-one-pass-is-enough-part-2-this-time-with-screenshots/ |archivedate=27 November 2012 |df=dmy }}</ref><ref>{{cite web\n | url = http://blogs.sans.org/computer-forensics/2009/01/15/overwriting-hard-drive-data/\n | title = Overwriting Hard Drive Data\n | date = 15 January 2009\n | first = Dr. Craig | last = Wright\n}}</ref>\n\n[[Solid-state drive]]s (SSD) overwrite data differently from [[hard disk drive]]s (HDD) which makes at least some of their data easier to recover. Most SSDs use [[flash memory]] to store data in pages and blocks, referenced by logical block addresses (LBA) which are managed by the flash translation layer (FTL). When the FTL modifies a sector it writes the new data to another location and updates the map so the new data appear at the target LBA. This leaves the pre-modification data in place, with possibly many generations, and recoverable by data recovery software.\n\n\n===Lost, Deleted & Formatted Data ===\n\nSometimes, data present in the physical drives (Internal/External Hard disk, Pen Drive, etc) gets lost, deleted and formatted due to circumstances like virus attack, accidental deletion or accidental use of SHIFT+DELETE. In these cases, data recovery software are used to recover/restore the data files. \n\n===Logical Bad Sector ===\n\nIn the list of logical failures of Hard disk, Logical bad sector is the most common in which data files can\'t be retrieved from particular sector of the media drives. To resolve this, software are incorporated to correct the logical sectors of media drive and is this is not enough, then there is need for replacement of hardware parts to make the logical bad sectors to be OK.\n\n==Remote data recovery==\nRecovery experts do not always need to have physical access to the damaged hardware.  When the lost data can be recovered by software techniques, they can often perform the recovery using remote access software over the Internet, LAN or other connection to the physical location of the damaged media.  The process is essentially no different from what the end user could perform by themselves.<ref>{{Cite web|url = http://datarecovery-overinternet.datarecoverydigest.com/|title = Data Recovery Over the Internet|date = 17 December 2012|accessdate = 29 April 2015|website = Data Recovery Digest|publisher = |last = Barton|first = Andre}}</ref>\n\nRemote recovery requires a stable connection with an adequate bandwidth. However, it is not applicable where access to the hardware is required, as in cases of physical damage.\n\n==Four phases of data recovery==\nUsually, there are four phases when it comes to successful data recovery, though that can vary depending on the type of data corruption and recovery required.<ref>{{cite web\n | url = http://www.dolphindatalab.com/the-four-phases-of-data-recovery/\n | title = [Infographic] Four Phases Of Data Recovery\n | date = 28 December 2012 | accessdate = 23 March 2015\n | author = Stanley Morgan | website = dolphindatalab.com\n}}</ref>\n\n; Phase 1: Repair the hard disk drive\n: Repair the hard disk drive so it is running in some form, or at least in a state suitable for reading the data from it. For example, if heads are bad they need to be changed; if the PCB is faulty then it needs to be fixed or replaced; if the spindle motor is bad the platters and heads should be moved to a new drive.\n\n; Phase 2: Image the drive to a new drive or a disk image file\n: When a hard disk drive fails, the importance of getting the data off the drive is the top priority. The longer a faulty drive is used, the more likely further data loss is to occur. Creating an image of the drive will ensure that there is a secondary copy of the data on another device, on which it is safe to perform testing and recovery procedures without harming the source.\n\n; Phase 3: Logical recovery of files, partition, MBR and filesystem structures\n: After the drive has been cloned to a new drive, it is suitable to attempt the retrieval of lost data. If the drive has failed logically, there are a number of reasons for that. Using the clone it may be possible to repair the partition table or [[master boot record]] (MBR) in order to read the file system\'s data structure and retrieve stored data.\n\n; Phase 4: Repair damaged files that were retrieved\n: Data damage can be caused when, for example, a file is written to a sector on the drive that has been damaged. This is the most common cause in a failing drive, meaning that data needs to be reconstructed to become readable. Corrupted documents can be recovered by several software methods or by manually reconstructing the document using a hex editor.\n\n== See also ==\n{{Portal|Computer security|Computing}}\n\n{{Div col||20em}}\n* [[Backup]]\n* [[Cleanroom]]\n* [[Comparison of file systems]]\n* [[Computer forensics]]\n* [[Continuous data protection]]\n* [[Data archaeology]]\n* [[Data loss]]\n* [[Error detection and correction]]\n* [[File carving]]\n* [[Hidden file and hidden directory]]\n* [[Undeletion]]\n{{Div col end}}\n\n== References ==\n{{Reflist|30em}}\n\n==Further reading==\n* Tanenbaum, A. & Woodhull, A. S. (1997). \'\'Operating Systems: Design And Implementation,\'\' 2nd ed. New York: Prentice Hall.\n* {{dmoz|Computers/Hardware/Storage/Data_Recovery/|Data recovery}}\n\n\n{{Data erasure}}\n\n{{DEFAULTSORT:Data recovery}}\n[[Category:Data recovery| ]]\n[[Category:Computer data]]\n[[Category:Data management]]\n[[Category:Transaction processing]]\n[[Category:Hard disk software|*]]\n[[Category:Backup|Recovery]]']
['Draft:Cloudiway', '52991127', "{{New unreviewed article\n| source = ArticleWizard\n| date = January 2017\n}}\n\n{{Infobox company\n | name = Cloudiway\n| logo = \n| type = [[Privately held company]]\n| founder = Emmanuel Dreux\n| area_served = Worldwide\n| industry = [[Information technology]]<br/>[[Computer software]]<br/>[[Cloud computing]]<br>[[Email management]]\n| products = Coexistence, mail migration, file migration, group migration, site migration, on-premises to cloud migration, mail archive migration, data migration\n| foundation = 2010\n| location_city = \n| location_country = \n| location = Annecy, France\n| locations = \n| homepage = [http://www.cloudiway.com/ www.cloudiway.com]\n}}\n\n'''Cloudiway''' is an international [[Software as a service|SaaS]] company founded in 2010 in Annecy, France. The company builds cloud-based tools such as enterprise coexistence (free/busy calendar sharing, mail routing and automatic address list synchronisation), data migration to the cloud, and identity access management tools.\n\nCloudiway are currently the only company to offer enterprise coexistence between G Suite, Office 365 and Microsoft Exchange.\n\n== Product history ==\nAfter releasing CloudAnywhere, a locally-installed identity access management tool, Cloudiway began working on mail migration tools to enable migration between remote systems such as [[Gmail|G Mail]], [[Office 365]], [[Internet Message Access Protocol|IMAP]] and [[Microsoft Exchange Server|Exchange]]. Soon after, file migration and site migration utilities were launched, followed by enterprise coexistence in 2016<ref>{{Cite web|url=http://inpublic.globenewswire.com/releaseDetails.faces?rId=1999484|title=GlobeNewswire: Cloudiway is launching its solution for Coexistence between Google apps and Office 365/Exchange|website=inpublic.globenewswire.com|access-date=2017-01-26}}</ref>. Mail archive migration and group migration utilities were launched shortly after, also in 2016. \n\nWith a stable set of utilities under its belt, Cloudiway introduced a wider variety of mail migration sources and destinations, including [[Lotus Notes Mail|Lotus Notes mail]], [[Zimbra]] and Amazon WorkMail. File migration, which had been launched with [[Google Drive]], [[OneDrive]] and [[SharePoint]], was expanded in early 2017 to include file systems (such as Windows servers), Azure [[Binary large object|blob]] storage and [[Amazon S3]] storage.\n\n== Company history ==\nCloudiway is headed by Emmanuel Dreux, a [[Microsoft Most Valuable Professional]] (MVP), who worked at IBM/Lotus and [[Microsoft]] before working on CloudAnywhere — Cloudiway's first product. The company moved to new headquarters in 2016<ref>{{Cite news|url=http://www.finanznachrichten.de/nachrichten-2016-06/37631705-cloudiway-cloudiway-continues-to-expand-its-global-expansion-and-announces-new-location-in-miami-florida-399.htm|title=CLOUDIWAY: Cloudiway continues to expand its global expansion and announces New location in Miami, Florida|newspaper=FinanzNachrichten.de|language=de|access-date=2017-01-26}}</ref> to accommodate growing staff numbers. \n\nCloudiway are a [https://partnercenter.microsoft.com/en-us/pcv/solution-providers/cloudiway_4298856859/861247_1 Microsoft Partner].  \n\nThe company offers consulting services in English and French, with offices in Florida, USA and Annecy, France. Through their consulting services and partnerships, the company has provided cloud migration solutions to a broad range of businesses, including education, the public sector, small private businesses and global enterprises. \n\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.cloudiway.com Official Cloudiway website]\n* [https://partnercenter.microsoft.com/en-us/pcv/solution-providers/cloudiway_4298856859/861247_1 Microsoft Partner Microsoft Partner page]\n* [http://www.distributique.com/actualites/lire-cloudiway-%C2%A0sur-le-cloud-le-marche-francais-n-etait-pas-pret%C2%A0-25414.html Article in French about Cloudiway]\n\n\n[[Category:Data management]]\n[[Category:Cloud computing]]\n[[Category:As a service]]\n[[Category:Software companies of France]]"]
['Category:Computer storage media', '891409', '{{Cat main|Computer storage media}}\nThis category refers to [[digital media]] used in [[computer storage]] devices.  Examples of such media include (a) magnetic disks, cards, tapes, and drums, (b) punched cards and paper tapes, (c) optical disks, (d) barcodes and (e) magnetic ink characters.\n\n\n{{Commons cat|Computer storage media}}\n\n[[Category:Electronic documents]]\n[[Category:Storage media]]\n[[Category:Digital media]]\n[[Category:Computer data storage| Media]]']
['Xplor International', '17762769', '{{about|the trade association|the software package|X-PLOR}}\n{{Refimprove|date=June 2008}}\n\'\'\'Xplor International\'\'\'<ref>\'\'\'Technology Trends: Xplor Directions Survey\'\'\',\nDr. Keith Davidson PhD, edp, \'\'Enterprise Journal, 1998\'\'. Example of Xplor industry research on [[transaction documents]] and [[laser printing]] trends http://esj.com/article.aspx?ID=10229843143PM</ref> also known as \'\'\'The Electronic Document Systems Association\'\'\' is an international [[trade association]] specifically focused on the issues of [[transaction document]]s. Transaction documents are legally relevant documents that are either printed and mailed or are electronically delivered e.g. [[Bill (payment)|bills]], [[bank statements]], [[insurance policies]] etc.\n\nThe acronym XPLOR was derived from \'\'Xerox Printer Liaison ORganization\'\', the original association name. Xplor expanded its mission in 1983 to include other vendors\' technology and adopted the acronym as the organization\'s name.\n\n== History ==\nXplor International was founded in 1980 as a trade association specifically for transaction document applications, due to the difference in emphasis on variable data. Originally a user group for the [[Xerox 9700]] laser printer, they reshaped its mission in the early 1980s to address the entire transaction document industry. Hardware companies like [[IBM]], [[Siemens]] (later [[Océ]]), [[Hewlett Packard]], [[Pitney Bowes]], [[Bell & Howell]], and [[Xerox]] have been actively involved as have software companies like Image Sciences (later Docucorp International), Document Sciences, [[Cincom Systems]], [[GMC Software AG|GMC Software Technology]], Xenos, Crawford Technologies, supported Xplor in order to promote a venue for the issues that are unique to the creation of transaction documents.\n\nIn the 1990s, Xplor began to shift from solely document “printing” to document “printing and presentation”, as transaction documents came to be presented on the Web.\n\n==Membership==\nXplor’s membership of users and vendors is worldwide, with approximately 45% of the membership in the early 2000s being outside the US.<ref>William J. \'Bill\' McCalpin edp, former General Manager of Xplor International, 2008</ref>\n\n== Xplor honours and awards ==\nXplor awards various technology providers with awards each year, including:\n\n*The Technology Application Award is presented to an individual, a company, or an organization to recognize outstanding achievement in the imaginative application of current technology and/or unique implementation of existing [[electronic document]] systems.\n*The Innovator of the Year Award honors an individual, company, or organization that has conceived and developed an original concept leading to a significant advancement in the industry. The "Innovator" has advanced a new program product or technology that notably enhances the capabilities of [[electronic document]] systems.\n*The Xplorer of the Year is Xplor International\'s most prestigious award; it honors significant service to the Association, dedication to the Xplor mission, and notable achievement promoting the interest of the [[electronic document]] systems industry.\n*The Brian Platte Lifetime Achievement Award, established in 2007, is given to an individual whose efforts and contributions have significantly changed the course and development of the digital document industry.\n\n=== Electronic Document Professional ===\n\nXplor manages the [[Electronic Document Professional]] (EDP) certification program for people experienced in [[electronic document]] systems and/or application development.\n\n==Associations in related fields==\n* [[Association for Information and Image Management]], the association for electronic content management\n* [[Association of Records Managers and Administrators]], the association for records management professionals\n\n==External links==\n* [http://www.xplor.org Xplor International] website\n\n== References ==\n\n{{Reflist}}\n\n[[Category:Electronic documents]]\n[[Category:International trade associations]]']
['Electronic document', '430436', "{{Unreferenced|date=July 2016}}\n[[File:Text-txt.svg|thumb|An example of a [[text file]] icon, one of the common representations of an electronic document.]]\nAn '''electronic document''' is any electronic media content (other than computer programs or system files) that are intended to be used in either an electronic form or as printed output.\nOriginally, any computer data were considered as something internal &mdash; the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. And the improvements in electronic display technologies mean that in most cases it is possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).\n\nHowever, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem &mdash; e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.\n\nEven more problems are connected with complex file formats of various [[word processor]]s, [[spreadsheet]]s and [[graphics software]]. To alleviate the problem, many software companies distribute free [[file viewer]]s for their proprietary file formats (one example is [[Adobe Systems|Adobe]]'s [[Portable Document Format|Acrobat Reader]]). The other solution is the development of standardized non-[[Proprietary software|proprietary]] file formats (such as [[HTML]] and [[OpenDocument]]), and electronic documents for specialized uses have specialized formats &ndash; the specialized [[electronic article]]s in physics  use [[TeX]] or [[PostScript]].\n{{reflist}} \n==See also==\n*[[Digital era governance]]\n*[[Electronic paper]]\n*[[Paperless office]]\n*[[Bureaucrat]]\n*[[E-Government Act of 2002]]\n*[[E-government]]\n*[[Public administration]]\n\n== External links ==\n* [http://people.ischool.berkeley.edu/~buckland/digdoc.html What is a digital document]\n* [http://www.msimaging.com/faq Digital Imaging Frequent Questions]\n\n[[Category:Electronic documents]]\n[[Category:Word processors]]"]
['E-bible', '25199152', "{{Orphan|date=December 2009}}\n{{Refimprove|date=December 2009}}\n\nSometimes known as document bibles or transaction deal bibles, '''e-bibles''' are a means of storing, indexing and comprehensively searching large volumes of [[document]]s related to any corporate transaction. \n\nThey are commonly used by [[Legal firm]]s to collate documents from a certain case in order to store or give to a client at the end of a project. e-bibles are a means of storing complex legal folders which were usually kept in hard copy.\n\nIn 2009, Proposals<ref>http://www.litig.org/index.php?option=com_content&task=category&sectionid=2&id=20&Itemid=33 </ref> were put in place in order to standardise the creation of e-bibles throughout the [[legal industry]].\n\nThere are few suppliers of COTS solutions, however Diskbuilder<ref>http://www.diskbuilder.co.uk</ref>  and Ideagen<ref>https://www.ideagenplc.com/</ref> (formerly Capgen) are notable exceptions.\n== References ==\n{{Reflist}}\n\n[[Category:Legal documents]]\n[[Category:Electronic documents]]\n[[Category:Document management systems]]"]
['Structured document', '23524003', '{{Refimprove|date=April 2014}}\nA \'\'\'structured document\'\'\' is an [[electronic document]] where some method of [[embedded coding]], such as [[markup language|mark-up]], is used to give the whole, and parts, of the document various structural meanings according to a [[Database schema|schema]]. A structured document whose mark-up doesn\'t break the schema and is designed to conform to and which obeys the [[syntax]] rules of its [[markup language|mark-up language]] is "well-formed".\n\n{{Quote|The [[Standard Generalized Markup Language]] (SGML) has pioneered the concept of structured documents|[http://www.w3.org/People/Janne/porject/paper.html Multi-purpose publishing using HTML, XML, and CSS], [http://www.w3.org/People/howcome/ Håkon Wium Lie] & [http://www.w3.org/People/Janne/ Janne Saarela]}}\n\nAs of 2009 the most widely used [[markup language]], in all its evolving forms, is [[HTML]], which is used to structure documents according to various [[Document Type Definition|Document Type Definition (DTD)]] schema defined and described by the [[W3C]], which continually reviews, refines and evolves the specifications.\n\n{{Quote|[[XML]] is the universal format for structured documents and data on the Web|[http://www.w3.org/MarkUp/#related XHTML2 Working Group], [[W3C]]}}\n\n==Structural semantics==\nIn writing structured documents the focus is on encoding the logical structure of a document, with no explicit concern in the structural markup for its presentation to humans by printed pages, screens or other means. Structured documents, especially well formed ones, can easily be processed by computer systems to extract and present [[metadata]] about the document. In most Wikipedia articles for example, a table of contents is automatically generated from the different heading tags in the body of the document. Popular [[word processor]]s can have such a function available.\n\nIn [[HTML]] a part of the logical structure of a document may be the document body; <code><nowiki><body></nowiki></code>, containing a first level heading; <code><nowiki><h1></nowiki></code>, and a paragraph; <code><nowiki><p></nowiki></code>.\n\n<code><nowiki><body></nowiki></code>\n:<code><nowiki><h1>Structured document</h1></nowiki></code>\n:<code><nowiki><p>A <strong class="selflink">structured document</strong> is an <a href="/wiki/Electronic_document" title="Electronic document">electronic document</a> where some method of <a href="/w/index.php?title=Embedded_coding&amp;action=edit&amp;redlink=1" class="new" title="Embedded coding (page does not exist)">embedded coding</a>, such as <a href="/wiki/Markup_language" title="Markup language">markup</a>, is used to give the whole, and parts, of the document various structural meanings according to a <a href="/wiki/Schema" title="Schema">schema</a>.</nowiki></code><code><nowiki></p></nowiki></code>\n<code><nowiki></body></nowiki></code>\n\nOne of the most attractive features of structured documents is that they can be reused in many contexts and presented in various ways on mobile phones, TV screens, speech synthesisers, and any other device which can be programmed to process them.\n\n=== Other semantics ===\nOther meaning can be ascribed to text which isn\'t structural. In the [[HTML]] fragment above, there is semantic markup which has nothing to do with structure; the first of these, the <code><nowiki><strong></nowiki></code> tag, means that the enclosed text should be given a strong emphasis. In visual terms this is equivalent to the bold, <code><nowiki><b></nowiki></code> tag, but in speech synthesisers this means a voice inflection giving strong emphasis is used. The term [[semantic markup]] excludes markup like the bold tag which has no meaning other than an instruction to a visual display. The strong tag means that the presentation of the enclosed text should have a strong emphasis in all presentation forms, not just visual.<br />\nThe anchor <code><nowiki><a></nowiki></code> tag is a more obvious example of semantic markup unconcerned with structure, with its href attribute set it means that the text it surrounds is a [[hyperlink]].\n\n[[HTML]] from early on has also had tags which gave presentational semantics, i.e. there were tags to give \'\'\'bold\'\'\' (<code><nowiki><b></nowiki></code>)or \'\'italic\'\' (<code><nowiki><i></nowiki></code>) text, or to alter <small>font sizes</small> or which had other effects on the presentation.<ref>{{cite web|url=http://www.w3.org/MarkUp/draft-ietf-iiir-html-01.txt|accessdate=5 March 2014}}</ref> Modern versions of markup languages discourage such markup in favour of [[Style sheet language|style sheets]]. Different style sheets can be attached to any markup, semantic or presentational, to produce different presentations. In [[HTML]], tags such as; <code><nowiki><a>, <blockquote>, <em>, <strong></nowiki></code> and others do not have a structural meaning, but do have a meaning.\n\n== See also ==\n* [[Document processor]]\n* [[Machine-Readable Documents]]\n\n {{reflist}}\n\n{{DEFAULTSORT:Structured Document}}\n[[Category:Electronic documents]]']
['Bibcode', '14092434', '{{selfref|For the Wikipedia template to link to bibcoded articles, see [[Template:bibcode]]}}\n{{Infobox identifier\n| name          = Bibcode\n| image         = \n| image_size    = \n| image_caption = \n| image_alt     = \n| image_border  = \n| full_name     = Bibliographic code\n| acronym       = \n| number        = \n| start_date    = 1990s<!-- {{Start date|YYYY|MM|DD|df=y}} -->\n| organisation  = \n| digits        = 19\n| check_digit   = none\n| example       = 1924MNRAS..84..308E\n| website       = <!-- {{URL|example.org}} -->\n}}\nThe \'\'\'bibcode\'\'\' (also known as the "refcode") is a compact identifier used by several [[astronomy|astronomical]] data systems to uniquely specify literature references.\n\n== Adoption ==\nThe Bibliographic Reference Code (REFCODE) was originally developed to be used in [[SIMBAD]] and the [[NASA/IPAC Extragalactic Database]] (NED), but it became a de facto standard and is now used more widely, for example, by the NASA [[Astrophysics Data System]] who coined and prefer the term "bibcode".<ref name=a>{{ cite book| url=http://cdsweb.u-strasbg.fr/simbad/refcode/refcode-paper.html| chapter= NED and SIMBAD Conventions for Bibliographic Reference Coding| title=Information & On-Line Data in Astronomy|editor= Daniel Egret|editor2= Miguel A. Albrecht|publisher= Kluwer Academic Publishers|date=1995|isbn =0-7923-3659-3|author= M. Schmitz|author2= G. Helou|author3= P. Dubois|author4= C. LaGue|author5= B.F. Madore|author6= H. G. Corwin Jr.|author7= S. Lesteven|last-author-amp= yes|accessdate= 2011-06-22| archiveurl= https://web.archive.org/web/20110607153038/http://cdsweb.u-strasbg.fr/simbad/refcode/refcode-paper.html| archivedate= 7 June 2011 <!--DASHBot-->| deadurl= no}}</ref><ref name=b>{{cite web|url=http://adsabs.harvard.edu/abs_doc/help_pages/data.html|title= The ADS Data, help page|publisher= NASA ADS |accessdate=November 5, 2007| archiveurl= https://web.archive.org/web/20071014195855/http://adsabs.harvard.edu/abs_doc/help_pages/data.html| archivedate= 14 October 2007 <!--DASHBot-->| deadurl= no}}</ref>\n\n== Format ==\nThe code has a fixed length of 19 characters and has the form\n<center><tt>YYYYJJJJJVVVVMPPPPA</tt></center>\nwhere <tt>YYYY</tt> is the four-digit year of the reference and <tt>JJJJJ</tt> is a code indicating where the reference was published. In the case of a journal reference, <tt>VVVV</tt> is the volume number, <tt>M</tt> indicates the section of the journal where the reference was published (e.g., <tt>L</tt> for a letters section), <tt>PPPP</tt> gives the starting page number, and <tt>A</tt> is the first letter of the last name of the first author. Periods (<tt>.</tt>) are used to fill unused fields and to pad fields out to their fixed length if too short; padding is done on the right for the publication code and on the left for the volume number and page number.<ref name=a /><ref name=b /> Page numbers greater than 9999 are continued in the <tt>M</tt> column. The 6-digit article ID numbers (in lieu of page numbers) used by the Physical Review publications since the late 1990s are treated as follows: The first two digits of the article ID, corresponding to the issue number, are converted to a lower-case letter (01 = a etc.) and inserted into column <tt>M</tt>. The remaining four digits are used in the page field.<ref name=b />\n\n== Examples ==\nSome examples of the code are:\n{| class="wikitable"\n|-\n! Bibcode\n! Reference\n|-\n| <tt>[http://adsabs.harvard.edu/abs/1974AJ.....79..819H 1974AJ.....79..819H]</tt>\n| {{cite journal\n   |last=Heintz |first=W. D.\n   |date=1974\n   |title=Astrometric study of four visual binaries\n   |journal=[[The Astronomical Journal]]\n   |volume=79 |pages=819–825\n   |doi=10.1086/111614\n   |bibcode = 1974AJ.....79..819H }}\n|-\n| <tt>[http://adsabs.harvard.edu/abs/1924MNRAS..84..308E 1924MNRAS..84..308E]</tt>\n| {{cite journal\n   |last=Eddington |first=A. S.\n   |date=1924\n   |title=On the relation between the masses and luminosities of the stars\n   |journal=[[Monthly Notices of the Royal Astronomical Society]]\n   |volume=84 |issue=5\n   |pages=308–332\n   |bibcode = 1924MNRAS..84..308E \n   | doi = 10.1093/mnras/84.5.308 }}\n|-\n| <tt>[http://adsabs.harvard.edu/abs/1970ApJ...161L..77K 1970ApJ...161L..77K]</tt>\n| {{cite journal\n   |last1=Kemp |first1=J. C.\n   |last2=Swedlund |first2=J. B.\n   |last3=Landstreet |first3=J. D.\n   |last4=Angel |first4=J. R. P.\n   |date=1970\n   |title=Discovery of circularly polarized light from a white dwarf\n   |journal=[[The Astrophysical Journal Letters]]\n   |volume=161 |pages=L77–L79\n   |doi=10.1086/180574\n |bibcode = 1970ApJ...161L..77K }}\n|-\n| <tt>[http://adsabs.harvard.edu/abs/2004PhRvL..93o0801M 2004PhRvL..93o0801M]</tt>\n| {{cite journal\n   |last1=Mukherjee |first1=M.\n   |last2=Kellerbauer |first2=A.\n   |last3=Beck |first3=D.\n   |last4=Blaum |first4=K.\n   |last5=Bollen |first5=G.\n   |last6=Carrel |first6=F.\n   |last7=Delahaye |first7=P.\n   |last8=Dilling |first8=J.\n   |last9=George |first9=S.\n   |last10=Guénaut |first10=C.\n   |last11=Herfurth |first11=F.\n   |last12=Herlert |first12=A.\n   |last13=Kluge |first13=H.-J.\n   |last14=Köster |first14=U.\n   |last15=Lunney |first15=D.\n   |last16=Schwarz |first16=S.\n   |last17=Schweikhard |first17=L.\n   |last18=Yazidjian |first18=C.\n   |display-authors=3\n   |date=2004\n   |title=The Mass of <sup>22</sup>Mg\n   |journal=[[Physical Review Letters]]\n   |volume=93 |issue=15\n   |pages=150801\n   |doi=10.1103/PhysRevLett.93.150801\n |bibcode = 2004PhRvL..93o0801M }}\n|}\n\n== See also ==\n* [[Digital object identifier]]\n\n== References ==\n{{Reflist|30em}}\n\n[[Category:Index (publishing)]]\n[[Category:Identifiers]]\n[[Category:Electronic documents]]\n[[Category:Computational astronomy]]']
['Category:Eprint archives', '43511230', 'An [[eprint]] archive contains electronic copies of [[Academic journal|research paper]]s (which can be closed or open).\n\n{{cat see also|Open-access archives|Bibliographic databases and indexes}}\n\n== External links ==\n[http://roar.eprints.org/ Registry of Open Access Repositories] (ROAR)\n\n{{DEFAULTSORT:Eprint archives}}\n[[Category:Academic publishing]]\n[[Category:Electronic documents]]\n[[Category:Electronic publishing]]\n[[Category:Lists of publications in science]]\n[[Category:Online archives]]\n[[Category:Digital libraries]]\n[[Category:Full text scholarly online databases]]']
['Teleadministration', '47533003', '\'\'\'Teleadministration\'\'\' is based on the concept that documents in electronic format have legal value. Administrative informatics is not new, but for many years it was merely Information Technology applied to legal documents, that is, the reproduction of paper-based legal documents into electronic file systems. Instead, Teleadministration turns this approach into its head. It is based on research conducted in 1978, the year when, at a conference promoted by the [[Court of Cassation (Italy)|Court of Cassation]], [[:it:Giovanni Duni|Giovanni Duni]] launched the then-futuristic idea that an electronic document could have legal value.<ref>1. Duni, G., L\'utilizzabilità delle tecniche elettroniche nell\'emanazione degli atti e nei procedimenti amministrativi. Spunto per una teoria dell\'atto amministrativo emanato nella forma elettronica, in "Rivista amm. della Repubblica italiana", 1978, pag.407 ss.</ref> 1978 was also the year in which the first research on digital signatures ([[RSA (cryptosystem)|RSA]])<ref>2. Rivest, Shamir e [[Leonard Adleman|Adleman]], A method for obtaining digital signature and public key cryptosystems, in Communications of the ACM, vol. 21, febbraio 1978, 120-126. This research referred to the asymmetric encryption technology (Diffie and Hellman, New directions in Cryptography, in IEEE Transaction on Information Theory, November 1976, 644 ss. Diffie and Hellman’s research was disseminated in Italy by  Gardner, “Un nuovo tipo di cifrario che richiederebbe milioni di anni per essere decifrato”, in Le Scienze, December 1977, 126 ss.), who added the regulation for issuing the keys and the public certification process associated to them.</ref> was published in the United States, yet it would take more than twenty-five years for jurists and mathematicians to start working together.<ref>3. The first application of the research by Rivest, Shamir and Adleman was the 1995 Utah Code, § from 46-3-101 to 46-3-504 (Enacted by l. 1995, ch. 61). The Utah code was analysed in the brilliant dissertation written by Francesca Flora, Evoluzione della informatica nel sistema di governo degli Stati Uniti d’America (Cagliari, dept. Of Politiacl Science, November 1996). For application at the federal level one had to wait until 1998: US Senate, S. 1594, Digital Signature and Electronic Authentication Law (SEAL) of 1998. — US House of Representatives, H.R. 3472, Digital Signature and Electronic Authentication Law (SEAL) of 1998.</ref>\n\nFor many years, and even before 1978, IT helped [[Public Administration]] but kept a “safe distance”, assuming that the ‘sacred nature’ of the Law demanded the use of pen and paper. Information Technology merely managed and filed copies of legal documents: it was known as “parallel IT”,<ref>4. Duni, G., Amministrazione digitale, Voce della Enciclopedia del diritto, Annali, I, Milano 2007, p. 13-49.</ref> since it was an accessory to the activity with formal value, the one based on pen and paper.\n\nThus, the logical, legal and material premise of Teleadministration is the conferment of legal value to IT documents.\n\n== Origins and terminology  ==\nIn Italy, the linguistic expression <ref>5. [http://www.treccani.it/enciclopedia/teleamministrazione_(Lessico-del-XXI-Secolo) Teleamministrazione, Lessico del XXI secolo], Treccani</ref> teleamministrazione was first used in 1991 at the Roman ‘La Sapienza’ university, during a conference organised by the Court of Cassation,<ref>6. Duni, G., Il progetto nazionale di teleamministrazione pubblica, in “L’informatica giuridica e il Ced della Corte di Cassazione”, proceedings of the conference held at the Univ. of Rome “La Sapienza”, 27-29 Nov. 1991, Milan 1992, p. 87 ss.</ref> in which it was said that: «the new system of administrative information technology is called “teleadministration” because all the work of the [[Public Administration]] will be carried out through devices, that could also be computers, linked to the central server through a network.» Teleadministration was indeed considered a type of teleworking.<ref>7. Applicazioni della multimedialità nella P.A.: teleamministrazione e telelavoro”, in Funzione Pubblica, special issue “I convegni di FORUM P.A. ’96”, volume I, p. 105.</ref>\n\nWith Teleadministration, amministrative procedures become electronic administrative procedures and, more specifically, those that are initiated by a party realize the electronic One Stop Shop.\n\n== The fundamentals of teleadministration ==\nIn the decades from 1970 to 1990, the [[Supreme Court of Cassation (Italy)|Court of Cassation]] was at the core of research on the relationship between IT and Law, organising international conferences every five years on the topic. The 1993 international conference featured the fundamentals of teleadministration, providing the details of the administrative systems behind the One Stop Shop concept:<ref>8. Duni, G., presentation at the 5th International Congress at the Court of Cassation on “IT and Legal Activity” Rome, 3–7 May 1993, I.P.Z.S. - Libreria dello Stato, 1994, II, p. 381 ss.</ref>\n# A citizen presents his/her claim to one administration, which then manages the entire procedure.\n# A single “administrative file” is created, no matter how many different administrations may be involved.\n# For both internal and external stages in an administrative procedure, a "warning signal" is sent telematically to the relevant office where the next stage is due, the employee in that office then becomes responsible for that phase of the procedure.\n# Any information concerning records already held by public administration is accessed telematically, without involving the citizen. \n# The (electronic) signature identifies the identity of the operator through sophisticated techniques.\n# The original of an administrative act is electronic and is therefore always available telematically to any administration that may need it. \n# The presence of an increasing amount of on-line data will necessitate greater use of automatic data processing in decision-making.\n# Saving data on multiple memory locations will guarantee the safekeeping of the acts. \n# Statistical data will be available in real time and under multiple profiles, with great benefits for top level decision-making.\n# Private citizens can obtain paper copies of the electronic acts.\nIt should be noted that the 5th fundamental mentions electronic rather than digital signatures.  This is because, in the jurists’ domain, digital signatures were not yet known. The generic reference to the electronic signature is however valid, and its general nature is actually suitable for the rules contained six years later in the Directive 1999/93/EC.\nAs we will see, the Directive refers in particular to all procedures initiated by private citizens, but the system remains valid for all procedures initiated by the administration offices as well.\n\n== Acknowledgement of the principles in current law ==\nThe legally accepted form of acts and documents evolved according to the following stages:\n# Acts only exist in paper form\n# Acts in electronic format are a possible option\n# Electronic format is compulsory, safe for a few exceptions\nIn Italy, Phase 2 was launched by Art 15, para, 2,  Law N. 59 of 15 March 1997, (the so-called ‘Bassanini 1’ law: it established the legal value of electronic documents, while regulations would establish the authentication criteria). The EC intervened later with its Directive 1999/93/EC of the [[European Parliament]] and the [[Council of the European Union|Council]] of 13 December 1999 (Transposed by Law Decree N.10 of 23 January 2002), which imposes an obligation on Member States to give legal value to documents with digital signatures (not directly named as such, but all their features are described in the directive). It also establishes that electronic documents should not be rejected a priori, hence opening to a range of different solutions to establish the authorship of a document (the so-called ‘weak signatures’).\n\nThe 1993 Directive was revoked and absorbed (for reasons of legal certainty and clarity) by Regulation 910/2014 of the European Parliament and Council of 23 July 2014 (also known as eIDAS regulation) in the OJEU 28 August 2014, which did not renege on the principle of also accepting the so-called ‘weak signatures’.\nIn Italy, The move to Phase 3 was established by Art.40 of Legislative sl. Decree N.82 of 7 March 2007, Code of Digital Administration (CAD), entitled “Creation of electronic documents”, which states: “Public Administrations make the original copy of their documents with electronic means, according to the provisions of the present Code and the technical specifications of Article 71”. Exceptions are extremely rare: Comma 3 states: by means of appropriate regulations…. , proposed by the delegated Ministers for Public Functions, Innovation and Technology and the Minister for Cultural Heritage and Activities, the categories of administrative documents that can be created on paper in original are identified, having regard to the special historical and archive value they will have by nature” (think, for example, of the resignation of a President of the Republic).\n\nUnfortunately, national administrations are ignoring this provision, and today it is only private companies that are no longer allowed paper-based communication with public administrations (Art. N. 5 bis of the CAD and D.P.C.M. 22 July 2011); compulsory electronic invoicing was added on 31 March 2015 by Law N. 44 of 24 December 2007, Art. N. 1, para 209-214 implemented by Ministerial  Decree N.55 of 3 April 2013, further clarified by Ministerial circular N.1 of 9 March 2015.\nThe modernisation of procedures was also touched by Presidential Decree N. 447 of 20 October 1998 (creation of the One Stop Shop, but only for production activities, and paper based), while interest for a telematic procedure only began with Legislative Decree N. 82 of 7 March 2005, CAD, which is not as relevant in its first version but was later modified by several interventions, particularly Legislative Decree N. 335 of 10 December 2013.\n\nEuropean sources are also essential. The EC, and later the European Union, have undertaken a wide range of actions on e-government: one of the most important was the launch of the IDABC programme (and financing) for Interoperable Delivery of European eGovernment Services to public Administrations, Business and Citizens, via Decision 2004/387/EC of the European Parliament and Council of 21 April 2004. However, the ultimate acknowledgement of the principles of teleadministration, with the telematic One Stop Shop, is contained in the Directive 2006/123/EC of the European Parliament and Council of 12 December 2006, on the Internal market for services, which provides for Member States to set up an electronic One Stop Shop in the wide field of administrative procedures.\n\n== Teleadministration and the ‘star’ procedure ==\nThis article is not meant to argue the great effect that teleadministration has on the efficiency of administrative activity, as we assume that the reader is fully aware that, once paper based documents are abandoned, the real-time flow of documents greatly improves time management and responsibility of the single offices/ operators, while direct online access improves transparency. Rather, this paragraph wants to emphasize how teleadministration promotes maximum usage of the “star procedure”, known and researched in Germany as “Sternverfahren”. This procedure, an alternative to the sequential procedure, which has by nature longer head times, in the paper-based world would require making several copies of the administrative file (which can be extremely voluminous) for each office and each administration that needs to express an opinion or issue an authorisation. With the One Stop Shop, the administration initiating the process is charged with this task. Electronic files clearly provide evident benefits for these procedures, since all involved administrations can directly and simultaneously access the file, view the part they need to evaluate and add their opinion or authorisation directly, using a star-shaped scheme.\n\n== Assessing the actual acceptance of teleadministration in current law and real life ==\nAs a scientific proposition, teleadministration sketched the system of telematic administrative procedures well ahead of the law, and particularly the electronic One Stop Shop concept. Both concepts are based on the dematerialisation of documents and on telematic administrative work.\n\nThe concept of documents’ dematerialisation,<ref>9. According to some commentators “dematerialization” is not the appropriate term for documents that are created in electronic form, but rather for those that are created in paper form and are later converted in digital format. Though conceptually correct, this observation ignores the fact that the expression now is generally understood to mean any “form that does without the material presence of paper, from the creation of the document”</ref> in existence since 1978 as a scientific notion,<ref>10. See note. 1.</ref> was first embraced in Italy (Law N. 59 of 15 March 1997, Art. 15, para 2) and later by the E.C. in Directive 1999/93/EC.\n\nOnce the principle that an electronic document can have legal value was accepted, it was possible to deal with its management within a telematic procedure.\nAs mentioned, configuring this procedure within the rules of teleadministration is today accepted in both European and Italian laws. European laws also provide quite a detailed description of the electronic One Stop Shop, with rules that fit nicely within the scientific rules of teleadministration; their main limitation is that they were specifically designed for the free circulation of services within Europe, and hence for the procedures these require. It is the above-mentioned 1996/123/EC Directive, whose Art.6 establishes the One Stop Shop and Art. 8 provides that it should be managed “remotely and electronically”, leaving further details to the Commission. And indeed the Commission, with its Decision of 16 October 2009, provided a number of measures to facilitate the use of electronic procedures through the “One Stop Shop” under Directive 2006/123/EC. These sources are clear and they apply to a wide-ranging sector: the problem is that any sector or procedure that is not related to the supply of services within the Union is not regulated, and Member States are therefore able to carry on with old-fashioned paper-based procedures.\n\nIn light of this limitation, a group of illustrious European Law academics, coordinated by Giovanni Duni, has drafted the most effective text for a Directive providing a universal system of telematic administrative procedure.<ref>11. The outcome of this collective research effort is the text for a draft Directive, that can be found on the CNR ITTIG journal  Informatica e diritto, Vol. XXI, 2012, N. 2, pp. 113-129: The telematic procedures in the European Union. Introducing a draft Directive, as well as  on line, with an Italian and an English language version of the draft on the site [http://www.teleamministrazione.it www.teleamministrazione.it]</ref>\n\nItalian sources are based on the Code of Digital Administration, the above-mentioned Law Decree N.82 of 7 March 2005 in its current version, following several modifications, which (if correctly interpreted and implemented) should make it compulsory for all public administrations to use teleadministration, thus making the telematic administrative procedure the default procedural method. Art. 14, the key provision, establishes that the proceeding administration creates an electronic file, to which all involved administrations can and should have access, and feed it with the acts of their competence. Private citizens can also access it under Law 241/90\nThus the electronic file is the technical and organizational specification of the telematic administrative procedure, as it is clear that its creation is an operative stage of the procedure and not simply a new filing system for the archives.\n\nArt. 10 of the CAD appears at first to be at odds with this interpretation, since it establishes that the One Stop Shop for productive activities provides its services electronically but leaves doubts about the possibility that the back office activities could be still paper-based. However, if Art. 10 and Art. 41 are interpreted together, the only possible conclusion is that the former is a clarification of front office activities, but all the administrative activity is based on the general rule of electronic files, and therefore on the teleadministration and the One Stop Shop. \nCompared to European and Italian law, reality is somewhat behind. Eight years after Directive 1996/123/CE, there would be grounds for an infraction procedure against Italy. But since Italy enjoys the company of several other non-compliant Member States, they are all ‘safe’ for the time being.\n\nThough the letter of the CAD may be not be respected, it seems very unlikely that this may determine the invalidity or nullity of the acts for violation of Art. 41(electronic file) or Art. 40 (statutory requirement of digital signature), because in front of a claim of this nature, the administrative judge would apply Artt. 21 septies and 21 octies of Law 241/90. The claimant should demonstrate that the use of the electronic format and electronic file wold have led to a different outcome.\n\n== Legal sources ==\n\n=== US Law ===\n1995 Utah Code, paras 46-3-101 to 46-3-504 (Enacted by Law 1995, Ch. 61). US Senate, S. 1594, Digital Signature and Electronic Authentication Law (SEAL) of 1998. — US House of Representatives, H.R. 3472, Digital Signature and Electronic Authentication Law (SEAL) of 1998\n\n=== EU Law ===\n*European Parliament and Council Directive 1999/93/EC of 13 December 1999, revoked and absorbed by European Parliament and Council Regulation 910/2014 of 23 July 2014, (known as the eIDAS regulation) in OJEU 28 August 2014. \n*European Parliament and Council Decision 2004/387/EC of 21 April 2004 on the interoperable delivery of European eGovernment services\n*European Parliament and Council Directive 2006/123/EC of 12 December 2006, on internal market services, providing for Member States to establish the electronic One Stop Shop in this vast field of administrative procedures. \n*Decision of 16 October 2009, establishing measures to facilitate electronic procedures through the «One Stop Shop» under Directive 2006/123/CE \nThe telematic procedures in the European Union. Introducing a draft Directive, research coordinated by Duni, G., in CNR ITTIG Informatica e diritto, Vol. XXI, 2012, n. 2, pp.&nbsp;113–129 and in www.teleamministrazione.it.\n\n=== Italian Law ===\n*l. 15 March 1997, N. 59, art. 15, para, 2. \n*D. lg. 23 January 2002, N. 10. \n*D.P.R. 20 October 1998, N. 447 \n*D. legils. 7 March 2005, N. 82, codice dell\'amministrazione digitale (CAD) \n*D.P.C.M. 22 July 2011 \n*D. Legisl 10 December 2013, N. 335\n\n== Bibliography ==\n*Contaldo, A., La teleamministrazione con reti transnazionali europee come strumento per l\'integrazione delle Pubbliche Amministrazioni dei paesi dell\'Unione Europea, in Riv. trim. diritto amministrativo, I, 2004, p.&nbsp;95 and later \n*Diffie and Hellman, New directions in Cryptography, in IEEE Transaction on Information Theory, November 1976, 644 ss \n*Duni, G., L\'utilizzabilità delle tecniche elettroniche nell\'emanazione degli atti e nei procedimenti amministrativi. Spunto per una teoria dell\'atto amministrativo emanato nella forma elettronica, in "Rivista amm. della Repubblica italiana", 1978, pag.407 ss. — Il progetto nazionale di teleamministrazione pubblica, in “L’informatica giuridica e il Ced della Corte di Cassazione”, proceedings of the conference held at Univ. of Rome “La Sapienza”, 27-29 Nov. 1991, Milan 1992, p.&nbsp;87 ss. — La teleamministrazione: una “scommessa” per il futuro del Paese, presentation at the 5th International Congress at the Court of Cassation on “IT and Legal Activity” Rome, 3–7 May 1993, I.P.Z.S. - Libreria dello Stato, 1994, II, p.&nbsp;381 ss. — Amministrazione digitale, Item under the Enciclopedia del diritto, Annali, I, Milan 2007, p.&nbsp;13-49 — L’amministrazione digitale. Il diritto amministrativo nell’evoluzione telematica, Giuffrè 2008. \n*Flora, F., Evoluzione della informatica nel sistema di governo degli Stati Uniti d’America. Dissertation, Cagliari, Dept. Of Political Science, November 1996. \n*Gagliotti, A., Teleamministrazione e concorsi pubblici, in Giustizia amministrativa n. 3/2003, http://www.giustamm.it/ago1/articoli/gaglioti_teleamministrazione.htm#_ednref5 \n*Gardner, Un nuovo tipo di cifrario che richiederebbe milioni di anni per essere decifrato, in Le Scienze, December 1977, 126 ss. \n*Masucci, Informatica pubblica, in Dizionario di diritto pubblico directed by S. Cassese, IV, Milan, 2006, 3115 ss.; \n*Notarmuzi, Il codice dell’amministrazione digitale, in Astrid Rassegna, www.astrid-online.it, 2006, n. 12; Id., Il procedimento amministrativo informatico, ivi, n. 16; \n*Osnaghi, Firme elettroniche e documento informatico: il codice richiede ulteriori integrazioni, ivi, n. 10; \n*Rabbito. c., L\'informatica al servizio della pubblica amministrazione. Dai principi della teleamministrazione ai piani di e-government, Gedit, 2007. \n*Rivest, Shamir e Adleman, A method for obtaining digital signature and public key cryptosystems, in Communications of the ACM, vol. 21, February 1978, 120-126 \n*The telematic procedures in the European Union. Introducing a draft Directive, ricerca coordinata da Duni, g., in CNR ITTIG Informatica e diritto, Vol. XXI, 2012, N. 2, pp.&nbsp;113–129 and in www.teleamministrazione.it. \n*Applicazioni della multimedialità nella P.A.: teleamministrazione e telelavoro”, in Funzione Pubblica, special issue “I convegni di FORUM P.A. ’96”, volume I, p.&nbsp;105.\n\n==See also==\n*[[Digital era governance]]\n*[[Electronic document]]\n*[[Electronic paper]]\n*[[Paperless office]]\n*[[Bureaucrat]]\n*[[E-Government Act of 2002]]\n*[[E-government]]\n*[[Public administration]]\n\n==References==\n<references/>\n\n[[Category:E-government]]\n[[Category:Administrative law]]\n[[Category:Public-key cryptography]]\n[[Category:Electronic documents]]\n[[Category:Public administration]]']
['Email', '9738', '{{about|the communications medium|the former manufacturing conglomerate|Email Limited}}\n{{redirect|Inbox|the Google product|Inbox by Gmail}}\n<!--Before adding {{lowercase title}} again, please see talk page.-->\n[[File:2016-03-22-trojita-home.png|thumb|right|400px|This screenshot shows the "Inbox" page of an email system, where users can see new emails and take actions, such as reading, deleting, saving, or responding to these messages]]\n[[File:(at).svg|thumb|100px|The [[at sign]], a part of every SMTP [[email address]]<ref>{{cite web|url=https://tools.ietf.org/html/rfc5321#section-2.3.11|title=RFC 5321 – Simple Mail Transfer Protocol|accessdate=19 January 2015|work=Network Working Group }}</ref>]]\n\'\'\'Electronic mail\'\'\', or \'\'\'email\'\'\', is a method of exchanging digital messages between people using digital devices such as computers, tablets and mobile phones. Email first entered substantial use in the 1960s and by the mid-1970s had taken the form now recognized as email. Email operates across [[computer network]]s, which in the 2010s is primarily the [[Internet]]. Some early email systems required the author and the recipient to both be [[Online and offline|online]] at the same time, in common with [[instant messaging]]. Today\'s email systems are based on a [[store-and-forward]] model. Email [[Server (computing)|servers]] accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need to connect only briefly, typically to a [[Message transfer agent|mail server]] or a [[webmail]] interface, for as long as it takes to send or receive messages.\n\nOriginally an [[ASCII]] text-only communications medium, Internet email was extended by [[Multipurpose Internet Mail Extensions]] (MIME) to carry text in other character sets and multimedia content attachments. [[International email]], with internationalized email addresses using [[UTF-8]], has been standardized, but as of 2016 it has not been widely adopted.{{citation needed|date=March 2016}}\n\nThe history of modern Internet email services reaches back to the early [[ARPANET]], with standards for encoding email messages published as early as 1973 (RFC 561). An email message sent in the early 1970s looks very similar to a basic email sent today. Email played an important part in creating the Internet,<ref>{{Harv|Partridge|2008}}</ref> and the conversion from ARPANET to the Internet in the early 1980s produced the core of the current services.\n\n==Terminology==\nHistorically, the term \'\'electronic mail\'\' was used generically for any electronic document transmission. For example, several writers in the early 1970s used the term to describe [[fax]] document transmission.<ref>Ron Brown, Fax invades the mail market, [https://books.google.com/books?id=Ry64sjvOmLkC&pg=PA218 New Scientist], Vol. 56, No. 817 (Oct., 26, 1972), pages 218–221.</ref><ref>Herbert P. Luckett, What\'s News: Electronic-mail delivery gets started, [https://books.google.com/books?id=cKSqa8u3EIoC&pg=PA85 Popular Science], Vol. 202, No. 3 (March 1973); page 85</ref> As a result, it is difficult to find the first citation for the use of the term with the more specific meaning it has today.\n\nElectronic mail has been most commonly called \'\'email\'\' or \'\'e-mail\'\' since around 1993,<ref>{{cite book|url=https://books.google.com/ngrams/graph?content=electronic+mail%2Ce-mail&year_start=1980&year_end=1995&corpus=15&smoothing=0&share= |title=Google Ngram Viewer |publisher=Books.google.com |accessdate=2013-04-21}}</ref> but variations of the [[spelling]] have been used:\n\n* \'\'email\'\' is the most common form used online, and is required by [[IETF]] [[Request for Comments|Requests for Comments]] (RFC) and working groups<ref>{{cite web|url=https://www.rfc-editor.org/rfc-style-guide/terms-online.txt|publisher=IETF|title=RFC Editor Terms List}} This is suggested by the [https://www.rfc-editor.org/rfc-style-guide/rfc-style-manual-08.txt RFC Document Style Guide]</ref> and increasingly by [[style guide]]s.<ref>{{cite web|url=http://styleguide.yahoo.com/word-list/e |title=Yahoo style guide |publisher=Styleguide.yahoo.com |accessdate=2014-01-09}}</ref><ref name="aces2011">[http://www.huffingtonpost.com/2011/03/18/ap-removes-hyphen-from-em_n_837833.html "AP Removes Hyphen From ‘Email’ In Style Guide"], 18 March 2011, huffingtonpost.com</ref> This spelling also appears in most dictionaries.<ref name="AskOxford Language Query team">{{cite web | url=http://www.askoxford.com/asktheexperts/faq/aboutspelling/email | title=What is the correct way to spell \'e\' words such as \'email\', \'ecommerce\', \'egovernment\'? | publisher=[[Oxford University Press]] | work=FAQ | accessdate=4 September 2009 | author=AskOxford Language Query team | archiveurl=https://web.archive.org/web/20080701194047/http://www.askoxford.com/asktheexperts/faq/aboutspelling/email?view=uk | quote=We recommend email, as this is now by far the most common form | archivedate=July 1, 2008}}</ref><ref name="Reference.com">{{cite web|url=http://dictionary.reference.com/browse/email |title=Reference.com |publisher=Dictionary.reference.com |accessdate=2014-01-09}}</ref><ref name="ReferenceA">Random House Unabridged Dictionary, 2006</ref><ref name="ReferenceB">The American Heritage Dictionary of the English Language, Fourth Edition</ref><ref name="Princeton University WordNet 3.0">Princeton University WordNet 3.0</ref><ref name="ReferenceC">The American Heritage Science Dictionary, 2002</ref><ref name="Merriam-Webster Dictionary">{{cite web|title=Merriam-Webster Dictionary|url=http://www.merriam-webster.com/dictionary/email|publisher=Merriam-Webster|accessdate=9 May 2014}}</ref>\n* \'\'e-mail\'\' is the format that sometimes appears in edited, published American English and British English writing as reflected in the [[Corpus of Contemporary American English]] data,<ref>{{cite web|url=http://english.stackexchange.com/questions/1925/email-or-e-mail|title= "Email" or "e-mail"|work=English Language & Usage – Stack Exchange|date=August 25, 2010|accessdate=September 26, 2010}}</ref> but is falling out of favor in style guides.<ref name="aces2011" /><ref name="ap">{{cite web|title=AP changes e-mail to email|url=http://www.aces2011.org/sessions/18/the-ap-stylebook-editors-visit-aces-2011/|work=15th National Conference of the American Copy Editors Society (2011, Phoenix)|publisher=ACES|accessdate=23 March 2011|author=Gerri Berendzen|authorlink=AP Stylebook editors share big changes|author2=Daniel Hunt}}</ref>\n* \'\'mail\'\' was the form used in the original protocol standard, \'\'RFC&nbsp;524\'\'.<ref name=rfc524>{{cite web|url=http://www.faqs.org/rfcs/rfc524.html |title=RFC 524 (rfc524) – A Proposed Mail Protocol |publisher=Faqs.org |date=1973-06-13 |accessdate=2016-11-18}}</ref> The service is referred to as \'\'mail\'\', and a single piece of electronic mail is called a \'\'message\'\'.<ref name="11above">{{cite web|url=http://www.faqs.org/rfcs/rfc1939.html |title=RFC 1939 (rfc1939) – Post Office Protocol – Version 3 |publisher=Faqs.org |accessdate=2014-01-09}}</ref><ref name="12above">{{cite web|url=http://www.faqs.org/rfcs/rfc3501.html |title=RFC 3501 (rfc3501) – Internet Message Access Protocol – version 4rev1 |publisher=Faqs.org |accessdate=2014-01-09}}</ref>\n* \'\'EMail\'\' is a traditional form that has been used in RFCs for the "Author\'s Address"<ref name="11above"/><ref name="12above"/> and is expressly required "for historical reasons".<ref>{{cite web|url=https://www.rfc-editor.org/rfc-style-guide/terms-online.txt |title=\'&#39;"RFC Style Guide"\'&#39;, Table of decisions on consistent usage in RFC |accessdate=2014-01-09}}</ref>\n* \'\'E-mail\'\' is sometimes used, capitalizing the initial \'\'E\'\' as in similar abbreviations like \'\'[[E-piano]]\'\', \'\'[[E-guitar]]\'\', \'\'[[A-bomb]]\'\', and \'\'[[H-bomb]]\'\'.<ref>{{cite web|url=http://alt-usage-english.org/excerpts/fxhowdoy.html |title=Excerpt from the FAQ list of the Usenet newsgroup alt.usage.english |publisher=Alt-usage-english.org |accessdate=2014-01-09}}</ref>\n\n== {{anchor|history}} Origin ==\nThe [[AUTODIN]] network, first operational in 1962, provided a message service between 1,350 terminals, handling 30 million messages per month, with an average message length of approximately 3,000 characters. Autodin was supported by 18 large computerized switches, and was connected to the [[United States General Services Administration]] Advanced Record System, which provided similar services to roughly 2,500 terminals.<ref name="NAS USPS">USPS Support Panel, Louis T Rader, Chair, Chapter IV: Systems, [https://books.google.com/books?id=5TQrAAAAYAAJ&pg=PA27 Electronic Message Systems for the U.S. Postal Service], National Academy of Sciences, Washington, D.C., 1976; pages 27–35.</ref> By 1968, AUTODIN linked more than 300 sites in several countries.\n\n===Host-based mail systems===\nWith the introduction of [[Massachusetts Institute of Technology|MIT]]\'s [[Compatible Time-Sharing System]] (CTSS) in 1961,<ref>"CTSS, Compatible Time-Sharing System" (September 4, 2006), [[University of South Alabama]], [http://www.cis.usouthal.edu/faculty/daigle/project1/ctss.htm USA-CTSS].</ref> multiple users could log in to a central system<ref>an [[IBM 7094]]</ref> from remote dial-up terminals, and store and share files on the central disk.<ref>[[Tom Van Vleck]], "The IBM 7094 and CTSS" (September 10, 2004), \'\'Multicians.org\'\'\n ([[Multics]]), web: [http://www.multicians.org/thvv/7094.html Multicians-7094].</ref> Informal methods of using this to pass messages were developed and expanded:\n* 1965 – [[Massachusetts Institute of Technology|MIT]]\'s [[Compatible Time-Sharing System|CTSS]] MAIL.<ref name="thvv">{{cite web|url=http://www.multicians.org/thvv/mail-history.html|title=The History of Electronic Mail|author=Tom Van Vleck}}</ref>\n\nDevelopers of other early systems developed similar email applications:\n<!-- Please do not delete references without first reading them. I\'ve added a page number. -->\n* 1962 – [[IBM Administrative Terminal System|1440/1460 Administrative Terminal System]]<ref>{{cite book\n |     author = IBM\n |      title = 1440/1460 Administrative Terminal System (1440-CX-07X and 1460-CX-08X) Application Description\n |    section =\n | sectionurl =\n |    version = Second Edition\n |  publisher = IBM\n |       date =\n |        url = http://bitsavers.org/pdf/ibm/144x/H20-0129-1_1440_admTermSys.pdf\n |         id = H20-0129-1\n | accessdate =\n |      quote =\n |       page = 10\n |      pages =\n |        ref =\n |mode=cs2\n }}</ref>\n* 1968 – [[IBM Administrative Terminal System|ATS/360]]<ref>{{cite book\n |     author = IBM\n |      title = System/36O Administrative Terminal System DOS (ATS/DOS) Program Description Manual\n |    section =\n | sectionurl =\n |    version =\n |  publisher = IBM\n |       date =\n |        url =\n |         id = H20-0508\n | accessdate =\n |      quote =\n |       page =\n |      pages =\n |        ref =\n |mode=cs2\n }}</ref><ref>{{cite book\n |     author = IBM\n |      title = System/360 Administrative Terminal System-OS (ATS/OS) Application Description Manual\n |    section =\n | sectionurl =\n |    version =\n |  publisher = IBM\n |       date =\n |        url =\n |         id = H20-0297\n | accessdate =\n |      quote =\n |       page =\n |      pages =\n |        ref =\n |mode=cs2\n }}</ref>\n* 1971 –  \'\'[[SNDMSG]]\'\', a local inter-user mail program incorporating the experimental file transfer program, \'\'CPYNET\'\', allowed the first [[Computer network|networked]] electronic mail<ref name="firstnetworkemail">{{cite web|author=Ray Tomlinson |url=http://openmap.bbn.com/~tomlinso/ray/firstemailframe.html |title=The First Network Email |publisher=Openmap.bbn.com |accessdate=2014-01-09}}</ref>\n* 1972 – [[Unix]] [[mail (Unix)|mail]] program<ref>{{cite web|url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V3/man/man1/mail.1 |title=Version 3 Unix mail(1) manual page from 10/25/1972 |publisher=Minnie.tuhs.org |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V6/usr/man/man1/mail.1 |title=Version 6 Unix mail(1) manual page from 2/21/1975 |publisher=Minnie.tuhs.org |accessdate=2014-01-09}}</ref>\n* 1972 – APL Mailbox by [[Lawrence M. Breed|Larry Breed]]<ref>[http://www.jsoftware.com/papers/APLQA.htm APL Quotations and Anecdotes], including [[Leslie H. Goldsmith|Leslie Goldsmith]]\'s story of the Mailbox</ref><ref>{{cite web|url=http://www.actewagl.com.au/Education/communications/Internet/historyOfTheInternet/InternetOnItsInfancy.aspx|title=Home > Communications > The Internet > History of the internet > Internet in its infancy|archive-url=https://web.archive.org/web/20110227151622/http://www.actewagl.com.au/Education/communications/Internet/historyOfTheInternet/InternetOnItsInfancy.aspx| archive-date=2011-02-27 |work=actewagl.com.au |accessdate=2016-11-03}}</ref><ref>{{cite web |url=https://www.youtube.com/watch?v=mjgkhK-nXmk | title=The STSC Story: It\'s About Time |date=c. 1979 |editor=[http://aprogramminglanguage.com Catherine Lathwell] |publisher=[[Scientific Time Sharing Corporation]] |at=7:08 |accessdate=2017-01-06 |}} Promotional video for Scientific Time Sharing Corporation, which features President [[Jimmy Carter]]\'s press secretary [[Jody Powell]] explaining how the company\'s "APL Mailbox" enabled the 1976 Carter presidential campaign to easily move information around the country to coordinate the campaign. </ref>\n* 1974 – The [[PLATO (computer system)|PLATO]] IV Notes on-line [[message board]] system was generalized to offer \'personal notes\' in August 1974.<ref name="NAS USPS" /><ref name=wooley>David Wooley, [http://www.thinkofit.com/plato/dwplato.htm#pnotes PLATO: The Emergence of an Online Community], 1994.</ref>\n* 1978 – \'\'Mail\'\' client written by Kurt Shoens for Unix and distributed with the Second Berkeley Software Distribution included support for aliases and distribution lists, forwarding, formatting messages, and accessing different mailboxes.<ref name="shoens">The Mail Reference Manual, Kurt Shoens, University of California, Berkeley, 1979.</ref> It used the Unix \'\'mail\'\' client to send messages between system users. The concept was extended to communicate remotely over the Berkeley Network.<ref name="berknet">An Introduction to the Berkeley Network, Eric Schmidt, University of California, Berkeley, 1979.</ref>\n* 1979 – \'\'EMAIL\'\', an application written by [[Shiva Ayyadurai]]. He has been associated with controversial, self-made claims that he had "invented" email due to its presence of certain functionality. These claims have been disputed by various parties.<ref>{{cite web|url=http://www.bizjournals.com/boston/news/2016/05/10/cambridge-man-who-claims-he-invented-email-sues.html|title=Cambridge man who claims he invented email sues Gawker for $35M - Boston Business Journal|last=Harris|first=David L.|date=May 10, 2016|website=Boston Business Journal|access-date=2016-05-16}}</ref><ref>[https://assets.documentcloud.org/documents/2829697/Gov-Uscourts-Mad-180248-1-0.pdf \'\'Shiva Ayyadurai v. Gawker Media, et. al\'\'., Complaint] (D. Mass, filed May 10, 2016)</ref><ref name=DavidCrockerWaPo>{{cite news|last=Crocker|first=David|title=A history of e-mail: Collaboration, innovation and the birth of a system|url=http://www.washingtonpost.com/national/on-innovations/a-history-of-e-mail-collaboration-innovation-and-the-birth-of-a-system/2012/03/19/gIQAOeFEPS_story.html|accessdate=10 June 2012|newspaper=Washington Post|date=20 March 2012}}</ref><ref>{{cite web|url=http://blogs.smithsonianmag.com/aroundthemall/2012/02/a-piece-of-email-history-comes-to-the-american-history-museum/|title=A Piece of Email History Comes to the American History Museum|date=22 February 2012|accessdate=11 June 2012|first=Joseph|last=Stromberg|publisher=[[Smithsonian Institution]]}}</ref><ref name=SmithsonianStatement>{{Cite press release|url=http://americanhistory.si.edu/press/releases/statement-national-museum-american-history-collection-materials-va-shiva-ayyudurai|title=Statement from the National Museum of American History: Collection of Materials from V.A. Shiva Ayyadurai|date=23 February 2012|accessdate=19 February 2013|publisher=National Museum of American History}}</ref>\n* 1979 – [[MH Message Handling System]] developed at RAND provided several tools for managing electronic mail on Unix.<ref name="borden-mh">A Mail Handling System, Bruce Borden, The Rand Corporation, 1979.</ref>\n* 1981 – [[IBM OfficeVision|PROFS]] by IBM<ref>[http://www.ibm.com/ibm100/us/en/icons/networkbus/ \'\'"...PROFS changed the way organizations communicated, collaborated and approached work when it was introduced by IBM\'s Data Processing Division in 1981..."\'\'], IBM.com</ref><ref>[https://fas.org/spp/starwars/offdocs/reagan/chron.txt \'\'"1982 – The National Security Council (NSC) staff at the White House acquires a prototype electronic mail system, from IBM, called the Professional Office System (PROFs)...."\'\'], fas.org</ref>\n* 1982 – [[ALL-IN-1]]<ref>{{cite web|url=https://research.microsoft.com/en-us/um/people/gbell/Digital/timeline/1982.htm |title=Gordon Bell\'s timeline of Digital Equipment Corporation |publisher=Research.microsoft.com |date=1998-01-30 |accessdate=2014-01-09}}</ref> by [[Digital Equipment Corporation]]\n* 1982 – HP Mail (later HP DeskManager) by [[Hewlett-Packard]]<ref>{{cite web|url=http://www.hpmuseum.net/divisions.php?did=10|title=HP Computer Museum}}</ref>\n\nThese original messaging systems had widely different features and ran on systems that were incompatible with each other. Most of them only allowed communication between users logged into the same host or "mainframe", although there might be hundreds or thousands of users within an organization.\n\n===LAN email systems===\nIn the early 1980s, networked [[personal computer]]s on [[LAN]]s became increasingly important. Server-based systems similar to the earlier mainframe systems were developed. Again, these systems initially allowed communication only between users logged into the same server infrastructure. Examples include:\n* [[cc:Mail]]\n* [[Lantastic]]\n* [[WordPerfect Office]]\n* [[Microsoft Mail]]\n* [[Banyan VINES]]\n* [[Lotus Notes]]\nEventually these systems too could link different organizations as long as they ran the same email system and proprietary protocol.<ref>with various vendors supplying gateway software to link these incompatible systems</ref>\n\n===Email networks===\nTo facilitate electronic mail exchange between remote sites and with other organizations, telecommunication links, such as dialup modems or leased lines, provided means to transport email globally, creating local and global networks. This was challenging for a number of reasons, including the widely [[Non-Internet email address|different email address formats]] in use. \n* In 1971 the first [[ARPANET]] email was sent,<ref>{{cite web|title=The First Network Email|url=http://openmap.bbn.com/~tomlinso/ray/firstemailframe.html|author=Ray Tomlinson}}</ref> and through RFC 561, RFC 680, RFC 724, and finally 1977\'s RFC 733, became a standardized working system.\n* PLATO IV was networked to individual terminals over leased data lines prior to the implementation of personal notes in 1974.<ref name=wooley/>\n* Unix mail was networked by 1978\'s [[uucp]],<ref>{{cite web|url=http://cm.bell-labs.com/7thEdMan/vol2/uucp.bun |title=Version 7 Unix manual: "UUCP Implementation Description" by D. A. Nowitz, and "A Dial-Up Network of UNIX Systems" by D. A. Nowitz and M. E. Lesk |accessdate=2014-01-09}}</ref> which was also used for [[USENET]] newsgroup postings, with similar headers.\n* BerkNet, the Berkeley Network, was written by [[Eric Schmidt]] in 1978 and included first in the Second Berkeley Software Distribution. It provided support for sending and receiving messages over serial communication links. The Unix mail tool was extended to send messages using BerkNet.<ref name="berknet"/> \n* The [[delivermail]] tool, written by [[Eric Allman]] in 1979 and 1980 (and shipped in 4BSD), provided support for routing mail over dissimilar networks, including Arpanet, UUCP, and BerkNet. (It also provided support for mail user aliases.)<ref name="joy-4bsd">Setting up the Fourth Berkeley Software Tape, William N. Joy, Ozalp Babaoglu, Keith Sklower, University of California, Berkeley, 1980.</ref>\n* The mail client included in 4BSD (1980) was extended to provide interoperability between a variety of mail systems.<ref name="shoens-mail">Mail(1), UNIX Programmer\'s Manual, 4BSD, University of California, Berkeley, 1980.</ref>\n* [[BITNET]] (1981) provided electronic mail services for educational institutions. It was based on the IBM VNET email system.<ref>[http://www.livinginternet.com/u/ui_bitnet.htm "BITNET History"], livinginternet.com</ref>\n* 1983 – [[MCI Mail]] Operated by MCI Communications Corporation.  This was the first commercial public email service to use the internet. MCI Mail also allowed subscribers to send regular postal mail (overnight) to non-subscribers.<ref>"[[MCI Mail]]", MCI Mail</ref>\n* In 1984, IBM PCs running DOS could link with [[FidoNet]] for email and shared bulletin board posting.\n\n===Email address internationalization===\nGlobally countries started adopting [[Internationalized domain name|IDN]] registrations for supporting country specific scripts (non-English) for domain names. In 2010  Egypt, the Russian Federation, Saudi Arabia, and the United Arab Emirates started offering IDN registrations. The government of India also registered [[Bhārat Gaṇarājya|.bharat]]<ref>{{Cite web|url=https://registry.in/Internationalized_Domain_Names_IDNs|title=Internationalized Domain Names (IDNs) {{!}} Registry.In|website=registry.in|access-date=2016-10-17}}</ref> in 8 languages/scripts in 2014. \nIn 2016 Data Xgen Technologies was credited as World\'s first email platform offering EAI in India and [[Russia]].<ref>http://economictimes.indiatimes.com/tech/internet/datamail-worlds-first-free-linguistic-email-service-supports-eight-india-languages/articleshow/54923001.cms</ref><ref>http://digitalconqurer.com/gadgets/made-india-datamail-empowers-russia-email-address-russian-language/</ref>\n\n===Attempts at interoperability===\n{{Refimprove section|date=August 2010}}\nEarly interoperability among independent systems included:\n* [[ARPANET]], a forerunner of the Internet, defined protocols for dissimilar computers to exchange email.\n* [[uucp]] implementations for Unix systems, and later for other operating systems, that only had dial-up communications available.\n* [[CSNET]], which initially used the UUCP protocols via dial-up to provide networking and mail-relay services for non-ARPANET hosts.\n* Action Technologies developed the [[Message Handling System]] (MHS) protocol (later bought by [[Novell]],<ref>[https://books.google.com/books?id=vxcEAAAAMBAJ&pg=PA64 "Delivering the Enterprise Message], 19 Sep 1994, Daniel Blum, Network World</ref><ref>[http://www.networkworld.com/archive/1994/94-03-07hot_.html \'\'"...offers improved performance, greater reliability and much more flexibility in everything from communications hardware to scheduling..."\'\'], 03/07/94, Mark Gibbs,\nNetwork World</ref><ref>{{cite web | url = http://support.microsoft.com/kb/118859 | title = MHS: Correct Addressing format to DaVinci Email via MHS | work = Microsoft Support Knowledge Base | accessdate = 2007-01-15 }}</ref> which abandoned it after purchasing the non-MHS WordPerfect Office&mdash;renamed [[Novell GroupWise|Groupwise]]).\n* [[HP OpenMail]] was known for its ability to interconnect several other APIs and protocols, including MAPI, cc:Mail, SMTP/MIME, and X.400.\n* Soft-Switch released its eponymous email gateway product in 1984, acquired by [[Lotus Software]] ten years later.<ref>https://www.linkedin.com/in/nickshelness</ref>\n* The [[Coloured Book protocols]] ran on [[United Kingdom|UK]] academic networks until 1992.\n* [[X.400]] in the 1980s and early 1990s was promoted by major vendors, and mandated for government use under [[GOSIP]], but abandoned by all but a few in favor of [[Internet]] [[Simple Mail Transfer Protocol|SMTP]] by the mid-1990s.\n\n===From SNDMSG to MSG===\nIn the early 1970s, [[Ray Tomlinson]] updated an existing utility called [[SNDMSG]] so that it could copy messages (as files) over the network. [[Lawrence Roberts (scientist)|Lawrence Roberts]], the project manager for the ARPANET development, took the idea of READMAIL, which dumped all "recent" messages onto the user\'s terminal, and wrote a programme for [[TOPS-20#TENEX|TENEX]] in [[Text Editor and Corrector|TECO]] macros called \'\'RD\'\', which permitted access to individual messages.<ref name="livinginternet1">{{cite web|url=http://www.livinginternet.com/e/ei.htm |title=Email History |publisher=Livinginternet.com |date=1996-05-13 |accessdate=2014-01-09}}</ref> Barry Wessler then updated RD and called it \'\'NRD\'\'.<ref>* {{Cite journal|last=Partridge|first=Craig|title=The Technical Development of Internet Email|journal=IEEE Annals of the History of Computing|volume=30|issue=2|publisher=IEEE Computer Society|location=Berlin|date=April–June 2008|url=http://www.ir.bbn.com/~craig/email.pdf|format=PDF | doi =  10.1109/mahc.2008.32|pages=3–29}}</ref>\n\nMarty Yonke rewrote NRD to include reading, access to SNDMSG for sending, and a help system, and called the utility \'\'WRD\'\', which was later known as \'\'BANANARD\'\'. John Vittal then updated this version to include three important commands: \'\'Move\'\' (combined save/delete command), \'\'Answer\'\' (determined to whom a reply should be sent) and \'\'Forward\'\' (sent an email to a person who was not already a recipient). The system was called \'\'MSG\'\'. With inclusion of these features, MSG is considered to be the first integrated modern email programme, from which many other applications have descended.<ref name="livinginternet1"/>\n\n===ARPANET mail===\nExperimental email transfers between separate computer systems began shortly after the creation of the [[ARPANET]] in 1969.<ref name="thvv" /> [[Ray Tomlinson]] is generally credited as having sent the first email across a network, initiating the use of the "[[At sign|@]]" sign to separate the names of the user and the user\'s machine in 1971, when he sent a message from one [[Digital Equipment Corporation]] [[DEC-10]] computer to another DEC-10. The two machines were placed next to each other.<ref name="firstnetworkemail"/><ref>Wave New World,Time Magazine, October 19, 2009, p.48</ref> Tomlinson\'s work was quickly adopted across the ARPANET, which significantly increased the popularity of email. Tomlinson is internationally known as the inventor of modern email.<ref>{{cite web|url=http://www.npr.org/2016/03/06/469428062/ray-tomlinson-inventor-of-modern-email-has-died|title=Ray Tomlinson, Inventor Of Modern Email, Dies|date=6 March 2016|work=NPR.org}}</ref>\n\nInitially addresses were of the form, \'\'username@hostname\'\'<ref>RFC 805, 8 February 1982, Computer Mail Meeting Notes</ref> but were extended to "username@host.domain" with the development of the [[Domain Name System]] (DNS).\n\nAs the influence of the ARPANET spread across academic communities, [[Gateway (telecommunications)|gateways]] were developed to pass mail to and from other networks such as [[CSNET]], [[JANET NRS|JANET]], [[BITNET]], [[X.400]], and [[FidoNet]]. This often involved addresses such as:\n:hubhost!middlehost!edgehost!user@uucpgateway.somedomain.example.com\nwhich routes mail to a user with a "[[UUCP#Mail routing|bang path]]" address at a UUCP host.\n\n==Operation==\nThe diagram to the right shows a typical sequence of events<ref>{{cite video|title=How E-mail Works|medium=internet video|publisher=howstuffworks.com|year=2008|url=http://www.webcastr.com/videos/informational/how-email-works.html}}</ref> that takes place when sender [[Placeholder names in cryptography|Alice]] transmits a message using a [[E-mail client|mail user agent]] (MUA) addressed to the [[email address]] of the recipient.\n<span style="float:right">[[File:email.svg|400px|Email operation]]</span>\n# The MUA formats the message in email format and uses the submission protocol, a profile of the [[Simple Mail Transfer Protocol]] (SMTP), to send the message to the local [[mail submission agent]] (MSA), in this case \'\'smtp.a.org\'\'.\n# The MSA determines the destination address provided in the SMTP protocol (not from the message header), in this case \'\'bob@b.org\'\'. The part before the @ sign is the \'\'local part\'\' of the address, often the [[username]] of the recipient, and the part after the @ sign is a [[domain name]]. The MSA resolves a domain name to determine the [[fully qualified domain name]] of the [[Message transfer agent|mail server]] in the [[Domain Name System]] (DNS).\n# The [[DNS server]] for the domain \'\'b.org\'\' (\'\'ns.b.org\'\') responds with any [[MX record]]s listing the mail exchange servers for that domain, in this case \'\'mx.b.org\'\', a [[message transfer agent]] (MTA) server run by the recipient\'s ISP.<ref>[https://dnsdb.cit.cornell.edu/explain_mx.html "MX Record Explanation"], it.cornell.edu</ref>\n# smtp.a.org sends the message to mx.b.org using SMTP. This server may need to forward the message to other MTAs before the message reaches the final [[message delivery agent]] (MDA).\n# The MDA delivers it to the [[Email Mailbox|mailbox]] of user \'\'bob\'\'.\n# Bob\'s MUA picks up the message using either the [[Post Office Protocol]] (POP3) or the [[Internet Message Access Protocol]] (IMAP).\n\nIn addition to this example, alternatives and complications exist in the email system:\n* Alice or Bob may use a client connected to a corporate email system, such as [[IBM]] [[Lotus Notes]] or [[Microsoft]] [[Microsoft Exchange Server|Exchange]]. These systems often have their own internal email format and their clients typically communicate with the email server using a vendor-specific, proprietary protocol. The server sends or receives email via the Internet through the product\'s Internet mail gateway which also does any necessary reformatting. If Alice and Bob work for the same company, the entire transaction may happen completely within a single corporate email system.\n* Alice may not have a MUA on her computer but instead may connect to a [[webmail]] service.\n* Alice\'s computer may run its own MTA, so avoiding the transfer at step 1.\n* Bob may pick up his email in many ways, for example logging into mx.b.org and reading it directly, or by using a webmail service.\n* Domains usually have several mail exchange servers so that they can continue to accept mail even if the primary is not available.\n\nMany MTAs used to accept messages for any recipient on the Internet and do their best to deliver them. Such MTAs are called \'\'[[open mail relay]]s\'\'. This was very important in the early days of the Internet when network connections were unreliable.{{citation needed|date=July 2015}}  However, this mechanism proved to be exploitable by originators of [[email spam|unsolicited bulk email]] and as a consequence open mail relays have become rare,<ref name="IMCR-016">{{cite web|url=http://www.imc.org/ube-relay.html |title=Allowing Relaying in SMTP: A Series of Surveys |accessdate=2008-04-13 |last=Hoffman |first=Paul |date=2002-08-20 |work=IMC Reports |publisher=[[Internet Mail Consortium]] |archiveurl=https://web.archive.org/web/20070118121843/http://www.imc.org/ube-relay.html |archivedate=2007-01-18 }}</ref> and many MTAs do not accept messages from open mail relays.\n\n==Message format {{anchor|Internet Message Format}}==\nThe Internet email message format is now defined by RFC 5322, with multimedia content attachments being defined in RFC 2045 through RFC 2049, collectively called \'\'[[Multipurpose Internet Mail Extensions]]\'\' or \'\'MIME\'\'. RFC 5322 replaced the earlier RFC 2822 in 2008, and in turn RFC 2822 in 2001 replaced RFC 822 – which had been the standard for Internet email for nearly 20 years. Published in 1982, RFC 822 was based on the earlier RFC 733 for the [[ARPANET]].<ref>{{cite web|first=Ken|last=Simpson|title=An update to the email standards|date=October 3, 2008|publisher=MailChannels Blog Entry|url= http://blog.mailchannels.com/2008/10/update-to-email-standards.html}}</ref>\n\nInternet email messages consist of two major sections, the message header and the message body.  The header is structured into [[Field (computer science)|fields]] such as From, To, CC, Subject, Date, and other information about the email. In the process of transporting email messages between systems, SMTP communicates delivery parameters and information using message header fields. The body contains the message, as unstructured text, sometimes containing a [[signature block]] at the end. The header is separated from the body by a blank line.\n\n===Message header===\n<!-- This section is linked from [[Bracket]] -->\nEach message has exactly one [[Header (computing)|header]], which is structured into [[Field (computer science)|fields]]. Each field has a name and a value. RFC 5322 specifies the precise syntax.\n\nInformally, each line of text in the header that begins with a [[Printable characters|printable character]] begins a separate field. The field name starts in the first character of the line and ends before the separator character ":". The separator is then followed by the field value (the "body" of the field). The value is continued onto subsequent lines if those lines have a space or tab as their first character. Field names and values are restricted to 7-bit [[ASCII]] characters. Non-ASCII values may be represented using MIME [[MIME#Encoded-Word|encoded words]].\n\n====Header fields====\nEmail header fields can be multi-line, and each line should be at most 78 characters long and in no event more than 998 characters long.<ref>{{cite web|url=https://tools.ietf.org/html/rfc5322|title=RFC 5322, Internet Message Format|author=P. Resnick, Ed.|date=October 2008|publisher=IETF}}</ref> Header fields defined by RFC 5322 can only contain [[US-ASCII]] characters; for encoding characters in other sets, a syntax specified in RFC 2047 can be used.<ref>{{cite web|last=Moore|first=K|title=MIME (Multipurpose Internet Mail Extensions) Part Three: Message Header Extensions for Non-ASCII Text|url=https://tools.ietf.org/html/rfc2047|publisher=[[Internet Engineering Task Force|IETF]]|accessdate=2012-01-21| date=November 1996 }}</ref> Recently the IETF EAI working group has defined some standards track extensions,<ref>{{cite web|url=https://tools.ietf.org/html/rfc6532|title=RFC 6532, Internationalized Email Headers|author=A Yang, Ed.|date=February 2012|publisher=IETF|ISSN=2070-1721}}</ref><ref>{{cite web|url=https://tools.ietf.org/html/rfc6531|title=RFC 6531, SMTP Extension for Internationalized Email Addresses|author=J. Yao, Ed., W. Mao, Ed.|date=February 2012|publisher=IETF|ISSN=2070-1721}}</ref> replacing previous experimental extensions, to allow [[UTF-8]] encoded [[Unicode]] characters to be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such addresses are supported by Google and Microsoft products, and promoted by some governments.<ref name="economictimes.indiatimes.com">{{Cite news|url=http://economictimes.indiatimes.com/tech/internet/now-get-your-email-address-in-hindi/articleshow/53830034.cms|title=Now, get your email address in Hindi - The Economic Times|newspaper=The Economic Times|access-date=2016-10-17}}</ref>\n\nThe message header must include at least the following fields:<ref>{{cite web|url=https://tools.ietf.org/html/rfc5322#section-3.6 |title=RFC 5322, 3.6. Field Definitions |publisher=Tools.ietf.org |date=October 2008|accessdate=2014-01-09}}</ref><ref>{{cite web|url=https://tools.ietf.org/html/rfc5322#section-3.6.4 |title=RFC 5322, 3.6.4. Identification Fields |publisher=Tools.ietf.org |date=October 2008|accessdate=2014-01-09}}</ref>\n* \'\'From\'\': The [[email address]], and optionally the name of the author(s). In many email clients not changeable except through changing account settings.\n* \'\'Date\'\': The local time and date when the message was written. Like the \'\'From:\'\' field, many email clients fill this in automatically when sending. The recipient\'s client may then display the time in the format and time zone local to him/her.\n\nRFC 3864 describes registration procedures for message header fields at the [[Internet Assigned Numbers Authority|IANA]]; it provides for [http://www.iana.org/assignments/message-headers/perm-headers.html permanent] and [http://www.iana.org/assignments/message-headers/prov-headers.html provisional] field names, including also fields defined for MIME, netnews, and HTTP, and referencing relevant RFCs. Common header fields for email include:<ref>{{cite web|url=https://tools.ietf.org/html/rfc5064 |title=RFC 5064 |publisher=Tools.ietf.org|date=December 2007 |accessdate=2014-01-09}}</ref>\n\n* \'\'To\'\': The email address(es), and optionally name(s) of the message\'s recipient(s). Indicates primary recipients (multiple allowed), for secondary recipients see Cc: and Bcc: below.\n* \'\'Subject\'\': A brief summary of the topic of the message. [[E-mail subject abbreviations|Certain abbreviations]] are commonly used in the subject, including [[E-mail subject abbreviations|"RE:" and "FW:"]].\n* \'\'Cc\'\': [[Carbon copy]]; Many email clients will mark email in one\'s inbox differently depending on whether they are in the To: or Cc: list. (\'\'Bcc\'\': [[Blind carbon copy]]; addresses are usually only specified during SMTP delivery, and not usually listed in the message header.)\n* [[Content-Type]]: Information about how the message is to be displayed, usually a [[MIME]] type.\n* \'\'Precedence\'\': commonly with values "bulk", "junk", or "list"; used to indicate that automated "vacation" or "out of office" responses should not be returned for this mail, e.g. to prevent vacation notices from being sent to all other subscribers of a mailing list. [[Sendmail]] uses this field to affect prioritization of queued email, with "Precedence: special-delivery" messages delivered sooner. With modern high-bandwidth networks, delivery priority is less of an issue than it once was. [[Microsoft Exchange Server|Microsoft Exchange]] respects a fine-grained automatic response suppression mechanism, the \'\'X-Auto-Response-Suppress\'\' field.<ref>Microsoft, Auto Response Suppress, 2010, [http://msdn.microsoft.com/en-us/library/ee219609(v=EXCHG.80).aspx microsoft reference], 2010 Sep 22</ref>\n* \'\'Message-ID\'\': Also an automatically generated field; used to prevent multiple delivery and for reference in In-Reply-To: (see below).\n* \'\'In-Reply-To\'\': [[Message-ID]] of the message that this is a reply to. Used to link related messages together. This field only applies for reply messages.\n* \'\'References\'\': [[Message-ID]] of the message that this is a reply to, and the message-id of the message the previous reply was a reply to, etc.\n* \'\'Reply-To\'\': Address that should be used to reply to the message.\n* \'\'Sender\'\': Address of the actual sender acting on behalf of the author listed in the From: field (secretary, list manager, etc.).\n* \'\'Archived-At\'\': A direct link to the archived form of an individual email message.\n\nNote that the \'\'To:\'\' field is not necessarily related to the addresses to which the message is delivered. The actual delivery list is supplied separately to the transport protocol, [[Simple Mail Transfer Protocol|SMTP]], which may or may not originally have been extracted from the header content. The "To:" field is similar to the addressing at the top of a conventional letter which is delivered according to the address on the outer envelope. In the same way, the "From:" field does not have to be the real sender of the email message. Some mail servers apply [[email authentication]] systems to messages being relayed. Data pertaining to server\'s activity is also part of the header, as defined below.\n\nSMTP defines the \'\'trace information\'\' of a message, which is also saved in the header using the following two fields:<ref>{{cite IETF|title=Simple Mail Transfer Protocol|rfc=5321|author=[[John Klensin]]|sectionname=Trace Information|section=4.4| date=October 2008 |publisher=[[Internet Engineering Task Force|IETF]]}}</ref>\n* \'\'Received\'\': when an SMTP server accepts a message it inserts this trace record at the top of the header (last to first).\n* \'\'Return-Path\'\': when the delivery SMTP server makes the \'\'final delivery\'\' of a message, it inserts this field at the top of the header.\n\nOther fields that are added on top of the header by the receiving server may be called \'\'trace fields\'\', in a broader sense.<ref>{{cite web|url=http://www.ietf.org/mail-archive/web/apps-discuss/current/msg04115.html|title=Trace headers|author=John Levine|date=14 January 2012|work=email message|publisher=[[Internet Engineering Task Force|IETF]]|accessdate=16 January 2012|quote=there are many more trace fields than those two}}</ref>\n* \'\'Authentication-Results\'\': when a server carries out authentication checks, it can save the results in this field for consumption by downstream agents.<ref>This extensible field is defined by RFC 7001, that also defines an [[Internet Assigned Numbers Authority|IANA]] registry of [http://www.iana.org/assignments/email-auth/ Email Authentication Parameters].</ref>\n* \'\'Received-SPF\'\': stores results of [[Sender Policy Framework|SPF]] checks in more detail than Authentication-Results.<ref>RFC 7208.</ref>\n* \'\'Auto-Submitted\'\': is used to mark automatically generated messages.<ref>Defined in RFC 3834, and updated by RFC 5436.</ref>\n* \'\'VBR-Info\'\': claims [[Vouch by Reference|VBR]] whitelisting<ref>RFC 5518.</ref>\n\n===Message body===\n\n====Content encoding====\nEmail was originally designed for 7-bit [[ASCII]].<ref>{{cite book|title=TCP/IP Network Administration|year=2002|isbn=978-0-596-00297-8|author=Craig Hunt|publisher=[[O\'Reilly Media]]|page=70}}</ref> Most email software is [[8-bit clean]] but must assume it will communicate with 7-bit servers and mail readers. The [[MIME]] standard introduced character set specifiers and two content transfer encodings to enable transmission of non-ASCII data: [[quoted printable]] for mostly 7-bit content with a few characters outside that range and [[base64]] for arbitrary binary data. The [[8BITMIME]] and [[BINARY]] extensions were introduced to allow transmission of mail without the need for these encodings, but many [[mail transport agent]]s still do not support them fully. In some countries, several encoding schemes coexist; as the result, by default, the message in a non-Latin alphabet language appears in non-readable form (the only exception is coincidence, when the sender and receiver use the same encoding scheme). Therefore, for international [[character set]]s, [[Unicode]] is growing in popularity.{{citation needed|date=September 2014}}\n\n====Plain text and HTML====\nMost modern graphic [[email client]]s allow the use of either [[plain text]] or [[HTML#HTML email|HTML]] for the message body at the option of the user. [[HTML email]] messages often include an automatically generated plain text copy as well, for compatibility reasons. Advantages of HTML include the ability to include in-line links and images, set apart previous messages in [[block quote]]s, wrap naturally on any display, use emphasis such as [[underline]]s and [[italics]], and change [[font]] styles. Disadvantages include the increased size of the email, privacy concerns about [[web bug]]s, abuse of HTML email as a vector for [[phishing]] attacks and the spread of [[malware|malicious software]].<ref>{{cite web|title=Email policies that prevent viruses|url=http://advosys.ca/papers/mail-policies.html}}</ref>\n\nSome web-based [[mailing list]]s recommend that all posts be made in plain-text, with 72 or 80 [[characters per line]]<ref>{{cite web|url=http://helpdesk.rootsweb.com/listadmins/plaintext.html |title=When posting to a RootsWeb mailing list... |publisher=Helpdesk.rootsweb.com |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://www.openbsd.org/mail.html |title=...Plain text, 72 characters per line... |publisher=Openbsd.org |accessdate=2014-01-09}}</ref> for all the above reasons, but also because they have a significant number of readers using [[List of email clients#Text-based|text-based email clients]] such as [[Mutt (email client)|Mutt]]. Some [[Microsoft]] [[email client]]s allow rich formatting using their proprietary [[Rich Text Format]] (RTF), but this should be avoided unless the recipient is guaranteed to have a compatible [[email client]].<ref>{{cite web|url=http://support.microsoft.com/kb/138053 |title=How to Prevent the Winmail.dat File from Being Sent to Internet Users |publisher=Support.microsoft.com |date=2010-07-02 |accessdate=2014-01-09}}</ref>\n\n==Servers and client applications==\n[[File:Mozilla Thunderbird 3.1.png|thumb|right|300px|The interface of an email client, [[Mozilla Thunderbird|Thunderbird]].]]\n<!-- This section is linked from [[Catch-all (Mail)]]. See [[WP:MOS#Section management]] -->\nMessages are exchanged between hosts using the [[Simple Mail Transfer Protocol]] with software programs called [[mail transfer agent]]s (MTAs); and delivered to a mail store by programs called [[mail delivery agent]]s (MDAs, also sometimes called local delivery agents, LDAs). Accepting a message obliges an MTA to deliver it,<ref>In practice, some accepted messages may nowadays not be delivered to the recipient\'s InBox, but instead to a Spam or Junk folder which, especially in a corporate environment, may be inaccessible to the recipient</ref> and when a message cannot be delivered, that MTA must send a [[bounce message]] back to the sender, indicating the problem.\n\nUsers can retrieve their messages from servers using standard protocols such as [[Post Office Protocol|POP]] or [[IMAP]], or, as is more likely in a large [[corporation|corporate]] environment, with a [[Proprietary software|proprietary]] protocol specific to [[Novell Groupwise]], [[Lotus Notes]] or [[Microsoft Exchange Server]]s.  Programs used by users for retrieving, reading, and managing email are called [[mail user agent]]s (MUAs).\n\nMail can be stored on the [[client (computing)|client]], on the [[Server (computing)|server]] side, or in both places. Standard formats for mailboxes include [[Maildir]] and [[mbox]]. Several prominent email clients use their own proprietary format and require conversion software to transfer email between them. Server-side storage is often in a proprietary format but since access is through a standard protocol such as [[IMAP]], moving email from one server to another can be done with any [[Mail user agent|MUA]] supporting the protocol.\n\nMany current email users do not run MTA, MDA or MUA programs themselves, but use a web-based email platform, such as Gmail, Hotmail, or Yahoo! Mail, that performs the same tasks.<ref>http://dir.yahoo.com/business_and_economy/business_to_business/communications_and_networking/internet_and_world_wide_web/email_providers/free_email/</ref> Such [[webmail]] interfaces allow users to access their mail with any standard [[web browser]], from any computer, rather than relying on an email client.\n\n===Filename extensions===\nUpon reception of email messages, [[email client]] applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the \'\'[[mbox]]\'\' format. The specific format used is often indicated by special [[filename extension]]s:\n;<tt>eml</tt>\n:Used by many email clients including [[Novell GroupWise]], [[Microsoft Outlook Express]], [[Lotus notes]], [[Windows Mail]], [[Mozilla Thunderbird]], and Postbox. The files are [[plain text]] in [[MIME]] format, containing the email header as well as the message contents and attachments in one or more of several formats.\n;<tt>emlx</tt>\n:Used by [[Apple Mail]].\n;<tt>msg</tt>\n:Used by [[Microsoft Outlook|Microsoft Office Outlook]] and [[OfficeLogic|OfficeLogic Groupware]].\n;<tt>mbx</tt>\n:Used by [[Opera Mail]], [[KMail]], and [[Apple Mail]] based on the [[mbox]] format.\n\nSome applications (like [[Apple Mail]]) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.\n\n===URI scheme mailto===\n{{main article|mailto}}\nThe [[URI scheme]], as registered with the [[Internet Assigned Numbers Authority|IANA]], defines the <tt>mailto:</tt> scheme for SMTP email addresses. Though its use is not strictly defined, URLs of this form are intended to be used to open the new message window of the user\'s mail client when the URL is activated, with the address as defined by the URL in the \'\'To:\'\' field.<ref>RFC 2368 section 3 : by Paul Hoffman in 1998 discusses operation of the "mailto" URL.</ref>\n\n==Types==\n\n===Web-based email===\n{{main article|Webmail}}\nMany email providers have a web-based email client (e.g. [[AOL Mail]], [[Gmail]], [[Outlook.com]], [[Hotmail]] and [[Yahoo! Mail]]). This allows users to log in to the email account by using any compatible [[web browser]] to send and receive their email. Mail is typically not downloaded to the client, so can\'t be read without a current Internet connection.\n\n===POP3 email services===\nThe [[Post Office Protocol]] 3 (POP3) is a mail access protocol used by a client application to read messages from the mail server. Received messages are often deleted from the [[server (computing)|server]]. POP supports simple download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC\'s).<ref name="Windows to Linux">{{cite book | last = Allen | first = David | title = Windows to Linux | publisher = Prentice Hall | year = 2004 | location = | page =192 | url = https://books.google.com/books?id=UD0h_GqgbHgC&printsec=frontcover&dq=network%2B+guide+to+networks&hl=en&src=bmrr&ei=hMnATfmmA8j00gGMsOC2Cg&sa=X&oi=book_result&ct=result&resnum=1&ved=0CE8Q6AEwAA#v=onepage&q&f=false}}</ref>\n\n===IMAP email servers===\nThe [[Internet Message Access Protocol]] (IMAP) provides features to manage a mailbox from multiple devices. Small portable devices like [[smartphone]]s are increasingly used to check email while travelling, and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually mail is left in folders in the mail server.\n\n===MAPI email servers===\n[[MAPI|Messaging Application Programming Interface]] (MAPI) is used by [[Microsoft Outlook]] to communicate to [[Microsoft Exchange Server]] - and to a range of other email server products such as [[Axigen|Axigen Mail Server]], [[Kerio Connect]], [[Scalix]], [[Zimbra]], [[HP OpenMail]], [[IBM Lotus Notes]], [[Zarafa (software)|Zarafa]], and [[Bynari]] where vendors have added MAPI support to allow their products to be accessed directly via Outlook.\n\n==Uses==\n{{Refimprove section|date=November 2007}}\n\n===Business and organizational use===\nEmail has been widely accepted by business, governments and non-governmental organizations in the developed world, and it is one of the key parts of an \'e-revolution\' in workplace communication (with the other key plank being widespread adoption of highspeed [[Internet]]). A sponsored 2010 study on workplace communication found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.<ref name=om>By Om Malik, GigaOm. "[http://gigaom.com/collaboration/is-email-a-curse-or-a-boon/ Is Email a Curse or a Boon?]" September 22, 2010. Retrieved October 11, 2010.</ref>\n\nIt has some key benefits to business and other organizations, including:\n; Facilitating logistics\n: Much of the business world relies on communications between people who are not physically in the same building, area, or even country; setting up and attending an in-person meeting, [[telephone call]], or [[conference call]] can be inconvenient, time-consuming, and costly. Email provides a method of exchanging information between two or more people with no set-up costs and that is generally far less expensive than a physical meeting or phone call.\n; Helping with synchronisation\n: With [[Real-time computing|real time]] communication by meetings or phone calls, participants must work on the same schedule, and each participant must spend the same amount of time in the meeting or call. Email allows [[wikt:asynchrony|asynchrony]]: each participant may control their schedule independently.\n; Reducing cost\n: Sending an email is much less expensive than sending postal mail, or [[long distance telephone call]]s, [[telex]] or [[telegrams]].\n; Increasing speed\n: Much faster than most of the alternatives.\n; Creating a "written" record\n: Unlike a telephone or in-person conversation, email by its nature creates a detailed written record of the communication, the identity of the sender(s) and recipient(s) and the date and time the message was sent. In the event of a contract or legal dispute, saved emails can be used to prove that an individual was advised of certain issues, as each email has the date and time recorded on it.\n\n====Email marketing====\nEmail marketing via "[[opt-in email|opt-in]]" is often successfully used to send special sales offerings and new product information.<ref name=brett>{{cite journal | last1 = Martin | first1 = Brett A. S. | last2 = Van Durme | first2 = Joel | last3 = Raulas | first3 = Mika | last4 = Merisavo | first4 = Marko | year = 2003 | title = E-mail Marketing: Exploratory Insights from Finland | url = http://www.basmartin.com/wp-content/uploads/2010/08/Martin-et-al-2003.pdf | format = PDF | journal = Journal of Advertising Research | volume = 43 | issue = 3| pages = 293–300 | doi=10.1017/s0021849903030265}}</ref> Depending on the recipient\'s culture,<ref>{{cite web|url=http://www.computerworld.com/article/2467778/endpoint-security/spam-culture--part-1--china.html|title=Spam culture, part 1: China|first=Amir|last=Lev|publisher=}}</ref> email sent without permission&mdash;such as an "opt-in"&mdash;is likely to be viewed as unwelcome "[[email spam]]".\n\n===Personal use===\n\n====Desktop====\nMany users access their personal email from friends and family members using a [[desktop computer]] in their house or apartment.\n\n====Mobile====\nEmail has become widely used on [[smartphone]]s and [[Wi-Fi]]-enabled [[laptop]]s and [[tablet computer]]s. Mobile "apps" for email increase accessibility to the medium for users who are out of their home. While in the earliest years of email, users could only access email on desktop computers, in the 2010s, it is possible for users to check their email when they are away from home, whether they are across town or across the world. Alerts can also be sent to the smartphone or other device to notify them immediately of new messages. This has given email the ability to be used for more frequent communication between users and allowed them to check their email and write messages throughout the day. Today, there are an estimated 1.4 billion email users worldwide and 50 billion non-spam emails that are sent daily.\n\nIndividuals often check email on smartphones for both personal and work-related messages. It was found that US adults check their email more than they browse the web or check their [[Facebook]] accounts, making email the most popular activity for users to do on their smartphones. 78% of the respondents in the study revealed that they check their email on their phone.<ref>{{cite web|url=http://marketingland.com/smartphone-activities-study-email-web-facebook-37954|title=Email Is Top Activity On Smartphones, Ahead Of Web Browsing & Facebook [Study]|date=28 March 2013|publisher=}}</ref> It was also found that 30% of consumers use only their smartphone to check their email, and 91% were likely to check their email at least once per day on their smartphone. However, the percentage of consumers using email on smartphone ranges and differs dramatically across different countries. For example, in comparison to 75% of those consumers in the US who used it, only 17% in India did.<ref>{{cite web|url=http://www.emailmonday.com/mobile-email-usage-statistics|title=The ultimate mobile email statistics overview|publisher=}}</ref>\n\n==Issues==\n{{Refimprove section|date=October 2016}}\n\n===Attachment size limitation===\n{{Main article|Email attachment}}\nEmail messages may have one or more attachments, which are additional files that are appended to the email. Typical attachments include [[Microsoft Word]] documents, [[pdf]] documents and scanned images of paper documents. In principle there is no technical restriction on the size or number of attachments, but in practice email clients, [[server (computing)|server]]s and Internet service providers implement various limitations on the size of files, or complete email - typically to 25MB or less.<ref>[http://exchangepedia.com/2007/09/exchange-server-2007-setting-message-size-limits.html \'\'"Setting Message Size Limits in Exchange 2010 and Exchange 2007"\'\'].</ref><ref>[http://www.geek.com/articles/news/google-updates-file-size-limits-for-gmail-and-youtube-20090629/#ixzz0oIzFY0Q8 \'\'"Google updates file size limits for Gmail and YouTube"\'\', geek.com].</ref><ref>[http://mail.google.com/support/bin/answer.py?answer=8770&topic=1517 \'\'"Maximum attachment size"\'\', mail.google,com].</ref> Furthermore, due to technical reasons, attachment sizes as seen by these transport systems can differ to what the user sees,<ref>{{cite web|url=http://technet.microsoft.com/en-us/magazine/2009.01.exchangeqa.aspx?pr=blog|title=Exchange 2007: Attachment Size Increase,...|date=2010-03-25|publisher=TechNet Magazine, Microsoft.com US}}</ref> which can be confusing to senders when trying to assess whether they can safely send a file by email. Where larger files need to be shared, [[file hosting service]]s of various sorts are available; and generally suggested.<ref>[https://support.office.com/en-us/article/Send-large-files-to-other-people-7005da19-607a-47d5-b2c5-8f3982c6cc83 "Send large files to other people"], Microsoft.com</ref><ref>[http://www.makeuseof.com/tag/8-ways-to-email-large-attachments/ "8 ways to email large attachments"], Chris Hoffman, December 21, 2012, makeuseof.com</ref> Some large files, such as digital photos, color presentations and video or music files are too large for some email systems.\n\n===Information overload===\nThe ubiquity of email for knowledge workers and "white collar" employees has led to concerns that recipients face an "[[information overload]]" in dealing with increasing volumes of email.<ref>{{cite web|last=Radicati|first=Sara|title=Email Statistics Report, 2010|url=http://www.radicati.com/wp/wp-content/uploads/2010/04/Email-Statistics-Report-2010-2014-Executive-Summary2.pdf}}</ref><ref>{{cite news|last=Gross|first=Doug|title=Happy Information Overload Day!|url=http://articles.cnn.com/2010-10-20/tech/information.overload.day_1_mails-marsha-egan-rss?_s=PM:TECH|work=CNN|date=July 26, 2011}}</ref> This can lead to increased stress, decreased satisfaction with work, and some observers even argue it could have a significant negative economic effect,<ref>{{cite news|url=http://www.nytimes.com/2008/04/20/technology/20digi.html?_r=2&oref=slogin&oref=slogin|title=Struggling to Evade the E-Mail Tsunami|date=2008-04-20|publisher=The New York Times|first=Randall|last=Stross|accessdate=May 1, 2010}}</ref> as efforts to read the many emails could reduce [[productivity]].\n\n===Spam===\n{{Main article|Email spam}}\nEmail "spam" is the term used to describe unsolicited bulk email. The low cost of sending such email meant that by 2003 up to 30% of total email traffic was already spam.<ref>[http://visionedgemarketing.com/growth-of-spam-email-2/ "Growth of Spam Email"]</ref><ref name="R">Rich Kawanagh. The top ten email spam list of 2005. ITVibe news, 2006, January 02, [http://itvibe.com/news/3837/ ITvibe.com]</ref><ref>How Microsoft is losing the war on spam [http://dir.salon.com/story/tech/feature/2005/01/19/microsoft_spam/index.html Salon.com]</ref> and was threatening the usefulness of email as a practical tool. The US [[CAN-SPAM Act of 2003]] and similar laws elsewhere<ref>Spam Bill 2003 ([http://www.aph.gov.au/library/pubs/bd/2003-04/04bd045.pdf PDF])</ref> had some impact, and a number of effective [[anti-spam techniques (email)|anti-spam techniques]] now largely mitigate the impact of spam by filtering or rejecting it for most users,<ref>[http://www.wired.com/2015/07/google-says-ai-catches-99-9-percent-gmail-spam/ "Google Says Its AI Catches 99.9 Percent of Gmail Spam"], Cade Metz, July 09 2015, wired.com</ref> but the volume sent is still very high&mdash;and increasingly consists not of advertisements for products, but malicious content or links.<ref name="securelist">[https://securelist.com/analysis/quarterly-spam-reports/74682/spam-and-phishing-in-q1-2016/ "Spam and phishing in Q1 2016"],  May 12, 2016, securelist.com</ref>\n\n===Malware===\nA range of malicious email types exist. These range from [[List of email scams|various types of email scams]], including [[Social engineering (security)|"social engineering"]] scams such as [[advance-fee scam]] "Nigerian letters", to [[phishing]], [[email bomb]]ardment and [[Computer worm|email worms]].\n\n===Email spoofing===\n{{Main article|Email spoofing}}\n[[Email spoofing]] occurs when the email message header is designed to make the message appear to come from a known or trusted source. [[Email spam]] and [[phishing]] methods typically use spoofing to mislead the recipient about the true message origin. Email spoofing may be done as a prank, or as part of a criminal effort to defraud an individual or organization. An example of a potentially fraudulent email spoofing is if an individual creates an email which appears to be an invoice from a major company, and then sends it to one or more recipients. In some cases, these fraudulent emails incorporate the logo of the purported organization and even the email address may appear legitimate.\n\n===Email bombing===\n{{main article|Email bomb}}\n\n[[Email bomb]]ing is the intentional sending of large volumes of messages to a target address. The overloading of the target email address can render it unusable and can even cause the mail server to crash.\n\n===Privacy concerns===\n{{Main article|Internet privacy}}\n\nToday it can be important to distinguish between Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender\'s or the recipient\'s control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although [[information technology]] personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.\n\nEmail privacy, without some security precautions, can be compromised because:\n* email messages are generally not encrypted.\n* email messages have to go through intermediate computers before reaching their destination, meaning it is relatively easy for others to intercept and read messages.\n* many Internet Service Providers (ISP) store copies of email messages on their mail servers before they are delivered. The backups of these can remain for up to several months on their server, despite deletion from the mailbox.\n* the "Received:"-fields and other information in the email can often identify the sender, preventing anonymous communication.\n\nThere are [[cryptography]] applications that can serve as a remedy to one or more of the above. For example, [[Virtual Private Network]]s or the [[Tor (anonymity network)|Tor anonymity network]] can be used to encrypt traffic from the user machine to a safer network while [[GNU Privacy Guard|GPG]], [[Pretty Good Privacy|PGP]], SMEmail,<ref>[http://www.arxiv.org/pdf/1002.3176 SMEmail – A New Protocol for the Secure E-mail in Mobile Environments], Proceedings of the Australian Telecommunications Networks and Applications Conference (ATNAC\'08), pp. 39–44, Adelaide, Australia, Dec. 2008.</ref> or [[S/MIME]] can be used for [[end-to-end principle|end-to-end]] message encryption, and SMTP STARTTLS or SMTP over [[Transport Layer Security]]/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.\n\nAdditionally, many [[mail user agent]]s do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as [[Simple Authentication and Security Layer|SASL]] prevent this. Finally, attached files share many of the same hazards as those found in [[Peer-to-peer|peer-to-peer filesharing]]. Attached files may contain [[Trojan horse (computing)|trojans]] or [[Computer virus|viruses]].\n\n===Flaming===\n[[Flaming (Internet)|Flaming]] occurs when a person sends a message (or many messages) with angry or antagonistic content. The term is derived from the use of the word "incendiary" to describe particularly heated email discussions. The ease and impersonality of email communications mean that the [[social norms]] that encourage civility in person or via telephone do not exist and civility may be forgotten.<ref>{{cite journal|author1=S. Kiesler |author2=D. Zubrow |author3=A.M. Moses |author4=V. Geller |title=Affect in computer-mediated communication: an experiment in synchronous terminal-to-terminal discussion|journal=Human-Computer Interaction|volume=1|pages=77–104|year=1985|doi=10.1207/s15327051hci0101_3}}</ref>\n\n===Email bankruptcy===\n{{main article|Email bankruptcy}}\nAlso known as "email fatigue", email bankruptcy is when a user ignores a large number of email messages after falling behind in reading and answering them. The reason for falling behind is often due to information overload and a general sense there is so much information that it is not possible to read it all. As a solution, people occasionally send a "boilerplate" message explaining that their email inbox is full, and that they are in the process of clearing out all the messages. [[Harvard University]] law professor [[Lawrence Lessig]] is credited with coining this term, but he may only have popularized it.<ref>{{cite news|title=All We Are Saying.|url=http://www.nytimes.com/2007/12/23/weekinreview/23buzzwords.html?ref=weekinreview|publisher=The New York Times|date=December 23, 2007|accessdate=2007-12-24|first=Grant|last=Barrett}}</ref>\n\n===Tracking of sent mail===\nThe original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the [[Internet Engineering Task Force|IETF]] introduced [[Delivery Status Notification]]s (delivery receipts) and [[Return receipt#Email|Message Disposition Notifications]] (return receipts); however, these are not universally deployed in production. (A complete Message Tracking mechanism was also defined, but it never gained traction; see RFCs 3885<ref>RFC 3885, \'\'SMTP Service Extension for Message Tracking\'\'</ref> through 3888.<ref>RFC 3888, \'\'Message Tracking Model and Requirements\'\'</ref>)\n\nMany ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:\n* Delivery Reports can be used to verify whether an address exists and if so, this indicates to a spammer that it is available to be spammed.\n* If the spammer uses a forged sender email address ([[email spoofing]]), then the innocent email address that was used can be flooded with NDRs from the many invalid email addresses the spammer may have attempted to mail. These NDRs then constitute spam from the ISP to the innocent user.\n\nIn the absence of standard methods, a range of system based around the use of [[web bug]]s have been developed. However, these are often seen as underhand or raising privacy concerns,<ref>{{cite news|url=http://query.nytimes.com/gst/fullpage.html?res=940CE0D9143AF931A15752C1A9669C8B63&sec=&spon=&pagewanted=print|title=Software That Tracks E-Mail Is Raising Privacy Concerns|author=Amy Harmon|publisher=The New York Times|date=2000-11-22|accessdate=2012-01-13}}</ref><ref>{{cite web|url=http://email.about.com/od/emailbehindthescenes/a/html_return_rcp.htm |title=About.com |publisher=Email.about.com |date=2013-12-19 |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://www.webdevelopersnotes.com/tips/yahoo/notification-when-yahoo-email-is-opened.php |title=Webdevelopersnotes.com |publisher=Webdevelopersnotes.com |accessdate=2014-01-09}}</ref> and only work with email clients that support rendering of HTML. Many mail clients now default to not showing "web content".<ref>[http://www.slipstick.com/outlook/email/microsoft-outlook-web-bugs-blocked-html-images "Outlook: Web Bugs & Blocked HTML Images"], slipstick.com</ref> [[Webmail]] providers can also disrupt web bugs by pre-caching images.<ref>[http://arstechnica.com/information-technology/2013/12/gmail-blows-up-e-mail-marketing-by-caching-all-images-on-google-servers/ "Gmail blows up e-mail marketing..."], Ron Amadeo, Dec 13 2013, Ars Technica</ref>\n\n==U.S. government==\nThe U.S. state and federal governments have been involved in electronic messaging and the development of email in several different ways. Starting in 1977, the U.S. Postal Service (USPS) recognized that electronic messaging and electronic transactions posed a significant threat to First Class mail volumes and revenue. The USPS explored an electronic messaging initiative in 1977 and later disbanded it. Twenty years later, in 1997, when email volume overtook postal mail volume, the USPS was again urged to embrace email, and the USPS declined to provide email as a service.<ref>{{cite web|url=http://www.fastcompany.com/1780716/can-technology-save-us-postal-service|title=Can Technology Save The U.S. Postal Service?|work=Fast Company}}</ref><ref>{{cite web|url=http://tech.mit.edu/V131/N60/emaillab.html|title=Can an MIT professor save the USPS? - The Tech|work=mit.edu}}</ref><ref>{{cite web|url=http://www.fedtechmagazine.com/article/2013/01/why-united-states-postal-service-taking-cues-silicon-valley|title=Why the USPS Is Taking Cues from Silicon Valley|work=FedTech}}</ref> The USPS initiated an experimental email service known as [[E-COM]]. E-COM provided a method for the simple exchange of text messages. In 2011, shortly after the USPS reported its state of financial bankruptcy, the USPS Office of Inspector General (OIG) began exploring the possibilities of generating revenue through email servicing.<ref>{{cite web|url=http://cmsw.mit.edu/usps-can-save-itself/|title=Shiva Ayyadurai: USPS can save itself|work=MIT Comparative Media Studies/Writing}}</ref><ref>{{cite web|url=http://bostinno.streetwise.co/2012/01/13/could-email-save-snail-mail-or-is-the-internet-too-reliant-on-the-usps/|title=Could Email Save Snail Mail, Or Is The Internet Too Reliant on the USPS?|date=6 March 2012|work=BostInno}}</ref><ref>{{cite web|url=http://www.bostonglobe.com/lifestyle/2012/03/02/dear-usps/V4GJ8w9UCcfV4v0WiVjvmK/story.html|title=‘Dear USPS . . .’|work=BostonGlobe.com}}</ref> Electronic messages were transmitted to a post office, printed out, and delivered as hard copy. To take advantage of the service, an individual had to transmit at least 200 messages. The delivery time of the messages was the same as First Class mail and cost 26 cents. Both the [[Postal Regulatory Commission]] and the [[Federal Communications Commission]] opposed E-COM. The FCC concluded that E-COM constituted common carriage under its jurisdiction and the USPS would have to file a [[tariff]].<ref>In re Request for declaratory ruling and investigation by Graphnet Systems, Inc., concerning the proposed E-COM service, FCC Docket No. 79-6 (September 4, 1979)</ref> Three years after initiating the service, USPS canceled E-COM and attempted to sell it off.<ref>Hardy, Ian R; [https://archive.org/web/*/http:/www.ifla.org/documents/internet/hari1.txt The Evolution of ARPANET Email]; 1996-05-13; History Thesis Paper; University of California at Berkeley</ref><ref>James Bovard, The Law Dinosaur: The US Postal Service, CATO Policy Analysis (February 1985)</ref><ref name="JayAkkad">{{cite web|url=http://www.cs.ucsb.edu/~almeroth/classes/F04.176A/homework1_good_papers/jay-akkad.html |title=Jay Akkad, The History of Email |publisher=Cs.ucsb.edu |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://www.gao.gov/archive/2000/gg00188.pdf |title=US Postal Service: Postal Activities and Laws Related to Electronic Commerce, GAO-00-188 |format=PDF |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://govinfo.library.unt.edu/ota/Ota_4/DATA/1982/8214.PDF |title=Implications of Electronic Mail and Message Systems for the U.S. Postal Service , Office of Technology Assessment, Congress of the United States, August 1982 |format=PDF |accessdate=2014-01-09}}</ref>\n\nThe early ARPANET dealt with multiple email clients that had various, and at times incompatible, formats. For example, in the [[Multics]], the "@" sign meant "kill line" and anything before the "@" sign was ignored, so Multics users had to use a command-line option to specify the destination system.<ref name="thvv"/> The [[United States Department of Defense|Department of Defense]] [[DARPA]] desired to have uniformity and interoperability for email and therefore funded efforts to drive towards unified inter-operable standards. This led to David Crocker, John Vittal, Kenneth Pogran, and [[Austin Henderson]] publishing RFC 733, "Standard for the Format of ARPA Network Text Message" (November 21, 1977), a subset of which provided a stable base for common use on the ARPANET, but which was not fully effective, and in 1979, a meeting was held at BBN to resolve incompatibility issues. [[Jon Postel]] recounted the meeting in RFC 808, "Summary of Computer Mail Services Meeting Held at BBN on 10 January 1979" (March 1, 1982), which includes an appendix listing the varying email systems at the time. This, in turn, led to the release of David Crocker\'s RFC 822, "Standard for the Format of ARPA Internet Text Messages" (August 13, 1982).<ref>{{cite web|url=http://www.livinginternet.com/e/ei.htm |title=Email History, How Email was Invented, Living Internet |publisher=Livinginternet.com |date=1996-05-13 |accessdate=2014-01-09}}</ref> RFC 822 is a small adaptation of RFC 733\'s details, notably enhancing the [[host (network)|host]] portion, to use [[Domain Name]]s, that were being developed at the same time.\n\nThe [[National Science Foundation]] took over operations of the ARPANET and Internet from the Department of Defense, and initiated [[NSFNet]], a new [[backbone network|backbone]] for the network. A part of the NSFNet AUP forbade commercial traffic.<ref>{{cite web|author=Robert Cannon |url=http://www.cybertelecom.org/notes/internet_history80s.htm |title=Internet History |publisher=Cybertelecom |accessdate=2014-01-09}}</ref> In 1988, [[Vint Cerf]] arranged for an interconnection of [[MCI Mail]] with NSFNET on an experimental basis. The following year [[Compuserve]] email interconnected with NSFNET. Within a few years the commercial traffic restriction was removed from NSFNETs AUP, and NSFNET was privatised. In the late 1990s, the [[Federal Trade Commission]] grew concerned with fraud transpiring in email, and initiated a series of procedures on spam, fraud, and phishing.<ref>[http://www.cybertelecom.org/spam/Spamref.htm Cybertelecom : SPAM Reference] {{webarchive |url=https://web.archive.org/web/20140919090804/http://www.cybertelecom.org/spam/Spamref.htm |date=September 19, 2014 }}</ref> In 2004, FTC jurisdiction over spam was codified into law in the form of the [[Can Spam Act|CAN SPAM Act.]]<ref>{{cite web|author=Robert Cannon |url=http://www.cybertelecom.org/spam/canspam.htm |title=Can Spam Act |publisher=Cybertelecom |accessdate=2014-01-09}}</ref> Several other U.S. federal agencies have also exercised jurisdiction including the [[United States Department of Justice|Department of Justice]] and the [[United States Secret Service|Secret Service]]. NASA has provided email capabilities to astronauts aboard the Space Shuttle and International Space Station since 1991 when a [[Macintosh Portable]] was used aboard [[Space Shuttle]] mission [[STS-43]] to send the first email via [[AppleLink]].<ref>{{cite web|last=Cowing |first=Keith |url=http://www.spaceref.com/news/viewnews.html?id=213 |title=2001: A Space Laptop &#124; SpaceRef – Your Space Reference |publisher=Spaceref.com |date=2000-09-18 |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://www.macobserver.com/columns/thisweek/2004/20040831.shtml |title=The Mac Observer – This Week in Apple History – August 22–31: "Welcome, IBM. Seriously," Too Late to License |publisher=Macobserver.com |date=2004-10-31 |accessdate=2014-01-09}}</ref><ref>{{cite book|last=Linzmayer|first=Owen W.|title=Apple confidential 2.0 : the definitive history of the world\'s most colorful company|year=2004|publisher=No Starch Press|location=San Francisco, Calif.|isbn=1-59327-010-0|edition=[Rev. 2. ed.].}}</ref> Today astronauts aboard the International Space Station have email capabilities via the [[WI-FI|wireless networking]] throughout the station and are connected to the ground at 10 [[Mbit/s]] Earth to station and 3 Mbit/s station to Earth, comparable to home [[DSL]] connection speeds.<ref name=issit>{{cite news|title=First Tweet from Space|url=http://bits.blogs.nytimes.com/2010/01/22/first-tweet-from-space/|newspaper=The New York Times|first=Nick|last=Bilton|date=January 22, 2010}}</ref>\n\n==See also==\n{{colbegin||22em}}\n* [[Anonymous remailer]]\n* [[Anti-spam techniques]]\n* [[biff]]\n* [[Bounce message]]\n* [[Comparison of email clients]]\n* [[Dark Mail Alliance]]\n* [[Disposable email address]]\n* [[E-card]]\n* [[Electronic mailing list]]\n* [[Email art]]\n* [[Email authentication]]\n* [[Email digest]]\n* [[Email encryption]]\n* [[Email hosting service]]\n* [[Email storm]]\n* [[Email tracking]]\n* [[HTML email]]\n* [[Information overload]]\n* [[Internet fax]]\n* [[Internet mail standard]]s\n* [[List of email subject abbreviations]]\n* [[MCI Mail]]\n* [[Netiquette]]\n* [[Posting style]]\n* [[Privacy-enhanced Electronic Mail]]\n* [[Push email]]\n* [[RSS]]\n* [[Telegraphy]]\n* [[Unicode and email]]\n* [[Usenet quoting]]\n* [[Webmail]], [[Comparison of webmail providers]]\n* [[X-Originating-IP]]\n* [[X.400]]\n* [[Yerkish]]\n{{colend}}\n\n==References==\n{{Reflist|2}}\n\n==Further reading==\n{{refbegin}}\n* Cemil Betanov, \'\'Introduction to X.400\'\', Artech House, ISBN 0-89006-597-7.\n* Marsha Egan, "[http://www.inboxdetox.com Inbox Detox and The Habit of Email Excellence]", Acanthus Publishing ISBN 978-0-9815589-8-1\n* Lawrence Hughes, \'\'Internet e-mail Protocols, Standards and Implementation\'\', Artech House Publishers, ISBN 0-89006-939-5.\n* Kevin Johnson, \'\'Internet Email Protocols: A Developer\'s Guide\'\', Addison-Wesley Professional, ISBN 0-201-43288-9.\n* Pete Loshin, \'\'Essential Email Standards: RFCs and Protocols Made Practical\'\', John Wiley & Sons, ISBN 0-471-34597-0.\n* {{Cite journal|last=Partridge|first=Craig|title=The Technical Development of Internet Email|journal=IEEE Annals of the History of Computing|volume=30|issue=2|publisher=IEEE Computer Society|location=Berlin|date=April–June 2008|url=http://www.ir.bbn.com/~craig/papers/email.pdf|format=PDF|issn=1934-1547|ref=harv|postscript={{inconsistent citations}} | doi =  10.1109/mahc.2008.32|pages=3–29}}\n* Sara Radicati, \'\'Electronic Mail: An Introduction to the X.400 Message Handling Standards\'\', Mcgraw-Hill, ISBN 0-07-051104-7.\n* John Rhoton, \'\'Programmer\'s Guide to Internet Mail: SMTP, POP, IMAP, and LDAP\'\', Elsevier, ISBN 1-55558-212-5.\n* John Rhoton, \'\'X.400 and SMTP: Battle of the E-mail Protocols\'\', Elsevier, ISBN 1-55558-165-X.\n* David Wood, \'\'Programming Internet Mail\'\', O\'Reilly, ISBN 1-56592-479-7.\n{{refend}}\n\n==External links==\n{{Wiktionary|email|outbox}}\n* [http://www.iana.org/assignments/message-headers/perm-headers.html IANA\'s list of standard header fields]\n* [http://emailhistory.org/ The History of Email] is Dave Crocker\'s attempt at capturing the sequence of \'significant\' occurrences in the evolution of email; a collaborative effort that also cites this page.\n* [http://www.multicians.org/thvv/mail-history.html The History of Electronic Mail] is a personal memoir by the implementer of an early email system\n* [http://www.circleid.com/posts/20140903_a_look_at_the_origins_of_network_email/ A Look at the Origins of Network Email] is a short, yet vivid recap of the key historical facts\n* [https://www.fbi.gov/news/stories/2015/august/business-e-mail-compromise Business E-Mail Compromise - An Emerging Global Threat], [[FBI]]\n<!-- please see http://en.wikipedia.org/wiki/WP:EL before adding links -->\n{{Computer-mediated communication}}\n{{E-mail clients}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Email}}\n[[Category:Email| ]]\n[[Category:Internet terminology]]\n[[Category:Electronic documents]]\n[[Category:History of the Internet]]\n[[Category:1971 introductions]]']
['Category:Legal citators', '24447352', '{{Cat main|Citator}}\n\n[[Category:Citation indices]]\n[[Category:Legal research]]\n[[Category:Legal citation]]']
['Materials Science Citation Index', '27789063', '{{Third-party|date=February 2013}}\n\'\'\'The Materials Science Citation Index\'\'\' is a [[citation index]], established in 1992, by [[Thomson ISI]] ([[Thomson Reuters]]). Its overall focus is [[citation|cited reference]] searching of the notable and significant [[science journal|journal literature]] in [[materials science]]. The database makes accessible the various [[physical properties|properties]], behaviors, and materials in the materials science discipline. This then encompasses [[applied physics]], [[ceramic engineering|ceramics]], [[Advanced composite materials (science & engineering)|composite materials]], [[metals]] and [[metallurgy]], [[polymer engineering]], [[semiconductors]], [[thin films]], [[biomaterial]]s, [[Dentistry|dental technology]], as well as [[optics]]. The [[database]] indexes relevant materials science information from over 6,000 [[scientific journal]]s that are part of the ISI database which is [[multidisciplinary]]. Author abstracts are searchable, which links articles sharing one or more [[bibliographic]] references. The database also allows a researcher to use an appropriate (or related to research) article as a base to search forward in time to discover more recently published articles that cite it.<ref name=msci-est>Pemberton, Julia K. "\'\'Two new databases from ISI\'\'." CD-ROM Professional 5.4 (1992): 107+. General OneFile. Web. 20 June 2010.</ref>\n\n\'\'Materials Science Citation Index\'\' lists 625 high impact journals, and is accessible via the [[Science Citation Index Expanded]] collection of databases.<ref name=msci-jnlList>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MS Materials Science Citation Index journal list]. Thomson Reuters. July 2010.</ref>\n\n==Editions==\nCoverage of Materials science is accomplished with the following editions:<ref name=MS-indexes>[http://science.thomsonreuters.com/mjl/scope/scope_scie/ Scope Notes]. Science Citation Index, Science Citation Index Expanded. Thomson Reuters. 2010.</ref><ref>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=D Subject categories]. Science Citation Index Expanded. Thomson Reuters. 2010</ref>\n*Materials Science, Ceramics\n*Materials Science, Characterization & Testing\n*Materials Science, Biomaterials\n*Materials Science, Coatings & Films\n*Materials Science, Composites\n*Materials Science, Paper & Wood\n*Materials Science, Multidisciplinary\n*Materials Science, Textiles\n\n==See also==\n* [[Science Citation Index]]\n* [[Academic publishing]]\n* [[List of academic databases and search engines]]\n* [[Social Sciences Citation Index]], which covers over 1500 journals, beginning with 1956\n* [[Arts and Humanities Citation Index]], which covers over 1000 journals, beginning with 1975\n* [[Impact factor]]\n* [[VINITI Database RAS]]\n\n==References==\n{{Reflist}}\n\n{{Thomson Reuters}}\n\n[[Category:Thomson Reuters]]\n[[Category:Bibliographic databases and indexes]]\n[[Category:Online databases]]\n[[Category:Citation indices]]\n[[Category:Materials science journals| ]]\n\n\n{{science-journal-stub}}']
['Citation index', '423362', '{{distinguish|Citation metric}}\n\nA \'\'\'citation index\'\'\' is a kind of [[bibliographic index]], an index of [[citation]]s between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by [[citator]]s such as [[Shepard\'s Citations]] (1873). In 1960, [[Eugene Garfield]]\'s [[Institute for Scientific Information]] (ISI) introduced the first citation index for papers published in [[academic journal]]s, first the \'\'[[Science Citation Index]]\'\' (SCI), and later the \'\'[[Social Sciences Citation Index]]\'\' (SSCI) and the \'\'[[Arts and Humanities Citation Index]]\'\' (AHCI). The first automated citation indexing was done by [[CiteSeer]] in 1997. Other sources for such data include [[Google Scholar]] and Elsevier\'s [[Scopus]].\n\n==History==\nThe earliest known citation index is an index of biblical citations in [[rabbinic literature]], the \'\'Mafteah ha-Derashot\'\', attributed to [[Maimonides]] and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study. The Talmudic citation index \'\'En Mishpat\'\' (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century \'\'Shepard\'s Citations\'\'.<ref>Bella Hass Weinberg, "The Earliest Hebrew Citation Indexes" in Trudi Bellardo Hahn, Michael Keeble Buckland, eds., \'\'Historical Studies in Information Science\'\', 1998, p. 51\'\'ff\'\'</ref><ref>Bella Hass Weinberg, "Predecessors of Scientific Indexing Structures in the Domain of Religion" in W. Boyden Rayward, Mary Ellen Bowden, \'\'The History and Heritage of Scientific and Technological Information Systems\'\', Proceedings of the 2002 Conference, 2004, p. 126\'\'ff\'\'</ref> Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed.\n\nIn English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with \'\'Raymond\'s Reports\'\' (1743) and followed by \'\'Douglas\'s Reports\'\' (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision.<ref name=\'shapiro\'/>\n\nThe first true citation index dates to the 1860 publication of Labatt\'s \'\'Table of Cases...California...\'\', followed in 1872 by Wait\'s \'\'Table of Cases...New York...\'\'. But the most important and best-known citation index came with the 1873 publication of [[Shepard\'s Citations]].<ref name=\'shapiro\'>Fred R. Shapiro, "Origins of Bibliometrics, Citation Indexing, and Citation Analysis: The Neglected Legal Literature" \'\'Journal of the American Society of Information Science\'\' \'\'\'43\'\'\':5:337-339 (1992)</ref>\n\n==Major citation indexing services==\n{{main article|Indexing and abstracting service}}\n{{main cat|Citation indices}}\nGeneral-purpose academic citation indexes include:\n*[[Web of Science]] by [[Clarivate Analytics]] (previously the Intellectual Property and Science business of [[Thomson Reuters]])\n*[[Scopus]] by [[Elsevier]], available online only, which similarly combines subject searching with citation browsing and tracking in the sciences and [[social sciences]].\n*[[Indian Citation Index (ICI)|Indian Citation Index]] is an online citation data which covers [[peer review]]ed journals published from India. It covers major subject areas such as scientific, technical, medical, and [[social sciences]] and includes arts and humanities. The citation database is the first of its kind in India.\nEach of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They differ widely in cost: Web of Science and Scopus are available by subscription (generally to libraries).\n\nIn addition, [[CiteSeer]] and [[Google Scholar]] are freely available online.\n\n==See also==\n* [[Microsoft Academic Search]]\n* [[Google Scholar]]\n* [[Scopus]]\n* [[Semantic Scholar]]\n* [[Citation analysis]]\n* [[Acknowledgment index]]\n* [[CiteSeer]]\n* [[CiteSeerX]]\n* [[Scientific journal]]\n* [[Science Citation Index]]\n* [[Indian Citation Index]]\n* [[Journal Citation Reports]]\n* [[Emerging Sources Citation Index (ESCI)]]\n* [[SciELO]]\n* [[Redalyc]]\n* [[Index Copernicus]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Citation Index}}\n[[Category:Academic publishing]]\n[[Category:Bibliometrics]]\n[[Category:Bibliographic databases and indexes]]\n[[Category:Reputation management]]\n[[Category:Citation indices| ]]']
["Kelly's Directory", '3119155', '\'\'\'Kelly\'s Directory\'\'\' (or more formally, the \'\'\'Kelly\'s, Post Office and Harrod & Co Directory\'\'\') was a [[trade directory]] in the United Kingdom that listed all businesses and tradespeople in a particular city or town, as well as a general directory of postal addresses of local [[gentry]], landowners, charities, and other facilities.  In effect, it was a Victorian version of today\'s [[Yellow Pages]].<ref>{{cite web|url=http://www.cottinghamhistory.co.uk/Directories.htm|title=Cottingham History|accessdate=11 July 2010}}</ref>  Many reference libraries still keep their copies of these directories, which are now an important source for historical research.\n\n==Origins==\nThe eponymous originator of the directory was [[Frederic Festus Kelly]].  In 1835 or 1836 he became chief inspector of letter-carriers for the inland or general post office, and took over publication of the Post Office London Directory, whose copyright was in private hands despite its semi-official association with the post office, and which Kelly had to purchase from the widow of his predecessor.\n\nHe founded Kelly & Co. and he and various family members gradually expanded the company over the next several decades, producing directories for an increasing number of UK [[county|counties]] and buying out or putting out of business various competing publishers of directories.<ref name="pollard"/><ref>http://www.huthwaite-online.net/hucknall/gazetteers/</ref>\n\nOther publications followed, including the \'\'Handbook to the Titled, Landed and Official Classes\'\' (1875) and \'\'Merchants, Manufacturers and Shippers\'\' (1877). In 1897, Kelly & Co Ltd became \'\'\'Kelly’s Directories Ltd.\'\'\'<ref name="lg"/>  This name stuck for another 106 years before being renamed Kellysearch in 2003 to reflect its focus away from hard copy directories and towards an Internet-based product search engine.\n\nThe front cover of a Kelly\'s Directory sometimes stated "Kelly\'s Directories Ltd., established 1799",<ref>{{cite web|title=Trade Directories|work=Stella & Rose\'s Books|url=http://www.stellabooks.com/articles/trade_directories.php|accessdate=2011-03-28}}</ref> however this was based on the date of issue of the first Post Office London Directory by an earlier inspector of letter carriers several decades before Kelly\'s involvement with that publication.<ref name="jenorton" />\n\n== Kellysearch ==\nFor a short time, Kelly\'s existed online as [http://www.kellysearch.co.uk/ Kellysearch (broken link)], a directory similar to the online [[Yellow Pages]]. Kellysearch.com was established in Boston in 2004. It was in many different languages and introduced a fully searchable online-catalogue library and product [[News release|press release]] section.\n\nThe old editions of the Kelly’s Directories are seen as highly collectable by many and have also become a useful reference tool for people tracing the history of local areas (with the ancient data now available to buy on CD Rom from many entrepreneurial sources for this purpose.)  Every edition of the Kelly’s Directory ever published is held in the [[Guildhall Library]]<ref>[http://www.cityoflondon.gov.uk/things-to-do/archives-and-city-history/guildhall-library/Documents/8-trade-directories-at-guildhall-library.pdf Trade directories and telephone books at Guildhall Library]</ref> in [[London]].\n\n==References==\n{{reflist | refs=\n\n<ref name="jenorton">{{cite journal | doi=10.1093/library/s5-XXI.4.293 | title=The Post Office London Directory | author=Jane Elizabeth Norton | journal=The Library (The Transactions of the [[Bibliographical Society]]) | series=5th series | volume=21 | issue=4 | year=1966 | pages=293–299 | quote=The Post Office London Directory was started by two inspectors of the Inland letter-carriers called Ferguson and Sparkes… A third inspector, called B. Critchett, joined the enterprise in 1803 and later it was carried on by Critchett alone, then by Critchett and Woods, and then again by Critchett alone until his death in 1835. \'\'[sic; he died 18 September 1836]\'\'}}<!-- Library (1966) s5-XXI(4): 293-299 --></ref>\n\n<ref name="pollard">{{cite book | title=The Earliest Directory of the Book Trade | first=John | last=Pendred | editor-first=Graham | editor-last=Pollard | chapter=Appendix H: General Directories | pages=83–84 | isbn=0-19-721759-1 | year=1955 | edition=reprint of 1785 | quote=The first directories of counties outside London were published by Kelly in 1845; and during the next sixteen years the series was extended throughout England. In 1892 Kelly\'s Directories Ltd. acquired the majority of shares in [[Isaac Slater]] Ltd. [...]; and the firm of [[White\'s Directories|William White]] of Sheffield [...] was absorbed in 1898.}}</ref>\n\n<ref name="lg">{{LondonGazette|issue=26876|date=23 July 1897|startpage=4149}}</ref>\n\n}}\n\n==Bibliography==\n*{{cite book| title=Guide to the national and provincial directories of England and Wales, excluding London, published before 1856 | author=Jane Elizabeth Norton | year=1950 | edition=1984 reprint | publisher=Offices of the Royal Historical Society | isbn=0-86193-102-5}} (original edition: ISBN 0-901050-15-6)\n*{{cite book| title=The development and growth of city directories| author=A. V. Williams| year=1913 | publisher=Williams directory co.}}\n*{{cite book| title=The Earliest Directory of the Book Trade | first=John | last=Pendred | editor-first=Graham | editor-last=Pollard | chapter=Appendix H: General Directories | pages=83–84 | isbn=0-19-721759-1 | year=1955 | edition=reprint of 1785}}\n*{{cite book| title=The directories of London, 1677-1977| author=Peter J. Atkins | publisher=Cassell and Mansell | year=1990\n| isbn=0-7201-2063-2}}\n\n==External links==\n{{Commons category|Kelly\'s Directory}}\n* [http://specialcollections.le.ac.uk/cdm/landingpage/collection/p16445coll4/hd/ Historical Directories] has extensive online versions of old editions for England and Wales\n* [http://forebears.co.uk/news/kellys-directories-project-complete#kellys Forebears] has transcriptions of one edition for each county\n* {{cite journal | doi=10.1093/library/s5-XXI.4.293 | title=The Post Office London Directory | last=Norton | first=Jane Elizabeth | journal=The Library (The Transactions of the [[Bibliographical Society]]) | series=5th series | volume=21 | issue=4 | year=1966 | pages=293–299}}\n* {{cite journal | journal=The London Journal | title=The Compilation and Reliability of London Directories | last=Atkins | first=Peter J. | volume=14 | issue=1 |date=May 1989 | pages=17–28 | publisher=Maney Publishing | issn=0305-8034 | url=http://www.ingentaconnect.com/content/maney/ldn/1989/00000014/00000001/art00002 | doi=10.1179/ldn.1989.14.1.17}}\n\n{{Reed Elsevier}}\n\n[[Category:Directories]]\n[[Category:Waltham, Massachusetts]]']
['Subcontractors Register', '6401934', '[[Image:Subcontractors Register.jpg|thumb|1937 edition of the \'\'Subcontractors Register\'\']] \n\nThe \'\'\'Subcontractors Register for the Allied Building Trades\'\'\' was a directory of [[subcontractors]] for the [[New York City]] area, listing companies by their trade. It was published by the "Society of the Allied Building Trades, Inc." and was published by Joseph O\'Malley (1893–1985) who was later joined by his nephew, [[Walter Francis O\'Malley]], as editor. The 1942 version calls itself: "A Classified List for the Allied Building Trades of Sub-Contractors, Material Dealers & Manufacturers, General Contractors & Builders, Architects - Engineers, Real Estate Management Firms".\n\n==External links==\n*[http://www.findagrave.com/cgi-bin/fg.cgi?page=gr&GSln=o\'malley&GSmid=46580804&GRid=7768640& Findagrave: Joseph O\'Malley]\n\n[[Category:O\'Malley family]]\n[[Category:Directories]]\n[[Category:History of New York City]]\n\n\n{{US-stub}}']
['Vsya Rossiya', '11017381', '{{italic title}}\n\'\'\'\'\'Vsya Rossiya\'\'\'\'\' (literally translated "\'\'All Russia\'\'" or "\'\'The whole Russia\'\'") was the title of a series of directories of the [[Russian Empire]] published by [[Aleksei Sergeevich Suvorin]] on a yearly basis from 1895 to 1923 and was continued under the name \'\'\'\'\'Ves SSSR\'\'\'\'\' (Literally translated \'\'All of the USSR\'\' or \'\'The whole USSR\'\') from 1924 to 1931. Each volume was anywhere between 500 and 1500 pages long. The directories contained detailed lists of government offices, public services and medium and large businesses present in major cities across Russia including [[Kiev]], [[Minsk]], . These directories are often used by [[genealogists]] today to trace family members who were living in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historians]] use them to research the [[social histories]] of late 19th century and early 20th century Russia.\n\n==Contents==\n\nThe following information can be found in most editions:\n*a surname index of over 100,000 names and thousands of companies\n*a directory of prominent landowners\n*Lists members of the Imperial House of Russia and government officials \n*statistical information about the Russian Empire\n*Population figures\n*information and guidelines about trade and industry in Russia\n*Lists of joint-stock companies\n*Sub-sections detailing a directory of each district of each province, listing administrative officials, merchants, industrial and commercial manufacturers\n*Original advertising\n\n== Availability ==\n\nMany original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the U.S., Europe (including the [[Baltic countries]], Finland the United Kingdom and Germany) however most only have an incomplete collection.\n\n==Other city directories in Russia ==\nSuvorin also published city directories for [[Saint Petersburg]] under the title \'\'[[Ves Petersburg]]\'\' (Literally translated \'\'All Petersburg\'\' or \'\'The Whole Saint Petersburg\'\') for the years 1894 to 1940 and for [[Moscow]] under the title \'\'[[Vsia Moskva]]\'\' (Literally translated \'\'All Moscow\'\' or \'\'The Whole Moscow\'\') for the years 1875 to 1936.\n\n== External links ==\n\n*[http://surname.litera-ru.ru/ A Russian website offering a search engine in Cyrillic for some city directories.]\n\n[[Category:Directories]]\n[[Category:Russian non-fiction books]]\n[[Category:Russian Empire]]\n[[Category:1895 books]]']
["Crockford's Clerical Directory", '9731047', '{{italictitle}}\'\'\'\'\'Crockford\'s Clerical Directory\'\'\'\'\' (\'\'\'\'\'Crockford\'\'\'\'\') is the authoritative directory of the [[Anglican Communion]] in the United Kingdom, containing details of English, Welsh and Irish benefices and churches, and biographies of around 26,000 clergy. It was first issued in 1858 by [[John Crockford]],{{sfn|Hough & Matthew|2004}} a London printer and publisher whose father – also named John – had been a Somerset schoolmaster.\n\n\'\'Crockford\'\' is currently compiled and published for the [[Archbishops\' Council]] by Church House Publishing.<ref>{{Cite web\n| title = Crockford\'s Clerical Directory\n| author =\n| work = The Church of England\n| date =\n| accessdate = 2014-08-12\n| url = https://www.churchofengland.org/clergy-office-holders/crockford.aspx\n| ref={{sfnref|CofE}}\n}}</ref>  It covers in detail the whole of the [[Church of England]] (including the [[Diocese in Europe]]), the [[Church in Wales]], the [[Scottish Episcopal Church]], and the [[Church of Ireland]], and it also gives some information – now more limited – about the world-wide [[Anglican Communion]].\n\n== Previous publishers ==\n\n[[File:Crockford1868-titlepage.jpg|thumb|\'\'Crockford\'s Clerical Directory 1868\'\', published by Horace Cox, London]]The actual title of the first edition was simply \'\'The Clerical Directory\'\', but a footnote showed that it was published by John Crockford, 29 Essex Street, [[Strand, London|the Strand]].  The original publisher died suddenly in 1865, shortly before the appearance of the third edition of what had by then become \'\'Crockford’s Clerical Directory\'\'.  For many subsequent issues the volumes were anonymously edited, but they were published under the imprint of Horace Cox – the nephew of John Crockford’s closest business associate, solicitor and publisher [[Edward William Cox]] (1809Ω–1879).  (His family was probably quite unrelated to the Charles Cox who coincidentally was the publisher of \'\'Crockford\'\'{{\'}}s chief rival, the \'\'[[Clergy List]]\'\'.{{efn|A two-part article "Shop-talk and mordant wit" by Christopher Currie & Glyn Paflin describes the background to the directory\'s first hundred editions, {{sfn|Currie & Paflin|7 December 2007}} }}) Horace Cox died in 1918{{efn|Horace Cox’s very brief obituary in \'\'The Times\'\', 11 October 1918; p. 5, states that he had retired in 1912 and had ceased to take an active part in his business, which also produced \'\'The Field\'\', \'\'The Queen\'\' and \'\'The Law Times\'\'}} and the title was subsequently sold in 1921 to the [[Oxford University Press]],{{sfn|Currie & Paflin|7 December 2007}} who continued as publishers right up until the early 1980s.  For the 1985/86 issue publication was transferred to the [[Church Commissioners]] and their Central Board of Finance (who worked from their own administrative lists and databases).  It is now collated by Church House Publishing.\n\n== Frequency of publication ==\n\nThe first four issues came  out in 1858, 1860 (with a supplement in 1861),{{efn|The 1861 supplement, experimentally issued when a switch to biennial publication was being contemplated, may be downloaded free of charge from Google Play}} 1865 {{efn|The 1865 edition was reprinted in a 1995 facsimile limited edition of 100 copies by Peter Bell (bookseller), Edinburgh.{{sfn|Bell|1995|p=}} It can also now be downloaded free of charge from Google play}} and 1868.  \'\'Crockford\'\' then reappeared biennially until 1876, when it began a long run of annual appearances which lasted until 1917. The next issue was a delayed 1918/19 edition, which had for the first time incorporated its main rival publication, the \'\'[[Clergy List]]\'\'. Further issues appeared for 1920 and 1921/22; then between 1923 and 1927{{efn|There was no issue in 1928, for what the editor called "technical reasons". Production difficulties in 1941/42, 1943 and 1944 meant that it was only possible to issue short supplements to the 1941 edition. \'\'Crockford Prefaces: The Editor Looks Back\'\' (Oxford, 1947), pp. i, 257, 272, 283.}} and 1929–1940 the directory reappeared annually, followed by more late issues in 1941 and 1947/48. Since that time \'\'Crockford\'\' has generally appeared every two years, although gradually worsening delays meant that the 87th and 88th editions were dated 1977/79 and 1980/82, and the book failed to appear at all during 1983/84. Biennial publication was once again resumed in 1985/86, although the volume issued late in 1997 was designated the 1998/99 edition. The 100th edition – eventually published for 2008/09 – included within its hardback version a few facsimile pages from the first edition, together with an extended historical note describing some of the earlier volumes.\n\nThe 1858 edition was later described as seemingly “assembled in a very haphazard fashion, with names added ‘as fast as they could be obtained’, out of alphabetical order and with an unreliable index”. But nevertheless the 1860 directory “had become a very much more useful work of reference”.{{efn|Quoted by Brenda Hough in her biographical note on John Crockford, published in the 1998/99 \'\'Crockford\'\' and reprinted (with minor modifications) in all subsequent editions; also on the official Crockford\'s website.<ref>{{Cite web\n| title = About John Crockford\n| author = Brenda Hough\n| work = Crockford\'s clerical Directory - online\n| date =\n| accessdate = 2014-08-12\n| url = http://www.crockford.org.uk/standard.asp?id=126\n| quote =\n}}</ref>}}  However the original volume was actually a consolidation of what in 1857 had been conceived as a mere series of supplements to an entirely different publication, the \'\'Clerical Journal\'\'.{{efn|The \'\'Oxford Dictionary of National Biography\'\' article on Edward William Cox states that he, together with John Crockford, had founded the \'\'Clerical Journal\'\' in 1853.}}  The editors explained in the preface that they  wished it to be understood that it was “but the foundation of a great work which, with the Cordial aid of the clergy, we shall hope to make more and more perfect every year”.\n\n== Scope of the directory ==\n\n[[File:Crockford1910BpLichfield.jpg|thumb|\'\'Crockford\'\', 1910: a biographical page in an older edition would typically include many abbreviations, including clergy academic backgrounds, and their dates ordained deacon [d] and priest [p] (the presiding bishop being indicated). Diocesan coats of arms were shown alongside episcopal entries; any publications were listed, and parish incomes and patrons were mentioned. Many overseas clergy would be covered.]] The 1858 issue was based on postal returns from the clergy in England and Wales, involving an outlay – as the preface pointed out – of "more than Five Hundred Pounds for Postage Stamps alone".  Simpler lists for the [[Scottish Episcopal Church]] and for a number of colonial clergy – obtained from alternative sources – had been added by the 1865 edition, whilst details of Irish clergy had also been extracted from [[Alexander Thom (almanac editor)|Alexander Thom]]\'s \'\'Irish Almanack and Official Directory\'\'.  From the 1870s onwards the scope was progressively extended to all parts of the Anglican communion with the notable exception of the [[Episcopal Church (United States)]].  The 1870 edition contained 940 pages, but this had increased to over 2,100 pages by 1892.\n\nThe earliest editions had also gradually added some details of diocesan office holders and administrators, together with the theological colleges, and the royal chapels.  They also acquired much fuller indexes – along with outline maps of dioceses, and increasingly complete lists of bishops, dating right back to the earliest years of their sees.  They further offered to all clergy an opportunity to list their publications, although these lists eventually had to be cut back as their overall length started to increase dramatically.\n\nBy the early 1980s severe economies had become necessary and 1985/86 edition had to be restricted to the "home" churches of England, Scotland and Wales.<ref>\'\'Crockford’s Clerical Directory 1987/88\'\', pp. 47-48</ref>  Retired clergy were temporarily restricted to just a few details of their final appointment, although it became possible to restore the Irish clergy in time for the 1987/88 edition.  Later editions saw a further return of the retired clergy, together with details of those overseas clergy who had originally been licensed or trained in the UK, or who occupied senior positions within their respective church hierarchies.  Details which had also become obtainable from the \'\'[[Church of England Yearbook]]\'\' or from similar sources were generally excluded. For a time too clergy who made their livings though secular jobs  were excluded from the biographies section, with the abbreviation NQ (Non-Qualifying Position) being used to cover such periods when clerics returned to parish work and were again eligible for inclusion. In that many such clergy retained diocesan licences or episcopal "Permissions to Officiate" during their periods of secular employment, this approach may have caused a degree of difficulty for clerics who needed to prove their clerical status.\n\nBy 1985/86 the first women deacons were being included (although [[Hong_Kong_Sheng_Kung_Hui#Social_issues|women priests ordained in Hong Kong]] were included even in the 1970s) while other more recent innovations – from the 1990s onwards – have included optional email addresses, together with lists of those clergy who have died since the previous edition.  Notes on "How to Address the Clergy"<ref>{{Cite web\n| title = How to address the clergy\n| author =\n| work = Crockford\'s Clerical Directory\n| date =\n| accessdate = 2014-08-12\n| url = http://www.crockford.org.uk/standard.asp?id=116\n| quote =\n}}</ref> have been retained. A small number of clergy have been excluded at their own request, or have allowed their biographies to appear minus a contact address.  The Church Commissioners soon replaced the traditional black hardback bindings in favour of red and also introduced a separate softback alternative version.\n\nSince 2004 there has also been a frequently updated Internet edition of \'\'Crockford\'\', which is available by subscription.<ref>{{Cite web\n| title = Welcome to the Crockford web site...\n| author =\n| work = Crockford\'s Clerical Directory\n| date =\n| accessdate = 2014-08-12\n| url = http://www.crockford.org.uk/\n| subscription=yes\n}}</ref> More recently the directory has also joined in with [[social networking]], operating a [[Twitter]] account since 2012. \n\nAn alternative to the main work, \'\'Crockford\'s Shorter Directory\'\', focused almost entirely on the Church of England and omitting all past biographical details, was issued as a single edition in 1953–54.\n\n== Prefaces ==\n\nThe well-known tradition of having an extensive but anonymous preface offering a general review of events within the Anglican communion – together with some occasionally sharp and controversial commentary – evolved gradually during the early part of the 20th century.{{sfn|Currie & Paflin|7 December 2007}} Previous prefaces had tended to be much briefer and they had often been limited merely to explaining the directory’s in-house policies.  After the events following the publication of the 1987/88 edition, which had ended with the death of Dr [[Gareth Bennett]], this tradition of the anonymous preface was discontinued.\n\nAn anthology \'\'Crockford Prefaces: The Editor Looks Back\'\', anonymously edited by [[Richard Henry Malden]]{{sfn|Currie & Paflin|7 December 2007}} and covering the previous 25 years, was published by the Oxford University Press in 1947.{{sfn|Anon|1947|p=}}\n\n== Locating previous issues ==\n\nCounty libraries each have their own policies, but there are good collections in a number of major academic and ecclesiastical libraries, including [[Cambridge University Library]], [[Lambeth Palace Library]], [[Canterbury Cathedral]] Library, [[York Minster]] Library, the [[Guildhall Library]] and the [[Society of Genealogists]].\n\nBesides the 1865 reprint,{{sfn|Bell|1995|p=}} a small number of early editions have been reissued in CD format by various publishers, including [[Archive CD Books]].  Scanned copies of other early editions have also begun to appear on the World Wide Web.\n\n== Crockford references in fiction ==\n\nCrockford is referenced in [[Dorothy Sayers]]\'s 1927 detective novel \'\'[[Unnatural Death]]\'\' (chapter XI) where [[Lord Peter Wimsey]] uses "this valuable work of reference" in trying to trace a clergyman who is important for solving the book\'s mystery.\n\nAnother fictional character holding Crockford on his bookshelves was [[Sherlock Holmes]], who during one of his final short stories ("The Adventure of the Retired Colourman"), consulted his copy before dispatching his colleague Dr Watson, together with another companion, to a distant part of Essex. There they interviewed “a big solemn rather pompous clergyman” who received them angrily in his study.\n\nThe character Dulcie Mainwaring prefers Crockford\'s format to \'\'[[Who\'s Who]]\'\' while reflecting on researching in the [[Public Record Office]] in London in [[Barbara Pym]]\'s \'\'No Fond Return of Love.  \'\'\n\n== Footnotes ==\n{{notelist}}\n\n== Notes ==\n{{reflist}}\n\n== References ==\n{{refbegin}}\n\n* {{Citation\n\n| title = Crockford, John (1824/5–1865)\n| first1 = Brenda\n| last1= Hough\n| first2=H. C. G.\n| last2= Matthew\n| work = Oxford Dictionary of National Biography\n| date =  2004\n| accessdate = 2014-08-12\n| url = http://dx.doi.org/10.1093/ref:odnb/37324\n| language =\n| ref={{sfnref|Hough & Matthew|2004}}\n}}\n\n* {{Cite web\n |title=Shop-talk and mordant wit \n |first1=Christopher \n |last1=Currie \n |first2=Glyn \n |last2=Paflin \n |work=The [[Church Times]] \n |date=7 December 2007 \n |issue=7552 \n |url=http://www.churchtimes.co.uk/content.asp?id=48255 \n |accessdate=2014-08-12 \n |archive-url=https://web.archive.org/web/20120407024756/http://www.churchtimes.co.uk/content.asp?id=48255 \n |archive-date=2012-04-07 \n |ref={{sfnref|Currie & Paflin|7 December 2007}} \n |subscription=yes \n |deadurl=yes \n |df= \n}}\n\n* {{cite book|ref={{sfnref|Bell|1995|p=}}|last=Bell|first=Peter|title=Crockford\'s Clerical Directory for 1865: Being a Biographical and Statistical Book of Reference for Facts Relating to the Clergy and the Church|url=https://books.google.com/books?id=oSqjAQAACAAJ|year=1995|publisher=Horace Cox  in 1865, republished by Peter Bell in 1995|location=Oxford and Edinburgh|isbn=978-1-871538-21-2}}\n* {{cite book|ref={{sfnref|Anon|1947|p=}}|author=Anon|authorlink=Richard Henry Malden|title=Crockford prefaces: the editor looks back|url=https://books.google.com/books?id=rItbAAAAMAAJ|year=1947|publisher=Oxford Univ. Press}}\n*{{cite book|ref=harv|author1=Church of England|author2=Central Board of Finance|author3=Church Commissioners|title=Crockford\'s Clerical Directory|url=https://books.google.com/books?id=BzkFAAAAYAAJ|year=1865|publisher=Oxford University Press}}\n\n{{refend}}\n\n== External links ==\n\n* {{Official website|http://www.crockford.org.uk/}}\n* [https://play.google.com/store/books/details/Church_of_England_Crockford_s_Clerical_Directory?id=BzkFAAAAYAAJ Crockford\'s Clerical Directory 1865 free download from Google play]\n* [http://www.chpublishing.co.uk/ Church House Publishing]\n* [https://archive.org/details/crockfordscleri00commgoog 1868 version available for free download at the archive.org]\n\n[[Category:Directories]]\n[[Category:Church of England]]\n[[Category:Church in Wales]]\n[[Category:Scottish Episcopal Church]]\n[[Category:Anglicanism]]']
["Artists' Bluebook", '19928728', "{{primary sources|date=October 2013}}\nThe '''Artists' Bluebook''' is an international [[database]] of over 270,000 visual artists developed by AskART since 1999 (http://www.askart.com/AskART/help/AskART_about_us.aspx). Revised from its original 1993 print and CD format to digital online access, the Artists' Bluebook is considered a favorite resource for research into artists' lives, artworks and values, and where to buy or sell.\n\n==External links==\n* [http://www.askart.com/AskART/index.aspx The Artists' Bluebook website]\n* [http://www.ala.org/rusa/sections/mars/marspubs/marsbestfreewebsites/marsbestref2003 AskART Bluebook 2003 - review by American Library Association(ALA)]\n[[Category:Directories]]"]
['Bulmer (directories)', '5853521', "'''Bulmer''' was a [[Victorian era|Victorian]] [[historian]], [[surveying|surveyor]], [[cartographer]] and compiler of [[Yellow Pages|directories]]. His directories provided a history and geography of a particular area. The directories listed and described all parishes; listed trades and professions and provided a helpful street index with the names of residents, together with other local information. Data CDs of Bulmer Directories are available from publishers in the UK.<ref>[[S&N Genealogy Supplies|S&N Publishing]]</ref>\n\n==List of directories==\n* Bulmer's History, Topography and Directory of East Cumberland, 1883\n* Bulmer's History, Topography and Directory of West Cumberland, 1884.\n* Bulmer's History, Directory and Topography of Westmorland, 1885\n* Bulmer's History, Topography and Directory of Northumberland (Hexham Division), 1886\n* Bulmer's History, Topography and Directory of Northumberland (Tyneside, Wansbeck and Berwick Divisions), 1887\n* Bulmer's History, and Directory of Newcastle upon Tyne, 1887\n* Bulmer's History, and Directory of North Yorkshire, 1890 (two Volumes)\n* Bulmer's History, Topography and Directory of East Yorkshire and Hull, 1892\n* Bulmer's Directory of Cumberland, 1901\n* T. Bulmer: History, Topography and Directory of Westmorland, 1906\n* T Bulmer: History, Topography and Directory of Furness and Cartmel, 1911\n* Bulmer's History, Topography and Directory of Furness, Cartmel and Egremont division of Cumbria, 1911\n* T Bulmer: History, Topography and Directory of Lancaster and district, 1912\n* J Bulmer: History, Topography and Directory of Lancaster and district, 1913\n\n== References ==\n\n{{reflist}}\n\n{{DEFAULTSORT:Bulmer (Directories)}}\n[[Category:Directories]]\n\n\n{{UK-hist-stub}}\n{{Ref-book-stub}}"]
['Dalilmasr', '27707778', "{{orphan|date=August 2010}}\n'''Dalilmasr''' is an online [[Egyptian language|Egyptian]] directory, which provides information about any service, product, and/or tool available in the Egyptian market. The website includes information about companies, shops, showrooms, and service providers in the Egyptian Region.\n\nIn the five main languages it serves more than two billion people.\n<br>'''Logo Identity:'''The logo is in two palms of the hand applauding (conducting the valued promised service) in black and red color along with white background (Egyptian Flag Colors), the other right and left acute blue triangles meaning the Nile River welfare.\n<ref>[[Al-Ahram|Al-Ahram newspaper]] in large two columns, says Dalilmasr is the first Egyptian search engine focused on contents rather only information and care about all Egyptian cities & suburbs. Issued on June 15, 2010 - page 22.</ref>\n<ref>[[Akhbar El Yom]] newspaper considered Dalilmasr as best Egyptian directory, appendix 3-page 6 on June 12, 2010</ref>\n<ref>[http://www.minia.edu.eg/doHtml.aspx?page=useful-sites.html Minia University's] directory proposed Dalilmasr as one of the recommended Egyptian websites.\n</ref>\n<ref>Wikiwak indexed Dalilmasr since website domain was dalil-masr.com</ref>\n<ref>Google rank it on the top for best Egyptian directory keyword [http://www.google.com.eg/search?hl=en&q=best+egyptian+directory&aq=0&aqi=g3&aql=&oq=best+egyptian+d&gs_rfai= Search Result]</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.dalilmasr.com Dalimasr website]\n\n[[Category:Directories]]"]
['Blogged.com', '15955818', '{{Orphan|date=February 2009}}\n{{Infobox dot-com company\n| name     = Blogged.com \n| logo     = \n| company_type     = [[Private company|Private]]\n| foundation       = 2008\n| founder          =\n| location         = [[Alhambra, California]], [[United States]]\n| key_people       = \n| revenue          = unknown\n| operating_income = \n| net_income       = \n| owner            = \n| num_employees    = number unknown\n| company_slogan   = \n| url              = [http://www.blogged.com/ www.blogged.com]\n| screenshot       = [http://www.techcrunch.com/wp-content/blogged-small.png]\n| caption          = Screenshot of Blogged.com home page\n| alexa            = {{IncreaseNegative}} 16,125,052 ({{as of|2014|4|1|alt=April 2014}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/blogged.com |title= Blogged.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}</ref><!--Updated monthly by OKBot.-->\n| website_type     = [[Wiki]] [[Blog directory]]\n| language         = multilingual\n| advertising      = \n| registration     = Optional\n| launch_date      = {{launch date and age|2008|2|24|p=y}}\n| current_status   = inactive{{Citation needed|date=May 2012}}\n}}\n\n\'\'\'Blogged.com\'\'\' is a blog directory that attempts to combine social networking with people\'s interests in blogging. It employs a method of niche social networking whereby people can connect to each other by their interests rather than their social connections. It attempts to use a method of [[crowdsourcing]] to help evaluate the quality of various blogs{{Citation needed|date=February 2008}}. Blogged symbolizes a trend of new sites that attempt to connect people with their interests rather than social connections{{Citation needed|date=February 2008}}. This type of niche social networking has been employed successfully by sites such as [[Flixster]], [[Yelp, Inc.|Yelp]], [[Last.FM]], and [[Stumbleupon]].  [[TechCrunch]] has recently compared Blogged.com to Yelp for blogs.<ref>\n{{cite news\n | first = Erick \n | last = Schonfeld\n | title = Blogged Hopes to Become the Yelp of Blog Directories\n | url = http://www.techcrunch.com/2008/02/24/blogged-hopes-to-become-the-yelp-of-blog-directories/\n |publisher=Tech Crunch\n |date=2008-02-24\n }}\n</ref>\n\n== Method ==\n\nBlogged.com focuses on blog discovery and displays expert reviews and ratings on popular blogs thereby providing a basis from which to introduce new blogs to a potential reader. Traditional blog search sites such as Technorati and Google Blog Search offer users a method of searching through individual blog entries or postings, but not the blog website itself. Therefore, it is sometimes difficult to gauge the quality or importance of the search results since the credibility of the website which contains the blog entry may be in question. This method of propagating high-quality blogs via user feedback has been used by websites such as Digg. Digg allows users to vote on the importance of articles and causes those articles which are most popular to rise to the top. This method, commonly called [[crowdsourcing]], is being used by Blogged to utilize user feedback to gauge the importance of various blogs.{{Citation needed|date=February 2008}}\n\n== Status ==\n\nAs of 2/3/2016 BLOGGED.COM does not resolve to any site and returns a "Not Found" message in the upper right corner.\n\nThe last known active date for Blogged.com is July 25, 2011.<ref>{{cite web|url=http://www.blogged.com/ |title=Blogged.com Last Crawled Date: July 25, 2011 |publisher=Internet Archive Wayback Machine |date= |accessdate=2012-05-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20110725121828/http://www.blogged.com/ |archivedate=July 25, 2011 }}</ref> The site currently redirects to [[Chime.in]].{{Citation needed|date=May 2012}}\n\n== See also ==\n\n* [[Facebook]]\n* [[Yelp, Inc.]]\n* [[Flixster]]\n* [[Friendster]]\n* [[Stumbleupon]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* "[http://marketwire.com/mw/release.do?id=825026&k=blogged.com Blogged.com Connects Bloggers With Readers; Increases Traffic and Promotes Quality Content Filtered by People]," Marketwire, 2/25/2008.\n* "[http://www.webware.com/8301-1_109-9877585-2.html Blogged.com launches blog directory, reviews]," \'\'[[CNET]] Blogs\'\', 2/25/2008.\n* "[http://mashable.com/2008/02/24/bloggedcom/ Blogged.com. More than Just Another Blog Search Tool]," \'\'[[Mashable]]\'\', 2/25/2008.\n* "[http://www.blogherald.com/2008/02/25/bloggedcom-new-blog-directory-officially-launches/ Blogged.com new blog directory officially launches]," Blogherald, 2/25/2008.\n* "[http://www.bloggingtips.com/2008/02/25/bloggedcom-public-beta-goes-live/ Blogged.com Public Beta Goes Live]," BloggingTips, 2/25/2008.\n* "[https://web.archive.org/web/20070214023246/http://publications.mediapost.com:80/index.cfm?fuseaction=Articles.showArticleHomePage Blogged.com Ranks Blogs For Consumers, Could Help Bloggers Monetize]," Online Media Daily, 2/25/2008\n\n==External links==\n* [https://web.archive.org/web/20110725121828/http://www.blogged.com/ Blogged.com]\n\n{{DEFAULTSORT:Blogged.Com}}\n[[Category:Blogging]]\n[[Category:Directories]]\n[[Category:American websites]]\n[[Category:Alhambra, California]]']
['Corporate Technology Directory', '33232179', 'The \'\'\'Corporate Technology Directory\'\'\' also known as the \'\'\'CorpTech directory of technology companies\'\'\' was a [[Trade directory|directory]] of technology companies published from 1986<!-- maybe 1987 -->-2004 by [[CorpTech]]. It listed thousands of technology companies including software, services, and hardware as well as developers.\n\nThe directory was later made available in digital form as a cd<ref name="SterlingBracken1998">{{cite book|last1=Sterling|first1=Christopher H.|last2=Bracken|first2=James K.|last3=Hill|first3=Susan M.|title=Mass communications research resources: an annotated guide|url=https://books.google.com/books?id=kwOo6BiWiFkC&pg=PA10|accessdate=27 September 2011|year=1998|publisher=Psychology Press|isbn=978-0-8058-2024-9|pages=10–}}</ref> and subsequently database subscription.\n\n==See also==\n* [[Major Information Technology Companies of the World]]\n\n==References==\n{{reflist}}\n<!-- https://books.google.com/books?id=96XpVBf6pvAC&pg=PA246 -->\n\n{{technology-stub}}\n\n[[Category:Directories]]']
['Clerical Guide or Ecclesiastical Directory', '34261059', '{{italic title}}\nThe \'\'\'\'\'Clerical Guide or Ecclesiastical Directory\'\'\'\'\' was the earliest ever specialist directory to cover the clergy of the [[Church of England]]. In its initial format it appeared just four times – in 1817, 1822, 1829 and 1836, under the editorial direction of [[Richard Gilbert (printer)|Richard Gilbert]].\n\nAnother edition was actually advertised for 1838,<ref name="paflin">[http://www.churchtimes.co.uk/content.asp?id=48255] [[Church Times]]: two-part article \'\'Shop-talk and mordant wit\'\', by Christopher Currie & Glyn Paflin, describing the background to [[Crockford\'s Clerical Directory]]\'s first hundred editions, 6–13 December 2007</ref> but no copies have in fact been found within the main academic libraries.\n\nThe title was briefly revived by Thomas Bosworth & Company during the 1880s.\n\n==Contents of the Clerical Guide==\n\nThe main alphabetical section of the directory included:\n\n*A list of benefices together with their populations, counties, dioceses and  archdeaconries\n*Their incumbents with the year of his institution\n*Their values (up to the 1829 edition) in the [[Valor Ecclesiasticus]] or King\'s Books\n*The names of their patrons.\n*The 1836 edition additionally gave the income of the benefice during the year 1831, the available capacity or "church room" for the congregation, and the name of any [[impropriator]].\n\nThe preliminary pages included:\n\n*Current lists of [[bishops]], members of [[cathedral chapter]], and other dignitaries, showing the values of their [[first fruits]]\n*A section on the Doctors of Laws, the [[canon law|canonical]] specialists\n*A section on the [[Chapel Royal]] together with the king\'s preachers and chaplains\n*Sections on [[Sion College]] and [[Gresham College]]\n*Sections on the two English universities ([[University of Oxford|Oxford]] and [[University of Cambridge|Cambridge]])\n*Sections on the fellows and schoolmasters of [[Eton College|Eton]], [[Winchester College|Winchester]], [[Westminster School|Westminster]], [[Harrow School|Harrow]], [[Manchester Grammar School|Manchester]] and [[St Paul\'s School, London|St Paul\'s]].\n\nThe alphabetical list of benefices was also followed by an alphabetical list of the prelates, dignitaries and beneficed clergy of the Church of England (generally omitting the unbeneficed clergy).\n\nThe directories concluded with lists of ecclesiastical patronage, giving the names of those benefices within the gift of the king and also those of the lord chancellor, the chancellor of the duchy of Lancaster, the various archbishops and bishops, and the two universities.\n\n==The publishers==\n\nThe 1817 edition stated that it was "printed for [[Rivington (publishers)|J. C. & F. Rivington]], 62 St Paul\'s Churchyard, by R. & R. Gilbert, St John\'s Square, [[Clerkenwell]]".  \'\'\'Richard Gilbert\'\'\' was a printer and an accountant with the [[SPCK]].  Although he appeared in the 1817 edition merely as the "printer" (alongside his brother Robert, who died the following year), he thereafter seems to have taken a more prominent role in its production.  The 1822 edition was "corrected by Richard Gilbert", as though he had been engaged in putting right someone else\'s mistakes.  He similarly wrote the prefaces for subsequent editions, and the 1836 edition still bore the names "Gilbert and Rivington, printers, St John\'s Square".\n\nGilbert, an industrious compiler who was additionally very active in the religious life of Clerkenwell, also produced a pocket-sized \'\'\'Clergyman\'s Almanack\'\'\' in 1819 <ref>Oxford Dictionary of National Biography: article on Richard Gilbert</ref>\n\n==The Clerical Guide after 1836==\n\nThe failure of the directory to appear after 1836 left open an opportunity for a rival publication.  This was filled after 1841 by the [[Clergy List]].\n\nAfter lying dormant for fifty years, the title \'\'\'Clerical Guide and Ecclesiastical Directory\'\'\' was briefly revived in 1886 by Thomas Bosworth & Company, 65 [[Great Russell Street]]. Once again the volume offered alternative listings of the clergy and the benefices, together with other "valuable information … from the office of the [[Ecclesiastical Commission (Church of England)|Ecclesiastical Commission]].<ref>The Times newspaper, Thursday, Mar 18, 1886; pg. 12</ref>  However the relaunched title was very quickly acquired by Hamilton Adams of [[Paternoster Row]], who in 1889 merged it with their other recent acquisition, the aforementioned Clergy List.<ref name="paflin" />\n\nIn the issue for 1918/19 the Clergy List was merged in its turn with [[Crockford\'s Clerical Directory]]. Thereafter until the 1930s the latter title still continued to advertise on its preliminary pages that it "incorporated the Clergy List", together with the "Clerical Guide and Ecclesiastical Directory".\n\nA microfiche version of the 1829 directory was produced during the 1980s by the [[Society of Genealogists]]. In more recent years scanned copies of the early editions have also appeared on the World Wide Web.<ref>All four editions of the Clerical Guide from 1817-1836 may be downloaded free of charge from the Google eBookstore [https://books.google.com/ebooks]</</ref>\n\n==See also==\n*[[Clergy of the Church of England database]]\n\n==References==\n{{reflist}}\n\n[[Category:Directories]]\n[[Category:Church of England]]\n[[Category:Church in Wales]]\n[[Category:Scottish Episcopal Church]]\n[[Category:Anglicanism]]']
['Category:Yellow pages', '36911912', '{{Commons category|Yellow pages}}\n{{Cat main|Yellow pages}}\n\n[[Category:Directories]]\n[[Category:Advertising by medium]]']
["Polk's Directory", '39021356', '#REDIRECT [[R.L. Polk & Company]]\n\n[[Category:Histories of cities in the United States]]\n[[Category:Publications established in the 1870s]]\n[[Category:Directories]]']
['Deutsches Geschlechterbuch', '41858428', "{{italic title}}\nThe '''''Deutsches Geschlechterbuch''''', until 1943 known as the '''''Genealogisches Handbuch bürgerlicher Familien''''', is a major German genealogical handbook of [[Bourgeoisie|bourgeois]] or [[patrician (post-Roman Europe)|patrician]] families. It is the bourgeois and patrician equivalent of the ''[[Genealogisches Handbuch des Adels]]'' and the former ''[[Almanach de Gotha]]''. It includes genealogies and coats of arms of the included families. The ''Genealogisches Handbuch bürgerlicher Familien'' was started in 1889 and prior to 1943, 119 volumes covering around 1,200 families were published under the original title. From 1956, the series were continued under the title ''Deutsches Geschlechterbuch''. In 2007, the 219th and latest volume was published. In total, around 4,000 families have been covered.\n\nThe ''Hamburgisches Geschlechterbuch'', comprising 17 volumes on the [[Hanseaten (class)|Hanseatic]] families of Hamburg, is an integral part of the work, and is regarded as the most comprehensive reference work of its kind on a single city.<ref>Hildegard von Marchthaler: ''Die Bedeutung des Hamburger Geschlechterbuchs für Hamburgs Bevölkerungskunde und Geschichte'', in: ''Hamburgisches Geschlechterbuch'', Bd. 9, Limburg an der Lahn 1961, S. XXIII</ref>\n\nThe publication has been highly influential and inspired several similar publications, such as ''[[Nederland's Patriciaat]]''. To some extent it corresponds to ''[[Burke's Landed Gentry]]'' in the United Kingdom, although it could also be said to be the equivalent of ''[[Burke's Peerage]]'' in its coverage of [[Hanseaten (class)|Hanseatic]] and patrician families who comprised the highest class in the former city-republics.\n\n==References==\n{{reflist}}\n\n==Bibliography==\n*Genealogisches Handbuch bürgerlicher Familien (1889–1943)\n*Deutsches Geschlechterbuch (1956-)\n\n[[Category:Biographical dictionaries]]\n[[Category:Genealogy publications]]\n[[Category:Directories]]\n[[Category:Publications established in 1889]]\n\n\n{{bio-dict-stub}}"]
['Category:Index (publishing)', '2198982', '[[Category:Library science]]\n[[Category:Publishing]]\n[[Category:Directories]]']
['Category:File system directories', '30139425', '[[Category:Computer file systems]]\n[[Category:Directories]]']
['Category:Metadata registry', '3675616', 'All Wikipedia articles in this category are either instances of a metadata registry or related to metadata registries.\n{{Cat main|Metadata registry}}\n[[Category:Metadata]]\n[[Category:Directories]]']
['Sands Directory', '45359850', '{{Use Australian English|date=March 2015}}\n{{Use dmy dates|date=March 2015}}\n\n[[File:Sands Directory 1899 (book).JPG|thumb|1899 edition of Sands Directory ([[National Library of Australia]])]]\nThe \'\'\'Sands Directories\'\'\', also published as the \'\'\'Sands and Kenny Directory\'\'\' and the \'\'\'Sands and McDougall Directory\'\'\' were annual publications in Australia.\n\nThey listed household, business, society, and Government contacts<ref name=":0" /> in [[Melbourne]], [[Adelaide]] and [[Sydney]] including some rural areas of Victoria and New South Wales from the 1850s.<ref name=Bibliog>{{cite book|last1=Eslick, Christine; Joy Hughes, and R. Ian Jack|title=Bibliography of New South Wales local history: an annotated bibliography of secondary works published before 1982 and New South Wales directories 1828 -1950|date=1987|publisher=New South Wales University Press|location=Kensington, NSW|url=http://library.sl.nsw.gov.au/record=b1187352~S2|isbn=0-86840-154-4|pages=372; 398}}</ref> [[City directory|City directories]] are an important resource for historical research, allowing individual addresses and occupations to be linked to specific streets and suburbs.<ref>{{cite book|last1=Williams|first1=A.V.|title=The development and growth of city directories|date=1913|publisher=Williams directory co.|location=Cincinnati, Ohio|url=http://babel.hathitrust.org/cgi/pt?id=nyp.33433082423645;view=1up;seq=5|accessdate=5 March 2015}}</ref>\n\n==Publisher==\n[[File:Sands Directory 1899 (cover).JPG|thumb|1899 edition of Sands Directory (cover)]]\n[[John Sands (printer)|John Sands]] (1818-1873) was an engraver, printer and stationer.  Born in England he moved to [[Sydney, Australia|Sydney]] in 1837.<ref name="ADB Sands">{{cite book|last1=Walsh|first1=G.P.|title=\'Sands, John (1818–1873)\', Australian Dictionary of Biography|date=1976|publisher=National Centre of Biography, Australian National University|url=http://adb.anu.edu.au/biography/sands-john-4536|accessdate=5 March 2015}}</ref>  Sands formed several business partnerships, in 1851 with his brother-in-law Thomas Kenny, and in 1860 with Dugald McDougall with the business being known as [[John Sands (company)|Sands, Kenny & Co.]]<ref name="ADB Sands"/> Directory titles changed as the publisher changed partners, and at different points the Sands Directories were also published as the \'Sands and Kenny\' or \'Sands and McDougall Directories\'.<ref name=Kingston />\n\n==Sands, Kenny & Co\'s commercial and general Melbourne directory==\nThe first Melbourne Directory was published by Sands and Kenny in 1857.<ref name=Kingston>{{cite web|title=Sands and McDougall Melbourne Directories|url=http://www.kingston.vic.gov.au/library/Library-Services/Family-History-Resources/Sands-and-McDougall-Melbourne-Directories|website=Kingston Libraries|publisher=Kingston Libraries|accessdate=10 February 2015|ref=Kingston}}</ref> By 1858 the second edition of the directory was distributed to public libraries in the major seaports of Great Britain, Ireland, the United States of America, and Canada.<ref>{{cite news |url=http://nla.gov.au/nla.news-article154855152 |title=Publications Received |newspaper=[[The Age]] |location=Melbourne |date=1 February 1858 |accessdate=5 March 2015 |page=6 }}</ref>  From 1862 to 1974 the Melbourne directories were published as the Sands and McDougall Melbourne Directory.<ref name=Kingston /><ref>{{cite news|last1=Stephens|first1=Andrew|title=Sands & McDougall directory exhibition brings old Melbourne back to life.|url=http://www.smh.com.au/entertainment/sands--mcdougall-directory-exhibition-brings-old-melbourne-back-to-life-20140811-102quc.html|accessdate=5 March 2015|work=Sydney Morning Herald|date=15 August 2014}}</ref>\n\nThe 1860 Melbourne directory was 400 pages long and contained over 10,000 entries.<ref name=":0">{{cite news |url=http://nla.gov.au/nla.news-article154880275 |title=Sands and Kenny\'s Melbourne Directory|newspaper=[[The Age]]|location=Melbourne |date=24 January 1860 |accessdate=5 March 2015 |page=5 }}</ref>\n\n==Sands Sydney, Suburban and Country Commercial Directory==\n[[File:Sands Directory 1899 (spine).JPG|thumb|1899 edition of Sands Directory (spine)]]\nThe \'\'Sands Sydney, Suburban and Country Commercial Directory\'\', first published in 1858,<ref name="FMP">{{cite web|title=New South Wales, Sydney Directory 1847-1913|url=http://www.findmypast.com.au/articles/world-records/full-list-of-australia-and-new-zealand-records/newspapers-directories-and-social-history/new-south-wales-sydney-directory-1847-1913|website=Find My Past|accessdate=5 March 2015}}</ref> included a variety of information including street addresses and businesses, farms and country towns, stock numbers (e.g. horses, cattle and sheep on each station) as well as information about public watering places including dams, tanks and wells.<ref>{{cite news |url=http://nla.gov.au/nla.news-article118819151 |title=Sands\' Directory |newspaper=[[The Evening News (Sydney)|The Evening News]] |location=Sydney |date=15 January 1923 |accessdate=5 March 2015 |page=9 }}</ref>  With the primary function of post office directory it provides lists of householders, businesses, public institutions and officials.<ref>{{cite web|title=Family history and genealogy|url=http://www.sl.nsw.gov.au/about/collections/documenting/social/famhist.html|website=State Library of NSW|accessdate=5 March 2015}}</ref>\n\nThe Sydney editions of the directory, covering the state of New South Wales, were published each year from 1858–59 to 1932–33.<ref name=Sydney>{{cite web|title=Sands Sydney, Suburban and Country Commercial Directory|url=http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory|website=The City of Sydney|publisher=The City of Sydney|accessdate=10 February 2015}}</ref>  There were four years when the directory did not appear during this time, they were 1872, 1874, 1878 and 1881.<ref name= CGHG>{{Citation | author1=Cridland, Marilyn | author2=Central Coast Family History Group (N.S.W.) | title=A guide to the Sands Directory | publication-date=1997 | publisher=Central Coast Family History Group Inc | page =1|url=http://trove.nla.gov.au/work/35275389 | accessdate=5 March 2015 }}</ref>  The directory is arranged by municipalities in which properties were located, listing the primary householder street by street.<ref>{{cite web|title=Sands Sydney Directory Guide|url=http://www.waverley.nsw.gov.au/__data/assets/pdf_file/0009/28557/Sands_Sydney_Directory_guide_for_LS_website_revised.pdf|website=Waverley Council|accessdate=5 March 2015}}</ref> As a consequence, the household and business information in the directories is used for research into Sydney history,<ref>{{cite web|title=Sands Directory – Researching your house\'s history|url=http://insidehistorymagazine.blogspot.com.au/2011/11/sands-directory-researching-your-houses.html|website=Inside History magazine|accessdate=5 March 2015}}</ref> with particular application for genealogical research.<ref name= CGHG/><ref name="Sands digital edition">{{cite web|title=Sands\' Directory [digital edition]|url=http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory|website=City of Sydney|accessdate=5 March 2015}}</ref><ref>{{cite web|last1=Royal Australian Historical Society|title=Sands Directories are now online!|url=http://www.rahs.org.au/sands-directories-are-now-online/|accessdate=5 March 2015}}</ref>\n\nBy 1909 the Sydney directory contained over 1700 pages.<ref name=SMH_Trove>{{cite news |url=http://nla.gov.au/nla.news-article15027528 |title=Sands\' Directory 1909. |newspaper=[[The Sydney Morning Herald]] |location=NSW |date=9 January 1909 |accessdate=11 February 2015 |page=11}}</ref> The full title of the 1913 edition of the directory of Sydney is \'\'Sands Sydney, Suburban and Country Directory for 1913 comprising, amongst other information, street, alphabetical, trade and professional, country towns, country alphabetical, pastoral, educational, governmental, parliamentary, law and miscellaneous lists\'\'.<ref name="FMP"/>\n\n==Sands & McDougall\'s South Australian directory==\nSands and McDougall arrived in [[Adelaide, South Australia|Adelaide]] in 1883.<ref name="SLSA">{{cite web|title=South Australian directories|url=http://guides.slsa.sa.gov.au/directories|website=State Library of South Australia|accessdate=5 March 2015}}</ref>  They took over the directory previously published by Josiah Boothby, publishing their first South Australian directory in January 1884.<ref name="SLSA"/><ref>{{Citation | author1=Sands & McDougall Limited | title=Sands & McDougall\'s South Australian directory : with which is incorporated Boothby\'s South Australian directory | publication-date=1884 | publisher=Printed and published by Sands & McDougall | url=http://trove.nla.gov.au/work/21481893 | accessdate=5 March 2015 }}</ref><ref>{{cite news |url=http://nla.gov.au/nla.news-article166349067 |title=South Australian Directory |newspaper=[[The Southern Cross (South Australia)|The Southern Cross]] |location=Adelaide |date=27 March 1896 |accessdate=5 March 2015 |page=4}}</ref> The Sands & McDougall\'s Directory of South Australia was published from 1884 to 1974.<ref name=Trove_SA>{{cite web | author1=Sands & McDougall Limited | title=Sands & McDougall\'s directory of South Australia | publication-date=1884 | publisher=Sands & McDougall | url=http://trove.nla.gov.au/work/21481863 | accessdate=11 February 2015 }}</ref>\n\n==See also==\n[[Western Australia Post Office Directory|Wise Directories]]\n\n==External links==\n* [http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory digitised Sydney sands directory], at the City of Sydney archives\n\n==References==\n{{reflist|2}}\n\n[[Category:Gazetteers]]\n[[Category:Directories]]\n[[Category:Australian directories]]']
['Bengali film directory', '19601961', "{{italic title}}\n{{Infobox book\n| name = Bengali Film Directory\n| image = Bengali film directory.jpg\n| caption =Cover page of ''Bengali Film Directory''\n| author = [[Ansu Sur]]\n| country = India\n| language = English\n| cover_artist =\n| genre = [[Trade directory|Directory]]\n| publisher = [[Nandan (Kolkata)|Nandan]]<br /> [[West Bengal]] Film Centre([[Calcutta]])\n| release_date = 1999\n| media_type = Print ([[Hardback]])\n| pages =319\n| preceded_by =\n| followed_by =\n}}\n\n'''''Bengali Film Directory''''' is an archive of [[Cinema of West Bengal|Bengali film]]s (in English).<ref>{{cite web |url=http://www.rosland.freeserve.co.uk/filmbooks5.htm |title=The Howard Summers Cinema Website-National Filmographies-Asian Cinema |publisher=www.rosland.freeserve.co.uk |accessdate=2008-10-23 |archiveurl = https://web.archive.org/web/20080616195358/http://www.rosland.freeserve.co.uk/filmbooks5.htm <!-- Bot retrieved archive --> |archivedate = 2008-06-16}}</ref> Published in March 1999 by [[Nandan (Kolkata)|Nandan]], [[West Bengal]] Film Centre ([[Calcutta]]), this directory was edited by Ansu Sur and was compiled by Abhijit Goswami. It includes all [[Bengali people|Bengali]] [[feature film]]s released from 1917 to 1998, described briefly, but including detailed cast and crew, director name, release date and release theater name.<ref>{{cite web |url=https://openlibrary.org/b/OL171681M |title=Bengali film directory (Open Library) |publisher=openlibrary.org |accessdate=2008-10-23}}\n</ref>\n\n==Contents==\n* Acknowledgements iv\n* A Note from the Editor v\n* Reference vi\n* Abbreviations vii\n* Filmography\n** Silent era 1\n** Sound era 9\n* Studios and Post-Production Centres 267\n* Production Companies 269\n* Distributions 271\n* Show-houses 274\n* Useful Addresses 277\n* Film Societies 278\n* Film Journals 280\n* Film Books 281\n* Index\n** Films 285\n** Directors 298\n** Actors and Actresses 303\n* Appendix: Films released in 1998 315<ref>{{cite web |url=http://www.cscsarchive.org/MediaArchive/Library.nsf/(docid)/5BD370ADB88260A6652571AF0037A748?OpenDocument&StartKey=Bengali&Count=100 |title=Bengali film directory |publisher=www.cscsarchive.org |accessdate=2008-10-23}} {{Dead link|date=October 2010|bot=H3llBot}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n\n{{Cinema of West Bengal}}\n{{Cinema of Bangladesh}}\n{{Cinema of India}}\n{{World cinema}}\n\n[[Category:Cinema of Bengal]]\n[[Category:Bengali language]]\n[[Category:1999 books]]\n[[Category:Bengali-language media]]\n[[Category:English-language media]]\n[[Category:Film guides]]\n[[Category:Archives in India]]\n[[Category:Directories]]\n\n\n{{film-org-stub}}"]
['Web search query', '11525372', 'A \'\'\'web search query\'\'\' is a query that a user enters into a [[web search engine]] to satisfy his or her [[information needs]]. Web search queries are distinctive in that they are often plain text or [[hypertext]] with optional search-directives (such as "and"/"or" with "-" to exclude). They vary greatly from standard [[query language]]s, which are governed by strict syntax rules as [[command language]]s with keyword or positional [[Parameter (computer science)|parameters]].\n\n== Types ==\nThere are three broad categories that cover most web search queries: informational, navigational, and transactional.<ref>Broder, A. (2002). A taxonomy of Web search. SIGIR Forum, 36(2), 3–10.</ref> These are also called "do, know, go."<ref>{{cite web|last=Gibbons|first=Kevin|title=Do, Know, Go: How to Create Content at Each Stage of the Buying Cycle|url=http://searchenginewatch.com/article/2235624/Do-Know-Go-How-to-Create-Content-at-Each-Stage-of-the-Buying-Cycle|publisher=Search Engine Watch|accessdate=24 May 2014}}</ref> Although this model of searching was not theoretically derived, the classification has been  empirically validated with actual search engine queries.<ref>Jansen, B. J., Booth, D., and Spink, A. (2008) [https://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_user_intent.pdf Determining the informational, navigational, and transactional intent of Web queries], Information Processing & Management. 44(3), 1251-1266.</ref>\n\n* \'\'\'Informational queries\'\'\' – Queries that cover a broad topic (e.g., \'\'colorado\'\' or \'\'trucks\'\') for which there may be thousands of relevant results.\n* \'\'\'Navigational queries\'\'\' – Queries that seek a single website or web page of a single entity (e.g., \'\'youtube\'\' or \'\'delta air lines\'\').\n* \'\'\'Transactional queries\'\'\' – Queries that reflect the intent of the user to perform a particular action, like purchasing a car or downloading a screen saver.\n\nSearch engines often support a fourth type of query that is used far less frequently:\n\n* \'\'\'Connectivity queries\'\'\' – Queries that report on the connectivity of the indexed [[web graph]] (e.g., Which links point to this [[Uniform Resource Locator|URL]]?, and How many pages are indexed from this [[domain name]]?).<ref>{{cite web|last=Moore|first=Ross|title=Connectivity servers|url=http://nlp.stanford.edu/IR-book/html/htmledition/connectivity-servers-1.html|publisher=Cambridge University Press|accessdate=24 May 2014}}</ref>\n\n== Characteristics ==\n\nMost commercial web search engines do not disclose their search logs, so information about what users are searching for on the Web is difficult to come by.<ref>Dawn Kawamoto and Elinor Mills (2006), [http://news.cnet.com/AOL-apologizes-for-release-of-user-search-data/2100-1030_3-6102793.html AOL apologizes for release of user search data]</ref> Nevertheless, research studies appeared in 1998.<ref>Jansen, B. J., Spink, A., Bateman, J., and Saracevic, T. 1998. [https://faculty.ist.psu.edu/jjansen/academic/jansen_sigir_forum.pdf Real life information retrieval: A study of user queries on the web]. SIGIR Forum, 32(1), 5 -17.</ref><ref>Silverstein, C., Henzinger, M., Marais, H., & Moricz, M. (1999). Analysis of a very large Web search engine query log. SIGIR Forum,\n33(1), 6–12.</ref> Later, a study in 2001<ref>{{cite journal|author1=Amanda Spink |author2=Dietmar Wolfram |author3=Major B. J. Jansen |author4=Tefko Saracevic | year = 2001 | title = [https://faculty.ist.psu.edu/jjansen/academic/jansen_public_queries.pdf Searching the web: The public and their queries] | journal = Journal of the American Society for Information Science and Technology | volume = 52 | issue = 3 | pages = 226–234 | doi = 10.1002/1097-4571(2000)9999:9999<::AID-ASI1591>3.3.CO;2-I }}</ref> analyzed the queries from the [[Excite]] search engine showed some interesting characteristics of web search:\n\n* The average length of a search query was 2.4 terms. \n* About half of the users entered a single query while a little less than a third of users entered three or more unique queries. \n* Close to half of the users examined only the first one or two pages of results (10 results per page).\n* Less than 5% of users used advanced search features (e.g., [[boolean operators]] like AND, OR, and NOT).\n* The top four most frequently used terms were, \'\' (empty search), and, of, \'\'and\'\' sex.\n\nA study of the same Excite query logs revealed that 19% of the queries contained a geographic term (e.g., place names, zip codes, geographic features, etc.).<ref>{{cite conference |author1=Mark Sanderson  |author2=Janet Kohler  |lastauthoramp=yes | year = 2004 | title = Analyzing geographic queries | booktitle = Proceedings of the Workshop on Geographic Information (SIGIR \'04) | url =http://supremacyseo.com/analyzing-geographic-queries }}</ref> Studies also show that, in addition to short queries (i.e., queries with few terms), there are also predictable patterns to how users change their queries.<ref>Jansen, B. J., Booth, D. L., & Spink, A. (2009). [[Patterns of query modification during Web searchinhttps://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_patterns_query_reformulation.pdf|Patterns of query modification during Web searchin]]g. Journal of the American Society for Information Science and Technology. 60(3), 557-570. 60(7), 1358-1371.</ref>\n\nA 2005 study of Yahoo\'s query logs revealed 33% of the queries from the same user were repeat queries and that 87% of the time the user would click on the same result.<ref>{{cite conference |author1=Jaime Teevan |author2=Eytan Adar |author3=Rosie Jones |author4=Michael Potts | year = 2005 | title = History repeats itself: Repeat Queries in Yahoo\'s query logs | booktitle = Proceedings of the 29th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR \'06) | pages = 703–704 | url =http://www.csail.mit.edu/~teevan/work/publications/posters/sigir06.pdf | doi=10.1145/1148170.1148326 }}</ref> This suggests that many users use repeat queries to revisit or re-find information. This analysis is confirmed by a Bing search engine blog post telling about 30% queries are navigational queries <ref>http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx</ref>\n\nIn addition, much research has shown that query term frequency distributions conform to the [[power law]], or \'\'long tail\'\' distribution curves. That is, a small portion of the terms observed in a large query log (e.g. > 100 million queries) are used most often, while the remaining terms are used less often individually.<ref name="baezayates1">{{cite journal | author = Ricardo Baeza-Yates | year = 2005 | title = Applications of Web Query Mining | booktitle = Lecture Notes in Computer Science | pages = 7–22 | volume = 3408 | publisher = Springer Berlin / Heidelberg | url = http://www.springerlink.com/content/kpphaktugag5mbv0/ | ISBN = 978-3-540-25295-5}}</ref> This example of the [[Pareto principle]] (or \'\'80–20 rule\'\') allows search engines to employ [[optimization techniques]] such as index or [[Partition (database)|database partitioning]], [[web cache|caching]] and pre-fetching. In addition, studies have been conducted on discovering linguistically-oriented attributes that can recognize if a web query is navigational, informational or transactional.<ref>{{cite journal | author = Alejandro Figueroa | year = 2015 | title = Exploring effective features for recognizing the user intent behind web queries | booktitle = Computers in Industry | pages = 162–169 | volume = 68 | publisher = Elsevier | url = https://www.researchgate.net/publication/271911317_Exploring_effective_features_for_recognizing_the_user_intent_behind_web_queries}}</ref>\n\nBut in a recent study in 2011 it was found that the average length of queries has grown steadily over time and average length of non-English languages queries had increased more than English queries.<ref>{{cite journal |author1=Mona Taghavi |author2=Ahmed Patel |author3=Nikita Schmidt |author4=Christopher Wills |author5=Yiqi Tew | year = 2011 | title = An analysis of web proxy logs with query distribution pattern approach for search engines | booktitle = Journal of Computer Standards & Interfaces | pages = 162–170 | volume = 34 | issue = 1 |publisher = Elsevier  | url = http://www.sciencedirect.com/science/article/pii/S0920548911000808 | doi=10.1016/j.csi.2011.07.001}}</ref> Google has implemented the [[Google Hummingbird|hummingbird]] update in August 2013 to handle longer search queries since more searches are conversational (i.e. "where is the nearest coffee shop?").<ref>{{cite web|last=Sullivan|first=Danny|title=FAQ: All About The New Google "Hummingbird" Algorithm|url=http://searchengineland.com/google-hummingbird-172816|publisher=Search Engine Land|accessdate=24 May 2014}}</ref> \nFor longer queries, [[Natural language processing]] helps, since parse trees of queries can be matched with that of answers and their snippets.<ref>{{vcite journal |author=Galitsky B|title=Machine learning of syntactic parse trees for search and classification of text|journal=Engineering Applications of Artificial Intelligence |volume=26 |issue=3 |date=2013 |pages=153-172|doi=10.1016/j.engappai.2012.09.017}}</ref> For multi-sentence queries where keywords statistics and [[Tf–idf]] is not very helpful, [[Parse thicket]] technique comes into play to structurally represent complex questions and answers.<ref>{{vcite journal |author=Galitsky B, Ilvovsky D, Kuznetsov SO, Strok F|title=Finding Maximal Common Sub-parse Thickets\nfor Multi-sentence Search |journal=Lecture Notes In Artificial Intelligence |volume = 8323 |date=2013 |http://www.aclweb.org/anthology/R13-1037\n}}</ref>\n\n== Structured queries ==\nWith search engines that support Boolean operators and parentheses, a technique traditionally used by librarians can be applied. A user who is looking for documents that cover several topics or \'\'facets\'\' may want to describe each of them by a [[logical disjunction|disjunction]] of characteristic words, such as <code>vehicles OR cars OR automobiles</code>. A \'\'faceted query\'\' is a [[logical conjunction|conjunction]] of such facets; e.g. a query such as <code>(electronic OR computerized OR DRE) AND (voting OR elections OR election OR balloting OR electoral)</code> is likely to find documents about electronic voting even if they omit one of the words "electronic" and "voting", or even both.<ref>{{Cite web\n|url=http://eprints.eemcs.utwente.nl/6918/01/TR-CTIT-06-57.pdf\n|title=Exploiting Query Structure and Document Structure to Improve Document Retrieval Effectiveness\n|author1=Vojkan Mihajlović |author2=Djoerd Hiemstra |author3=Henk Ernst Blok |author4=Peter M.G. Apers |postscript=<!--None-->}}</ref>\n\n== See also ==\n* [[Information retrieval]]\n* [[Web search engine]]\n* [[Web query classification]]\n* [[Taxonomy for search engines]]\n\n== References ==\n{{reflist|2}}\n\n{{Internet search}}\n\n[[Category:Internet search]]']
['Mystery Seeker', '25176438', '{{Infobox website\n| name               = Mystery Seeker\n| logo               = MYSTERYSEEKER logo.jpeg\n| logo_alt           = The mysteryseeker.com logo\n| logocaption       = The logo found at [http://www.mysteryseeker.com mysteryseeker.com], decorated with fog and a moon in the background.\n| screenshot         = mysteryseeker.com screenshot.jpeg\n| collapsible        = y\n| screenshot_alt     = Screenshot of [http://www.mysteryseeker.com mysteryseeker.com]\n| caption            = Screenshot of [http://www.mysteryseeker.com mysteryseeker.com].\n| url                =  {{URL|www.mysteryseeker.com}}\n| slogan             = “What will you search for?”\n| commercial         = <!-- "Yes", "No" or leave blank -->\n| type               = Search engine\n| registration       = None\n| language           = [[English language|English]]\n| content_license    = &copy; Mystery Seeker, 2009\n| owner              = Mystery Seeker\n| launch_date        = {{start date and age|2009|10|02|df=yes}}\n| alexa              = {{decrease}} [http://www.alexa.com/siteinfo/mysteryseeker.com 3110015] (Global, November 2015)\n| current_status     = Online but defunct \n| footnotes          = \n}}\n\'\'\'Mystery Seeker\'\'\' is a website based on the [[Google]] search engine.<ref name="Chivers 2009" /> Until November 30, 2009, the website was known as \'\'\'Mystery Google\'\'\', but on December 1, 2009, the name changed to \'\'\'Mystery Seeker\'\'\'. It has been featured in a number of technology blogs.<ref>{{cite news| url=http://www.huffingtonpost.com/2009/10/12/mystery-google-surprise-y_n_318089.html | work=Huffington Post | first=Bianca | last=Bosker | title=Mystery Google: Surprise Yourself With Someone Else\'s Search Results | date=October 12, 2009}}</ref><ref>[http://mashable.com/2009/10/12/mystery-google/ Mystery Google: The “I’m Feeling Lucky” Button Re-imagined]</ref><ref>[http://www.geekologie.com/2009/11/i_wasnt_looking_for_that_myste.php I Wasn\'t Looking For That: Mystery Google Gives You Previous Person\'s Search Query | Geekologie]</ref> Upon a search query, Mystery Seeker returns the results from the previous search, so “you get what the person before you searched for.”<ref name="Chivers 2009">{{Cite news|url=http://www.telegraph.co.uk/technology/google/6316140/Mystery-google-returns-other-peoples-search-results.html|title=Mystery Google returns other people\'s search results|accessdate=2009-11-23|publisher=The Telegraph|date=13 Oct 2009|author=Tom Chivers|location=London}}</ref>\n\nThere is a trend among the people on Mystery Seeker to add so-called "missions", where the next user is asked to do something. For example, "Your mission is to copy and paste this until you see it again. Then and only then will you be a true ninja".<ref>[http://www.softsailor.com/news/12457-how-to-receivegive-google-mystery-missions-and-why-they-are-fun.html Tech Source]</ref> Other examples of possible missions include telling someone you love them, sending someone a get well card, mailing a banana to someone, etc. There are also references to [[MyLifeIsAverage|MLIA]]. Due to the high number of posted missions involving phone numbers, Mystery Seeker received enough complaints to remove phone numbers from the site. However, the developers are testing Mystery Missions Beta in order to allow the continuance of missions.\n\nA number of phrases yield intentional responses ([[easter egg (media)|easter egg]]s).\n\nIn November 2009 Mystery Seeker had 440,000 unique visitors,<ref>[http://siteanalytics.compete.com/mysterygoogle.com/ mysterygoogle.com UVs for November 2012 | Compete]</ref> making it one of the most highly trafficked social entertainment sites online.\n\nGoogle has not commented on any possible connection to the site.<ref name="Chivers 2009" /> The [[domain name]] \'\'www.mysterygoogle.com\'\' is registered to a private registrant {{As of|2009|10|2|lc=on}}.<ref>http://whois.domaintools.com/mysterygoogle.com</ref>\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* [http://www.mysteryseeker.com/ Mystery Seeker site]\n\n[[Category:Websites]]\n[[Category:Internet search]]']
['Search/Retrieve via URL', '8465350', '{{MOS|date=September 2015|reason=OMG! =)}}\n\'\'\'Search/Retrieve via URL\'\'\' (\'\'\'SRU\'\'\') is a standard search protocol for [[Internet search]] queries, utilizing [[Contextual Query Language]] (CQL), a standard query syntax for representing queries.\n\nSamplecode of a complete answer for this SRU Query-URL:\nhttp://philosophy-science-humanities-controversies.com/XML/sru.php?version=1.1&operation=searchRetrieve&query=dc.title=Darwinism \n\n<pre>\n<?xml version="1.0"?>\n<sru:searchRetrieveResponse xmlns:sru="http://www.loc.gov/zing/srw/" xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/" xmlns:xcql="http://www.loc.gov/zing/cql/xcql/" xmlns:dc="http://purl.org/dc/elements/1.1/">\n  <sru:version>1.1</sru:version>\n  <sru:numberOfRecords>4</sru:numberOfRecords>\n  <sru:records>\n    <sru:record>\n      <sru:recordSchema>info:srw/schema/1/dc-v1.1</sru:recordSchema>\n      <sru:recordPacking>XML</sru:recordPacking>\n      <sru:recordData>\n        <sru:dc>\n          <sru:title>Darwinism</sru:title>\n          <sru:creator>Dennett</sru:creator>\n          <sru:subject>The rule of the Local is a basic principle of Darwinism -  it corresponds to the principle that there is no Creator, no intelligent foresight. I 262</sru:subject>\n        </sru:dc>\n      </sru:recordData>\n      <sru:recordNumber>1</sru:recordNumber>\n    </sru:record>\n    <sru:record>\n      <sru:recordSchema>info:srw/schema/1/dc-v1.1</sru:recordSchema>\n      <sru:recordPacking>XML</sru:recordPacking>\n      <sru:recordData>\n        <sru:dc>\n          <sru:title>Darwinism</sru:title>\n          <sru:creator>McGinn</sru:creator>\n          <sru:subject>Design argument/William Paley: organisms have a brilliant design: We have not designed them, so we have to assume that a foreign intelligence did it. Let s call this intelligence "God". So God exists. II 98\nDarwinVsPaley: intelligent design does not require a Creator. Selection is sufficient. II 98\nMind/consciousness/evolution/McGinn: evolution does not explain consciousness! nor sensations. II 99\n Reason: sensation and consciousness cannot be explained through the means of Darwinian principles and physics, because if selection were to explain how sensations are supposed to be created by it, it must be possible to mold the mind from matter. II 100\n (s) Consciousness or sensations would have to be visible for selection! (Similar GouldVsDawkins)</sru:subject>\n        </sru:dc>\n      </sru:recordData>\n      <sru:recordNumber>2</sru:recordNumber>\n    </sru:record>\n    <sru:record>\n      <sru:recordSchema>info:srw/schema/1/dc-v1.1</sru:recordSchema>\n      <sru:recordPacking>XML</sru:recordPacking>\n      <sru:recordData>\n        <sru:dc>\n          <sru:title>Darwinism</sru:title>\n          <sru:creator>Putnam</sru:creator>\n          <sru:subject>Rorty: Darwinism / Putnam: he does noit like the image of man as a more complicated animal  (scientistic and reductionist physicalism).\nRorty VI 63</sru:subject>\n        </sru:dc>\n      </sru:recordData>\n      <sru:recordNumber>3</sru:recordNumber>\n    </sru:record>\n    <sru:record>\n      <sru:recordSchema>info:srw/schema/1/dc-v1.1</sru:recordSchema>\n      <sru:recordPacking>XML</sru:recordPacking>\n      <sru:recordData>\n        <sru:dc>\n          <sru:title>Darwinism</sru:title>\n          <sru:creator>Rorty</sru:creator>\n          <sru:subject>Darwinism/Rorty provides a useful vocabulary. "Darwinism": for me is a fable about human beings as animals with special skills and organs. But these organs and skills are just as little in a representational relation to the world as the anteater s snout. VI 69 ff\nDarwinism/Rorty: it demands that we consider our doing and being as part of the same continuum, which also includes the existence of amoebae, spiders and squirrels. One way to do that is to say that our experience is just more complex. VI 424</sru:subject>\n        </sru:dc>\n      </sru:recordData>\n      <sru:recordNumber>4</sru:recordNumber>\n    </sru:record>\n  </sru:records>\n</sru:searchRetrieveResponse>\n</pre>\n==See also==\n* [[Search/Retrieve Web Service]]\n\n==External links==\n* [http://www.loc.gov/standards/sru/ Search/Retrieve via URL] at [[Library of Congress]]\n* http://www.philosophy-science-humanities-controversies.com/XML/index.php Sample Page from the Lexicon of Arguments.\n* http://philosophy-science-humanities-controversies.com/XML/sru.php?version=1.1&operation=searchRetrieve&query=dc.title=Darwinism This is a complete example with query and answer.\n{{Internet search}}\n\n{{DEFAULTSORT:Search Retrieve via URL}}\n[[Category:Internet search]]\n[[Category:Information retrieval techniques]]\n[[Category:Uniform Resource Locator]]\n\n{{web-stub}}']
['Online search', '31385661', '\'\'\'Online search\'\'\' is the process of interactively searching for and retrieving requested information via a computer from [[database]]s that are [[online]].<ref name="whatis?">{{cite journal|last1=Hawkins|first1= Donald T.|last2= Brown|first2= Carolyn P.|date=Jan 1980|title=What Is an Online Search?|journal=Online|volume=4|issue=1|pages=12–18|id=Eric:EJ214713| accessdate=2011-04-04}}</ref> Interactive searches became possible in the 1980s with the advent of faster databases and [[smart terminal]]s.<ref name="whatis?"/> In contrast, [[computerized batch searching]] was prevalent in the 1960s and 1970s.<ref name="whatis?"/> Today, searches through [[web search engine]]s constitute the majority of online searches.\n\nOnline searches often supplement reference transactions.{{cn|date=June 2015}}\n\n==References==\n{{reflist}}\n\n{{Internet search}}\n\n[[Category:Internet terminology]]\n[[Category:Information retrieval genres]]\n[[Category:Internet search| ]]\n\n{{web-stub}}']
['Text Retrieval Conference', '1897206', '{{Other uses of|TREC|TREC (disambiguation)}}\nThe \'\'\'Text REtrieval Conference\'\'\' (\'\'\'TREC\'\'\') is an ongoing series of [[workshop]]s focusing on a list of different [[information retrieval]] (IR) research areas, or \'\'tracks.\'\' It is co-sponsored by the [[National Institute of Standards and Technology]] (NIST) and the [[Intelligence Advanced Research Projects Activity]] (part of the office of the [[Director of National Intelligence]]), and began in 1992 as part of the [[DARPA TIPSTER Program|TIPSTER Text program]]. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale \'\'evaluation\'\' of [[text retrieval]] methodologies and to increase the speed of lab-to-product [[technology transfer|transfer of technology]].\n\nEach track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable [[Features (pattern recognition)|features]]. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.\n\n== Tracks ==\n\n===Current tracks===\n\'\'New tracks are added as new research needs are identified, this list is current for TREC 2016.\'\'<ref>http://trec.nist.gov/pubs/call2016.html</ref>\n* [http://www.trec-cds.org/ Clinical Decision Support Track] - \'\'\'Goal:\'\'\' to investigate techniques for linking medical cases to information relevant for patient care\n* [http://sites.google.com/site/treccontext/ Contextual Suggestion Track] - \'\'\'Goal:\'\'\' to investigate search techniques for complex information needs that are highly dependent on context and user interests.\n* [http://trec-dd.org/ Dynamic Domain Track] - \'\'\'Goal:\'\'\'  to investigate  domain-specific search algorithms that adapt to the dynamic information needs of professional users as they explore in complex domains. \n* [http://trec-liveqa.org/ LiveQA Track] -  \'\'\'Goal:\'\'\'  to  generate answers to real questions originating from real users via a live question stream, in real time. \n* [http://trec-open-search.org/ OpenSearch Track] - \'\'\'Goal:\'\'\' to  explore an evaluation paradigm for IR that involves real users of operational search engines. For this first year of the track the task will be ad hoc Academic Search.\n* Real-Time Summarization Track -  \'\'\'Goal:\'\'\' to explore techniques for constructing real-time update summaries from social media streams in response to users\' information needs. \n* [http://www.cs.ucl.ac.uk/tasks-track-2016/ Tasks Track] - \'\'\'Goal:\'\'\'  to test whether systems can induce the possible tasks users might be trying to accomplish given a query. \n* [http://trec-total-recall.org/ Total Recall Track] -  \'\'\'Goal:\'\'\': to evaluate methods to achieve very high recall, including methods that include a human assessor in the loop.\n\n===Past tracks===\n* Chemical Track - \'\'\'Goal:\'\'\' to develop and evaluate technology for large scale search in [[chemistry]]-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically [[patent search]]ers and chemists.\n* [[Crowdsourcing]] Track - \'\'\'Goal:\'\'\' to provide a collaborative venue for exploring [[crowdsourcing]] methods both for evaluating search and for performing search tasks. \n* [[TREC Genomics|Genomics Track]] - \'\'\'Goal:\'\'\' to study the retrieval of [[Genomics|genomic]] data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.\n* [[Enterprise search|Enterprise Track]] - \'\'\'Goal:\'\'\' to study search over the data of an organization to complete some task. Last ran on TREC 2008.\n* Entity Track - \'\'\'Goal:\'\'\' to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.\n* [[Cross-language information retrieval|Cross-Language]] Track - \'\'\'Goal:\'\'\' to investigate the ability of retrieval systems to find documents topically regardless of source language.\n* [[Federated search|FedWeb]] Track - \'\'\'Goal:\'\'\' to select best resources to forward a query to, and merge the results so that most relevant are on the top.\n* Federated Web Search Track - \'\'\'Goal:\'\'\' to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.\n* Filtering Track - \'\'\'Goal:\'\'\' to binarily decide retrieval of new incoming documents given a stable [[information need]].\n* HARD Track - \'\'\'Goal:\'\'\' to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.\n* Interactive Track - \'\'\'Goal:\'\'\' to study user [[Human-computer interaction|interaction]] with text retrieval systems.\n* Knowledge Base Acceleration Track - \'\'\'Goal:\'\'\' to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.\n* Legal Track - \'\'\'Goal:\'\'\' to develop search technology that meets the needs of lawyers to engage in effective [[discovery (law)|discovery]] in digital document collections.\n* Medical Records Track - \'\'\'Goal:\'\'\' to explore methods for searching unstructured information found in patient medical records. \n* [[Microblog]] Track - \'\'\'Goal:\'\'\' to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter. \n* Novelty Track - \'\'\'Goal:\'\'\' to investigate systems\' abilities to locate new (i.e., non-redundant) information.\n* [[Question answering|Question Answering]] Track - \'\'\'Goal:\'\'\' to achieve more [[information retrieval]] than just [[document retrieval]] by answering factoid, list and definition-style questions.\n* Robust Retrieval Track - \'\'\'Goal:\'\'\' to focus on individual topic effectiveness.\n* [[Relevance feedback|Relevance Feedback]] Track - \'\'\'Goal:\'\'\' to further deep evaluation of relevance feedback processes.\n* Session Track - \'\'\'Goal:\'\'\' to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.\n* [[Spam (electronic)|Spam]] Track - \'\'\'Goal:\'\'\' to provide a standard evaluation of current and proposed [[spam filter]]ing approaches.\n* Temporal Summarization Track - \'\'\'Goal:\'\'\' to develop systems that allow users to efficiently monitor the information associated with an event over time.\n* [[Terabyte]] Track - \'\'\'Goal:\'\'\' to investigate whether/how the [[information retrieval|IR]] community can scale traditional IR test-collection-based evaluation to significantly large collections.\n* [[Video search engine|Video]] Track - \'\'\'Goal:\'\'\' to research in automatic segmentation, [[index (search engine)|index]]ing, and content-based retrieval of [[digital video]].\n:In 2003, this track became its own independent evaluation named [[TRECVID]].\n* Web Track - \'\'\'Goal:\'\'\' to explore information seeking behaviors common in general web search.\n\n===Related events===\nIn 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called [http://research.nii.ac.jp/ntcir/ NTCIR] ([[National Institute of Informatics|NII]] Test Collection for IR Systems), and in 2000, a European counterpart was launched, called [http://www.clef-campaign.org/ CLEF] (Cross Language Evaluation Forum).\n\n== Conference contributions to search effectiveness==\n\nNIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled.<ref>[http://trec.nist.gov/overview.html From TREC homepage: "... effectiveness approximately doubled in the first six years of TREC"]</ref> The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of [http://trec.nist.gov/pubs.html publications]. Technology first developed in TREC is now included in many of the world\'s commercial [[search engine]]s.  An independent report by RTII found that "about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia."\n<ref>{{cite web|url=http://rti.org/page.cfm?objectid=75E125DC-5056-B100-31A5A6BDE897DE6D |title=NIST Investment Significantly Improved Search Engines |publisher=Rti.org |date= |accessdate=2012-01-19}}</ref>\n<ref>http://www.nist.gov/director/planning/upload/report10-1.pdf</ref>\n\nWhile one study suggests that the state of the art for ad hoc search has not advanced substantially in the past decade,<ref>Timothy G. Armstrong, Alistair Moffat, William Webber, Justin Zobel.  Improvements that don\'t add up: ad hoc retrieval results since 1998.  CIKM 2009.  ACM.</ref> it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.\n\nThe test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.\n\nTREC systems often provide a baseline for further research.  Examples include:\n* [[Hal Varian]], Chief Economist at [[Google]], says \'\'Better data makes for better science. The history of information retrieval illustrates this principle well," and describes TREC\'s contribution.<ref>[http://googleblog.blogspot.com/2008/03/why-data-matters.html Why Data Matters]</ref>\n* TREC\'s Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.<ref>[http://blogs.the451group.com/information_management/2009/01/29/standards-in-e-discovery-%E2%80%93-walking-the-walk/ The 451 Group: Standards in e-Discovery -- walking the walk]</ref>\n* The [[IBM]] researcher team building [[IBM Watson]] (aka [[DeepQA]]), which beat the world\'s best [[Jeopardy!]] players,<ref>[http://www-03.ibm.com/press/us/en/presskit/27297.wss IBM and Jeopardy! Relive History with Encore Presentation of Jeopardy!: The IBM Challenge]</ref> used data and systems from TREC\'s QA Track as baseline performance measurements.<ref>[http://www.aaai.org/AITopics/articles&columns/Ferrucci-Watson2010.pdf David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welt. \'\'\'Building Watson:  An Overview of the DeepQA Project\'\'\']</ref>\n\n== Participation ==\nThe conference is made up of a varied, international group of researchers and developers.<ref>{{cite web|url=https://wiki.ir-facility.org/index.php/Participants |title=Participants - IRF Wiki |publisher=Wiki.ir-facility.org |date=2009-12-01 |accessdate=2012-01-19}}</ref><ref>http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf</ref><ref>{{cite web|url=http://trec.nist.gov/pubs/trec17/appendices/million.query.results.html |title=Text REtrieval Conference (TREC) TREC 2008 Million Query Track Results |publisher=Trec.nist.gov |date= |accessdate=2012-01-19}}</ref> In 2003, there were 93 groups from both academia and industry from 22 countries participating.\n\n==References==\n{{reflist}}\n\n== External links ==\n*[http://trec.nist.gov/ TREC website at NIST]\n*[http://www.nist.gov/itl/div894/894.02/related_projects/tipster/ TIPSTER]\n*[http://www.amazon.com/TREC-Experiment-Evaluation-Information-Electronic/dp/0262220733/ The TREC book (at Amazon)]\n\n[[Category:Information retrieval organizations]]\n[[Category:Computational linguistics]]\n[[Category:Natural language processing]]\n[[Category:Computer science competitions]]']
['DtSearch', '14388058', '{{Lowercase}}\n\n{{Infobox company |\n  name   = dtSearch Corp. |\n  slogan = "The Smart Choice for Text Retrieval since 1991" |\n  type   =  Private company |\n  foundation     = 1991 |\n  location       = [[Bethesda, Maryland|Bethesda]], [[Maryland]], [[United States|US]] |\n  key_people     = David Thede, President |\n  industry       = [[Software]] |\n\n  homepage       = [http://www.dtsearch.com/ www.dtsearch.com]\n}}\n\n\'\'\'dtSearch Corp.\'\'\' is a [[software company]] which specializes in [[text retrieval]] software. It was founded in 1991, and is headquartered in [[Bethesda, Maryland|Bethesda]], [[Maryland]]. Its current range of software includes products for enterprise [[desktop search]], Intranet/Internet [[spidering]] and search, and [[search engines]] for developers ([[Software development kit|SDK]]) to integrate into other software applications.\n\n==History==\ndtSearch Corp was founded by David Thede;<ref>[http://www.lets-talk-computers.com/guests/dtsearch/6.2/index.htm Lets talk computers - Interview May 31, 2003]</ref><ref>[https://www.google.com/patents/US6782380 Method and system for indexing and searching contents of extensible mark-up language(XML) documents US 6782380 B1]</ref><ref>[https://www.google.com/patents/US7464098 Method for rapidly searching elements or attributes or for rapidly filtering fragments in binary representations of structured, for example, XML-based documents US 7464098 B2]</ref> the company started research and development in text retrieval in 1988  and incorporated in Virginia in 1991 as D T Software. Marketing of dtSearch 1.0 a DOS Text Retrieval software product began in the first quarter of 1991. Initially it was distributed as [[Association of Shareware Professionals]]-approved [[shareware]]. The product was featured in an article entitled "Text Retrieval Software" in an early edition of \'\'[[PC Magazine]]\'\'<ref>"Text Retrieval Software". (July 1992). [[PC Magazine]] (UK ed)</ref> as a shareware alternative to the commercial products reviewed; these included [[ISYS Search Software|ISYS]], [[ZyLAB Technologies|ZyIndex]], Strix, [[askSam]], [[ideaList]], Assassin PC, [[Folio Corporation|Folio Views]] and Lotus SmartText.\n\nIn the first few years after its initial release, dtSearch was an end-user application only. Then, in 1994, [[Symantec]] approached dtSearch about including its search technology into one of the first applications for 32-bit Windows; the dtSearch end-user application was developed into a [[Dynamic-link library]] (DLL) which Symantec embedded in Norton Navigator, which was released alongside Microsoft’s initial release of its 32-bit Windows operating system, [[Windows 95]].<ref>[http://www.processor.com/editorial/article.asp?article=articles%2Fp3012%2F11p12%2F11p12.asp dtSearch Performs Incredible Feats. Processor Mag. March 21, 2008]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\nIn 2007 the company was listed in the [[EContent]] 100 list, a list of companies that matter most in the digital content industry.<ref>[http://www.econtentmag.com/Articles/ArticleReader.aspx?ArticleID=40160&PageNum=22007 EContent 100 list]</ref>\n\n==Products==\nThe current (v 7.7) product range is [[Unicode]]-based and has an index that can handle over 1 [[terabyte|TB]] of data per index.\n\n*dtSearch Desktop with Spider -  Windows client Desktop search software (32 and 64 bit indexers)\n*dtSearch Network with Spider -  as dtSearch Desktop but licensed for Network use (32 and 64 bit indexers)\n*dtSearch Web with Spider -  browser based search-only client for Intranet/Internet usage based on Microsoft IIS (32 and 64 bit indexers)\n*dtSearch Engine with Spider - SDK with C++, .NET, COM, Java, Delphi APIs (32-bit and 64-bit versions)\n*dtSearch Engine for Linux - SDK with C++ and Java APIs\n*dtSearch Publish <ref>[http://www.law.com/jsp/lawtechnologynews/PubArticleLTN.jsp?id=1202463957873&slreturn=1&hbxlogin=1 dtSearch Publish for EDD Production Law Technology News July 29, 2010]</ref> - a search front-end for CD\\DVD publishing (32 and 64 bit indexers)\n\n==Licensing Partners==\n* COMPANY:  PRODUCT\n* Docupoint, LLC:  DrawingSearcher\n* FileHold Systems Inc.: FileHold Document Management System\n* ...\n\n==See also==\n* [[Enterprise search]]\n* [[List of enterprise search vendors]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.dtsearch.com/ Company Website]\n*[http://www.searchtools.com/tools/dtsearch.html Product description on SearchTools.com ]\n*[http://www.windowsitpro.com/article/desktop-management/dtsearch-7-desktop-with-spider.aspx The index is mightier than the sword - Windows IT Pro. August 27, 2008]\n*[http://www.infoworld.com/t/platforms/desktop-search-gets-down-business-610 Desktop search gets down to business - InfoWorld. September 01, 2005]\n*[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC150357/ Integrating Query of Relational and Textual Data in Clinical Databases - J Am Med Inform Assoc. 2003 Jan–Feb]\n*[http://radiographics.rsna.org/content/29/5/1233.full.pdf Informatics in Radiology. Render: An Online Searchable Radiology Study Repository - RadioGraphics 2009; 29:1233–1246] \n*[http://jms.ndmctsgh.edu.tw/fdarticlee%5C2606199.pdf Use Of Intelligent Computer Search for the Patterns of Abnormal Lymphatic Uptake by F-18 FDG PET in Primary Lung Cancers - J Med Sci 2006;26(6):199-204]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n{{DEFAULTSORT:Dtsearch Corp.}}\n[[Category:Desktop search engines]]\n[[Category:Information retrieval organizations]]\n[[Category:Software companies based in Maryland]]']
['Dandelon.com', '41725036', '\n{{more footnotes|date=January 2014}}\n\'\'\'Dandelon.com\'\'\' is a collaborative community of libraries in multiple countries as well as a [[Search engine (computing)|search engine]], a search or discovery service, a library information system for the academic community. It is additionally a platform allowing registered libraries to exchange library catalogue enrichment data: tables of content of monographs, deep indexing data, cover pages and bibliographic descriptions of articles published in periodicals, with abstracts and / or full texts provided for part of the items. The domain name was created in 2004. It is derived from the plant [[Taraxacum|dandelion]]. The name is an allusion to the flower\'s worldwide occurrence: It is thought to spread around the world as easily as human words and thoughts. Dandelon\'s aim is to uncover knowledge assets for students from around the world. It is free of charge for private use and without user tracking or advertising.<ref>[http://www.dandelon.com Dandelon.com<!-- Bot generated title -->]</ref>\n\nTraditionally, the number of searchable relevant subject words comes up to about five semantically different subjects words, located in titles and generated by human indexing. A book registered at dandelon.com typically is assigned between 20 and 500 subject words depending on the size of the book and the knowledge domain. Based on this extended set of terms representing each library item, queries can be more specific, and relevance ranking can be more efficient. Dandelon.com also expands user queries by adding closely related words (default: synonyms and translations, optionally: narrower terms) from multilingual [[Thesaurus|thesauri]] from various knowledge domains.\n\nSearch results can be restricted to a specific library. Automatic backlinks to the related library management system allows online access or requesting a book. Dandelon.com does not replace library management systems, it is an additional option for searching and first of all a platform for data exchange between libraries associated with its community. Its user interface supports a number of languages, and it provides content in about 130 languages.\n\nThe core of dandelon.com is the content production software ““intelligentCAPTURE mobile”” employed by all member libraries. It reads from and sends data to each library management system, receiving text content via digitization and [[optical character recognition]] (OCR) for close to 200 languages or via native digital content import. Additionally, it automatically extracts major subject words, which are translated into 60 languages by machine translation. Computers and scanners can be placed in a special mobile furniture to be used between shelves and narrow compactus.\n\nThe provider of production software and search and distribution services is the German-based company AGI-Information Management Consultants <ref>[http://www.agi-imc.de<!-- Bot generated title -->]</ref> as well as the hosting center of  GBV - Gemeinsamer Bibliotheksverbund - , a state-owned German library service center for more than 800 libraries.<ref>[http://www.gbv.de<!-- Bot generated title -->]</ref> The solution was invented in 2001 by Manfred Hauer of AGI and Karl Raedler from Vorarlberger Landesbibliothek, Austria.<ref>[http://vlb.vorarlberg.at<!-- Bot generated title -->]</ref> Dandelon.com shares part of its data with GBV. GBV, in turn, exchanges some of its catalogue enrichment data with OCLC, [[WorldCat]] and other service centers. HEBIS,<ref>[http://www.hebis.de<!-- Bot generated title -->]</ref> another state-owned service center shares with the German National Library. German National Library charges fees for enrichment content.<ref>[http://www.dnb.de/kataloganreicherung<!-- Bot generated title -->]</ref> AGI and a number of the producing libraries have been pioneering catalogue enrichment in Europe since 2001 and form one of the largest communities of producers of digitalized tables of content of monographs in Europe. In 2013 close to 2&nbsp;million tables of contents were digitalized, not all of which are available on dandelon.com for the general public. The large collection produced for the [[German National Library]] is not yet shared and was announced for public use in 2014. Dandelon.com and intelligentCAPTURE are [[IBM Lotus Domino|IBM Domino and Notes]] applications. Dandelon.com runs [[Apache Lucene]] as retrieval engine.\n\n== References ==\n{{Reflist}}\n*Manfred Hauer: 2012 "[http://www.agi-imc.de/internet.nsf/dda9df579aa6429dc12567f5004ad7ed/659e168d74f0bc38c12579bb004ea5b8/$FILE/HAUER_SLA_Bahrain_2012_gb.pdf Web 2.0: Which features are wanted by academic library clients? A HEBIS Survey Report]" (PDF; 172&nbsp;kB) \'\'Gulf special library association (SLA)\'\', Conference Proceeding - on CD\n*Nienerza,Heike / Sunckel, Bettina / Meier, Berthold: 2011 "[http://www.degruyter.com/view/j/abitech.2011.31.issue-3/ABI.2011.020/ABI.2011.020.xml?format=INT Unser Katalog soll besser werden! Kataloge und Portale im Web-2.0-Zeitalter. Ergebnisse einer Online-Umfrage im HeBIS-Verbund]"  \'\'ABI-Technik\'\', De Gruyter, Berlin, Issue 31, pp. 130-149, DOI: 10.1515/ABI.2011.020\n*Manfred Hauer: 2013 "[http://www.agi-imc.de/internet.nsf/26efb65f701b0871c125751a00413614/3d26118ce2a8ebccc1257b1800356e8b?OpenDocument Zur Bedeutung normierter Terminologien in Zeiten moderner Sprach- und Information-Retrieval-Technologien]" (PDF; 205&nbsp;kB) \'\'[http://www.degruyter.com/view/j/abitech] ABI-Technik\'\', De Gruyter, Berlin, issue 1, pp. 2-6\n*Manfred Hauer, Rainer Diedrichs: 2010 "[http://www.agi-imc.de/internet.nsf/26efb65f701b0871c125751a00413614/3f191bb231f0d57ec1257749004a9e7d/$FILE/Kataloganreicherung_in_Europa_2010_c.pdf  Kataloganreicherung in Europa]" (PDF; 525&nbsp;kB) \'\'Buch und Bibliothek [http://www.b-u-b.de/]\'\', issue 5, pp.&nbsp;394–397\n*Manfred Hauer: 2005 \'\'[http://www.agi-imc.de/internet.nsf/94280a18b17ee318c12567d2003c3bb2/3267dae6428c5f02c125711600527ffd?OpenDocument/Vergleich der Retrievalleistungen von Bibliothekskatalogen gegen erweiterte und neue Konzept. Benchmarking: Google Scholar, dandelon.com, Vorarlberger Landesbibliothek, weitere OPACs.] In: [http://www.degruyter.com/view/j/abitech] ABI-Technik, De Gruyter, Berlin, December, pp.&nbsp;295–301.\n\n[[Category:Information retrieval organizations]]\n[[Category:Information retrieval systems]]\n[[Category:Digital library projects]]']
['Smartlogic', '39083726', '{{Multiple issues|\n{{advert|date=June 2015}}\n{{COI|date=June 2015}}\n{{notability|Companies|date=November 2015}}\n}}\n{{Infobox company |\nname = Smartlogic  |\nlogo =  |\nslogan = "The Content Intelligence Company" |\ntype = [[Privately held company|Private]] |\nfoundation = 2006 |\nlocation = United States, UK | \narea_served = Global |\nindustry = [[Information retrieval]] |\nproducts = Semaphore Cloud, Semaphore Ontology Editor, Semaphore Classification Server, Semaphore Semantic Enhancement Server, Advanced Language Packs, Search Appliance Framework, Text Miner, Classification Review Tool, Classification Analysis Tool  | \nnum_employees  = 55|\nhomepage = http://www.smartlogic.com\n}}\n\n\'\'\'Smartlogic \'\'\' is a [[software company]] which specializes in developing [[information retrieval]], [[text analytics]] and [[knowledge management]] solutions.\n\n==History==\nSmartlogic was founded in the United Kingdom in 2006. It is a privately held company and has offices in San Jose, CA; Alexandria, VA; Cambridge, MA and London, UK. The company develops and sells a suite of products; Semaphore Ontology Editor, Classification Server, Advanced Language Packs, Semantic Enhancement Server, Text Miner, Classification Review tool, and Classification Analysis tool.\n\n==Products==\n\n===Semaphore Ontology Editor===\nSemaphore Ontology Editor is a web-based tool used to build taxonomies, ontologies, controlled vocabularies as well as other knowledge organization systems. Models are used by organizations to enhance the capabilities of enterprise search engines,<ref>[http://www.cmswire.com/events/item/webinar-leverage-metadata-to-drive-critical-business-processes-022370.php] CMSWire Leverage Metadata to Drive Critical Business Processes</ref> content management and workflow systems deployed by clients to augment and enhance their investment.\n\n===Semaphore Classification Server===\nSemaphore Classification Server uses the model structure from Semaphore Ontology Editor and auto classifies unstructured information assets by applying metadata tags to the unstructured information.\n\n===Semaphore Advanced Language Packs===\n\n===Semantic Enhancement Server===\n\n==Integrations==\nSemaphore integrates with [[Microsoft Sharepoint]],<ref>[http://www.cmswire.com/cms/information-management/sharepoint-2013-office-365-get-semantic-search-with-smartlogic-semaphore-018353.php] CMSWire SharePoint 2013 Office 365 Get Semantic Search with Smartlogic Semaphore</ref> [[Google Search Appliance]],<ref>[https://www.google.com/enterprise/marketplace/viewVendorListings?vendorId=33&pli=1] Google Enterprise Catalogue</ref> [[Apache Solr]],<ref>[http://www.flatironssolutions.com/blog/alfresco-semaphore-integration/] Alfresco-Semaphore Integration</ref> FAST ESP<ref>[http://arnoldit.com/wordpress/2009/10/23/smartlogic-and-fast-esp-integration/] Stephen E. Arnold - Beyond Search</ref> and others.\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.smartlogic.com/ Smartlogic]\n\n[[Category:Software companies of the United Kingdom]]\n[[Category:Information retrieval organizations]]\n[[Category:Analytics companies]]\n[[Category:Knowledge management]]']
['Information Retrieval Facility', '16635934', "{{Advert|date=May 2012}}\n\n[[Image:IRF logo 350x350.png|thumb|200px|right|IRF logo]]\n\nThe '''Information Retrieval Facility''' ('''IRF'''), founded 2006 and located in [[Vienna]], [[Austria]], was a research platform for networking and collaboration for professionals in the field of [[information retrieval]]. It ceased operations in 2012.\n\nThe IRF had members in the following categories:\n\n* Researchers in [[information retrieval]] (IR) or related scientific areas\n* Industrial/corporate information management professionals\n* Patent authorities and governmental institutions\n* Students of one of the above\n\n==The Scientific Board==\n'''Maristella Agosti''', Professor, [http://www.dei.unipd.it/wdyn/?IDsezione=1 Department of Information Engineering, University of Padova]\n\n'''Gerhard Budin''', Director of the [http://transvienna.univie.ac.at/forschung/professuren/dr-gerhard-budin/ Center of Translation Studies at the University of Vienna],\nDirector of the [http://www.oeaw.ac.at/icltt/ Department of Corpuslinguistics and Text Technology, Austrian Academy of Sciences]\n\n'''Jamie Callan''', Professor, [http://www.cs.cmu.edu/~callan/Bio.html Language Technologies Institute, CMU, Carnegie Mellon University]\n\n'''Yves Chiaramella''', Professor Emeritus, [http://www-clips.imag.fr/mrim/User/yves.chiaramella/ Department of Computer Science and Applied Mathematics, Joseph Fourier University]\n\n'''Kilnam Chon''', Professor, Computer Science Department, [http://cosmos.kaist.ac.kr/salab/professor/index02.html Korea Advanced Institute of Science and Technology (KAIST)]\n\n'''W. Bruce Croft''', Distinguished Professor, [http://ciir.cs.umass.edu/personnel/croft.html Department of Computer Science and Director Center for Intelligent IR University of Massachusetts Amherst]\n\n'''Hamish Cunningham''', Research Professor, [http://www.dcs.shef.ac.uk/~hamish/ Computer Science Department University Sheffield]\n\n'''Norbert Fuhr''', Chairman of the Scientific Board, Professor, [http://www.is.informatik.uni-duisburg.de/staff/fuhr.html Institute of Informatics and Interactive Systems University Duisburg-Essen]\n\n'''David Hawking''', Science Leader, Project Leader, [http://es.csiro.au/people/Dave/ CSIRO ICT Centre]\n\n'''Noriko Kando''', Professor, [http://www.nii.ac.jp/index.shtml.en Software Engineering Research, Software Research Division, National Institute of Informatics (NII)]\n\n'''Arcot Desai Narasimhalu''', Associate Dean, [http://www.sis.smu.edu.sg/faculty/infosys/arcotdesai.asp School of Information Systems Singapore Management University]\n\n'''John Tait''', Chief Scientific Officer of the IRF, [http://www.johntait.net/ Until July 2007 Professor of Intelligent Information Systems and Associate Dean of the School of Computing and Technology]\n\n'''Benjamin T'sou''', Director, [http://www.cityu.edu.hk/ Language Information Sciences Research Centre, City University of Hong Kong]\n\n'''[[C. J. van Rijsbergen|C.J. van Rijsbergen]]''',\n[http://www.dcs.gla.ac.uk/~keith/ Dept. Computer Science at the University of Glasgow]\n\n==Scientific goals==\n\n* Modelling innovative and specialised information retrieval systems for global patent document collections.\n* Investigating and developing an adequate technical infrastructure that allows interactive experimentation with formal, mathematical retrieval concepts for very large-scale document collections.<\n* Studying the usability of multi modal user-interfaces to very large-scale information retrieval systems.\n* Integrating real users with actual information needs into the research process of modelling information retrieval systems to allow accurate performance evaluation.\n* Ability to create different views of patent data depending on the focus of the information need.\n* Defining standardised methods for benchmarking the information retrieval process in patent document collections.\n* Ability to handle text and non-text parts of a patent in a coherent manner.\n* Designing, experimenting and evaluating search engines able to retrieve structured and semi-structured documents in very large-scale patent collections.\n* Integrating the temporal dimension of patent documents in retrieval strategies.\n* Improving effectiveness and precision of patent retrieval, based on ontologies and natural-language understanding techniques.\n* Refining IR methods that allow unstructured querying by exploiting available structure within the patent documents.\n* Formal (mathematical) identification and specification of relevant business information needs in the field of intellectual property information.\n* Investigating efficient scaling mechanisms for information retrieval taking into account the characteristics of patent data.\n* Investigating and experimenting with computing architectures for very high-capacity information management.\n* Establishing an open [[eScience]] platform that enables a standardised and easy way of creating and performing IR experiments on a common research infrastructure.\n* Discovering and investigating novel use cases and business applications deriving from intellectual property information.\n* Enabling the formal information retrieval, natural language and semantic processing research to grow into the field of applied sciences in the global, industrial context.\n* Development and integration of different information access methods.\n* Research on effective methods for interactive information retrieval.\n\n==Semantic supercomputing==\nCurrent technologies to extract concepts from unstructured documents are extremely computational intensive. To allow interactive experimentation with rich and huge text corpora, the IRF has built a high performance computing environment, into which the latest technological advances have been implemented:\n\n* multi-node clusters (currently 80 cores, up to 1024)\n* highest speed interconnect technology\n* single system image with large compound memory (currently 320 GB, up to 4 TB)\n* fully integrated configurable computing (currently 4 FPGA cores, up to 256)\n\nThe combination of these HPC features to accelerate text mining represents the IRF implementation of semantic supercomputing.\n\n==The World Patent Corpus==\nThe IRF aims to bring state-of-the-art information retrieval technology to the community of patent information professionals. We expect information retrieval (IR) technology to become the focus of information technology very soon. All industry sectors can profit from applying modern and future text mining processes to the special requirements of patent research. Although all ideas and concepts are universally applicable to all sorts of intellectual property information, patents require the most sophistication, and confront us with challenging technical and organisational problems. \nThe entire body of patent-related documents possibly constitutes the largest corpus of compound documents, making it a rewarding target for text mining scientists and end-users alike. What’s more, patents have become a crucial issue, in particular for large global corporations and universities. The industrial users of patent data are among the most demanding and important information professionals. As a consequence, they could benefit the most from technology that relieves the burden of researching the large body of patent information.\n\n== Research collections ==\nThe IRF provides a number of test data collections that have either been developed by the IRF, by one of its members or by third parties. These data collections can be used freely for scientific experimentations.\n\nThe MAtrixware REsearch Collection ([[MAREC]]) is the first standardised patent data corpus for research purposes. It consists of 19 million patent documents in different languages, normalised to a highly specific XML format. The collection has been developed by Matrixware for the IRF.\n\nThe ClueWeb09 collection is a 25 terabyte dataset of about 1 billion web pages crawled in January and February, 2009. It has been created by the Language Technologies Institute at [[Carnegie Mellon University]] to support research on information retrieval and related human language technologies.\n\n==References==\n* [http://www.iwr.co.uk/information-world-review/analysis/2231880/patent-medicine-info-retrievers?page=2 Patent medicine for information retrievers, Information World Review]\n* [http://ecir2008.dcs.gla.ac.uk/industry.html The IRF and its Role in Professional Information Research, ECIR 2008]\n\n==External links==\n* [http://www.ir-facility.org/ Official site: ir-facility.org]\n* [https://www.youtube.com/watch?v=XpXtRu0XfeA YouTube: The future of information retrieval Part1] \n* [https://www.youtube.com/watch?v=dRaTeTaHBsI YouTube: The future of information retrieval Part2]\n\n[[Category:Organizations established in 2006]]\n[[Category:Computer science organizations]]\n[[Category:Information retrieval organizations]]\n[[Category:Education in Vienna]]"]
['EXCLAIM', '8239120', '{{For|the Canadian magazine|Exclaim!}}\nThe \'\'\'EXtensible Cross-Linguistic Automatic Information Machine (EXCLAIM)\'\'\' was an integrated tool for [[cross-language information retrieval]] (CLIR), created at the [[University of California, Santa Cruz]] in early 2006, with some support for more than a dozen languages. The lead developers were Justin Nuger and Jesse Saba Kirchner.\n\nEarly work on CLIR depended on manually constructed parallel corpora for each pair of languages. This method is labor-intensive compared to parallel corpora created automatically. A more efficient way of finding data to train a CLIR system is to use matching pages on the [[World Wide Web|web]] which are written in different languages.<ref>\n{{cite web\n|title=Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web\n|url=http://www.iro.umontreal.ca/%7Enie/Publication/nie-sigir99.pdf\n|format=PDF|publisher=ACM-SIGIR 1999\n|accessdate=2006-12-02\n}}\n</ref>\n\nEXCLAIM capitalizes on the idea of latent parallel corpora on the [[World Wide Web|web]] by automating the alignment of such corpora in various domains. The most significant of these is [[Wikipedia]] itself, which includes articles in [http://meta.wikimedia.org/wiki/Complete_list_of_language_Wikipedias_available 250 languages]. The role of EXCLAIM is to use [[semantics]] and [[linguistics|linguistic]] analytic tools to align the information in these Wikipedias so that they can be treated as parallel corpora. EXCLAIM is also extensible to incorporate information from many other sources, such as the [[Chinese Community Health Resource Center]] (CCHRC).\n\nOne of the main goals of the EXCLAIM project is to provide the kind of computational tools and CLIR tools for [[minority languages]] and [[endangered languages]] which are often available only for powerful or prosperous majority languages.\n\n==Current status==\n\nIn 2009, EXCLAIM was in a beta state, with varying degrees of functionality for different languages. Support for CLIR using the Wikipedia dataset and the most current version of EXCLAIM (v.0.5), including full UTF-8 support and Porter stemming for the English component, was available for the following twenty-three languages:\n\n{| class="wikitable"\n| [[Albanian language|Albanian]]\n|-\n| [[Amharic]]\n|-\n| [[Bengali language|Bengali]]\n|-\n| [[Gothic language|Gothic]]\n|-\n| [[Greek language|Greek]]\n|-\n| [[Icelandic language|Icelandic]]\n|-\n| [[Indonesian language|Indonesian]]\n|-\n| [[Irish language|Irish]]\n|-\n| [[Javanese language|Javanese]]\n|-\n| [[Latvian language|Latvian]]\n|-\n| [[Malagasy language|Malagasy]]\n|-\n| [[Mandarin Chinese]]\n|-\n| [[Nahuatl]]\n|-\n| [[Navajo language|Navajo]]\n|-\n| [[Quechua languages|Quechua]]\n|-\n| [[Sardinian language|Sardinian]]\n|-\n| [[Swahili language|Swahili]]\n|-\n| [[Tagalog language|Tagalog]]\n|-\n| [[Standard Tibetan|Tibetan]]\n|-\n| [[Turkish language|Turkish]]\n|-\n| [[Welsh language|Welsh]]\n|-\n| [[Wolof language|Wolof]]\n|-\n| [[Yiddish]]\n|}\n\nSupport using the Wikipedia dataset and an earlier version of EXCLAIM (v.0.3) is available for the following languages:\n\n{| class="wikitable"\n|-\n| [[Dutch language|Dutch]]\n|-\n| [[Spanish language|Spanish]]\n|}\n\nSignificant developments in the most recent version of EXCLAIM include support for Mandarin Chinese. By developing support for this language, EXCLAIM has added solutions to [[text segmentation|segmentation]] and [[character encoding|encoding]] problems which will allow the system to be extended to many other languages written with non-European orthographic conventions. This support is supplied through the Trimming And Reformatting Modular System ([[TARMS]]) toolkit.\n\nFuture versions of EXCLAIM will extend the system to additional languages. Other goals include incorporation of available latent datasets in addition to the Wikipedia dataset.\n\nThe EXCLAIM development plan calls for an integrated CLIR instrument usable searching from English for information in any of the supported languages, or searching from any of the supported languages for information in English when EXCLAIM 1.0 is released. Future versions will allow searching from any supported language into any other, and searching from and into multiple languages.\n\n==Further applications==\n\nEXCLAIM has been incorporated into several projects which rely on cross-language [[query expansion]] as part of their [[Front and back ends|backend]]s. One such project is a cross-linguistic [[readability]] software generation framework, detailed in work presented at [[Association for Computational Linguistics|ACL 2009]].<ref>{{cite web\n|title=A crosslinguistic readability framework\n|url=http://www.aclweb.org/anthology/W/W09/W09-3103.pdf\n|format=PDF|publisher=ACL-IJNLP 2009\n|accessdate=2009-09-04\n}}\n</ref>\n\n==Notes and references==\n\n{{reflist}}\n\n==External links==\n*[http://www.soe.ucsc.edu/~jnuger/cgi-bin/exclaim.cgi EXCLAIM Website] (dead link)\n*[http://www.w3.org/DesignIssues/Semantic.html Semantic Web Roadmap]\n*[http://www.cchphmo.com/cchrchealth/index_E.html Chinese Cultural Health Resource Center]\n*[http://ju-st.in/ Justin Nuger\'s professional webpage]\n\n{{DEFAULTSORT:Exclaim}}\n[[Category:Information retrieval systems]]']
['Agrep', '308939', '{{lowercase|title=agrep}}\n{{Infobox software\n| name                   = agrep\n| logo                   = <!-- Image name is enough -->\n| logo caption           = \n| logo_size              = \n| logo_alt               = \n| screenshot             = <!-- Image name is enough -->\n| caption                = \n| screenshot_size        = \n| screenshot_alt         = \n| collapsible            = \n| developer              = {{Plainlist|\n* [[Udi Manber]]\n* Sun Wu\n}}\n| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = \n| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\n| status                 = \n| programming language   = C\n| operating system       = {{Plainlist|\n* [[Unix-like]]\n* [[OS/2]]\n* [[DOS]]\n* [[Microsoft Windows|Windows]]\n}}\n| platform               = \n| size                   = \n| language               = \n| language footnote      = \n| genre                  = [[Pattern matching]]\n| license                = [https://raw.githubusercontent.com/Wikinaut/agrep/master/COPYRIGHT ISC open source license]\n| standard               = \n| website                = {{URL|http://www.tgries.de/agrep}}\n}}\n\n\'\'\'agrep\'\'\' (approximate [[grep]]) is an [[open-source]] [[approximate string matching]] program, developed by [[Udi Manber]] and Sun Wu between 1988 and 1991, for use with the [[Unix]] operating system. It was later ported to [[OS/2]], [[DOS]], and [[Microsoft Windows|Windows]].\n\nIt selects the best-suited algorithm for the current query from a variety of the known fastest (built-in) [[string searching algorithm]]s, including Manber and Wu\'s [[bitap algorithm]] based on [[Levenshtein distance]]s.\n\nagrep is also the [[Search engine (computing)|search engine]] in the indexer program [[GLIMPSE]]. agrep is under a free [[ISC License]].<ref>[http://webglimpse.net/sublicensing/licensing.html WebGlimpse, Glimpse and also AGREP license] since 18.09.2014 ([http://opensource.org/licenses/ISC ISC License]).</ref>\n\n== Alternative implementations ==\nA more recent agrep is the command-line tool provided with the [[TRE (computing)|TRE]] regular expression library. TRE agrep is more powerful than Wu-Manber agrep since it allows weights and total costs to be assigned separately to individual groups in the pattern. It can also handle Unicode.<ref>{{cite web | title=TRE - TRE regexp matching package - Features | url=http://laurikari.net/tre/about }}</ref> Unlike Wu-Manber agrep, TRE agrep is licensed under a [[BSD licenses#BSD-style licenses|2-clause BSD-like license]].\n\nFREJ (Fuzzy Regular Expressions for Java) open-source library provides command-line interface which could be used in the way similar to agrep. Unlike agrep or TRE it could be used for constructing complex substitutions for matched text.<ref>{{cite web | title=FREJ - Fuzzy Regular Expressions for Java - Guide and Examples | url=http://frej.sf.net/rules.html }}</ref> However its syntax and matching abilities differs significantly from ones of ordinary [[regular expression]]s.\n\n==References==\n{{Reflist}}\n\n==External links==\n* Wu-Manber agrep\n**[http://www.tgries.de/agrep AGREP home page]\n**[ftp://ftp.cs.arizona.edu/agrep/ For Unix]  (To compile under OSX 10.8, add <code>-Wno-return-type</code> to the <code>CFLAGs  = -O</code> line in the Makefile)\n*[http://wiki.christophchamp.com/index.php/Agrep_(command) Entry for "agrep" in Christoph\'s Personal Wiki]\n*See also\n**[http://laurikari.net/tre TRE regexp matching package]\n**[https://web.archive.org/web/20080513225010/http://www1.bell-labs.com/project/wwexptools/cgrep/ cgrep a defunct command line approximate string matching tool]\n**[http://www.dcc.uchile.cl/~gnavarro/software/ nrgrep] a command line approximate string matching tool\n**[http://finzi.psych.upenn.edu/R/library/base/html/agrep.html agrep as implemented in R]\n\n[[Category:Information retrieval systems]]\n[[Category:Unix text processing utilities]]\n[[Category:Software using the ISC license]]']
['Reverse DNS lookup', '1286913', '{{Redirect|Reverse DNS|Java-like naming convention|Reverse domain name notation}}\n{{Refimprove|date=September 2016}}\n\nIn [[computer networking]], \'\'\'reverse DNS lookup\'\'\' or \'\'\'reverse DNS resolution\'\'\' (\'\'\'rDNS\'\'\') is the determination of a [[domain name]] associated with an [[IP address]] via querying [[Domain Name System|DNS]] – the reverse of the usual "forward" DNS lookup of an IP from a domain name.\n\nThe process of reverse resolving an IP address uses [[PTR record]]s. The reverse DNS database of the Internet is rooted in the [[.arpa|arpa]] [[top-level domain]].\n\nAlthough the informational RFC 1912 (Section 2.1) specifies that "Every Internet-reachable host should have a name" and that "For every IP address, there should be a matching PTR record...", it is not an [[Internet Standard]] requirement, and not all IP addresses have a reverse entry.\n\n== Implementation details ==\n===IPv4 reverse resolution===\nReverse DNS lookups for [[IPv4]] addresses use the special domain <code>in-addr.arpa</code>. In this domain, an IPv4 address is represented as a concatenated sequence of four decimal numbers, separated by dots, to which is appended the second level domain suffix <code>.in-addr.arpa</code>. The four decimal numbers are obtained by splitting the 32-bit IPv4 address into four [[octet (computing)|octet]]s and converting each octet into a decimal number. These decimal numbers are then concatenated in the order: least significant octet first (leftmost), most significant octet last (rightmost). It is important to note that this is the reverse order to the usual dotted-decimal convention for writing IPv4 addresses in textual form.\n\nFor example, to do a reverse lookup of the IP address <code>8.8.4.4</code> the PTR record for the domain name <code>4.4.8.8.in-addr.arpa</code> would be looked up, and found to point to <code>google-public-dns-b.google.com</code>. \n\nIf the [[A record]] for <code>google-public-dns-b.google.com</code> in turn pointed back to <code>8.8.4.4</code> then it would be said to be [[Forward-confirmed reverse DNS|forward-confirmed]].\n\n====Classless reverse DNS method====\nHistorically, Internet registries and Internet service providers allocated IP addresses in blocks of 256 (for Class C) or larger octet-based blocks for classes B and A. By definition, each block fell upon an octet boundary. The structure of the reverse DNS domain was based on this definition. However, with the introduction of [[Classless Inter-Domain Routing]], IP addresses were allocated in much smaller blocks, and hence the original design of pointer records was impractical, since autonomy of administration of smaller blocks could not be granted. RFC 2317 devised a methodology to address this problem by using [[CNAME record]]s.\n\n===IPv6 reverse resolution===\nReverse DNS lookups for [[IPv6]] addresses use the special domain <code>ip6.arpa</code> (previously <code>ip6.int</code><ref>RFC 4159</ref>). An IPv6 address appears as a name in this domain as a sequence of [[nibble]]s in reverse order, represented as hexadecimal digits as subdomains. For example, the pointer domain name corresponding to the IPv6 address <code>2001:db8::567:89ab</code> is <code>b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa</code>.\n\n===Multiple pointer records===\nWhile most rDNS entries only have one PTR record, DNS does not restrict the number. However, having multiple PTR records for the same IP address is generally not recommended{{by whom|date=August 2016}}, unless there is a specific need. For example, if a web server supports many [[virtual host]]s, there may be one PTR record for each host and some versions of name server software will allocate this automatically. Multiple PTR records can cause problems, however, including triggering bugs in programs that only expect single PTR records.<ref>[http://sources.redhat.com/bugzilla/show_bug.cgi?id=5790 glibc bug #5790]</ref> In the case of a large web server, having hundreds of PTR records can cause the DNS packets to be much larger than normal, which can cause the query to be requested over TCP when they exceed the DNS 512 byte UDP message limit.\n\n===Records other than PTR records===\nRecord types other than PTR records may also appear in the reverse DNS tree. For example, encryption keys may be placed there for [[IPsec]], [[Secure Shell|SSH]] and [[Internet Key Exchange|IKE]]. [[Zero-configuration networking#DNS-SD|DNS-Based Service Discovery]] uses specially-named records in the reverse DNS tree to provide hints to clients about subnet-specific service discovery domains.<ref>{{Citation | publisher = IETF | title = RFC\u202f6763 | url = http://tools.ietf.org/html/rfc6763#section-11}}</ref> Less standardized usages include comments placed in [[TXT record]]s and [[LOC record]]s to identify the geophysical location of an IP address.\n\n==Uses==\nThe most common uses of the reverse DNS include:\n\n* The original use of the rDNS: network troubleshooting via tools such as [[traceroute]], [[Ping (networking utility)|ping]], and the "Received:" trace header field for [[SMTP]] e-mail, web sites tracking users (especially on [[Internet forum]]s), etc.\n* One [[anti-spam techniques (e-mail)#PTR.2Freverse DNS checks|e-mail anti-spam technique]]: checking the domain names in the rDNS to see if they are likely from dialup users, or dynamically assigned addresses unlikely to be used by legitimate mail servers. Owners of such IP addresses typically assign them generic rDNS names such as "1-2-3-4-dynamic-ip.example.com." Some anti-spam filters assume that email that originates from such addresses is likely to be spam, and may refuse connection.<ref>[http://www.spamhaus.org/faq/answers.lasso?section=ISP%20Spam%20Issues#131 spamhaus\'s FAQ]</ref><ref>[http://postmaster.aol.com/info/rdns.html reference page from AOL] {{webarchive |url=https://web.archive.org/web/20061210223820/http://postmaster.aol.com/info/rdns.html |date=December 10, 2006 }}</ref>\n* A [[forward-confirmed reverse DNS]] (FCrDNS) verification can create a form of authentication showing a valid relationship between the owner of a domain name and the owner of the server that has been given an IP address. While not very thorough, this validation is strong enough to often be used for [[whitelist]]ing purposes, since [[Spam (electronic)|spammers]] and [[Phishing|phishers]] usually cannot achieve forward validation when they use [[zombie computer]]s to forge domain records.\n* System logging or monitoring tools often receive entries with the relevant devices specified only by IP addresses. To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address.\n\n==References==\n{{reflist}}\n\n==External links==\n* {{dmoz|Computers/Internet/Protocols/DNS/Web_Tools|Web-based DNS lookup tools}}\n* [http://dns.icann.org ICANN DNS Operations]\n* [https://tools.ietf.org/html/rfc3596 RFC 3596 DNS Extensions to Support IP Version 6]\n* RDNS policies: [https://web.archive.org/web/20121106162649/http://postmaster.aol.com:80/Postmaster.Errors.php#554rlyb1#whatisrdns AOL], [http://customer.comcast.com/help-and-support/internet/fix-a-554-error/ Comcast], [http://www.craigslist.org/about/help/rdns_failure Craigslist], [https://www.misk.com/kb/reverse-dns Misk.com]\n\n[[Category:Information retrieval systems]]\n[[Category:Domain name system]]\n\n[[nl:Domain Name System#Omgekeerde lookups]]']
['MAREC', '24979660', "{{other uses}}\nThe '''MA'''trixware '''RE'''search '''C'''ollection ('''MAREC''') is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions.<ref>Merz C., (2003) A Corpus Query Tool For Syntactically Annotated Corpora Licentiate Thesis, The University of Zurich, Department of Computation linguistic, Switzerland</ref><ref>Biber D., Conrad S., and Reppen R. (2000) Corpus Linguistics: Investigating Language Structure and Use. Cambridge University Press, 2nd edition</ref> It consists of 19 million patent documents in different languages, normalised to a highly specific [[XML]] schema.\n\nMAREC is intended as raw material for research in areas such as [[information retrieval]], [[natural language processing]] or [[machine translation]], which require large amounts of complex documents.<ref>Manning, C. D. and Schütze, H. (2002) Foundations of statistical natural language processing Cambridge, MA, Massachusetts Institute of Technology (MIT)  ISBN 0-262-13360-1.</ref> The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text.\n\nIn MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as [[International Patent Classification|IPC]] codes.<ref>European Patent Office (2009) [http://documents.epo.org/projects/babylon/eponet.nsf/0/1AFC30805E91D074C125758A0051718A/$File/guidelines_2009_complete_en.pdf Guidelines for examination in the European Patent Office], Published by European Patent Office, Germany (April 2009)</ref>\n\nMAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics – news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language.<ref>Järvelin A. , Talvensaari T. , Järvelin Anni, (2008) Data driven methods for improving mono- and cross-lingual IR performance in noisy environments, Proceedings of the second workshop on Analytics for noisy unstructured text data, (Singapore)</ref> Since the patent document refers to the same “invention” or “concept of idea” the text is a translation of the invention, but it does not have to be a direct translation of the text itself – text parts could have been removed or added for clarification reasons.\n\nThe 19,386,697 XML files measure a total of 621 GB and are hosted by the [[Information Retrieval Facility]]. Access and support are free of charge for research purposes.\n\n== Use Cases ==\n* MAREC is used in the [[Patent Language Translations Online (PLuTO)]] project.\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.ir-facility.org/prototypes/marec User guide and statistics]\n* [http://ir-facility.org Information Retrieval Facility]\n\n[[Category:Corpora]]\n[[Category:Information retrieval systems]]\n[[Category:Machine translation]]\n[[Category:Natural language processing]]\n[[Category:XML]]"]
['Quandl', '39810775', '{{Infobox dot-com company\n|name               = Quandl, Inc.\n|logo               = [[File:Quandl-logo.png|100px]]\n|company_type       = [[Private company|Private]]\n|founder            = {{Plainlist|\n* Tammer Kamel\n* Abraham Thomas\n}}\n|location           = [[Toronto]], [[Canada]]\n|area_served        = Worldwide\n|key_people         = {{Plainlist|\n* Tammer Kamel <small>(CEO)</small>\n* Abraham Thomas <small>(CDO)</small>\n}}\n|industry           = [[Internet]]\n|products           = Quandl Data Marketplace\n|services           = Data [[subscriptions]]\n|num_employees      = 16\n|url                = {{URL|quandl.com}}\n|programming_language = [[Ruby (programming language)|Ruby]] and [[Java (programming language)|Java]]\n|website_type       = [[E-commerce]]\n|language           = English\n|launch_date        = {{start date and age|2013|01|01|df=yes}}\n}}\n\n\'\'\'Quandl\'\'\' ({{IPAc-en|ˈ|k|w|ɑː|n|d|əl}}) is a Toronto-based platform for financial, economic, and alternative data, serving investment professionals. Quandl sources data from over 500 publishers.<ref>{{Cite web|url=https://www.producthunt.com/tech/quandl|title=Quandl - Product Hunt|website=Product Hunt|language=en-US|access-date=2016-09-01}}</ref>  All Quandl\'s data are accessible via an [[API]].<ref>{{cite web |url= http://www.econometricsbysimulation.com/2013/05/quandl-package-5000000-free-datasets-at.html |title= Quandl Package - 5,000,000 free datasets at the tip of your fingers! |date= 5 May 2013 |publisher=EconBS}}</ref> API access is possible through packages for multiple programming languages including [[R (programming language)|R]], [[Python (programming language)|Python]], [[Matlab]], [[Maple (software)]] and [[Stata]].<ref>{{cite web |url= http://blogs.computerworld.com/business-intelligenceanalytics/21881/quandl-wikipedia-data |title= Quandl: Wikipedia for data  |date= 8 March 2013 |publisher=Computer World |last = Machlis |first = Sharon }}</ref><ref>{{cite web |url=https://www.quandl.com/tools/full-list |title=Full list of tools supported on Quandl}}</ref>\n\nAn Excel add-in allows access to data, including stock price information.\n \nQuandl\'s sources include the [[United Nations|UN]], [[Worldbank]], [[CLS Group]], Zacks, and several hundred more.<ref>{{cite web |url= http://gigaom.com/2013/05/31/its-a-beautiful-thing-when-free-data-meets-free-analytics/ |title= It\'s a beautiful thing when free data meets free analytics |date= 31 May 2013 |publisher=Gigaom |last = Harris |first = Derrick }}</ref><ref>{{cite web |url= http://www.quandl.com/resources/data-sources |title= Quandl Data Sources}}</ref>\n\n== History ==\nQuandl was founded in 2012 by Tammer Kamel and Abraham Thomas.<ref>{{Cite web|url=https://www.quandl.com/about|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}</ref> In March 2013, Quandl raised $1.5m,<ref>{{Cite web|url=https://www.crunchbase.com/organization/quandl|title=Quandl {{!}} CrunchBase|website=www.crunchbase.com|access-date=2016-09-01}}</ref> in seed funding followed by a $5.4m Series A from [[August Capital]] in 2014.<ref>{{Cite web|url=http://blogs.wsj.com/venturecapital/2014/11/13/quandl-raises-5-4-million-for-its-financial-data-marketplace/|title=Quandl Raises $5.4 Million for Its Financial-Data Marketplace|last=Gage|first=Deborah|access-date=2016-09-01}}</ref>\n\nSince its launch, Quandl has been discussed as a disruptive force in the anachronistic financial data sector.<ref>{{Cite web|url=http://mattturck.com/2014/03/19/can-the-bloomberg-terminal-be-toppled/|title=Can the Bloomberg Terminal be "Toppled"?|date=2014-03-19|website=Matt Turck|access-date=2016-09-01}}</ref> With over 100,000 users<ref>{{Cite web|url=https://www.integrity-research.com/new-data-provider-quandl/|title=New Data Provider Quandl Moves Toward Alternative Data • Integrity Research|language=en-US|access-date=2016-09-01}}</ref> Quandl is positioning itself as a possible replacement for both Bloomberg and Reuters terminals.<ref>{{Cite web|url=http://www.huffingtonpost.com/irene-aldridge/blindsided-by-innovation-_b_9025960.html|title=Blindsided by innovation like Bloomberg? Don\'t become a statistic.|last=AbleMarkets.com|first=Irene Aldridge Quantitative portfolio manager; MD at|last2=speaker|date=2016-01-22|website=The Huffington Post|access-date=2016-09-01|last3=author|last4=Trading\'|first4=\'High-Frequency}}</ref> Quandl is an alternative for people who are unable to afford the expensive licensing fees of Bloomberg and Reuters.<ref>{{Cite web|url=https://openforum.hbs.org/challenge/understand-digital-transformation-of-business/data/quandl-a-marketplace-for-financial-data|title=Quandl: A Marketplace for Financial Data|access-date=2016-09-01}}</ref>\n\n== Products ==\nQuandl\'s main focus, and area of expertise, is in the realm of alternative data.<ref name=":0">{{Cite web|url=http://www.waterstechnology.com/inside-market-data/news/2462823/quandl-embarks-on-quest-for-alternative-data|title=Quandl Embarks on Quest for Alternative Data|access-date=2016-09-01}}</ref> Quandl sells alternative datasets, defined as "any data that is not typically made available to Wall Street firms by traditional sources".<ref name=":0" /> Quandl "sources, evaluates and productizes undiscovered data" and then sells it to financial institutions, who use it to enhance their trading strategies.<ref>{{Cite web|url=https://www.quandl.com/institutions|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}</ref>\n\nQuandl also offers market data through its marketplace. Some data sets are free while others require a subscription. Different datasets have different prices. They have hundreds of databases and providers ranging from stock price history to global fundamentals to commodities data to Asian market data.<ref>{{Cite web|url=http://www.quandl.com/vendors|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}</ref>\n\nQuandl has an exclusive relationship with CLS Group in London, and is the only source of commercial [[Foreign exchange market|FX]] volume data<ref>{{Cite web|url=http://www.waterstechnology.com/inside-market-data/news/2465222/quandl-adds-cls-fx-trade-volume-data-to-online-platform|title=Quandl Adds CLS FX Trade, Volume Data to Online Platform|access-date=2016-09-01}}</ref>\n\n== References ==\n{{Reflist|30em}}\n\n[[Category:Information retrieval systems]]']
['Pleade', '35952152', '{{Infobox software\n| name                   = Pleade-infoxbox\n| title                  = Pleade\n| logo                   = [[File:Pleade-logo.png]]\n| logo caption           = Logo de Pleade\n| screenshot             = <!-- [[File: ]] -->\n| caption                = \n| collapsible            = \n| author                 = AJLSM\n| developer              = AJLSM\n| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = 3.4\n| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| latest preview version = <!-- 3.5 -->\n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\n| programming language   = [[Java]], [[XSLT]], [[Apache Cocoon|Cocoon]]\n| operating system       = [[Unix-like]], [[Microsoft Windows]]\n| platform               = \n| size                   = \n| language               = French, English, German, Chinese\n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| status                 = Active\n| genre                  = Digital Library\n| license                = GNU General Public License\n| alexa                  = \n| website                = {{URL|http://www.pleade.com/}}\n}}\n\n\'\'\'Pleade\'\'\' is an open source [[search engine]] and browser for [[Finding aid|archival finding aids]] encoded in [[Encoded Archival Description|EAD]] (an XML standard for encoding archival finding aids). Based on the [[Secure Document Exchange|SDX]] platform, it is a very flexible web application.\n\n== History ==\nThe software was jointly started by the companies AJLSM and Anaphore and was originally intended for publication and dissemination only of archival research tools like EAD finding aids, but it has become a library portal and a medium for digital libraries.<ref>[http://www.digicult.info/downloads/dc_info_issue6_december_20031.pdf DigiCult.Info issue #6, page 16]</ref>\n\n==Technologies==\nPleade is published in GPL 3. It is based on the [[Apache Cocoon|Apache Cocoon framework]] and it works with the search engine SDX.\n\nIt is able to publish and distribute the following format : [[Encoded Archival Description|EAD]], [[Comma-separated values|CSV]] (internally converted to XML), [[XMLMarc]], [[Text Encoding Initiative|TEI]], [[Dublin Core]]. Support for [[Metadata Encoding and Transmission Standard|METS]] and [[ALTO (XML)|ALTO]] is under active development.<ref>[http://pleade.com/ Pleade 2012 : les imprimés numérisés et les formats XML METS / ALTO]</ref>\n\n== Features ==\n* Customizable publication ;\n* Customizable index creation ;\n* Customizable search form ;\n* Simple and advanced search among publish documents ;\n* Federate search among different bases (e.g. EAD, METS) ;\n* basket (for database and for images), a search history, printing, etc. ;\n* document viewer supporting : [[JPEG]], [[TIFF]] and for high resolution TIFF and [[JPEG2000]] it use [http://iipimage.sourceforge.net/ IIPImage image server] ;\n* [[OAI-PMH]] repositories and expose them, by default, the format EAD, Dublin Core and [[Dublin Core#Qualified Dublin Core|Qualified DC]] ;\n* The viewer has a Pleade indexing module (paleographic) that can be used to permit correction of the OCR. This tool is a TEI export of data input. A workflow management allows annotators and validation records seized ;\n* Printing resulting and finding aids as PDF documents (with embedded images) ;\n* Compatible with standard archival format : [[Text Encoding Initiative|TEI]], [[BiblioML]] ;\n* Ability to import metadata from an [[Integrated library system|ILS]].\n\n=== Pleade-Entreprise ===\n* Pleade-Entreprise extended features to others XML format, such as [[Metadata Encoding and Transmission Standard|METS]] and [[ALTO (XML)|ALTO]].\n\n== Examples ==\nThese are examples of websites based on Pleade:\n{{columns-list|2|\n* Archival portals\n** [http://archives-inventaires.loire-atlantique.fr/ Departmental records of Loire-Atlantique (AD 44) (AD 44)]\n** [http://gael.gironde.fr/ GAEL : GAEL: Gironde archives online]\n** [http://odysseo.org/ Odysseo: Resources for the history of immigration]\n** [http://taubira.anaphore.org/ Parliamentary work of Christiane Taubira]\n** [http://archivesetmanuscrits.bnf.fr/ Archives and manuscrits of the BNF French National Library]\n** [http://jubilotheque.upmc.fr/ Jubilothèque, UPMC\'s scientific digital library]\n** [http://lbf-ehess.ens-lyon.fr/pages/fonds.html Michel Foucault\'s Library "les Mots et les Choses" ENS]\n* Portals documentary\n** [http://www.michael-culture.org/fr/home Michael]\n** [http://www.numerique.culture.fr/mpf/pub-fr/index.html Digital Heritage]\n* Digital Libraries\n** Digital Library of Lille\n** Lille III\n** [http://archivesetmanuscrits.bnf.fr/ BNF: Archives and manuscripts (French National Library)]\n}}\n\n== Related resources ==\n* {{Official website|http://pleade.com}}\n* [http://demo.pleade.com Official demo]\n* [http://www.pleadeenpratique.org/ Pleade in practice]\n* [http://www.ajlsm.com/produits/sdx SDX]\n* [http://www.ajlsm.com AJLSM company]\n\n== References ==\n<references/>\n\n[[Category:Digital library software]]\n[[Category:Free software]]\n[[Category:Information retrieval systems]]\n[[Category:Archival science]]']
['Search engine (computing)', '27804', "{{multiple issues|\n{{more footnotes|date=August 2014}}\n{{one source|date=August 2014}}\n}}\n\nA '''search engine''' is an [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented in a list and are commonly called ''hits''. Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing [[information overload]]. {{Citation needed|date=December 2007}}\n\nThe most public, visible form of a search engine is a [[Web search engine]] which searches for information on the [[World Wide Web]].\n\n==How search engines work==\nSearch engines provide an [[interface (computer science)|interface]] to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a [[Web search query|search query]]. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired [[concept]] that one or more [[document]]s may contain.<ref>Voorhees, E.M. [http://www.indexnist.gov/itl/iad/894.02/works/papers/nlp_ir.ps Natural Language Processing and Information Retrieval]. National Institute of Standards and Technology. March 2000.</ref> There are several styles of search query [[syntax]] that vary in strictness. It can also switch names within the search engines from previous sites.  Whereas some text search engines require users to enter two or three words separated by [[Whitespace (computer science)|white space]], other search engines may enable users to specify entire documents, pictures, sounds, and various forms of [[natural language]]. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as [[query expansion]].\n\n[[Image:search-engine-diagram-en.svg|right|thumb|Index-based search engine]]\n\nThe list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. [[probability|Probabilistic]] search engines rank items based on measures of [[String metric|similarity]] (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes [[popularity]] or [[authority]] (see [[Bibliometrics]]) or use [[relevance feedback]]. [[Boolean logic|Boolean]] search engines typically only return items which match exactly without regard to order, although the term ''boolean search engine'' may simply refer to the use of boolean-style syntax (the use of operators [[Logical_conjunction|AND]], [[Logical_disjunction|OR]], NOT, and [[Exclusive nor gate|XOR]]) in a probabilistic context.\n\nTo provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect [[metadata]] about the group of items under consideration beforehand through a process referred to as [[Index (search engine)|indexing]]. The index typically requires a smaller amount of [[computer storage]], which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the [[Serpent (album)|search engine result page]]. Alternatively, the search engine may store a copy of each item in a [[cache (computing)|cache]] so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.\n\nOther types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). [[Meta search engine]]s store neither an index nor a cache and instead simply reuse the index or results of one or more other search engines to provide an aggregated, final set of results.\n\n==Types of search engines==\n\n; By source\n\n*[[Desktop search]]\n*[[Federated search]]\n*[[Human search engine]]\n*[[Metasearch engine]]\n*[[Multisearch]]\n*[[Search aggregator]]\n*[[Web search engine]]\n\n; By content type\n\n*[[Full text search]]\n*[[Image search]]\n*[[Video search engine]]\n\n; By interface\n\n*[[Incremental search]]\n*[[Instant answer]]\n*[[Semantic search]]\n*[[Selection-based search]]\n\n; By topic\n\n*[[Bibliographic database]]\n*[[Enterprise search]]\n*[[Medical literature retrieval]]\n*[[Vertical search]]\n\n==See also==\n{{Portal|Computer Science}}\n{{div col|colwidth=30em}}\n*[[Automatic summarization]]\n*[[Emanuel Goldberg]] (inventor of early search engine)\n*[[Index (search engine)]]\n*[[Inverted index]]\n*[[List of search engines]]\n*[[List of enterprise search vendors]]\n*[[Search engine optimization]]\n*[[Search suggest drop-down list]]\n* [[Solver (computer science)]]\n*[[Spamdexing]]\n*[[SQL]]\n*[[Text mining]]\n{{div col end}}\n\n==References==\n{{Reflist}}\n{{Internet search}}\n\n{{Authority control}}\n{{DEFAULTSORT:Search Engine (Computing)}}\n[[Category:Information retrieval systems]]"]
['Champion list', '26304039', "{{orphan|date=January 2011}}\n\nA '''champion list''', also called '''top doc''' or '''fancy list''' is a precomputed list sometimes used with the [[vector space model]] to avoid computing relevancy rankings for all documents each time a document collection is queried. The champion list contains a set of n documents with the highest weights for the given term. The number n can be chosen to be different for each term and is often higher for rarer terms. The weights can be calculated by for example [[tf-idf]].\n\n[[Category:Information retrieval evaluation]]\n\n\n{{computing-stub}}"]
['Queries per second', '26039201', "{{distinguish-redirect|Query rate|Query throughput}}\n'''Queries per second''' (QPS) is a common measure of the amount of search traffic an [[information retrieval]] system, such as a [[search engine]] or a [[database]], receives during one second.<ref>[http://www.microsoft.com/enterprisesearch/en/us/search-glossary.aspx#Q Microsoft's search glossary]</ref> The term is used more broadly for any [[request–response]] system, more correctly called [[requests per second]] (RPS).\n\nHigh-traffic systems must watch their QPS in order to know when to scale the system to handle more load.\n\n== References ==\n{{reflist}}\n\n[[Category:Units of frequency]]\n[[Category:Information retrieval evaluation]]\n\n{{computer-stub}}"]
['Category:Search algorithms', '1406201', '{{Commons category|Search algorithms}}\n{{Cat main|Search algorithms}}\n\n[[Category:Algorithms]]\n[[Category:Information retrieval techniques]]']
['Ordered weighted averaging aggregation operator', '14893994', 'In applied mathematics – specifically in [[fuzzy logic]] – the \'\'\'ordered weighted averaging (OWA) operators\'\'\' provide a [[parameter]]ized class of mean type aggregation operators. They were introduced by [[Ronald R. Yager]]. Many notable mean operators such as the max, [[arithmetic average]], median and min, are members of this class. They have been widely used in [[computational intelligence]] because of their ability to model linguistically expressed aggregation instructions.\n\n== Definition ==\n\nFormally an \'\'\'OWA\'\'\' operator of dimension <math> \\ n </math> is a mapping <math> F: R_n \\rightarrow R </math> that has an associated collection of weights <math> \\  W = [w_1, \\ldots, w_n] </math> lying in the unit interval and summing to one and with \t\t\n\n:<math> F(a_1, \\ldots , a_n) =  \\sum_{j=1}^n  w_j b_j</math>\n\nwhere <math> b_j </math> is the \'\'j\'\'<sup>th</sup> largest of the <math> a_i </math>.\n\nBy choosing different \'\'W\'\' one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the \'\'b\'\'<sub>\'\'j\'\'</sub>.\n\n== Properties ==\n\nThe OWA operator is a mean operator. It is [[Bounded operator|bounded]], [[monotonic]], [[symmetric operator|symmetric]], and [[idempotent]], as defined below.\n\n{|class="wikitable"\n|[[Bounded operator|Bounded]]\n|<math>   \\min(a_1, \\ldots, a_n) \\le F(a_1, \\ldots, a_n) \\le \\max(a_1, \\ldots, a_n) </math>\n|-\n|[[Monotonic]]\n|<math>   F(a_1, \\ldots, a_n) \\ge F(g_1, \\ldots, g_n) </math> if <math> a_i \\ge g_i </math> for <math>\\ i = 1,2,\\ldots,n </math>\n|-\n|[[symmetric operator|Symmetric]]\n|<math>   F(a_1, \\ldots, a_n)  = F(a_\\boldsymbol{\\pi(1)}, \\ldots, a_\\boldsymbol{\\pi(n)})</math> if <math>\\boldsymbol{\\pi} </math> is a permutation map\n|-\n|[[Idempotent]]\n|<math>  \\ F(a_1, \\ldots, a_n)  =  a </math> if all <math> \\ a_i = a </math>\n|}\n\n== Notable OWA operators ==\n:<math> \\ F(a_1, \\ldots, a_n) = \\max(a_1, \\ldots, a_n) </math> if <math> \\ w_1 = 1 </math> and <math> \\ w_j = 0 </math> for <math> j \\ne 1 </math>\n\n:<math> \\ F(a_1, \\ldots, a_n) = \\min(a_1, \\ldots, a_n) </math> if <math> \\ w_n = 1 </math> and <math> \\ w_j = 0 </math> for <math> j \\ne n </math>\n\n== Characterizing features ==\n\nTwo features have been used to characterize the OWA operators. The first is the attitudinal character(orness).\n\nThis is defined as\n:<math>A-C(W)= \\frac{1}{n-1} \\sum_{j=1}^n (n - j) w_j. </math>\n\nIt is known that <math> A-C(W) \\in [0, 1] </math>.\n\nIn addition \'\'A\'\'&nbsp;&minus;&nbsp;\'\'C\'\'(max) = 1, A&nbsp;&minus;&nbsp;C(ave) = A&nbsp;&minus;&nbsp;C(med) = 0.5 and A&nbsp;&minus;&nbsp;C(min) = 0. Thus the A&nbsp;&minus;&nbsp;C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).\n\nThe second feature is the dispersion. This defined as\n\n:<math>H(W) = -\\sum_{j=1}^n w_j \\ln (w_j).</math>\n\nAn alternative definition is <math>E(W) = \\sum_{j=1}^n w_j^2 .</math> The dispersion characterizes how uniformly the arguments are being used\nÀĚ\n\n== A literature survey: OWA (1988-2014)==\nThe historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager\'s paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183–190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full)\n\n== Type-1 OWA aggregation operators ==\n\nThe above Yager\'s OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The\n\'\'\'[[Type-1 OWA operators]]\'\'\' have been proposed for this purpose. So the \'\'\'[[type-1 OWA operators]]\'\'\' provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\n\nThe \'\'\'[[Type-1 OWA operators|type-1 OWA operator]]\'\'\' is defined according to the alpha-cuts of fuzzy sets as follows:\n\nGiven the \'\'n\'\' linguistic weights <math>\\left\\{ {W^i} \\right\\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\\;\\;1]</math>, then for each <math>\\alpha \\in [0,\\;1]</math>, an <math>\\alpha </math>-level type-1 OWA operator with <math>\\alpha </math>-level sets <math>\\left\\{ {W_\\alpha ^i } \\right\\}_{i = 1}^n </math> to aggregate the <math>\\alpha </math>-cuts of fuzzy sets <math>\\left\\{ {A^i} \\right\\}_{i =1}^n </math> is given as\n\n: <math>\n\\Phi_\\alpha \\left( {A_\\alpha ^1 , \\ldots ,A_\\alpha ^n } \\right) =\\left\\{ {\\frac{\\sum\\limits_{i = 1}^n {w_i a_{\\sigma (i)} } }{\\sum\\limits_{i = 1}^n {w_i } }\\left| {w_i \\in W_\\alpha ^i ,\\;a_i } \\right. \\in A_\\alpha ^i ,\\;i = 1, \\ldots ,n} \\right\\}</math>\n\nwhere <math>W_\\alpha ^i= \\{w| \\mu_{W_i }(w) \\geq \\alpha \\}, A_\\alpha ^i=\\{ x| \\mu _{A_i }(x)\\geq \\alpha \\}</math>, and <math>\\sigma :\\{\\;1, \\ldots ,n\\;\\} \\to \\{\\;1, \\ldots ,n\\;\\}</math> is a permutation function such that <math>a_{\\sigma (i)} \\ge a_{\\sigma (i + 1)} ,\\;\\forall \\;i = 1, \\ldots ,n - 1</math>, i.e., <math>a_{\\sigma (i)} </math> is the <math>i</math>th largest\nelement in the set <math>\\left\\{ {a_1 , \\ldots ,a_n } \\right\\}</math>.\n\nThe computation of the \'\'\'[[Type-1 OWA operators|type-1 OWA]]\'\'\' output is implemented by computing the left end-points and right end-points of the intervals <math>\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\ldots ,A_\\alpha ^n } \\right)</math>:\n<math>\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\ldots ,A_\\alpha ^n } \\right)_{-} </math> and <math>\n\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\ldots ,A_\\alpha ^n } \\right)_ {+},</math>\nwhere <math>A_\\alpha ^i=[A_{\\alpha-}^i, A_{\\alpha+}^i], W_\\alpha ^i=[W_{\\alpha-}^i, W_{\\alpha+}^i]</math>. Then membership function of resulting aggregation fuzzy set is:\n\n:<math>\\mu _{G} (x) = \\mathop \\vee _{\\alpha :x \\in \\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots\n,A_\\alpha ^n } \\right)_\\alpha } \\alpha </math>\n\nFor the left end-points, we need to solve the following programming problem:\n\n:<math> \\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots ,A_\\alpha ^n } \\right)_{-} = \\min\\limits_{\\begin{array}{l} W_{\\alpha - }^i \\le w_i \\le W_{\\alpha + }^i A_{\\alpha - }^i \\le a_i \\le A_{\\alpha + }^i  \\end{array}} \\sum\\limits_{i = 1}^n {w_i a_{\\sigma (i)} / \\sum\\limits_{i = 1}^n {w_i } } </math>\n\nwhile for the right end-points, we need to solve the following programming problem:\n\n:<math>\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots , A_\\alpha ^n } \\right)_{+} = \\max\\limits_{\\begin{array}{l} W_{\\alpha - }^i \\le w_i \\le W_{\\alpha + }^i  A_{\\alpha - }^i \\le a_i \\le A_{\\alpha + }^i  \\end{array}} \\sum\\limits_{i = 1}^n {w_i a_{\\sigma (i)} / \\sum\\limits_{i =\n1}^n {w_i } } </math>\n\n[http://dx.doi.org/10.1109/TKDE.2010.191 This paper] has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.\n\n== References ==\n\n* Yager, R. R., "On ordered weighted averaging aggregation operators in multi-criteria decision making," IEEE Transactions on Systems, Man and Cybernetics 18, 183–190, 1988.\n* Yager, R. R. and Kacprzyk, J., [http://www.amazon.com/dp/079239934X The Ordered Weighted Averaging Operators: Theory and Applications], Kluwer: Norwell, MA, 1997.\n* Liu, X., "The solution equivalence of minimax disparity and minimum variance problems for OWA operators," International Journal of Approximate Reasoning 45, 68–81, 2007.\n* Emrouznejad (2009) SAS/OWA: ordered weighted averaging in SAS optimization, Soft Computing [http://www.springerlink.com/content/7277l73334r108x5/]\n* Emrouznejad, A. and M. Marra (2014), Ordered Weighted Averaging Operators 1988–2014: A citation-based literature survey, International Journal of Intelligent Systems, 29:994-1014 [http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full  & http://onlinelibrary.wiley.com/store/10.1002/int.21673/asset/supinfo/int21673-sup-0001-SupMat.docx?v=1&s=c0d8bdd220a31c876eb5885521cfa16d191f334d].\n* Torra, V. and Narukawa, Y., Modeling Decisions: Information Fusion and Aggregation Operators, Springer: Berlin, 2007.\n* Majlender, P., "OWA operators with maximal Rényi entropy," Fuzzy Sets and Systems 155, 340–360, 2005.\n* Szekely, G. J. and Buczolich, Z., " When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter?" Advances in Applied Mathematics 10, 1989, 439–456.\n* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers," Fuzzy Sets and Systems, Vol.159, No.24, pp.&nbsp;3281–3296, 2008 [http://dx.doi.org/10.1016/j.fss.2008.06.018]\n* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments," IEEE Transactions on Knowledge and Data Engineering, vol. 23, no.10, 2011, pp.&nbsp;1455–1468.[http://dx.doi.org/10.1109/TKDE.2010.191]\n* S.-M. Zhou, R. I. John, F. Chiclana and J. M. Garibaldi, "On aggregating uncertain information by type-2 OWA operators for soft decision making," International Journal of Intelligent Systems, vol. 25, no.6, pp.&nbsp;540–558, 2010.[http://dx.doi.org/10.1002/int.20420]\n\n[[Category:Artificial intelligence]]\n[[Category:Logic in computer science]]\n[[Category:Fuzzy logic]]\n[[Category:Information retrieval techniques]]']
['Type-1 OWA operators', '33591382', '{{Underlinked|date=July 2016}}\n\nThe [[Ordered weighted averaging aggregation operator|Yager\'s OWA (ordered weighted averaging) operators]]{{R|yagerOWA}} are used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decision making and  multi-criteria/multi-expert decision making).{{R|Yager|YagerBeliakov}} It is widely accepted that [[Fuzzy set]]s{{R|Zadeh}} are more suitable for representing preferences of criteria in decision making.\n\nThe type-1 OWA operators{{R|fssT1OWA|kdeT1OWA}} have been proposed for this purpose. The type-1 OWA operators provides a technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and [[data mining]], where these uncertain objects are modelled by fuzzy sets.\n\nThe two definitions for type-1 OWA operators are based on Zadeh\'s Extension Principle and <math>\\alpha</math>-cuts of fuzzy sets. The two definitions lead to equivalent results.\n\n==Definitions==\n\n===Definition 1===\nLet <math>F(X)</math> be the set of fuzzy sets with domain of discourse <math>X</math>, a type-1 OWA operator is defined as follows:{{R|kdeT1OWA}}\n\nGiven n linguistic weights <math>\\left\\{ {W^i} \\right\\}_{i = 1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,1]</math>, a type-1 OWA operator is a mapping, <math>\\Phi</math>,\n\n:<math>\\Phi \\colon F(X)\\times \\cdots \\times F(X)  \\longrightarrow  F(X)</math>\n:<math>(A^1 , \\cdots ,A^n)  \\mapsto   Y</math>\n\nsuch that\n\n:<math>\\mu _{Y} (y) =\\displaystyle \\sup_{\\displaystyle \\sum_{k =1}^n \\bar {w}_i a_{\\sigma (i)}  = y }\\left({\\begin{array}{*{1}l}\\mu _{W^1 } (w_1 )\\wedge \\cdots \\wedge \\mu_{W^n } (w_n )\\wedge \\mu _{A^1 } (a_1 )\\wedge \\cdots \\wedge \\mu _{A^n } (a_n )\\end{array}}\\right)</math>\n\nwhere <math>\\bar {w}_i = \\frac{w_i }{\\sum_{i = 1}^n {w_i } }</math>,and <math>\\sigma \\colon \\{1, \\cdots ,n\\} \\longrightarrow \\{1, \\cdots ,n\\}</math> is a permutation function such that <math>a_{\\sigma (i)} \\geq a_{\\sigma (i + 1)},\\ \\forall i = 1, \\cdots ,n - 1</math>, i.e., <math>a_{\\sigma(i)} </math> is the <math>i</math>th highest element in the set <math>\\left\\{ {a_1 , \\cdots ,a_n } \\right\\}</math>.\n\n===Definition 2===\nUsing the alpha-cuts of fuzzy sets:{{R|kdeT1OWA}}\n\nGiven the n linguistic weights <math>\\left\\{ {W^i} \\right\\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\\;\\;1]</math>, then for each <math>\\alpha \\in [0,\\;1]</math>, an <math>\\alpha </math>-level type-1 OWA operator with <math>\\alpha </math>-level sets <math>\\left\\{ {W_\\alpha ^i } \\right\\}_{i = 1}^n </math> to aggregate the <math>\\alpha </math>-cuts of fuzzy sets <math>\\left\\{ {A^i} \\right\\}_{i =1}^n </math> is:\n\n: <math>\n\\Phi_\\alpha \\left( {A_\\alpha ^1 , \\ldots ,A_\\alpha ^n } \\right) =\\left\\{ {\\frac{\\sum\\limits_{i = 1}^n {w_i a_{\\sigma (i)} } }{\\sum\\limits_{i = 1}^n {w_i } }\\left| {w_i \\in W_\\alpha ^i ,\\;a_i } \\right. \\in A_\\alpha ^i ,\\;i = 1, \\ldots ,n} \\right\\}</math>\n\nwhere  <math>W_\\alpha ^i= \\{w| \\mu_{W_i }(w) \\geq \\alpha \\}, A_\\alpha ^i=\\{ x| \\mu _{A_i }(x)\\geq \\alpha \\}</math>, and <math>\\sigma :\\{\\;1, \\cdots ,n\\;\\} \\to \\{\\;1, \\cdots ,n\\;\\}</math> is a permutation function such that <math>a_{\\sigma (i)} \\ge a_{\\sigma (i + 1)} ,\\;\\forall \\;i = 1, \\cdots ,n - 1</math>, i.e., <math>a_{\\sigma (i)} </math> is the <math>i</math>th largest\nelement in the set <math>\\left\\{ {a_1 , \\cdots ,a_n } \\right\\}</math>.\n\n== Representation theorem of Type-1 OWA operators==\n\nGiven the \'\'n\'\' linguistic weights <math>\\left\\{ {W^i} \\right\\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\\;\\;1]</math>, and the fuzzy sets <math>A^1, \\cdots ,A^n</math>, then we have that{{R|kdeT1OWA}}\n:<math>Y=G</math>\n\nwhere <math>Y</math> is the aggregation result obtained by Definition 1, and <math>G</math> is the result obtained by in Definition 2.\n\n==Programming problems for Type-1 OWA operators==\n\nAccording to the Representation Theorem of Type-1 OWA Operators, a general type-1 OWA operator can be decomposed into a series of <math>\\alpha</math>-level type-1 OWA operators. In practice, this series of  <math>\\alpha</math>-level type-1 OWA operators is used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals <math>\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots ,A_\\alpha ^n } \\right)</math>. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:\n\n:<math>\\mu _{G} (x) = \\operatorname{ \\bigvee} \\limits_{\\alpha :x \\in \\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots\n,A_\\alpha ^n } \\right)_\\alpha } \\alpha </math>\n\nFor the left end-points, we need to solve the following programming problem:\n:<math> \\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots ,A_\\alpha ^n } \\right)_{-} = \\operatorname {\\min }\\limits_{\\begin{array}{l} W_{\\alpha - }^i \\le w_i \\le W_{\\alpha + }^i A_{\\alpha - }^i \\le a_i \\le A_{\\alpha + }^i  \\end{array}} \\sum\\limits_{i = 1}^n {w_i a_{\\sigma (i)} / \\sum\\limits_{i = 1}^n {w_i } } </math>\n\nwhile for the right end-points, we need to solve the following programming problem:\n:<math>\\Phi _\\alpha \\left( {A_\\alpha ^1 , \\cdots , A_\\alpha ^n } \\right)_{+} = \\operatorname {\\max }\\limits_{\\begin{array}{l} W_{\\alpha - }^i \\le w_i \\le W_{\\alpha + }^i  A_{\\alpha - }^i \\le a_i \\le A_{\\alpha + }^i  \\end{array}} \\sum\\limits_{i = 1}^n {w_i a_{\\sigma (i)} / \\sum\\limits_{i =\n1}^n {w_i } } </math>\n\nA fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.{{R|kdeT1OWA}}\n\n==Alpha-level approach to Type-1 OWA operation==\n\nThree-step process:{{R|kdeT1OWA}}\n* Step 1&mdash;To set up the <math>\\alpha </math>- level resolution in [0, 1].\n* Step 2&mdash;For each <math>\\alpha \\in [0,1]</math>,\n:*Step 2.1&mdash;To calculate <math>\\rho _{\\alpha +} ^{i_0^\\ast } </math>\n# Let <math>i_0 = 1</math>;\n# If <math>\\rho _{\\alpha +} ^{i_0 } \\ge A_{\\alpha + }^{\\sigma (i_0 )} </math>, stop, <math>\\rho _{\\alpha +} ^{i_0 } </math> is the solution; otherwise go to Step 2.1-3.\n# <math>i_0 \\leftarrow i_0 + 1</math>, go to Step 2.1-2.\n\n:*Step 2.2 To calculate<math>\\rho _{\\alpha -} ^{i_0^\\ast } </math>\n# Let <math>i_0 = 1</math>;\n# If <math>\\rho _{\\alpha -} ^{i_0 } \\ge A_{\\alpha - }^{\\sigma (i_0 )} </math>, stop, <math>\\rho _{\\alpha -} ^{i_0 } </math> is the solution; otherwise go to Step 2.2-3.\n#<math>i_0 \\leftarrow i_0 + 1</math>, go to step Step 2.2-2.\n\n* Step 3&mdash;To construct the aggregation resulting fuzzy set <math>G</math> based on all the available intervals <math>\\left[ {\\rho _{\\alpha -} ^{i_0^\\ast } ,\\;\\rho _{\\alpha +} ^{i_0^\\ast } } \\right]</math>:\n\n:<math>\\mu _{G} (x) = \\operatorname \\bigvee \\limits_{\\alpha :x \\in \\left[ {\\rho _{\\alpha -} ^{i_0^\\ast } ,\\;\\rho _{\\alpha +} ^{i_0^\\ast } } \\right]} \\alpha </math>\n\n==Special cases==\n* Any OWA operators, like maximum, minimum, mean operators;<ref name="yagerOWA" />\n* Join operators of (type-1) fuzzy sets,{{R|MT}} i.e., fuzzy maximum operators;\n* Meet operators of (type-1) fuzzy sets,{{R|MT|zadehJ}} i.e., fuzzy minimum operators;\n* Join-like operators of (type-1) fuzzy sets;{{R|kdeT1OWA|bookT1OWA}}\n* Meet-like operators of (type-1) fuzzy sets.{{R|kdeT1OWA|bookT1OWA}}\n\n==Generalizations==\nType-2 OWA operators{{R|Zhou}} have been suggested to aggregate the [[Type-2 fuzzy sets and systems|type-2 fuzzy sets]] for soft decision making.\n\n==References==\n{{reflist|30em|refs=\n\n<ref name="yagerOWA">{{cite journal|last=Yager|first=R.R|title=On ordered weighted averaging aggregation operators in multi-criteria decision making|journal=IEEE Transactions on Systems, Man and Cybernetics|year=1988|volume=18|pages=183–190|doi=10.1109/21.87068}}</ref>\n\n<ref name=Yager>{{cite book|last=Yager|first=R. R. and Kacprzyk, J|title=The Ordered Weighted Averaging Operators: Theory and Applications|year=1997|publisher=Kluwer: Norwell, MA}}</ref>\n\n<ref name=YagerBeliakov>{{cite book|last=Yager|first=R.R, Kacprzyk, J. and Beliakov, G|title=Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|publisher=Springer}}</ref>\n\n<ref name=Zadeh>{{cite journal|last=Zadeh|first=L.A|title=Fuzzy sets|journal=Information and Control |year=1965|volume=8 |pages=338–353|doi=10.1016/S0019-9958(65)90241-X}}</ref>\n\n<ref name="kdeT1OWA">{{cite journal|last=Zhou|first=S. M. |author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi|title=Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments|journal=IEEE Transactions on Knowledge and Data Engineering|year=2011|volume=23|issue=10|pages=1455–1468|doi=10.1109/TKDE.2010.191}}</ref>\n\n<ref name="fssT1OWA">{{cite journal|last=Zhou|first=S. M. |author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi|title=Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers|journal=Fuzzy Sets and Systems|year=2008|volume=159|issue=24|pages=3281–3296|doi=10.1016/j.fss.2008.06.018}}</ref>\n\n<ref name="MT">{{cite journal|last=Mizumoto|first=M.|author2=K. Tanaka |title=Some Properties of fuzzy sets of type 2|journal=Information and Control|year=1976|volume=31|pages=312–40|doi=10.1016/s0019-9958(76)80011-3}}</ref><ref name="zadehJ">{{cite journal|last=Zadeh|first=L. A.|title=The concept of a linguistic variable and its application to approximate reasoning-1|journal=Information Sciences|year=1975|volume=8|pages=199–249|doi=10.1016/0020-0255(75)90036-5}}</ref>\n\n<ref name="bookT1OWA">{{cite journal|last=Zhou|first=S. M.|author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi |title=Fuzzificcation of the OWA Operators in Aggregating Uncertain Information|journal=R. R. Yager, J. Kacprzyk and G. Beliakov (ed): Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|volume=Springer|pages=91–109|doi=10.1007/978-3-642-17910-5_5}}</ref>\n\n<ref name=Zhou>{{cite journal|last=Zhou|first=S.M. |author2=R. I. John |author3=F. Chiclana |author4=J. M. Garibaldi|title=On aggregating uncertain information by type-2 OWA operators for soft decision making|journal=International Journal of Intelligent Systems|year=2010|volume=25|issue=6|pages=540–558|doi=10.1002/int.20420}}</ref>\n}}\n\n[[Category:Artificial intelligence]]\n[[Category:Fuzzy logic]]\n[[Category:Information retrieval techniques]]\n[[Category:Logic in computer science]]']
['Search-oriented architecture', '7470226', "{{unreferenced|date=October 2007}}\nThe use of [[search engine technology]] is the main integration component in an [[information system]]. In a traditional business environment the [[architectural layer]] usually occupied by a [[relational database management system]] (RDBMS) is supplemented or replaced with a search engine or the indexing technology used to build search engines. Queries for information which would usually be performed using [[Structured Query Language]] (SQL) are replaced by keyword or fielded (or field-enabled) searches for structured, [[Semi-structured model|semi-structured]], or unstructured data.\n\nIn a typical [[Multitier architecture|multi-tier]] or [[Multitier architecture|N tier]] architecture information is maintained in a data tier where it can be stored and retrieved from a database or file system. The data tier is queried by the logic or business tier when information is needed using a data retrieval language like SQL.\n\nIn a '''search-oriented architecture''' the data tier may be replaced or placed behind another tier which contains a search engine and search engine index which is queried instead of the database management system. Queries from the business tier are made in the search engine query language instead of SQL. The search engine itself crawls the relational database management system in addition to other traditional data sources such as web pages or traditional file systems and consolidates the results when queried.\n\nThe benefit of adding a search layer to the architecture stack is rapid response time large dynamic datasets made possible by search indexing technology such as an [[inverted index]]. \n\n== Contrast with ==\n* [[Service-oriented architecture]] (SOA)\n* [[Service-Oriented Modeling]]\n\n== See also ==\n* [[Hibernate search]]\n \n[[Category:Software architecture]]\n[[Category:Information retrieval techniques]]"]
['Extended Boolean model', '25271852', "The '''Extended Boolean model''' was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in [[information retrieval]]. The Boolean model doesn't consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the [[Vector Space Model]] with the properties of [[Boolean algebra (logic)|Boolean algebra]] and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the [[Standard Boolean model]] it wasn't.<ref>\t\n{{citation | url=http://portal.acm.org/citation.cfm?id=358466 | last1=Salton | first1=Gerard | first2=Edward A. | last2=Fox | first3= Harry | last3=Wu | title=Extended Boolean information retrieval | publisher=Communications of the ACM, Volume 26,  Issue 11 | year=1983 }}</ref>\n\nThus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing.  Other research has shown that [[relevance feedback]] and [[query expansion]] can be integrated with extended Boolean query processing.\n\n==Definitions==\nIn the '''Extended Boolean model''', a document is represented as a vector (similarly to in the vector model). Each ''i'' [[Dimension (vector space)|dimension]] corresponds to a separate term associated with the document.\n\nThe weight of term {{math|''K<sub>x</sub>''}} associated with document {{math|''d<sub>j</sub>''}} is measured by its normalized [[Term frequency]] and can be defined as:\n\n<math>\nw_{x,j}=f_{x,j}*\\frac{Idf_{x}}{max_{i}Idf_{i}}\n</math>\n\nwhere {{math|''Idf<sub>x</sub>''}} is [[inverse document frequency]].\n\nThe weight vector associated with document {{math|''d<sub>j</sub>''}} can be represented as:\n\n<math>\\mathbf{v}_{d_j} = [w_{1,j}, w_{2,j}, \\ldots, w_{i,j}]</math>\n\n==The 2 Dimensions Example==\n{{multiple image\n | width     = 150\n | image1    = 2D_Extended_Boolean_model_OR_example.png\n | alt1      = Figure 1\n | caption1  = '''Figure 1:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.\n | image2    = 2D_Extended_Boolean_model_AND_example.png\n | alt2      = Figure 2\n | caption2  = '''Figure 2:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.\n}}\n\nConsidering the space composed of two terms {{math|''K<sub>x</sub>''}} and {{math|''K<sub>y</sub>''}} only, the corresponding term weights are {{math|''w''<sub>1</sub>}} and {{math|''w''<sub>2</sub>}}.<ref>[http://www.cs.cityu.edu.hk/~cs5286/Lectures/Lwang.ppt Lusheng Wang]</ref>  Thus, for query {{math|''q<sub>or</sub>'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}}, we can calculate the similarity with the following formula:\n \n<math>sim(q_{or},d)=\\sqrt{\\frac{w_1^2+w_2^2}{2}}</math>\n\nFor query {{math|''q<sub>and</sub>'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}}, we can use:\n\n<math>sim(q_{and},d)=1-\\sqrt{\\frac{(1-w_1)^2+(1-w_2)^2}{2}}</math>\n\n==Generalizing the idea and P-norms==\nWe can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances.\n\nThis can be done using [[P-norm]]s which extends the notion of distance to include p-distances, where {{math|1 &le; ''p'' &le; &infin;}} is a new parameter.<ref>{{ citation | last=Garcia | first= Dr. E. | url=http://www.miislita.com/term-vector/term-vector-6-boolean-model.html | title=The Extended Boolean Model - Weighted Queries: Term Weights, p-Norm Queries and Multiconcept Types. Boolean OR Extended? AND that is the Query }}</ref>\n\n*A generalized conjunctive query is given by:\n:<math>q_{or}=k_1 \\lor^p k_2 \\lor^p .... \\lor^p k_t  </math>\n\n*The similarity of <math>q_{or}</math> and <math>d_j</math> can be defined as:\n''':<math>sim(q_{or},d_j)=\\sqrt[p]{\\frac{w_1^p+w_2^p+....+w_t^p}{t}}</math>'''\n\n*A generalized disjunctive query is given by:\n:<math>q_{and}=k_1 \\land^p k_2 \\land^p .... \\land^p k_t  </math>\n\n*The similarity of <math>q_{and}</math> and <math>d_j</math> can be defined as:\n:<math>sim(q_{and},d_j)=1-\\sqrt[p]{\\frac{(1-w_1)^p+(1-w_2)^p+....+(1-w_t)^p}{t}}</math>\n\n==Examples==\nConsider the query {{math|''q'' {{=}} (''K''<sub>1</sub> &and; ''K''<sub>2</sub>) &or; ''K''<sub>3</sub>}}. The similarity between query {{math|''q''}} and document {{math|''d''}} can be computed using the formula:\n\n<math>sim(q,d)=\\sqrt[p]{\\frac{(1-\\sqrt[p]{(\\frac{(1-w_1)^p+(1-w_2)^p}{2}}))^p+w_3^p}{2}}</math>\n\n==Improvements over the Standard Boolean Model==\n\nLee and Fox<ref>{{citation | last1=Lee | first1=W. C. | first2=E. A. | last2=Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries | url = http://eprints.cs.vt.edu/archive/00000112/01/TR-88-27.pdf}}</ref> compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC.\nUsing P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively.<br>\nThe P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even [[Fuzzy retrieval]] techniques. The [[Standard Boolean model]] is still the most efficient.\n\n==Further reading==\n* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.1997 Adaptive Feedback Methods in an Extended Boolean Model  by Dr.Jongpill Choi]\n* [http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6VC8-454T5MS-2&_user=513551&_rdoc=1&_fmt=&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1117914301&_rerunOrigin=google&_acct=C000025338&_version=1&_urlVersion=0&_userid=513551&md5=4eab0da46bfe361afa883e48f2060feb Interpolation of the extended Boolean retrieval model ]\n* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last1=Fox | first1=E. | first2=S. | last2=Betrabet | first3=M. | last3=Koushik | first4=W. | last4=Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}\n* {{citation | title=Experiments with Automatic Query Formulation in the Extended Boolean Model | url=http://www.springerlink.com/content/tk1t141253257613/ | first1= Lucie | last1= Skorkovská | first2=Pavel | last2=Ircing | year=2009 | publisher= Springer Berlin / Heidelberg}}\n\n==See also==\n*[[Information retrieval]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Extended Boolean Model}}\n[[Category:Information retrieval techniques]]"]
['Semantic technology', '4416107', "{{no footnotes|date=March 2013}}\n[[File:SemanticNetExample.jpg|thumb|Simplistic example of the sort of semantic net used in Semantic Web technology]]\nIn [[software]], '''semantic technology''' encodes meanings separately from data and content files, and separately from application code. \n\nThis enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.\n\nWith traditional [[information technology]], on the other hand, meanings and relationships must be predefined and “hard wired” into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.\n\nOff-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.\n\nSemantic technologies are “meaning-centered.” They include tools for:\n\n* autorecognition of topics and concepts, \n* information and meaning extraction, and\n* categorization. \n\nGiven a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.\n\nSemantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone.\n\n== See also ==\n* [[Business Intelligence 2.0]] (BI 2.0)\n* [[Metadata]]\n* [[Ontology (computer science)]]\n* [[Semantic web]]\n\n==References==\n\n* J.T. Pollock, R. Hodgson. ''Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration.'' [[J. Wiley and Sons]], October 2004\n* R. Guha, R. McCool, and E. Miller. Semantic search. In ''WWW2003 — Proc. of the 12th international conference on World Wide Web'', pp 700–709. [[ACM Press]], 2003.\n* I. Polikoff and D. Allemang. [https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf Semantic technology.] ''TopQuadrant Technology Briefing'' v1.1, September 2003.\n* [[Tim Berners-Lee|T. Berners-Lee]], J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. ''[[Scientific American]]'', May 2001.\n* A.P. Sheth, C. Ramakrishnan. [http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis. Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.] ''[[IEEE Data Engineering Bulletin]]'', 2003.\n* Steffen Staab, Rudi Studer  (Ed.), Handbook on Ontologies, Springer, \n* Mills Davis. The Business Value of Semantic Technologies. Presentation and Report. Semantic Technologies for E-Government, September\n2004.\n* P. Hitzler, M. Krötzsch, S. Rudolph, Foundations of Semantic Web Technologies, Chapman&Hall/CRC, 2009, ISBN 978-1-4200-9050-5\n\n== External links ==\n* [http://semtech2010.semanticuniverse.com Semantic Technology Conference]\n\n[[Category:Information retrieval techniques]]\n[[Category:Semantics]]"]
['Binary Independence Model', '25957127', '{{context|date=June 2012}}\nThe \'\'\'Binary Independence Model\'\'\' (BIM)<ref name="cyu76" /><ref name="jones77"/> is a probabilistic [[information retrieval]] technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.\n\n==Definitions==\nThe Binary Independence Assumption is that documents are [[bit array|binary vector]]s. That is, only the presence or absence of terms in documents are recorded. Terms are [[independence (probability theory)|independently]] distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.\nThe representation is an ordered set of [[Boolean data type|Boolean]] variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector \'\'d = (x<sub>1</sub>, ..., x<sub>m</sub>)\'\' where \'\'x<sub>t</sub>=1\'\' if term \'\'t\'\' is present in the document \'\'d\'\' and \'\'x<sub>t</sub>=0\'\' if it\'s not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.\n"Independence" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the "naive" assumption of a [[Naive Bayes classifier]], where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a [[Vector space model]] by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.\n\nThe probability \'\'P(R|d,q)\'\' that a document is relevant derives from the probability of relevance of the terms vector of that document \'\'P(R|x,q)\'\'. By using the [[Bayes rule]] we get:\n\n<math>P(R|x,q) = \\frac{P(x|R,q)*P(R|q)}{P(x|q)}</math>\n\nwhere \'\'P(x|R=1,q)\'\' and \'\'P(x|R=0,q)\'\' are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document\'s representation is \'\'x\'\'.\nThe exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used.\n\n\'\'P(R=1|q)\'\' and \'\'P(R=0|q)\'\' indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query \'\'q\'\'. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.\nSince a document is either relevant or nonrelevant to a query we have that:\n\n<math>P(R=1|x,q) + P(R=0|x,q) = 1</math>\n\n=== Query Terms Weighting ===\nGiven a binary query and the [[dot product]] as the similarity function between a document and a query, the problem is to assign weights to the\nterms in the query such that the retrieval effectiveness will be high. Let <math>p_i</math> and <math>q_i</math> be the probability that a relevant document and an irrelevant document has the <math>i^{th}</math> term respectively. Yu and [[Gerard Salton|Salton]],<ref name="cyu76" /> who first introduce BIM, propose that the weight of the <math>i^{th}</math> term is an increasing function of <math>Y_i =  \\frac{p_i *(1-q_i)}{(1-p_i)*q_i}</math>. Thus, if <math>Y_i</math> is higher than <math>Y_j</math>, the weight\nof term <math>i</math> will be higher than that of term <math>j</math>. Yu and Salton<ref name="cyu76" /> showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. [[Stephen Robertson (computer scientist)|Robertson]] and [[Karen Spärck Jones|Spärck Jones]]<ref name="jones77"/> later showed that if the <math>i^{th}</math> term is assigned the weight of <math>log Y_i</math>, then optimal retrieval effectiveness is obtained under the Binary Independence Assumption.\n\nThe Binary Independence Model was introduced by Yu and Salton.<ref name="cyu76" /> The name Binary Independence Model was coined by Robertson and Spärck Jones.<ref name="jones77"/>\n\n== See also ==\n\n* [[Bag of words model]]\n\n==Further reading==\n* {{citation | url=http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html | title=Introduction to Information Retrieval | author=Christopher D. Manning | author2=Prabhakar Raghavan|author3=Hinrich Schütze | publisher=Cambridge University Press | year=2008}}\n* {{citation | url=http://www.ir.uwaterloo.ca/book/ | title=Information Retrieval: Implementing and Evaluating Search Engines | author=Stefan B&uuml;ttcher | author2=Charles L. A. Clarke |author3= Gordon V. Cormack | publisher=MIT Press | year=2010}}\n\n==References==\n{{Reflist|refs=\n<ref name="cyu76">{{Cite journal | doi = 10.1145/321921.321930| title = Precision Weighting – An Effective Automatic Indexing Method| journal = Journal of the ACM| volume = 23| pages = 76| year = 1976| last1 = Yu | first1 = C. T.| last2 = Salton | first2 = G. | authorlink2 = Gerard Salton}}</ref>\n<ref name="jones77">{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Spärck Jones | first2 = K. | authorlink2 = Karen Spärck Jones}}</ref> \n}}\n\n[[Category:Information retrieval techniques]]\n[[Category:Probabilistic models]]']
['Faceted search', '10715937', '\'\'\'Faceted search\'\'\', also called \'\'\'faceted navigation\'\'\' or \'\'\'faceted browsing\'\'\', is a technique for accessing information organized according to a [[faceted classification]] system, allowing users to explore a collection of information by applying multiple filters. A faceted classification system classifies each information element along multiple explicit dimensions, called facets, enabling the classifications to be accessed and ordered in multiple ways rather than in a single, pre-determined, [[taxonomy (general)|taxonomic]] order.<ref name="Faceted Search">[http://www.morganclaypool.com/doi/abs/10.2200/S00190ED1V01Y200904ICR005 Faceted Search], Morgan & Claypool, 2009</ref>\n\nFacets correspond to properties of the information elements. They are often derived by analysis of the text of an item using [[entity extraction]] techniques or from pre-existing fields in a database such as author, descriptor, language, and format. Thus, existing web-pages, product descriptions or online collections of articles can be augmented with navigational facets.\n\nWithin the academic community, faceted search has attracted interest primarily among [[library and information science]] researchers, and to some extent among [[computer science]] researchers specializing in [[information retrieval]].<ref name="sigir06">[http://facetedsearch.googlepages.com SIGIR\'2006 Workshop on Faceted Search - Call for Participation]</ref>\n\n==Development==\n\nThe [[Association for Computing Machinery]]\'s [[Special Interest Group on Information Retrieval]] provided the following description of the role of faceted search for a 2006 workshop:\n<blockquote>\nThe web search world, since its very beginning, has offered two paradigms:\n*Navigational search uses a hierarchy structure (taxonomy) to enable users to browse the information space by iteratively narrowing the scope of their quest in a predetermined order, as exemplified by [[Yahoo! Directory]], [[Open Directory Project|DMOZ]], etc.\n*Direct search allows users to simply write their queries as a bag of words in a text box. This approach has been made enormously popular by [[Web search engine]]s. \nOver the last few years, the direct search paradigm has gained dominance and the navigational approach became less and less popular. Recently, a new approach has emerged, combining both paradigms, namely the faceted search approach. Faceted search enables users to navigate a multi-dimensional information space by combining text search with a progressive narrowing of choices in each dimension. It has become the prevailing user interaction mechanism in e-commerce sites and is being extended to deal with [[semi-structured data]], continuous dimensions, and [[Folksonomy | folksonomies]].<ref name="sigir06">[http://facetedsearch.googlepages.com SIGIR\'2006 Workshop on Faceted Search - Call for Participation]</ref>\n</blockquote>\n\n==Mass market use==\n\nFaceted search has become a popular technique in commercial search applications, particularly for online retailers and libraries. An increasing number of [[List of Enterprise Search Vendors|enterprise search vendors]] provide software for implementing faceted search applications.\n\nOnline retail catalogs pioneered the earliest applications of faceted search, reflecting both the faceted nature of product data (most products have a type, brand, price, etc.) and the ready availability of the data in retailers\' existing information-systems. In the early 2000s retailers started using faceted search. A 2014 benchmark of 50 of the largest US based online retailers reveals that despite the benefits of faceted search, only 40% of the sites have implemented it. <ref name="Smashing Magazine: The Current State of E-Commerce Search (2014)">[http://www.smashingmagazine.com/2014/08/18/the-current-state-of-e-commerce-search/ Smashing Magazine: The Current State of E-Commerce Search] Retrieved on 2014-08-27.</ref> Examples include the filtering options that appear in the left column on [[amazon.com]] or [[Google Shopping]] after a keyword search has been performed.\n\n==Libraries and information science==\n\nIn 1933, the noted librarian [[S. R. Ranganathan|Ranganathan]] proposed a [[faceted classification]] system for library materials, known as [[colon classification]]. In the pre-computer era, he did not succeed in replacing the pre-coordinated [[Dewey Decimal Classification]] system.<ref name="Major classification systems : the Dewey Centennial">[https://archive.org/details/majorclassificat00alle Major classification systems : the Dewey Centennial]</ref>\n\nModern online library catalogs, also known as [[OPAC]]s, have increasingly adopted faceted search interfaces. Noted examples include the [[North Carolina State University]] library catalog (part of the Triangle Research Libraries Network) and the [[Online Computer Library Center|OCLC]] Open [[WorldCat]] system. The [[CiteSeerX]] project<ref>[http://citeseerx.ist.psu.edu/ CiteSeerX]. Citeseerx.ist.psu.edu. Retrieved on 2013-07-21.</ref> at the [[Pennsylvania State University]] allows faceted search for academic documents and continues to expand into other facets such as table search.\n\n==See also==\n* [[Enterprise search]]\n* [[Exploratory search]]\n* [[Faceted classification]]\n* [[Human–computer information retrieval]]\n* [[Information extraction]]\n* [[NoSQL]]\n\n==References==\n<References/>\n\n{{DEFAULTSORT:Faceted Search}}\n[[Category:Information retrieval techniques]]']
['Proximity search (text)', '1934622', 'In [[natural language processing|text processing]], a \'\'\'proximity search\'\'\' looks for documents where two or more separately matching term occurrences are within a specified [[string distance|distance]], where distance is the number of intermediate words or characters. In addition to proximity, some implementations may also impose a constraint on the word order, in that the order in the searched text must be identical to the order of the search query. Proximity searching goes beyond the simple matching of words by adding the constraint of proximity and is generally regarded as a form of advanced search.\n\nFor example, a search could be used to find "red brick house", and match phrases such as "red house of brick" or "house made of red brick". By limiting the proximity, these phrases can be matched while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.\n\n== Rationale ==\nThe basic linguistic assumption of proximity searching is that the proximity of the words in a document implies a [[semantic relation|relationship]] between the words. Given that authors of documents try to formulate sentences which contain a single idea, or cluster of related ideas within neighboring sentences or organized into paragraphs, there is an inherent, relatively high, probability within the document structure that words used together are related. On the other hand, when two words are on the opposite ends of a book, the probability of a relationship between the words is relatively weak. By limiting search results to only include matches where the words are within the specified maximum proximity, or distance, the search results are assumed to be of higher relevance than the matches where the words are scattered.\n\nCommercial internet search engines tend to produce too many matches (known as recall) for the average search query. Proximity searching is one method of reducing the number of pages matches, and to improve the relevance of the matched pages by using word proximity to assist in ranking. As an added benefit, proximity searching helps combat [[spamdexing]] by avoiding webpages which contain dictionary lists or shotgun lists of thousands of words, which would otherwise rank highly if the search engine was heavily biased toward [[word frequency]].\n\n== Boolean syntax and operators ==\nNote that a proximity search can designate that only some keywords must be within a specified distance. Proximity searching can be used with other search syntax and/or controls to allow more articulate search queries. Sometimes query operators like NEAR, NOT NEAR, FOLLOWED BY, NOT FOLLOWED BY, SENTENCE or FAR are used to indicate a proximity-search limit between specified keywords: for example, "brick NEAR house".\n\n== Usage in commercial search engines ==\nIn regards to implicit/automatic versus explicit proximity search, as of November 2008, most Internet [[search engine]]s only implement an implicit proximity search functionality. That is, they automatically rank those search results higher where the user keywords have a good "overall proximity score" in such results. If only two keywords are in the search query, this has no difference from an explicit proximity search which puts a NEAR operator between the two keywords. However, if three or more than three keywords are present, it is often important for the user to specify which subsets of these keywords expect a proximity in search results. This is useful if the user wants to do a [[prior art]] search (e.g. finding an existing approach to complete a specific task, finding a document that discloses a system that exhibits a procedural behavior collaboratively conducted by several components and links between these components).\n\n[[Web search engine]]s which support proximity search via an explicit proximity operator in their query language include  [[Walhello]], [[Exalead]], [[Yandex]], [[Yahoo!]], [[Altavista]], and [[Bing (search engine)|Bing]]:\n* When using the [[Walhello]] search-engine, the proximity can be defined by the number of characters between the keywords.<ref>[http://www.walhello.com/aboutgl.html "About Walhello"], visited 23 December 2009</ref>\n* The search engine Exalead allows the user to specify the required proximity, as the maximum number of words between keywords. The syntax is <tt>(keyword1 NEAR/n keyword2)</tt> where n is the number of words.<ref>[http://www.exalead.com/search/web/search-syntax/#proximity_search "Web Search Syntax"], visited 23 December 2009</ref>\n* [[Yandex]] uses the syntax <tt>keyword1 /n keyword2</tt> to search for two keywords separated by at most <math>n - 1</math> words, and supports a few other variations of this syntax.<ref>[http://help.yandex.ru/search/?id=481939 Yandex help page on query language] (in Russian)</ref>\n* [[Yahoo!]] and [[Altavista]] both support an undocumented NEAR operator.<ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+additional "Successful Yahoo! proximity query"] (22 Feb 2010)</ref><ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+unused "Unsuccessful Yahoo! proximity query"] (22 Feb 2010)</ref> The syntax is <tt>keyword1 NEAR keyword2</tt>.\n* [[Google Search]] supports AROUND(#).<ref>[http://www.guidingtech.com/16116/google-search-little-known-around-operator/ "GuidingTech: Meet Google Search\'s Little Known AROUND Operator"]</ref><ref>[http://www.netforlawyers.com/content/google-offers-proximity-search-around-connector-0015/ "Google Offers Proximity Search"] (8 Feb 2011)</ref>\n* [[Bing (search engine)|Bing]] supports NEAR.<ref>[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ "How to Use Bing’s Advanced Search Operators"]</ref> The syntax is <tt>keyword1 near:n keyword2</tt> where n=the number of maximum separating words.\n\nOrdered search within the [[Google]] and [[Yahoo!]] search engines is possible using the asterisk (*) full-word [[Wildcard character|wildcard]]s: in Google this matches one or more words,<ref>[http://www.google.com/support/websearch/bin/answer.py?answer=136861 "More Google Search Help" visited 23 December 2009]</ref> and an in Yahoo! Search this matches exactly one word.<ref>[http://www.searchengineshowdown.com/features/yahoo/review.html "Review of Yahoo! Search", by Search Engine Showdown, visited 23 December 2009]</ref>  (This is easily verified by searching for the following phrase in both Google and Yahoo!: "addictive * of biblioscopy".)\n\nTo emulate unordered search of the NEAR operator can be done using a combination of ordered searches.  For example, to specify a close co-occurrence of "house" and "dog", the following search-expression could be specified: "house dog" OR "dog house" OR "house * dog" OR "dog * house" OR "house * * dog" OR "dog * * house".\n\n== See also ==\n* [[Compound term processing]]\n* [[Edit distance]]\n* [[Information retrieval]]\n* [[Search engine]]\n* [[Search engine indexing]] - how texts are indexed to support proximity search\n* [[Semantic proximity]]\n\n== Notes ==\n{{Reflist}}\n\n[[Category:Information retrieval techniques]]\n[[Category:Internet search algorithms]]']
['Communication engine', '20895417', "{{Orphan|date=February 2009}}\nA '''communication engine''' is a tool that sends user requests to several other [[communication protocols]] and/or [[database]]s and aggregates the results into a single list or displays them according to their source. Communication engines enable users to enter communication account authorization once and access several communication avenues simultaneously. Communication engines operate on the premise that the [[World Wide Web]] is too large for any one engine to index it all and that more productive results can be obtained by combining the results from several engines dynamically. This may save the user from having to use multiple engines separately.\n\n[[Category:Information retrieval techniques]]\n[[Category:Computing terminology]]\n\n\n{{Web-stub}}"]
['Music alignment', '49926925', "[[Image:MusicAlignment_BeethovenFifth.png|thumb|300px|right|First theme of Symphony No. 5 by Ludwig van Beethoven in a sheet music, audio,\nand piano-roll representation. The red bidirectional arrows indicate the aligned time positions of corresponding note events in the different representations.]]\n\n[[Music]] can be described and represented in many different ways including [[sheet music]], symbolic representations, and audio recordings. For each of these representations, there may exist different versions that correspond to the same musical work. The general goal of '''music alignment''' (sometimes also referred to as '''music synchronization''') is to automatically link the various data streams, thus interrelating the multiple information sets related to a given musical work. More precisely, music alignment is taken to [[mean]] a procedure which, for a given position in one representation of a piece of music, determines the corresponding position within another representation.<ref name=Mueller15_Chapter3FMP_SPRINGER>\n{{cite book\n| last = Müller\n| first = Meinard\n| title = Music Synchronization. In Fundamentals of Music Processing, chapter 3, pages 115-166\n| url = http://www.music-processing.de\n| publisher = Springer\n| year = 2015\n| doi = 10.1007/978-3-319-21945-5\n| isbn = 978-3-319-21944-8 }}\n</ref> In the figure on the right, such an alignment is visualized by the red bidirectional arrows. Such [[synchronization]] results form the basis for novel interfaces that allow users to access, search, and browse musical content in a convenient way.<ref name=DammFTCKM12_DML_IJDL>\n{{cite journal\n|last1=Damm\n|first1=David \n|last2=Fremerey\n|first2=Christian\n|last3=Thomas\n|first3=Verena\n|last4=Clausen\n|first4=Michael\n|last5=Kurth\n|first5=Frank\n|last6=Müller\n|first6=Meinard\n|title=A digital library framework for heterogeneous music collections: from document acquisition to cross-modal interaction\n|url = http://link.springer.com/article/10.1007%2Fs00799-012-0087-y \n|journal=International Journal on Digital Libraries: Special Issue on Music Digital Libraries\n|volume=12\n|issue=2-3\n|year=2012\n|pages=53–71\n|doi=10.1007/s00799-012-0087-y}}\n</ref><ref name=MuellerCKEF10_Sync_ISR>\n{{cite journal\n|last1=Müller\n|first1=Meinard \n|last2=Clausen\n|first2=Michael\n|last3=Konz\n|first3=Verena\n|last4=Ewert\n|first4=Sebastian\n|last5=Fremerey\n|first5=Christian \n|title=A Multimodal Way of Experiencing and Exploring Music\n|url = https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/03-publications/2010_MuellerClausenKonzEwertFremerey_MusicSynchronization_ISR.pdf \n|journal=Interdisciplinary Science Reviews (ISR)\n|volume=35\n|issue=2\n|year=2010\n|pages=138–153\n|doi=10.1179/030801810X12723585301110}}</ref>\n\n==Basic procedure==\n[[File:MusicAlignment Procedure.png|thumb|300px|right|Overview of the processing pipeline of a typical music alignment procedure.]]\n\nGiven two different music representations, typical music alignment approaches proceed in two steps.<ref name=Mueller15_Chapter3FMP_SPRINGER/> In the first step, the two representations are transformed into sequences of suitable features. In general, such feature representations need to find a compromise between two conflicting goals. On the one hand, features should show a large degree of [[robustness]] to variations that are to be left unconsidered for the task at hand. On the other hand, features should capture enough characteristic information to accomplish the given task. For music alignment, one often uses '''[[chroma feature|chroma-based features]]''' (also called [[chromagram]]s or [[harmonic pitch class profiles|pitch class profiles]]), which capture harmonic and melodic characteristics of music, while being robust to changes in timbre and instrumentation, are being used.\n\nIn the second step, the derived feature sequences have to be brought into (temporal) correspondence. To this end, techniques related to '''[[dynamic time warping|dynamic time warping (DTW)]]''' or '''[[hidden Markov model|hidden Markov models (HMMs)]]''' are used to compute an optimal alignment between two given feature sequences.\n\n==Related tasks==\nMusic alignment and related synchronization tasks have been studied extensively within the field of [[music information retrieval]]. In the following, we give some pointers to related tasks. Depending upon the respective types of music representations, one can distinguish between various synchronization scenarios. For example, audio alignment refers to the task of temporally aligning two different audio recordings of a piece of music. Similarly, the goal of score–audio alignment is to coordinate note events given in the score representation with audio data. In the  [[offline]] scenario, the two data streams to be aligned are known prior to the actual alignment. In this case, one can use global optimization procedures such as [[dynamic time warping|dynamic time warping (DTW)]] to find an optimal alignment. In general, it is harder to deal with scenarios where the data streams are to be processed online. One prominent online scenario is known as '''[[score following]]''', where a musician is performing a piece according to a given musical score. The goal is then to identify the currently played musical events depicted in the score with high accuracy and low latency.<ref>\n{{cite journal\n|last1=Cont\n|first1=Arshia\n|title=A Coupled Duration-Focused Architecture for Real-Time Music-to-Score Alignment\n|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence\n|volume=32\n|issue=6\n|year=2010\n|pages=974–987\n|issn=0162-8828\n|doi=10.1109/TPAMI.2009.106}}</ref><ref>{{cite journal\n|last1=Orio\n|first1=Nicola\n|last2=Lemouton\n|first2=Serge\n|last3=Schwarz\n|first3=Diemo\n|title=Score following: State of the art and new developments\n|url = http://recherche.ircam.fr/equipes/temps-reel/suivi/resources/orio.2002.nime.pdf\n|journal=Proceedings of the International Conference on New Interfaces for Musical Expression (NIME)\n|date=2003\n|pages=36–41}}</ref> In this scenario, the score is known as a whole in advance, but the performance is known only up to the current point in time. In this context, alignment techniques such as hidden Markov models or particle filters have been employed, where the current score position and tempo are modeled in a statistical sense.<ref>\n{{cite journal\n|last1=Duan\n|first1=Zhiyao\n|last2=Pardo\n|first2=Bryan\n|journal = Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)\n|title=A state space model for online polyphonic audio-score alignment\n|url = http://www.ece.rochester.edu/~zduan/resource/DuanPardo_ScoreFollowing_ICASSP11.pdf\n|year=2011\n|pages=197–200\n|doi=10.1109/ICASSP.2011.5946374}}</ref><ref>{{cite journal\n|last1=Montecchio\n|first1=Nicola\n|last2=Cont\n|first2=Arshia\n|title=A unified approach to real time audio-to-score and audio-to-audio alignment using sequential Montecarlo inference techniques\n|url = http://articles.ircam.fr/textes/Montecchio11a/index.pdf\n|year=2011\n|pages=193–196\n|doi=10.1109/ICASSP.2011.5946373}}</ref> As opposed to classical DTW, such an online synchronization procedure inherently has a running time that is linear in the duration of the performed version. However, as a main disadvantage, an online strategy is very sensitive to local tempo variations and deviations from the score - once the procedure is out of sync, it is very hard to recover and return to the right track. A further online synchronization problem is known as '''[[Pop music automation#Automatic accompaniment|automatic accompaniment]]'''. Having a solo part played by a musician, the task of the computer is to accompany the musician according to a given score by adjusting the tempo and other parameters in real time. Such systems were already proposed some decades ago.<ref>{{cite journal\n|last1=Dannenberg\n|first1=Roger B.\n|title=An on-line algorithm for real-time accompaniment\n|journal=Proceedings of the International Computer Music Conference (ICMC)\n|url = http://www.cs.cmu.edu/~rbd/papers/icmc84accomp.pdf\n|date=1984\n|pages=193–198}}</ref><ref>\n{{cite journal\n|last1=Raphael\n|first1=Christopher\n|title=A probabilistic expert system for automatic musical accompaniment\n|journal = Journal of Computational and Graphical Statistics\n|url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6559&rep=rep1&type=pdf\n|year=2001\n|pages=487–512\n}}</ref><ref>\n{{cite journal\n|last2=Raphael\n|first2=Christopher\n|year=2006\n|title=Music score alignment and computer accompaniment\n|url=http://www.cs.cmu.edu/~rbd/papers/accompaniment-cacm-06.pdf\n|journal=Communications of the ACM\n|volume=49\n|issue=8\n|pages=38–43\n|doi=10.1145/1145287.1145311\n|issn=0001-0782\n|last1=Dannenberg\n|first1=Roger B.}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Music information retrieval]]\n[[Category:Music technology]]\n[[Category:Musicology]]\n[[Category:Information retrieval techniques]]"]
['XML retrieval', '21106742', '{{Multiple issues|\n{{expert-subject|Computer science|date=January 2015}}\n{{COI|date=February 2009}}\n}}\n\n\'\'\'XML retrieval\'\'\', or XML Information Retrieval, is the content-based retrieval of documents structured with [[XML]] (eXtensible Markup Language). As such it is used for computing [[Relevance (information retrieval)|relevance]] of XML documents.<ref>{{Cite web|url=ftp://ftp.tm.informatik.uni-frankfurt.de/pub/papers/ir/An%20Architecture%20for%20XML%20Information%20Retrieval%20in%20a%20Peer-to-Peer%20Environment_2007.pdf|title=An Architecture for XML Information Retrieval in a Peer-to-Peer Environment|last=Winter|first=Judith|author2=Drobnik, Oswald |date=November 9, 2007|publisher=ACM|accessdate=2009-02-10}}</ref>\n\n==Queries==\nMost XML retrieval approaches do so based on techniques from the [[information retrieval]] (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain [[Data structure|structural]] [[Hint (SQL)|hints]]. So-called "content and structure" (CAS) queries enable users to specify what structure the requested content can or must have.\n\n==Exploiting XML structure==\nTaking advantage of the [[Self-documenting|self-describing]] structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments.\n\n==Ranking==\nRanking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request.<ref name="INEX2006">{{Cite web|url=http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf |title=Overview of INEX 2006 |last=Malik |first=Saadia |author2=Trotman, Andrew |author3=Lalmas, Mounia |author4=Fuhr, Norbert |year=2007 |work=Proceedings of the Fifth Workshop of the INitiative for the Evaluation of XML Retrieval |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20081016101202/http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf |archivedate=October 16, 2008 }}</ref>\n\n==Existing XML search engines==\nAn overview of two potential approaches is available.<ref>{{Cite web|url=http://www.sigmod.org/record/issues/0612/p16-article-yahia.pdf|title=XML Search: Languages, INEX and Scoring|last=Amer-Yahia|first=Sihem|author2=Lalmas, Mounia |year=2006|publisher=SIGMOD Rec. Vol. 35, No. 4|accessdate=2009-02-10}} {{Dead link|date=October 2010|bot=H3llBot}}</ref><ref>{{Cite paper|citeseerx = 10.1.1.109.5986|title=XML Retrieval: A Survey|last=Pal|first=Sukomal|date=June 30, 2006|publisher=Technical Report, CVPR }}</ref> The INitiative for the Evaluation of XML-Retrieval (\'\'INEX\'\') was founded in 2002 and provides a platform for evaluating such [[algorithm]]s.<ref name="INEX2006" /> Three different areas influence XML-Retrieval:<ref name="INEX2002">{{Cite web|url=http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf |title=INEX: Initiative for the Evaluation of XML Retrieval |last=Fuhr |first=Norbert |author2=Gövert, N. |author3=Kazai, Gabriella |author4=Lalmas, Mounia |year=2003 |work=Proceedings of the First INEX Workshop, Dagstuhl, Germany, 2002 |publisher=ERCIM Workshop Proceedings, France |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20081121135758/http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf |archivedate=November 21, 2008 }}</ref>\n\n===Traditional XML query languages===\n[[Query language]]s such as the [[W3C]] standard [[XQuery]]<ref>{{Cite web|url=http://www.w3.org/TR/2007/REC-xquery-20070123/|title=XQuery 1.0: An XML Query Language|last=Boag|first=Scott|author2=Chamberlin, Don |author3=Fernández, Mary F. |author4=Florescu, Daniela |author5=Robie, Jonathan |author6= Siméon, Jérôme |date=23 January 2007|work=W3C Recommendation|publisher=World Wide Web Consortium|accessdate=2009-02-10}}</ref> supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents\' [[Database schema|schemas]].<ref name="Schlieder2002">{{Cite journal|url=http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz |title=Querying and Ranking XML Documents |last=Schlieder |first=Torsten |author2=Meuss, Holger |year=2002 |work=Journal of the American Society for Information Science and Technology, Vol. 53, No. 6 |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20070610002349/http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz |archivedate=June 10, 2007 }}</ref>\n\n===Databases===\nClassic [[database]] systems have adopted the possibility to store [[Semi-structured model|semi-structured data]]<ref name="INEX2002" /> and resulted in the development of [[XML database]]s. Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries.\n\n===Information retrieval===\nClassic information retrieval models such as the [[vector space model]] provide relevance ranking, but do not include document structure; only flat queries are  supported. Also, they apply a static document concept, so retrieval units usually are entire documents.<ref name="Schlieder2002"/> They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document [[subtree]]s (index terms plus structure) as dimensions of the vector space.<ref>{{Cite web|url=http://www.cobase.cs.ucla.edu/tech-docs/sliu/SIGIR04.pdf|title=Configurable Indexing and Ranking for XML Information Retrieval|last=Liu|first=Shaorong|author2=Zou, Qinghua |author3=Chu, Wesley W. |year=2004|work=SIGIR\'04|publisher=ACM|accessdate=2009-02-10}}</ref>\n\n==See also==\n*[[Document retrieval]]\n*[[Information retrieval applications]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Xml-Retrieval}}\n[[Category:XML]]\n[[Category:Information retrieval genres]]']
['Multimedia search', '5987236', "'''Multimedia search''' enables information [[Search engine technology|search]] using queries in multiple data types including text and other [[multimedia]] formats.\nMultimedia search can be implemented through [[multimodal search]] interfaces, i.e., interfaces that allow to submit [[search queries]] not only as textual requests, but also through other media.\nWe can distinguish two methodologies in multimedia search:\n*'''Metadata search''': the search is made on the layers of [[metadata]].\n* '''[[Query by example]]''': The interaction consists in submitting a piece of information (e.g., a video, an image, or a piece of audio) at the purpose of finding similar multimedia items.\n\n==Metadata search==\n\nSearch is made using the layers in metadata which contain information of the content of a multimedia file. Metadata search is easier, faster and effective because instead of working with complex material, such as an audio, a video or an image, it searches using text.\n\nThere are three processes which should be done in this method:\n*'''[[Multimedia information retrieval#Feature extraction methods|Summarization of media content]]''' ([[feature extraction]]). The result of feature extraction is a description.\n*'''[[Multimedia information retrieval#Feature extraction methods|Filtering of media descriptions]]''' (for example, elimination of [[Redundancy (linguistics)|Redundancy]])\n*'''[[Multimedia information retrieval#Categorization methods|Categorization of media descriptions]]''' into classes.\n\n==Query by example==\n\nIn [[query by example]], the element used to search is a [[multimedia]] content (image, audio, video). In other words, the query is a media. Often, it's used [[Search engine indexing|audiovisual indexing]]. It will be necessary to choose the criteria we are going to use for creating metadata. The process of search can be divided in three parts:\n*Generate descriptors for the media which we are going to use as query and the descriptors for the media in our [[database]].\n*Compare descriptors of the query and our database’s media.\n*List the media sorted by maximum coincidence.\n\n==Multimedia search engine==\nThere are two big search families, in function of the content:\n* [[Visual search engine]]\n*[[Audio search engine]]\n\n===Visual search engine===\nInside this family we can distinguish two topics: [[image search]] and [[video search]]\n\n*'''[[Image search]]''': Although usually it's used simple metadata search, increasingly is being used indexing methods for making the results of users queries more accurate using [[query by example]]. For example, [[QR codes]].\n*'''[[Video search]]''': Videos can be searched for simple metadata or by complex metadata generated by indexing. The audio contained in the videos is usually scanned by audio search engines.\n\n===Audio search engine===\nThere are different methods of [[Audio search engine|audio searching]]:\n*Voice search engine: Allows the user to search using speech instead of text. It uses algorithms of [[speech recognition]]. An example of this technology is [[Google Voice Search]].\n*Music search engine: Although most of applications which searches music works on simple metadata (artist, name of track, album…) . There are some programs of [[music recognition]], for example [[Shazam (service)|Shazam]] or [[SoundHound]].\n\n==See also==\n*''[[Journal of Multimedia]]''\n*[[List of search engines#Multimedia|List of search engines]]\n*[[Multimedia]]\n*[[Multimedia information retrieval]]\n*[[Search engine indexing]]\n*[[Streaming media]]\n*[[Video search engine]]\n\n==External links==\n\n[[Category:Information retrieval genres]]\n[[Category:Multimedia]]"]
['Cross-language information retrieval', '296950', '{{refimprove|date=September 2014}}\n\n\'\'\'Cross-language information retrieval (CLIR)\'\'\' is a subfield of [[information retrieval]] dealing with retrieving information written in a language different from the language of the user\'s query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques.<ref>"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]", Mittal et al., IJIIDS, 5(2), 119-142, 2011.</ref>  CLIR techniques can be classified into different categories based on different translation resources: \n* Dictionary-based CLIR techniques\n* Parallel corpora based CLIR techniques\n* Comparable corpora based CLIR techniques\n* Machine translator based CLIR techniques\n\nThe first workshop on CLIR was held in Zürich during the SIGIR-96 conference.<ref>The proceedings of this workshop can be found in the book \'\'Cross-Language Information Retrieval\'\' (Grefenstette, ed; Kluwer, 1998) ISBN 0-7923-8122-X.</ref> Workshops have been held yearly since 2000 at the meetings of the [[Cross Language Evaluation Forum]] (CLEF).\n\nThe term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.\n\n[[Google Search]] had a cross-language search feature that was removed in 2013.<ref>{{cite web|url=http://searchengineland.com/google-drops-translated-foreign-pages-search-option-due-to-lack-of-use-160157|title=Google Drops "Translated Foreign Pages" Search Option Due To Lack Of Use|date=20 May 2013|publisher=}}</ref>\n\n==See also==\n*[[EXCLAIM]] (EXtensible Cross-Linguistic Automatic Information Machine)\n\n==References==\n<references />\n\n==External links==\n*[http://www.glue.umd.edu/~oard/research.html A resource page for CLIR]\n*[http://www.2lingual.com/ A search engine for CLIR]\n{{DEFAULTSORT:Cross-Language Information Retrieval}}\n[[Category:Information retrieval genres]]\n[[Category:Natural language processing]]\n\n\n{{linguistics-stub}}']
['Exploratory search', '4881262', '\'\'\'Exploratory search\'\'\' is a specialization of information exploration which represents the activities carried out by searchers who are:<ref>Ryen W. White and Resa A. Roth (2009). \'\'Exploratory Search: Beyond the Query-Response Paradigm\'\', San Rafael, CA: Morgan and Claypool.</ref>\n* unfamiliar with the domain of their goal (i.e. need to learn about the topic in order to understand how to achieve their goal)\n* or unsure about the ways to achieve their goals (either the technology or the process)\n* or unsure about their goals in the first place.\n\nConsequently, exploratory search covers a broader class of activities than typical [[information retrieval]], such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; [[exploratory data analysis]] is another example of an information exploration activity. Typically, therefore, such users generally combine querying and browsing strategies to foster learning and investigation.\n\n==History==\nExploratory search is a topic that has grown from the fields of [[information retrieval]] and [[information seeking]] but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a [[Google]]-like keyword search). The research is motivated by questions like "what if the user doesn\'t know which keywords to use?" or "what if the user isn\'t looking for a single answer?". Consequently, research has begun to focus on defining the broader set of \'\'information behaviors\'\' in order to learn about the situations when a user is, or feels, limited by only having the ability to perform a keyword search.\n\nIn the last few years,{{When|date=April 2016}} a series of workshops has been held at various related and key events. In 2005, the Exploratory Search Interfaces workshop focused on beginning to define some of the key challenges in the field.<ref>{{cite web|url=http://research.microsoft.com/~ryenw/xsi/index.html|title=HCIL SOH 2005 Workshop on Exploratory Search Interfaces|publisher=Microsoft|accessdate=8 April 2016}}</ref> Since then a series of other workshops has been held at related conferences: Evaluating Exploratory Search<ref>{{cite web|url=http://research.microsoft.com/~ryenw/eess/index.html|title=SIGIR 2006 Workshop - Evaluating Exploratory Search Systems|publisher=Microsoft|accessdate=8 April 2016}}</ref> at SIGIR06<ref>{{cite web|url=http://www.sigir2006.org|title=Sigir 2006|publisher=|accessdate=8 April 2016}}</ref> and Exploratory Search and HCI<ref>{{cite web|url=http://research.microsoft.com/~ryenw/esi/index.html|title=CHI 2007 Workshop - Exploratory Search and HCI|publisher=Microsoft|accessdate=8 April 2016}}</ref> at CHI07<ref>{{cite web|url=http://www.chi2007.org|title=CHI 2007 Reach Beyond - welcome|publisher=|accessdate=8 April 2016}}</ref> (in order to meet with the experts in [[human–computer interaction]]).\n\nIn March 2008, an \'\'Information Processing and Management\'\' special issue<ref>{{cite web|url=http://www.sciencedirect.com/science/journal/03064573|title=Information Processing & Management|publisher=|accessdate=8 April 2016}}</ref><ref>Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). \'\'Evaluating exploratory search systems: Introduction to special topic issue of information processing and management\'\' Vol. 44, Issue 2, (2008), pp.&nbsp;433–436</ref> focused particularly on the challenges of evaluating exploratory search, given the reduced assumptions that can be made about scenarios of use.\n\nIn June 2008, the [[National Science Foundation]] sponsored an invitational workshop to identify a research agenda for exploratory search and similar fields for the coming years.<ref>{{cite web|url=http://www.ils.unc.edu/ISSS_workshop/|title=Moved|publisher=|accessdate=8 April 2016}}</ref>\n\n==Research challenges==\n\n===Important scenarios===\nWith the majority of research in the [[information retrieval]] community focusing on typical keyword search scenarios, one challenge for exploratory search is to further understand the scenarios of use for when keyword search is not sufficient. An example scenario, often used to motivate the research by mSpace,<ref>[http://mspace.fm mSpace]</ref> states: if a user does not know much about classical music, how should they even begin to find a piece that they might like.\n\n===Designing new interfaces===\nWith one of the motivations being to support users when keyword search is not enough, some research has focused on identifying alternative user interfaces and interaction models that support the user in different ways. An example is [[Faceted classification|faceted search]] which presents diverse category-style options to the users, so that they can choose from a list instead of guess a possible keyword query.\n\nMany of the [[human–computer information retrieval|interactive forms of search]], including [[faceted browser]]s, are being considered for their support of exploratory search conditions.\n\nComputational cognitive models of exploratory search have been developed to capture the cognitive complexities involved in exploratory search. Model-based dynamic presentation of information cues are proposed to facilitate exploratory search performance.<ref>Fu, W.-T., Kannampalill, T. G., & Kang, R. (2010). [http://portal.acm.org/citation.cfm?id=1719970.1719998 Facilitating exploratory search by model-based navigational cues.] In Proceedings of the ACM International conference on Intelligent User Interface. 199–208. </ref>\n\n===Evaluating interfaces===\nAs the tasks and goals involved with exploratory search are largely undefined or unpredictable, it is very hard to evaluate systems with the measures often used in information retrieval. Accuracy was typically used to show that a user had found a correct answer, but when the user is trying to summarize a domain of information, the \'\'correct\'\' answer is near impossible to identify, if not entirely subjective (for example: possible hotels to stay in Paris). In exploration, it is also arguable that spending more time (where time efficiency is typically desirable) researching a topic shows that a system provides increased support for investigation. Finally, and perhaps most importantly, giving study participants a well specified task could immediately prevent them from exhibiting exploratory behavior.{{cn|date=April 2016}}\n\n===Models of exploratory search behavior===\nThere have been recent{{When|date=April 2016}} attempts to develop a process model of exploratory search behavior, especially in social information system (e.g., see [[models of collaborative tagging]].<ref>{{Citation\n  | doi = 10.1145/1460563.1460600\n  | last1 = Fu  | first1 = Wai-Tat\n  | title = The Microstructures of Social Tagging: A Rational Model\n  | journal = Proceedings of the ACM 2008 conference on Computer Supported Cooperative Work.\n  | pages = 66–72\n  | date = April 2008\n  | url = http://portal.acm.org/citation.cfm?id=1460600\n  | isbn = 978-1-60558-007-4 }}\n</ref>\n<ref>{{Citation\n  | last1 = Fu  | first1 = Wai-Tat\n  | title = A Semantic Imitation Model of Social Tagging\n  | journal = Proceedings of the IEEE conference on Social Computing\n  | pages = 66–72\n  | date = Aug 2009\n  | url = http://www.humanfactors.illinois.edu/Reports&PapersPDFs/IEEESocialcom09/A%20Semantic%20Imitation%20Model%20of%20Social%20Tag%20Choices%20(2).pdf }}</ref> The process model assumes that user-generated information cues, such as social tags, can act as navigational cues that facilitate exploration of information that others have found and shared with other users on a social information system (such as [[social bookmarking]] system). These models provided extension to existing process model of information search that characterizes information-seeking behavior in traditional fact-retrievals using search engines.<ref>\n{{Citation\n  | last1 = Fu  | first1 = Wai-Tat\n  | last2 = Pirolli  | first2 = Peter\n  | title = SNIF-ACT: a cognitive model of user navigation on the world wide web\n  | journal = Human-Computer Interaction\n  | pages = 335–412\n  | year = 2007\n  | url = http://portal.acm.org/citation.cfm?id=1466608\n  | volume = 22}}</ref><ref>Kitajima, M., Blackmon, M. H., & Polson, P. G. (2000). A comprehension-based model of Web navigation and its application to Web usability analysis. In S. Mc-Donald, Y. Waern, & G. Cockton (Eds.), People and computers XIV—Usability or else!\nNew York: Springer-Verlag.</ref><ref>Miller, C. S., & Remington, R.W. (2004). Modeling information navigation: Implications for information architecture. Human Computer Interaction, 19, 225–271.</ref>\nRecent{{When|date=April 2016}} development in exploratory search is often concentrated in predicting users\' search intents in interaction with the user.<ref>\n{{Citation\n  | last1 = Ruotsalo  | first1 = Tuukka\n  | last2 = Athukorala  | first2 = Kumaripaba\n  | last3 = Glowacka  | first3 = Dorota\n  | last4 = Konuyshkova  | first4 = Ksenia\n  | last5 = Oulasvrita  | first5 = Antti\n  | last6 = Kaipiainen  | first6 = Samuli\n  | last7 = Kaski  | first7 = Samuel\n  | last8 = Jacucci  | first8 = Giulio\n  | title = Supporting exploratory search tasks with interactive user modeling\n  | journal = Proceedings of the 76th Annual Meeting of the American Society for Information Science and Technology ASIS&T\n  | year = 2013}}\n</ref>\nSuch predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs.<ref>\n{{Citation\n  | last1 = Ruotsalo  | first1 = Tuukka\n  | last2 = Peltonen  | first2 = Jaakko\n  | last3 = Eugster | first3 = Manuel J.A.\n  | last4 = Glowacka  | first4 = Dorota\n  | last5 = Konuyshkova  | first5 = Ksenia\n  | last6 = Athukorala  | first6 = Kumaripaba\n  | last7 = Kosunen | first7 = Ilkka   \n  | last8 = Reijonen  | first8 = Aki\n  | last9 = Myllymäki | first9 = Petri\n  | last10 = Kaski  | first10 = Samuel\n  | last11 = Jacucci  | first11 = Giulio\n  | title = Directing Exploratory Search with Interactive Intent Modeling\n  | journal = Proceedings of the ACM Conference of Information and Knowledge Management CIKM\n  | year = 2013}}\n</ref>\n<ref>\n{{Citation\n  | last1 = Glowacka  | first1 = Dorota\n  | last2 = Ruotsalo  | first2 = Tuukka\n  | last3 = Konuyshkova  | first3 = Ksenia\n  | last4 = Athukorala  | first4 = Kumaripaba\n  | last5 = Kaski  | first5 = Samuel\n  | last6 = Jacucci  | first6 = Giulio\n  | title = Directing exploratory search: Reinforcement learning from user interactions with keywords\n  | journal = Proceedings of the ACM Conference of Intelligent User Interfaces IUI\n  | url = http://dl.acm.org/citation.cfm?id=2449413\n  | pages = 117–128\n  | year = 2013}}\n</ref>\n\n==Major figures==\nKey figures, including experts from both [[information seeking]] and [[human–computer interaction]], are:{{Says who|date=April 2016}}\n* [[Marcia Bates]]\n* Nicholas Belkin<ref>{{cite web|url=http://comminfo.rutgers.edu/~belkin/|title=Nick\'s home page|publisher=|accessdate=17 April 2016}}</ref>\n* Gary Marchionini<ref>{{cite web|url=http://ils.unc.edu/~march|title=Gary\'s Home Page|publisher=|accessdate=8 April 2016}}</ref>\n* m.c. schraefel<ref>{{cite web|url=http://users.ecs.soton.ac.uk/mc|title=m.c. schraefel: design for innovation, creativity, discovery|publisher=|accessdate=8 April 2016}}</ref>\n* Ryen White<ref>{{cite web|url=http://research.microsoft.com/~ryenw|title=Ryen W. White|publisher=Microsoft|accessdate=8 April 2016}}</ref>\n\n==References==\n<references />\n\n==Sources==\n# White, R.W., Kules, B., Drucker, S.M., and schraefel, m.c. (2006). \'\'Supporting Exploratory Search\'\', Introduction to Special Section of Communications of the ACM, Vol. 49, Issue 4, (2006), pp.&nbsp;36–39.\n# Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). \'\'Evaluating exploratory search systems: Introduction to special topic issue of information processing and management\'\' Vol. 44, Issue 2, (2008), pp.&nbsp;433–436\n# Ryen W. White and Resa A. Roth (2009). \'\'Exploratory Search: Beyond the Query-Response Paradigm\'\', San Rafael, CA: Morgan and Claypool.\n# P. Papadakos, S. Kopidaki, N. Armenatzoglou and Y. Tzitzikas (2009). \'\'Exploratory Web Searching with Dynamic Taxonomies and Results Clustering\'\',13th European Conference on Digital Libraries (ECDL\'09), Corfu, Greece, Sep-Oct 2009\n\n{{DEFAULTSORT:Exploratory Search}}\n[[Category:Human–computer interaction]]\n[[Category:Information retrieval genres]]\n[[Category:Information science]]']
['Multimedia information retrieval', '33407925', '{{COI|date=July 2014}}\n{{Original research|date=July 2014}}\n{{Use dmy dates|date=February 2012}}\n\'\'\'Multimedia information retrieval\'\'\' (\'\'\'MMIR\'\'\' or \'\'\'MIR\'\'\') is a research discipline of [[computer science]] that aims at extracting semantic information from [[multimedia]] data sources.<ref name=Eidenberger>H Eidenberger. \'\'Fundamental Media Understanding\'\', atpress, 2011, p. 1.</ref>{{FV|date=July 2014}} Data sources include directly perceivable media such as [[Content (media and publishing)|audio]], [[image]] and [[video]], indirectly perceivable sources such as [[Written language|text]], biosignals as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups:\n\n# Methods for the summarization of media content ([[feature extraction]]). The result of feature extraction is a description.\n# Methods for the filtering of media descriptions (for example, elimination of [[Data redundancy|redundancy]])\n# Methods for the [[categorization]] of media descriptions into classes.\n\n== Feature extraction methods ==\n\nFeature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness.<ref name=Eidenberger/>{{rp|2}}{{FV|date=July 2014}} Generally, two possible goals can be achieved by feature extraction:\n\n* Summarization of media content. Methods for summarization include in the audio domain, for example, [[Mel-frequency cepstrum|mel-frequency cepstral coefficients]], Zero Crossings Rate, Short-Time Energy. In the visual domain, color histograms<ref>A Del Bimbo. \'\'Visual Information Retrieval\'\', Morgan Kaufmann, 1999.</ref> such as the [[MPEG-7]] Scalable Color Descriptor can be used for summarization.\n* Detection of patterns by [[auto-correlation]] and/or [[cross-correlation]]. Patterns are recurring media chunks that can either be detected by comparing chunks over the media dimensions (time, space, etc.) or comparing media chunks to templates (e.g. face templates, phrases). Typical methods include Linear Predictive Coding in the audio/biosignal domain,<ref>HG Kim, N Moreau, T Sikora.\'\' MPEG-7 Audio and Beyond", Wiley, 2005.</ref> texture description in the visual domain and n-grams in text information retrieval.\n\n== Merging and filtering methods ==\n\nMultimedia Information Retrieval implies that multiple channels are employed for the understanding of media content.<ref>MS Lew (Ed.). \'\'Principles of Visual Information Retrieval\'\', Springer, 2001.</ref> Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions – as they frequently occur in motion description – have to be normalized to a fixed length first.\n\nFrequently used methods for description filtering include [[factor analysis]] (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the [[Kalman filter]] are used for merging of descriptions.\n\n== Categorization methods ==\n\nGenerally, all forms of machine learning can be employed for the categorization of multimedia descriptions<ref name=Eidenberger/>{{rp|125}}{{FV|date=July 2014}} though some methods are more frequently used in one area than another. For example, [[hidden Markov models]] are state-of-the-art in [[speech recognition]], while [[dynamic time warping]] – a semantically related method – is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following:\n\n* Metric approaches ([[Cluster analysis]], [[vector space model]], [[Minkowski]] distances, dynamic alignment)\n* Nearest Neighbor methods ([[K-nearest neighbors algorithm]], K-means, [[self-organizing map]])\n* Risk Minimization (Support vector regression, [[support vector machine]], [[linear discriminant analysis]])\n* Density-based Methods (Bayes nets, [[Markov process]]es, mixture models)\n* Neural Networks ([[Perceptron]], associative memories, spiking nets)\n* Heuristics ([[Decision trees]], random forests, etc.)\n\nThe selection of the best classifier for a given problem (test set with descriptions and class labels, so-called [[ground truth]]) can be performed automatically, for example, using the [[Weka]] Data Miner.\n\n== Open problems ==\n\nThe quality of MMIR Systems<ref>JC Nordbotten. "[http://nordbotten.com/ADM/ADM_book/MIRS-frame.htm Multimedia Information Retrieval Systems]". Retrieved 14 October 2011.</ref> depends heavily on the quality of the training data. Discriminative descriptions can be extracted from media sources in various forms. Machine learning provides categorization methods for all types of data. However, the classifier can only be as good as the given training data. On the other hand, it requires considerable effort to provide class labels for large databases. The future success of MMIR will depend on the provision of such data.<ref>H Eidenberger. \'\'Frontiers of Media Understanding\'\', atpress, 2012.</ref> The annual [[TRECVID]] competition is currently one of the most relevant sources of high-quality ground truth.\n\n== Related areas ==\n\nMMIR provides an overview over methods employed in the areas of information retrieval.<ref>H Eidenberger. \'\'Professional Media Understanding\'\', atpress, 2012.</ref><ref>{{cite journal |last=Raieli |first=Roberto ||date= |title=Introducing Multimedia Information Retrieval to libraries |url=http://leo.cineca.it/index.php/jlis/article/view/11530 |journal=JLIS.it |volume=7 |issue=3 |pages=9-42 |doi=10.4403/jlis.it-11530 |access-date=8 October 2016 }}</ref> Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as:\n\n* [[Bioinformatics|Bioinformation analysis]]\n* [[Biosignal|Biosignal processing]]\n* [[Content-based image retrieval|Content-based image and video retrieval]]\n* [[Facial recognition system|Face recognition]]\n* [[Music information retrieval|Audio and music classification]]\n* [[Speech recognition]]\n* [[Technical analysis|Technical chart analysis]]\n* [[Video Browsing|Video browsing]]\n* [[Information retrieval|Text information retrieval]]\n\nThe \'\'Journal of Multimedia Information Retrieval\'\'<ref>"[http://www.springer.com/computer/journal/13735 Journal of Multimedia Information Retrieval]", Springer, 2011, Retrieved 21 October 2011.</ref> documents the development of MMIR as a research discipline that is independent of these areas. See also \'\'Handbook of Multimedia Information Retrieval\'\'<ref>H Eidenberger. \'\'Handbook of Multimedia Information Retrieval\'\', atpress, 2012.</ref> for a complete overview over this research discipline.\n\n==References==\n{{reflist}}\n\n[[Category:Information retrieval genres]]']
['Maarten de Rijke', '31200516', '[[File:Maarten de Rijke - CLEF 2011 (cropped).jpg|thumb|Maarten de Rijke, 2011]]\'\'\'Maarten de Rijke\'\'\' (born 1 August 1961) is a [[Netherlands|Dutch]] computer scientist. His work initially focused on [[modal logic]] and [[knowledge representation]], but since the early years of the 21st century he has worked mainly in [[information retrieval]]. His work is supported by grants from the [[Nederlandse Organisatie voor Wetenschappelijk Onderzoek]] (NWO), public-private partnerships, and the European Commission (under the Sixth and Seventh Framework programmes).\n\n\n==Biography==\nMaarten de Rijke was born in [[Vlissingen]].  He studied philosophy (MSc 1989) and mathematics (MSc 1990) and wrote a PhD thesis, defended in 1993, on extended modal logics, under the supervision of [[Johan van Benthem (logician)|Johan van Benthem]].\n\nDe Rijke worked as a postdoc at the [[Centrum Wiskunde & Informatica]], before becoming a Warwick Research Fellow at the [[University of Warwick]]. He joined the [[University of Amsterdam]] in 1998, and was appointed professor of Information Processing and Internet at the [[Informatics Institute]] of the University of Amsterdam in 2004.<ref name="MdR11Bio">[http://staff.science.uva.nl/~mdr/Bio/ Bio of Maarten de Rijke] at the University of Amsterdam. Retrieved 16 March 2011.</ref>\n\nHe leads the Information and Language Processing group<ref>[http://ilps.science.uva.nl Information and Language Processing group]</ref> at the University of Amsterdam, the Intelligent Systems Lab Amsterdam<ref>[http://isla.science.uva.nl Intelligent Systems Lab Amsterdam] within the Informatics Institute of the University of Amsterdam.</ref> and the Center for Creation, Content and Technology.<ref>[http://www.ccct.uva.nl Center for Creation, Content and Technology] at the University of Amsterdam.</ref>\n\n==Work==\nDuring the first ten years of his scientific career Maarten de Rijke worked on formal and applied aspects of modal logic. At the start of the 21st century, De Rijke switched to information retrieval. He has since worked on [[XML retrieval]], [[question answering]], [[expert finding]] and [[social media analysis]].\n\n==Publications==\nMaarten de Rijke has published more than 600 papers and books.<ref name="MdR11Pubs">[http://staff.science.uva.nl/~mdr/Publications/ List of publications of Maarten de Rijke] at the University of Amsterdam". Retrieved 16 March 2011.</ref>\n\n==References==\n{{reflist}}\n*[http://albumacademicum.uva.nl/cgi/b/bib/bib-idx?type=simple;lang=en;c=ap;rgn1=entirerecord;q1=rijke;x=0;y=0;cc=ap;view=reslist;sort=achternaam;fmt=long;page=reslist;size=1;start=14 Prof. dr. M. de Rijke, 1961 -] at the [[University of Amsterdam]] \'\'Album Academicum\'\' website\n\n==External links==\n* [http://staff.science.uva.nl/~mdr Home page]\n\n{{DEFAULTSORT:Rijke, Maarten De}}\n[[Category:1961 births]]\n[[Category:Living people]]\n[[Category:Dutch computer scientists]]\n[[Category:University of Amsterdam alumni]]\n[[Category:University of Amsterdam faculty]]\n[[Category:People from Vlissingen]]\n[[Category:Information retrieval researchers]]']
['C. J. van Rijsbergen', '1514191', '{{Use dmy dates|date=March 2014}}\n{{Use British English|date=March 2014}}\n{{multiple issues|\n{{BLP sources|date=January 2012}}\n{{More footnotes|date=January 2012}}\n}}\n\n{{Infobox scientist\n|name              = Cornelis Joost van Rijsbergen\n|image             = C J van Rijsbergen.jpg\n|image_size        =\n|caption           = C. J. "Keith" van Rijsbergen\n|birth_date        = {{birth year and age|1943}}\n|birth_place       = [[Rotterdam]]\n|residence         = \n|citizenship       =\n|nationality       = \n|fields            = [[Information Retrieval]]\n|workplaces        = [[Monash University]], [[University of Glasgow]]\n|alma_mater        = [[University of Western Australia]], [[University of Cambridge]]\n|doctoral_advisor  = \n|academic_advisors =\n|doctoral_students =\n|notable_students  =\n|known_for         = \n|author_abbrev_bot =\n|author_abbrev_zoo =\n|influences        =\n|influenced        =\n|awards            =\n|signature         = <!--(filename only)-->\n|footnotes         =\n}}\n\n\'\'\'C. J. "Keith" van Rijsbergen\'\'\' [[FREng]]<ref name=fellow>{{cite web|title=List of Fellows|url=http://www.raeng.org.uk/about-us/people-council-committees/the-fellowship/list-of-fellows}}</ref> (\'\'\'Cornelis Joost van Rijsbergen\'\'\') (born 1943) is a professor of [[computer science]] and the leader of the Glasgow Information Retrieval Group based at the [[University of Glasgow]]. He is one of the founders of modern [[Information Retrieval]] and the author of the seminal monograph \'\'Information Retrieval\'\' and of the textbook \'\'The Geometry of Information Retrieval\'\'.\n\nHe was born in [[Rotterdam]], and educated in the [[Netherlands]], [[Indonesia]], [[Namibia]] and [[Australia]].\nHis first degree is in mathematics from the [[University of Western Australia]], and in 1972 he completed a\nPhD in computer science at the [[University of Cambridge]].\nHe spent three years lecturing in information retrieval and artificial intelligence at [[Monash University]]\nbefore returning to [[University of Cambridge|Cambridge]] to hold a [[Royal Society]] Information Research Fellowship. \nIn 1980 he was appointed to the chair of computer science at [[University College Dublin]];\nfrom there he moved in 1986 to [[Glasgow University]].\nSince 2007 he has been Chairman of the Scientific Board of the [[Information Retrieval Facility]].\n\n==Awards and honors==\nIn 2003 he was inducted as a Fellow of the [[Association for Computing Machinery]]. In 2004 he was awarded the [[Tony Kent Strix award]].\nIn 2004 he was appointed a [[Fellow]]<ref name=fellow /> of the [[Royal Academy of Engineering]].<ref name=fellow />\nIn 2006, he was awarded the [[Gerard Salton Award]] for \'\'Quantum haystacks\'\'.\n\n==See also==\n*[[F1 score]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.dcs.gla.ac.uk/~keith/ C. J. "Keith" van Rijsbergen - The University of Glasgow]\n*[http://ir.dcs.gla.ac.uk/ Glasgow Information Retrieval Group]\n*[http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval book - C. J. van Rijsbergen 1979]\n*[http://www.ir-facility.org/ Information Retrieval Facility]\n*{{worldcat id|id=lccn-n83-236586}}\n* [http://www.alanmacfarlane.com/ancestors/rijsbergen.htm Keith van Rijsbergen interviewed by Alan Macfarlane 15 July 2009 (film)]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Rijsbergen, C. J. van}}\n[[Category:1943 births]]\n[[Category:Living people]]\n[[Category:Dutch computer scientists]]\n[[Category:Fellows of the Association for Computing Machinery]]\n[[Category:People from Rotterdam]]\n[[Category:University of Western Australia alumni]]\n[[Category:Information retrieval researchers]]\n\n\n{{Netherlands-scientist-stub}}\n{{compu-scientist-stub}}']
['Cutter Expansive Classification', '7515', 'The \'\'\'Cutter Expansive Classification\'\'\' system is a [[library classification]] system devised by [[Charles Ammi Cutter]]. The system was the basis for the top categories of the [[Library of Congress Classification]].<ref>LaMontagne, Leo E. \'\'American Library Classification: With Special Reference to the Library of Congress\'\'. Hamden, CT, Shoe String Press. 1961, p. 226.</ref>\n\n==History of the Expansive Classification==\n[[Charles Ammi Cutter]] (1837&ndash;1903), inspired by the decimal classification of his contemporary [[Melvil Dewey]], and with Dewey\'s initial encouragement, developed his own classification scheme for the Winchester Town Library and then the [[Boston Athenaeum]],<ref>LaMontagne, Leo E. \'\'American Library Classification: With Special Reference to the Library of Congress\'\'. Hamden, CT, Shoe String Press. 1961, p. 208.</ref> at which he served as librarian for twenty-four years. He began work on it around the year 1880, publishing an overview of the new system in 1882. The same classification would later be used, but with a different notation, also devised by Cutter, at the [[Cary Memorial Library|Cary Library]] in [[Lexington, Massachusetts]].<ref>Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 \'\'Expansive Classification: Part I: The First Six Classifications\'\']. Boston, C. A. Cutter. 1891–93, p. 1.</ref>\n\nMany libraries found this system too detailed and complex for their needs, and Cutter received many requests from librarians at small libraries who wanted the classification adapted for their collections. He devised the Expansive Classification in response, to meet the needs of growing libraries, and to address some of the complaints of his critics\n.<ref>For the Expansive Classification as a response to Cutter\'s critics, see: Miksa, Francis L., ed. \'\'Charles Ammi Cutter: Library Systematizer". Littleton, CO, Libraries Unlimited. 1977, p. 58.\n* For the Expansive Classification as a response to the growing needs of libraries, see Miksa, above, and also: LaMontagne, Leo E. \'\'American Library Classification: With Special Reference to the Library of Congress\'\'. Hamden, CT, Shoe String Press. 1961, p. 209.\n* The above issues are also discussed by Cutter in his [https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 \'\'Expansive Classification: Part I: The First Six Classifications\'\']. Boston, C. A. Cutter. 1891–93.\n</ref> Cutter completed and published an introduction and schedules for the first six classifications of his new system ([https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 \'\'Expansive Classification: Part I: The First Six Classifications\'\']), but his work on the seventh was interrupted by his death in 1903.<ref>LaMontagne, Leo E. \'\'American Library Classification: With Special Reference to the Library of Congress\'\'. Hamden, CT, Shoe String Press. 1961, p. 210.</ref>\n\nThe Cutter Expansive Classification, although adopted by comparatively few libraries, mostly in [[New England]]{{Citation needed|date=August 2011}}, has been called one of the most logical and scholarly of American classifications{{Citation needed|date=August 2011}}. Library historian Leo E. LaMontagne writes:\n\n<blockquote>Cutter produced the best classification of the nineteenth century. While his system was less "scientific" than that of [[J. P. Lesley]], its other key features – notation, specificity, and versatility – make it deserving of the praise it has received.<ref>LaMontagne, Leo E. \'\'American Library Classification: With Special Reference to the Library of Congress\'\'. Hamden, CT, Shoe String Press. 1961, p. 215</ref></blockquote>\n\nIts top level divisions served as a basis for the Library of Congress classification, which also took over some of its features.<ref>LaMontagne, Leo E. \'\'American Library Classification: With Special Reference to the Library of Congress\'\'. Hamden, Connecticut, Shoe String Press. 1961, p. 226.</ref> It did not catch on as did Dewey\'s system because Cutter died before it was completely finished, making no provision for the kind of development necessary as the bounds of knowledge expanded and scholarly emphases changed throughout the twentieth century.<ref>https://journals.ala.org/index.php/lrts/article/view/5419/6654</ref>\n\n==Structure of the Expansive Classification==\nThe Expansive Classification uses seven separate schedules, each designed to be used by libraries of different sizes. After the first, each schedule was an expansion of the previous one,<ref>Miksa, Francis L., ed. \'\'Charles Ammi Cutter: Library Systematizer". Littleton, CO, Libraries Unlimited. 1977, p. 58.</ref> and Cutter provided instructions for how a library might change from one expansion to another as it grows.<ref>Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 \'\'Expansive Classification: Part I: The First Six Classifications\'\']. Boston, C. A. Cutter. 1891–93, p. 21–23.</ref>\n\n==Summary of the Expansive Classification Schedules==\n\n===First Classification===\nThe first classification is meant for only the very smallest libraries. The first classification has only seven top level classes, and only eight classes in all.\n\n* \'\'\'A\'\'\' Works of reference and general works which include several of the following sections, and so could not go in any one.\n* \'\'\'B\'\'\' [[Outline of philosophy|Philosophy]] and [[Outline of religion|Religion]]\n* \'\'\'E\'\'\' Biography\n* \'\'\'F\'\'\' [[Outline of history|History]] and [[Outline of geography|Geography]] and Travels\n* \'\'\'H\'\'\' [[Outline of social science|Social sciences]]\n* \'\'\'L\'\'\' [[Outline of natural science|Natural sciences]] and [[The arts|Arts]]\n* \'\'\'Y\'\'\' [[Outline of linguistics|Language]] and [[Outline of literature|Literature]]\n* \'\'\'YF\'\'\' [[Outline of fiction|Fiction]]\n\n===Further Classifications===\nFurther expansions add more top level classes and subdivisions. Many subclasses arranged systematically, with common divisions, such as those by geography and language, following a consistent system throughout.<ref>https://archive.org/details/cu31924092476229</ref>\n\nBy the fifth classification all the letters of the alphabet are in use for top level classes. These are:\n\n* \'\'\'A\'\'\' General Works\n* \'\'\'B\'\'\' [[Outline of philosophy|Philosophy]]\n* \'\'\'C\'\'\' [[Outline of Christianity|Christianity]] and [[Outline of Judaism|Judaism]]\n* \'\'\'D\'\'\' Ecclesiastical History\n* \'\'\'E\'\'\' Biography\n* \'\'\'F\'\'\' [[Outline of history|History]], Universal History\n* \'\'\'G\'\'\' [[Outline of geography|Geography]] and Travels\n* \'\'\'H\'\'\' [[Outline of social science|Social Sciences]]\n* \'\'\'I\'\'\' Demotics, [[Outline of sociology|Sociology]]\n* \'\'\'J\'\'\' Civics, Government, [[Outline of political science|Political Science]]\n* \'\'\'K\'\'\' Legislation\n* \'\'\'L\'\'\' [[Outline of science|Science]] and [[The arts|Arts]] together\n* \'\'\'M\'\'\' Natural History\n* \'\'\'N\'\'\' [[Outline of botany|Botany]]\n* \'\'\'O\'\'\' [[Outline of zoology|Zoölogy]]\n* \'\'\'P\'\'\' [[Outline of anthropology|Anthropology]] and Ethnology\n* \'\'\'Q\'\'\' [[Outline of medicine|Medicine]]\n* \'\'\'R\'\'\' Useful arts, [[Outline of technology|Technology]]\n* \'\'\'S\'\'\' Constructive arts ([[Outline of engineering|Engineering]] and [[Outline of construction|Building]])\n* \'\'\'T\'\'\' [[Outline of manufacturing|Manufactures]] and Handicrafts\n* \'\'\'U\'\'\' [[Outline of military science and technology|Art of War]]\n* \'\'\'V\'\'\' Recreative arts, [[Outline of sports|Sports]], [[Outline of games|Games]], [[Outline of festivals|Festivals]]\n* \'\'\'W\'\'\' [[Outline of the visual arts|Art]]\n* \'\'\'X\'\'\' English Language\n* \'\'\'Y\'\'\' English and American literature\n* \'\'\'Z\'\'\' Book arts\n\nThese schedules were not meant to be fixed, but were to be adapted to meet the needs of each library. For example, books on the English language may be put in X, and books on language in general in a subclass of X, or this can be reversed. The first option is less logical, but results in shorter marks for most English language libraries.<ref>Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 \'\'Expansive Classification: Part I: The First Six Classifications\'\']. Boston, C. A. Cutter. 1891–93, p. 27.</ref>\n\n==How Expansive Classification call numbers are constructed==\n{{Expand section|citations and corrections|date=August 2011}}\nMost call numbers in the Expansive Classification follow conventions offering clues to the book\'s subject. The first line represents the subject, the second the author (and perhaps title), the third and fourth dates of editions, indications of translations, and critical works on particular books or authors. All numbers in the Expansive Classification are (or should be) shelved as if in decimal order.\n\nSize of volumes is indicated by points (.), pluses (+), or slashes (/ or //).\n\nFor some subjects a numerical geographical subdivision follows the classification letters on the first line. The number 83 stands for the United States&mdash;hence, F83 is U.S. history, G83 U.S. travel, JU83 U.S. politics, WP83 U.S. painting. Geographical numbers are often further expanded decimally to represent more specific areas, sometimes followed by a capital letter indicating a particular city.\n\nThe second line usually represents the author\'s name by a capital letter plus one or more numbers arranged decimally. This may be followed by the first letter or letters of the title in lower-case, and/or sometimes the letters a,b,c indicating other printings of the same title. When appropriate, the second line may begin with a \'form\' number&mdash;e.g., 1 stands for history and criticism of a subject, 2 for a bibliography, 5 for a dictionary, 6 for an atlas or maps, 7 for a periodical, 8 for a society or university publication, 9 for a collection of works by different authors.\n\nOn the third line a capital Y indicates a work about the author or book represented by the first two lines, and a capital E (for English&mdash;other letters are used for other languages) indicates a translation into English. If both criticism and translation apply to a single title, the number expands into four lines.\n\n=== Cutter Numbers (Cutter Codes) ===\n{{Expand section|examples and additional citations|date=August 2011}}\nOne of the features adopted by other systems, including Library of Congress, is the Cutter number. It is an alphanumeric device to code text so that it can be arranged in alphabetical order using the fewest characters. It contains one or two initial letters and Arabic numbers, treated as a decimal. To construct a Cutter number, a cataloguer consults a Cutter table as required by the classification rules.  Although Cutter numbers are mostly used for coding the names of authors, the system can be used for titles, subjects, geographic areas, and more.\n\n{| class=wikitable\n|+Cutter table\n|-\n! || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9\n|-\n| S || a || ch || e || h-i || m-p || t || u || w-z\n|-\n| Qu || || a || e || i || o || r || t || y\n|-\n| other consonants || || a || e || i || o || r || u || y\n|-\n| vowels || b || d || l-m || n || p || r || s-t || u-y\n|-\n| additional letters || || a-d || e-h || i-l || m-o || p-s || t-v || w-z\n|}\n\nInitial letters Qa-Qt are assigned Q2-Q29, while entries beginning with numerals have a Cutter number A12-A19, therefore sorting before the first A entry.<ref>{{cite web|title=LC Cutter Tables |url=http://staff.library.mun.ca/staff/toolbox/tables/lccutter.htm |website=Queen Elizabeth II Libraries |publisher=Memorial University of Newfoundland |accessdate=14 August 2014 |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20140814173419/http://staff.library.mun.ca/staff/toolbox/tables/lccutter.htm |archivedate=14 August 2014 |df= }}</ref>\n\nSo to make the three digit Cutter number for "Cutter", you would start with "C", then looking under \'\'other consonants\'\', find that "u" gives the number 8, and under \'\'additional letters\'\', "t" is 8, giving a Cutter number of "C88".\n\n==Notes==\n{{reflist}}\n\n==References==\n* Bliss, Henry Evelyn. \'\'The Organization of Knowledge in Libraries: and the Subject-Approach to Books\'\', 2nd ed. New York: H. W. Wilson, 1939.\n* Cutter, Charles A. \'\'Rules for a Dictionary Catalog\'\'. W. P. Cutter, ed. 4th ed. Washington, D.C.: Government Printing Office, 1904. London: The Library Association, 1962.\n* Cutter, William Parker. \'\'Charles Ammi Cutter\'\'. Chicago: American Library Association, 1931. Ann Arbor, MI: University Microfilms, 1969.\n* Foster, William E. "Charles Ammi Cutter: A Memorial Sketch". \'\'Library Journal\'\' 28 (1903): 697-704.\n* Hufford, Jon R. "The Pragmatic Basis of Catalog Codes: Has the User Been Ignored?". \'\'Cataloging and Classification Quarterly\'\' 14 (1991): 27-38.\n* Immroth, John Philip. "Cutter, Charles Ammi". \'\'Encyclopedia of Library and Information Science\'\'. [[Allen Kent]] and Harold Lancour, ed. 47 vols. New York, M. Dekker [1968- ]\n* LaMontagne, Leo E. \'\'American Library Classification: With Special Reference to the Library of Congress\'\'. Hamden, CT, Shoe String Press. 1961.\n*Slavis, Dobrica. "CUTT-x: An Expert System for Automatic Assignment of Cutter Numbers". \'\'Cataloging and Classification Quarterly\'\'. Vol 22, no. 2, 1996.\n* Tauber, Maurice F., and Edith Wise. "Classification Systems". [[Ralph R. Shaw (Librarian)|Ralph R. Shaw]], ed.. \'\'The State of the Library Art\'\'. New Brunswick, NJ: Rutgers U. Graduate School of Library Service, 1961. 1-528.\n\n==External links==\n* [http://catalog.bostonathenaeum.org/cutterguide.html The Boston Athenaeum\'s Guide to the classification system developed by Cutter for their collection]\n* [http://www.forbeslibrary.org/research/index.php?n=Main.CutterClassification Forbes Library\'s Outline of Cutter\'s Expansive Classification system]\n* [http://www.forbeslibrary.org/pathfinders/Shelvingrules.pdf A brief guide to the Expansive Classification from Forbes Library]\n* [http://digital.library.unt.edu/permalink/meta-dc-1048:1 \'\'Rules for a dictionary catalog, by Charles A. Cutter, fourth edition\'\'], hosted by the [http://digital.library.unt.edu/ UNT Libraries Digital Collections]\n* [http://www.loc.gov/aba/pcc/053/table.html Library of Congress Guidelines for using the LC Online Shelflist and formulating a literary author number: Cutter Table]\n* [http://www.oclc.org/dewey/support/program/default.htm Dewey Cutter Program]\n\n{{Library classification systems}}\n\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]']
['Faceted classification', '796657', '{{mergefrom|Faceted search|date=January 2015}}\n\'\'\'Faceted classification\'\'\' is a [[classification scheme]] used in organizing knowledge into a systematic order. A faceted classification uses semantic categories, either general or subject-specific, that are combined to create the full classification entry. Many library classification systems use a combination of a fixed, enumerative taxonomy of concepts with subordinate facets that further refine the topic.\n\n== Definition ==\n\nThere are two primary types of classification used for information organization: enumerative and faceted. An enumerative classification contains a full set of entries for all concepts.<ref name=lcsh>{{Citation\n |publisher = Libraries Unlimited\n |isbn = 1591581540\n |publication-place = Westport, Conn\n |title = Library of Congress subject headings\n |url = http://openlibrary.org/books/OL3311856M/Library_of_Congress_subject_headings\n |author = Lois Mai Chan\n |publication-date = 2005\n |id = 1591581540\n }}</ref> A faceted classification system uses a set of semantically cohesive categories that are combined as needed to create an expression of a concept. In this way, the faceted classification is not limited to already defined concepts. While this makes the classification quite flexible, it also makes the resulting expression of topics complex.<ref name=sven>{{Citation\n        |publisher = MIT Press\n        |isbn = 0262194333\n        |publication-place = Cambridge, Mass\n        |title = The intellectual foundation of information organization\n        |url = http://openlibrary.org/books/OL44967M/The_intellectual_foundation_of_information_organization\n        |author = Elaine Svenonius\n        |publication-date = 2000\n        |id = 0262194333\n        }}</ref> To the extent possible, facets represent "clearly defined, mutually exclusive, and collectively exhaustive aspects, properties or characteristics of a class or specific subject".<ref name=taylor>Taylor, A. G. (1992). Introduction to Cataloging and Classification. 8th ed. Englewood, Colorado: Libraries Unlimited.</ref> Some commonly used general-purpose facets are time, place, and form.<ref name=chan />\n\nThere are few purely faceted classifications; the best known of these is the [[Colon Classification]] of [[S. R. Ranganathan]], a general knowledge classification for libraries. Some other faceted classifications are specific to special topics, such as the Art and Architecture Thesaurus and the faceted classification of occupational safety and health topics created by D. J. Foskett for the International Labour Organization.<ref name=coyle />\n\nMany library classifications combine the enumerative and faceted classification techniques. The [[Dewey Decimal Classification]], the [[Library of Congress Classification]], and the [[Universal Decimal Classification]] all make use of facets at various points in their enumerated classification schedules. The allowed facets vary based on the subject area of the classification. These facets are recorded as tables that represent recurring types of subdivisions within subject areas. There are general facets that can be used wherever appropriate, such as geographic subdivisions of the topic. Other tables are applied only to specific areas of the schedules. Facets can be combined to create a complex subject statement.<ref name=chan />\n\nArlene Taylor describes faceted classification using an analogy: “If one thinks of each of the faces of a cut and polished diamond as a facet for the whole diamond, one can picture a classification notation that has small notations  standing for subparts of the whole topic strung together to create a complete classification notation”.<ref>{{cite book|last1=Taylor|first1=Arlene G.|year=2004|title=The organization of information|location=Westport, CT|publisher=Libraries Unlimited}}</ref>\n\nFaceted classifications exhibit many of the same problems as classifications based on a hierarchy. In particular, some concepts could belong in more than one facet, so their placement in the classification may appear to be arbitrary to the classifier. It also tends to result in a complex notation because each facet must be distinguishable as recorded.<ref name=sven />\n\n== Retrieval ==\n\nSearch in systems with faceted classification can enable a user to navigate information along multiple paths corresponding to different orderings of the facets. This contrasts with traditional taxonomies in which the hierarchy of categories is fixed and unchanging.<ref name="Star, S.L. 1998">Star, S.L. (1998, Fall). Grounded classification: grounded theory and faceted classification. [Electronic version]. Library Trends. 47.2, 218.</ref>  It is also possible to use facets to filter search results to more quickly find desired results.\n\n==Examples of Faceted Classifications==\n\n=== Colon classification for library materials ===\nThe [[colon classification]] developed by [[S. R. Ranganathan]] is an example of general faceted classification designed to be applied to all library materials. In the Colon Classification system, a book is assigned a set of values from each independent facet.<ref>Garfield, E. (1984, February). A tribute to S.R. Ranganathan, the father of Indian library science. Essays of an Information Scientist, 7, 37-44.</ref>  This facet formula uses punctuation marks and symbols placed between the facets to connect them. Colon classification was named after its use of the colon as the primary symbol in its notation.<ref>Chan, L.M. (1994). Cataloging and classification.  New York: McGraw-Hill, Inc.</ref><ref>[http://www.essessreference.com/servlet/esGetBiblio?bno=000374 Colon Classification (6th Edition)] by Dr. S.R. Ranganathan, published by Ess Publications, Delhi, India.</ref>\n\nRanganathan stated that hierarchical classification schemes like the Dewey Decimal Classification (DDC) or the Library of Congress Subject Headings are too limiting and finite to use for modern classification and that many items can pertain information to more than one subject.  He organized his classification scheme into 42 classes.  Each class can be categorized according to particular characteristics, that he called facets.  Ranganathan said that there are five fundamental categories that can be used to demonstrate the facets of a subject: personality, material, energy, space and time.  He called this the PMEST formula:<ref>Ranganathan, S. R (1987). Colon classification, 7th ed. revised and edited by M.A. Gopinath. Bangalore: Sarada Ranganathan Endowment for Library Science, 1987</ref>  \n*Personality is the most specific or focal subject.\n*Matter is the substance, properties or materials of the subject.\n*Energy includes the processes, operations and activities.\n*Space relates to the geographic location of the subject.\n*Time refers to the dates or seasons of the subject.\n\n=== Universal Decimal Classification ===\nAnother example of a faceted classification scheme is the [[Universal Decimal Classification]] (UDC), the UDC is considered to be a complex multilingual classification that can be used in all fields of knowledge.<ref>About universal decimal classification and the udc consortium. (2006). Retrieved November 30, 2013, from http://www.udcc.org/about.htm</ref>\nThe Universal Decimal Classification scheme was created at the end of the nineteenth century by Belgian bibliographers [[Paul Otlet]] and [[Henri la Fontaine]]. The goal of their system was to create an index that would be able to record knowledge even if it is stored in non-conventional ways including materials in notebooks and ephemera. They also wanted their index to organize material systematically instead of alphabetically.<ref>Batty, D. (2003). Universal decimal classification.  Encyclopedia of Library and Information Science.</ref>\n\nThe UDC has an overall taxonomy of knowledge that is extended with a number of facets, such as language, form, place and time. Each facet has its own symbol in the notation, such as: "=" for language; "-02" for materials, "[...]" for subordinate concepts.<ref name=chan>{{Cite book     |publisher = The Scarecrow Press, Inc.     |isbn = 978-0-8108-5944-9     |title = Cataloging and classification     |url = http://openlibrary.org/books/OL9558667M/Cataloging_and_Classification     |last = Chan|first=Lois Mai|edition = Third     |publication-date = 2007 |page=321|id = 0810859440}}</ref>\n\n===Faceted Classification for Occupational Safety and Health===\n\n[[Douglas John Foskett|D. J. Foskett]], a member of the [[Classification Research Group]] in London, developed classification of occupational safety and health materials for the library of the [[International Labour Organization]].<ref name=coyle>{{cite journal|last1=Coyle|first1=Karen|title=A Faceted Classification for Occupational Safety and Health|journal=Special Libraries|date=1975|volume=66|issue=5-6|pages=256–9}}</ref><ref name=foskett>{{cite book|last1=Foskett|first1=D. J.|title=Proceedings of the International Conference on Scientific Information|chapter=Construction of a Faceted Classification for a Special Subject|date=1959|publisher=National Science Foundation|isbn=0-309-57421-8|pages=867–888}}</ref> After a study of the literature in the field, he created the classification with the following facets:\n\n*Facet A: Occupational Safety and Health: General\n*Facet B: Special Classes of Workers, Industries\n*Facet C: Sources of Hazards: Fire, Machinery, etc.\n*Facet D: Industrial Accidents and Diseases\n*Facet E: Preventive Measures, Protection\n*Facet F: Organisation, Administration\n\nNotation was solely alphabetic, with the sub-facets organized hierarchically using extended codes, such as "g Industrial equipment and processes", "ge Machines".<ref name=foskett />\n\n===Art and Architecture Thesaurus (AAT)===\n\nWhile not strictly a classification system, the [[Art and Architecture Thesaurus|AAT]] uses facets similar to those of Ranganathan\'s Colon Classification:\n\n*Associated Concepts (e.g., philosophy)\n*Physical Attributes\n*Styles and Periods\n*Agents (People/Organizations)\n*Activities (similar to Ranganathan\'s Energy)\n*Materials (similar to Ranganathan\'s Matter)\n*Objects (similar to Ranganathan\'s Personality)<ref name=denton>{{cite web |url=https://www.miskatonic.org/library/facet-web-howto.html|author=William Denton|title=How to Make a Faceted Classification and Put it on the Web}}</ref>\n\n==Comparison between faceted and single hierarchical classification==\nHierarchical classification refers to the classification of objects using one \'\'single\'\' hierarchical taxonomy. Faceted classification may actually employ hierarchy in one or more of its facets, but allows for the use of more than one taxonomy to classify objects.\n\n*Faceted classification systems allow the assignment of multiple classifications to an object, and enable those classifications to be applied by searchers in multiple ways, rather than in a single, predetermined order. Multiple facets may be used as a first step in a search process.<ref name="Categories, Facets—and Browsable Facets?">Sirovich, Jaimie (2011). Categories, Facets—and Browsable Facets?, from http://www.uxmatters.com/mt/archives/2011/08/categories-facetsand-browsable-facets.php</ref> For example, one may \'\'start\'\' from language or subject.\n*Hierarchical classification systems are developed classes that are subdivided from the most general subjects to the most specific.<ref>Reitz, Joan M. (2004). Dictionary for library and information science. Westport, CT: Libraries Unlimited</ref>\n*Faceted classification systems allow for the combination of facets to [[Filter (software)|filter]] the set of objects rapidly. In addition, the facets can be used to address multiple classification criteria.<ref>Godert, Winfried. F. (1991). Facet classification in online retrieval. International Classification, 18, 98-109</ref>\n*A faceted system focuses on the important, essential or persistent characteristics of content objects, helping it to be useful for categorization of fine-grained rapidly changing repositories.\n*In faceted classification systems one does not have to know the name of the category into which an object is placed a priori. A controlled vocabulary is presented with the number of documents matching each vocabulary term.\n*New facets may be created at any time without disruption of a single hierarchy or reorganizing other facets.\n*Faceted classification systems make few assumptions about the scope and organization of the domain. It is difficult to \'\'break\'\' a faceted classification schema.<ref>Adkisson, Hiedi P. (2005).  Use of faceted classification.  Retrieved December 1, 2013, from http://www.webdesignpractices.com/navigation/facets.html</ref>\n\n==See also==\n* [[Classification Research Group]]\n* [[Controlled vocabulary]]\n* [[Findability]]\n* [[Folksonomy]]\n* [[Information architecture]]\n* [[Tag (metadata)]]\n* [[Universal Decimal Classification]]\n\n==References==\n{{Reflist|colwidth=35em}}\n\n==External links==\n* [http://eprints.soton.ac.uk/271488/ How to \'\'Reuse\'\' a Faceted Classification and Put It On the \'\'Semantic\'\' Web]\n\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]']
['Resource Description Framework', '53847', '{{Infobox technology standard\n| title             = RDF 1.1 Concepts and Abstract Syntax\n| status            = Published, W3C Recommendation\n| year_started      = 1997\n| editors           = Richard Cyganiak, David Wood, Markus Lanthaler\n| base_standards    = [[URI]]\n| related_standards = [[RDFS]], [[Web Ontology Language|OWL]], [[Rule Interchange Format|RIF]], [[RDFa]]\n| domain            = [[Semantic Web]]\n| abbreviation      = RDF\n| website           = {{url|http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/}}\n}}\n\nThe \'\'\'Resource Description Framework\'\'\' (\'\'\'RDF\'\'\') is a family of [[World Wide Web Consortium]] (W3C) [[specification]]s<ref>{{cite web|url=http://www.dblab.ntua.gr/~bikakis/XMLSemanticWebW3CTimeline.pdf |title=XML and Semantic Web W3C Standards Timeline\n|date=2012-02-04}}</ref> originally designed as a [[metadata]] [[data model]]. It has come to be used as a general method for conceptual description or modeling of information that is implemented in [[web resource]]s, using a variety of syntax notations and [[data serialization]] formats. It is also used in [[knowledge management]] applications.\n\nRDF was adopted as a W3C recommendation in 1999. The RDF 1.0 specification was published in 2004, the RDF 1.1 specification in 2014.\n\n== Overview ==\nThe RDF data model<ref>http://www.w3.org/TR/PR-rdf-syntax/ "Resource Description Framework\n(RDF) Model and Syntax Specification"</ref>  is similar to classical conceptual modeling approaches (such as [[entity–relationship model|entity–relationship]] or [[class diagram]]s). It is based upon the idea of making [[statement (programming)|statement]]s about [[resource (computer science)|resource]]s (in particular [[web resource]]s) expressions, known as \'\'[[Semantic triple|triples]]\'\'.  Triples are so named because they follow a <var>subject</var>–<var>predicate</var>–<var>object</var> structure. The <var>subject</var> denotes the resource, and the <var>predicate</var> denotes traits or aspects of the resource, and expresses a relationship between the <var>subject</var> and the <var>object</var>. \n\nFor example, one way to represent the notion "The sky has the color blue" in RDF is as the triple: a [[Subject (grammar)|subject]] denoting "the sky", a [[Predicate (grammar)|predicate]] denoting "has the color", and an [[Object (grammar)|object]] denoting "blue". Therefore, RDF swaps <var>object</var> for <var>subject</var> in contrast to the typical approach of an [[entity–attribute–value model]] in [[object-oriented design]]: entity (sky), attribute (color), and value (blue).  \n\nRDF is an abstract model with several [[Serialization|serialization formats]] (i.e. file formats), so the particular encoding for resources or triples varies from format to format.\n\nThis mechanism for describing resources is a major [[software componentry|component]] in the W3C\'s [[Semantic Web]] activity: an evolutionary stage of the [[World Wide Web]] in which automated software can store, exchange, and use machine-readable information distributed throughout the Web, in turn enabling users to deal with the information with greater efficiency and [[certainty]]. RDF\'s simple data model and ability to model disparate, abstract concepts has also led to its increasing use in [[knowledge management]] applications unrelated to Semantic Web activity.\n\nA collection of RDF statements intrinsically represents a [[Glossary of graph theory|labeled, directed multi-graph]]. This theoretically makes an RDF [[data model]] better suited to certain kinds of [[knowledge representation]] than other [[relational model|relational]] or [[Ontology (computer science)|ontological]] models. However, in practice, RDF data is often persisted in [[RDBMS|relational database]] or native representations (also called [[Triplestore]]s—or Quad stores, if context (i.e. the [[named graph]]) is also persisted for each RDF triple).<ref>[http://sw.deri.org/2005/02/dexa/yars.pdf Optimized Index Structures for Querying RDF from the Web] Andreas Harth, Stefan Decker, 3rd Latin American Web Congress, Buenos Aires, Argentina, October 31 to November 2, 2005, pp. 71–80</ref> \n\nShEX, or Shape Expressions,<ref>[http://www.w3.org/2001/sw/wiki/ShEx]  Shape Expressions language</ref> is a language for expressing constraints on RDF graphs. It includes the cardinality constraints from [[Open Services for Lifecycle Collaboration|OSLC]] Resource Shapes and [[Dublin Core]] Description Set Profiles, as well as logical connectives for disjunction and polymorphism. \n\nAs [[RDFS]] and [[Web Ontology Language|OWL]] demonstrate, one can build additional [[ontology language]]s upon RDF.\n\n== History ==\nThe initial RDF design, intended to "build a vendor-neutral and operating system-independent system of metadata,"<ref name="press-release-1997">{{Cite news| last = | first = | title = World Wide Web Consortium Publishes Public Draft of Resource Description Framework| work = W3C| location = Cambridge, MA| date = 1997-10-03| url = http://www.w3.org/Press/RDF}}</ref>  derived from the W3C\'s [[Platform for Internet Content Selection]] (PICS), an early web content labelling system,<ref name="lash" /> but the project was also  shaped by ideas from [[Dublin Core]], and from the [[Meta Content Framework]] (MCF),<ref name="press-release-1997" /> which had been developed during 1995–1997 by [[Ramanathan V. Guha]] at [[Apple Computer|Apple]] and [[Tim Bray]] at [[Netscape Communications Corporation|Netscape]].<ref>{{Cite book| publisher = O’Reilly| isbn = 0-596-00881-3| last = Hammersley| first = Ben| title = Developing Feeds with RSS and Atom| pages=2–3|location = Sebastopol| date = 2005}}</ref>\n\nA first public draft of RDF appeared in October 1997,<ref>{{Cite web| last1 = Lassila| first1 = Ora| last2 = Swick| first2 = Ralph R.| title = Resource Description Framework (RDF): Model and Syntax| work = W3C| accessdate = 2015-11-24| date = 1997-10-02| url = http://www.w3.org/TR/WD-rdf-syntax-971002/}}</ref><ref>{{Cite web|last=Swick |first=Ralph |title=Resource Description Framework (RDF) |work=W3C |accessdate=2015-11-24 |date=1997-12-11 |url=http://www13.w3.org/RDF/Overview.html |deadurl=yes |archiveurl=https://web.archive.org/web/19980214043631/http://www13.w3.org/RDF/Overview.html |archivedate=February 14, 1998 }}</ref> issued by a W3C working group that included representatives from [[IBM]], [[Microsoft]], [[Netscape]], [[Nokia]], [[Reuters]], [[SoftQuad Software|SoftQuad]], and the [[University of Michigan]].<ref name="lash">{{Cite news|last=Lash |first=Alex |title=W3C takes first step toward RDF spec |work=CNET News |accessdate=2015-11-28 |date=1997-10-03 |url=http://news.cnet.com/2100-1001-203893.html |deadurl=yes |archiveurl=https://web.archive.org/web/20110616023126/http://news.cnet.com/2100-1001-203893.html |archivedate=June 16, 2011 }}</ref>\n\nThe W3C published a specification of RDF\'s data model and an [[XML]] serialization as a recommendation in February 1999.<ref>{{cite web|url=http://www.w3.org/TR/1999/REC-rdf-syntax-19990222|title=Resource Description Framework (RDF) Model and Syntax Specification| date=22 Feb 1999|accessdate=5 May 2014}}</ref>\n\nTwo persistent misunderstandings developed around RDF at this time: firstly, from the MCF influence and the RDF "Resource Description" acronym, the idea that RDF was specifically for use in representing metadata. Secondly that RDF was an XML format, rather than RDF being a data model and only the RDF/XML serialisation being XML-based. RDF saw little take-up in this period, but there was significant work carried out in [[Bristol]], around ILRT at [[Bristol University]] and [[HP Labs]], and also in Boston at [[MIT]]. [[RSS 1.0]] and [[FOAF (ontology)|FOAF]] became exemplar applications for RDF in this period.\n\nThe recommendation of 1999 was replaced in 2004 by a set of six specifications: "The RDF Primer",<ref>{{Citation| publisher = W3C| last1 = Manola| first1 = Frank| last2 = Miller| first2 = Eric| title = RDF Primer| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-primer-20040210/}}</ref> "RDF Concepts and Abstract",<ref>{{Citation| publisher = W3C| last1 = Klyne| first1 = Graham| last2 = Carroll| first2 = Jeremy J.| title = Resource Description Framework (RDF): Concepts and Abstract Syntax| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/}}</ref> "RDF/XML Syntax Specification (revised)",<ref>{{Citation| publisher = W3C| last = Beckett| first = Dave| title = RDF/XML Syntax Specification (Revised)| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-syntax-grammar-20040210/}}</ref> "RDF Semantics",<ref>{{Citation| last = Hayes| first = Patrick| title = RDF Semantics| accessdate = 2015-11-21| date = 2014-02-10| url = http://www.w3.org/TR/2004/REC-rdf-mt-20040210/}}</ref> "RDF Vocabulary Description Language 1.0",<ref>{{Citation| publisher = W3C| last1 = Brickley| first1 = Dan| last2 = Guha| first2 = R.V.| title = RDF Vocabulary Description Language 1.0: RDF Schema: W3C Recommendation 10 February 2004| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-schema-20040210/}}</ref> and "The RDF Test Cases".<ref>{{Citation| publisher = W3C| last1 = Grant| first1 = Jan| last2 = Beckett| first2 = Dave| title = RDF Test Cases| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-testcases-20040210/}}</ref>\n\nThis series was superseded in 2014 by the following six "RDF 1.1" documents: "RDF 1.1 Primer,"<ref>{{Citation| publisher = W3C| last1 = Schreiber| first1 = Guus| last2 = Raimond| first2 = Yves| title = RDF 1.1 Primer| accessdate = 2015-11-22| date = 2014-06-24| url = http://www.w3.org/TR/2014/NOTE-rdf11-primer-20140624/}}</ref> "RDF 1.1 Concepts and Abstract Syntax,"<ref>{{Citation| publisher = W3C| last1 = Cyganiak| first1 = Richard| last2 = Wood| first2 = David| last3 = Lanthaler| first3 = Markus| title = RDF 1.1 Concepts and Abstract Syntax| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/}}</ref> "RDF 1.1 XML Syntax,"<ref>{{Citation| publisher = W3C| last1 = Gandon| first1 = Fabien| last2 = Schreiber| first2 = Guus| title = RDF 1.1 XML Syntax| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/rdf-syntax-grammar/}}</ref>\n"RDF 1.1 Semantics,"<ref>{{Citation| publisher = W3C| last1 = Hayes| first1 = Patrick J.| last2 = Patel-Schneider| first2 = Peter F.| title = RDF 1.1 Semantics| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/REC-rdf11-mt-20140225/}}</ref> "RDF Schema 1.1,"<ref>{{Citation| publisher = W3C| last1 = Brickley| first1 = Dan| last2 = Guha| first2 = R.V.| title = RDF Schema 1.1| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/rdf-schema/}}</ref> and "RDF 1.1 Test Cases".<ref>{{Citation| publisher = W3C| last1 = Kellogg| first1 = Gregg| last2 = Lanthaler| first2 = Markus| title = RDF 1.1 Test Cases| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/NOTE-rdf11-testcases-20140225/}}</ref>\n\n==RDF topics==\n\n===RDF vocabulary===\nThe vocabulary defined by the RDF specification is as follows:<ref name="rdfschema">{{cite web|url=http://www.w3.org/TR/rdf-schema/|title=RDF Vocabulary Description Language 1.0: RDF Schema|publisher=[[W3C]]|date=2004-02-10|accessdate=2011-01-05}}</ref>\n\n====Classes====\n\n===== rdf =====\n* \'\'\'<code>rdf:XMLLiteral</code>\'\'\' – the class of XML literal values\n* \'\'\'<code>rdf:Property</code>\'\'\' – the class of properties\n* \'\'\'<code>rdf:Statement</code>\'\'\' – the class of RDF statements\n* \'\'\'<code>rdf:Alt</code>\'\'\', \'\'\'<code>rdf:Bag</code>\'\'\', \'\'\'<code>rdf:Seq</code>\'\'\' – containers of alternatives, unordered containers, and ordered containers (<code>rdfs:Container</code> is a super-class of the three)\n* \'\'\'<code>rdf:List</code>\'\'\' – the class of RDF Lists\n* \'\'\'<code>rdf:nil</code>\'\'\' – an instance of <code>rdf:List</code> representing the empty list\n\n===== rdfs =====\n* \'\'\'<code>rdfs:Resource</code>\'\'\' – the class resource, everything\n* \'\'\'<code>rdfs:Literal</code>\'\'\' – the class of literal values, e.g. [[string literal|string]]s and [[integer]]s\n* \'\'\'<code>rdfs:Class</code>\'\'\' – the class of classes\n* \'\'\'<code>rdfs:Datatype</code>\'\'\' – the class of RDF datatypes\n* \'\'\'<code>rdfs:Container</code>\'\'\' – the class of RDF containers\n* \'\'\'<code>rdfs:ContainerMembershipProperty</code>\'\'\' – the class of container membership properties, <code>rdf:_1</code>, <code>rdf:_2</code>, ..., all of which are sub-properties of <code>rdfs:member</code>\n\n====Properties====\n\n=====rdf=====\n* \'\'\'<code>rdf:type</code>\'\'\' – an instance of <code>rdf:Property</code> used to state that a resource is an instance of a class\n* \'\'\'<code>rdf:first</code>\'\'\' – the first item in the subject RDF list\n* \'\'\'<code>rdf:rest</code>\'\'\' – the rest of the subject RDF list after <code>rdf:first</code>\n* \'\'\'<code>rdf:value</code>\'\'\' – idiomatic property used for structured values\n* \'\'\'<code>rdf:subject</code>\'\'\' – the subject of the subject RDF statement\n* \'\'\'<code>rdf:predicate</code>\'\'\' – the predicate of the subject RDF statement\n* \'\'\'<code>rdf:object</code>\'\'\' – the object of the subject RDF statement\n\n<code>rdf:Statement</code>, <code>rdf:subject</code>, <code>rdf:predicate</code>, <code>rdf:object</code> are used for [[reification (knowledge representation)|reification]] (see [[#Statement reification and context|below]]).\n\n=====rdfs=====\n* \'\'\'<code>rdfs:subClassOf</code>\'\'\' – the subject is a subclass of a class\n* \'\'\'<code>rdfs:subPropertyOf</code>\'\'\' – the subject is a subproperty of a property\n* \'\'\'<code>rdfs:domain</code>\'\'\' – a domain of the subject property\n* \'\'\'<code>rdfs:range</code>\'\'\' – a range of the subject property\n* \'\'\'<code>rdfs:label</code>\'\'\' – a human-readable name for the subject\n* \'\'\'<code>rdfs:comment</code>\'\'\' – a description of the subject resource\n* \'\'\'<code>rdfs:member</code>\'\'\' – a member of the subject resource\n* \'\'\'<code>rdfs:seeAlso</code>\'\'\' – further information about the subject resource\n* \'\'\'<code>rdfs:isDefinedBy</code>\'\'\' – the definition of the subject resource\n\nThis vocabulary is used as a foundation for [[RDF Schema]] where it is extended.\n\n=== Serialization formats ===\n{{Infobox file format\n| name = RDF 1.1 Turtle serialization\n| icon = \n| extension = .ttl\n| mime = text/turtle<ref>{{cite web |url=http://www.w3.org/TR/turtle/#h2_sec-mediaReg |title=RDF 1.1 Turtle: Terse RDF Triple Language |publisher=W3C |date=9 Jan 2014 |accessdate=2014-02-22}}</ref>\n| owner = [[World Wide Web Consortium]]\n| standard = [http://www.w3.org/TR/turtle/ RDF 1.1 Turtle: Terse RDF Triple Language] {{release date and age|2014|01|09}}\n| free = Yes\n}}\n\n{{Infobox file format\n| name = RDF/XML serialization\n| icon = [[Image:XML.svg|100px]]\n| extension = .rdf\n| mime = application/rdf+xml<ref>{{cite web |url=http://tools.ietf.org/html/rfc3870 |title=application/rdf+xml Media Type Registration |page=2 |publisher=IETF |date=September 2004 |accessdate=2011-01-08}}</ref>\n| owner = [[World Wide Web Consortium]]\n| standard = [http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/ Concepts and Abstract Syntax] {{release date and age|2004|02|10}}\n| free = Yes\n}}\n\nSeveral common [[Serialization|serialization formats]] are in use, including:\n* \'\'\'[[Turtle (syntax)|Turtle]],\'\'\'<ref name="turtle">{{cite web\n  |title=RDF 1.1 Turtle: Terse RDF Triple Language\n  |url=http://www.w3.org/TR/turtle/\n  |date=9 January 2014\n  |publisher=W3C\n}}</ref> a compact, human-friendly format.\n* \'\'\'[[N-Triples]],\'\'\'<ref name="n-triples" >{{cite web\n  |title=RDF 1.1 N-Triples: A line-based syntax for an RDF graph\n  |date=9 January 2014\n  |url=http://www.w3.org/TR/n-triples/\n  |publisher=[[W3C]]\n}}</ref> a very simple, easy-to-parse, line-based format that is not as compact as Turtle.\n* \'\'\'[[N-Quads]],\'\'\'<ref>{{cite web\n  |title=N-Quads: Extending N-Triples with Context\n  |date=2012-06-25 \n  |url=http://sw.deri.org/2008/07/n-quads/\n}}</ref><ref name="n-quads" >{{cite web\n  |title=RDF 1.1 N-Quads\n  |date=January 2014\n  |url=http://www.w3.org/TR/n-quads/\n  |publisher=[[W3C]]\n}}</ref> a superset of N-Triples, for serializing multiple RDF graphs.\n* \'\'\'[[JSON-LD]],\'\'\'<ref name="json-ld">{{cite web|title=\nJSON-LD 1.0: A JSON-based Serialization for Linked Data|url=http://www.w3.org/TR/json-ld/|publisher=W3C}}</ref> a [[JSON]]-based serialization.\n* \'\'\'N3\'\'\' or [[Notation3]], a non-standard serialization that is very similar to Turtle, but has some additional features, such as the ability to define inference rules.\n* \'\'\'[[RDF/XML]]\'\'\',<ref name="rdf-xml" >{{cite web\n  |title=RDF 1.1 XML Syntax\n  |date=25 February 2014\n  |url=http://www.w3.org/TR/rdf-syntax-grammar/\n  |publisher=[[W3C]]\n}}</ref> an XML-based syntax that was the first standard format for serializing RDF.\n\nRDF/XML is sometimes misleadingly called simply RDF because it was introduced among the other W3C specifications defining RDF and it was historically the first W3C standard RDF serialization format. However, it is important to distinguish the RDF/XML format from the abstract RDF model itself. Although the RDF/XML format is still in use, other RDF serializations are now preferred by many RDF users, both because they are more human-friendly,<ref name="rdf-xml-syntax-criticism">{{cite web|title=\nProblems of the RDF syntax|url=http://milicicvuk.com/blog/2011/07/21/problems-of-the-rdf-syntax/|publisher=Vuk Miličić}}</ref>  and because some RDF graphs are not representable in RDF/XML due to restrictions on the syntax of XML [[QName]]s.\n\nWith a little effort, virtually any arbitrary [[XML]] may also be interpreted as RDF using [[GRDDL]] (pronounced \'griddle\'), Gleaning Resource Descriptions from Dialects of Languages.\n\nRDF triples may be stored in a type of database called a [[triplestore]].\n\n=== Resource identification ===\nThe subject of an RDF statement is either a [[uniform resource identifier]] (URI) or a [[blank node]], both of which denote  [[web resource|resource]]s. Resources indicated by [[blank node]]s are called anonymous resources. They are not directly identifiable from the RDF statement. The predicate is a URI which also indicates a resource, representing a relationship. The object is a URI, blank node or a [[Unicode]] [[string literal]]. \nAs of RDF 1.1 resources are identified by IRI\'s. IRI is a generalization of URI.<ref>RDF 1.1 Concepts and Abstract Syntax https://www.w3.org/TR/rdf11-concepts/</ref>\n\nIn Semantic Web applications, and in relatively popular applications of RDF like [[RSS (file format)|RSS]] and [[FOAF (software)|FOAF]] (Friend of a Friend), resources tend to be represented by URIs that intentionally denote, and can be used to access, actual data on the World Wide Web. But RDF, in general, is not limited to the description of Internet-based resources. In fact, the URI that names a resource does not have to be dereferenceable at all. For example, a URI that begins with "http:" and is used as the subject of an RDF statement does not necessarily have to represent a resource that is accessible via [[HTTP]], nor does it need to represent a tangible, network-accessible resource — such a URI could represent absolutely anything. However, there is broad agreement that a bare URI (without a # symbol) which returns a 300-level coded response when used in an HTTP GET request should be treated as denoting the internet resource that it succeeds in accessing.\n\nTherefore, producers and consumers of RDF statements must agree on the semantics of resource identifiers. Such agreement is not inherent to RDF itself, although there are some controlled vocabularies in common use, such as [[Dublin Core]] Metadata, which is partially mapped to a URI space for use in RDF. The intent of publishing RDF-based ontologies on the Web is often to establish, or circumscribe, the intended meanings of the resource identifiers used to express data in RDF. For example, the URI:\n<blockquote>\n<code>\n<nowiki>\nhttp://www.w3.org/TR/2004/REC-owl-guide-20040210/wine#Merlot\n</nowiki>\n</code>\n</blockquote>\nis intended by its owners to refer to the class of all [[Merlot]] red wines by vintner (i.e., instances of the above URI each represent the class of all wine produced by a single vintner), a definition which is expressed by the OWL ontology — itself an RDF document — in which it occurs.  Without careful analysis of the definition, one might erroneously conclude that an instance of the above URI was something physical, instead of a type of wine.\n\nNote that this is not a \'bare\' resource identifier, but is rather a [[Uniform Resource Identifier#URI reference|URI reference]], containing the \'#\' character and ending with a [[fragment identifier]].\n\n=== Statement reification and context ===\nThe body of knowledge modeled by a collection of statements may be subjected to [[Reification (knowledge representation)|reification]], in which each \'\'statement\'\' (that is each triple \'\'subject-predicate-object\'\' altogether) is assigned a URI and treated as a resource about which additional statements can be made, as in "\'\'Jane says that\'\' John is the author of document X". Reification is sometimes important in order to deduce a level of confidence or degree of usefulness for each statement.\n\nIn a reified RDF database, each original statement, being a resource, itself, most likely has at least three additional statements made about it: one to assert that its subject is some resource, one to assert that its predicate is some resource, and one to assert that its object is some resource or literal. More statements about the original statement may also exist, depending on the application\'s needs.\n\nBorrowing from concepts available in [[logic]] (and as illustrated in graphical notations such as [[conceptual graphs]] and [[topic map]]s), some RDF model implementations acknowledge that it is sometimes useful to group statements according to different criteria, called \'\'situations\'\', \'\'contexts\'\', or \'\'scopes\'\', as discussed in articles by RDF specification co-editor [[Graham Klyne]].<ref>[http://www.ninebynine.org/RDFNotes/RDFContexts.html Contexts for RDF Information Modelling]</ref><ref>[http://www.ninebynine.org/RDFNotes/UsingContextsWithRDF.html Circumstance, Provenance and Partial Knowledge]</ref> For example, a statement can be associated with a context, named by a URI, in order to assert an "is true in" relationship. As another example, it is sometimes convenient to group statements by their source, which can be identified by a URI, such as the URI of a particular RDF/XML document. Then, when updates are made to the source, corresponding statements can be changed in the model, as well.\n\nImplementation of scopes does not necessarily require fully reified statements. Some implementations allow a single scope identifier to be associated with a statement that has not been assigned a URI, itself.<ref>[http://uche.ogbuji.net/tech/akara/nodes/2003-01-01/scopes The Concept of 4Suite RDF Scopes]</ref><ref>[http://librdf.org/notes/contexts.html Redland RDF Library – Contexts]</ref>  Likewise \'\'named graphs\'\' in which a set of triples is named by a URI can represent context without the need to reify the triples.<ref>[http://www.w3.org/2004/03/trix/ Named Graphs]</ref>\n\n=== Query and inference languages ===\n{{main|RDF query language}}\nThe predominant query language for RDF graphs is [[SPARQL]]. SPARQL is an [[SQL]]-like language, and a [[W3C recommendation|recommendation]] of the [[W3C]] as of January 15, 2008.\n\nAn example of a SPARQL query to show country capitals in Africa, using a fictional ontology.\n<source lang="sparql">\nPREFIX ex: <http://example.com/exampleOntology#>\nSELECT ?capital ?country\nWHERE {\n  ?x ex:cityname ?capital ;\n     ex:isCapitalOf ?y .\n  ?y ex:countryname ?country ;\n     ex:isInContinent ex:Africa .\n}\n</source>\n\nOther non-standard ways to query RDF graphs include:\n* [[RDQL]], precursor to [[SPARQL]], SQL-like\n* Versa, compact syntax (non–SQL-like), solely implemented in [[4Suite]] ([[Python (programming language)|Python]])\n* RQL, one of the first declarative languages for uniformly querying RDF schemas and resource descriptions, implemented in RDFSuite.<ref name=RQL>{{cite web|title=The RDF Query Language (RQL)|url=http://139.91.183.30:9090/RDF/RQL/index.html|work=The ICS-FORTH RDFSuite|publisher=ICS-FORTH}}</ref>\n* [[SeRQL]], part of [[Sesame (framework)|Sesame]]\n* [[XUL]] has a template element in which to declare rules for matching data in RDF. XUL uses RDF extensively for databinding.\n\n== Examples ==\n\n=== Example 1: RDF Description of a person named Eric Miller<ref name="rdf-primer">{{cite web|url= http://www.w3.org/TR/rdf-primer/|title= RDF Primer |publisher=[[W3C]]|accessdate=2009-03-13}}</ref> ===\n\nThe following  example is taken from the W3C website<ref name="rdf-primer" /> describing a resource with statements "there is a Person identified by <nowiki>http://www.w3.org/People/EM/contact#me</nowiki>, whose name is Eric Miller, whose email address is e.miller123(at)example (changed for security purposes), and whose title is Dr. [[Image:Rdf graph for Eric Miller.png|thumb|An RDF Graph Describing Eric Miller<ref name="rdf-primer" />]]\n\nThe resource "<nowiki>http://www.w3.org/People/EM/contact#me</nowiki>" is the subject.\n\nThe objects are:\n* "Eric Miller" (with a predicate "whose name is"),\n* <nowiki>mailto:e.miller123</nowiki>(at)example (with a predicate "whose email address is"), and\n* "Dr." (with a predicate "whose title is").\n\nThe subject is a URI.\n\nThe predicates also have URIs. For example, the URI for each predicate:\n* "whose name is" is <nowiki>http://www.w3.org/2000/10/swap/pim/contact#fullName</nowiki>,\n* "whose email address is" is <nowiki>http://www.w3.org/2000/10/swap/pim/contact#mailbox</nowiki>,\n* "whose title is" is <nowiki>http://www.w3.org/2000/10/swap/pim/contact#personalTitle</nowiki>.\n\nIn addition, the subject has a type (with URI <nowiki>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</nowiki>), which is person (with URI <nowiki>http://www.w3.org/2000/10/swap/pim/contact#Person</nowiki>).\n\nTherefore, the following "subject, predicate, object" RDF triples can be expressed:\n* <nowiki>http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#fullName,</nowiki> "Eric Miller"\n* <nowiki>http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#mailbox, mailto:e.miller123(at)example</nowiki>\n* <nowiki>http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#personalTitle,</nowiki> "Dr."\n* <nowiki>http://www.w3.org/People/EM/contact#me, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://www.w3.org/2000/10/swap/pim/contact#Person</nowiki>\n\nIn standard N-Triples format, this RDF can be written as:\n<source lang="turtle">\n<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/2000/10/swap/pim/contact#fullName> "Eric Miller" .\n<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/2000/10/swap/pim/contact#mailbox> <mailto:e.miller123(at)example> .\n<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/2000/10/swap/pim/contact#personalTitle> "Dr." .\n<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2000/10/swap/pim/contact#Person> .\n</source>\n\nEquivalently, it can be written in standard Turtle (syntax) format as:\n\n<source lang="turtle">\n@prefix eric:    <http://www.w3.org/People/EM/contact#> .\n@prefix contact: <http://www.w3.org/2000/10/swap/pim/contact#> .\n@prefix rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n\neric:me contact:fullName "Eric Miller" .\neric:me contact:mailbox <mailto:e.miller123(at)example> .\neric:me contact:personalTitle "Dr." .\neric:me rdf:type contact:Person .\n</source>\n\nOr, it can be written in RDF/XML format as:\n<source lang="xml" enclose="div">\n<?xml version="1.0" encoding="utf-8"?>\n<rdf:RDF xmlns:contact="http://www.w3.org/2000/10/swap/pim/contact#" xmlns:eric="http://www.w3.org/People/EM/contact#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">\n  <rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me">\n    <contact:fullName>Eric Miller</contact:fullName>\n  </rdf:Description>\n  <rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me">\n    <contact:mailbox rdf:resource="mailto:e.miller123(at)example"/>\n  </rdf:Description>\n  <rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me">\n    <contact:personalTitle>Dr.</contact:personalTitle>\n  </rdf:Description>\n  <rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me">\n    <rdf:type rdf:resource="http://www.w3.org/2000/10/swap/pim/contact#Person"/>\n  </rdf:Description>\n</rdf:RDF>\n</source>\n\n=== Example 2: The postal abbreviation for New York ===\n\nCertain concepts in RDF are taken from [[logic]] and [[linguistics]], where subject-predicate and subject-predicate-object structures have meanings similar to, yet distinct from, the uses of those terms in RDF. This example demonstrates:\n\nIn the [[English language]] statement \'\' \'New York has the postal abbreviation NY\' \'\',\'\' \'New York\' \'\' would be the subject, \'\' \'has the postal abbreviation\' \'\' the predicate and \'\' \'NY\' \'\' the object.\n\nEncoded as an RDF triple, the subject and predicate would have to be resources named by URIs. The object could be a resource or literal element. For example, in the N-Triples form of RDF, the statement might look like:\n\n<source lang="turtle">\n<urn:x-states:New%20York> <http://purl.org/dc/terms/alternative> "NY" .\n</source>\n\nIn this example, "<nowiki>urn:x-states:New%20York</nowiki>" is the URI for a resource that denotes the US state [[New York (state)|New York]], "<nowiki>http://purl.org/dc/terms/alternative</nowiki>" is the URI for a predicate (whose human-readable definition can be found at here <ref>[http://dublincore.org/documents/dcmi-terms/index.shtml#terms-alternative DCMI Metadata Terms]. Dublincore.org. Retrieved on 2014-05-30.</ref>), and "NY" is a literal string.  Note that the URIs chosen here are not standard, and don\'t need to be, as long as their meaning is known to whatever is reading them.\n\n=== Example 3: A Wikipedia article about Tony Benn ===\nIn a like manner, given that <nowiki>"http://en.wikipedia.org/wiki/Tony_Benn"</nowiki> identifies a particular resource (regardless of whether that URI could be traversed as a hyperlink, or whether the resource is \'\'actually\'\' the [[Wikipedia]] article about [[Tony Benn]]), to say that the title of this resource is "Tony Benn" and its publisher is "Wikipedia" would be two assertions that could be expressed as valid RDF statements. In the N-Triples form of RDF, these statements might look like the following:\n\n<source lang="turtle">\n<http://en.wikipedia.org/wiki/Tony_Benn> <http://purl.org/dc/elements/1.1/title> "Tony Benn" .\n<http://en.wikipedia.org/wiki/Tony_Benn> <http://purl.org/dc/elements/1.1/publisher> "Wikipedia" .\n</source>\n\nTo an English-speaking person, the same information could be represented simply as:\n<blockquote>The title of this resource, which is published by Wikipedia, is \'Tony Benn\'</blockquote>\nHowever, RDF puts the information in a formal way that a machine can understand. The purpose of RDF is to provide an [[Semantics encoding|encoding]] and interpretation mechanism so that [[Resource (computer science)|resources]] can be described in a way that particular [[software]] can understand it; in other words, so that software can access and use information that it otherwise couldn\'t use.\n\nBoth versions of the statements above are wordy because one requirement for an RDF resource (as a subject or a predicate) is that it be unique. The subject resource must be unique in an attempt to pinpoint the exact resource being described. The predicate needs to be unique in order to reduce the chance that the idea of [[Title]] or [[Publisher]] will be ambiguous to software working with the description. If the software recognizes  \'\'<nowiki>http://purl.org/dc/elements/1.1/title</nowiki>\'\' (a specific [[definition]] for the [[concept]] of a title established by the [[Dublin Core]] Metadata Initiative), it will also know that this title is different from a land title or an honorary title or just the letters t-i-t-l-e put together.\n\nThe following example, written in Turtle, shows how such simple claims can be elaborated on, by combining multiple RDF vocabularies. Here, we note that the primary topic of the Wikipedia page is a "Person" whose name is "Tony Benn":\n\n<source lang="turtle">\n@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n@prefix dc:   <http://purl.org/dc/elements/1.1/> .\n\n<http://en.wikipedia.org/wiki/Tony_Benn>\n    dc:publisher "Wikipedia" ;\n    dc:title "Tony Benn" ;\n    foaf:primaryTopic [\n        a foaf:Person ;\n        foaf:name "Tony Benn"\n    ] .\n</source>\n\n== Applications ==\n* [[DBpedia]] – Extracts facts from Wikipedia articles and publishes them as RDF data.\n* [[Creative Commons]] – Uses RDF to embed license information in web pages and mp3 files.\n* [[FOAF (software)|FOAF (Friend of a Friend)]] – designed to describe [[person|people]], their interests and interconnections.\n* [[Haystack (PIM)|Haystack client]] – Semantic web browser from MIT CS & AI lab.<ref>[http://groups.csail.mit.edu/haystack/ Haystack]</ref>\n* [[IDEAS Group]] – developing a formal [[Ontology components|4D ontology]] for [[Enterprise Architecture]] using RDF as the encoding.<ref>[http://www.ideasgroup.org The IDEAS Group Website]</ref>\n* Microsoft shipped a product, Connected Services Framework,<ref>[http://www.microsoft.com/serviceproviders/solutions/connectedservicesframework.mspx Connected Services Framework]</ref> which provides RDF-based Profile Management capabilities.\n* [[MusicBrainz]] – Publishes information about Music Albums.<ref>[http://wiki.musicbrainz.org/RDF RDF on MusicBrainz Wiki]</ref>\n* [[NEPOMUK (framework)|NEPOMUK]], an open-source software specification for a Social Semantic desktop uses RDF as a storage format for collected metadata. NEPOMUK is mostly known because of its integration into the [[KDE Software Compilation 4|KDE SC 4]] desktop environment.\n* [[Press Association]] is a news agency in the UK. They use ontologies to dynamically identify and link their NoSQL data to do [[semantic publishing]] but in a dynamic, rules based way that creates custom content on the fly.<ref>[http://www.datalanguage.com/blog/2012/05/17/ontology-driven-software-engineering/]</ref>\n* RDF Site Summary – one of several "[[RSS (file format)|RSS]]" languages for publishing information about updates made to a web page; it is often used for disseminating news article summaries and sharing [[weblog]] content.\n* [[Simple Knowledge Organization System]] (SKOS) – a KR representation intended to support vocabulary/thesaurus applications\n* [[SIOC|SIOC (Semantically-Interlinked Online Communities)]] – designed to describe online communities and to create connections between Internet-based discussions from message boards, weblogs and mailing lists.<ref>[http://sioc-project.org/ SIOC (Semantically-Interlinked Online Communities)]</ref>\n* [[Smart-M3]] – provides an infrastructure for using RDF and specifically uses the ontology agnostic nature of RDF to enable heterogeneous mashing-up of information<ref>Oliver Ian, Honkola Jukka, Ziegler Jurgen (2008). “Dynamic, Localized Space Based Semantic Webs”. IADIS WWW/Internet 2008. Proceedings, p.426, IADIS Press, ISBN 978-972-8924-68-3</ref>\n\nSome uses of RDF include research into social networking. It will also help people in business fields understand better their relationships with members of industries that could be of use for product placement.<ref>An RDF Approach for Discovering the Relevant Semantic Associations in a Social Network By Thushar A.K, and P. Santhi Thilagam</ref>  It will also help scientists understand how people are connected to one another.\n\nRDF is being used to have a better understanding of road traffic patterns.  This is because the information regarding traffic patterns is  on different websites, and RDF is used to integrate information from different sources on the web. Before, the common methodology was using keyword searching, but this method is problematic because it does not  consider synonyms. This is why ontologies are useful in this situation. But one of the issues that comes up when trying to efficiently study traffic is that to fully understand traffic,  concepts related to people, streets, and roads must be well understood. Since these are human  concepts, they require the addition of [[fuzzy logic]]. This is because values that are useful  when describing roads, like slipperiness, are not precise concepts and cannot be measured. This would imply that the best solution would incorporate both fuzzy logic and ontology.<ref>Traffic Information Retrieval Based on Fuzzy Ontology and RDF on the Semantic Web By Jun Zhai, Yi Yu, Yiduo Liang, and Jiatao Jiang (2008)</ref>\n\n== See also ==\n;Notations for RDF\n* [[TriG (syntax)|TRiG]]\n* [[TriX (syntax)|TRiX]]\n* [[RDF/XML]]\n* [[RDFa]]\n* [[JSON-LD]]\n;Similar concepts\n* [[Entity-attribute-value model]]\n* [[Graph theory]] – An RDF model is a labeled, directed multi-graph.\n* [[Website Parse Template]]\n* [[Tag (metadata)|Tagging]]\n* [[SciCrunch]]\n* [[Semantic network]]\n; Other (unsorted):\n*[[Associative model of data]]\n*[[Business Intelligence 2.0]] (BI 2.0)\n*DataPortability\n* [[EU Open Data Portal]]\n*[[Folksonomy]]\n*[[LSID|Life Science Identifiers]]\n*[[Swoogle]]\n*[[Universal Networking Language]] (UNL)\n\n== References ==\n{{Reflist|2}}\n\n== Further reading ==\n* [http://www.w3.org/RDF/ W3C\'s RDF at W3C]: specifications, guides, and resources\n* [http://www.w3.org/TR/2004/REC-rdf-mt-20040210/ RDF Semantics]: specification of semantics, and complete systems of inference rules for both RDF and RDFS\n\n== External links ==\n{{Commons category|Resource Description Framework}}\n*{{DMOZ|Reference/Libraries/Library_and_Information_Science/Technical_Services/Cataloguing/Metadata/RDF/}}\n{{Semantic Web}}\n{{W3C Standards}}\n{{Data Exchange}}\n\n{{Authority control}}\n\n[[Category:Resource Description Framework| ]]\n[[Category:Knowledge representation]]\n[[Category:World Wide Web Consortium standards]]\n[[Category:XML]]\n[[Category:XML-based standards]]\n[[Category:Metadata]]\n[[Category:Semantic Web]]\n[[Category:Bibliography file formats]]']
['Findability', '1025538', '\'\'\'Findability\'\'\' is a term for the ease with which information contained on a [[website]] can be found, both from outside the website (using [[search engine]]s and the like) and by users already on the website.<ref>{{Cite journal|url = |title = Information architecture|author1=Jacob, Elin K.  |author2=Loehrlein, Aaron|date = 2009|journal = Annual Review of Information Science and Technology|doi = 10.1002/aris.2009.1440430110|pmid = |access-date = |publication-date = }}</ref> Although findability has relevance outside the [[World Wide Web]], the term is usually used in that context. Most relevant websites do not come up in the top results because designers and engineers do not cater to the way ranking algorithms work currently.<ref>{{Cite book|title=Ambient Findability|last=Morville|first=Peter|publisher=Oreilly|year=2005|isbn=978-0-596-00765-2|location=Sebastopol, CA|pages=|quote=|via=}}</ref> Its importance can be determined from the first law of [[e-commerce]], which states "If the user can’t find the product, the user can’t buy the product."<ref>{{Cite web|url = http://www.nngroup.com/reports/ecommerce|title = E-Commerce user experience: High-level strategy, Nielsen Norman Group|date = 2001|accessdate = |website = |publisher = }}</ref> As of December 2014, out of 10.3 billion monthly [[Google]] searches by Internet users in the [[United States]], an estimated 78% are made to research products and services online.<ref>{{Cite web|url = http://www.cmocouncil.org/facts-stats-categories.php?category=internet-marketing|title = Internet Marketing|date = |accessdate = |website = |publisher = }}</ref>\n\nFindability encompasses aspects of [[information architecture]], [[user interface design]], [[accessibility]] and [[search engine optimization]] (SEO), among others.\n\n==Introduction==\nFindability is similar to, but different from [[discoverability]], which is defined as the ability of something, especially a piece of content or information, to be found. It is different from web search in that the word \'find\' refers to locating something in a known space while \'search\' is in an unknown space or not in an expected location.<ref name="every-page" />\n\nMark Baker, the author of "Every Page is Page One",<ref name="every-page">{{Cite book|title = Every Page is Page One|last = Baker|first = Mark|publisher = XML Press|year = 2013|isbn = 978-1937434281|location = |pages = }}</ref> mentions that findability "is a content problem, not a search problem".<ref>{{cite web|last1=Baker|first1=Mark|title=Findability is a Content Problem, not a Search Problem|url=http://everypageispageone.com/2013/05/28/findability-is-a-content-problem-not-a-search-problem/|website=Every Page is Page One|accessdate=2015-04-25}}</ref> Even when the right content is present, users often find themselves deep within the content of a website but not in the right place. He further adds that findability is intractable, perfect findability is unattainable, but we need to focus on reducing the effort for finding that a user would have to do for themselves.\n\nFindability can be divided into external findability and on-site findability, based on where the customers need to find the information.\n\n==History==\n[[Heather Lutze]] is thought to have created the term in the early 2000s.<ref>{{cite web | url=http://www.huffingtonpost.com/liz-wainger/the-shtickiness-factor_b_3471675.html | title=The Shtickiness Factor |last1=Wainger | first1=Liz | publisher=The Huffington Post | date=20 June 2013 | accessdate=12 September 2013}}</ref> The popularization of the term "findability" for the Web is usually credited to [[Peter Morville]].{{citation needed|date=April 2015}} In 2005 he defined it as: "the ability of users to identify an appropriate Web site and navigate the pages of the site to discover and retrieve relevant information resources", though it appears to have been first coined in a public context referring to the web and information retrieval by Alkis Papadopoullos in a 2005 article entitled "Findability".<ref>{{cite journal|author=Alkis Papadopoulos|title=The Key to Enterprise Search|journal=KM World|date=April 1, 2005|url=http://news-business.vlex.com/vid/findability-key-to-enterprise-search-62406335}}</ref><ref>Though the word has been used to mean "ease of finding information" since at least 1943: see Urban A. Avery, "The \'Findability\' of the Law", \'\'Chicago Bar Record\'\' \'\'\'24\'\'\':272, April 1943, reprinted in the \'\'Journal of the American Judicature Society\'\' \'\'\'27\'\'\':25 [http://heinonline.org/HOL/LandingPage?collection=journals&handle=hein.journals/judica27&div=12&id=&page=]</ref>\n\n==External findability==\nExternal findability is the domain of [[internet marketing]] and [[Search engine optimization|search engine optimization (SEO)]] tactics. Several factors affect external findability:<ref>{{cite web|title=Findability Factors Found|url=http://www.econtentstrategies.com/Article_FindabilityFactorsFoundFinal_EContent_200701.pdf}}</ref>\n#\'\'Search Engine Indexing\'\': As the very first step, webpages need to be found by indexing crawler in order to be shown in the search results. It would be helpful to avoid factors that may lead to webpages being ignored by indexing crawlers. Those factors may include elements that require user interaction, such as entering log-in credentials. Algorithms for indexing vary by the search engine which means the number of webpages of a website successfully being indexed may be very different between Google and Yahoo!\'s search engines. Also, in countries like [[China]], [[Great Firewall|government policies]] could significantly influence the indexing algorithms. In this case, local knowledge about laws and policies could be valuable.<ref>{{cite web|title=Online Marketing in China|url=http://chineseseoshifu.com/china-online-marketing/}}</ref>\n#\'\'Page Descriptions in Search Results\'\': Now that the webpages are successfully indexed by web crawlers and show in the search results with decent ranking, the next step is to attract customers to click the link to the web pages. However, the customers can\'t see the whole web pages at this point; they can only see an excerpt of the webpage\'s content and metadata. Therefore, displaying meaningful information in a limited space, usually a couple of sentences, in search results is important for increasing click traffic of the webpages, and thus the findability of the web content on your webpages. \n#\'\'Keyword Matching\'\': At a semantic level, terminology used by the searcher and the content producer be different. Bridging the gap between the terms used by customers and developers is helpful for making web content more findable to more potential content consumers.\n\n==On-site findability==\nOn-site findability is concerned with the ability of a potential customer to find what they are looking for within a specific site. More than 90 percent of customers use internal searches in a website compared to browsing. Of those, only 50 percent find what they are looking for.<ref name="findability-solution">{{cite web|title=The Findability Solution|url=http://marriottschool.byu.edu/strategy/docs/TheFindabilitySolution-StrategyWhitePaper.pdf}}</ref> Improving the quality of on-site searches highly improves the business of the website. Several factors affect findability on a website:\n\n#\'\'Site search\'\': If searchers within a site do not find what they are looking for, they tend to leave rather than browse through the website. Users who had successful site searches are twice as likely to ultimately convert.<ref name="findability-solution" />\n#\'\'Related Links and Products\'\': User experience can be enhanced by trying to understand the needs of the customer and provide suggestions for other, related information.\n#\'\'Site Match to Customer Needs and Preferences\'\': Site design, content creation, and recommendations are major factors for affecting the customer experience.\n#\'\'Cross Device Experience\'\': With the rise of computing devices other than desktop computers, companies like Microsoft have focused more on smoothing the transition between devices to increase customer satisfaction.<ref>{{Cite web|url = http://research.microsoft.com/en-us/projects/courier/|title = Cross-Device User Experiences|date = |accessdate = |website = |publisher = }}</ref>\n\n==Evaluation and measures==\n\'\'\'Baseline Findability\'\'\' is the existing findability before changes are made in order to improve it. This is measured by participants who represent the customer base of the website, who try to locate a sample set of items using the existing navigation of the website.<ref>{{Cite book|title = Customer Analytics For Dummies|last = Sauro|first = Jeff|publisher = John Wiley & Sons|year = |isbn = 978-1-118-93759-4|location = |pages = |url = http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118937597.html}}</ref><ref>{{Cite web|url = http://www.measuringu.com/blog/measure-findability.php|title = How to Measure Findability|date = |accessdate = |website = |publisher = }}</ref>\n\nIn order to evaluate how easily information can be found by searching a site using a search engine or information retrieval system, [[retrievability]] measures were developed, and similarly, navigability measures now measure ease of information access through browsing a site (e.g. [[PageRank]], MNav, InfoScent (see [[Information foraging|Information Foraging]]), etc.).\n\nFindability also can be evaluated via the following techniques:\n* [[Usability testing]]: Conducted to find out how and why users navigate through a website to accomplish tasks.\n* [[Tree testing]]: An [[information architecture]] based technique, to determine if critical information can be found on the website.\n* [[Card sorting|Closed card sorting]]: A usability technique based on information architecture, for evaluating the strength of categories.\n* [[Click testing]]: Accounts for the implicit data collected through clicks on the user interface.<ref>{{Cite web|url = http://www.nngroup.com/articles/navigation-ia-tests/|title = Low Findability and Discoverability: Four Testing Methods to Identify the Causes|date = July 6, 2014|accessdate = |website = |publisher = }}</ref>\n\n==Beyond findability==\nFindability Sciences defines a findability index in terms of each user\'s influence, context, and sentiments. For seamless search, current websites focus on a combination of structured hypertext-based information architectures and rich Internet application-enabled visualization techniques.<ref>{{Cite journal|url = http://journalofia.org/volume2/issue1/03-spagnolo/|title = Beyond Findability - Search-Enhanced Information Architecture for Content-Intensive Rich Internet Applications|date = 2010|journal = |doi = |pmid = |access-date = }}</ref>\n\n==See also==\n* [[Information retrieval]]\n* [[Knowledge mining]]\n* [[Search engine optimization]]\n* [[Subject (documents)]]\n* [[Usability]]\n* [[User interface]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* Morville, P. (2005) Ambient findability. Sebastopol, CA: O\'Reilly\n* Wurman, R.S. (1996). Information architects. New York: Graphis.\n\n==External links==\n* [http://findability.org/ findability.org]: a collection of links to people, software, organizations, and content related to findability\n* [http://semanticstudios.com/publications/semantics/000007.php The age of findability] (article)\n* [http://www.useit.com/alertbox/search-keywords.html Use Old Words When Writing for Findability] (article on the findability impact of a site\'s choice of words)\n* [http://buildingfindablewebsites.com/ Building Findable Websites: Web Standards SEO and Beyond] (book)\n* [http://www.FindabilityFormula.com The Findability Formula: The Easy, Non-Technical Guide to Search Engine Marketing by Heather Lutze]\n\n[[Category:Web design]]\n[[Category:Knowledge representation]]\n[[Category:Information science]]\n[[Category:Information architecture]]']
['Attempto Controlled English', '6520028', '{{Refimprove|date=April 2016}}\n\'\'\'Attempto Controlled English\'\'\' (\'\'\'ACE\'\'\') is a [[controlled natural language]], i.e. a subset of standard [[English grammar|English]] with a restricted syntax and restricted semantics described by a small set of construction and interpretation rules.<ref>{{cite conference |author1=Norbert E. Fuchs |author2=Kaarel Kaljurand |author3=Gerold Schneider | title = Attempto Controlled English Meets the Challenges of Knowledge Representation, Reasoning, Interoperability and User Interfaces | booktitle = FLAIRS 2006 | date = 2006 | url = http://attempto.ifi.uzh.ch/site/publications/papers/FLAIRS0601FuchsN.pdf | format = [[PDF]]}}</ref> It has been under development at the [[University of Zurich]] since 1995. In 2013, ACE version 6.7 was announced.<ref>http://attempto.ifi.uzh.ch/site/news/</ref>\n\nACE can serve as [[knowledge representation]], [[specification language|specification]], and [[query language]], and is intended for professionals who want to use formal notations and formal methods, but may not be familiar with them. Though ACE appears perfectly natural – it can be read and understood by any speaker of English – it is in fact a [[formal language]].\n\nACE and its related tools have been used in the fields of [[requirements analysis|software specifications]], [[theorem proving]], [[automatic summarization|text summaries]], [[ontologies]], rules, querying, [[health informatics|medical documentation]] and [[planning]].\n\nHere are some simple examples:\n\n# Every woman is a human.\n# A woman is a human.\n# A man tries-on a new tie. If the tie pleases his wife then the man buys it.\n\nACE construction rules require that each noun be introduced by a determiner (\'\'a\'\', \'\'every\'\', \'\'no\'\', \'\'some\'\', \'\'at least 5\'\', ...). ACE interpretation rules decide that (1) is interpreted as [[Universal quantification|universally quantified]], while (2) is interpreted as [[Existential quantification|existentially quantified]]. Sentences like "Women are human" do not follow ACE syntax and are consequently not valid.\n\nInterpretation rules resolve the [[Deixis#Anaphoric reference|anaphoric references]] in (3): \'\'the tie\'\' and \'\'it\'\' of the second sentence refer to \'\'a new tie\'\' of the first sentence, while \'\'his\'\' and \'\'the man\'\' of the second sentence refer to \'\'a man\'\' of the first sentence. Thus an ACE text is a coherent entity of anaphorically linked sentences.\n\nThe Attempto Parsing Engine (APE) translates ACE texts unambiguously into [[Discourse Representation Theory|discourse representation structures]] (DRS) that use a variant of the language of [[first-order logic]].<ref>{{cite conference |author1=Norbert E. Fuchs |author2=Kaarel Kaljurand |author3=Tobias Kuhn | title = Discourse Representation Structures for ACE 6.6 | booktitle = Technical Report ifi-2010.0010, Department of Informatics, University of Zurich | date = 2010 | url = http://attempto.ifi.uzh.ch/site/pubs/papers/drs_report_66.pdf| format = [[PDF]] }}</ref> A DRS can be further translated into other [[formal languages]], for instance AceRules with various semantics,<ref>{{cite conference | author = Tobias Kuhn | title = AceRules: Executing Rules in Controlled Natural Language | booktitle = First International Conference on Web Reasoning and Rule Systems (RR 2007) | year = 2007  | url = http://attempto.ifi.uzh.ch/site/pubs/papers/kuhn07acerules.pdf| format = [[PDF]]}}</ref>   [[Web Ontology Language|OWL]],<ref>{{cite conference |author1=Kaarel Kaljurand |author2=Norbert E. Fuchs | title = Verbalizing OWL in Attempto Controlled English | booktitle = OWL: Experiences and Directions (OWLED 2007) | year = 2007  | url = http://attempto.ifi.uzh.ch/site/pubs/papers/owled2007_kaljurand.pdf | format = [[PDF]]}}</ref> and [[Semantic Web Rule Language|SWRL]]. Translating an ACE text into (a fragment of) first-order logic allows users to  [[inference|reason]] about the text, for instance to [[formal verification|verify]], to [[formal verification|validate]], and to [[Information retrieval|query]] it.\n\n== ACE in a nutshell ==\n{{unreferenced section|date=May 2013}}\nAs an overview of the current version 6.6 of ACE this section:\n\n* Briefly describes the vocabulary\n* Gives an account of the syntax\n* Summarises the handling of ambiguity\n* Explains the processing of anaphoric references.\n\n=== Vocabulary ===\n\nThe vocabulary of ACE comprises:\n\n* Predefined function words (e.g. determiners, conjunctions)\n* Predefined phrases (e.g. "it is false that ...", "it is possible that ...")\n* Content words (e.g. nouns, verbs, adjectives, adverbs).\n\n=== Grammar ===\n\nThe grammar of ACE defines and constrains the form and the meaning of ACE sentences and texts. ACE\'s grammar is expressed as a set of  [http://attempto.ifi.uzh.ch/site/docs/ace_constructionrules.html  construction rules]. The meaning of sentences is described as a small set of [http://attempto.ifi.uzh.ch/site/docs/ace_interpretationrules.html  interpretation rules]. A [http://attempto.ifi.uzh.ch/site/docs/ace_troubleshooting.html Troubleshooting Guide] describes how to use ACE and how to avoid pitfalls.\n\n==== ACE texts ====\n\nAn ACE text is a sequence of declarative sentences that can be anaphorically interrelated. Furthermore, ACE supports questions and commands.\n\n==== Simple sentences ====\n\nA simple sentence asserts that something is the case — a fact, an event, a state.\n\n:The temperature is -2 °C.\n:A customer inserts 2 cards. \n:A card and a code are valid.\n\nSimple ACE sentences have the following general structure:\n\n:subject + verb + complements + adjuncts\n\nEvery sentence has a subject and a verb. Complements (direct and indirect objects) are necessary for transitive verbs (\'\'insert something\'\') and ditransitive verbs (\'\'give something to somebody\'\'), whereas adjuncts (adverbs, prepositional phrases) are optional.\n\nAll elements of a simple sentence can be elaborated upon to describe the situation in more detail. To further specify the nouns \'\'customer\'\' and \'\'card\'\', we could add adjectives:\n\n:A trusted customer inserts two valid cards.\n\npossessive nouns and \'\'of\'\'-prepositional phrases:\n\n:John\'s customer inserts a card of Mary.\n\nor variables as appositions:\n\n:John inserts a card A.\n\nOther modifications of nouns are possible through relative sentences:\n\n:A customer who is trusted inserts a card that he owns.\n\nwhich are described below since they make a sentence composite. We can also detail the insertion event, e.g. by adding an adverb:\n\n:A customer inserts some cards manually.\n\nor, equivalently:\n\n:A customer manually inserts some cards.\n\nor, by adding prepositional phrases:\n\n:A customer inserts some cards into a slot.\n\nWe can combine all of these elaborations to arrive at:\n\n:John\'s customer who is trusted inserts a valid card of Mary manually into a slot A.\n\n==== Composite sentences ====\n\nComposite sentences are recursively built from simpler sentences through [[coordination (linguistics)|coordination]], [[subordination (linguistics)|subordination]], [[Quantification (linguistics)|quantification]], and [[negation]]. Note that ACE composite sentences overlap with what linguists call compound sentences and complex sentences.\n\n===== Coordination =====\n\nCoordination by \'\'and\'\' is possible between sentences and between phrases of the same syntactic type.\n\n:A customer inserts a card and the machine checks the code.\n:There is a customer who inserts a card and who enters a code.\n:A customer inserts a card and enters a code.\n:An old and trusted customer enters a card and a code.\n\nNote that the coordination of the noun phrases \'\'a card\'\' and \'\'a code\'\' represents a plural object.\n\nCoordination by \'\'or\'\' is possible between sentences, verb phrases, and relative clauses.\n\n:A customer inserts a card or the machine checks the code.\n:A customer inserts a card or enters a code.\n:A customer owns a card that is invalid or that is damaged.\n\nCoordination by \'\'and\'\' and \'\'or\'\' is governed by the standard binding order of logic, i.e. \'\'and\'\' binds stronger than \'\'or\'\'. Commas can be used to override the standard binding order. Thus the sentence:\n\n:A customer inserts a VisaCard or inserts a MasterCard, and inserts a code.\n\nmeans that the customer inserts a VisaCard and a code, or alternatively a MasterCard and a code.\n\n===== Subordination =====\n\nThere are four constructs of subordination: relative sentences, \'\'if-then\'\' sentences, modality, and sentence subordination.\n\nRelative sentences starting with \'\'who\'\', \'\'which\'\', and \'\'that\'\' allow to add detail to nouns:\n\n:A customer who is trusted inserts a card that he owns.\n\nWith the help of \'\'if-then\'\' sentences we can specify conditional or hypothetical situations:\n\n:If a card is valid then a customer inserts it.\n\nNote the anaphoric reference via the pronoun \'\'it\'\' in the \'\'then\'\'-part to the noun phrase \'\'a card\'\' in the \'\'if\'\'-part.\n\nModality allows us to express possibility and necessity:\n\n:A trusted customer can/must insert a card.\n:It is possible/necessary that a trusted customer inserts a card.\n\nSentence subordination comes in various forms:\n\n:It is true/false that a customer inserts a card.\n:It is not provable that a customer inserts a card.\n:A clerk believes that a customer inserts a card.\n\n===== Quantification =====\n\nQuantification allows us to speak about all objects of a certain class ([[universal quantification]]), or to denote explicitly the existence of at least one object of this class ([[existential quantification]]). The textual occurrence of a universal or existential quantifier opens its scope that extends to the end of the sentence, or in coordinations to the end of the respective coordinated sentence.\n\nTo express that all involved customers insert cards we can write\n\n:Every customer inserts a card.\n\nThis sentence means that each customer inserts a card that may, or may not, be the same as the one inserted by another customer. To specify that all customers insert the same card — however unrealistic that situation seems — we can write:\n\n:A card is inserted by every customer.\n\nor, equivalently:\n\n:There is a card that every customer inserts.\n\nTo state that every card is inserted by a customer we write:\n\n:Every card is inserted by a customer.\n\nor, somewhat indirectly:\n\n:For every card there is a customer who inserts it.\n\n===== Negation =====\n\nNegation allows us to express that something is not the case:\n\n:A customer does not insert a card.\n:A card is not valid.\n\nTo negate something for all objects of a certain class one uses \'\'no\'\':\n\n:No customer inserts more than 2 cards.\n\nor, \'\'there is no\'\':\n\n:There is no customer who inserts a card.\n\nTo negate a complete statement one uses sentence negation:\n\n:It is false that a customer inserts a card.\n\nThese forms of negation are logical negations, i.e. they state that something is provably not the case. Negation as failure states that a state of affairs cannot be proved, i.e. there is no information whether the state of affairs is the case or not.\n\n:It is not provable that a customer inserts a card.\n\n==== Queries ====\n\nACE supports two forms of queries: \'\'yes/no\'\'-queries and \'\'wh\'\'-queries.\n\n\'\'Yes/no\'\'-queries ask for the existence or non-existence of a specified situation. If we specified:\n\n:A customer inserts a card.\n\nthen we can ask:\n\n:Does a customer insert a card?\n\nto get a positive answer. Note that interrogative sentences always end with a question mark.\n\nWith the help of \'\'wh\'\'-queries, i.e. queries with query words, we can interrogate a text for details of the specified situation. If we specified:\n\n:A trusted customer inserts a valid card manually in the morning in a bank.\n\nwe can ask for each element of the sentence with the exception of the verb.\n\n:Who inserts a card?\n:Which customer inserts a card?\n:What does a customer insert?\n:How does a customer insert a card?\n:When does a customer enter a card?\n:Where does a customer enter a card?\n\nQueries can also be constructed by a sequence of declarative sentences followed by one interrogative sentence, for example:\n\n:There is a customer and there is a card that the customer enters. Does a customer enter a card?\n\n==== Commands ====\n\nACE also supports commands. Some examples:\n\n:John, go to the bank!\n:John and Mary, wait!\n:Every dog, bark!\n:A brother of John, give a book to Mary!\n\nA command always consists of a noun phrase (the addressee), followed by a comma, followed by an uncoordinated verb phrase. Furthermore, a command has to end with an exclamation mark.\n\n=== Constraining ambiguity ===\n\nTo constrain the ambiguity of full natural language ACE employs three simple means:\n\n* Some ambiguous constructs are not part of the language; unambiguous alternatives are available in their place\n* All remaining ambiguous constructs are interpreted deterministically on the basis of a small number of interpretation rules\n* Users can either accept the assigned interpretation, or they must rephrase the input to obtain another one.\n\n==== Avoidance of ambiguity ====\n\nIn natural language, relative sentences combined with coordinations can introduce ambiguity:\n\n:A customer inserts a card that is valid and opens an account.\n\nIn ACE the sentence has the unequivocal meaning that the customer opens an account, as reflected by the paraphrase:\n\n:A card is valid. A customer inserts the card. The customer opens an account.\n\nTo express the alternative — though not very realistic — meaning that the card opens an account, the relative pronoun \'\'that\'\' must be repeated, thus yielding a coordination of relative sentences:\n\n:A customer inserts a card that is valid and that opens an account.\n\nThis sentence is unambiguously equivalent in meaning to the paraphrase:\n\n:A card is valid. The card opens an account. A customer inserts the card.\n\n==== Interpretation rules ====\n\nNot all ambiguities can be safely removed from ACE without rendering it artificial. To deterministically interpret otherwise syntactically correct ACE sentences we use a small set of interpretation rules. For example, if we write:\n\n:A customer inserts a card with a code.\n\nthen \'\'with a code\'\' attaches to the verb \'\'inserts\'\', but not to \'\'a card\'\'. However, this is probably not what we meant to say. To express that \'\'the code\'\' is associated with \'\'the card\'\' we can employ the interpretation rule that a relative sentence always modifies the immediately preceding noun phrase, and rephrase the input as:\n\n:A customer inserts a card that carries a code.\n\nyielding the paraphrase:\n\n:A card carries a code. A customer inserts the card.\n\nor — to specify that the customer inserts a card and a code — as:\n\n:A customer inserts a card and a code.\n\n=== Anaphoric references ===\n\nUsually ACE texts consist of more than one sentence:\n\n:A customer enters a card and a code. If a code is valid then SimpleMat accepts a card.\n\nTo express that all occurrences of card and code should mean the same card and the same code, ACE provides anaphoric references via the definite article:\n\n:A customer enters a card and a code. If the code is valid then SimpleMat accepts the card.\n\nDuring the processing of the ACE text, all anaphoric references are replaced by the most recent and most specific accessible noun phrase that agrees in gender and number. As an example of "most recent and most specific", suppose an ACE parser is given the sentence:\n\n:A customer enters a red card and a blue card.\n\nThen:\n\n:The card is correct.\n\nrefers to the second card, while:\n\n:The red card is correct.\n\nrefers to the first card.\n\nNoun phrases within \'\'if-then\'\' sentences, universally quantified sentences, negations, modality, and subordinated sentences cannot be referred to anaphorically from subsequent sentences, i.e. such noun phrases are not "accessible" from the following text. Thus for each of the sentences:\n\n:If a customer owns a card, then they enter it.\n:Every customer enters a card.\n:A customer does not enter a card.\n:A customer can enter a card.\n:A clerk believes that a customer enters a card.\n\nwe cannot refer to \'\'a card\'\' with:\n\n:The card is correct.\n\nAnaphoric references are also possible via personal pronouns:\n\n:A customer enters a card and a code. If it is valid then SimpleMat accepts the card.\n\nor via variables:\n\n:A customer enters a card X and a code Y. If Y is valid then SimpleMat accepts X.\n\nAnaphoric references via definite articles and variables can be combined:\n\n:A customer enters a card X and a code Y. If the code Y is valid then SimpleMat accepts the card X.\n\nNote that proper names like \'\'SimpleMat\'\' always refer to the same object.\n\n==See also==\n*[[Gellish]]\n*[[Natural Language Processing]]\n*[[Knowledge Representation]]\n*[[Natural language programming]]\n*[[Structured English]]\n**[[ClearTalk]], another machine-readable knowledge representation language\n**[[Inform 7]], a programming language with English syntax\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://attempto.ifi.uzh.ch Project Attempto]\n\n[[Category:Controlled English]]\n[[Category:Knowledge representation]]\n[[Category:Controlled natural languages]]\n[[Category:Natural language processing]]\n[[Category:Natural language parsing]]']
['Agent Communications Language', '3348350', "{{Refimprove|date=September 2014}}\n'''Agent Communication Language''' ('''ACL'''), proposed by the [[Foundation for Intelligent Physical Agents]] (FIPA), is a proposed standard language for [[Software agent|agent]] communications.  [[Knowledge Query and Manipulation Language]] (KQML) is another proposed standard.\n\nThe most popular ACLs are:\n* FIPA-ACL <ref>{{cite journal|last=Poslad|first=Stefan|title=Specifying Protocols for Multi-agent System Interaction|year=2007|journal=ACM Transactions on Autonomous and Adaptive Systems|volume=4|issue=4|doi=10.1145/1293731.1293735}}</ref> (by the [[Foundation for Intelligent Physical Agents]], a standardization consortium)\n* [[KQML]] <ref>{{cite journal|last=Finin|first=Tim|author2= Richard Fritzson, Don McKay and Robin McEntire |title=KQML as an agent communication language|year=1994|conference=Proceedings of the third international conference on Information and knowledge management, CIKM '94 |pages= 456–463}}</ref> (Knowledge Query and Manipulation Language)\n\nBoth rely on [[speech act]] theory developed by [[John Searle|Searle]] in the 1960s <ref>{{cite book|last= Searle|first=J.R.|year=1969|title=Speech Acts|publisher=Cambridge University Press, Cambridge, UK}}</ref> and enhanced by [[Terry Winograd|Winograd]] and [[Fernando Flores|Flores]] in the 1970s. They define a set of [[Performative utterance|performatives]], also called Communicative Acts, and their meaning (e.g. ask-one). The content of the performative is not standardized, but varies from system to system.\n\nTo make agents understand each other they have to not only speak the same language, but also have a common [[Ontology (computer science)|ontology]]. An ontology is a part of the agent's knowledge base that describes what kind of things an agent can deal with and how they are related to each other.\n\nExamples of frameworks that implement a standard agent communication language (FIPA-ACL) include FIPA-OS <ref>{{cite conference|last=Poslad|first=Stefan|author2=Philip Buckle and Robert Hadingham|title=The FIPA-OS agent platform: Open Source for Open Standards|year=2000|conference=Proceedings of 5th International Conference on the Practical Application Of Intelligent Agents And Multi-Agent Technology (PAAM)|pages=355–368}}</ref><ref>{{cite journal|last=Poslad|first= S|author2=Buckle P, Hadingham R.G|title=Open Source, Standards and Scaleable Agencies|journal=Lecture Notes in Computer Science|year= 2001|volume=1887|pages=296–303|DOI=10.1007/3-540-47772-1_30}}</ref>\nand [[Java Agent Development Framework|Jade]]. <ref>{{cite conference|last=Bellifeminee|first=Fabio|author2=Agostino Poggi and Giovanni Rimassa|title=JADE: a FIPA2000 compliant agent development environment|year=2001|conference=Proceedings of the fifth international conference on Autonomous agents|pages=216–217}}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Formal languages]]\n[[Category:Knowledge representation]]\n[[Category:Multi-agent systems]]\n\n\n{{Measurement-stub}}\n{{computing-stub}}"]
['Ramification problem', '496055', '{{Cleanup|date=April 2011}}\n\nIn [[philosophy]] and [[artificial intelligence]] (especially, knowledge based systems), the \'\'\'ramification problem\'\'\' is concerned with the indirect consequences of an action. It might also be posed as \'\'how to represent what happens implicitly due to an action\'\' or how to control the secondary and tertiary effects of an action. It is strongly connected to, and is opposite the [[qualification problem|qualification side]] of, the [[frame problem]].\n\nLimit theory helps in [[operational]] usage. For instance, in [[Knowledge-based engineering|KBE]] derivation of a populated design (geometrical objects, etc., similar concerns apply in shape theory), equivalence assumptions allow convergence where potentially large, and perhaps even computationally indeterminate, solution sets are handled deftly. Yet, in a chain of computation, downstream events may very well find some types of results from earlier resolutions of \'\'\'ramification\'\'\' as problematic for their own algorithms.\n\n==See also==\n*[[Non-monotonic logic]]\n*[[Ramification (mathematics)]]\n\n==External links==\n*Nikos Papadakis [http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/&toc=comp/proceedings/ictai/2002/1849/00/1849toc.xml&DOI=10.1109/TAI.2002.1180791 "Actions with Duration and Constraints: the Ramification Problem in Temporal Databases"] IEEE ICTAI\'02\n*Deepak Kumar "[http://blackcat.brynmawr.edu/~dkumar/UGAI/planning.html AI Planning]" Bryn Mawr College\n\n[[Category:Logic programming]]\n[[Category:Knowledge representation]]\n[[Category:Epistemology]]\n\n\n{{epistemology-stub}}']
['Living graph', '10308920', 'In terms of knowledge representation, a \'\'\'living graph\'\'\' (also referred to as a "lifeline", "living timeline"<ref name="history"/> or "fortune line".<ref name="NatStrat"/>) is a graph similar to a [[chronology]] timeline which places events along a vertical axis to reflect changes over time. The vertical axis can be used to represent many factors, such as relative importance, degrees of success/failure, danger/safety or happiness/sadness. In this sense they have been described as being "timelines with attitude".<ref name="history"/>\n\n==References==\n{{Reflist|refs=\n<ref name="history"> {{Cite web\n  | last = Dawson\n  | first = Ian\n  | authorlink = \n  |author2=Dawson, Patricia Ann\n   | title = Introducing Living Graphs\n  | work = \n  | publisher = \n  | url = http://thinkinghistory.co.uk/ActivityModel/ActModTimeline.html#graph\n  | format = \n  | doi = \n  | accessdate = 25 March 2010}} from Thinking History website\n</ref>\n<ref name="NatStrat"> {{Cite web\n  | last = \n  | first = \n  | authorlink = \n  | title = Living Graphs and Fortune Lines\n  | work = \n  | publisher = The National Strategies\n  | url = http://downloads.nationalstrategies.co.uk/pdf/67dbff4bdcf5e5534122e1d6ead53abc.pdf\n  | format = pdf\n  | doi = \n  | accessdate = 25 March 2010}}</ref>}}\n\n==External links==\n*[http://classtools.net/samples/sample.php?livingGraph Interactive Living Graph Templates in Flash]\n*[http://www.face-online.org.uk/index.php?option=com_content&task=view&id=57&Itemid=172 FACE Living Graph Exercise]\n\n[[Category:Knowledge representation]]\n[[Category:Diagrams]]\n[[Category:Quality control tools]]\n{{comm-design-stub}}']
['Knowledge space', '11851855', "{{about|knowledge spaces in mathematical psychology|the concept studied by philosopher Pierre Lévy|Knowledge space (philosophy)}}\n\nIn [[mathematical psychology]], a '''knowledge space''' is a [[antimatroid|combinatorial structure]] describing the possible states of knowledge of a human learner.<ref>{{citation|title=Knowledge Spaces|last1=Doignon|first1=J.-P.|last2=Falmagne|first2=J.-Cl.|author2-link=Jean-Claude Falmagne|publisher=Springer-Verlag|year=1999|isbn = 3-540-64501-2}}.</ref>\nTo form a knowledge space, one models a domain of knowledge as a [[set (mathematics)|set]] of concepts, and a feasible state of knowledge as a [[subset]] of that set containing the concepts known or knowable by some individual. Typically, not all subsets are feasible, due to prerequisite relations among the concepts. The knowledge space is the family of all the feasible subsets. Knowledge spaces were introduced in 1985 by [[Jean-Paul Doignon]] and [[Jean-Claude Falmagne]]<ref>{{citation|last1=Doignon|first1=J.-P.|last2=Falmagne|first2=J.-Cl.|author2-link=Jean-Claude Falmagne|year=1985|title=Spaces for the assessment of knowledge|journal=International Journal of Man-Machine Studies|volume=23|issue=2|pages=175–196|doi=10.1016/S0020-7373(85)80031-6}}.</ref> and have since been studied by many other researchers.<ref>{{citation|title=Knowledge Spaces. Applications in Education|last1=Falmagne|first1=J.-Cl.|author1-link=Jean-Claude Falmagne|last2=Albert|first2=D.|last3=Doble|first3=C.|last4=Eppstein|first4=D.|author4-link=David Eppstein|last5=Hu|first5=X.|publisher=Springer|year=2013}}.</ref><ref>A [http://kst.hockemeyer.at/kst-bib.html bibliography on knowledge spaces] maintained by Cord Hockemeyer contains over 400 publications on the subject.</ref> They also form the basis for two computerized tutoring systems, [http://wundt.kfunigraz.ac.at/rath/ RATH] and [[ALEKS]].<ref>[http://wundt.uni-graz.at/MathPsych/cda/overview_sokrates.htm Introduction to Knowledge Spaces: Theory and Applications], Christof Körner, Gudrun Wesiak, and Cord Hockemeyer, 1999 and 2001.</ref>\n\nIt is possible to interpret a knowledge space as a special form of a restricted [[latent class model]].<ref>{{citation|title=About the connection between knowledge structures and latent class models |last1=Schrepp |first1=M. |journal=Methodology|volume=1|issue=3|pages=92–102|year=2005|doi=10.1027/1614-2241.1.3.92}}.</ref>\n\n==Definitions==\nSome basic definitions used in the knowledge space approach -\n*A tuple <math>(Q, K)</math> consisting of a non-empty set <math>Q</math> and a set <math>K</math> of subsets from <math>Q</math> is called a ''knowledge structure'' if <math>K</math> contains the empty set and <math>Q</math>.\n*A knowledge structure is called a ''knowledge space'' if it is closed under union, i.e. if <math>S, T \\in Q</math> implies <math>S\\cup T \\in Q</math>.\n*A knowledge space is called a ''quasi-ordinal knowledge space'' if it is in addition closed under intersection, i.e. if <math>S, T \\in Q</math> implies <math>S\\cap T \\in Q</math>. Closure under both unions and intersections gives (''Q'',∪,∩) the structure of a [[distributive lattice]]; [[Birkhoff's representation theorem]] for distributive lattices shows that there is a one-to-one correspondence between the set of all [[preorder|quasiorders]] on Q and the set of all quasi-ordinal knowledge spaces on Q. I.e., each quasi-ordinal knowledge space can be represented by a quasi-order and vice versa.\n\nAn important subclass of knowledge spaces, the ''well-graded knowledge spaces'' or ''learning spaces'', can be defined as satisfying two additional mathematical axioms:\n# If <math>S</math> and <math>T</math> are both feasible subsets of concepts, then <math>S\\cup T</math> is also feasible. In educational terms: if it is possible for someone to know all the concepts in ''S'', and someone else to know all the concepts in ''T'', then we can posit the potential existence of a third person who combines the knowledge of both people.\n# If <math>S</math> is a nonempty feasible subset of concepts, then there is some concept ''x'' in ''S'' such that <math>S\\setminus\\{x\\}</math> is also feasible. In educational terms: any feasible state of knowledge can be reached by learning one concept at a time, for a finite set of concepts to be learned.\nA set family satisfying these two axioms forms a [[mathematical structure]] known as an [[antimatroid]].\n\n==Construction of knowledge spaces==\nIn practice, there exist several methods to construct knowledge spaces. The most frequently used method is querying experts. There exist several querying algorithms that allow one or several experts to construct a knowledge space by answering a sequence of simple questions.<ref>{{citation|title=Extracting human expertise for constructing knowledge spaces: An algorithm |last1=Koppen |first1=M. |journal=Journal of Mathematical Psychology|volume=37|pages=1–20 |year=1993 |doi=10.1006/jmps.1993.1001}}.</ref><ref>{{citation|title=How to build a knowledge space by querying an expert |last1=Koppen |first1=M. |last2=Doignon |first2=J.-P. |journal=Journal of Mathematical Psychology|volume=34|issue=3|pages=311–331 |year=1990 |doi=10.1016/0022-2496(90)90035-8}}.</ref><ref>{{citation|title=A simulation study concerning the effect of errors on the establishment of knowledge spaces by querying experts |last1=Schrepp |first1=M. |last2=Held |first2=T. |journal=Journal of Mathematical Psychology|volume=39|issue=4|pages=376–382 |year=1995|doi=10.1006/jmps.1995.1035}}</ref>\n\nAnother method is to construct the knowledge space by explorative data analysis (for example by [[Item tree analysis]]) from data.<ref>{{citation|title=Extracting knowledge structures from observed data |last1=Schrepp |first1=M. |journal=[[British journal of mathematical and statistical psychology]]|volume= 52|issue=2 |pages=213–224 |year=1999|doi=10.1348/000711099159071}}</ref><ref>{{citation|title=A method for the analysis of hierarchical dependencies between items of a questionnaire |last1=Schrepp |first1=M. |journal= Methods of Psychological Research Online|volume=19|pages=43–79  |year=2003|url=http://www.dgps.de/fachgruppen/methoden/mpr-online/issue19/art3/mpr106_04.pdf }}</ref>\nA third method is to derive the knowledge space from an analysis of the problem solving processes in the corresponding domain.<ref>{{citation|title=Knowledge Spaces: Theories, Empirical Research, Applications|last1=Albert|first1=D.|last2=Lukas|first2=J.|publisher=Lawrence Erlbaum Associates, Mahwah, NJ|year=1999}}</ref>\n\n==References==\n\n{{reflist}}\n\n[[Category:Cognition]]\n[[Category:Knowledge representation]]"]
['Belief revision', '1187311', '\'\'\'Belief revision\'\'\' is the process of changing beliefs to take into account a new piece of information. The [[formal logic|logical]] formalization of belief revision is researched in [[philosophy]], in [[databases]], and in artificial intelligence for the design of [[intelligent agent|rational agent]]s.\n\nWhat makes belief revision non-trivial is that several different ways for performing this operation may be possible. For example, if the current knowledge includes the three facts "<math>A</math> is true", "<math>B</math> is true" and "if <math>A</math> and <math>B</math> are true then <math>C</math> is true", the introduction of the new information "<math>C</math> is false" can be done preserving consistency only by removing at least one of the three facts. In this case, there are at least three different ways for performing revision. In general, there may be several different ways for changing knowledge.\n\n==Revision and update==\n\nTwo kinds of changes are usually distinguished:\n\n; update : the new information is about the situation at present, while the old beliefs refer to the past; update is the operation of changing the old beliefs to take into account the change;\n\n; revision : both the old beliefs and the new information refer to the same situation; an inconsistency between the new and old information is explained by the possibility of old information being less reliable than the new one; revision is the process of inserting the new information into the set of old beliefs without generating an inconsistency.\n\nThe main assumption of belief revision is that of minimal change: the knowledge before and after the change should be as similar as possible. In the case of update, this principle formalizes the assumption of inertia. In the case of revision, this principle enforces as much information as possible to be preserved by the change.\n\n===Example===\n\nThe following classical example shows that the operations to perform in the two settings of update and revision are not the same. The example is based on two different interpretations of the set of beliefs <math>\\{a \\vee b\\}</math> and the new piece of information <math>\\neg a</math>:\n\n; update : in this scenario, two satellites, Unit A and Unit B, orbit around Mars; the satellites are programmed to land while transmitting their status to Earth; Earth has received a transmission from one of the satellites, communicating that it is still in orbit; however, due to interference, it is not known which satellite sent the signal; subsequently, Earth receives the communication that Unit A has landed; this scenario can be modeled in the following way; two [[propositional variable]]s <math>a</math> and <math>b</math> indicate that Unit A and Unit B, respectively, are still in orbit; the initial set of beliefs is <math>\\{a \\vee b\\}</math> (either one of the two satellites is still in orbit) and the new piece of information is <math>\\neg a</math> (Unit A has landed, and is therefore not in orbit); the only rational result of the update is <math>\\neg a</math>; since the initial information that one of the two satellites had not landed yet was possibly coming from the Unit A, the position of the Unit B is not known;\n\n; revision : the play "Six Characters in Search of an Author" will be performed in one of the two local theatres; this information can be denoted by <math>\\{a \\vee b\\}</math>, where <math>a</math> and <math>b</math> indicates that the play will be performed at the first or at the second theatre, respectively; a further information that "Jesus Christ Superstar" will be performed at the first theatre indicates that <math>\\neg a</math> holds; in this case, the obvious conclusion is that "Six Characters in Search of an Author" will be performed at the second but not the first theatre, which is represented in logic by <math>\\neg a \\wedge b</math>.\n\nThis example shows that revising the belief <math>a \\vee b</math> with the new information <math>\\neg a</math> produces two different results <math>\\neg a </math> and <math>\\neg a \\wedge b</math> depending on whether the setting is that of update or revision.\n\n==Contraction, expansion, revision, consolidation, and merging==\n\nIn the setting in which all beliefs refer to the same situation, a distinction between various operations that can be performed is made:\n\n; contraction : removal of a belief;\n\n; expansion : addition of a belief without checking consistency;\n\n; revision : addition of a belief while maintaining consistency;\n\n; consolidation : restoring consistency of a set of beliefs;\n\n; merging : fusion of two or more sets of beliefs while maintaining consistency.\n\nRevision and merging differ in that the first operation is done when the new belief to incorporate is considered more reliable than the old ones; therefore, consistency is maintained by removing some of the old beliefs. Merging is a more general operation, in that the priority among the belief sets may or may not be the same.\n\nRevision can be performed by first incorporating the new fact and then restoring consistency via consolidation. This is actually a form of merging rather than revision, as the new information is not always treated as more reliable than the old knowledge.\n\n==The AGM postulates==\n\nThe AGM postulates (named after the names of their proponents, [[Carlos Alchourrón|Alchourrón]], [[Peter Gärdenfors|Gärdenfors]], and [[David Makinson|Makinson]]) are properties that an operator that performs revision should satisfy in order for that operator to be considered rational. The considered setting is that of revision, that is, different pieces of information referring to the same situation. Three operations are considered: expansion (addition of a belief without a consistency check), revision (addition of a belief while maintaining consistency), and contraction (removal of a belief).\n\nThe first six postulates are called "the basic AGM postulates". In the settings considered by Alchourrón, Gärdenfors, and Makinson, the current set of beliefs is represented by a [[Deductive closure|deductively closed]] set of logical formulae <math>K</math> called belief base, the new piece of information is a logical formula <math>P</math>, and revision is performed by a binary operator <math>*</math> that takes as its operands the current beliefs and the new information and produces as a result a belief base representing the result of the revision. The <math>+</math> operator denoted expansion: <math>K+P</math> is the deductive closure of <math>K \\cup \\{P\\}</math>. The AGM postulates for revision are:\n\n# Closure: <math>K*P</math> is a belief base (i.e., a deductively closed set of formulae);\n# Success: <math>P \\in K*P</math>\n# Inclusion: <math>K*P \\subseteq K+P</math>\n# Vacuity: <math>\\text{If }(\\neg P) \\not \\in K,\\text{ then }K*P=K+P</math>\n# <math>K*P</math> is [[inconsistent]] only if <math>P</math> is inconsistent or <math>K</math> is inconsistent\n# Extensionality: <math>\\text{If }P\\text{ and }Q\\text{ are logically equivalent, then }K*P=K*Q</math> (see [[logical equivalence]])\n# <math>K*(P \\wedge Q) \\subseteq (K*P)+Q</math>\n# <math>\\text{If }(\\neg Q) \\not\\in K*P\\text{ then }(K*P)+Q \\subseteq K*(P \\wedge Q)</math>\n\nA revision operator that satisfies all eight postulates is the full meet revision, in which <math>K*P</math> is equal to <math>K+P</math> if consistent, and to the deductive closure of <math>P</math> otherwise. While satisfying all AGM postulates, this revision operator has been considered to be too conservative, in that no information from the old knowledge base is maintained if the revising formula is inconsistent with it.{{Citation needed|date=November 2011}}\n\n==Conditions equivalent to the AGM postulates==\n\nThe AGM postulates are equivalent to several different conditions on the revision operator; in particular, they are equivalent to the revision operator being definable in terms of structures known as selection functions, epistemic entrenchments, systems of spheres, and preference relations. The latter are [[reflexive relation|reflexive]], [[transitive relation|transitive]], and [[total relation]]s over the set of models.\n\nEach revision operator <math>*</math> satisfying the AGM postulates is associated to a set of preference relations <math>\\leq_K</math>, one for each possible belief base <math>K</math>, such that the models of <math>K</math> are exactly the minimal of all models according to <math>\\leq_K</math>. The revision operator and its associated family of orderings are related by the fact that <math>K*P</math> is the set of formulae whose set of models contains all the minimal models of <math>P</math> according to <math>\\leq_K</math>. This condition is equivalent to the set of models of <math>K*P</math> being exactly the set of the minimal models of <math>P</math> according to the ordering <math>\\leq_K</math>.\n\nA preference ordering <math>\\leq_K</math> represents an order of implausibility among all situations, including those that are conceivable but yet currently considered false. The minimal models according to such an ordering are exactly the models of the knowledge base, which are the models that are currently considered the most likely. All other models are greater than these ones, and are indeed considered less plausible. In general, <math>I <_K J</math> indicates that the situation represented by the model <math>I</math> is believed to be more plausible than the situation represented by <math>J</math>. As a result, revising by a formula having <math>I</math> and <math>J</math> as models should select only <math>I</math> to be a model of the revised knowledge base, as this model represent the most likely scenario among those supported by <math>P</math>.\n\n==Contraction==\n\nContraction is the operation of removing a belief <math>P</math> from a knowledge base <math>K</math>; the result of this operation is denoted by <math>K-P</math>. The operators of revision and contractions are related by the Levi and Harper identities:\n\n: <math>K*P=(K-\\neg P)+P</math>\n: <math>K-P=K \\cap (K*\\neg P)</math>\n\nEight postulates have been defined for contraction. Whenever a revision operator satisfies the eight postulates for revision, its corresponding contraction operator satisfies the eight postulates for contraction, and vice versa. If a contraction operator satisfies at least the first six postulates for contraction, translating it into a revision operator and then back into a contraction operator using the two identities above leads to the original contraction operator. The same holds starting from a revision operator.\n\nOne of the postulates for contraction has been longly discussed: the recovery postulate:\n\n: <math>K=(K-P)+P</math>\n\nAccording to this postulate, the removal of a belief <math>P</math> followed by the reintroduction of the same belief in the belief base should lead to the original belief base. There are some examples showing that such behavior is not always reasonable: in particular, the contraction by a general condition such as <math>a \\vee b</math> leads to the removal of more specific conditions such as <math>a</math> from the belief base; it is then unclear why the reintroduction of <math>a \\vee b</math> should also lead to the reintroduction of the more specific condition <math>a</math>. For example, if George was previously believed to have German citizenship, it was also believed to be European. Contracting this latter belief amounts to stop believing that George is European; therefore, that George has German citizenship is also retracted from the belief base. If George is later discovered to have Austrian citizenship, then the fact that he is European is also reintroduced. According to the recovery postulate, however, the belief that he also has German citizenship should also be reintroduced.\n\nThe correspondence between revision and contraction induced by the Levi and Harper identities is such that a contraction not satisfying the recovery postulate is translated into a revision satisfying all eight postulates, and that a revision satisfying all eight postulates is translated into a contraction satisfying all eight postulates, including recovery. As a result, if recovery is excluded from consideration, a number of contraction operators are translated into a single revision operator, which can be then translated back into exactly one contraction operator. This operator is the only one of the initial group of contraction operators that satisfies recovery; among this group, it is the operator that preserves as much information as possible.\n\n==The Ramsey test==\n<!--[[Ramsey test]], [[Ramsey Test]], [[Ramsey\'s test]], [[Ramsey\'s Test]] redirect here.-->\n\nThe evaluation of a [[counterfactual conditional]] <math>a > b</math> can be done, according to the \'\'\'Ramsey test\'\'\' (named for [[Frank P. Ramsey]]), to the hypothetical addition of <math>a</math> to the set of current beliefs followed by a check for the truth of <math>b</math>. If <math>K</math> is the set of beliefs currently held, the Ramsey test is formalized by the following correspondence:\n\n: <math>a > b</math> if and only if <math>b \\in K * a</math>\n\nIf the considered language of the formulae representing beliefs is propositional, the Ramsey test gives a consistent definition for counterfactual conditionals in terms of a belief revision operator. However, if the language of formulae representing beliefs itself includes the counterfactual conditional connective <math>></math>, the Ramsey test leads to the Gardenfors triviality result: there is no non-trivial revision operator that satisfies both the AGM postulates for revision and the condition of the Ramsey test. This result holds in the assumption that counterfactual formulae like <math>a>b</math> can be present in belief bases and revising formulae. Several solutions to this problem have been proposed.\n\n==Non-monotonic inference relation==\n\nGiven a fixed knowledge base <math>K</math> and a revision operator <math>*</math>, one can define a non-monotonic inference relation using the following definition: <math>P \\vdash Q</math> if and only if <math>K*P \\models Q</math>. In other words, a formula <math>P</math> [[logical consequence|entails]] another formula <math>Q</math> if the addition of the first formula to the current knowledge base leads to the derivation of <math>Q</math>. This inference relation is non-monotonic.\n\nThe AGM postulates can be translated into a set of postulates for this inference relation. Each of these postulates is entailed by some previously considered set of postulates for non-monotonic inference relations. Vice versa, conditions that have been considered for non-monotonic inference relations can be translated into postulates for a revision operator. All these postulates are entailed by the AGM postulates.\n\n==Foundational revision==\n\nIn the AGM framework, a belief set is represented by a deductively closed set of [[propositional formula]]e. While such sets are infinite, they can always be finitely representable. However, working with deductively closed sets of formulae leads to the implicit assumption that equivalent belief bases should be considered equal when revising. This is called the \'\'principle of irrelevance of syntax\'\'.\n\nThis principle has been and is currently debated: while <math>\\{a, b\\}</math> and <math>\\{a \\wedge b\\}</math> are two equivalent sets, revising by <math>\\neg a</math> should produce different results. In the first case, <math>a</math> and <math>b</math> are two separate beliefs; therefore, revising by <math>\\neg a</math> should not produce any effect on <math>b</math>, and the result of revision is <math>\\{\\neg a, b\\}</math>. In the second case, <math>a \\wedge b</math> is taken a single belief. The fact that <math>a</math> is false contradicts this belief, which should therefore be removed from the belief base. The result of revision is therefore <math>\\{\\neg a\\}</math> in this case.\n\nThe problem of using deductively closed knowledge bases is that no distinction is made between pieces of knowledge that are known by themselves and pieces of knowledge that are merely consequences of them. This distinction is instead done by the \'\'foundational\'\' approach to belief revision, which is related to [[foundationalism]] in philosophy. According to this approach, retracting a non-derived piece of knowledge should lead to retracting all its consequences that are not otherwise supported (by other non-derived pieces of knowledge).  This approach can be realized by using knowledge bases that are not deductively closed and assuming that all formulae in the knowledge base represent self-standing beliefs, that is, they are not derived beliefs. In order to distinguish the foundational approach to belief revision to that based on deductively closed knowledge bases, the latter is called the \'\'coherentist\'\' approach. This name has been chosen because the coherentist approach aims at restoring the coherence\n(consistency) among \'\'all\'\' beliefs, both self-standing and derived ones. This approach is related to [[coherentism]] in philosophy.\n\nFoundationalist revision operators working on non-deductively closed belief bases typically select some subsets of <math>K</math> that are consistent with <math>P</math>, combined them in some way, and then conjoined them with <math>P</math>. The following are two non-deductively closed base revision operators.\n\n; WIDTIO : (When in Doubt, Throw it Out) the maximal subsets of <math>K</math> that are consistent with <math>P</math> are intersected, and <math>P</math> is added to the resulting set; in other words, the result of revision is composed by <math>P</math> and of all formulae of <math>K</math> that are in all maximal subsets of <math>K</math> that are consistent with <math>P</math>;\n\n; Ginsberg-Fagin-Ullman-Vardi : the maximal subsets of <math>K \\cup \\{P\\}</math> that are consistent and contain <math>P</math> are combined by disjunction;\n\n; Nebel : similar to the above, but a priority among formulae can be given, so that formulae with higher priority are less likely to being retracted than formulae with lower priority.\n\nA different realization of the foundational approach to belief revision is based on explicitly declaring the dependences among beliefs. In the [[truth maintenance system]]s, dependence links among beliefs can be specified. In other worlds, one can explicitly declare that a given fact is believed because of one or more other facts; such a dependency is called a \'\'justification\'\'. Beliefs not having any justifications play the role of non-derived beliefs in the non-deductively closed knowledge base approach.\n\n==Model-based revision and update==\n\nA number of proposals for revision and update based on the set of models of the involved formulae were developed independently of the AGM framework. The principle behind this approach is that a knowledge base is equivalent to a set of \'\'possible worlds\'\', that is, to a set of scenarios that are considered possible according to that knowledge base. Revision can therefore be performed on the sets of possible worlds rather than on the corresponding knowledge bases.\n\nThe revision and update operators based on models are usually identified by the name of their authors: [[Marianne Winslett|Winslett]], Forbus, Satoh, Dalal, Hegner, and Weber. According to the  first four of these proposal, the result of revising/updating a formula <math>K</math> by another formula <math>P</math> is characterized by the set of models of <math>P</math> that are the closest to the models of <math>K</math>. Different notions of closeness can be defined, leading to the difference among these proposals.\n\n; Dalal : the models of <math>P</math> having a minimal [[Hamming distance]] to models of <math>K</math> are selected to be the models that result from the change;\n\n; Satoh : similar to Dalal, but distance between two models is defined as the set of literals that are given different values by them; similarity between models is defined as set containment of these differences;\n\n; Winslett : for each model of <math>K</math>, the closest models of <math>P</math> are selected; comparison is done using set containment of the difference;\n\n; Borgida : equal to Winslett\'s if <math>K</math> and <math>P</math> are inconsistent; otherwise, the result of revision is <math>K \\wedge P</math>;\n\n; [[Ken Forbus|Forbus]] : similar to Winslett, but the Hamming distance is used.\n\nThe revision operator defined by Hegner makes <math>K</math> not to affect  the value of the variables that are mentioned in <math>P</math>. What results from this operation is a formula <math>K\'</math> that is consistent with <math>P</math>, and can therefore be conjoined with it. The revision operator by Weber is similar, but the literals that are removed from <math>K</math> are not all literals of <math>P</math>, but only the literals that are evaluated differently by a pair of closest models of <math>K</math> and <math>P</math> according to the Satoh measure of closeness.\n\n==Iterated revision==\n\nThe AGM postulates are equivalent to a preference ordering (an ordering over models) to be associated to every knowledge base <math>K</math>. However, they do not relate the orderings corresponding to two non-equivalent knowledge bases. In particular, the orderings associated to a knowledge base <math>K</math> and its revised version <math>K*P</math> can be completely different. This is a problem for performing a second revision, as the ordering associated with <math>K*P</math> is necessary to calculate <math>K*P*Q</math>.\n\nEstablishing a relation between the ordering associated with <math>K</math> and <math>K*P</math> has been however recognized not to be the right solution to this problem. Indeed, the preference relation should depend on the previous history of revisions, rather than on the resulting knowledge base only. More generally, a preference relation gives more information about the state of mind of an agent than a simple knowledge base. Indeed, two states of mind might represent the same piece of knowledge <math>K</math> while at the same time being different in the way a new piece of knowledge would be incorporated. For example, two people might have the same idea as to where to go on holiday, but yet they differ on how they would change this idea if they win a million-dollar lottery. Since the basic condition of the preference ordering is that their minimal models are exactly the models of their associated knowledge base, a knowledge base can be considered implicitly represented by a preference ordering (but not vice versa).\n\nGiven that a preference ordering allows deriving its associated knowledge base but also allows performing a single step of revision, studies on iterated revision have been concentrated on how a preference ordering should be changed in response of a revision. While single-step revision is about how a knowledge base <math>K</math> has to be changed into a new knowledge base <math>K*P</math>, iterated revision is about how a preference ordering (representing both the current knowledge and how much situations believed to be false are considered possible) should be turned into a new preference relation when <math>P</math> is learned. A single step of iterated revision produces a new ordering that allows for further revisions.\n\nTwo kinds of preference ordering are usually considered: numerical and non-numerical. In the first case, the level of plausibility of a model is  representing by a non-negative integer number; the lower the rank, the more plausible the situation corresponding to the model. Non-numerical preference orderings correspond to the preference relations used in the AGM framework: a possibly total ordering over models. The non-numerical preference relation were initially considered unsuitable for iterated revision because of the impossibility of reverting a revision by a number of other revisions, which is instead possible in the numerical case.\n\nDarwiche and [[Judea Pearl|Pearl]]<ref name="darwiche-pearl">Darwiche, A. and Pearl, J. (1997) On the logic of iterated belief revision.  \'\'Artificial Intelligence\'\' \'\'\'89\'\'\'(1-2): 1-29.</ref> formulated the following postulates for iterated revision.\n\n# if <math>\\alpha \\models \\mu</math> then <math>(\\psi * \\mu) * \\alpha \\equiv \\psi * \\alpha</math>;\n# if <math>\\alpha \\models \\neg \\mu</math>, then <math>(\\psi * \\mu) * \\alpha \\equiv \\psi * \\alpha</math>;\n# if <math>\\psi * \\alpha \\models \\mu</math>, then <math>(\\psi * \\mu) * \\alpha \\models \\mu</math>;\n# if <math>\\psi * \\alpha \\not\\models \\neg \\mu</math>, then <math>(\\psi * \\mu) * \\alpha \\not\\models \\neg \\mu</math>.\n\nSpecific iterated revision operators have been proposed by Spohn, Boutilier, Williams, Lehmann, and others.\n\n; Spohn rejected revision : this non-numerical proposal has been first considered by Spohn, who rejected it based on the fact that revisions can change some orderings in such a way the original ordering cannot be restored with a sequence of other revisions; this operator change a preference ordering in view of new information <math>P</math> by making all models of <math>P</math> being preferred over all other models; the original preference ordering is maintained when comparing two models that are both models of <math>P</math> or both non-models of <math>P</math>;\n\n; Natural revision : while revising a preference ordering by a formula <math>P</math>, all minimal models (according to the preference ordering) of <math>P</math> are made more preferred by all other ones; the original ordering of models is preserved when comparing two models that are not minimal models of <math>P</math>; this operator changes the ordering among models minimally while preserving the property that the models of the knowledge base after revising by <math>P</math> are the minimal models of <math>P</math> according to the preference ordering;\n\n; Transmutations : these are two forms of revision, conditionalization and adjustment, which work on numerical preference orderings; revision requires not only a formula but also a number indicating its degree of plausibility; while the preference ordering is still inverted (the lower a model, the most plausible it is) the degree of plausibility of a revising formula is direct (the higher the degree, the most believed the formula is);\n\n; Ranked revision : a ranked model, which is an assignment of non-negative integers to models, has to be specified at the beginning; this rank is similar to a preference ordering, but is not changed by revision; what is changed by a sequence of revisions are a current set of models (representing the current knowledge base) and a number called the rank of the sequence; since this number can only monotonically non-decrease, some sequences of revision lead to situations in which every further revision is performed as a full meet revision.\n\n==Merging==\n\nThe assumption implicit in the revision operator is that the new piece of information <math>P</math> is always to be considered more reliable than the old knowledge base <math>K</math>. This is formalized by the second of the AGM postulates: <math>P</math> is always believed after revising <math>K</math> with <math>P</math>. More generally, one can consider the process of merging several pieces of information (rather than just two) that might or might not have the same reliability. Revision becomes the particular instance of this process when a less reliable piece of information <math>K</math> is merged with a more reliable <math>P</math>.\n\nWhile the input to the revision process is a pair of formulae <math>K</math> and <math>P</math>, the input to merging is a [[multiset]] of formulae <math>K</math>, <math>T</math>, etc. The use of multisets is necessary as two sources to the merging process might be identical.\n\nWhen merging a number of knowledge bases with the same degree of plausibility, a distinction is made between arbitration and majority. This distinction depends on the assumption that is made about the information and how it has to be put together.\n\n; arbitration : the result of arbitrating two knowledge bases <math>K</math> and <math>T</math> entails <math>K \\vee T</math>; this condition formalizes the assumption of maintaining as much as the old information as possible, as it is equivalent to imposing that every formula entailed by both knowledge bases is also entailed by the result of their arbitration; in a possible world view, the "real" world is assumed one of the worlds considered possible according to at least one of the two knowledge bases;\n\n; majority : the result of merging a knowledge base <math>K</math> with other knowledge bases can be forced to entail <math>K</math> by adding a sufficient number of other knowledge bases equivalent to <math>K</math>; this condition corresponds to a kind of vote-by-majority: a sufficiently large number of knowledge bases can always overcome the "opinion" of any other fixed set of knowledge bases.\n\nThe above is the original definition of arbitration. According to a newer definition, an arbitration operator is a merging operator that is insensitive to the number of equivalent knowledge bases to merge. This definition makes arbitration the exact opposite of majority.\n\nPostulates for both arbitration and merging have been proposed. An example of an arbitration operator satisfying all postulates is the classical disjunction. An example of a majority operator satisfying all postulates is that selecting all models that have a minimal total Hamming distance to models of the knowledge bases to merge.\n\nA merging operator can be expressed as a family of orderings over models, one for each possible multiset of knowledge bases to merge: the models of the result of merging a multiset of knowledge bases are the minimal models of the ordering associated to the multiset. A merging operator defined in this way satisfies the postulates for merging if and only if the family of orderings meets a given set of conditions. For the old definition of arbitration, the orderings are not on models but on pairs (or, in general, tuples) of models.\n\n==Social choice theory==\n\nMany revision proposals involve orderings over models representing the relative plausibility of the possible alternatives. The problem of merging amounts to combine a set of orderings into a single one expressing the combined  plausibility of the alternatives. This is similar with what is done in [[social choice theory]], which is the study of how the preferences of a group of agents can be combined in a rational way. Belief revision and social choice theory are similar in that they combine a set of orderings into one. They differ on how these orderings are interpreted: preferences in social choice theory; plausibility in belief revision. Another difference is that the alternatives are explicitly enumerated in social choice theory, while they are the propositional models over a given alphabet in belief revision.\n\n==Complexity==\n\nThe problem about belief revision that is the most studied from the point of view of [[Computational complexity theory|computational complexity]] is that of query answering in the propositional case. This is the problem of establishing whether a formula follows from the result of a revision, that is, <math>K*P \\models Q</math>, where <math>K</math>, <math>P</math>, and <math>Q</math> are propositional formulae. More generally, query answering is the problem of telling whether a formula is entailed by the result of a belief revision, which could be update, merging, revision, iterated revision, etc. Another problem that has received some attention is that of model checking, that is, checking whether a model satisfies the result of a belief revision. A related question is whether such result can be represented in space polynomial in that of its arguments.\n\nSince a deductively closed knowledge base is infinite, complexity studies on belief revision operators working on deductively closed knowledge bases are done in the assumption that such deductively closed knowledge base are given in the form of an equivalent finite knowledge base.\n\nA distinction is made among belief revision operators and belief revision schemes. While the former are simple mathematical operators mapping a pair of formulae into another formula, the latter depend on further information such as a preference relation. For example, the Dalal revision is an operator because, once two formulae <math>K</math> and <math>P</math> are given, no other information is needed to compute <math>K*P</math>. On the other hand, revision based on a preference relation is a revision scheme, because <math>K</math> and <math>P</math> do not allow determining the result of revision if the family of preference orderings between models is not given. The complexity for revision schemes is determined in the assumption that the extra information needed to compute revision is given in some compact form. For example, a preference relation can be represented by a sequence of formulae whose models are increasingly preferred. Explicitly storing the relation as a set of pairs of models is instead not a compact representation of preference because the space required is exponential in the number of propositional letters.\n\nThe complexity of query answering and model checking in the propositional case is in the second level of the [[polynomial hierarchy]] for most belief revision operators and schemas. Most revision operators suffer from the problem of representational blow up: the result of revising two formulae is not necessarily representable in space polynomial in that of the two original formulae. In other words, revision may exponentially increase the size of the knowledge base.\n\n==Implementations==\n\nSystems specifically implementing belief revision are: [http://portal.acm.org/citation.cfm?id=122296.122301 Immortal], [https://web.archive.org/web/20051018054730/http://magic.it.uts.edu.au:80/systems/saten.html SATEN], and [http://www.dis.uniroma1.it/~liberato/brels/brels.html BReLS]. Two systems including a belief revision feature are [http://www.cse.buffalo.edu/sneps/ SNePS] and [[Cyc]]. [[Truth maintenance systems]] are used in [[Artificial Intelligence]] to implement belief revision.\n\n==See also==\n\n* [[Artificial intelligence]]\n* [[Inquiry]]\n* [[Knowledge representation]]\n* [[Belief propagation]]\n* [[Reason maintenance]]\n* [[Epistemic closure]]\n* [[Non-monotonic logic]]\n* [[Defeasible reasoning]]\n* [[Reasoning]]\n* [[Philosophy of science]]\n* [[Discursive dilemma]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* C. E. Alchourròn, P. Gärdenfors, and D. Makinson (1985). On the logic of theory change: Partial meet contraction and revision functions. \'\'Journal of Symbolic Logic\'\', 50:510–530.\n* C. Boutilier (1993). Revision sequences and nested conditionals. In \'\'Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI\'93)\'\', pages 519–525.\n* C. Boutilier (1995). Generalized update: belief change in dynamic settings. In \'\'Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI\'95)\'\', pages 1550–1556.\n* C. Boutilier (1996). Abduction to plausible causes: an event-based model of belief update. \'\'Artificial Intelligence\'\', 83:143–166.\n* M. Cadoli, F. M. Donini, P. Liberatore, and M. Schaerf (1999). The size of a revised knowledge base. \'\'Artificial Intelligence\'\', 115(1):25–64.\n* T. Chou and [[Marianne Winslett|M. Winslett]] (1991). Immortal: A model-based belief revision system. In \'\'Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR\'91)\'\', pages 99–110. Morgan Kaufmann Publishers.\n* M. Dalal (1988). Investigations into a theory of knowledge base revision: Preliminary report. In \'\'Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI\'88)\'\', pages 475–479.\n* T. Eiter and G. Gottlob (1992). On the complexity of propositional knowledge base revision, updates and counterfactuals. \'\'Artificial Intelligence\'\', 57:227–270.\n* T. Eiter and G. Gottlob (1996). The complexity of nested counterfactuals and iterated knowledge base revisions. \'\'Journal of Computer and System Sciences\'\', 53(3):497–512.\n* R. Fagin, J. D. Ullman, and M. Y. Vardi (1983). On the semantics of updates in databases. In \'\'Proceedings of the Second ACM SIGACT SIGMOD Symposium on Principles of Database Systems (PODS\'83)\'\', pages 352–365.\n* M. A. Falappa, G. Kern-Isberner, G. R. Simari (2002): Explanations, belief revision and defeasible reasoning. \'\'Artificial Intelligence\'\', 141(1–2): 1–28.\n* M. Freund and D. Lehmann (2002). Belief Revision and Rational Inference. [http://arxiv.org/abs/cs.AI/0204032 Arxiv preprint cs.AI/0204032].\n* N. Friedman and J. Y. Halpern (1994). A knowledge-based framework for belief change, part II: Revision and update. In \'\'Proceedings of the Fourth International Conference on the Principles of Knowledge Representation and Reasoning (KR\'94)\'\', pages 190–200.\n* A. Fuhrmann (1991). Theory contraction through base contraction. \'\'Journal of Philosophical Logic\'\', 20:175–203.\n* D. Gabbay, G. Pigozzi, and J. Woods (2003). Controlled Revision&nbsp;– An algorithmic approach for belief revision, \'\'Journal of Logic and Computation\'\', 13(1): 15–35.\n* P. Gärdenfors and D. Makinson (1988). Revision of knowledge systems using epistemic entrenchment. In \'\'Proceedings of the Second Conference on Theoretical Aspects of Reasoning about Knowledge (TARK\'88)\'\', pages 83–95.\n* P. Gärdenfors and H. Rott (1995). Belief revision. In \'\'Handbook of Logic in Artificial Intelligence and Logic Programming, Volume 4\'\', pages 35–132. Oxford University Press.\n* G. Grahne and [[Alberto O. Mendelzon]] (1995). Updates and subjunctive queries. \'\'Information and Computation\'\', 2(116):241–252.\n* G. Grahne, [[Alberto O. Mendelzon]], and P. Revesz (1992). Knowledge transformations. In \'\'Proceedings of the Eleventh ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS\'92)\'\', pages 246–260.\n* S. O. Hansson (1999). \'\'A Textbook of Belief Dynamics\'\'. Dordrecht: Kluwer Academic Publishers.\n* A. Herzig (1996). The PMA revised. In \'\'Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning (KR\'96)\'\', pages 40–50.\n* A. Herzig (1998). Logics for belief base updating. In D. Dubois, D. Gabbay, H. Prade, and P. Smets, editors, \'\'Handbook of defeasible reasoning and uncertainty management\'\', volume 3 – Belief Change, pages 189–231. Kluwer Academic Publishers.\n* H. Katsuno and A. O. Mendelzon (1991). On the difference between updating a knowledge base and revising it. In \'\'Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR\'91)\'\', pages 387–394.\n* H. Katsuno and A. O. Mendelzon (1991). Propositional knowledge base revision and minimal change. \'\'Artificial Intelligence\'\', 52:263–294.\n* S. Konieczny and R. Pino Perez (1998). On the logic of merging. In \'\'Proceedings of the Sixth International Conference on Principles of Knowledge Representation and Reasoning (KR\'98)\'\', pages 488–498.\n* D. Lehmann (1995). Belief revision, revised. In \'\'Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI\'95)\'\', pages 1534–1540.\n* P. Liberatore (1997). The complexity of iterated belief revision. In \'\'Proceedings of the Sixth International Conference on Database Theory (ICDT\'97)\'\', pages 276–290.\n* P. Liberatore and M. Schaerf (1998). Arbitration (or how to merge knowledge bases). \'\'IEEE Transactions on Knowledge and Data Engineering\'\', 10(1):76–90.\n* P. Liberatore and M. Schaerf (2000). BReLS: A system for the integration of knowledge bases. In \'\'Proceedings of the Seventh International Conference on Principles of Knowledge Representation and Reasoning (KR 2000)\'\', pages 145–152.\n* D. Makinson (1985). How to give up: A survey of some formal aspects of the logic of theory change. \'\'Synthese\'\', 62:347–363.\n* A. Perea (2003). \'\'Proper Rationalizability and Belief Revision in Dynamic Games\'\'. Research Memoranda 048: METEOR, Maastricht Research School of Economics of Technology and Organization.\n* B. Nebel (1991). Belief revision and default reasoning: Syntax-based approaches. In \'\'Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR\'91)\'\', pages 417–428.\n* B. Nebel (1994). Base revision operations and schemes: Semantics, representation and complexity. In \'\'Proceedings of the Eleventh European Conference on Artificial Intelligence (ECAI\'94)\'\', pages 341–345.\n* B. Nebel (1996). How hard is it to revise a knowledge base? Technical Report 83, Albert-Ludwigs-Universität Freiburg, Institut für Informatik.\n* G. Pigozzi (2005). Two aggregation paradoxes in social decision making: the Ostrogorski paradox and the [[discursive dilemma]], \'\'Episteme: A Journal of Social Epistemology\'\', 2(2): 33–42.\n* G. Pigozzi (2006). [http://pigozzi.org/Pigozzi_Judgment_Aggregation.pdf Belief merging and the discursive dilemma: an argument-based account to paradoxes of judgment aggregation]. \'\'Synthese\'\' 152(2): 285–298.\n* P. Z. Revesz (1993). On the semantics of theory change: Arbitration between old and new information. In \'\'Proceedings of the Twelfth ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS\'93)\'\', pages 71–82.\n* K. Satoh (1988). Nonmonotonic reasoning by minimal belief revision. In \'\'Proceedings of the International Conference on Fifth Generation Computer Systems (FGCS\'88)\'\', pages 455–462.\n* {{cite book | last1=Shoham | first1=Yoav | last2=Leyton-Brown | first2=Kevin | title=Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations | publisher=[[Cambridge University Press]] | isbn=978-0-521-89943-7 | url=http://www.masfoundations.org | year=2009 | location=New York}} See Section 14.2; [http://www.masfoundations.org/download.html downloadable free online].\n* V. S. Subrahmanian (1994). Amalgamating knowledge bases. \'\'ACM Transactions on Database Systems\'\', 19(2):291–331.\n* A. Weber (1986). Updating propositional formulas. In \'\'Proc. of First Conf. on Expert Database Systems\'\', pages 487–500.\n* M. Williams (1994). Transmutations of knowledge systems. In \'\'Proceedings of the Fourth International Conference on the Principles of Knowledge Representation and Reasoning (KR\'94)\'\', pages 619–629.\n* M. Winslett (1989). Sometimes updates are circumscription. In \'\'Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI\'89)\'\', pages 859–863.\n* M. Winslett (1990). \'\'Updating Logical Databases\'\'. Cambridge University Press.\n* Y. Zhang and N. Foo (1996). Updating knowledge bases with disjunctive information. In \'\'Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI\'96)\'\', pages 562–568.\n\n==External links==\n* {{PhilPapers|category|belief-revision}}\n* {{InPho|idea|1448|Logic of Belief Revision}}\n* {{cite SEP |url-id=logic-belief-revision |title=Logic of Belief Revision}}\n* [http://www.beliefrevision.org/ Beliefrevision.org]\n* [http://plato.stanford.edu/entries/reasoning-defeasible/#4.3 Defeasible Reasoning: 4.3 Belief Revision Theory] at [[Stanford Encyclopedia of Philosophy]]\n\n[[Category:Belief revision| ]]\n[[Category:Belief]]\n[[Category:Formal epistemology]]\n[[Category:Knowledge representation]]\n[[Category:Logic]]\n[[Category:Logic programming]]']
['SERVQUAL', '2755912', '{{marketing}}\n\n\'\'\'SERVQUAL\'\'\' is a multi-dimensional research instrument, designed to capture consumer expectations and perceptions of a service along the five dimensions that are believed to represent service quality. SERVQUAL is built on the expectancy-disconfirmation paradigm, which in simple terms means that service quality is understood as the extent to which consumers\' pre-consumption expectations of quality are confirmed or disconfirmed by their actual perceptions of the service experience. When the SERVQUAL questionnaire was first published in 1988 by a team of academic researchers, A. Parasurman, [[Valarie Zeithaml]] and [[Leonard Berry (professor)|Leonard L. Berry]]  to measure quality in the service sector,<ref>Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality\' \'\'Journal of Retailing,\'\' Vo. 62,  no. 1, 1988, pp 12-40 <online:https://www.researchgate.net/publication/225083802_SERVQUAL_A_multiple-_Item_Scale_for_measuring_consumer_perceptions_of_service_quality></ref> it represented a breakthrough in the measurement methods used for service quality research. The diagnostic value of the instrument is supported by the \'\'model of service quality\'\' which forms the conceptual framework for the development of the scale (i.e. instrument or questionnaire). The instrument has been widely applied in a variety of contexts and cultural settings and found to be relatively robust. It has become the dominant measurement scale in the area of service quality. In spite of the long-standing interest in SERVQUAL and its myriad of context-specific applications, it has attracted some criticism from researchers.\n\n==The SERVQUAL instrument==\n\n[[File:Measuring service quality using SERVQUAL model (Kumar et al, 2009).png|thumb|The five dimensions of service quality)]]\n\nSERVQUAL is a multidimensional instrument (i.e. questionnaire or measurement scale) designed to measure service quality by capturing respondents’ expectations and perceptions along the five dimensions of service quality.<ref>Parasuraman, A., Berry, L.L. and Zeithaml, V.A.,  “Refinement and Reassessment of the SERVQUAL scale,” \'\'Journal of Retailing,\'\' Vol. 67, no. 4, 1991, pp 57-67</ref>  The questionnaire consists of matched pairs of items; 22 expectation items and 22 perceptions items, organised into five dimensions which are believed to  align with the consumer’s mental map of service quality dimensions. Both the expectations component and the perceptions component of the questionnaire consist a total of 22 items, comprising 4 items to capture tangibles, 5 items to capture reliabiility, 4 items for responsiveness, 5 items for assurance and 5 items to capture empathy.<ref>Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality\' \'\'Journal of Retailing,\'\' Vol. 62,  no. 1, 1988, p. 25</ref> The questionnaire is designed to be administered in a face-to-face interview and requires a moderate to large size sample for statistical reliability. In practice, it is customary to add additional items such as the respondent\'s demographics, prior experience with the brand or category and behavioural intentions (intention to revisit/ repurchase, loyalty intentions and propensity to give word-of-mouth referrals). Thus, the final questionnaire may have up to 60 items and typically takes at least one hour, per respondent, to administer. The length of the questionnaire combined with sample size requirements contribute to substantial costs in administration and data analysis.\n\n\n{| class="wikitable"\n|+ Summary of SERVQUAL items <ref>Based on Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality\' \'\'Journal of Retailing,\'\' Vol. 62,  no. 1, 1988, p. 22, 25 and 29</ref>\n|-\n! Dimension\n! No. of Items in Questionnaire\n! Definition\n|-\n| \'\'\'Reliability\'\'\'\n| colspan="1" style="text-align: center;" | 5 \n| The ability to perform the promised service dependably and accurately \n|-\n| \'\'\'Assurance\'\'\'\n| colspan="1" style="text-align: center;" | 5 \n| The knowledge and courtesy of  employees and their ability to convey trust and confidence\n|-\n| \'\'\'Tangibles\'\'\'\n| colspan="1" style="text-align: center;" | 4 \n| The appearance of physical facilities, equipment, personnel and communication materials \n|-\n| \'\'\'Empathy\'\'\'\n| colspan="1" style="text-align: center;" | 5 \n| The provision of caring, individualized attention to customer\n|-\n| \'\'\'Responsiveness\'\'\'\n| colspan="1" style="text-align: center;" | 4 \n| The willingness to help customers and to provide prompt service\n|}\n\nThe instrument was developed over a five year period; was tested, pre-tested and refined before appearing in its final form. The instrument\'s developers, claim that it is a highly reliable and valid instrument.<ref>Zeithaml, V., Parasuraman, A. and Berry, L.L., \'\'Delivering Service Quality: Balancing Customer Perceptions and Expectations,\'\' N.Y., The Free Press, 1990</ref> Certainly, it has been widely used and adapted in service quality research for numerous industries and various geographic regions. In application, many researchers are forced to make minor modifications to the instrument as necessary for context-specific applications. Some researchers label their revised instruments with innovative labels such as EDUQUAL (educational context),<ref>Mahapatra, S.S. and Khan, M.S., "A Methodology for Evalution of Service Quality Using Neural Networks," in \'\'Proceedings of the International Conference on Global Manufacturing and Innovation,\' July 27–29, 2006</ref> HEALTHQUAL (hospital context) <ref>Lee, D., "HEALTHQUAL: a multi-item scale for assessing healthcare service quality," Service Business, 2016; pp 1-26, doi:10.1007/s11628-016-0317-2</ref> and ARTSQUAL (art museum).<ref>Higgs, B., Polonsky M.J. and Hollick, M., “Measuring Expectations: Pre and Post Consumption: Does It Matter?” \'\'Journal of Retailing and Consumer Services\'\', vol. 12, no. 1, 2005</ref>\n\n\n{| class="wikitable"\n|+ Examples of matched pairs of items in the SERVQUAL questionnaire <ref>Based on Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality\' \'\'Journal of Retailing,\'\' Vol. 62,  no. 1, 1988, [Appendix: SERVQUAL questionnaire, pp 37-40</ref>\n|-\n! Dimension\n! Sample expectations item\n! Sample perceptions item\n|-\n| \'\'\'Reliability\'\'\'\n| When excellent telephone companies promise to do something by a certain time, they do so\n| XYZ company provides it services at the promised time \n|-\n| \'\'\'Assurance\'\'\'\n| The behaviour of employees in excellent banks will instill confidence in customers\n| The behaviour of employees in the XYZ bank instils confidence in you.\n|-\n| \'\'\'Tangibles\'\'\'\n| Excellent telephone companies will have modern looking equipment\n| XYZ company has modern looking equipment\n|-\n| \'\'\'Empathy\'\'\'\n| Excellent banks will have operating hours convenient to customers\n| XYZ bank has convenient operating hours\n|-\n| \'\'\'Responsiveness\'\'\'\n| Employees of excellent telephone companies will never be too busy to help a customer\n| XYZ employees are never too busy to help you\n|}\n\nThe SERVQUAL questionnaire has been described as "the most popular standardized questionnaire to measure service quality." <ref>Caruanaa,A., Ewing, M.T and Ramaseshanc, B., "Assessment of the Three-Column Format SERVQUAL: An Experimental Approach," \'\'Journal of Business Research,\'\' Vol. 49, no. 1, July 2000, pp 57–65</ref> It is widely used by service firms, most often in conjunction with other measures of service quality and customer satisfaction. The SERVQUAL instrument was developed as part of a broader conceptualisation of how customers understand service quality. This conceptualisation is known as the \'\'model of service quality\'\' or more popularly as the \'\'gaps model.\'\'\n\n==The model of service quality==\n\nThe model of service quality, popularly known as the \'\'gaps model\'\' was developed by a group of American authors, A. Parasuraman, [[Valarie Zeithaml|Valarie A. Zeithaml]] and [[Leonard Berry (professor)|Len Berry]], in a systematic research program carrie out between 1983 and 1988. The model identifies the principal dimensions (or components) of service quality; proposes a scale for measuring service quality (SERVQUAL) and suggests possible causes of service quality problems. The model\'s developers originally identified [[SERVQUAL#Determinants|ten dimensions of service quality]], but after testing and retesting, some of the dimensions were found to be autocorrelated and the total number of dimensions was reduced to five, namely - reliability, assurance, tangibles, empathy and responsiveness.  These five dimensions are thought to represent the dimensions of service quality across a range of industries and settings. <ref>Zeithaml, V.A., Berry, L.L. and Parasuraman, A., "Communication and Control Processes in the Delivery of Service Quality," \'\'Journal of Marketing,\'\' Vol. 52, No. 2, 1988, pp. 35-48 </ref> Among students of marketing, the memnonic,  \'\'\'RATER\'\'\', an acronym formed from the first letter of each of the five dimensions is often used as an aid to recall.\n\n[[File:Servqual.jpg|thumb|A simplified model of service quality]]\n\nBusinesses use the SERVQUAL instrument (i.e. questionnaire) to measure potential service quality problems and the model of service quality to help diagnose possible causes of the problem. The model of service quality is built on the \'\'expectancy-confirmation paradigm\'\' which suggests that consumers perceive quality in terms of their perceptions of how well a given service delivery meets their expectations of that delivery.<ref>Oliver, R.L., Balakrishnan, P.V. S. and Barry, B., "Outcome Satisfaction in Negotiation: A Test of Expectancy Disconfirmation," \'\'Organizational Behavior and Human Decision Processes,\'\' Vol. 60, no. 2, 1994, Pages 252-275</ref> Thus, service quality can be conceptualised as a simple equation:\n\n\'\'\'SQ = P- E\'\'\'\n: where;\n\n: \'\'\'SQ\'\'\' is service quality\n: \'\'\'P\'\'\' is the individual\'s perceptions of given service delivery\n: \'\'\'E\'\'\' is the individual\'s expectations of a given service delivery\n\nWhen customer expectations are greater than their perceptions of received delivery, service quality is deemed low. When perceptions exceed expectations then service quality is high. The model of service quality identifies five gaps that may cause customers to experience poor service quality. In this model, gap 5 is the service quality gap and is the \'\'only\'\' gap that can be directly measured. In other words, the SERVQUAL instrument was specifically designed to capture gap 5. In contrast, Gaps 1-4 cannot be measured, but have diagnostic value.\n\n\n{| class="wikitable"\n|+ Summary of Gaps with Diagnostic Indications <ref>Based on Zeithaml, V.A., Berry, L.L. and Parasuraman, A., "Communication and Control Processes in the Delivery of Service Quality," \'\'Journal of Marketing,\'\' Vol. 52, No. 2, 1988, pp. 35-48 </ref>\n|-\n! \'\'\'Gap\'\'\'\n! \'\'\'Brief description\'\'\'\n! \'\'\'Probable Causes\n|-\n| \'\'\'Gap 1\'\'\'\nThe Knowledge Gap\n| Difference between the target market’s expected service and management’s perceptions of the target market’s expected service\n| \n* Insufficient marketing research\n* Inadequate upward communications\n* Too many layers of management\n|-\n| \'\'\'Gap 2\'\'\'\nThe standards Gap \n| Difference between management’s perceptions of customer expectations and the translation into service procedures and specifications\n| \n* Lack of management commitment to service quality\n* Employee perceptions of infeasibility\n* Inadequate goal setting\n* Inadequate task standardisation\n|-\n|\'\'\'Gap 3\'\'\' \nThe Delivery Gap\n| Difference between service quality specifications and the service actually delivered\n| \n* Technical breakdowns or malfunctions\n*  Role conflict/ ambiguity\n*  Lack of perceived control\n* Poor employee-job fit\n* Poor technology- fit\n* Poor supervision or training\n|-\n| \'\'\'Gap 4\'\'\' \nThe Communications Gap\n| Difference between service delivery intentions and what is communicated to the customer\n| \n*Lack of horizontal communications\n* Poor communication with advertising agency\n* Inadequate communications between sales and operations\n* Differences in policies and procedures across branches or divisions of an entity\n* Propensity to overpromise\n|}\n\n== Development of the model ==\n\nThe development of the model of service quality involved a systematic research undertaking which began in 1983 and after various refinements resulted in the publication of the SERVQUAL instrument in 1988.<ref>Parasuraman, A., Berry, L.L.,  Zeithaml, V. A., "Understanding Customer Expectations of Service," \'\'Sloan Management Review,\'\' Vol. 32, no. 3, 1991, p. 39</ref> The model\'s developers began with an exhaustive literature search in order to identify items that were believed to impact on perceived service quality. This initial search identified some 100 items which were used in the first rounds of consumer testing. Prelimiary data analysis, using a data reduction technique known as [[factor analysis]] (also known as [[principal components analysis]]) revealed that these items loaded onto ten dimensions (or components) of service quality. The initial ten dimensions that were believed to represent service quality were:\n\n# \'\'\'[[competence (human resources)|Competence]]\'\'\' is the possession of the required skills and knowledge to perform the service. For example, there may be competence in the knowledge and skill of contact personnel, knowledge and skill of operational support personnel and research capabilities of the organization.\n# \'\'\'Courtesy\'\'\' is the consideration for the customer\'s property and a clean and neat appearance of contact personnel, manifesting as politeness, respect, and friendliness.\n# \'\'\'Credibility\'\'\' includes factors such as trustworthiness, belief and honesty. It involves having the customer\'s best interests at prime position. It may be influenced by company name, company reputation and the personal characteristics of the contact personnel.\n# \'\'\'Security\'\'\' enables the customer to feel free from danger, risk or doubt including physical safety, financial security and confidentiality.\n# \'\'\'Access\'\'\' is approachability and ease of contact. For example, convenient office operation hours and locations.\n# \'\'\'Communication\'\'\' means both informing customers in a language they are able to understand and also listening to customers. A company may need to adjust its language for the varying needs of its customers. Information might include for example, explanation of the service and its cost, the relationship between services and costs and assurances as to the way any problems are effectively managed.\n# \'\'\'Knowing the customer\'\'\' means making an effort to understand the customer\'s individual needs, providing individualized attention, recognizing the customer when they arrive and so on. This in turn helps to delight the customers by rising above their expectations. \n# \'\'\'Tangibles\'\'\' are the physical evidence of the service, for instance, the  appearance of the physical facilities, tools and equipment used to provide the service; the appearance of personnel and communication materials and the presence of other customers in the service facility.\n# \'\'\'Reliability\'\'\'  is the ability to perform the promised service in a dependable and accurate manner. The service is performed correctly on the first occasion, the accounting is correct, records are up to date and schedules are kept.\n# \'\'\'Responsiveness\'\'\' is the readiness and willingness of employees to help customers by providing prompt timely services, for example, mailing a transaction slip immediately or setting up appointments quickly.\n\nFurther testing suggested that some of the ten preliminary dimensions of service quality were closely related or autocorrelated. Thus the ten initial dimensions were reduced and the labels amended to accurately reflect the revised dimensions. By the early 1990s, the authors had refined the model to five factors which in testing, appear to be relatively stable and robust.\n\n# \'\'\'Reliability:\'\'\' the ability to perform the promised service dependably and accurately\n# \'\'\'Assurance:\'\'\' the knowledge and courtesy of employees and their ability to convey trust and confidence\n# \'\'\'Tangibles:\'\'\' the appearance of physical facilities, equipment, personnel and communication materials\n# \'\'\'Empathy:\'\'\' the provision of caring, individualized attention to customers\n# \'\'\'Responsiveness:\'\'\' the willingness to help customers and to provide prompt service\n\nThese are the five dimensions of service quality that form the basis of the individual items in the SERVQUAL research instrument (questionnaire). The acronym RATER, is often used to help students of marketing remember the five dimensions of quality explicitly mentioned in the research instrument.\n\nNyeck, Morales, Ladhari, and Pons (2002) stated the SERVQUAL measuring tool “appears to remain the most complete attempt to conceptualize and measure service quality” (p.&nbsp;101). The SERVQUAL measuring tool has been used by many researchers across a wide range of service industries and contexts, such as healthcare, banking, financial services, and education (Nyeck, Morales, Ladhari, & Pons, 2002).\n\n== Criticisms of SERVQUAL and the model of service quality ==\n\nAlthough the SERVQUAL instrument has been widely applied in a variety of industry and cross-cultural contexts, there are many criticisms of the approach. Francis Buttle published one of the most comprehensive criticisms of the model of service quality and the associated SERVQUAL instrument in 1996 in which both operational and theoretical concerns were identified.<ref>Buttle, F., “SERVQUAL: Review, Critique, Research Agenda,"  \'\'European Journal of Marketing,\'\' Vol. 30, no.  1, pp. 8-32 1996</ref> Some of the more important criticisms include:\n\n: \'\'Face validity\'\': The model of service quality has its roots in the expectancy-disconfimation paradigm that informs customer satisfaction.<ref>Oliver, R.L., \'\'Satisfaction: A Behavioural Perspective on the Consumer,\'\' Boston, MA, Irwin McGraw-Hill, 1996</ref> A number of researchers have argued that the research instrument actually captures \'\'satisfaction\'\' rather than \'\'service quality\'\'.<ref>Souca, Ma. L., "SERVQUAL - Thirty years of research on service quality with implications for customer satisfaction," in \'\'Marketing - from Information to Decision,\'\' [Proceedings of the International Conference], Cluj-Napoca: Babes Bolyai University, 2011, pp 420 -429</ref> Other researchers have questioned the validity of conceptualising service quality as a gap.<ref>van Dyke, T.P.,   Kappelman, L.A. and Prybutok, V.R.,"Measuring Information Systems Service Quality: Concerns on the Use of the SERVQUAL Questionnaire," \'\'MIS Quarterly,\'\' Vol. 21, No. 2, 1997, pp. 195-208, <Online:  http://www.jstor.org/stable/249419></ref>\n\n: \'\'Construct validity\'\': The model\'s developers tested and retested the SERVQUAL scale for reliability and validity. However, at the same time the model\'s developers recommended that applied use of the instrument should modify or adapt the for specific contexts. Any attempt to adapt or modify the scale will have implications for the validity of items with implications for the validity of the dimensions of reliability, assurance, tangibles, empathy and repsonsiveness.<ref>Smith, A.M., "Measuring Service Quality: Is SERVQUAL now redundant?  \'\'Journal of Marketing Management,\'\' [Special Issue: Marketing in the Services Sector],  Vol 11, no. 1, 1995, pp 257-276</ref>\n \n: \'\'Ambiguity of expectations construct\'\': SERVQUAL is designed to be administered after respondents have experienced a service. They are therefore asked to \'\'recall\'\' their pre-experience expectations. However,  recall is not always accurate, raising concerns about whether the research design accurately captures true pre-consumption expectations. In addition, studies show that expectations actually change over time. Consumers are continually modifying their expectations as they gain experience with a product category or brand.<ref>Parasuraman, A.; Berry, Leonard L.; Zeithaml, Valarie A., "Understanding Customer Expectations of Service," \'\'Sloan Management Review,\'\' Vol. 32, no. 3, 1991, pp 39 - 48</ref> In light of these insights, concerns have been raised about whether the act of experiencing the service might colour respondents\' expectations.\n\n: The way that expectations has been operationalised also represents a concern for theorists investigating the validity of the gaps model. The literature identifies different types of expectations.<ref>Parasuraman, A., Zeithaml, V. A., Berry, L. L., "Reassessment of Expectations as a Comparison Standard in Measuring Service Quality: Implications for Further Research," \'\'Journal of Marketing,\'\' Vol. 58 January 1994, pp 111–124</ref> Of these, there is an argument that only \'\'forecast expectations\'\' are true expectations. Yet, the SERVQUAL instrument appears to elicit \'\'ideal expectations\'\'.<ref>Johnson, C. and Mathews, B.P. , "The influence of experience on service expectations", \'\'International Journal of Service Industry Management,\'\' Vol. 8 no. 4, pp 290-305</ref>  Note the wording in the questionnaire in the preceding figure which grounds respondents in their expectations of what \'\'excellent\'\' companies will do. Subtle use of words can elicit different types of expectations.  Capturing true expectations is important because it has implications for service quality scores. When researchers elicit ideal expectations, overall service quality scores are likely to be lower, making it much more difficult for marketers to deliver on those expectations.<ref>Boulding, W., Kalra, A., Staelin, R. and Zeithaml, V. A., "Dynamic Process Model of Service Quality: From Expectations to Behavioral Intentions," \'\'Journal of Marketing Research,\'\' Vol. 30, no 1, 1993, pp 7-27</ref>\n\n:\'\'Questionnaire length:\'\'  The matched pairs design of the questionnaire (total of 22 expectation items plus 22 perception items= 44 total items) makes for a very long questionnaire. If researchers add demographic and other behavioural items such as prior experience with product or category and the standard battery of demographics including: age, gender, occupation, educational attainment etc. then the average questionnaire will have around 60 items. In practical terms, this means that the questionnaire would take more than one hour per respondent to administer in a face-to-face interview. Lengthy questionnaires are known to induce \'\'respondent fatigue\'\' which may have potential implications for data reliability. In addition, lengthy questionnaires add to the time and cost involved in data collection and data analysis. Coding, collation and interpretation of data is very time consuming and in the case of lengthy questionnaires administered across large samples, the findings cannot be used to address urgent quality-related problems. In some cases, it may be necessary to carry out \'quick and dirty\' research while waiting for the findings of studies with superior research design.\n\n: Some analysts have pointed out that the SERVPERF instrument, developed by Cronin and Taylor,<ref>Cronin, J. J. and Taylor, S. A., "Measuring Service Quality: A Re-examination and Extension," \'\'Journal of Marketing,\'\' Vol. 56, no. 3, 1992, pp 55-68.</ref><ref>Cronin J.J., Steven, J. and Taylor, A., "SERVPERF versus SERVQUAL: Reconciling performance based  and  perceptions-minus-expectations  measurement  of  service  quality," \'\'Journal  of  Marketing,\'\'  Vol.  58, January, 1994, pp. 125-131</ref> and which reduced the number of questionnaire items by half (22 perceptions items only), achieves results that correlate well with SERVQUAL, with no reduction in diagnostic power, improved data accuracy through reductions in respondent boredom and fatigue and savings in the form of reduced administration costs.\n\n:\'\'Dimensional instability\'\':  A number of studies have reported that the five dimensions of service quality implicit in the model (reliability, assurance, tangibles, empathy and responsiveness) do not hold up when the research is replicated in different countries, different industries, in different market segments or even at different time periods.<ref>Carman, J.M., "Consumer Perceptions of Service Quality: An assessment of the SERVQUAL dimensions," \'\'Journal of Retailing,\'\' Vol. 66, no 1, 1990</ref><ref>Lam, S. K and Woo, K. S.,  "Measuring Service Quality: A test-retest reliability investigation of SERVQUAL,"  \'\'Journal of the Market Research Society, \'\' Vol. 39, no. 2, 1997, pp 381-396</ref> Some studies report that the SERVQUAL items do not always load onto the same factors. In some empirical research, the items load onto fewer dimensions, while other studies report tht the items load onto more than five dimensions of quality. In statistical terms, the robustness of the factor loadings is known as a model\'s \'\'dimensional stability.\'\' Across a wide range of empirical studies, the factors implicit in the SERVQUAL instrument have been shown to be unstable.<ref>Niedricha, R.W., Kiryanovab, E. and Black, W.C., "The Dimensional Stability of the Standards used in the Disconfirmation Paradigm," \'\'Journal of Retailing,\'\' Vol. 81, no. 1, 2005, pp 49–57</ref> Problems associated  with  the stability of the factor loadings may be attributed, at least in part, to the  requirement  that  each  new  SERVQUAL  investigation needed  to  make  context-sensitive  modifications to  the  instrument  in  order  to  accommodate the  unique  aspects  of  the focal service setting or problem. However, it has also been hypothesised that the dimensions of service quality represented by the SERVQUAL research instrument fail to capture the true dimensionality of the service quality construct and that there may not be a universal set of service quality dimensions that are relevant across all service industries.<ref>Miller, R.E., Hardgrave, B.C. and Jones, R.W., "SERVQUAL Dimensionality: An investigation of presentation order effect," \'\'International Journal of Services and Standards,\'\' Vol. 7, no. 1 DOI: 10.1504/IJSS.2011.040639</ref>\n\nIn spite of these criticisms, the SERVQUAL instrument, or any one of its variants (i.e. modified forms), dominates current research into service quality.<ref>Ladhari, R., "A review of twenty years of SERVQUAL research", \'\'International Journal of Quality and Service Sciences,\'\' Vol. 1 no. 2, pp.172 - 198</ref>  In a review of more than 40 articles that made use of SERVQUAL. a team of researchers found that  “few researchers concern themselves with the validation of the measuring tool”.<ref>Nyeck, S., Morales, M., Ladhari, R., & Pons, F., "10 Years of Service Quality  Measurement: Reviewing the use of the SERVQUAL Instrument," \'\'Cuadernos de Difusion,\'\' Vol. 7, no 13, pp 101-107.</ref>  SERVQUAL is not only the subject of academic papers, but it is also widely used by industry practitioners.<ref>Asubonteng, P., McCleary, K.J. and Swan, J.E.,  "SERVQUAL revisited: a critical review of service quality", \'\'Journal of Services Marketing,\'\' Vol. 10, no 6, 1996, pp 62-81</ref>\n\n== See also==\n\n* [[Customer satisfaction]]\n* [[Customer satisfaction research]]\n* [[Disconfirmed expectancy]]\n* [[Quality management]]\n* [[Service quality]]\n* [[Services marketing]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n\n*[http://www.farrell-associates.com.au/BSOM/Papers/SERVQUAL.doc/ SERVQUAL questionnaire]  - Annotated copy of SERVQUAL questionnaire\n*[http://www.kinesis-cem.com/pdf/ServQual.pdf/ SERVQUAL Instructions] - Detailed instructions for administering the SERVQUAL questionnaire\n\n==Further reading==\n* Luis Filipe Lages & Joana Cosme Fernandes, 2005, "The SERPVAL scale: A multi-item instrument for measuring service personal values", \'\'Journal of Business Research,\'\' Vol.58, Issue 11, pp 1562–1572.\n* Deborah McCabe, Mark S. Rosenbaum, and Jennifer Yurchisin (2007), “Perceived Service Quality and Shopping Motivations:  A Dynamic Relationship,” \'\'Services Marketing Quarterly,\'\' 29 (1), pp 1–21.\n\n[[Category:Knowledge representation]]\n[[Category:Quality management]]\n[[Category:Service industries]]']
['Unique name assumption', '15056340', "The '''unique name assumption''' is a simplifying assumption made in some [[ontology (computer science)|ontology]] languages and [[description logic]]s. In logics with the unique name assumption, different names always refer to different entities in the world.<ref>{{Cite AIMA |edition=2 |pages=333}}</ref>\n\nThe standard ontology language [[Web Ontology Language|OWL]] does not make this assumption, but provides explicit constructs to express whether two names denote the same or distinct entities.<ref>{{cite conference |first1=Jiao |last1=Tao |first2=Evren |last2=Sirin |first3=Jie |last3=Bao |first4=Deborah L. |last4=McGuinness |title=Integrity constraints in OWL |conference=Proc. AAAI |year=2010 |url=http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1931}}</ref><ref>[http://www.w3.org/TR/owl-ref/ OWL Web Ontology Language Reference]</ref>\n* <code>owl:sameAs</code> is the OWL property that asserts that two given names or identifiers (e.g., URIs) refer to the same individual or entity.\n* <code>owl:differentFrom</code> is the OWL property that asserts that two given names or identifiers (e.g., URIs) refer to different individuals or entities.\n\n==See also==\n* [[Closed-world assumption]]\n* [[Coreference]]\n\n==References==\n{{reflist}}\n\n[[Category:Knowledge representation]]\n[[Category:Ontology (information science)]]\n\n{{logic-stub}}"]
['FrameNet', '2112884', '{{refimprove|date=March 2012}}\nIn [[computational linguistics]], \'\'\'FrameNet\'\'\' is a project housed at the [[International Computer Science Institute]] in [[Berkeley, California]] which produces an electronic resource based on a theory of meaning called\n[[Frame semantics (linguistics)|frame semantics]]. FrameNet reveals for example that the sentence "John sold a car to Mary" essentially describes the same basic situation (semantic frame) as "Mary bought a car from John", just from a different perspective. A semantic frame can be thought of as a conceptual structure describing an event, relation, or object and the participants in it. The FrameNet [[lexical database]] contains over 1,200 semantic \'\'frames\'\', 13,000 \'\'lexical units\'\' (a pairing of a [[word]] with a [[Meaning (linguistics)|meaning]]; [[Polysemy|polysemous]] words are represented by several \'\'lexical units\'\') and 202,000 example sentences. FrameNet is largely the creation of [[Charles J. Fillmore]], who developed the theory of frame semantics that the project is based on, and was initially the project leader when the project began in 1997.<ref name="Goddard2011">{{cite book|author=Cliff Goddard|title=Semantic Analysis: A Practical Introduction|url=https://books.google.com/books?id=qfar1cmATvUC&pg=PA78|accessdate=21 March 2012|date=25 September 2011|publisher=Oxford University Press|isbn=978-0-19-956028-8|pages=78–81}}</ref> Collin Baker became the project manager in 2000.<ref name="Linguistic Analysis">{{cite book|title=The Oxford Handbook of Linguistic Analysis|url=https://books.google.com/books?id=7plqH2gSq1wC&pg=PP20|accessdate=21 March 2012|editor1-last=Heine|editor1-first=Bernd|editor2-last=Narrog|editor2-first=Heiko|publisher=Oxford University Press|isbn=978-0-19-160925-1|page=20}}</ref> The FrameNet project has been influential in both linguistics and natural language processing, where it led to the task of automatic [[Semantic Role Labeling]].\n\n==Concepts==\n\n===Frames===\nA frame is a schematic representation of a situation involving various participants, props, and other conceptual roles. Examples of frame names are <tt>Being_born</tt> and <tt>Locative_relation</tt>. A frame in FrameNet contains a textual description of what it represents (a frame definition), associated frame elements, lexical units, example sentences, and frame-to-frame relations.\n\n===Frame elements===\nFrame elements (FE) provide additional information to the semantic structure of a sentence. Each frame has a number of core and non-core FEs which can be thought of as semantic roles. Core FEs are essential to the meaning of the frame while non-core FEs are generally descriptive (such as time, place, manner, etc.).<ref>https://framenet.icsi.berkeley.edu/fndrupal/glossary#core</ref>\n\nSome examples include:\n\n* The only core FE of the <tt>Being_born</tt> frame is called <tt>Child</tt>; non-core FEs being <tt>Time</tt>, <tt>Place</tt>, <tt>Relatives</tt>, etc.<ref>https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=frame_report&name=Being_born</ref>\n* Core FEs of the <tt>Commerce_goods-transfer</tt> include the <tt>Seller</tt>, <tt>Buyer</tt>, <tt>Goods</tt>, among other things, while non-core FEs include a <tt>Place</tt>, <tt>Purpose</tt>, etc.<ref>https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=frame_report&name=Commerce_goods-transfer</ref>\n\nFrameNet includes shallow data on syntactic roles that frame elements play in the example sentences. For an example sentence like "She was born about AD 460", FrameNet would mark "She" as a [[noun phrase]] referring to the <tt>Child</tt> FE, and "about AD 460" as a [[noun phrase]] corresponding to the <tt>Time</tt> frame element. Details of how frame elements can be realized in a sentence are important because this reveals important information about the [[subcategorization frame]]s as well as possible [[diathesis alternation]]s (e.g. "John broke the window" vs. "The window broke")\nof a verb.\n\n===Lexical units===\nLexical units (LU) are lemmas, with their part of speech, that evoke a specific frame. In other words, when a LU is identified in a sentence, that specific LU can be associated with its specific frame(s). For each frame, there are many LUs associated to one frame and many frames that share multiple LUs, this is typically the case with LUs that have multiple word senses.<ref>https://framenet.icsi.berkeley.edu/fndrupal/glossary</ref> Alongside the frame, each lexical unit is associated with specific frame elements by means of the annotated example sentences.\n\nExample:\n\nLexical units that evoke the <tt>Complaining</tt> frame (or more specific perspectivized versions of it, to be precise), include the verbs "complain", "grouse", "lament", and others.<ref>https://framenet2.icsi.berkeley.edu/fnReports/data/frameIndex.xml?frame=Complaining</ref>\n\n===Example sentences===\nFrames are associated with example sentences and frame elements are marked within the sentences. Thus the sentence\n:\'\'She was \'\'\'born\'\'\' about AD 460\'\'\nis associated with the frame <tt>Being_born</tt>, while "She" is marked as the frame element <tt>Child</tt> and "about AD 460" is marked as <tt>Time</tt>.\n(See the [http://framenet.icsi.berkeley.edu/fnReports/displayReport.php?anno=9791 FrameNet Annotation Report] for <tt>born.v</tt>.)\nFrom the start, the FrameNet project has been committed to looking at evidence from actual language use as found in text collections like the [[British National Corpus]]. \nBased on such example sentences, automatic [[semantic role labeling]] tools are able to determine frames and mark frame elements in new sentences.\n\n===Valences===\nFrameNet also exposes the statistics on the \'\'valences\'\' of the \'\'frames\'\', that is the number and the position of the \'\'frame elements\'\' within example sentences. The sentence\n:\'\'She was \'\'\'born\'\'\' about AD 460\'\'\nfalls in the valence pattern\n:\'\'\'NP Ext, INI --, NP Dep\'\'\'\nwhich occurs two times in the [https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu9791.xml example sentences] in FrameNet,\nnamely in:\n:She\'\' was \'\'\'born\'\'\' \'\'about AD 460\'\', daughter and granddaughter of Roman and Byzantine emperors, whose family had been prominent in Roman politics for over 700 years.\'\'\n:\'\'He was soon posted to north Africa, and never met their only child, \'\'a daughter\'\' \'\'\'born\'\'\' \'\'8 June 1941\'\'.\'\'\n\n===Frame Relations===\n\nFrameNet additionally captures relationships between different frames using relations. These include the following.\n\n* Inheritance: When one frame is a more specific version of another, more abstract parent frame. Anything that is true about the parent frame must also be true about the child frame, and a mapping is specified between the frame elements of the parent and the frame elements of the child.\n* Perspectivized_in: A neutral frame (like <tt>Commerce_transfer-goods</tt>) is connected to a frame with a specific perspective of the same scenario (e.g. the <tt>Commerce_sell</tt> frame, which assumes the perspective of the seller or the <tt>Commerce_buy</tt> frame, which assumes the perspective of the buyer)\n* Subframe: Some frames like the <tt>Criminal_process</tt> frame refer to complex scenarios that consist of several individual states or events that can be described by separate frames like <tt>Arrest</tt>, <tt>Trial</tt>, and so on.\n* Precedes: The Precedes relation captures a temporal order that holds between subframes of a complex scenario.\n* Causative_of and Inchoative_of: There is a fairly systematic relationship between stative descriptions (e.g. the <tt>Position_on_a_scale</tt> frame, "She had a high salary") and causative descriptions (<tt>Cause_change_of_scalar_position</tt>, "She raised his salary") or inchoative descriptions (<tt>Change_position_on_a_scale</tt>, e.g. "Her salary increased").\n* Using: A relationship that holds between a frame that in some way involves another frame. For instance, the <tt>Judgment_communication</tt> frame uses both the <tt>Judgment</tt> frame and the <tt>Statement</tt> frame, but does not inherit from either of them because there is no clear correspondence of the frame elements.\n* See_also: Connects frames that bear some resemblance but need to be distinguished carefully.\n\n==Applications==\n\nFrameNet has proven useful in a number of computational applications, because computers need additional knowledge in order to recognize that "John sold a car to Mary" and "Mary bought a car from John" describe essentially the same situation, despite using two very different verbs, different prepositions and a different word order. FrameNet has been used in applications like [[question answering]], paraphrasing, recognizing textual entailment, and information extraction, either directly or by means of [[Semantic Role Labeling]] tools. The first automatic system for [[Semantic Role Labeling]] (SRL, sometimes also referred to as "shallow semantic parsing") was developed by Daniel Gildea and Daniel Jurafsky based on FrameNet in 2002, and Semantic Role Labelling has since become one of the standard tasks in natural language processing.\n\nSince frames are essentially semantic descriptions, they are similar across languages, and several projects have arisen over the years that have relied on the original FrameNet as the basis for additional non-English FrameNets, for Spanish, Japanese, German, and Polish, among others.\n\n==See also==\n*[[BabelNet]]: a multilingual semantic network integrating FrameNet\n*[[PropBank]]\n*[[Null instantiation]]\n*[[Frame language]]\n*[[UBY]]: a database of 10 resources including FrameNet\n\n==References==\n{{Reflist}}\n\n===Further reading===\n*[https://framenet2.icsi.berkeley.edu/docs/r1.5/book.pdf FrameNet II: Extended Theory and Practice] (e-book)\n\n==External links==\n*[http://framenet.icsi.berkeley.edu/ FrameNet home page]\n*[http://sccfn.sxu.edu.cn/ Chinese FrameNet]\n*[http://framenet.dk/ Danish FrameNet]\n*[http://gframenet.gmc.utexas.edu/ German FrameNet]\n*[http://jfn.st.hc.keio.ac.jp/ Japanese FrameNet]\n*[http://www.ramki.uw.edu.pl/en/index.html Polish FrameNet]\n*[http://www.ufjf.br/framenetbr/ Portuguese FrameNet (Brazil)]\n*[http://gemini.uab.es/SFN/ Spanish FrameNet]\n*[http://spraakbanken.gu.se/eng/swefn/ Swedish FrameNet]\n\n[[Category:Lexical databases]]\n[[Category:Knowledge representation]]\n[[Category:Corpus linguistics]]\n[[Category:History of the Internet]]\n[[Category:Hypertext]]\n[[Category:Online dictionaries]]\n[[Category:Science and technology in the San Francisco Bay Area]]']
['Knowledge integration', '4144848', "'''Knowledge integration''' is the process of synthesizing multiple [[knowledge model]]s (or representations) into a common model (representation).\n\nCompared to [[information integration]], which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives.\n\nFor example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index.\n\nThe [http://wise.berkeley.edu Web-based Inquiry Science Environment (WISE)], from the [[University of California at Berkeley]] has been developed along the lines of knowledge integration theory.\n\n'''Knowledge integration''' has also been studied as the process of incorporating new information into a body of existing knowledge with an [[interdisciplinary]] approach.  This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge.\n\nA learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps.  By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information.\n\nThe [[machine learning]] program KI, developed by Murray and Porter at the [[University of Texas at Austin]], was created to study the use of automated and semi-automated knowledge integration to assist [[knowledge engineers]] constructing a large [[knowledge base]].\n\nA possible technique which can be used is [[semantic matching]]. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on [[Minimal mappings|Minimal Mappings]]. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).\n\nThe [[University of Waterloo]] operates a Bachelor of Knowledge Integration [[undergraduate degree]] program as an academic major or minor. The program started in 2008.\n\n==See also==\n* [[Knowledge value chain]]\n\n==References==\n{{Reflist}}<!--added under references heading by script-assisted edit-->\n\n==Further reading==\n* Linn, M. C. (2006) The Knowledge Integration Perspective on Learning and Instruction. R. Sawyer (Ed.). In ''The Cambridge Handbook of the Learning Sciences.'' Cambridge, MA. Cambridge University Press\n* Murray, K. S. (1996) KI: A tool for Knowledge Integration. Proceedings of the Thirteenth National Conference on Artificial Intelligence\n* Murray, K. S. (1995) [http://www.ai.sri.com/pubs/files/1636.pdf Learning as Knowledge Integration], Technical Report TR-95-41, The University of Texas at Austin\n* Murray, K. S. (1990) Improving Explanatory Competence, Proceedings of the Twelfth Annual Conference of the Cognitive Science Society\n* Murray, K. S., Porter, B. W. (1990) Developing a Tool for Knowledge Integration: Initial Results. International Journal for Man-Machine Studies, volume 33\n* Murray, K. S., Porter, B. W. (1989) Controlling Search for the Consequences of New Information during Knowledge Integration. Proceedings of the Sixth International Machine Learning Conference\n\n[[Category:Knowledge representation]]\n[[Category:Learning]]\n[[Category:Machine learning]]"]
['User modeling', '12781902', '\'\'\'User modeling\'\'\' is the subdivision of [[human–computer interaction]] which describes the\nprocess of building up and modifying a conceptual understanding of the user. The main goal of user modeling is customization and [[Adaptation (computer science)|adaptation of systems]] to the user\'s specific needs. The system needs to "say the \'right\' thing at the \'right\' time in the \'right\' way".<ref name=Fischer>{{Citation\n  | last1 = Fischer | first1 = Gerhard\n  | title = User Modeling in Human-Computer Interaction\n  | journal = User Modeling and User-Adapted Interaction 11\n  | pages = 65–68\n  | year = 2001 }}</ref> To do so it needs an internal representation of the user. Another common purpose is modeling specific kinds of users, including modeling of their skills and declarative knowledge, for use in automatic software-tests.<ref name=JohnsonTaatgen> {{Citation\n  | last1 = Johnson | first1 = Addie\n  | last2=Taatgen | first2 = Niels\n  | chapter = User Modeling\n  | title = Handbook of human factors in Web design\n  | pages = 424–439\n  | publisher = Lawrence Erlbaum Associates\n  | year = 2005 }}</ref> User-models can thus serve as a cheaper alternative to [[user testing]].\n\n== Background ==\n\nA user model is the collection and categorization of personal data associated with a specific user. Therefore, it is the basis for any adaptive changes to the system\'s behavior. Which data is included in the model depends on the purpose of the application. It can include personal information such as users\' names and ages, their interests, their skills and knowledge, their goals and plans, their preferences and their dislikes or data about their behavior and their interactions with the system.\n\nThere are different design patterns for user models, though often a mixture of them is used.<ref name=JohnsonTaatgen /><ref>{{Citation\n  | last1 = Hothi | first1 = Jatinder\n  | last2=Hall | first2 = Wendy\n  | title = An Evaluation of Adapted Hypermedia Techniques Using Static User Modelling\n  | journal = Proceedings of the 2nd Workshop on Adaptive Hypertext and Hypermedia\n  | place = Southampton University, Electronics and Computer Science University Road, Southampton, Hampshire, UK\n  | year = June 1998\n  | url = http://wwwis.win.tue.nl/ah98/Hothi/Hothi.html }}</ref>\n* \'\'\'Static user models\'\'\'\n:Static user models are the most basic kinds of user models. Once the main data is gathered they are normally not changed again, they are static. Shifts in users\' preferences are not registered and no learning algorithms are used to alter the model.\n* \'\'\'Dynamic user models\'\'\'\n:Dynamic user models allow a more up to date representation of users. Changes in their interests, their learning progress or interactions with the system are noticed and influence the user models. The models can thus be updated and take the current needs and goals of the users into account.\n* \'\'\'Stereotype based user models \'\'\'\n:Stereotype based user models are based on [[Demographics|demographic statistics]]. Based on the gathered information users are [[Classification_in_machine_learning|classified]] into common stereotypes. The system then adapts to this stereotype. The application therefore can make assumptions about a user even though there might be no data about that specific area, because demographic studies have shown that other users in this stereotype have the same characteristics. Thus, stereotype based user models mainly rely on statistics and do not take into account that personal attributes might not match the stereotype. However, they allow predictions about a user even if there is rather little information about him or her.\n* \'\'\'Highly adaptive user models\'\'\'\n:Highly adaptive user models try to represent one particular user and therefore allow a very high adaptivity of the system. In contrast to stereotype based user models they do not rely on demographic statistics but aim to find a specific solution for each user. Although users can take great benefit from this high adaptivity, this kind of model needs to gather a lot of information first.\n\n== Data gathering ==\n\nInformation about users can be gathered in several ways. There are three main methods:\n\n* \'\'\'Asking for specific facts while (first) interacting with the system\'\'\'<ref name=JohnsonTaatgen />\n:Mostly this kind of data gathering is linked with the registration process. While registering users are asked for specific facts, their likes and dislikes and their needs. Often the given answers can be altered afterwards.\n* \'\'\'Learning users\' preferences by observing and interpreting their interactions with the system\'\'\'<ref name=JohnsonTaatgen />\n:In this case users are not asked directly for their personal data and preferences, but this information is derived from their behavior while interacting with the system. The ways they choose to accomplish a tasks, the combination of things they takes interest in, these observations allow inferences about a specific user. The application dynamically learns from observing these interactions. Different [[machine learning]] algorithms may be used to accomplish this task.\n* \'\'\'A hybrid approach which asks for explicit feedback and alters the user model by adaptive learning\'\'\'<ref name=Montaner>{{Citation\n  | last = Montaner | first = Miguel\n  | last2 = López | first2 = Beatriz\n  | last3 = De La Rosa | first3 = Josep Lluís\n  | title = A Taxonomy of Recommender Agents on the Internet,\n  | journal = Artif. Intell. Rev.\n  | volume = 19\n  | pages = 285–330\n  | year = 2003 }}</ref>\n:This approach is a mixture of the ones above. Users have to answer specific questions and give explicit feedback. Furthermore, their interactions with the system are observed and the derived information are used to automatically adjust the user models.\n\nThough the first method is a good way to quickly collect main data it lacks the ability to automatically adapt to shifts in users\' interests. It depends on the users\' readiness to give information and it is unlikely that they are going to edit their answers once the registration process is finished. Therefore, there is a high likelihood that the user models are not up to date. However, this first method allows the users to have full control over the collected data about them. It is in their decision which information they are willing to provide. This possibility is missing in the second method. Adaptive changes in a system that learns users\' preferences and needs only by interpreting their behavior might appear a bit opaque to the users, because they cannot fully understand and reconstruct why the system behaves the way it does.<ref name=Montaner /> Moreover, the system is forced to collect a certain amount of data before it is able to predict the users\' needs with the required accuracy. Therefore, it takes a certain learning time before a user can benefit from adaptive changes. However, afterwards these automatically adjusted user models allow a quite accurate adaptivity of the system. The hybrid approach tries to combine the advantages of both methods. Through collecting data by directly asking its users it gathers a first stock of information which can be used for adaptive changes. By learning from the users\' interactions it can adjust the user models and reach more accuracy. Yet, the designer of the system has to decide, which of these information should have which amount of influence and what to do with learned data that contradicts some of the information given by a user.\n\n== System adaptation ==\n\nOnce a system has gathered information about a user it can evaluate that data by preset analytical algorithm and then start to adapt to the user\'s needs. These adaptations may concern every aspect of the system\'s behavior and depend on the system\'s purpose. Information and functions can be presented according to the user\'s interests, knowledge or goals by displaying only relevant features, hiding information the user does not need, making proposals what to do next and so on. One has to distinguish between [[Adaptation (computer science)#Adaptivity_and_adaptability|adaptive and adaptable systems]].<ref name=Fischer /> In an adaptable system the user can manually change the system\'s appearance, behavior or functionality by actively selecting the corresponding options. Afterwards the system will stick to these choices. In an [[adaptive system]] a dynamic adaption to the user is automatically performed by the system itself, based on the built user model. Thus, an adaptive system needs ways to interpret information about the user in order to make these adaptations. One way to accomplish this task is implementing rule-based filtering. In this case a set of IF... THEN... rules is established that covers the [[knowledge base]] of the system.<ref name=JohnsonTaatgen /> The IF-conditions can check for specific user-information and if they match the THEN-branch is performed which is responsible for the adaptive changes. Another approach is based on [[collaborative filtering]].<ref name=JohnsonTaatgen /><ref name=Montaner /> In this case information about a user is compared to that of other users of the same systems. Thus, if characteristics of the current user match those of another, the system can make assumptions about the current user by presuming that he or she is likely to have similar characteristics in areas where the model of the current user is lacking data. Based on these assumption the system then can perform adaptive changes.\n\n== Usages ==\n\n* [[Adaptive hypermedia]]: In an adaptive hypermedia system the displayed content and the offered hyperlinks are chosen on basis of users\' specific characteristics, taking their goals, interests, knowledge and abilities into account. Thus, an adaptive hypermedia system aims to reduce the "lost in hyperspace" syndrome by presenting only relevant information.\n* [[Adaptive educational hypermedia]]: Being a subdivision of adaptive hypermedia the main focus of adaptive educational hypermedia lies on education, displaying content and hyperlinks corresponding to the user\'s knowledge on the field of study.\n* [[Intelligent tutoring system]]: Unlike adaptive educational hypermedia systems intelligent tutoring systems are stand-alone systems. Their aim is to help students in a specific field of study. To do so, they build up a user model where they store information about abilities, knowledge and needs of the user. The system can now adapt to this user by presenting appropriate exercises and examples and offering hints and help where the user is most likely to need them. \n* [[Expert systems]]: Expert systems are computer systems that emulate the decision-making ability of a human expert in order to help the user solving a problem in a specific area. Step by step they ask questions to identify the current problem and to find a solution. User models can be used to adapt to the current user\'s knowledge, differentiating between experts and novices. The system can assume, that experienced users are able to understand and answer more complex questions than someone who is new to the topic. Therefore, it can adjust the used vocabulary and the type of question which are presented to the user, thus reducing the steps needed to find a solution.\n* [[Recommender system]]: The basic idea of recommender systems is to present a selection of items to the user which best fit his or her needs. This selection can be based on items the user has bookmarked, rated, bought, recently viewed, etc. Recommender systems are often used in [[e-commerce]] but may also cover areas like social networks, websites, news, etc.\n* [[Usability testing|User-Simulation]]: Since user modeling allows the system to hold an internal representation of a specific user, different types of users can be simulated by artificially modeling them. Common types are "experts" or "novices" on the scope of the system or the usage of the system. Based on these characteristics user tests can be simulated.\n\n== Standards==\nA certain number of representation formats and standards are available for representing the users in computer systems,<ref>Nabeth Thierry (2005), [http://www.fidis.net/resources/fidis-deliverables/identity-of-identity/#c1753: Models], FIDIS Deliverable, October 2005. </ref> such as:\n* [[IMS-LIP]] (IMS &ndash; Learner Information Packaging, used in [[e-learning]]) \n* [[HR-XML Standards|HR-XML]] (used in [[human resource management]])\n* [[JXDM]] (Justice with the Global Justice Extensible Markup)\n* [[Europass]] (the Europass online CV)\n\n== See also ==\n\n* [[Personalization]]\n* [[Cognitive model]]\n* [[User profile]]\n* [[Identity management]]\n\n== References ==\n<references/>\n\n== External references ==\n* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] The Journal of Personalization Research \n* [http://www.cs.cmu.edu/~bej/cogtool/ CogTool Project at CMU]\n* [http://www.iit.demokritos.gr/um2007/ UserModeling conference 2007]\n\n[[Category:Knowledge representation]]']
['Chinese Library Classification', '5625552', "The '''Chinese Library Classification''' ({{Zh|c = 中国图书馆分类法|s = |t = }}; CLC), also known as '''Classification for Chinese Libraries''' (CCL){{FACT|date=December 2016}}, is effectively the national [[library classification]] scheme in [[China]]. It is used in almost all primary and secondary schools, universities, academic institutions, as well as public [[libraries]]. It is also used by publishers to classify all books published in China.\n\nThe '''Book Classification of Chinese Libraries''' (BCCL) was first published in 1975, under the auspices of China's Administrative Bureau of Cultural Affairs. Its fourth edition (1999) was renamed CLC. In September 2010, the fifth edition was published by National Library of China Publishing House.\nCLC has twenty-two top-level categories, and inherits a [[Marxist]] orientation from its earlier editions.<ref>[http://research.dils.tku.edu.tw/joemls/41/41-1/1-22.pdf Zhang, Wenxian (2003). ''Classification for Chinese Libraries (CCL): Histories, accomplishments, problems and its comparisons''. Journal of Educational Media & Library Sciences, vol. 41, nr. 1, p. 1-22.] (PDF)</ref> (For instance, category A is [[Marxism]], [[Leninism]], [[Maoism]] & [[Deng Xiaoping Theory]].) It contains a total of 43600 categories, many of which are recent additions, meeting the needs of a rapidly changing nation.<ref>''The Standardization of Chinese Library Classification'', Xiaochun Liu, [[Cataloging & Classification Quarterly]], Volume 16, Issue 2, ISSN 0163-9374, Pub Date: 8/13/1993\n</ref>\n\n== The CLC System ==\nThe 22 top categories and selected sub-categories of CLC (5th Edition) are as follows:\n\n=== A.  [[Marxism]], [[Leninism]], [[Maoism]] & [[Deng Xiaoping Theory]] ===\n* A1 The Works of [[Karl Marx]] and [[Friedrich Engels]]\n* A2 The Works of [[Vladimir Lenin]]\n* A3 The Works of [[Joseph Stalin]]\n* A4 The Works of [[Mao Zedong]]\n**   A49 The works of [[Deng Xiaoping]]\n* A5 The Symposium/Collection of Marx, Engels, Lenin, Stalin, Mao and Deng Xiaoping\n* A7 The biobibliography and [[biography]] of Marx, Engels, Lenin, Stalin, Mao and Deng Xiaoping\n* A8 Study and Research of Marxism, Leninism, Maoism & Deng Xiaoping Theory\n\n=== B.  [[Philosophy]] and [[Religions]] ===\n* B-4 Education and dissemination of philosophy\n** B-49 Learners' book und popular literature of philosophy\n* B0 theory of philosophy\n** B0-0 Marxist philosophy\n** B01 Basic problems of philosophy\n*** B014 Object, purpose and method of philosophy\n*** B015 [[Materialism]] and [[idealism]]\n*** B016 [[Ontology]]\n**** B016.8 [[Cosmology]]\n**** B016.9 Time-space-theory\n*** B017 [[Epistemology]]\n**** B017.8 [[Determinism]] and [[Indeterminism]]\n**** B017.9 Self Theory\n*** B018 [[Axiology]]\n**** B019.1 Materialism\n***** B019.11 Naive materialism\n***** B019.12 Metaphysical materialism\n***** B019.13 [[Dialectical materialism]]\n**** B019.2 [[Idealism]]\n** B02 [[Dialectical materialism]]\n*** B024 [[Materialist dialectics]]\n*** B025 Categories of materialist dialectics\n*** B026 [[Methodology]]\n*** B027 Application of dialectical materialism\n*** B028 [[Natural philosophy]]\n*** B029 [[Dialectics of nature]]\n** B03 [[Historical materialism]]\n*** B031 Social material requirements of life\n*** B032 Basic social conflict\n**** B032.1 [[Productive forces]] und [[relations of production]]\n**** B032.2 [[Base and superstructure]]\n*** B033 [[Marxian Class Theory|Class Theory]]\n*** B034 [[Marxism#Revolution|Theory of Revolution]]\n*** B035 Theory of country\n*** B036 [[Social being]] and [[social consciousness]]\n*** B037 [[On Contradiction#Basics of Contradiction and its History|Contradictions among the People]]\n*** B038 Role of the people in historical development\n** B08 [[Philosophical schools]] and research\n*** B081 [[Idealism]]\n**** B081.1 [[Metaphysics]]\n**** B081.2 Epistemology of idealism, apriorism\n*** B082 [[Positivism]], [[Machism]]\n*** B083 [[Voluntarism (metaphysics)|Voluntarism]] and [[philosophy of life]]\n*** B084 [[Neo-Kantianism]] and [[Neohegelianism]]\n*** B085 [[New realism (philosophy)|Neorealism]], [[logical positivism]] (new positivism, logical empiricism)\n*** B086 [[Existentialism]] ([[survivalism]])\n*** B087 [[Pragmatism]]\n*** B088 [[Neo-Thomism]] (new scholasticism)\n*** B089 Other philosophical schools\n**** B089.1 [[Western Marxism]]\n**** B089.2 [[Philosophical hermeneutics]]\n**** B089.3 [[Philosophical anthropology]]\n*  B1 Philosophy (Worldwide)\n*  B2 Philosophy in China\n**    B22 Pre-[[Qin Dynasty]] Philosophy (~before 220 BC)\n***       B222 The Confucian School\n****           B222.2 [[Confucius]] (Kǒng Qiū, 551-479 BC)\n*  B3 Philosophy in Asia\n*  B4 [[African philosophy|Philosophy in Africa]]\n*  B5 Philosophy in Europe\n*  B6 Philosophy in Australasia\n*  B7 [[Philosophy in America]]\n*  B8 [[Cognitive science]]\n*  B9 Religions\n**    B91 [[Sociology of Religion]], [[Religion]] and [[Science]]\n**    B92 [[Philosophy of religion|Philosophy]] and [[History of Religion]]\n**    B93 [[Mythology]] and [[Animism|Primitive religion]]\n**    B94 [[Buddhism]]\n**    B95 [[Taoism]]\n**    B96 [[Islam]]\n**    B97 [[Christianity]]\n***       B971 [[Bible]]\n****           B971.1 [[Old Testament]]\n****           B971.2 [[New Testament]]\n***       B972 [[Doctrine]], [[Theology]]\n***       B975 [[Evangelism]], [[Sermon]]\n***       B976 [[Christian Denomination]]\n****           B976.1 [[Roman Catholic Church]]\n****           B976.2 Orthodox Christianity ([[Eastern Orthodox Church|Eastern Orthodoxy]], [[Oriental Orthodoxy]])\n****           B976.3 [[Protestantism]] ([[Protestant Reformation]])\n***       B977 [[Ecclesiastical polity]]\n***       B978 Research on Christianity\n***       B979 [[History of Christianity]]\n****           B979.9 Biography\n**    B98 Other Religions\n**    B99 [[Augury]], [[Superstition]]\n\n=== C.  [[Social Sciences]] ===\n*  C0 Social Scientific Theory and Methodology\n*  C1 Present and Future of Social Sciences\n*  C2 Organisations, Groups, Conferences\n*  C3 Method of Research in Social Sciences\n*  C4 Education and Popularization of Social Sciences\n*  C5 Serials, [[Anthology|Anthologies]], Periodicals in Social Sciences\n*  C6 Reference Materials in Social Sciences\n*  C7 (no longer used)\n*  C8 Statistics in Social Sciences\n*  C9 [[Sociology]]\n\n=== D.  [[Politics]] and [[Law]] ===\n*  D0 [[Political theory]]\n*  D1 International Campaign of [[Communism]]\n*  D2 [[Communist Party of China]]\n*  D3 [[Communist Parties]] of other Countries\n*  D4 Labor, Peasant, Youth, Female Organizations and Movements\n*  D5 Politics (worldwide)\n*  D6 Politics in China\n*  D7 Politics in individual [[Countries]]\n*  D8 [[Diplomacy]], [[International relations]]\n*  D9 [[Law]]\n\n=== E.  [[Military Science]] ===\n*  E0 Military Theory\n*  E1 Military (worldwide)\n*  E2 [[Military in China]]\n*  E3 Military in Asia\n*  E4 Military in Africa\n*  E5 Military in Europe\n*  E6 Military in Australasia\n*  E7 Military in America\n*  E8 Strategies, Tactics, and Battles\n*  E9 [[Military technology|Military Technology]]\n\n=== F.  [[Economics]] ===\n*  F0 [[Economics]]\n*  F1 [[Economics]], [[Economic history]] and [[Economic geography]] of individual countries\n*  F2 Economic Planning and Management\n*  F3 [[Agricultural Economics]]\n*  F4 Industrial Economics\n*  F5 Economics of [[Transport]]\n*  F6 Economics of Postal and Cable Services\n*  F7 Economics of [[Commerce]]\n*  F8 [[Finance]], [[Banking]]\n\n=== G.  [[Culture]], [[Science]], [[Education]] and [[Sports]] ===\n*  G0 Philosophy of Culture\n*  G1 Culture\n*  G2 Knowledge transmission\n*  G3 [[Science]], [[Scientific Research]]\n*  G4 [[Education]]\n*  G5 Education in individual [[Countries]]\n*  G6 Education (Primary, Secondary, Tertiary)\n*  G7 Education (specialized)\n*  G8 Sports\n\n=== H.  [[Languages]] and [[Linguistics]] ===\n*  H0 Linguistics\n**    H01 [[Phonetics]]\n***      H109 Method of Recitation, Oratory of Speech\n**    H02 [[Grammatology]]\n**    H03 [[Semantics]], [[Lexicology]] and Meaning of words\n***      H033 [[Idiom]]\n***      H034 [[Adage]]\n**    H04 [[Syntax]]\n**    H05 Study of writing, [[Rhetoric]]\n***      H059 Study of translation\n**    H06 [[Lexicography]]\n***      H061 [[Dictionary]]\n*  H1 [[Chinese language]]\n**    H10<!--to be filled-->\n***      H102 Regulation, Standardisation of Chinese language, Promotion of [[Putonghua]]\n***      H109<!--to be filled-->\n****        H109.2 Ancient Chinese language\n****        H109.4 Modern Chinese language\n**    H11 [[Phone (phonetics)|Phone]] ([[Historical Chinese phonology]])\n**    H12 Grammatology\n*  H2  [[Languages of China|Languages of China's ethnic minorities]]\n*  H3 Commonly Used Foreign Languages\n**    H31 [[English language]]\n**    H32 [[French language]]\n**    H33 [[German language]]\n**    H34 [[Spanish language]]\n**    H35 [[Russian language]]\n**    H36 [[Japanese language]]\n**    H37 [[Arabic language]]\n*  H4 Family of [[Sino-Tibetan languages]] ([[China]], [[Tibet]] and [[Burma]])\n*  H5 Family of [[Altaic languages]] ([[Turkic languages|Turkic]], [[Mongolian language|Mongolian]] and [[Tungusic languages|Tungusic]])\n*  H6 [[Language family|Language families]] in other areas of the World\n**    H61 [[Austroasiatic languages]] and [[Tai languages]] ([[Southeast Asia|Mainland Southeast Asia]]))\n**    H62 [[Dravidian languages]] ([[South India]])\n**    H63 [[Austronesian languages]] ([[Malayo-Polynesian]])\n**    H64 [[Paleosiberian languages]] ([[Siberia]])\n**    H65 [[Ibero-Caucasian languages]] ([[Caucasus Mountains]])\n**    H66 [[Uralic languages]]\n**    H67 [[Afroasiatic languages]] ([[Southwest Asia]], [[Arabian Peninsula]], [[North Africa]])\n*  H7 [[Indo-European languages]]\n*  H8 [[Language family|Language families]] on other Continents\n**    H81 [[African languages]]\n**    H83 [[Indigenous languages of the Americas|American languages]]\n**    H84 [[Papuan languages]]\n*  H9 International Auxiliary Languages ([[Interlingua]], [[Ido (language)|Ido]], [[Esperanto]], etc.)\n\n=== I.  [[Literature]] ===\n*  I0 [[Literary Theory]]\n*  I1 Literature (worldwide)\n*  I2 Literature in China\n*  I3 Literature in Asia\n*  I4 Literature in Africa\n*  I5 Literature in Europe\n*  I6 Literature in Australasia\n*  I7 Literature in America\n\n=== J.  Art ===\n*  J0 Theory of [[Fine Art]]\n*  J1 Fine Art of the World\n*  J2 [[Painting]]\n*  J3 [[Sculpture]]\n*  J4 [[Photography]]\n*  J5 [[Applied arts]]\n*  J6 [[Music]]\n*  J7 [[Dance]]\n*  J8 [[Drama]]\n*  J9 [[Cinematography]], [[Television]]\n\n=== K.  [[History]] and [[Geography]] ===\n* K0 Historical Theory\n* K1 [[History of the World]]\n* K2 [[History of China]]\n* K3 [[History of Asia]]\n* K4 [[History of Africa]]\n* K5 [[History of Europe]]\n* K6 [[History of Australasia]]\n* K7 [[History of the Americas|History of America]]\n* K8 Biography, [[Archaeology]]\n* K9 Geography\n\n=== N.  [[Natural Science]] ===\n*  N0 Theory and Methodology\n*  N1 Present state\n*  N2 Organisations, Groups, Conferences\n*  N3 Research Methodology\n*  N4 Education and Popularization\n*  N5 Serials, Anthologies, Periodicals\n*  N6 Reference Materials\n*  N8 Field Surveys\n*  N9 Minor Sciences\n\n=== O.  Mathematics, Physics and Chemistry ===\n* O1 [[Mathematics]]\n* O2 [[Applied Mathematics]]\n* O3 [[Mechanics]]\n* O4 [[Physics]]\n* O6 [[Chemistry]]\n* O7 [[Crystallography]]\n\n=== P.  [[Astronomy]] and [[Geoscience]] ===\n*  P1 [[Astronomy]]\n*  P2 [[Geodesy]]\n*  P3 [[Geophysics]]\n*  P4 [[Meteorology]]\n*  P5 [[Geology]]\n*  P6 [[Mineralogy]]\n*  P7 [[Oceanography]]\n*  P9 [[Physiography]]\n\n=== Q.  [[Life Sciences]] ===\n*  Q1 [[Biology|General Biology]]\n*  Q2 [[Cell biology|Cytology]]\n*  Q3 [[Genetics]]\n*  Q4 [[Physiology]]\n*  Q5 [[Biochemistry]]\n*  Q6 [[Biophysics]]\n*  Q7 [[Molecular Biology]]\n*  Q8 [[Bioengineering]]\n*  Q9 [[Zoology]] and [[Botany]]\n\n=== R.  [[Medicine]] and [[Health Sciences]] ===\n*  R1 [[Preventive Medicine]], [[Public health]]\n*  R2 [[Traditional Chinese Medicine]]\n*  R3 [[Human anatomy]], [[Physiology]], [[Pathology]], [[Microbiology]], [[Parasitology]]\n*  R4 [[Clinical Medicine]]\n*  R5 [[Internal medicine]]\n*  R6 [[Surgery]]\n*  R7 [[:Category:Medical specialties|Medical Specialties]]\n** R71 [[Obstetrics]], [[Gynecology]]\n** R72 [[Pediatrics]]\n** R73 [[Oncology]]\n** R74 [[Neurology]], [[Psychiatry]]\n** R75 [[Dermatology]], [[Venereology]]\n** R76 [[Otolaryngology]]\n** R77 [[Ophthalmology]]\n** R78 [[Dentistry]]\n** R79 Non-Chinese [[Traditional medicine|Traditional Medicine]]\n*  R8 [[Radiology]], [[Sport medicine]], [[Diving medicine]], [[Aerospace medicine]]\n*  R9 [[Pharmacology]], [[Pharmacy]]\n\n=== S.  [[Agricultural Science]] ===\n*  S1 Fundamental Agricultural Science\n*  S2 [[Agricultural Engineering]]\n*  S3 [[Agronomy]]\n*  S4 [[Phytopathology]]\n*  S5 [[Crop|Individual Crops]]\n*  S6 [[Horticulture]]\n*  S7 [[Forestry]]\n*  S8 [[Animal Husbandry]], [[Veterinary medicine]], [[Hunting]], [[Sericulture]], [[Apiculture]]\n*  S9 [[Aquaculture]], [[Fishery]]\n\n=== T.  Industrial Technology ===\n*  TB General Industrial Technology\n*  TD [[Mining Engineering]]\n*  TE [[Petroleum]], [[Natural Gas]]\n*  TF [[Extractive metallurgy]], [[Smelting]]\n*  TG [[Metallurgy]], [[Metalworking]]\n*  TH [[Machinery]], [[Instrumentation]]\n*  TJ [[Military technology and equipment|Military Technology]]\n*  TK [[Power Plant]]\n*  TL [[Nuclear technology]]\n*  TM [[Electrical Engineering]]\n*  TN [[Electronic Engineering]], [[Telecommunication|Telecommunication Engineering]]\n*  TP [[Automation]], [[Computer Engineering]]\n*  TQ [[Chemical Engineering]]\n*  TS [[Light industry|Light Industry]], [[Handicraft]]\n*  TU [[Construction Engineering]]\n*  TV [[Water Resources]], [[Hydraulics|Hydraulic Engineering]]\n\n=== U.  [[Transportation]] ===\n*  U1 General [[Transport]]\n*  U2 [[Railway]] Transport\n*  U4 [[Highway]] Transport\n*  U6 [[Ship transport|Marine Transport]]\n\n=== V.  [[Aviation]] and Aerospace ===\n*  V1 Research and Exploration of Aviation and Aerospace Technology\n*  V2 Aviation\n*  V4 [[Aerospace]] ([[Spaceflight]])\n\n=== X.  [[Environmental Science]] ===\n*  X1 Fundamental Environmental Science\n*  X2 Environmental Research\n*  X3 [[Environmental Protection]] and Management\n*  X4 Disaster Protection\n*  X5 [[Pollution|Pollution Control]]\n*  X7 [[Waste Management]] and [[Recycling]]\n*  X8 Environmental Quality Monitoring\n*  X9 [[Occupational safety and health]]\n\n=== Z.  General Works ===\n*  Z1 Collectanea/Generalia ([[Book series]])\n** Z12 Collectanea of China\n*** Z121 General Collectanea\n**** Z121.2 Song Dynasty\n**** Z121.3 Yuan Dynasty\n**** Z121.4 Ming Dynasty\n**** Z121.5 Qing Dynasty\n**** Z121.6 Republic period\n**** Z121.7 Modern\n*** Z122 Collectanea of a particular locality\n*** Z123 Collectanea by members of a particular family\n*** Z124 Collectanea by individual writers\n*** Z125 Collectanea of lost books\n*** Z126 Collectanea of [[Chinese Classics]]\n**** Z126.1 Collection of [[Confucian Classics]]\n**** Z126.2 Collection of treatises\n***** Z126.21 General Collection\n***** Z126.22 Remake of lost books\n***** Z126.23 Collection of a particular theme\n***** Z126.24 [[Timeline of Chinese history|Chronological tables]], tablets, illustrated works\n***** Z126.25 Works on [[phonetics]], [[semantics]] and [[Verisimilitude|authenticity]]\n***** Z126.27 Research, critics and proves\n** Z13 Collectanea and Book series of Asia\n** Z14 Book series of Africa\n** Z15 Book series of Europe\n** Z16 Book series of Oceania\n** Z17 Book series of America\n*  Z2 [[Encyclopedia]] and Chinese Encyclopedia (Leishu)\n** Z22 Chinese Encyclopedia\n*** Z221 Tang Dynasty\n*** Z222 Song Dynasty\n*** Z223 Yuan Dynasty\n*** Z224 Ming Dynasty\n*** Z225 Qing Dynasty\n*** Z226 Republic\n*** Z227 Modern\n*** Z228 General popular Literature \n**** Z228.1 Children's book\n**** Z228.2 Popular Youth Book\n**** Z228.3 Elders' book\n**** Z228.4 Women's reader\n**** Z228.5 Men's reader\n** Z23 Encyclopedia of Asia\n** Z24 Encyclopedia of Africa\n** Z25 Encyclopedia of Europe\n** Z26 Encyclopedia of Oceania\n** Z27 Encyclopedia of America\n** Z28 Encyclopedia of a particular field\n*  Z3 [[Dictionary]]\n*  Z4 [[Symposium]], [[Anthologies]], Selected Works, [[Essay]]\n*  Z5 [[Almanac]]\n*  Z6 [[Serial (literature)|Serial]], [[Periodicals]]\n*  Z8 Catalogue, Abstract, Index\n\n== Other classifications ==\nThe other library classifications in China are:\n\n* [[Library Classification of the People’s University of China]] (LCPUC)\n* [[Library Classification of the Chinese Academy of Sciences]] (LCCAS)\n* [[Library Classification for Medium and Small Libraries]] (MSL)\n* [[Library Classification of Wuhan University]] (LCWU)\n\nThe other library classifications for Chinese materials outside mainland China are:\n* [http://www.lib.cam.ac.uk/mulu/class.html Cambridge University Library Chinese Classification System], Classification Scheme for Chinese Books devised by Profs. Haloun and P. van der Loon for Cambridge University, UK.\n* ''University of Leeds Classification of Books in Chinese, UK'' ([http://library.leeds.ac.uk/downloads/file/126/chinese 36 pages of Catalog in pdf])\n* [[Harvard-Yenching Classification]] System\n* [[New Classification Scheme for Chinese Libraries]] (commonly used in [[Taiwan]], [[Hong Kong]] and [[Macau]].)\n\n== See also ==\n* [[Libraries in the People's Republic of China]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* [http://clc.nlc.gov.cn/ Official website ]\n* [http://www.ifla.org/IV/ifla62/62-qiyz.htm Contemporary Classification Systems and Thesauri in China, Zhang Qiyu, Liu Xiangsheng, Wang Dongbo, 62nd IFLA General Conference - Conference Proceedings - August 25-31, 1996]\n* [http://www.nlc.gov.cn/old/old/newpages/english/org/clce.htm Chinese Library Classification Editorial Board]\n* [http://www.zju.edu.cn/jzus/download/clc.pdf Abridged third (obsolete) edition of CLC ]{{Zh icon}}\n* [http://www.33tt.com/tools/ztf/ CLC Online ]{{Zh icon}}\n* [http://journals.sfu.ca/dcpapers/2004/Paper_12.pdf Research on Interoperability of Metadata in Classification Schemes-construction of automatic mapping system between CLC and DDC, Jianbo Dai, Hanqing Hou, Ling Cao, Dept. of Libr. & Inform. Sci., Nanjing Agri. Univ., Nanjing, China 210095]\n* [ftp://ext-ftp.fao.org/GI/Agris/aims/publications/workshops/AOS_5/ppt/3-3.pdf Construction of Knowledge Base for Automatic Indexing and Classification based on CLC, Hanqing Hou, Chunxiang Xue, Nanjing Agri. Univ., Nanjing, China 210095]\n* [http://www.fao.org/Agris/AOS/ConferencesW/FifthAOS_China04/AOS_Proceedings/docs/4-1.pdf#search='chinese%20library%20classification' An Intelligent Retrieval System for Chinese Agricultural Literature indexed by Chinese Classification System, Ping Qian, Xiaolu Su, Chinese Academy of Agricultural Sciences, China]\n* [http://www.freewebs.com/yahnkim/East%20Asian%20Library%20Classification%20Systems%5B1%5D.doc East Asian Library Classification Systems], [http://archive.is/20121209181441/http://webcache.googleusercontent.com/search?q=cache:z3hxqowOTFoJ:www.freewebs.com/yahnkim/East%2520Asian%2520Library%2520Classification%2520Systems%255B1%255D.doc+chinese+library+classification&hl=en&gl=hk&ct=clnk&cd=410 archived]\n* [http://www.nii.ac.jp/publications/CJK-WS3/cjk3-04a.pdf The Development of Authority Database in National Library of China (NLC), March 2002, Beixin Sun of NLC] NLC's classification subject thesaurus database based on CLC.\n* [http://www.ifla.org/IV/ifla72/papers/109-Gu-en.pdf National Bibliographies: the Chinese Experience, 72nd IFLA Conference at Seoul in Korea, August 2006, Ben Gu of NLC] An overview of the current situation of the National Bibliography and classification systems in China.\n* [http://pubs.nrc-cnrc.gc.ca/jchla/jchla26/c05-018.pdf A month at the Shanghai Library, November 2004, Helen Michael, University of Toronto] A librarian from Canada shared her experience of working in a library of China.\n\n{{Library classification systems}}\n\n[[Category:1975 introductions]]\n[[Category:1975 establishments in China]]\n[[Category:Library cataloging and classification]]\n[[Category:Classification systems]]\n[[Category:Knowledge representation]]\n[[Category:Chinese culture]]"]
['Reification (knowledge representation)', '5662676', '{{multiple issues|{{refimprove|date=December 2009}}\n{{Expert-subject|Information Science|ex2=Cyc}}}}\n\n\n\'\'\'Reification\'\'\' in [[knowledge representation]] is the process of turning a predicate into an object.<ref>{{cite web |last=Hunt |first=Matthew |title=Notes on Semantic Nets and Frames |url=http://www.eecs.qmul.ac.uk/~mmh/AINotes/AINotes4.pdf |date=1996 |access-date=15 June 2016}}</ref> Reification involves the representation of factual assertions that are referred to by \'\'other\'\' assertions, which might then be manipulated in some way; e.g., comparing [[logical assertion]]s from different [[witness]]es in order to determine their [[credibility]].\n\nThe message "John is six feet tall" is an assertion involving truth that commits the speaker to its factuality, whereas the reified statement "Mary reports that John is six feet tall" defers such commitment to Mary. In this way, the statements can be incompatible without creating contradictions in [[reasoning]]. For example, the statements "John is six feet tall" and "John is five feet tall" are mutually exclusive (and thus incompatible), but the statements "Mary reports that John is six feet tall" and "Paul reports that John is five feet tall" are not incompatible, as they are both governed by a conclusive rationale that either Mary or Paul is (or both are), in fact, incorrect.\n\nIn Linguistics, reporting, telling, and saying are recognised as \'\'verbal processes that project a wording (or locution)\'\'. If a person says that "Paul told x" and "Mary told y", this person stated only that the telling took place. In this case, the person who made these two statements did not represent a person inconsistently. In addition, if two people are talking to each other, let\'s say Paul and Mary, and Paul tells Mary "John is five feet tall" and Mary rejects Paul\'s statement by saying "No, he is actually six feet tall", the socially constructed model of John does not become inconsistent. The reason for that is that statements are to be understood as an attempt to convince the addressee of something (Austin\'s How to do things with words), alternatively as a request to add some attribute to the model of Paul. The response to a statement can be an acknowledgement, in which case the model is changed, or it can be a statement rejection, in which case the model does not get changed. Finally, the example above for which John is said to be "five feet tall" or "six feet tall" is only incompatible because John can only be a single number of feet tall. If the attribute were a possession as in "he has a dog" or "he also has a cat", a model inconsistency would not happen. In other words, the issue of model inconsistency has to do with our model of the domain element (John) and not with the ascription of different range elements (measurements such as "five feet tall" or "six feet tall") nor with statements.\n\n==See also==\n*[[Reification (computer science)]]\n*[[Reification (fallacy)]]\n*[[Reification (linguistics)]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Reification (Knowledge Representation)}}\n[[Category:Knowledge representation]]']
['Reason maintenance', '849986', '{{more footnotes|date=September 2009}}\n\n\'\'\'Reason maintenance\'\'\'<ref name="insNouts">Doyle, J.: The ins and outs of reason maintenance.</ref><ref name="originalTR">Doyle, J.: Truth maintenance systems for problem solving. Tech. Rep. AI-TR-419,\nDep. of Electrical Engineering and Computer Science of MIT (1978)</ref> is a [[knowledge representation]] approach to efficient handling of inferred information that is explicitly stored. Reason maintenance distinguishes between base facts, which can be [[Defeasible reasoning|defeated]], and derived facts. As such it differs from [[belief revision]] which, in its basic form, assumes that all facts are equally important. Reason maintenance was originally developed as a technique for implementing problem solvers.<ref name="originalTR"/> It encompasses a variety of techniques that share a common architecture:<ref name="mcAllesterInterface">McAllester, D.A.: Truth maintenance. AAAI90 (1990)</ref> two components - a reasoner and a reason maintenance system - communicate with each other via an interface. The reasoner uses the reason maintenance system to record its inferences and justifications of ("reasons" for) the inferences. The reasoner also informs the reason maintenance system which are the currently valid base facts (assumptions). The reason maintenance system uses the information to compute the truth value of the stored derived facts and to restore consistency if an inconsistency is derived.\n\nA \'\'\'truth maintenance system\'\'\', or \'\'\'TMS\'\'\', is a [[knowledge representation]] method for representing both beliefs and their dependencies and an algorithm called the "truth maintenance algorithm" that manipulates and maintains the dependencies. The name \'\'truth maintenance\'\' is due to the ability of these systems to restore consistency.   \n\nIt is also termed as a belief revision system, a truth maintenance system maintains consistency between old believed knowledge and current believed knowledge in the knowledge base (KB) through revision. If the current believed statements contradict the knowledge in the KB, then the KB is updated with the new knowledge. It may happen that the same data will again come into existence, and the previous knowledge will be required in the KB. If the previous data is not present, it is required for new inference. But if the previous knowledge was in the KB, then no retracing of the same knowledge was needed. Hence the use of TMS to avoid such retracing; it keeps track of the contradictory data with the help of a dependency record. This record reflects the retractions and additions which makes the inference engine (IE) aware of its current belief set.\n\nEach statement having at least one valid justification is made a part of the current belief set. When a contradiction is found, the statement(s) responsible for the contradiction are identified and an appropriate is retraced. This process is called dependency-directed backtracking.\n\nThe TMS algorithm maintains the records in the form of a dependency network. The nodes in the network are one of the entries in the KB (a premise, antecedent, or inference rule etc.) Each arc of the network represent the inference steps from which the node was derived.\n\nA premise is a fundamental belief which is assumed to be always true. They do not need justifications. Considering premises are base from which justifications for all other nodes will be stated.\n\nThere are two types of justification for each node. They are:\n\n# Support List [SL]\n# Conditional Proof (CP)\n\nMany kinds of truth maintenance systems exist.   Two major types are single-context and multi-context truth maintenance.   \nIn single context systems, consistency is maintained among all facts in memory (database) and relates to the notion of consistency found in [[classical logic]]. Multi-context systems support [[paraconsistency]] by allowing consistency to be  relevant to a subset of facts in memory (a context) according to the history of logical inference.  This is achieved by tagging each fact or deduction with its logical history. Multi-agent truth maintenance systems perform truth maintenance across multiple memories, often located on different machines. de Kleer\'s assumption-based truth maintenance system (ATMS, 1986) was utilized in systems based upon [[AI winter#The fall of expert systems|KEE]] on the [[Lisp Machine]]. The first multi-agent TMS was created by Mason and Johnson. It was a multi-context system.  Bridgeland and Huhns created the first single-context multi-agent system.\n\n==See also==\n* [[Knowledge representation]]\n* [[Artificial intelligence]]\n* [[Belief revision]]\n* [[Knowledge acquisition]]\n\n==References==\n<references />\n\n==Other references==\n* Bridgeland, D. M. & Huhns, M. N.,  Distributed Truth Maintenance. Proceedings of. AAAI–90: Eighth National Conference on Artificial Intelligence, 1990.\n* J. de Kleer (1986). An assumption-based TMS. \'\'Artificial Intelligence\'\', 28:127–162.\n* J. Doyle. A Truth Maintenance System. AI. Vol. 12. No 3, pp.&nbsp;251–272. 1979.\n* U. Junker and K. Konolige (1990). Computing the extensions of autoepistemic and default logics with a truth maintenance system. In \'\'Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI\'90)\'\', pages 278–283. [[MIT Press]].\n* Mason, C. and Johnson, R. DATMS: A Framework for Assumption Based Reasoning, in Distributed Artificial Intelligence, Vol. 2, [[Morgan Kaufmann Publishers]], Inc., 1989.\n* D. A. McAllester. A three valued maintenance system. [[Massachusetts Institute of Technology]], Artificial Intelligence Laboratory. AI Memo 473. 1978.\n* G. M. Provan (1988). A complexity analysis of assumption-based truth maintenance systems. In B. Smith and G. Kelleher, editors, \'\'Reason Maintenance Systems and their Applications\'\', pages 98–113. Ellis Horwood, New York.\n* G. M. Provan (1990). The computational complexity of multiple-context truth maintenance systems. In \'\'Proceedings of the Ninth European Conference on Artificial Intelligence (ECAI\'90)\'\', pages 522–527.\n* R. Reiter and J. de Kleer (1987). Foundations of assumption-based truth maintenance systems: Preliminary report. In \'\'Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI\'87)\'\', pages 183–188. [http://www2.parc.com/spl/members/dekleer/Publications/Foundations%20of%20Assumption-Based%20Truth%20Maintenance%20Systems.pdf  PDF]\n\n==External links==\n* [http://scholar.google.com/scholar?q=Truth+maintenance+system&ie=UTF-8&oe=UTF-8&hl=en&btnG=Search Google Scholar on TMSs]\n* [http://plato.stanford.edu/entries/logic-ai/#3.2.1 Belief Revision and TMSs] at [[Stanford Encyclopedia of Philosophy]]\n\n[[Category:Belief revision]]\n[[Category:Knowledge representation]]\n[[Category:Information systems]]']
['Vivid knowledge', '25154733', "'''Vivid knowledge''' refers to a specific kind of [[knowledge representation]].\n\nThe idea of a '''vivid knowledge base''' is to get an interpretation mostly straightforward out of it &ndash; it implies the interpretation. Thus, any query to such a [[knowledge base]] can be reduced to a [[database]]-like query.\n\n== Propositional knowledge base ==\n\nA [[Propositional logic|propositional]] [[knowledge base]] KB is '''vivid''' ''iff'' KB is a [[Completeness (knowledge bases)|complete]] and [[consistency (knowledge bases)|consistent]] set of [[Literal (mathematical logic)|literals]] (over some vocabulary).<ref>Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 337</ref>\n\nSuch a knowledge base has the property that it as exactly one interpretation, i.e. the interpretation is unique. A check for entailment of a sentence can simply be broken down into its literals and those can be answered by a simple database-like check of KB.\n\n== First-order knowledge base ==\n\nA [[First-order logic|first-order]] knowledge base KB is '''vivid''' ''iff'' for some finite set of positive function-free ground literals KB<sup>+</sup>,\n\n: KB = KB<sup>+</sup> ∪ Negations ∪ DomainClosure ∪ UniqueNames,\n\nwhereby\n\n: Negations ≔ { ¬p | p is atomic and KB ⊭ p },\n: DomainClosure ≔ { (c<sub>i</sub> ≠ c<sub>j</sub>) | c<sub>i</sub>, c<sub>j</sub> are distinct constants },\n: UniqueNames ≔ { ∀x: (x = c<sub>1</sub>) ∨ (x = c<sub>2</sub>) ∨ ..., where the c<sub>i</sub> are all the constants in KB<sup>+</sup> }.\n\n<ref>Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 337</ref>\n\nAll interpretations of a vivid first-order knowledge base are isomorphic.<ref>Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 339</ref>\n\n== See also ==\n* [[Closed world assumption]]\n\n{{computable knowledge}}\n\n== References ==\n\n<references/>\n\n[[Category:Knowledge representation]]\n\n\n{{logic-stub}}\n{{database-stub}}"]
['Category:Ontology (information science)', '26259157', '{{Cat main|Ontology (information science)}}\n{{Commons category|Ontology}}\n\n[[Category:Knowledge representation]]\n[[Category:Metadata]]\n[[Category:Ontology|Information science]]']
['E-services', '202311', 'The concept of \'\'\'e-service\'\'\' (short for electronic service) represents one prominent application of utilizing the use of [[Information and communication technology|information and communication technologies]] (ICTs) in different areas. However, providing an exact definition of e-service is hard to come by as researchers have been using different definitions to describe e-service. Despite these different definitions, it can be argued that they all agree about the role of technology in facilitating the delivery of services which make them more of electronic services.\n\nIt seems compelling to adopt Rowley (2006)<ref name=autogenerated1>Rowley, J. (2006) An analysis of the e-service literature: towards a research agenda. Internet Research, 16 (3), 339-359</ref> approach who defines e-services as: “…deeds, efforts or performances whose delivery is mediated by information technology. Such e-service includes the service element of e-tailing, customer support, and service delivery”. This definition reflect three main components- service provider, service receiver and the channels of service delivery (i.e., technology). For example, as concerned to public e-service, public agencies are the service provider and citizens as well as businesses are the service receiver. The channel of service delivery is the third requirement of e-service. Internet is the main channel of e-service delivery while other classic channels (e.g. telephone, call center, public kiosk, mobile phone, television) are also considered.\n\n==Definitions and origin of the term e-service==\nSince its conceptual inception in the late 1980s in Europe{{Citation needed|date=October 2012}} and formal introduction in 1993 by the US Government,<ref>Alasem, A. (2009). An Overview of e-Government Metadata Standards and Initiatives based on Dublin Core. Electronic Journal of e-Government, 7(1), 1 – 10</ref> the term ‘[[E-Government]]’ has now become one of the recognized research domains especially in the context of public policy and now has been rapidly gaining strategic importance in public sector modernization.<ref>Wimmer, M., Codagnone, C. and Janssen, M. (2008) “Future of e-Government Research: 13 research themes identified in the eGovRTD2020 project’. Proceeding of the 41st Hawaii International Conference on System Sciences, USA</ref> E-service is one of the branches of this domain and its attention has also been creeping up among the practitioners and researchers.<ref>Lӧfstedt, U. (2005) ‘Assessment of current research and some proposals for future direction’, International Journal of Public IS</ref>\n\nE-service (or eservice) is a highly generic term, usually referring to ‘The provision of services via the Internet (the prefix \'e\' standing for ‘electronic’, as it does in many other usages), thus e-Service may also include e-Commerce, although it may also include non-commercial services (online), which is usually provided by the government.’ (Irma Buntantan & G. David Garson, 2004: 169-170; Muhammad Rais & Nazariah, 2003: 59, 70-71).\n\n\'E-Service constitutes the online services available on the Internet, whereby a valid transaction of buying and selling (procurement) is possible, as opposed to the traditional websites, whereby only descriptive information are available, and no online transaction is made possible.\' (Jeong, 2007).<ref>Jeong Chun Hai @Ibrahim. (2007). \'\'Fundamental of Development Administration.\'\' Selangor: Scholar Press. ISBN 978-967-5-04508-0</ref>\n\n==Importance of E-service ==\nLu (2001)<ref>Lu, J. (2001). Measuring cost/benefits of e-business applications and customer satisfaction”, Proceedings of the 2nd International Web Conference, 29–30 November, Perth, Australia, 139-47</ref> identifies a number of benefits for e-services, some of these are:\n\n* Accessing a greater customer base\n* Broadening market reach\n* Lowering of entry barrier to new markets and cost of acquiring new customers\n* Alternative communication channel to customers\n* Increasing services to customers\n* Enhancing perceived company image\n* Gaining competitive advantages\n* Potential for increasing [[Customer knowledge]]\n\n==Importance and advantages of E-shopping==\n*E-shops are open 24 hours a day.\n*There is no need to travel to the malls or wait at the checkout counters.\n*There is usually a wide selection of goods and services.\n*It is easy to compare prices and quality by using the E-shopping tool.\n*Price reduction and discounts are electronically conveyed.\n\n==E-service domain==\nThe term ‘e-service’ has many applications and can be found in many disciplines. The two dominant application areas of e-services are\n\nE-business (or e-commerce): e-services mostly provided by businesses or [NGO|non-government organizations] (NGOs) (private sector).\n\nE-government: e-services provided by government to citizens or business (public sector is the supply side). The use and description of the e-service in this page will be limited to the context of e-government only where of the e-service is usually associated with prefix “public”: Public e-services. In some cases, we will have to describe aspects that are related to both fields like some conferences or journals which cover the concept of “e-Service” in both domains of e-government and [[e-business]].\n\n==Architecture==\nDepending on the types of services, there are certain functionalities required in the certain layers of e-service architectural framework, these include but are not limited to – Data layer (data sources), processing layers (customer service systems, management systems, data warehouse systems, integrated customer content systems), exchange layer ([[Enterprise application integration|Enterprise Application Integration]]– EAI), interaction layer ( integrating e-services), and presentation layer (customer interface through which the web pages and e-services are linked).\n\n==E-service quality==\nMeasuring [[service quality]] and service excellence are important in a competitive organizational environment. The [[SERVQUAL]]- service quality model is one of the widely used tools for measuring quality of the service on various aspects. The five attributes of this model are: reliability, responsiveness, assurance, tangibles, and empathy. The following table summarizes some major of these:\n\n{| class="sortable wikitable"\n|-\n!\'\'\'SERVQUAL<ref>Jiang, J.J.; Klein, G. and Crampton, S.M. (2000). A note on SERVQUAL reliability and validity in information system service quality measurement. Decision Sciences. Atlanta: Summer 2000. Vol. 31, Iss. 3; p. 725</ref>\'\'\'\n!\'\'\'Kaynama & Black (2000)\'\'\'<ref>Kaynama, S. A., and Black, C. I.  (2000). A Proposal to assess the Service Quality of Online Travel Agencies: An Exploratory Study. Journal of Professional Services Marketing (21:1), 63-88</ref>\n!\'\'\'Zeithaml (2002)\'\'\'<ref>Zeithaml, V. A. (2002). Service Excellence in Electronic Channels. Managing Service Quality (12:3), 2002, 135-138</ref>\n!\'\'\'Janda et al. (2002)\'\'\'<ref>Janda, S., Trocchia, P. J., and Gwinner, K. (2002). Consumer perceptions of Internet Retail Service Quality. International Journal of Service Industry Management (13:5),  412-431</ref>\n!\'\'\'Alawattegama & Wattegama (2008)\'\'\'<ref>Alawattegama, L. and Wattegama, C. (2008). Benchmarking Asia Pacific National Telecom Regulatory Authority Websites. LIRNEasia</ref>\n|-\n|| Reliability      ||Content      ||Access      ||Access      || Factual information\n|-\n||Responsiveness      ||Access      ||Ease of navigation      ||Security     || Business information\n|-\n||Assurance      ||Navigation      ||Efficiency     ||Sensation        || General information\n|-\n||Tangibles      ||Design      ||Flexibility      ||Information/content      || Consumer‐ related information\n|-\n||Empathy      ||Response      ||Reliability ||     ||\n|-\n||     ||Background      ||Personalization      ||     ||\n|-\n||     ||Personalization     ||Security/privacy      ||     ||\n|-\n||     ||     ||Responsiveness      ||     ||\n|-\n||     ||     ||Assurance/trust     ||     ||\n|-\n||     ||     ||Site aesthetics      ||     ||\n|-\n||     ||    ||Price knowledge      ||     ||\n|}\n\nThe [[LIRNEasia]] study on benchmarking national telecom regulator websites focuses on content than on accessibility and ease of use, unlike the other studies mentioned here. Websites are increasingly important portals to government agencies, especially in the context of [[information society]] reforms. Stakeholders, including businesses, investors and even the general public, are interested in information produced by these agencies, and websites can help to increase their transparency and accountability. The quality of its website also demonstrates how advanced a regulatory agency is.\n\n==E-service cost factor==\nSome major cost factors are (Lu, 2001):<ref>Lu, J. (2001). Measuring cost/benefits of e-business applications and customer\nsatisfaction”, Proceedings of the 2nd International Web Conference, 29–30 November, Perth, Australia, 139-47</ref>\n\n* Expense of setting up applications\n* Maintaining applications\n* Internet connection\n* Hardware/software\n* Security concerns\n* legal issues\n* Training; and\n* Rapid technology changes\n\n==Practical examples of e-services in the Developing World==\nInformation technology is a powerful tool for accelerating economic development. Developing countries have focused on the development of ICT during the last two decades and as a result, it has been recognized that ICT is critical to economy and is as a catalyst of economic development. So, in recent years there seems to have been efforts for providing various e-services in many developing countries since ICT is believed to offer considerable potential for the sustainable development of e-Government and as a result, e-Services.<ref name=autogenerated4>Ndou,V.(2004)E-Government for developing countries: Opportunities and Challenges, EJISDC 18, 1, 1-24</ref>\n\nMany government agencies in developed countries have taken progressive steps toward the web and ICT use, adding coherence to all local activities on the Internet, widening local access and skills, opening up interactive services for local debates, and increasing the participation of citizens on promotion and management of the territory(Graham and Aurigi, 1997).<ref>Graham, S. and Aurigi, A. (1997) Virtual Cities, Social Polarisation, and the Crisis in Urban Public Space, Journal of Urban Technology, 4, 1, 19-52</ref>\n\nBut the potential for e-government in developing countries remains largely unexploited, even though. ICT is believed to offer considerable potential for the sustainable development of e-government. Different human, organizational and technological factors,\nissues and problems pertain in these countries, requiring focused studies and appropriate approaches. ICT, in general, is referred to as an “enabler”, but on the other hand it should also be regarded as a challenge and a peril in itself. The organizations, public or private,which ignore the potential value and use of ICT may suffer pivotal competitive disadvantages. Nevertheless, some e-government initiatives have flourished in developing countries too, e.g. Brazil, India, Chile, etc.<ref name=autogenerated4 /> What the experience in these countries shows, is that governments in the developing world can effectively exploit and appropriate the benefits of ICT, but e-government success entails the accommodation of certain unique conditions,needs and obstacles. The adaptive challenges of e-government go far beyond technology, they call for organizational structures and skills, new forms of leadership, transformation of public-private partnerships (Allen et al., 2001).<ref>Allen, A.B., Juillet, L., Paquet, G. and Roy, J. (2001) E-Governance and Government Online in Canada: Partnerships, People and Prospects, Government Information Quarterly,18, 93-104.)</ref>\n\nFollowing are a few examples regarding e-services in some developing countries:\n\n===E-services in Rwanda===\nOnly a decade after emerging from the fastest genocide of the 20th Century, Rwanda, a small country in Eastern Central Africa,\nhas become one of the continent’s leaders in, and model on, bridging the digital divide through e-government. Rwanda has undergone a rapid turnaround from one of the most technologically deficient countries only a decade ago to a country\nwhere legislative business is conducted online and wireless access to the Internet is available anywhere in the country. This is\npuzzling when viewed against the limited progress made in other comparable developing countries, especially those located in the\nsame region, sub-Saharan Africa, where the structural and institutional constraints to e-government diffusion are similar.<ref>Mawangi, W.(2006) The social relations of e-government diffusion in developing countries: the case of Rwanda, Proceedings of the 2006 international conference on Digital government research, May 21–24, 2006, San Diego, California</ref>\n\n===E-services in South Africa===\nIn South Africa, there continues to be high expectations of government in respect to improved delivery of service and of closer consultation with citizens. Such expectations are not unique to this country, and in this regard there is a need for governments to recognise that the implementation of e-government systems and e-services affords them the opportunity to enhance service delivery and good governance.<ref name=autogenerated3>van Brakel, P.A.(2009) Proceedings of the 11th Annual Conference on World Wide Web Applications, Port Elizabeth, 2–4 September</ref> The implementation of e-Government has been widely acclaimed in that it provides new impetus to deliver services quickly and efficiently (Evans & Yen, 2006:208).<ref>Evans, D. & Yen, D. C. 2006. e-Government: evolving relationship of citizens and government, domestic, and international development. Government Information Quarterly, 23(2): 207-235.)</ref> In recognition of these benefits, various arms of the South African government have embarked on a number of e-government programmes for example the [http://www.gov.za/  Batho Pele portal], [[Sars efiling|SARS e-filing]], the [http://www.enatis.com/  e-Natis system], electronic processing of grant applications from remote sites, and a large number of departmental information websites. Also a number of well publicised e-government ventures such as the latter, analysts and researchers consider the state of e-government in South Africa to be at rudimentary stages. There are various factors\nwhich collectively contribute to such an assessment. Amongst these, key factors relate to a lack of a clear strategy to facilitate uptake and adoption of e-government services as well as evaluation frameworks to assess expectations of citizens who are one of the primary user groups of these services.<ref name=autogenerated3 />\n\n===E-services in Malaysia===\nE-Services is one of the pilot projects under the Electronic Government Flagship within the Multimedia Super Corridor ([http://www.mscmalaysia.my/  MSC]) initiative. With E-Services, one can now conduct transactions with Government agencies, such as the Road Transport Department (RTD) and private utility companies such as Tenaga Nasional Berhad ([http://www.tnb.com.my  TNB]) and Telekom Malaysia Berhad ([http://www.tm.net.my  TM]) through various convenient channels such as the eServices kiosks and internet. No more queuing, traffic jams or bureaucratic hassles and one can now conduct transaction at one’s own convenience. Also, Electronic Labour Exchange ([http://www.elx.gov.my  ELX])is one stop-centre for labor market information, as supervised by the Ministry of Human Resource ([http://www.mohr.gov.my  MOHR]), to enable employers and job seekers to communicate on the same platform.\n\n[http://www.esyariah.gov.my/  e-Syariah] is the seventh project under the Electronic Government flagship application of the Multimedia Super Corridor (MSC). A case management system that integrates the processes related to management of cases for the Syariah Courts.\n\n==Challenges to e-services in the Developing World==\nThe future of e-service is bright but some challenges remain. There are some challenges in e-service, as Sheth & Sharma (2007)<ref>Sheth., J.N. , Sharma, A., (2007). E-Services: A framework for growth.  Journal of Value Chain Management, 1(1/2)</ref> identify, are:\n\n* Low penetration of ICT especially in the developing countries;\n* [[Internet fraud|Fraud]] on the internet space which is estimated around 2.8billion USD\n* [[Internet privacy|Privacy]] due the emergence of various types of spyware and security holes, and\n* intrusive characteristics of the service (e.g. mobile phones based) as customers may not like to be contacted with the service providers at any time and at any place.\n\nThe first challenge and primary obstacle to the e-service platform will be penetration of the internet. In some developing countries, the access to the internet is limited and speeds are also limited. In these cases firms and customers will continue to use traditional platforms. The second issue of concern is fraud on the internet. It is anticipated that the fraud on the e-commerce internet space costs $2.8 billion. Possibility of fraud will continue to reduce the utilization of the internet. The third issue is of privacy. Due to both spyware and security holes in operating systems, there is concern that the transactions that consumers undertake have privacy limitations.  For example, by stealthily following online activities, firms can develop fairly accurate descriptions of customer profiles.  Possibility of privacy violations will reduce the utilizations of the internet. The final issue is that e-service can also become intrusive as they reduce time and location barriers of other forms of contract. For example, firms can contact people through mobile devices at any time and at any place. Customers do not take like the intrusive behavior and may not use the e-service platform. (Heiner and lyer, 2007)<ref>Heiner and lyer (2007) E-Service opportunities and Threats, Journal of value chain management, 1, 11.</ref>\n\n==Major e-service keywords==\nA considerable amount of research efforts already exists on the subject matter exploring different aspects of e-service and e-service delivery ; one worth noting effort is Rowley’s study (2006)<ref name=autogenerated1 /> who did a review study on the e-service literature. The key finding of his study is that there is need to explore dimensions of e-service delivery not focusing only on service quality “In order to understand e-service experiences it is necessary to go beyond studies of e-service quality dimensions and to also take into account the inherent characteristics of e-service delivery and the factors that differentiate one service experience from another.”\n\nSome of the major keywords of e-service as found in the e-government research are as follows:\n\n===Acceptance===\nUser acceptance of technology is defined according to Morris (1996, referred by Wu 2005, p.&nbsp;1)<ref>Wu, Philip F. (2009). User Acceptance of Emergency Alert Technology: A Case Study. Proceedings of the 6th International ISCRAM Conference – Gothenburg, Sweden</ref> as “the demonstrable willingness within a user group to employ information technology for the tasks it is designed to support”. This definition can be brought into the context of e-service where acceptance can be defined as the users’ willingness to use e-service or the willingness to decide when and how to use the e-service.\n\n===Accessibility===\nUsers’ ability to access to the e-service is important theme in the previous literature. For example, Huang (2003)<ref>Huang, C.J. (2003). Usability of E-Government Web Sites for People with Disabilities, In Proceedings of the 36th Hawaii International Conference on System Sciences (HICSS’03), IEEE Computer Society, 2003</ref> finds that most of the websites in general fail to serve users with disabilities. Recommendation to improve accessibility is evident in previous literature including Jaeger (2006)<ref>Jaeger, P.T. Assessing Section 508 compliance on federal e-government Web sites: A multi-method, user-centered evaluation of accessibility for persons with disabilities. Government Information Quarterly 23 (2006) 169–190</ref> who suggests the following to improve e-services’ accessibility like: design for accessibility from the outset of website development, Involve users with disabilities in the testing of the site …Focus on the benefits of an accessible Web site to all users.\n\n===Administrative literacy===\nAccording to Grönlund et al. (2007),<ref>Grönlund, Å., Hatakka, M. and Ask, A. (2007) ‘ Inclusion in the E-Service Society – Investigating Administrative Literacy Requirements for Using E-Services’. 6th International Conference (EGOV 2007, Regensburg, Germany), 4656</ref> for a simple e-service, the needs for knowledge and skills, content and procedures are considerably less. However, in complicated services there are needed to change some prevailed skills, such as replacing verbal skills with skill in searching for information online.\n\n===Benchmarking===\nThis theme is concerned with establishing standards for measuring e-services or the best practices within the field. This theme also includes the international benchmarking of e-government services (UN reports, EU reports); much critic has been targeting these reports being incomprehensive and useless. According to Bannister (2007)<ref>Bannister F. (2007). The curse of the benchmark: an assessment of the validity and value of e-government comparisons, International Review of Administrative Sciences, 73 (2), 171-188</ref> “… benchmarks are not a reliable tool for measuring real e-government progress. Furthermore, if they are poorly designed, they risk distorting government policies as countries may chase the benchmark rather than looking at real local and national needs”\n\n===Digital divide===\n[[Digital divide]] is considered one of the main barriers to implementing e-services; some people do not have means to access the e-services and some others do not know how to use the technology (or the e-service). According to Helbig et al. (2009),<ref>Helbig, N; Gil-García, J ; Ferro, E (2009). Understanding the complexity of electronic government: Implications from the digital divide literature. Government Information Quarterly, 26(2009), 89–97</ref> “we suggest E-Government and the digital divide should be seen as complementary social phenomena (i.e., demand and supply). Moreover, a serious e-government digital divide is that services mostly used by social elites."\n\n===E-readiness===\nMost of the reports and the established criteria focus on assessing the services in terms of infrastructure and public policies ignoring the citizen participation or [[e-readiness]]. According to by Shalini (2009),<ref>Shalini, R. (2009). Are Mauritians ready for e-Government services?. Government Information Quarterly 26 (2009) 536–539</ref> “the results of the research project reveal that a high [http://www.eiu.com/site_info.asp?info_name=eiu_2007_e_readiness_rankings&rf=0|e-readiness index] may be only indicating that a country is e-ready in terms of ICT infrastructure and info-structure, institutions, policies, and political commitment, but it is a very poor measure of the e-readiness of citizens. To summarize the findings, it can be said that Mauritius is ready but the Mauritians are not”\n\n``E-readiness, as the Economist Intelligence Unit defines, is the measure of a country’s ability to leverage digital channels for communication, commerce and government in order to further economic and social development. Implied in this measure is the extent to which the usage of communications devices and Internet services creates efficiencies for business and citizens, and the extent to which this usage is leveraged in the development of information and communications technology (ICT) industries. In general terms, the definition of e-readiness is relative,for instance depending on a country in question\'s priorities and perspective.<ref>GeoSINC International (2002). E-Readiness Guide. Available at http://www.apdip.net/documents/evaluation/e-readiness/geosinc01042002.pdf</ref>\n\n===Efficiency===\nAs opposed to effectiveness, efficiency is focused on the internal competence within the government departments when delivering e-services. There is a complaint that researchers focus more on effectiveness “There is an emerging trend seemingly moving away from the efficiency target and focusing on users and governance outcome. While the latter is worthwhile, efficiency must still remain a key priority for eGovernment given the budget constraints compounded in the future by the costs of an ageing population. Moreover, efficiency gains are those that can be most likely proven empirically through robust methodologies”<ref>Codagnone, C.  Undheim T.A (2008).  Benchmarking eGovernment: tools, theory, and practice. European Journal of ePractice. Nº 4 • August 2008</ref>\n\n===Security===\nSecurity is the most important challenge that faces the implementation of e-services because without a guarantee of privacy and security citizens will not be willing to take up e-government services. These security concerns, such as hacker attacks and the theft of credit card information, make governments hesitant to provide public online services.  According to the GAO report<ref>GAO.(2002). E-Government: Proposal addresses Critical Challenges. U.S General Accounting Office, Govt of the USA</ref> of 2002 “security concerns present one of the toughest challenges to extending the reach of e-government.The rash of hacker attacks, Web page defacing, and credit card information being posted on electronic bulletin boards can make many federal agency officials—as well as the general public—reluctant to conduct sensitive government transactions involving personal or financial data over the Internet.” By and Large,  Security is one of the major challenges that faces the implementation and development of electronic services. people want to be assured that they are safe when they are conducting online services and that their information will remain secure and confidential\n\n===Stakeholders===\nAxelsson et al. (2009)<ref>Axelsson, K, Melin, f, Lindgren, I, (2009) DEVELOPING PUBLIC E-SERVICES FOR SEVERAL STAKEHOLDERS – A MULTIFACETED VIEW OF THE NEEDS FOR AN E-SERVICE. 17th European Conference on Information Systems</ref> argue that the stakeholder concept-which was originally used in private firms-, can be used in public setting and in the context of e-government. According to them, several scholars have discussed the use of the [[stakeholder theory]] in public settings.<ref>Scholl, H. J. (2001). Applying stakeholder theory to e-government: Benefits and Limits. Kluwer Academic Publishers, Massachusetts</ref> The stakeholder theory suggests that need to focus on all the involved stakeholder s when designing the e-service; not only on the government and citizens.\n\n===Usability===\nCompared to Accessibility, There is sufficient literature that addresses the issue of usability; researchers have developed different models and methods to measure the usability and effectiveness of eGovernment websites. However, But still there is call to improve these measures and make it more compressive<ref>Kaylor, C.,  Deshazo, R. & Eck, D. V. (2001) "Gauging e-government: A report on implementing services among American cities". Government Information Quarterly (GIQ), 18(4), 293 - 307</ref>\n\n``The word usability has cropped up a few times already in this unit. In the context of biometric identification, usability referred to the smoothness of enrollment and other tasks associated with setting up an identification system. A system that produced few false matches during enrollment of applicants was described as usable. Another meaning of usability is related to the ease of use of an interface. Although this meaning of the term is often used in the context of computer interfaces, there is no reason to confine it to computers.<ref>[http://openlearn.open.ac.uk/mod/resource/view.php?id=211245 Open Learning - OpenLearn - Open University]</ref>´´\n\n==Social, cultural and ethical implications of e-services==\nThe perceived effectiveness of e-service can be influenced by public’s view of the social and cultural implications of e-technologies and e-service.\n\nImpacts on individuals’ rights and privacy – as more and more companies and government agencies use technology to collect, store, and make accessible data on individuals, privacy concerns have grown. Some companies monitor their employees\' computer usage patterns in order to assess individual or workgroup performance.<ref>Asgarkhani, M. (2002). Strategic Management of Information systems and Technology in an e-World”, Proceedings of the 21st IT Conference, Sri Lanka, pp103-111.</ref> Technological advancements are also making it much easier for businesses, government and other individuals to obtain a great deal of information about an individual without their knowledge. There is a growing concern<ref name=autogenerated2>Asgarkhani, M. (2002b) “e-Governance in Asia Pacific”, Proceedings of the International Conference on Governance in Asia, Hong Kong.</ref> that access to a wide range of information can be dangerous within politically corrupt government agencies.\n\nImpact on Jobs and Workplaces - in the early days of computers, management scientists anticipated that computers would replace human decision-makers. However, despite significant technological advances, this prediction is no longer a mainstream concern. At the current time, one of the concerns associated with computer usage in any organization (including governments) is the health risk – such as injuries related to working continuously on a computer keyboard. Government agencies are expected to work with regulatory groups in order to avoid these problems.\n\nPotential Impacts on Society – despite some economic benefits of ICT to individuals, there is evidence that the computer literacy and access gap between the haves and have-nots may be increasing. Education and information access are more than ever the keys to economic prosperity, yet access by individuals in different countries is not equal - this social inequity has become known as the digital divide.\n\nImpact on Social Interaction – advancements in ICT and e-Technology solutions have enabled many government functions to become automated and information to be made available online. This is a concern to those who place a high value on social interaction.\n\nInformation Security - technological advancements allow government agencies to collect, store and make data available online to individuals and organizations. Citizens and businesses expect to be allowed to access data in a flexible manner (at any time and from any location). Meeting these expectations comes at a price to government agencies where it concerns managing information – more specifically, ease of access; data integrity and accuracy; capacity planning to ensure the timely delivery of data to remote (possibly mobile) sites; and managing the security of corporate and public information.<ref name=autogenerated2 />\n\n==E-service awards==\nThe benefits of e-services in advancing businesses efficiency and in promoting good governance are huge; recognizing the importance of these benefits has resulted in number of international awards that are dedicated to recognize the best designed e-services. In the section, we will provide description of some international awards\n\n===Best online e-service in Europe===\n[http://www.epractice.eu/en/awards|The European eGovernment Awards program] started 2003 to recognize the best online public service in Europe. The aim of Awards is to encourage the deployment of e-services and to bring the attention to best practices in the field. The winners of the [http://ec.europa.eu/idabc/en/document/7842|4th European eGovernment Awards] were announced in the award ceremony that took place at the [http://www.egov2009.se/ 5th Ministerial eGovernment Conference] on 19 November 2009 (Sweden); the winners in their respective categories are:\n\n* Category 1. eGovernment supporting the Single Market: EU-OPA, the European Order for Payment Application ({{flag|Austria}} and {{flag|Germany}})\n* Category 2a. eGovernment empowering citizens: Genvej ({{flag|Denmark}})\n* Category 2b. eGovernment empowering businesses: MEPA, the Public Administration eMarketplace ({{flag|Italy}})\n* Category 3. eGovernment enabling administrative efficiency and effectiveness: Licensing of Hunters via the “Multibanco” ATM Network ({{flag|Portugal}})\n* Public prize: SMS Information System ({{flag|Turkey}})\n\n===Other awards===\n\n[http://www.ita.gov.om/HMAward/ Sultan Qaboos Award for excellence in eGovernance] {{flag|Oman}}(Started 2009) The award has five categories: Best eContent, Best eService, Best eProject, eEconomy, eReadiness.\n\n[http://www.egovawards.bh/AboutEn.aspx?Id=1|Bahrain eGovernment Excellence Awards] {{flag|Bahrain}}(Started 2007) The program has three categories: Government Awards: Best eContent, Best eService, Best eProject, eEconomy, eEducation, eMaturity Business Awards: Best ICT solution Provider, eEconomy,eEducation Citizen Awards: Best eContent, eCitizen.\n\n[http://www.e-servicesphils.com/esp2010/ Philippines e-Service Awards] {{flag|Philippines}}(Started 2001) Categories: Outstanding Client Application of the Year, Outstanding Customer Application of the year, Groundbreaking Technology of the Year, Most Progressive Homegrown Company of the Year.\n\n==Major journals focusing on e-services==\nThere are some journals particularly interested for “e-Service “. Some of these are:\n* [http://www.igi-global.com/journal/international-journal-services-mobile-applications/1114  International Journal of E-services and Mobile Applications]\n* [http://eservicejournal.org/ eService Journal]\n* [http://www.palgrave-journals.com/ejis/index.html  European Journal of Information Systems]\n* [http://www.misq.org/  MIS Quarterly]\n* [http://www.elsevier.com/wps/find/journaldescription.cws_home/505553/description#description  Information & Management]\n* [http://www.wiley.com/bw/journal.asp?ref=1350-1917&site=1  Information Systems Journal]\n* [http://www.igi-global.com/Bookstore/TitleDetails.aspx?TitleId=1091  International Journal of Electronic Government]\n* [http://www.ejeg.com/  Electronic Journal of e-Government]\n* [http://www.gvsu.edu/business/ijec/  International Journal of Electronic Commerce]\n* [http://www.emeraldinsight.com/Insight/viewContainer.do?containerType=Journal&containerId=11229  Internet Research]\n* [http://www.palgrave-journals.com/jit/index.html  Journal Information Technology]\n* [http://www.elsevier.com/wps/find/journaldescription.cws_home/525447/description#description  Journal of Strategic Information Systems]\n* [http://aisel.aisnet.org/jais/  Journal of the Association for Information Systems]\n* [http://www.elsevier.com/wps/find/journaldescription.cws_home/620202/description#description  Government Information Quarterly]\n* [http://www.wiley.com/bw/journal.asp?ref=0033-3352  Public Administration Review]\n\n==Major conferences focusing on e-services==\n\nMajor conferences considering e-service as one of the themes are:\n* [http://ec.europa.eu/information_society/newsroom/cf/itemdetail.cfm?item_id=5649&utm_campaign=isp&utm_medium=rss&utm_source=newsroom&utm_content=tpa-8  eServices in European Civil Registration conference]\n* [http://www.iist.unu.edu/I3E/IFIP  Conference on e-Business, e-Services, and e-Society]\n* [http://www.eafricaconference.org/  International ICST Conference on e-service]\n* [http://www.e-servicesphils.com/esp2010/  E-service Global Sourcing Conference & Exhibition]\n* [http://www.hicss.hawaii.edu/hicss_43/minitracks/eg-sin.htm  Annual Hawaii International Conference on Systems Sciences]\n* [http://www.egov-conference.org/egov-2011-preview  Electronic Government Conference (EGOV)]\n* [http://www.dexa.org/  International Conference on Electronic Government and the Information Systems Perspective (EGOVIS)]\n* [http://www.icegov.org/  International Conference on Theory and Practice of Electronic Governance ( ICEGOV)]\n\n==See also==\n* [[Electronic services delivery]]\n* [[Customer knowledge]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.e-govwatch.org.nz/criteria/e-services_delivery.html  E-services delivery]\n* [http://www.computerworld.com/s/article/9005371/Report_Card_The_Best_E_Government_Sites  The Best E-Government Sites]\n* [http://egov.infodev.org/en/Section.78.html#citizen-or-business-centric-portals  The World Bank (InfoDev) e-Government toolkit]\n\n[[Category:Digital divide]]\n[[Category:E-commerce]]\n[[Category:Information technology]]\n[[Category:Knowledge representation]]\n[[Category:Open government]]\n[[Category:Technology in society]]']
['FAO Country Profiles', '24515769', '{{ infobox software\n| name                   = FAO Country Profiles \n| logo                   = [[File:FAO countryprofiles logo.jpg]]\n| caption                = \n| developer              = [[FAO]] of the [[United Nations]]\n| latest_release_version = 2012\n| latest_release_date    = 2001\n| operating_system       = \n| genre                  = [[Knowledge Representation]], [[Ontology]] Editor\n| website                = [http://www.fao.org/countryprofiles/ FAO Country Profiles]\n}}\n\nThe [[FAO]] [[Country]] Profiles is a [[multilingual]]<ref>Arabic, Chinese, English, French, Russian and Spanish are the languages of the Organization. See FAO\'s Basic texts http://www.fao.org/docrep/010/k1713e/k1713e02b.htm#47. The FAO Country Profiles system provides information in Arabic, Chinese, English, French and Spanish. Russian is in preparation.</ref> [[web portal]] which repackages the [[Food and Agriculture Organization]] of the United Nations (FAO) vast archive of information on its global activities in [[agriculture]] and [[food security]] in a single area and catalogues it exclusively by [[country]] and thematic areas.\n\nThe portal\'s purpose is to offer decision-makers, [[researchers]] and project formulators around the world a fast and reliable way to access country-specific information on national [[food security]] situations without the need to search individual [[databases]] and [[systems]]. It gives added-value to [[FAO]]\'s wealth of information by providing an easy-to-use [[User interface|interface]] containing [[interactive]] [[maps]] and [[charts]].<ref>For reviews of the [[FAO]] Country profiles initiatives, please see the [http://news.eoportal.org/didyouknow/080226_did2.html Sharing Earth and Observations Resources portal], [http://www.sciencecentral.com/category/962945 Science Central], [http://www.scinet.cc/dir/Science/Agriculture/ SciNet Science & Technology Search, News, Articles], etc.</ref>\n\n== Background ==\n\n[[FAO]] has always highlighted [[information]] and [[Knowledge sharing]] as priority areas in fighting [[hunger]] and achieving [[food security]].<ref>See ARTICLE I of FAO Constitution: The Organization shall collect, analyze, interpret, and disseminate information relating to nutrition, food and agriculture. http://www.fao.org/docrep/x5584E/x5584e0i.htm</ref> In this context, [[FAO]] identified that [[countries]] could improve their national programmes on [[agriculture]] and [[food security]] if they could access [[FAO]]\'s information through a cross-sectoral (or [[interdisciplinary]]) country-based approach.<ref>Programme of Work and Budget 2002–2003:  http://www.fao.org/docrep/meeting/003/y1194e/y1194e06b.htm#P11324_311453</ref><ref>Programme of Work and Budget 2004–2005: http://www.fao.org/DOCREP/MEETING/006/y9859e/Y9859e07a.htm#P10820_371793</ref> However, despite the existence of a large number of country-based [[information systems]] in FAO, the information managed by the various systems lacked [[System integration|integration]]. Information tended to be generated and used in a circumscribed manner and tailored to a specific system, department or [[sector (economic)|sector]].\n\nThe [http://www.fao.org/countryprofiles/ FAO Country Profiles] portal, initially called FAO Country Profiles and Mapping Information System, was launched in 2002 responding to the Organization’s need to provide [[FAO]] web site’s users an easy to use mechanism to find FAO country-specific [[information]] without the need to [[Search engine technology|search]] individual [[FAO]] [[web sites]], [[databases]] or [[systems]]. The system was designed to integrate [[Scientific modelling|analytical]] and [[multilingual]] information with thematic databases and [[Digital data|digital]] [[map]] [[Disciplinary repository|repositories]] and to facilitate access to information on multiple factors contributing to national [[food insecurity]].\n\nSince its launch, the system has grown by incorporating more and more [[FAO Country Profiles#Data sources|data sources]]. This was achieved thanks to a [[corporate]] effort to reduce [[information silo]]s and the adoption of [[international standards]] for country-based [[information management]] throughout the Organization.\n\n== Country Profiles ==\n\nThe methodology behind the [[FAO]] Country Profiles is rather simple; it links, reuses and repackages data and information from most relevant existing [[FAO]] [[databases]] and [[systems]].\n\nThe [[FAO]] Country Profiles covers current FAO Members and Associated Nations.<ref>FAO membership as the 17 November 2007: http://www.fao.org/Legal/member-e.htm</ref> Once a country is selected, the portal presents to the user [[documents]], news feeds, [[statistical data]], project details and [[maps]] from relevant [[FAO Country Profiles#Data sources|FAO databases and systems]] for the selected [[country]] and categorized according to thematic areas.\n\nThe thematic areas are grouped in two categories:\n\n* FAO Core Activities: these correspond to  [[FAO]]\'s main areas of expertise, such as, [[natural resources]], [[economics]], [[agriculture]], [[forestry]], [[fisheries]] and technical cooperation. This grouping is based on the work of the corresponding [[FAO]] departments.<ref>For a list of FAO departments and divisions, please see http://www.fao.org/about/depart/en/</ref>\n* Global issues: these are themes that [[FAO]] identified as priority areas for action, and include [[biodiversity]], [[biotechnology]], [[climate change]], [[diseases]] and [[Pest (organism)|pests]], [[emergency]] and aid, [[food security]] and [[food safety|safety]], [[trade]] and [[prices]], [[water management]]. These priority areas correspond to [[FAO]]\'s strategic response to a fast-changing world where issues ranging from [[biotechnology]] to [[climate change]] and [[trade]] present new challenges and choices to governments and the general public.\n\n===Data sources ===\n\nCountry pages provide access to or integrate the following thematic profiles and systems.<ref name="Inventory of resources">[http://www.fao.org/countryprofiles/resources.asp Inventory of data sources used in the FAO country profiles]</ref>\n\n==== FAO data sources ====\n* [http://www.fao.org/nr/water/aquastat/countries/index.stm Aquastat Country Profiles]: The AQUASTAT country profiles describe the state of [[water resources]] and [[agricultural]] [[water use]] in the respective country. Special attention is given to [[water resource]], [[irrigation]], and [[drainage]] sub-sectors.\n* [http://www.fao.org/biotech/inventory_admin/dep/country_rep_search.asp?lang=en Biotechnology Country Profiles]: The objective of the profiles is to provide a platform on which [[developing country]] [[biotechnology]]-related [[policies]], [[regulations]] and activities can be readily accessed, directing the user to key, updated sources of information.\n* [http://www.fao.org/biotech/inventory_admin/dep/default.asp?lang=en BIODEC Biotechnologies in Developing Countries]: FAO-BioDeC is a database meant to gather, store, organize and disseminate, updated baseline information on the state-of-the-art of [[crop]] [[biotechnology]] [[Product (business)|products]] and [[Scientific technique|techniques]], which are in use, or in the pipeline in [[developing countries]]. The database includes about 2000 entries from 70 [[developing countries]], including countries with [[economies in transition]].\n* [http://www.fao.org/ag/AGP/AGPC/doc/Counprof/regions/index.htm Country Pasture/Forage Resource Profiles]: The Country [[Pasture]]/[[Forage]] Resource Profile provides a broad overview of relevant general, [[topographical]], [[climatic]] and [[agro-ecological]] information with focus on [[livestock]] production systems and the [[pasture]]/[[forage]] resources.\n* [http://www.fao.org/documents FAO Corporate Document Repository]: The FAO Corporate Document Repository houses FAO documents and publications, as well as selected non-FAO publications, in electronic format.\n* [http://www.fao.org/tc/tcom/index_en.htm FAO Projects in the country]: From the Field Programme Management Information System.\n* [http://www.fao.org/faoterm/ FAO Terminology - Names of Countries]: In order to standardize and harmonize the vast quantity of terms used in FAO documents and publications, the Organization developed the [[terminology]] database [[FAOTERM]]. The Corporate NAMES OF COUNTRIES database also aims at facilitating the consultation and harmonization of country names throughout the Organization.\n* [http://www.fao.org/fishery/countryprofiles/search/en Fisheries and Aquaculture Country Profiles]: FAO\'s [[Fisheries]] and [[Aquaculture]] Department prepares and publishes Fishery and Aquaculture Country Profiles. Each profile summarizes the Department\'s assessment of activities and trends in fisheries and aquaculture for the country concerned.  Economic and [[demographic data]] are based on [[UN]] or [[World Bank]] sources; data on fisheries are generally those published by the FAO Fisheries and Aquaculture Department.\n* [http://www.fao.org/forestry/country/en/ Forestry Country Profiles]: The [[forestry]] country profiles provide detailed information on [[forests]] and the forest sector: [[forest cover]] (types, extent and change), [[forest management]], policies, products and trade, and more - in all some 30 pages for each country in the world.\n* [http://www.fao.org/geonetwork/srv/en/main.home FAO-GeoNetwork]: FAO-GeoNetwork is a web-based Geographic Data and Information Management System. It enables easy access to local and distributed [[geospatial information]] catalogues and makes available data, graphics, documents for immediate download. FAO-GeoNetwork holds approximately 5000 standardized [[metadata]] records for digital and paper maps, most of them at the global, continent and national level.\n* [http://www.fao.org/giews/english/index.htm Global Information and Early Warning System on Food and Agriculture (GIEWS)]: The System aims to provide policy-makers and policy-analysts with the most up-to-date information available on all aspects of [[food supply]] and demand, warning of imminent [[food crises]], so that timely interventions can be planned.\n* [http://www.fao.org/ag/againfo/resources/en/pubs_sap.html Livestock Sector Briefs]: The purpose of the [[Livestock]] Sector Briefs is to provide a concise overview of livestock production in the selected countries through tables, maps and graphs.\n* [http://www.fao.org/ag/agn/nutrition/profiles_en.stm Nutrition Country Profiles]: The [[Nutrition]] Country Profiles (NCP) provide concise analytical summaries describing the food and nutrition situation in individual countries.\n\n==== Partnerships data sources ====\n* [http://www.agrifeeds.org/ AgriFeeds]: AgriFeeds is a service that allows users to search and filter news and events from several agricultural information sources. It harvests, stores and re-aggregates news and events from feeds published by agricultural organizations and information services.\n* [http://www.ipfsaph.org/En/default.jsp International Portal on Food Safety, Animal & Plant Health (IPFSAPH)]: IPFSAPH facilitates trade in food and agriculture by providing a single access point to authorized official international and national information across the sectors of food safety, animal and plant health.  It has been developed by FAO in association with the organizations responsible for international standard setting in sanitary and phytosanitary matters.\n\n==== Non-FAO data sources ====\n* [http://earthtrends.wri.org/gsearch.php?kw=country&action=results Earthtrends], [[World Resources Institute]]: EarthTrends is a comprehensive online database, maintained by the World Resources Institute, that focuses on the environmental, social, and economic trends that shape the world. The Earthtrends country profiles present environmental information about key variables for different topic areas.\n* International Fund for Agricultural Development ([[IFAD]]): Rural poverty country profiles are produced by IFAD.\n\n== Standards ==\n\n[[File:Geopolitical Ontology in Country Profiles August 12 2009 v 1.png||thumb|200px|right| Geopolitical information section in the FAO Country Profiles.]]\n\nThere are various [[international standards]] and [[coding systems]] to manage country information. Historically, systems dealing with different types of data used different coding systems that were tailored to specific data type requirements. For example, [[statistical systems]] in the [[United Nations]] commonly use the M-49 classification and pigmentation<ref>Standard Country or Area Codes for Statistical Use http://unstats.un.org/unsd/methods/m49/m49.htm</ref> (also known as [[UN]] code) or the [[FAOSTAT]] area classification;<ref>FAOSTAT standardized list of country/territories and groupings: http://faostat.fao.org/site/441/default.aspx</ref> mapping systems could use [[geographic coordinates]] or [[Global Administrative Unit Layers (GAUL)|GAUL]] codes; textual systems (document repositories or web sites) could use [[ISO 3166-1 alpha-2]], [[ISO 3166-1 alpha-3]] or [[AGROVOC]] keywords; etc.\n\nThe FAO Country Profiles provide access to systems managing [[statistics]], [[documents]], [[maps]], [[news feeds]], etc., therefore one of its key aspects to succeed was the mapping of all these [[country codes]].\n\nFor this purpose a [[geopolitical ontology]] was developed.<ref>For linking country-based heterogeneous data at [[FAO]], please see:[http://www.semanticuniverse.com/articles-integrating-country-based-heterogeneous-data-united-nations-fao%E2%80%99s-geopolitical-ontology-and Integrating Country-based heterogeneous data at the United Nations: FAO\'s geopolitical ontology and services.]</ref> This ontology, among other features, maps [[ISO 3166-1 alpha-2|ISO2]], [[ISO 3166-1 alpha-3|ISO3]], [[AGROVOC]], [[Food and Agriculture Organization Corporate Statistical Database|FAOSTAT]], [http://www.fao.org/faoterm/index.asp?lang=EN FAOTERM], [[Global Administrative Unit Layers (GAUL)|GAUL]], [[UN]], and [[UNDP]] codes for all countries.\n\n== Global Resources ==\n\nBesides the profiles for each country the portal also provides access to other important global resources, such as:\n\n=== Low-Income Food Deficit Countries (LIFDC) ===\nThe FAO Country Profiles keeps updated for the public the list of [[LIFDC]] countries. This list is revised every year according to the methodology explained below. The new list of the LIFDC,<ref>For an updated list of Low-Income Food Deficit Countries, please check this page: http://www.fao.org/countryprofiles/lifdc/en/</ref> stands at 62 countries, four less than in the (2012) list. These are: [[Georgia (country)|Georgia]], [[Syrian Arab Republic]], [[Timor-Leste]], [[Republic of Moldova]]. While [[Moldova]] graduated from the list on the basis of net food-exporter criterion, the other graduated based on income criterion.\n\n==== LIFDC methodology====\n\nThe classification of a country as low-income food-deficit used for analytical purposes by [[FAO]] is traditionally determined by three criteria:\n\n# A country should have a [[per capita income]] below the "historical" ceiling used by the [[World Bank]]<ref>For operational and analytical purposes, the World Bank’s main criterion for classifying economies is gross national income (GNI) per capita. Classifications are set each year on 1 July. These official analytical classifications are fixed during the World Bank\'s fiscal year (ending on 30 June), thus countries remain in the categories in which they are classified irrespective of any revisions to their per capita income data. (Source: [[The World Bank]])</ref> to determine eligibility for [[International Development Association|IDA]] assistance and for 20-year [[IBRD]] terms, applied to countries included in the World Bank categories I and II.<ref>Several important distinctions among member countries are commonly used at the World Bank Group. Countries choose whether they are part of Part I or Part II primarily on the basis of their economic standing. Part I are almost all industrial countries and donors to IDA and they pay their contributions in freely convertible currency. Part II countries are almost all developing countries, some of which are donors to IDA. Part II countries are entitled to pay most of their contribution to IDA in local currency. Please see: "A Guide to the World Bank Group", The World Bank, 2003</ref> For instance, the historical ceiling of per capita [[gross national income]] (GNI) for 2006, based on the World Bank Atlas method,<ref>Please see: [http://web.worldbank.org/WBSITE/EXTERNAL/DATASTATISTICS/0,,contentMDK:20452009~menuPK:64133156~pagePK:64133150~piPK:64133175~theSitePK:239419~isCURL:Y~isCURL:Y,00.html The World Bank Atlas Method]</ref> was US$1,735, i.e. higher than the level established for 2005 ($1,675).\n# The net food [[trade]]<ref>Net food trade refers to the gross imports less gross exports of food</ref> position of a country averaged over the preceding three years for which statistics are available, in this case from 2003 to 2005. Trade volumes for a broad basket of basic foodstuffs ([[cereals]], [[root]]s and [[tubers]], [[pulses]], [[oilseeds]] and oils other than tree crop oils, [[meat]] and [[dairy products]]) are converted and aggregated by the [[calorie]] content of individual [[commodities]].\n# A self-exclusion criterion is applied when countries that meet the above two criteria specifically request FAO to be excluded from the LIFDC category.\n\nIn order to avoid countries changing their LIFDC status too frequently - typically due to short-term, [[exogenous]] shocks - an additional factor was introduced in 2001. This factor, called "persistence of position", would postpone the "exit" of a LIFDC from the list, despite the country not meeting the LIFDC [[income]] criterion or the [[food-deficit]] criterion, until the change in its status is verified for three consecutive years.<ref>For a list of countries and economies sorted by their gross domestic product (GDP) at purchasing power parity (PPP) per capita, please see [[List of countries by GDP (PPP) per capita]]</ref>\n\n=== FAO Member Countries and Flags ===\n\nThe FAO Country Profiles is FAO\'s source for dissemination of [[FAO]]\'s Member Nations and Associated Nations<ref>The list of FAO member countries and date of entry is available at: http://www.fao.org/Legal/member-e.htm</ref> official flags.<ref>The list of FAO member countries and flags is available at http://www.fao.org/countryprofiles/flags/</ref> The update of any [[country flag]] is coordinated with the other [[United Nations]] agencies. All flags are made available in a standardized manner which also aims to help web site owners to ensure that they always display the official country flag.\n\nThe standard URL for any given country flag would be composed by: the generic URL: "http://www.fao.org/countryprofiles/flags/" to which the [[ISO 3166-1 alpha-2|ISO 3166-1 Alpha-2]] code for the country is added, plus the image format suffix ".gif". For instance, the URL for the [[Argentine flag|Argentina flag]] would be: http://www.fao.org/countryprofiles/flags/AR.gif, with AR being the [[ISO 3166-1 alpha-2]] code of [[Argentina]].<ref>One of several international coding systems (some of the others being: [[ISO2]], [[ISO3]], [[AGROVOC]], [[FAOSTAT]], [[FAOTERM]], [[GAUL]], [[UN]], and [[UNDP]]) for territories and groups.</ref>\n\n== Criticism ==\n\nEarly criticism of the [[FAO]] Country Profiles was that, in its inception phase, it only contained very few resources. Since 2002, the number of available resources has increased to cover country-based information and data, directly linked from [[FAO]]\'s web pages or [[FAO]]\'s digital repositories.<ref name="Inventory of resources"/> Over the last years, another identified area for improvement was the simplicity of the system methodology, being the resources only linked from country pages and thus, lacking real integration. This need was addressed by starting to integrate additional data, such as, the fisheries charts or the news and events items taken from [[AgriFeeds]]. In addition, in order to provide more complete country profiles, the system started to link or integrate  non-[[FAO]] resources.\n\n== See also ==\n* [[Agricultural Information Management Standards]]\n* [[AGROVOC]]\n* [[Country codes]]\n* [[Food and Agriculture Organization]]\n* [[Forestry Information Centre]]\n* [[Geopolitical ontology]]\n\n== References ==\n{{reflist|33em}}\n\n==External links==\n* [http://www.fao.org/countryprofiles/default.asp?lang=en FAO Country Profiles]\n* [http://www.fao.org/Legal/member-e.htm FAO membership]\n* [http://www.fao.org/countryprofiles/lifdc.asp?lang=en Low-Income Food-Deficit Countries (LIFDC)]\n* [http://www.fao.org/sids/index_en.asp Small Island Developing States (SIDS)]\n\n{{DEFAULTSORT:Fao Country Profiles}}\n[[Category:Agriculture]]\n[[Category:Agriculture by country| FAO]]\n[[Category:Knowledge representation]]\n[[Category:Information systems]]\n[[Category:Food and Agriculture Organization]]\n[[Category:Country codes]]']
['Geopolitical ontology', '20250365', 'The \'\'\'FAO geopolitical ontology\'\'\' is an [[Ontology (information science)|Ontology]] developed by the [[FAO|Food and Agriculture Organization of the United Nations (FAO)]] to describe, manage and exchange data related to geopolitical entities such as countries, territories, regions and other similar areas.\n\n==Definitions and examples==\nAn [[ontology (information science)|ontology]] is a kind of dictionary that describes information  in a certain domain using concepts and relationships. It is often implemented using [[Web Ontology Language|OWL]] (Web Ontology Language), an [[XML]]-based standard language  that can be interpreted by computers.\n\n* A \'\'Concept\'\' is defined as abstract knowledge. For example, in the geopolitical ontology a [[United Nations list of Non-Self-Governing Territories|non-self-governing territory]] or a [[geographical region|geographical group]] are concepts. Concepts are explicitly implemented in the ontology with individuals and classes:\n** An \'\'individual\'\' is defined as an object perceived from the real world. In the geopolitical domain   [[Ethiopia]] or the [[least developed countries]] group are individuals.\n** A \'\'class\'\' is defined as a set of individuals sharing common properties. In the geopolitical domain, [[Ethiopia]], [[Republic of Korea]] or [[Italy]] are individuals of the class \'\'self-governing\'\' territory; and [[least developed countries]] is an individual of the class \'\'special group\'\'.\n* Relationships between concepts are explicitly implemented by:\n** \'\'[[Object (computer science)|Object]] properties\'\' between individuals of two classes. For example, \'\'has member\'\' and \'\'is in group\'\' properties, as shown in Figure 1.\n** \'\'[[Datatype]] properties\'\' between individuals and literals or [[XML]] datatypes. For example, the individual [[Afghanistan]] has the datatype property \'\'CodeISO3\'\' with the value "AFG".\n** \'\'Restrictions\'\' in classes and/or properties. For example, the property \'\'official English name\'\' of the class \'\'self-governing\'\' territory has been restricted to have only \'\'one\'\' value, this means that a self-governing territory (or country) can only have one internationally recognized official English name.<ref>Official names of countries from [http://www.fao.org/faoterm/nocs/pages/homeNocs.jsp?members=allC&lang=en&lang2=en FAO terminology database]</ref>\n\n[[Image:Concepts November 19 2008 v 2.png||thumb|600px|center|Figure 1. An example of concepts and relationship in the geopolitical ontology.]]\n\nThe advantage of describing information in an ontology is that it enables to acquire domain knowledge by defining hierarchical structures of classes, adding individuals, setting object properties and datatype properties, and assigning restrictions.\n\n==FAO ontology==\nThe geopolitical ontology provides names in seven languages (Arabic, Chinese, French, English, Spanish, Russian and Italian) and identifiers in various international coding systems ([[ISO 3166-1 alpha-2|ISO2]], [[ISO 3166-1 alpha-3|ISO3]], [[AGROVOC]], [[Food and Agriculture Organization Corporate Statistical Database|FAOSTAT]], [http://www.fao.org/faoterm/index.asp?lang=EN FAOTERM], [[Global Administrative Unit Layers (GAUL)|GAUL]], [[UN]], [[List of UNDP country codes|UNDP]] and [[DBPedia]]ID codes) for territories and groups. Moreover, the [[FAO]] geopolitical ontology tracks historical changes from 1985 up until today;<ref>Country or area code changes since 1982:  [http://unstats.un.org/unsd/methods/m49/m49chang.htm United Nations Statistics Division - country or area codes added or changed]</ref> provides [[geolocation]] (geographical coordinates); implements relationships among [[countries]] and countries, or countries and groups, including properties such as \'\'has border with\'\', \'\'is predecessor of\'\', \'\'is successor of\'\', \'\'is administered by\'\', \'\'has members\'\', and \'\'is in group\'\'; and disseminates country statistics including country area, land area, agricultural area, [[GDP]] or [[population]].\n\nThe FAO geopolitical ontology provides a structured description of data sources. This includes: source name, source identifier, source creator and source\'s update date. Concepts are described using the [[Dublin Core]] vocabulary (http://purl.org/dc/elements/1.1/description).\n\nIn summary, the main objectives of the FAO geopolitical ontology are:\n\n* To provide the most updated geopolitical information (names, codes, relationships, statistics)\n* To track historical changes in geopolitical information\n* To improve information management and facilitate standardized data sharing of geopolitical information\n* To demonstrate the benefits of the geopolitical ontology to improve [[interoperability]] of [[corporate]] [[information systems]]\n\nIt is possible to \'\'\'download\'\'\' the FAO geopolitical ontology in [http://aims.fao.org/geopolitical.owl OWL] and [http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ RDF] formats. Documentation is available in the [[FAO Country Profiles]] [http://www.fao.org/countryprofiles/geoinfo.asp?lang=en Geopolitical information] web page.\n\n==Features of the FAO ontology==\nThe geopolitical ontology contains :\n\n*Area types:<ref>When an area (territory or group) changed but kept the same name, the ontology differentiates the two areas by sub-fixing the name of the obsolete one with the year (e.g. “FAO 2006”). The year indicates the beginning of validity of that particular area.</ref>\n**Territories: [[List of sovereign states|self-governing]], [[United Nations list of Non-Self-Governing Territories|non-self-governing]], [[Disputed area|disputed]], other.<ref>The area type \'\'Other\'\' is used for [[Antarctica]] which has no government and belongs to no country. See also [[Antarctica#Politics|Politics in Antarctica]].</ref>\n**Groups: [[organizations]], [[subregion|geographic]], [[economic union|economic]] and special groups.<ref>Special groups term is used for non-economical or greographical territory groups like the [[Small Island Developing States]], [[Landlocked countries|Land Locked Countries]], Low Income Food Deficit Countries, [[Least Developed Countries]], etc.</ref>\n*Names <ref>UN official names: [http://unstats.un.org/unsd/geoinfo/uncsgnreports.htm Reports of the United Nations Conference on the Standardization of Geographical Names]</ref> (official, short and names for lists) in Arabic, Chinese, English, French, Spanish, Russian and Italian.\n*International codes: UN code – M49, [[ISO 3166]] Alpha-2 and Alpha-3, [[List of UNDP country codes|UNDP code]], [[Global Administrative Unit Layers (GAUL)|GAUL]] code, FAOSTAT, [[AGROVOC]] FAOTERM and [[DBPedia]]ID.\n*Coordinates: maximum [[latitude]], minimum [[latitude]], maximum [[longitude]], minimum [[longitude]].\n*Basic country statistics: country area, land area, agricultural area, GDP, population.\n*Currency names and codes.\n*Adjectives of nationality.\n*Relations:\n**Groups membership.\n**Neighbours (land [[border]]), administration of [[United Nations list of Non-Self-Governing Territories|non-self-governing]].\n**Historic changes: predecessor, successor, valid since,<ref>The value of the datatype property "validSince" is the first year of validity  of a territory or group. The geopolitical ontology traces back historic changes only until 1985. Therefore if an area has a validSince = 1985, this indicates that the area is valid at least since 1985.</ref> valid until.<ref>The value of the datatype property "validUntil" is the last year of validity of the territory or group. In case the area is currently valid, this value is set by default to 9999.</ref>\n\n==Implementation into  OWL==\nThe [[FAO]] geopolitical ontology is implemented in [[Web Ontology Language|OWL]]. It consists of classes, properties, individuals and restrictions. Table 1 shows all classes, gives a brief description and lists some individuals that belong to each class. Note that the current version of the geopolitical ontology does not provide individuals of the class "disputed" territories. Table 2 and Table 3 illustrate datatype properties and object properties.\n\n<!-- Deleted image removed: [[Image:Class and instances in the geopolitical ontology v 1.png||thumb|667px|center|Table 1. Classes and instances in the geopolitical ontology.]] -->\n\n<!-- Deleted image removed: [[Image:Datatype properties in the geopolitical ontology v 1.png||thumb|667px|center|Table 2. Datatype properties in the geopolitical ontology.]] -->\n\n[[Image:Object properties in the geopolitical ontology v 1.png||thumb|674px|center|Table 3. Object properties in the geopolitical ontology.]]\n\n== Geopolitical ontology in Linked Open Data ==\n<!-- Deleted image removed: [[File:Geopol LOD.png|thumb|200px|left|Figure 2. RDF version of FAO geopolitical ontology]]  -->\n\nThe FAO Geopolitical ontology is embracing the [http://linkeddata.org W3C Linked Open Data (LOD) initiative] and released its RDF version of the geopolitical ontology in March 2011. \nThe term \'Linked Open Data\' refers to a set of best practices for publishing and connecting structured data on the Web. The key technologies that support Linked Data are URIs, HTTP and RDF.\n\nThe RDF version of the geopolitical ontology is compliant with all [http://www.w3.org/DesignIssues/LinkedData.html Linked data principles] to be included in the [http://richard.cyganiak.de/2007/10/lod/ Linked Open Data cloud], as explained in the following.\n\n==Resolvable http:// URIs ==\nEvery resource in the OWL format of the FAO Geopolitical Ontology has a unique URI. Dereferenciation was implemented to allow for three different URIs to be assigned to each resource as follows:  \n* URI identifying the non-information  resource\n* Information resource with an RDF/XML representation\n* Information resource with an HTML representation\nIn addition the current URIs used for OWL format needed to be kept to allow for backwards compatibility for other systems that are using them. Therefore, the new URIs for the FAO Geopolitical Ontology in LOD were carefully created, using  “Cool URIs for Semantic Web”  and considering other good practices for URIs, such as DBpedia URIs.\n\n==New URIs==\nThe URIs of the geopolitical ontology need to be permanent, consequently all transient information, such as year, version, or format was avoided in the definition of the URIs. \nThe new URIs can be accessed at\nhttp://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ \nFor example, for the resource “Italy” the URIs are the following: \n;http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/Italy\n: identifies the non-information resource. \n;http://www.fao.org/countryprofiles/geoinfo/geopolitical/data/Italy\n: identifies the resource with an RDF/XML representation. \n;http://www.fao.org/countryprofiles/geoinfo/geopolitical/page/Italy\n:identifies the information resource with an HTML representation.\nIn addition, “owl:sameAs” is used to map the new URIs to the OWL representation.\n\n==Dereferencing URIs==\nWhen a non-information resource is looked up without any specific representation format, then the server needs to redirect the request to information resource with an HTML representation. \nFor example, to retrieve the resource “Italy” (http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/Italy), which is a non-information resource, the server redirects to the html page of “Italy” (http://www.fao.org/countryprofiles/geoinfo/geopolitical/page/Italy).\n\n==At least 1000 triples in the datasets==\nThe total number of triple statements in FAO Geopolitical Ontology is 22,495. \nAt least 50 links to a dataset already in the current LOD Cloud:  \nFAO Geopolitical Ontology has 195 links to [http://www.dbpedia.org DBpedia], which is already part of the LOD Cloud.\n\n==Access to the entire dataset==\nFAO Geopolitical Ontology provides the entire dataset as a RDF dump. It is available at http://www.fao.org/countryprofiles/geoinfo/geopolitical/data\n\nThe RDF version of the FAO Geopolitical Ontology has been already registered in CKAN (http://ckan.net/package/fao-geopolitical-ontology) and it was requested to add it into the LOD Cloud.\n\n==Example of use==\n[[Image:Geopolitical Ontology in Country Profiles August 12 2009 v 1.png||thumb|200px|right|Figure 3. a website of introducing the geopolitical ontology in FAO Country Profiles.]]\n\nThe [[FAO Country Profiles]] is an information retrieval tool which groups the FAO\'s vast archive of information on its global activities in [[agriculture]] and [[rural development]] in one single area and catalogues it exclusively by country.\n\nThe [[FAO Country Profiles]] system provides access to country-based heterogeneous data sources.<ref>[http://www.fao.org/countryprofiles/resources.asp Inventory of data sources used in the FAO country profiles]</ref> By using the  geopolitical ontology in the system, the following benefits are expected:<ref>[http://semanticweb.com/integrating-country-based-heterogeneous-data-at-the-united-nations-fao-s-geopolitical-ontology-and-services_b10681 Integrating country-based heterogeneous data at the United Nations: FAO\'s geopolitical ontology and services.]</ref>\n\n* Enhanced system functionality for content aggregation and synchronization from the multiple source repositories.\n* Improved information access and browsing through comparison of data in neighbor countries and groups.\n\nFigure 3 shows a page in the [[FAO Country Profiles]] where the geopolitical ontology is described.\n\n==See also==\n*[[Agricultural Information Management Standards]]\n*[[AGROVOC]]\n*[[Country code]]\n*[[FAO Country Profiles]]\n*[[Global Administrative Unit Layers (GAUL)|Global Administrative Unit Layers]] (GAUL)\n*[[International Organization for Standardization]] (ISO)\n\n==References==\n{{reflist|2}}\n\n==External links==\n*[http://aims.fao.org/geopolitical.owl Geopolitical ontology in OWL format]\n*[http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ Geopolitical ontology in RDF format]\n*[http://www.fao.org/countryprofiles/geoinfo.asp?lang=en Geopolitical information in the FAO Country Profiles]\n*[http://www.slideshare.net/faoaims/faos-geopolitical-ontology-and-services FAO’s Geopolitical Ontology and Services] (Slides about FAO\'s geopolitical ontology)\n*[http://www.fao.org/countryprofiles/default.asp?lang=en FAO Country Profiles]\n*[http://www.fao.org/faoterm FAO Terminology] (FAOTERM)\n*[http://faostat.fao.org FAOSTAT]\n*[http://unstats.un.org/unsd/methods/m49/m49.htm UN Statistics Division - M49 codes]\n*[http://www.iso.org/iso/english_country_names_and_code_elements ISO - Maintenance Agency for ISO 3166 country codes]\n\n{{DEFAULTSORT:Geopolitical Ontology}}\n[[Category:Ontology]]\n[[Category:Ontology (information science)]]\n[[Category:Knowledge representation]]\n[[Category:Country codes]]']
['Issue trees', '30713569', '{{Orphan|date=August 2012}}\n\n\'\'\'An issue tree\'\'\', also called "logic tree" or "issue map", is a graphical breakdown of a question that dissects it into its different components vertically and that progresses into details as it reads to the right.<ref>[https://global.oup.com/academic/product/strategic-thinking-in-complex-problem-solving-9780190463908?q=chevallier&lang=en&cc=us  Chevallier, Arnaud (2016). Strategic Thinking in Complex Problem Solving. Oxford, UK, Oxford University Press. p.47]</ref>\n\nIssue trees are useful in [[problem solving]] to identify the root causes of a problem as well as to identify its potential solutions. They also provide a reference point to see how each piece fits into the whole picture of a problem.<ref>http://webarchive.nationalarchives.gov.uk/20060213205515/http://strategy.gov.uk/downloads/survivalguide/downloads/ssg_v2.1.pdf</ref>\n\nThere are two types of issue trees: diagnostic ones and solution ones.\n\nDiagnostic trees breakdown a "why" key question, identifying all the possible root causes for the problem.\nSolution tree breakdown a "how" key question, identifying all the possible alternatives to fix the problem.\n\nTo be effective, an issue tree needs to obey four basic rules:<ref>http://powerful-problem-solving.com/build-logic-trees</ref>\n# Consistently answer a “why” or a “how” question\n# Progress from the key question to the analysis as it moves to the right\n# Have branches that are mutually exclusive and collectively exhaustive ([[MECE]])\n# Use an insightful breakdown\n\nThe requirement for issue trees to be collectively exhaustive implies that [[divergent thinking]] is a critical skill.\n\nA profitability tree is an example of an issue tree. It looks at different ways in which a company can increase its profitability. Starting from the key question on the right, it breaks it down between revenues and costs, and break these down into further details.\n[[File:An issue tree showing how a company can increase profitability.png|thumb|An issue tree showing how a company can increase profitability]]\n\n==References==\n<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n\n[[Category:Articles created via the Article Wizard]]\n[[Category:Knowledge representation]]\n[[Category:Problem solving methods]]\n\n\n{{logic-stub}}']
['Linear belief function', '24134105', '{{Notability|date=January 2010}}\n\n\'\'\'Linear belief function\'\'\' is an extension of the [[Dempster–Shafer theory]] of [[belief functions]] to the case when variables of interest are [[Continuous function|continuous]]. Examples of such variables include financial asset prices, portfolio performance, and other antecedent and consequent variables. The theory was originally proposed by [[Arthur P. Dempster]]<ref>A. P. Dempster, "Normal belief functions and the [[Kalman filter]]," in \'\'Data Analysis from Statistical Foundations\'\', A. K. M. E. Saleh, Ed.: Nova Science Publishers, 2001, pp. 65–84.</ref> in the context of Kalman Filters and later was reelaborated, refined, and applied to knowledge representation in artificial intelligence and decision making in finance and accounting by Liping Liu.<ref>Liu, Liping, Catherine Shenoy, and Prakash P. Shenoy, “Knowledge Representation and Integration for Portfolio Evaluation Using Linear Belief Functions,” IEEE Transactions on Systems, Man, and Cybernetics, Series A, vol. 36 (4), 2006, pp. 774–785.</ref>\n\n== Concept ==\n\nA linear belief function intends to represent our belief regarding the location of the true value as follows: We are certain that the truth is on a so-called certainty [[hyperplane]] but we do not know its exact location; along some dimensions of the certainty hyperplane, we believe the true value could be anywhere from –∞ to +∞ and the probability of being at a particular location is described by a [[normal distribution]]; along other dimensions, our knowledge is [[vacuous]], i.e., the true value is somewhere from –∞ to +∞ but the associated probability is unknown. A [[belief function]] in general is defined by a [[mass function]] over a class of [[focal elements]], which may have nonempty intersections. A linear belief function is a special type of [[belief function]] in the sense that its [[focal elements]] are exclusive, parallel sub-hyperplanes over the certainty hyperplane and its [[mass function]] is a [[normal distribution]] across the sub-hyperplanes.\n\nBased on the above geometrical description, Shafer<ref>G. Shafer, "A note on Dempster\'s Gaussian belief functions," School of Business, University of Kansas, Lawrence, KS, Technical Report 1992.</ref> and Liu<ref>L. Liu, "A theory of Gaussian belief functions," \'\'International Journal of Approximate Reasoning\'\', vol. 14, pp. 95–126, 1996</ref>  propose two mathematical representations of a LBF: a wide-sense inner product and a linear functional in the variable space, and as their duals over a hyperplane in the sample space. Monney <ref>P. A. Monney, \'\'A Mathematical Theory of Arguments for Statistical Evidence\'\'. New York, NY: Springer, 2003.</ref> proposes still another structure called Gaussian hints. Although these representations are mathematically neat, they tend to be unsuitable for knowledge representation in expert systems.\n\n== Knowledge representation ==\n\nA linear belief function can represent both logical and probabilistic knowledge for three types of variables: deterministic such as an observable or controllable, random whose distribution is normal, and vacuous on which no knowledge bears. Logical knowledge is represented by linear equations, or geometrically, a certainty hyperplane. Probabilistic knowledge is represented by a normal distribution across all parallel focal elements.\n\nIn general, assume X is a vector of multiple normal variables with mean μ and covariance Σ. Then, the multivariate normal distribution can be equivalently represented as a moment matrix:\n\n: <math>\nM(X) = \\left( \\begin{array}{*{20}c}\n   \\mu \\\\\n   \\Sigma\n\\end{array} \\right).\n</math>\n\nIf the distribution is non-degenerate, i.e., Σ has a full rank and its inverse exists, the moment matrix can be fully swept:\n\n: <math> \nM(\\vec X) = \\left( \\begin{array}{*{20}c}\n   \\mu \\Sigma^{-1} \\\\\n   -\\Sigma^{-1}\n\\end{array} \\right)\n</math>\n\nExcept for normalization constant, the above equation completely determines the normal density function for \'\'X\'\'. Therefore,  <math>M(\\vec X)</math> represents the probability distribution of \'\'X\'\' in the potential form.\n\nThese two simple matrices allow us to represent three special cases of linear belief functions. First, for an ordinary normal probability distribution M(X) represents it. Second, suppose one makes a direct observation on X and obtains a value μ. In this case, since there is no uncertainty, both variance and covariance vanish, i.e., Σ = 0. Thus, a direct observation can be represented as:\n\n: <math>M(X) = \\left( \\begin{array}{*{20}c}\n   \\mu \\\\\n   0\n\\end{array} \\right)\n</math>\n\nThird, suppose one is completely ignorant about X. This is a very thorny case in Bayesian statistics since the density function does not exist. By using the fully swept moment matrix, we represent the vacuous linear belief functions as a zero matrix in the swept form follows:\n\n: <math>M(\\vec X) = \\left[ \\begin{array}{*{20}c}\n   0 \\\\\n   0\n\\end{array} \\right]\n</math>\n\nOne way to understand the representation is to imagine complete ignorance as the limiting case when the variance of X approaches to ∞, where one can show that Σ<sup>−1</sup> = 0 and hence <math>M(\\vec X)</math> vanishes. However, the above equation is not the same as an improper prior or normal distribution with infinite variance. In fact, it does not correspond to any unique probability distribution. For this reason, a better way is to understand the vacuous linear belief functions as the neutral element for combination (see later).\n\nTo represent the remaining three special cases, we need the concept of partial sweeping. Unlike a full sweeping, a partial sweeping is a transformation on a subset of variables. Suppose X and Y are two vectors of normal variables with the joint moment matrix:\n\n: <math>M(X,Y) = \\left[ \\begin{array}{*{20}c}\n   \\begin{array}{*{20}c}\n   \\mu_1 \\\\\n   \\Sigma _{11} \\\\\n   \\Sigma_{21}\n\\end{array} & \\begin{array}{*{20}c}\n   \\mu_2 \\\\\n   \\Sigma _{12} \\\\\n   \\Sigma_{22}\n\\end{array}\n\\end{array} \\right]</math>\n\nThen M(X, Y) may be partially swept. For example, we can define the partial sweeping on X as follows:\n\n: <math> M(\\vec X,Y) = \\left[ \\begin{array}{*{20}c}\n   \\begin{array}{*{20}c}\n   \\mu _1 (\\Sigma_{11})^{-1} \\\\\n   -(\\Sigma_{11})^{-1} \\\\\n   \\Sigma_{21} (\\Sigma_{11})^{-1}\n\\end{array} & \\begin{array}{*{20}c}\n   \\mu_2  - \\mu_1 (\\Sigma_{11} )^{-1} \\Sigma_{12} \\\\\n   (\\Sigma_{11} )^{-1} \\Sigma_{12} \\\\\n   \\Sigma_{22} - \\Sigma_{21} (\\Sigma_{11})^{-1} \\Sigma_{12}\n\\end{array}\n\\end{array} \\right]\n</math>\n\nIf \'\'X\'\' is one-dimensional, a partial sweeping replaces the variance of \'\'X\'\' by its negative inverse and multiplies the inverse with other elements. If \'\'X\'\' is multidimensional, the operation involves the inverse of the covariance matrix of \'\'X\'\' and other multiplications. A swept matrix obtained from a partial sweeping on a subset of variables can be equivalently obtained by a sequence of partial sweepings on each individual variable in the subset and the order of the sequence does not matter. Similarly, a fully swept matrix is the result of partial sweepings on all variables.\n\nWe can make two observations. First, after the partial sweeping on&nbsp;\'\'X\'\', the mean vector and covariance matrix of \'\'X\'\' are respectively <math> \\mu_1 (\\Sigma _{11} )^{-1} </math> and <math> -(\\Sigma_{11} )^{-1} </math>, which are the same as that of a full sweeping of the marginal moment matrix of&nbsp;\'\'X\'\'. Thus, the elements corresponding to X in the above partial sweeping equation represent the marginal distribution of X in potential form. Second, according to statistics, <math> \\mu_2  - \\mu_1 (\\Sigma_{11} )^{-1} \\Sigma_{12} </math>is the conditional mean of \'\'Y\'\' given \'\'X\'\'&nbsp;=&nbsp;0; <math> \\Sigma_{22} - \\Sigma_{21} (\\Sigma_{11} )^{-1} \\Sigma_{12} </math>  is the conditional covariance matrix of \'\'Y\'\' given \'\'X\'\'nbsp;=&nbsp;0; and  <math>(\\Sigma_{11} )^{-1} \\Sigma_{12} </math> is the slope of the regression model of \'\'Y\'\' on&nbsp;\'\'X\'\'. Therefore, the elements corresponding to Y indices and the intersection of \'\'X\'\' and \'\'Y\'\' in <math> M(\\vec X,Y)</math>represents the conditional distribution of \'\'Y\'\' given&nbsp;\'\'X\'\'&nbsp;=&nbsp;0.\n\nThese semantics render the partial sweeping operation a useful method for manipulating multivariate normal distributions. They also form the basis of the moment matrix representations for the three remaining important cases of linear belief functions, including proper belief functions, linear equations, and linear regression models.\n\n=== Proper linear belief functions ===\nFor variables \'\'X\'\' and \'\'Y\'\', assume there exists a piece of evidence justifying a normal distribution for variables \'\'Y\'\' while bearing no opinions for variables&nbsp;\'\'X\'\'. Also, assume that \'\'X\'\' and \'\'Y\'\' are not perfectly linearly related, i.e., their correlation is less than&nbsp;1. This case involves a mix of an ordinary normal distribution for Y and a vacuous belief function for&nbsp;\'\'X\'\'. Thus, we represent it using a partially swept matrix  as follows:\n\n: <math>M(\\vec X,Y) = \\left[ \\begin{array}{*{20}c}\n   \\begin{array}{*{20}c}\n   0  \\\\\n   0  \\\\\n   0\n\\end{array} & \\begin{array}{*{20}c}\n   \\mu_2 \\\\\n   0  \\\\\n   \\Sigma_{22} \\\\\n\\end{array}\n\\end{array} \\right]\n</math>\n\nThis is how we could understand the representation. Since we are ignorant on&nbsp;\'\'X\'\', we use its swept form and set  <math> \\mu_1 (\\Sigma_{11})^{-1} = 0</math> and <math> - (\\Sigma_{11})^{-1} = 0</math>. Since the correlation between \'\'X\'\' and \'\'Y\'\' is less than&nbsp;1, the regression coefficient of \'\'X\'\' on \'\'Y\'\' approaches to 0 when the variance of \'\'X\'\' approaches to&nbsp;∞. Therefore,  <math>(\\Sigma_{11})^{-1} \\Sigma_{12} = 0</math>. Similarly, one can prove that <math>\\mu_1 (\\Sigma_{11})^{-1} \\Sigma_{12}  = 0</math> and  <math> \\Sigma_{21} (\\Sigma_{11})^{-1} \\Sigma_{12} = 0</math>.\n\n=== Linear equations ===\nSuppose X and Y are two row vectors, and Y = XA + b, where A and b are the coefficient matrices. We represent the equation using a partially swept matrix as follows:\n\n: <math>M(\\vec X,Y) = \\left[ \\begin{array}{*{20}c}\n   \\begin{array}{*{20}c}\n   0  \\\\\n   0  \\\\\n   A^T\n\\end{array} & \\begin{array}{*{20}c}\n   b  \\\\\n   A  \\\\\n   0\n\\end{array}\n\\end{array} \\right]\n</math>\n\nWe can understand the representation based on the fact that a linear equation contains two pieces of knowledge: (1) complete ignorance about all variables; and (2) a degenerate conditional distribution of dependent variables given independent variables. Since X is an independent vector in the equation, we are completely ignorant about it. Thus, <math> \\mu_1 (\\Sigma _{11})^{-1} = 0</math> and <math> -(\\Sigma_{11})^{-1} = 0</math>. Given \'\'X\'\' = 0, \'\'Y\'\' is completely determined to be b. Thus, the conditional mean of Y is b and the conditional variance is 0. Also, the regression coefficient matrix is A.\n\nNote that the knowledge to be represented in linear equations is very close to that in a proper linear belief functions, except that the former assumes a perfect correlation between X and Y while the latter does not. This observation is interesting; it characterizes the difference between partial ignorance and linear equations in one parameter — correlation.\n\n=== Linear regression models ===\nA linear regression model is a more general and interesting case than previous ones. Suppose X and Y are two vectors and Y = XA + b + E, where A and b are the appropriate coefficient matrices and E is an independent white noise satisfying E ~ N(0, Σ). We represent the model as the following partially swept matrix:\n\n: <math>\nM(\\vec X,Y) = \\left[ \\begin{array}{*{20}c}\n   \\begin{array}{*{20}c}\n   0  \\\\\n   0  \\\\\n   A^T\n\\end{array} & \\begin{array}{*{20}c}\n   b  \\\\\n   A \\\\\n   \\Sigma\n\\end{array}\n\\end{array} \\right]\n</math>\n\nThis linear regression model may be considered as the combination of two pieces of knowledge (see later), one is specified by the linear equation involving three variables X, Y, and E, and the other is a simple normal distribution of E, i.e., E ~ N(0, Σ). Alternatively, one may consider it similar to a linear equation, except that, given X = 0, Y is not completely determined to be b. Instead, the conditional mean of Y is b while the conditional variance is Σ. Note that, in this alternative interpretation, a linear regression model forms a basic building block for knowledge representation and is encoded as one moment matrix. Besides, the noise term E does not appear in the representation. Therefore, it makes the representation more efficient.\n\nFrom representing the six special cases, we see a clear advantage of the moment matrix representation, i.e., it allows a unified representation for seemingly diverse types of knowledge, including linear equations, joint and conditional distributions, and ignorance. The unification is significant not only for knowledge representation in artificial intelligence but also for statistical analysis and engineering computation. For example, the representation treats the typical logical and probabilistic components in statistics — observations, distributions, improper priors (for Bayesian statistics), and linear equation models — not as separate concepts, but as manifestations of a single concept. It allows one to see the inner connections between these concepts or manifestations and to interplay them for computational purposes.\n\n== Knowledge operations ==\n\nThere are two basic operations for making inferences in [[expert system]]s using linear belief functions: combination and marginalization. Combination corresponds to the integration of knowledge whereas marginalization corresponds to the coarsening of knowledge. Making an inference involves combining relevant knowledge into a full body of knowledge and then projecting the full body of knowledge to a partial domain, in which an inference question is to be answered.\n\n=== Marginalization ===\nMarginalization projects a linear belief function into one with fewer variables. Expressed as a moment matrix, it is simply the restriction of a nonswept moment matrix to a submatrix corresponding to the remaining variables. For example, for the joint distribution M(X, Y), its marginal to Y is:\n\n: <math>\nM^{\\downarrow Y} (X,Y) = \\left[ {\\begin{array}{*{20}c}\n   \\mu_2  \\\\\n   \\Sigma_{22}\n\\end{array}} \\right]\n</math>\n\nWhen removing a variable, it is important that the variable has not been swept on in the corresponding moment matrix, i.e., it does not have an arrow sign above the variable. For example, projecting the matrix <math>M(\\vec X,Y)</math> to Y produces:\n\n: <math> M^{ \\downarrow Y} (\\vec X,Y) = \\left[ {\\begin{array}{*{20}c}\n   \\mu _2  - \\mu _1 (\\Sigma _{11} )^{-1} \\Sigma _{12} \\\\\n   \\Sigma_{22} - \\Sigma_{21} (\\Sigma _{11})^{-1} \\Sigma_{12}\n\\end{array}} \\right]\n</math>\n\nwhich is not the same linear belief function of Y. However, it is easy to see that removing any or all variables in Y from the partially swept matrix will still produce the correct result — a matrix representing the same function for the remaining variables.\n\nTo remove a variable that has been already swept on, we have to reverse the sweeping using partial or full reverse sweepings. Assume  <math>M(\\vec X)</math> is a fully swept moment matrix,\n\n: <math>\nM(\\vec X) = \\left( {\\begin{array}{*{20}c}\n   {\\bar \\mu }  \\\\\n   {\\bar \\Sigma }  \\\\\n\\end{array}} \\right)\n</math>\n\nThen a full reverse sweeping of <math>M(\\vec X)</math> will recover the moment matrix M(X) as follows:\n\n: <math>\nM(X) = \\left( {\\begin{array}{*{20}c}\n   { - \\bar \\mu \\bar \\Sigma ^{ - 1} }  \\\\\n   { - \\bar \\Sigma ^{ - 1} }  \\\\\n\\end{array}} \\right)\n</math>\n\nIf a moment matrix is in a partially swept form, say\n\n: <math>\nM(\\vec X,Y) = \\left[ {\\begin{array}{*{20}c}\n   {\\begin{array}{*{20}c}\n   {\\bar \\mu _1 }  \\\\\n   {\\bar \\Sigma _{11} }  \\\\\n   {\\bar \\Sigma _{21} }  \\\\\n\\end{array}} & {\\begin{array}{*{20}c}\n   {\\bar \\mu _2 }  \\\\\n   {\\bar \\Sigma _{12} }  \\\\\n   {\\bar \\Sigma _{22} }  \\\\\n\\end{array}}  \\\\\n\\end{array}} \\right]\n</math>\n\nits partially reverse sweeping on X is defined as follows:\n\n: <math> M(X,Y) = \\left[ {\\begin{array}{*{20}c}\n   {\\begin{array}{*{20}c}\n   { - \\bar \\mu _1 (\\bar \\Sigma _{11} )^{ - 1} }  \\\\\n   { - (\\bar \\Sigma _{11} )^{ - 1} }  \\\\\n   { - \\bar \\Sigma _{21} (\\bar \\Sigma _{11} )^{ - 1} }  \\\\\n\\end{array}} & {\\begin{array}{*{20}c}\n   {\\bar \\mu _2  - \\bar \\mu _1 (\\bar \\Sigma _{11} )^{ - 1} \\bar \\Sigma _{12} }  \\\\\n   { - (\\bar \\Sigma _{11} )^{ - 1} \\bar \\Sigma _{12} }  \\\\\n   {\\bar \\Sigma _{22}  - \\bar \\Sigma _{21} (\\bar \\Sigma _{11} )^{ - 1} \\bar \\Sigma _{12} }  \\\\\n\\end{array}}  \\\\\n\\end{array}} \\right]\n</math>\n\nReverse sweepings are similar to those of forward ones, except for a sign difference for some multiplications. However, forward and reverse sweepings are opposite operations. It can be easily shown that applying the fully reverse sweeping to <math>M(\\vec X)</math>   will recover the initial moment matrix  M(X). It can also be proved that applying a partial reverse sweeping on X to the matrix  <math> M(\\vec X,Y)</math> will recover the moment matrix M(X,Y). As a matter of fact, Liu<ref>L. Liu, "Local Computation of Gaussian Belief Functions," \'\'International Journal of Approximate Reasoning\'\', vol. 22, pp. 217–248, 1999</ref> proves that a moment matrix will be recovered through a reverse sweeping after a forward sweeping on the same set of variables. It can be also recovered through a forward sweeping after a reverse sweeping. Intuitively, a partial forward sweeping factorizes a joint into a marginal and a conditional, whereas a partial reverse sweeping multiplies them into a joint.\n\n=== Combination ===\nAccording to [[Dempster’s rule]], the combination of belief functions may be expressed as the intersection of focal elements and the multiplication of probability density functions. [[Liping Liu]] applies the rule to linear belief functions in particular and obtains a formula of combination in terms of density functions. Later he proves a claim by [[Arthur P. Dempster]] and reexpresses the formula as the sum of two fully swept matrices. Mathematically, assume <math>M_1 (\\vec X) = \\left( {\\begin{array}{*{20}c}\n   {\\bar \\mu _1 }  \\\\\n   {\\bar \\Sigma _1 }  \\\\\n\\end{array}} \\right)\n</math>  and <math> M_2 (\\vec X) = \\left( {\\begin{array}{*{20}c}\n   {\\bar \\mu _2 }  \\\\\n   {\\bar \\Sigma _2 }  \\\\\n\\end{array}} \\right)\n</math>  are two LBFs for the same vector of variables X. Then their combination is a fully swept matrix:\n\n: <math> M(\\vec X) = \\left( {\\begin{array}{*{20}c}\n   {\\bar \\mu _1  + \\bar \\mu _2 }  \\\\\n   {\\bar \\Sigma _1  + \\bar \\Sigma _2 }  \\\\\n\\end{array}} \\right)\n</math>\n\nThis above equation is often used for multiplying two normal distributions. Here we use it to define the combination of two linear belief functions, which include normal distributions as a special case. Also, note that a vacuous linear belief function (0 swept matrix) is the neutral element for combination. When applying the equation, we need to consider two special cases. First, if two matrices to be combined have different dimensions, then one or both matrices must be vacuously extended, i.e., assuming ignorance on the variables that are no present in each matrix. For example, if M<sub>1</sub>(X,Y)  and M<sub>2</sub>(X,Z)  are to be combined, we will first extend them into <math> M_1 (X,Y,\\vec Z)</math>  and <math> M_2 (X,\\vec Y,Z)</math>  respectively such that  <math> M_1 (X,Y,\\vec Z)</math> is ignorant about Z and  <math> M_2 (X,\\vec Y,Z)</math> is ignorant about Y. The vacuous extension was initially proposed by Kong <ref>A. Kong, "Multivariate belief functions and graphical models," in Department of Statistics. Cambridge, MA: Harvard University, 1986</ref> for discrete belief functions. Second, if a variable has zero variance, it will not permit a sweeping operation. In this case, we can pretend the variance to be an extremely small number, say ε, and perform the desired sweeping and combination. We can then apply a reverse sweeping to the combined matrix on the same variable and let ε approaches 0. Since zero variance means complete certainty about a variable, this ε-procedure will vanish ε terms in the final result.\n\nIn general, to combine two linear belief functions, their moment matrices must be fully swept. However, one may combine a fully swept matrix with a partially swept one directly if the variables of the former matrix have been all swept on in the later. We can use the linear regression model — Y = XA + b + E — to illustrate the property. As we mentioned, the regression model may be considered as the combination of two pieces of knowledge: one is specified by the linear equation involving three variables X, Y, and E, and the other is a simple normal distribution of E, i.e., E ~ N(0, Σ). Let  <math>M_1 (\\vec X,\\vec {\\rm E},Y) = \\left[ {\\begin{array}{*{20}c}\n   0 & 0 & b  \\\\\n   0 & 0 & A  \\\\\n   0 & 0 & I  \\\\\n   {A^T } & I & 0  \\\\\n\\end{array}} \\right]\n</math> and <math> M_2 (\\vec {\\rm E}) = \\left[ {\\begin{array}{*{20}c}\n   0  \\\\\n   { - \\Sigma ^{ - 1} }  \\\\\n\\end{array}} \\right]\n</math>  be their moment matrices respectively. Then the two matrices can be combined directly without sweeping <math> M_1 (\\vec X,\\vec {\\rm E},Y)\n</math>  on Y first. The result of the combination is a partially swept matrix as follows:\n\n: <math> M(\\vec X,\\vec {\\rm E},Y) = \\left[ {\\begin{array}{*{20}c}\n   0 & 0 & b  \\\\\n   0 & 0 & A  \\\\\n   0 & { - \\Sigma ^{ - 1} } & I  \\\\\n   {A^T } & I & 0  \\\\\n\\end{array}} \\right]\n</math>\n\nIf we apply a reverse sweeping on E and then remove E from the matrix, we will obtain the same representation of the regression model.\n\n== Applications ==\n\nWe may use an audit problem to illustrate the three types of variables as follows. Suppose we want to audit the ending balance of accounts receivable (\'\'E\'\'). As we saw earlier, \'\'E\'\' is equal to the beginning balance (\'\'B\'\') plus the sales (\'\'S\'\') for the period minus the cash receipts (\'\'C\'\') on the sales plus a residual (\'\'R\'\') that represents insignificant sales returns and cash discounts. Thus, we can represent the logical relation as a linear equation:\n\n: <math>E=B+S-C+R</math>\n\t\nFurthermore, if the auditor believes \'\'E\'\' and \'\'B\'\' are 100 thousand dollars on the average with a standard deviation 5 and the covariance 15, we can represent the belief as a multivariate normal distribution. If historical data indicate that the residual R is zero on the average with a standard deviation of 0.5 thousand dollars, we can summarize the historical data by normal distribution \'\'R\'\'&nbsp;~&nbsp;N(0,&nbsp;0.5<sup>2</sup>).  If there is a direct observation on cash receipts, we can represent the evidence as an equation say, C = 50 (thousand dollars). If the auditor knows nothing about the beginning balance of accounts receivable, we can represent his or her ignorance by a vacuous LBF. Finally, if historical data suggests that, given cash receipts&nbsp;\'\'C\'\', the sales \'\'S\'\' is on the average 8\'\'C\'\'&nbsp;+&nbsp;4 and has a standard deviation 4 thousand dollars, we can represent the knowledge as a linear regression model \'\'S\'\'&nbsp;~&nbsp;N(4&nbsp;+&nbsp;8\'\'C\'\',&nbsp;16).\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Linear Belief Function}}\n[[Category:Knowledge representation]]']
['Agricultural Information Management Standards', '5465644', "{{Infobox website\n|name=Agricultural Information Management Standards (AIMS)\n|logo =[[File:Agricultural Information Management Standards (AIMS) logo.jpg|100px|AIMS logo]]\n|slogan = ''Standards, Tools, Services & Advice''\n|url ={{URL|http://aims.fao.org}}\n|type = [[Community of Practice]]\n|commercial      = No\n|registration    = Optional\n|language        = English\n|launch date = 2006\n|current status = Online\n|screenshot     = \n}}\n\n[http://aims.fao.org/ '''Agricultural Information Management Standards'''], abbreviated to '''AIMS''' is a space for accessing and discussing agricultural information management standards, tools and methodologies connecting information workers worldwide to build a global community of practice. Information management standards, tools and good practices can be found on AIMS:\n\n* to support the implementation of structured and linked information and knowledge to enable institutions and individuals from different technical backgrounds to build open and interoperable information systems;\n* to provide advice on how to best manage, disseminate, share and exchange agricultural scientific information;\n* to promote good practices widely applicable and easy to implement, and;\n* to foster communities of practices centered on interoperability, reusability and cooperation.\n\n== Users ==\n\nAIMS is primarily intended for information workers—librarians, information managers, software developers—but is also of interest to those who are simply passionate about knowledge and information sharing. The success of AIMS depends upon its communities reaching a critical mass to show that the investment in interoperability standards has a return.\n\n== Community ==\n\nAIMS holds [http://aims.fao.org/communities-aims 9 communities of practice]. They are intended to discuss and share information about the different ongoing initiatives under the AIMS umbrella. AIMS supports collaboration through forums and blogs amongst institutions and individuals that wish to share expertise on how to use tools, standards and methodologies. Moreover, news and events are published on AIMS as part of its ‘one-stop” access to interoperability and reusability of information resources. The AIMS communities are aimed at the global agricultural community, including information providers, from research institutes, academic institutions, educational and extension institutions and also the private sector.\n\n== Content ==\n\n=== Vocabularies ===\n\n* [[AGROVOC]] is a comprehensive multilingual vocabulary that contains close to 40,000 concepts in over 20 languages covering subject fields in agriculture, forestry and fisheries together with cross-cutting themes such as land use, rural livelihoods and food security.<ref>{{cite web|url=ftp://ftp.fao.org/docrep/fao/010/ai144e/ai144e00.pdf |title=Basic Guidelines for Managing AGROVOC|year=2008 |accessdate=2011-08-01}}</ref> It standardizes data description to enable a set of core integration goals: interoperability, reusability and cooperation.<ref>{{cite web|url=http://www.fao.org/docrep/008/af238e/af238e04.htm |title=Agricultural Information Systems and Common Exchange Standards|year=2005 |accessdate=2011-08-01}}</ref> In this spirit of collaboration, [[AGROVOC]] also works with other organizations that are using [[Linked Open Data]] techniques to connect vocabularies and build the backbone of the next generation of internet data; data that is marked up not just for style but for meaning. It is maintained by a global community of librarians, terminologists, information managers and software developers<ref>{{cite web|url=http://aims.fao.org/standards/agrovoc/community |title=AGROVOC Community |accessdate=2011-08-01}}</ref> using [http://aims.fao.org/tools/vocbench-2 VocBench], a multilingual, web-based vocabulary editor and workflow management tool that allows for simultaneous, distributed editing.<ref>{{cite web|url=http://aims.fao.org/tools/vocbench-2 |title=VocBench Homepage |accessdate=2011-08-01}}</ref>\n* In addition to AGROVOC, AIMS provides access to other vocabularies like the [[Geopolitical ontology]] and [http://aims.fao.org/standards/agvocabularies/fisheries-ontology Fisheries Ontologies]. The [[Geopolitical ontology]] is used to facilitate data exchange and sharing in a standardized manner among systems managing information about countries and/or regions. The network of fisheries ontologies was created as a part of the [http://www.neon-project.org/nw/Welcome_to_the_NeOn_Project NeOn Project] and it covers the following areas: Water areas: for statistical reporting, jurisdictional ([[EEZ]]), environmental (LME), Species: taxonomic classification, ISSCAAP commercial classification, Aquatic resources, Land areas, Fisheries commodities, Vessel types and size, Gear types, [[AGROVOC]], ASFA.<ref>{{cite web|url=http://www.fao.org/docrep/field/009/ai254e/ai254e00.htm |title=Revised and enhanced fisheries ontologies |accessdate=2011-08-01}}</ref>\n* [[AgMES]] is as a namespace designed to include agriculture specific extensions for terms and refinements from established standard metadata namespaces like [[Dublin Core]] or [[Australian Government Locator Service|AGLS]], used for Document-like Information Objects, for example like publications, articles, books, web sites, papers, etc.<ref>{{cite web|url=ftp://193.43.36.44/gi/gil/gilws/aims/publications/workshops/coherence0/ppt/agmes.pdf |title=Agricultural Metadata Element Set: Standardization and Information Dissemination|accessdate=2011-08-01}}</ref>\n* Linked Open Data (LOD) - Enabled Bibliographic Data [http://aims.fao.org/lode/bd (LODE-BD) Recommendations 2.0] are a reference tool that assists bibliographic data providers in selecting appropriate encoding strategies according to their needs in order to facilitate metadata exchange by, for example, constructing crosswalks between their local data formats and widely used formats or even with a [[Linked Data]] representation\n\n=== Tools ===\n* [http://aims.fao.org/tools/agridrupal AgriDrupal] is both a suite of solutions for agricultural information management and a community of practice around these solutions.  The AgriDrupal community is made up of people who work in the community of agricultural information management specialists and have been experimenting with IM solutions in [[Drupal]].<ref>{{cite web|url=http://www.fao.org/docrep/article/am642e.pdf |title=AgriDrupal: repository management integrated into a content management system |accessdate=2011-08-01}}</ref>\n* [http://aims.fao.org/agriocean-dspace AgriOcean DSpace] is a joint initiative of the [[United Nations]] agencies of [[FAO]] and [[UNESCO]]-IOC/IODE to provide a customized version of [[DSpace]]. It uses standards for [[metadata]], [[thesauri]] and other [[controlled vocabularies]] for [[oceanography]], [[marine science]], food, agriculture, development, [[fisheries]], [[forestry]], [[natural resources]] and other related sciences.<ref>{{cite web|url=http://eprints.rclis.org/handle/10760/15812 |title=AgriOcean DSpace : FAO and UNESCO/IOC-IODE Combine Efforts in their Support of Open Access |accessdate=2011-08-01}}</ref>\n* [http://aims.fao.org/tools/vocbench-2 VocBench] is a web-based multilingual vocabulary management tool developed by [[FAO]] and hosted by [[MIMOS Berhad]]. It transforms thesauri, authority lists and glossaries into [[SKOS]]/[[Resource Description Framework|RDF]] concept schemes for use in a linked data environment. VocBench also manages the workflow and editorial processes implied by vocabulary evolution such as user rights/roles, validation and versioning. VocBench  supports a growing set of user communities, including the global, distributed group of terminologists who manage [[AGROVOC]].<ref>{{cite web|url=http://semtech2011.semanticweb.com/uploads/handouts/MON_600_Jaques_3910.pdf |title=VocBench: vocabulary editing and workflow management |accessdate=2011-08-01}}</ref>\n* [http://aims.fao.org/tools/webagris-2 WebAGRIS] is a multilingual Web-based system for distributed data input, processing and dissemination (through the Internet or on CD-Rom), of agricultural bibliographic information. It is based on common standards of data input and dissemination formats ([[XML]], [[Html|HTML]], ISO2709), as well as subject categorization schema and [[AGROVOC]].<ref>{{cite web|url=ftp://ftp.fao.org/docrep/fao/010/ai161e/ai161e00.pdf |title=FAO’s experience in metadata exchange from CDS/ISIS bibliographic databases using XML format, compliant to Dublin Core standard |accessdate=2011-08-01}}</ref>\n\n=== Services ===\n* [http://www.agrifeeds.org/ AgriFeeds] is a service that allows users to search and filter news and events from several agricultural information sources and to create custom feeds based on the filters applied.<ref>{{cite web|url=ftp://ftp.fao.org/docrep/fao/011/ak182e/ak182e00.pdf |title=AgriFeeds: The Agricultural News and Events Aggregator |accessdate=2011-08-01}}</ref> AgriFeeds was designed in the context on [http://www.ciard.net/ CIARD] (Coherence in Information for Agricultural Research for Development). Within CIARD, the partners who designed and implemented AgriFeeds are [[FAO]] and [[Global Forum on Agricultural Research|GFAR]]. AgriFeeds is currently maintained by [[FAO]].\n* [[AGRIS]] is a global public domain database with nearly 3 million structured bibliographical records on agricultural science and technology. The database is maintained by [[FAO]], with the content provided by more than 100 participating institutions from 65 countries.<ref>{{cite web|url=http://agris.fao.org/knowledge-and-information-sharing-through-agris-network |title=Knowledge and information sharing through the AGRIS Network |accessdate=2011-08-01}}</ref>\n* [http://ring.ciard.net/ CIARD Routemap to Information Nodes and Gateways (RING)] is a project implemented within CIARD and is led by [[Global Forum on Agricultural Research|GFAR]]. The RING is a global registry of web-based services that give access to any kind of information pertaining to agricultural research for development (ARD). It allows information providers to register their services in various categories and so facilitate the discovery of sources of agriculture-related information across the world.<ref>{{cite web|url=http://www.fao.org/docrep/012/al207e/al207e00.pdf |title=The CIARD RING, an infrastructure for interoperability of agricultural research information services |year=2010 |accessdate=2011-08-01}}</ref>\n* Since January 2011, AIMS supports [[E-lis|E-LIS]], the international electronic [[Open Archives Initiative|archive]] for [[Library science|library and information science]] (LIS). E-LIS is established, managed and maintained by an international team of 73 librarians and information scientists from 47 countries and support for 22 languages. It is freely accessible, aligned with the [[Open access (publishing)|Open Access]] (OA) movement and is a voluntary enterprise. Currently it is the largest international repository in the LIS field. Searching or browsing E-LIS is a kind of multilingual, multicultural experience, an example of what could be accomplished through open access archives to bring the people of the world together.<ref>{{cite web|url=http://eprints.rclis.org/handle/10760/6634 |title=E-LIS: an international open archive towards building open digital libraries |year=2005 |accessdate=2011-08-02}}</ref>\n* [http://aims.fao.org/vest-registry VEST Registry] is a catalog of controlled vocabularies (such as authority files, classification systems, [[concept maps]], controlled lists, dictionaries, [[ontologies]] or subject headings); [[metadata]] sets ([[metadata]] element sets, namespaces and application profiles); and tools (such as library management software, content management systems or document repository software). It is concerned primarily with collecting and maintaining a consistent set of [[metadata]] for each resource. The scope of the VEST Registry is to provide a clearing house for tools, [[metadata]] sets and vocabularies used in food, [[agriculture]], development, [[fisheries]], [[forestry]] and [[natural resources]] [[information management]] context.\n\n==See also==\n* [[AGRIS]]\n* [[AGROVOC]]\n* [[E-LIS]]\n* [[IMARK]]\n* [[Geopolitical ontology]]\n\n== References ==\n\n{{Reflist|2}}\n\n== External links ==\n* [http://aims.fao.org/home Agricultural Information Management Standards]\n\n[[Category:Agriculture]]\n[[Category:Food and Agriculture Organization]]\n[[Category:Standards]]\n[[Category:Information science]]\n[[Category:Knowledge]]\n[[Category:Knowledge representation]]\n[[Category:Library science]]"]
['Brand page', '34215536', 'A \'\'\'brand page\'\'\' (also known as a \'\'\'page\'\'\' or \'\'\'fan page\'\'\'), in online social networking parlance, is a profile on a social networking website which is considered distinct from an actual [[user profile]] in that it is created and managed by at least one other registered user as a representation of a non-personal [[online identity]]. This feature is most used to represent the brands of organizations associated with, properties owned by, or general interests favored by a user of the hosting network.\n\nWhile also being potentially manageable by more than one registered user, pages are distinguished from [[Group (online social networking)|groups]] in that pages are usually designed for the managers to direct messages and posts to subscribing users (akin to a [[newsletter]] or [[blog]]) and promote a brand, while groups are usually and historically formed for discussion purposes.\n\n==History==\nPrior to 2007, only a few websites made use of non-personal profile pages. [[Last.fm]], established in 2002, used its music recommendation service to automatically generate "artist pages" which serve as portals for biographies, events and artist-related playlists. This approach, however, is not explicitly controlled by artists or music groups because of the automatic nature of artist pages; pages, for example, could be created from erroneous misspellings and miscredits of works which are accepted as-is by the Audioscrobbler recommendation service used by Last.fm. Furthermore, Last.fm has never advertised itself as a social networking service, despite accruing myriad social features since 2002.\n\nThe most high-profile usage of this model is [[Facebook]]\'s Pages (formerly known as "Fan Page" until 2010) feature, launched in 2007; one could "be a fan of" a page until April 2010, when the parlance was replaced with "Like".<ref>{{cite web|url=http://www.allfacebook.com/2010/04/facebooks-become-a-fan-officially-switches-to-like/ | title=Facebook’s "Become A Fan" Officially Switches To "Like" | author=[[Nick O\'Neill]]|publisher = AllFacebook.com|date = April 19, 2010 <!-- 4:37 PM --> }}</ref> [[Foursquare]], a location-oriented social networking site, launched its "Brands" feature allowing for the creation of specialized brand pages in January 2010 (with [[Intel]] being the first user), but they did not become "self-serve" (controllable by individuals employed by page brand owners) until August 2011.<ref>{{cite web|url = http://blog.foursquare.com/2011/08/02/pages-are-now-self-serve-a-new-home-for-brands-and-organizations-on-foursquare/|title = Pages are now self-serve! A new home for brands and organizations on foursquare.|date = Aug 2, 2011|publisher = Foursquare}}</ref> [[LinkedIn]], an enterprise-oriented social networking service, launched "Company Pages" in November 2010.<ref>{{cite web|url = http://blog.linkedin.com/2010/11/01/linkedin-company-pages/|title = Recommend your favorite products and services on LinkedIn Company Pages|author = Ryan Roslansky|publisher = LinkedIn|date = November 1, 2010}}</ref> [[Google+]], the current social networking service operated by [[Google]], launched its own "Pages" feature in October 2011.<ref>{{cite web|url = http://googleblog.blogspot.com/2011/11/google-pages-connect-with-all-things.html|title = Google+ Pages: connect with all the things you care about|publisher = Google|date = 11-07-2011 <!-- 10:01:00 AM --> }}</ref> On November 19th, 2012, [[Amazon.com|Amazon]] announced Amazon Pages giving brands self-service control over their presence on the site.<ref>{{cite web|url = http://techcrunch.com/2012/11/20/amazon-offers-amazon-pages-for-brands-to-customize-with-their-own-urls-and-amazon-posts-for-social-media-marketing/|title = Amazon Offers ‘Amazon Pages’ For Brands To Customize With Their Own URLs, And ‘Amazon Posts’ For Social Media Marketing|author = TechCrunch|date = November 20, 2012}}</ref> On 8 December, [[Twitter]] announced that it would roll out "brand pages" as part of a major user interface redesign in 2012.<ref>{{cite web|url = http://advertising.twitter.com/2011/12/let-your-brand-take-flight-on-twitter.html|title = Let your brand take flight on Twitter with enhanced profile pages|publisher = Twitter Advertising Blog|author = TwitterAds|date = December 8, 2011}}</ref><ref>{{cite web|url = http://adage.com/article/digital/twitter-joins-facebook-google-launches-brand-pages-marketers/231448/|title = Twitter Joins Facebook, Google, Launches \'Brand Pages\' for Marketers|author = Cotton Delo|date = December 8, 2011}}</ref>\n\n==Features==\nIncreasingly, brand pages make use of the following features: \n* Header banners\n* The ability to post blogs or replies on the brand page in the name of the brand page\n* The ability to administer multiple pages\n* Photos\n* Video\n* Maps (including physical location of the page)\n* Subscribers\n* Other apps\n\nTwitter made use of header banners in their launch of brand pages, and Facebook made use of "cover photos" in their re-design of brand pages in March 2011.\n\n==Uses==\nOrganizations and brands regularly make use of pages in order to syndicate news and upcoming events, especially off-site blog posts, to subscribing users. Page subscription numbers can also be used as a metric of trust or interest in the associated brand.\n\nInterests can also be indexed as pages, and are often the basis for the formation of mass social movements (i.e., the [[Arab Spring]], [[Occupy Wall Street]]).\n\n===Newsroom accounts===\nPages are also used as newsroom accounts.\n\nA \'\'\'[[newsroom]] account\'\'\' refers to any microblogging or social networking account branded by or owned by a publishing or broadcasting organization which is dedicated solely to syndicating content from a particular category of content as published on the original website of the organization. Such accounts have come into increased usage by news organizations as means by which:\n# A news organization\'s presence on a social networking or microblogging website is increased\n# A news organization can specialize content syndication to selective users who wish to subscribe\n\nNews organizations who make use of multiple newsroom accounts typically allow for either online editors or multiple employed authors to edit and update the syndications of newsroom content. Such accounts are typically marked by graphic icons which make use of the brand symbol combined with distinct colors assigned to each account.\n\nExamples of newsroom accounts and pages include the Facebook pages for both \'\'[[The Guardian]]\'\' and the newspaper\'s Technology newsroom.\n\n==Impact==\n\n===Pseudonyms===\nThe usage of pseudonyms on social networking services, long considered a preserve of user privacy, has been partially affected by the promotion of pseudonyms, as social networking services have encouraged users to create pages for pseudonyms and implemented legal name requirements for user profile registration (i.e., New York resident Stefani Germanotta keeping a separate personal user profile under her legal name while maintaining a fan page under her stage name and pseudonym [[Lady Gaga]]).\n\n===Interest-based connections===\nAs pages can be created to represent interests, the number of attempts to create vertical social networking services (i.e., [[Ning (website)|Ning]]) has leveled off in the 2010s. [[Social network advertising]] can also be targeted to users based upon their page subscriptions.\n\n==See also==\n* [[Fansite]]\n* [[Landing page]]\n\n==References==\n{{reflist}}\n\n{{Online social networking}}\n{{Microblogging}}\n\n{{DEFAULTSORT:Page (online social networking)}}\n[[Category:Software features]]\n[[Category:Knowledge representation]]\n[[Category:Identity management]]']
['Category:Belief revision', '36312376', '{{Cat main}}\n{{see also|WP:WikiProject Philosophy/Resources}}\n\n[[Category:Belief]]\n[[Category:Formal epistemology]]\n[[Category:Knowledge representation]]\n[[Category:Logic]]\n[[Category:Logic programming]]']
['Prezi', '23948922', "{{Infobox Website\n| name           = Prezi\n| logo           = [[File:Prezi logo transparent 2012.svg|frameless|150px]]\n| url            = {{url|https://www.prezi.com/}}\n| origin         = [[Hungary]]\n| founder        = [[Adam Somlai-Fischer]]<br>Peter Halacsy<br>[[Peter Arvai]]\n| language       = [[English language|English]], [[Portuguese language|Portuguese]], [[Spanish language|Spanish]], [[Korean language|Korean]], [[Japanese language|Japanese]], [[German language|German]], [[Italian language|Italian]], [[French language|French]], [[Hungarian language|Hungarian]]\n| type           = [[Presentation Software|Presentation]] [[Collaboration tool|Collaboration]]\n| launch date    = {{start date and age|2009|4|5}}\n| current status = Active\n}}\n\n'''Prezi''' is a visual storytelling software alternative to traditional slide-based presentation formats. Prezi presentations feature a map-like, schematic overview that lets users pan between topics at will, zoom in on desired details, and pull back to reveal context.\n\nThis freedom of movement enables “conversational presenting,” a new presentation style in which presentations follow the flow of dialogue, instead of vice-versa.\n\nFounded in 2009, and with offices in San Francisco, Budapest, and Mexico City, Prezi now fosters a community of over 75 million users with more than 260 million prezis around the world.\n\nThe company launched Prezi Business in 2016, with a suite of creation, collaboration, and analytics tools for teams. Prezi Business is an HTML5 application that runs on JavaScript.\n\nThe word ''Prezi'' is the short form of “presentation” in Hungarian.\n\n== History ==\n\nPrezi was founded in 2008 in Budapest, Hungary by Adam Somlai-Fischer, Peter Halacsy, and Peter Arvai. \n\nThe earliest zooming presentation prototype had been previously developed by Somlai-Fischer to showcase his media-art pieces. Halacsy, an engineer, saw one of these presentations and proposed to improve the software. They were joined by entrepreneur and future CEO Arvai with the goal of making Prezi a globally recognized SaaS company.\n\nThe company established incorporation on May 20, 2009 and received its first major investment from TED two months later. A San Francisco office was opened that December. \n\nEarly 2011 saw the launch of Prezi’s first iPad application, followed by $14M in Series B funding led by Accel Partners. A Prezi iPhone app was launched in late 2012. \n\nIn March of 2014, Prezi pledged $100M in free licenses to Title 1 schools as part of the Obama administration’s ConnectED program. November of that year saw the announcement of $57M in new funding from Spectrum Equity and Accel Partners. \n\nPrezi for Android was launched in 2015, and in June of 2016, the company launched Prezi Business. As of June 2, 2016, Prezi reports 75 million registered users and 1 billion ‘prezi’ presentation views worldwide.\n\n== Products and features ==\n[[File:Path Tool.png|thumb|Prezi Path Tool]]\n\n=== Prezi ZUI ===\nThe Prezi online and offline ZUI editors employ a common tool palette, allowing users to pan and zoom, and to size, rotate, or edit an object. The user places objects on a canvas and navigates between videos, images, texts and other presentation media. Frames allow grouping of presentation media together as a single presentation object. Paths are navigational sequences that connect presentation objects for the purposes of structuring a linear presentation.\n\n=== Prezi Desktop ===\nPrezi Desktop<ref>{{cite web|url=http://prezi.com/desktop/ |title=Desktop |publisher=Prezi |date= |accessdate=2012-06-05}}</ref> allows Prezi Pro or Edu Pro subscribers to work off-line and create and save their presentations on their own [[Microsoft Windows|Windows]] or [[Mac OS X|Mac]] systems. Prezi Desktop Editor allows users to work on the presentation off-line in a .pez file format. Users can have files up to 500 MB in size when signing up with a school-affiliated e-mail address. This storage capability doesn't affect when users use an appropriate third-party conversion software with [[FLV]] or [[SWF]] format.<ref>{{cite web|url=http://prezi.com/desktop/ |title=Desktop |publisher=Prezi |date= |accessdate=2012-07-23}}</ref>\n\n=== Prezi Collaborate ===\nPrezi Collaborate is an online collaboration feature that allows up to ten people (co-located or geographically separated) to co-edit and show their presentations in real time. Users participate in a prezi simultaneously, and each is visually represented in the presentation window by a small avatar. Although Prezi Meetings can be done simultaneously, that is not the only option. Participants can be invited to edit the Prezi presentation at a later time if they wish. A link will be sent and the participant has up to ten days to edit the presentation. Prezi Meeting is included in all license types.\n\n===Prezi Viewer for iPad===\nPrezi Viewer<ref>{{cite web|url=http://prezi.com/ipad/ |title=Viewer for iPad |publisher=Prezi |date= |accessdate=2012-06-05}}</ref> is an app developed for the [[iPad]] for viewing prezis created on one's Prezi online account. The iPad [[touchscreen]] and [[multi-touch]] [[user interface]] enables users to pan, and pinch to zoom in or out of their media.\n\nPrezzip also offers templates for PreziU, with tool kits and visuals for file presentations.<ref>{{cite web|url=http://www.prezzip.com/index.php/footer-pages/about/prezi-ipad-viewer/|title=Prezi iPad viewer |publisher=Prezzip |accessdate=25 July 2012}}</ref><ref>{{cite web|url=http://www.prezzip.com/index.php/footer-pages/about/what-we-do/ |title=what we do |publisher=Prezzip |date= |accessdate=2012-07-25}}</ref>\n\n== Revenue model ==\nPrezi uses the [[freemium]] model. Customers who use the product's Public license must publish their work on the Prezi.com website, which is publicly viewable. Customers who pay for a Prezi Enjoy or Prezi Pro can make their presentations private. Only Pro license users have access to Prezi Desktop, which enables offline editing. Prezi also offers an educational license for students and educators.\n\n== Uses ==\n\n=== Business and conferences ===\nSome users at the [[World Economic Forum]] are currently using Prezi for their presentations.<ref>{{cite web|url=http://blog.prezi.com/2009/11/03/how-to-create-a-good-prezi-for-the-world-economic-forum/ |title=zoomintoprezi - Latest - How to create a good prezi - World Economic Forum |publisher=Blog.prezi.com |date=2009-11-03 |accessdate=2012-06-05}}</ref> Many [[TED (conference)|TED Conference]] speakers have used Prezi, including TED curator [[Chris Anderson (entrepreneur)|Chris Anderson]], who used a Prezi for his TEDGlobal 2010 presentation: How Web Video Powers Global Innovation.<ref>{{cite web|author= |url=https://www.youtube.com/watch?annotation_id=annotation_962757&feature=iv&src_vid=X6Zo53M0lcY&v=LnQcCgS7aPQ |title=Chris Anderson: How YouTube is driving innovation |publisher=YouTube |date=2010-09-14 |accessdate=2015-05-06}}</ref> Michael Chasen, President/CEO of [[Blackboard, Inc.]], used Prezi to deliver the keynote at their BbWorld 2011 annual users' conference.<ref>{{cite web|author= |url=https://www.youtube.com/watch?v=rlGA9_p_--c |title=BbWorld 2011 Corporate Keynote |publisher=YouTube |date=2011-07-26 |accessdate=2012-06-05}}</ref> [[FBLA]] members have recently started using this software.{{citation needed|reason=|date=September 2013}}\n\n=== Education ===\nPrezi is used at [[Oregon State University]],<ref>{{cite web|author= not fuly true. should expand further|url=http://calendar.oregonstate.edu/event/63614 |title=Prezi in the  classroom |publisher=Oregon University State University calendar |date= |accessdate=2012-07-24}}</ref> as well as at the [[Dwight School]]<ref>{{cite news|last=Anderson |first=Jenny |url=http://cityroom.blogs.nytimes.com/2011/06/21/at-a-private-school-virtual-learning-and-the-rock/ |title=At Dwight School, Virtual Learning and the Rock - NYTimes.com |location=Manhattan (NYC) |publisher=Cityroom.blogs.nytimes.com |date=2011-06-21 |accessdate=2012-06-05}}</ref> and elsewhere in primary education and higher education.<ref>{{cite web|author=Zoltan Radnai|url=http://edu.prezi.com/article/27827/-Prezi-makes-you-stop-and-think/|title=Prezi makes you stop and think|publisher=Prezi|accessdate=23 July 2012}}</ref> It can be used by teachers and students to collaborate on presentations with multiple users able to access and edit the same presentation,<ref>{{cite web|author=Tilt |url=https://www.youtube.com/watch?v=lZyv6MTVsjc |title=Student Web 2.0 |publisher=YouTube |date= |accessdate=2012-07-18}}</ref> and to allow students to construct and present their knowledge in different learning styles.<ref>{{cite web|url=http://www.nactateachers.org/attachments/article/1060/NACTA%20Journal%20Vol%2055%20Sup%201.pdf/|title=Thinking outside of slide |publisher=NACTA |accessdate=24 July 2012}}</ref> The product is also being used in [[e-learning]] and [[edutainment]].<ref>{{cite web|author= |url=https://www.youtube.com/watch?v=s7nDT_KgPpk |title=Daniel Gallichan - 1. Platz beim 1. Freiburger Science Slam |publisher=YouTube |date= |accessdate=2012-06-05}}</ref> However note that Prezi is considered by Web2Access to be an 'inaccessible service'.<ref>{{cite web|url=http://www.web2access.org.uk/product/172/ |title=Results for Prezi|publisher=Web2Access JISC TechDis |date= |accessdate=2014-03-01}}</ref> Educators have been advised that Prezi is not ADA/508 compliant and that an accessible PowerPoint version of the presentation should be provided online for students where a Prezi has been used.<ref>{{cite web|url=http://webaccessibility.gmu.edu/prezi.html |title=Prezi Known Accessibility Issues|publisher=George Mason University |date= |accessdate=2014-03-01}}</ref>\n\n=== Information visualization ===\nIn July 2011, ''[[The Guardian]]'' used Prezi to publish a new world map graphic on their website, for an article about the newly independent South Sudan.<ref>{{cite news|author=Simon Rogers, Jenny Ridley |url=https://www.theguardian.com/news/datablog/interactive/2011/jul/08/world-map-new-south-sudan |title=The new world map: download it for yourself &#124; World news &#124; guardian.co.uk |publisher=Guardian |date= 2011-07-08|accessdate=2012-06-05 |location=London}}</ref>\n\n== Platform compatibility ==\nPrezi is developed in Adobe Flash, Adobe AIR and built on top of Django. It is compatible with most modern computers and web browsers. \n\nPrezi Business is an HTML5 application which runs on JavaScript. It also is compatible with most modern systems.\n\n== Criticism ==\nThe company has acknowledged that the “[[zooming user interface]] (ZUI)” has the potential to induce nausea, and offers tutorials with recommendations for use of layout to avoid excessive visual stimulation.<ref>{{cite web|url=http://prezi.com/learn/grouping-and-layering/ |title=Why the Best Prezis use Grouping & Layering &#124; Prezi Learn Center |publisher=Prezi.com |date= |accessdate=2012-06-05}}</ref>\nThere has also been criticism of Prezi’s lack of font and color options. Notably, Presentation Zen author Garr Reynolds once stated that he had never seen a good presentation using Prezi and was looking for one;<ref>{{cite web|url=http://garr.posterous.com/have-you-ever-seen-a-great-talk-given-with-th |title=Have you ever seen a great talk given with the help of Prezi? Do you have a link? - Garr's posterous |publisher=Garr.posterous.com |date=2010-09-10 |accessdate=2012-06-05 |deadurl=unfit |archiveurl=https://web.archive.org/web/20120402080355/http://garr.posterous.com/have-you-ever-seen-a-great-talk-given-with-th |archivedate=April 2, 2012 }}\n</ref> in a later post, he refers to Chris Anderson’s talk at TED Global 2010 as one of the best TED talks ever, commenting that it was a good use of Prezi.<ref>\n{{cite web|url=http://garr.posterous.com/on-train-to-tokyo-watching-one-of-the-best-te |title=On train to Tokyo watching one of the best TED talks ever - Garr's posterous |publisher=Garr.posterous.com |date=2010-09-14 |accessdate=2012-06-05 |deadurl=unfit |archiveurl=https://web.archive.org/web/20120402080641/http://garr.posterous.com/on-train-to-tokyo-watching-one-of-the-best-te |archivedate=April 2, 2012 }}\n</ref>\n\nAs Prezi is a Flash-based online zooming tool, most elements of the presentation cannot be read aloud by users with disabilities by means of a screen reader (e.g. it is not possible to add [[alt attribute]]s to images and [[iframe]]s used for the page design, and templates have been built to work without [[accessibility]] options). Prezi is considered by Web2Access to be an 'inaccessible service'.<ref>{{cite web|url=http://www.web2access.org.uk/product/172/ |title=Results for Prezi|publisher=Web2Access JISC TechDis |date= |accessdate=2014-03-01}}</ref> American educators have been advised that Prezi is not compliant with the Americans With Disabilities Act (ADA/508) and that an accessible PowerPoint version of the presentation should be provided online for students where a Prezi has been used.<ref>{{cite web|url=http://barrydahl.com/2015/01/08/accessibility-concerns-of-using-prezi-in-education/|title=Accessibility Concerns of Using Prezi in Education |publisher=Barry Dahl |date= |accessdate=2015-10-07}}</ref>\n\n==See also==\n* [[Scientific visualization]]\n* [[Data Presentation Architecture]]\n\n==References==\n{{reflist|30em}}\n\n==External links==\n{{Commons category|Mind maps}}\n* {{Official website|https://www.prezi.com}}\n\n{{Mindmaps}}\n{{Presentation software}}\n{{Notetaking softwares}}\n\n[[Category:Zoomable user interfaces]]\n[[Category:Panorama viewers]]\n[[Category:Presentation software]]\n[[Category:Knowledge representation]]\n[[Category:Diagrams]]\n[[Category:Note-taking software]]"]
['Spatial–temporal reasoning', '3342061', "{{about|spatial-temporal reasoning in information technology|spatial-temporal reasoning in psychology|Spatial visualization ability}}\n{{technical|date=October 2012}}\n'''Spatial–temporal reasoning''' is an area of [[Artificial Intelligence|artificial intelligence]] which draws from the fields of [[computer science]], [[cognitive science]], and [[cognitive psychology]]. The theoretic goal—on the cognitive side—involves representing and reasoning spatial-temporal knowledge in mind. The applied goal—on the computing side—involves developing high-level control systems of robots for navigating and understanding time and space. \n\n== Influence from cognitive psychology ==\nA convergent result in cognitive psychology is that the connection relation is the first spatial relation that human babies acquire, followed by understanding orientation relations and distance relations. Internal relations among the three kinds of spatial relations can be computationally and systematically explained within the theory of cognitive prism as follows: (1) the connection relation is primitive; (2) an orientation relation is a distance comparison relation: you being in front of me can be interpreted as you are nearer to my front side than my other sides; (3) a distance relation is connection relations using a third object: you being one meter away from me can be interpreted as an object with the maximum extension of one meter can be connected with you and me simultaneously. \n\n== Fragmentary representations of temporal calculi ==\nWithout addressing internal relations among spatial relations, AI researchers contributed many fragmentary representations. Examples of temporal calculi include [[Allen's interval algebra]], and Vilain's & Kautz's [[point algebra]]. The most prominent spatial calculi are [[Mereotopology|mereotopological calculi]], [[Andrew U. Frank|Frank]]'s [[cardinal direction calculus]], Freksa's double cross calculus, Egenhofer and Franzosa's [[9-intersection calculus|4- and 9-intersection calculi]], Ligozat's [[flip-flop calculus]], various [[region connection calculus|region connection calculi]] (RCC), and the [[Oriented Point Relation Algebra]]. Recently, spatio-temporal calculi have been designed that combine spatial and temporal information. For example, the [[spatiotemporal constraint calculus]] (STCC) by Gerevini and Nebel combines Allen's interval algebra with RCC-8. Moreover, the [[qualitative trajectory calculus]] (QTC) allows for reasoning about moving objects.\n\n== Quantitative abstraction ==\nAn emphasis in the literature has been on [[Qualitative reasoning|qualitative]] spatial-temporal reasoning which is based on qualitative abstractions of temporal and spatial aspects of the common-sense background knowledge on which our human perspective of physical reality is based.  Methodologically, qualitative [[Constraint satisfaction|constraint]] calculi restrict the vocabulary of rich mathematical theories dealing with temporal or spatial entities such that specific aspects of these theories can be treated within [[Decidability (logic)|decidable]] fragments with simple qualitative (non-[[Metric (mathematics)|metric]]) languages. Contrary to mathematical or physical theories about space and time, qualitative constraint calculi allow for rather inexpensive reasoning about entities located in space and time.  For this reason, the limited expressiveness of qualitative representation formalism calculi is a benefit if such reasoning tasks need to be integrated in applications.  For example, some of these calculi may be implemented for handling spatial [[Geographic information system|GIS]] queries efficiently and some may be used for navigating, and communicating with, a mobile [[robot]].\n\n== Relation algebra ==\nMost of these calculi can be formalized as abstract [[relation algebra]]s, such that reasoning can be carried out at a symbolic level. For computing solutions of a [[constraint network]], the [[Local consistency#Path_consistency|path-consistency algorithm]] is an important tool.\n\n== Software ==\n* [http://www.sfbtr8.spatial-cognition.de/de/projekte/reasoning/r4-logospace/research-tools/gqr/ GQR], constraint network solver for calculi like RCC-5, RCC-8, Allen's interval algebra, point algebra, cardinal direction calculus, etc.\n\n== See also ==\n*[[Cerebral cortex]]\n*[[Diagrammatic reasoning]]\n*[[Temporal logic]]\n*[[Visual thinking]]\n*[[Spatial ability]]\n\n== Notes ==\n{{reflist}}\n\n==References==\n*J. Renz, B. Nebel, [http://users.rsise.anu.edu.au/~jrenz/papers/renz-nebel-los.pdf Qualitative Spatial Reasoning using Constraint Calculi], in: M. Aiello, I. Pratt-Hartmann, J. van Benthem (eds.): Handbook of Spatial Logics, Springer 2007.\n*T. Dong: [http://www.jstor.org/stable/41217909?seq=1#page_scan_tab_contents A COMMENT ON RCC: FROM RCC TO RCC⁺⁺]. Journal of Philosophical Logic, Vol 34, No. 2, pp. 319--352\n*M. Vilain, H. Kautz, P. van Beek, [http://www.cs.rochester.edu/~kautz/papers/vilain-kautz-book.pdf Constraint propagation algorithms for temporal reasoning: A Revised Report], 1987.\n*T. Dong. [http://www.springer.com/de/book/9783642240577 Recognizing Variable Environment -- The Theory of Cognitive Prism]. Studies in Computational Intelligence, Vol. 388, Springer-Verlag, Berlin Heidelberg, 2012.\n\n{{DEFAULTSORT:Spatial-temporal reasoning}}\n[[Category:Cognitive science]]\n[[Category:Knowledge representation]]\n[[Category:Educational psychology]]\n[[Category:Logical calculi]]\n[[Category:Reasoning]]"]
['Category:Dewey Decimal Classification', '39327232', '{{catmore}}\n\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]\n[[Category:OCLC]]']
['DOAP', '4842020', '{{one source|date=October 2012}}\n\n\'\'\'DOAP\'\'\' (\'\'\'Description of a Project\'\'\') is an [[RDF Schema]] and [[XML]] vocabulary to describe  software projects, in particular [[free and open source software]].\n\nIt was created and initially developed by [[Edd Dumbill]] to convey semantic information associated with open source software projects.\n\n== Adoption ==\n\nThere are currently generators, [[validator]]s, viewers, and converters to enable more projects to be able to be included in the [[semantic web]]. [[Freshmeat]]\'s 43 000 projects are now available published with DOAP.<ref>{{ cite web | url = http://fgiasson.com/blog/index.php/2007/08/04/freshmeatnet-now-available-in-doap-43-000-new-doap-projects/ | title = Freshmeat.net now available in DOAP: 43 000 new DOAP projects | first = Frederick | last = Giasson | accessdate = 2010-04-08 }}</ref> It is currently used in the [[Mozilla Foundation]]\'s project page and in several other software repositories, notably the [[Python Package Index]].\n\nMajor properties include: doap:homepage, doap:developer, doap:programming-language, doap:os\n\n== Examples ==\n\nThe following is an example in RDF/XML:\n\n<source lang="xml">\n <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:doap="http://usefulinc.com/ns/doap#">\n  <doap:Project>\n   <doap:name>Example project</doap:name>\n   <doap:homepage rdf:resource="http://example.com" />\n   <doap:programming-language>javascript</doap:programming-language>\n   <doap:license rdf:resource="http://example.com/doap/licenses/gpl"/>\n  </doap:Project>\n </rdf:RDF>\n</source>\n\nOther properties include <code>Implements specification, anonymous root, platform, browse, mailing list, category, description, helper, tester, short description, audience, screenshots, translator, module, documenter, wiki, repository, name, repository location, language, service endpoint, created, download mirror, vendor, old homepage, revision, download page, license, bug database, maintainer, blog, file-release</code> and <code>release.</code>{{citation needed|date=January 2013}}\n\n==References==\n\n{{reflist}}\n\n==External links==\n* {{github|edumbill/doap/|Doap Project}}\n* [http://www.oss-watch.ac.uk/resources/doap.xml OSS Watch DOAP Briefing Note]\n* [http://crschmidt.net/semweb/doapamatic/ doapamatic]: DOAP generator\n\n{{Semantic Web}}\n\n[[Category:Knowledge representation]]\n[[Category:Semantic Web]]\n[[Category:Ontology (information science)]]']
['ERIL', '42325907', '{{COI|date=June 2016}}\n{{Orphan|date=April 2014}}\n\n[[File:A simple example ERIL diagram.png|thumb|right|An example ERIL diagram with 3 classes and 3 one-to-many relationships.]]\n\n\'\'\'ERIL\'\'\' (\'\'\'Entity-Relationship and Inheritance Language\'\'\') is a [[visual language]] for representing the data structure of a computer system.\nAs its name suggests, ERIL is based on [[Entity–relationship model|entity-relationship]] diagrams and [[class diagram]]s.\nERIL combines the [[Relational data model|relational]] and [[Object-oriented programming|object-oriented]] approaches to [[data model]]ing.\n\n== Overview ==\nERIL can be seen as a set of guidelines aimed at improving the readability of structure diagrams.\nThese guidelines were borrowed from [[DRAKON]], a variant of [[flowchart]]s created within the Russian space program.\nERIL itself was developed by Stepan Mitkin.\n\nThe ERIL guidelines for drawing diagrams:\n* Lines must be straight, either strictly vertical or horizontal.\n* Vertical lines mean ownership ([[Object composition|composition]]).\n* Horizontal lines mean peer relationships ([[Object composition#Aggregation|aggregation]]).\n* Line intersections are not allowed.\n* It is not recommended to fit the whole data model on a single diagram. Draw many simple diagrams instead.\n* The same class (table) can appear several times on the same diagram.    \n* Use the following standard symbols to indicate the type of the relationship.\n** One-to-one: a simple line.\n** One-to-many, two-way: a line with a "paw".\n** One-to-many, one-way: an arrow.\n** Many-to-many: a line with two "paws".    \n* Do not lump together inheritance and data relationships.<ref>[http://drakon-editor.sourceforge.net/eril.html ERIL: a Visual Language for Data Modelling]</ref>\n\n== Indexes ==\nA class (table) in ERIL can have several indexes.\nEach index in ERIL can include one or more fields, similar to indexes in [[relational database]]s.\nERIL indexes are logical. They can optionally be implemented by real data structures.\n\n== Links ==\nLinks between classes (tables) in ERIL are implemented by the so-called "link" fields.\nLink fields can be of different types according to the link type:\n* reference;\n* collection of references.\n    \nExample: there is a one-to-many link between \'\'Documents\'\' and \'\'Lines\'\'. One \'\'Document\'\' can have many \'\'Lines\'\'. Then the \'\'Document.Lines\'\' field is a collection of references to the lines that belong to the document. \'\'Line.Document\'\' is a reference to the document that contains the line.\n\nLink fields are also logical. They may or may not be implemented physically in the system.\n\n== Usage ==\n\nERIL is supposed to model any kind of data regardless of the storage. \nThe same ERIL diagram can represent data stored in a [[relational database]], in a [[Nosql|NoSQL]] database, [[Xml|XML]] file or in the memory.\n\nERIL diagrams serve two purposes.\nThe primary purpose is to explain the data structure of an existing or future system or component.\nThe secondary purpose is to automatically generate source code from the model.\nCode that can be generated includes specialized collection classes, hash and comparison functions, data retrieval and modification procedures, [[Data definition language|SQL data-definition]] code, etc. Code generated from ERIL diagrams can ensure referential and uniqueness [[data integrity]].\nSerialization code of different kinds can also be automatically generated.\nIn some ways ERIL can be compared to [[object-relational mapping]] frameworks.\n\n== See also ==\n* [[Model-driven engineering]]\n* [[Unified Modeling Language|UML]]\n* [[Entity–relationship model]]\n* [[Flowchart]]s\n* [[Class diagram]]\n* [[DRAKON]]\n\n== Notes ==\n{{Reflist}}\n\n\n[[Category:Architecture description language]]\n[[Category:Data modeling languages]]\n[[Category:Data modeling diagrams]]\n[[Category:Diagrams]]\n[[Category:Knowledge representation]]\n[[Category:Specification languages]]\n[[Category:Software modeling language]]']
['Knowledge Engineering Environment', '11856314', "'''KEE''' (Knowledge Engineering Environment) is a [[Frame language|frame-based]] development tool for [[Expert system|Expert Systems]].<ref>[http://portal.acm.org/citation.cfm?id=62001 An evaluation of expert system development tools]</ref> KEE was developed and sold by [[IntelliCorp (Software)|IntelliCorp]]. It was first released in 1983 and ran on [[Lisp Machine]]s. KEE was later ported to Lucid [[Common Lisp]] with [[CLX (Common Lisp)|CLX]] (X11 interface for Common Lisp). This version was available on various Workstations.\n\nOn top of KEE several extensions were offered:\n\n* Simkit,<ref>[http://doi.acm.org/10.1145/76738.76766 The SimKit system: knowledge-based simulation and modeling tools in KEE]</ref><ref>[http://portal.acm.org/citation.cfm?id=62001 SimKit: a model-building simulation toolkit]</ref> a frame-based simulation library\n* KEEconnection,<ref>[http://portal.acm.org/citation.cfm?id=62001 KEEConnection: a bridge between databases and knowledge bases]</ref> [[database connection]] between the frame system and relational databases.\n\nFrames are called ''Units'' in KEE. Units are used for both individual instances and classes. Frames have ''slots'' and slots have ''facets''. Facets for example describe the expected values of a slot, the inheritance rule for the slot or the value of a slot. Slots can have multiple values. Behavior can be implemented using the message-passing paradigm.\n\nKEE provides an extensive graphical user interface to create, browse and manipulate frames.\n\nKEE also includes a frame-based [[Production system (computer science)|rule system]]. Rules themselves are frames in the KEE knowledge base. Both forward and backward chaining inference is available.\n\nKEE supports non-monotonic reasoning through the concepts of ''worlds''. Worlds allow provide alternative slot-values of frames. Through an assumption-based [[Truth maintenance system]] inconsistencies can be detected and analyzed.<ref>[http://portal.acm.org/citation.cfm?id=62001 Reasoning with worlds and truth maintenance]</ref>\n\n''ActiveImages'' allows graphical displays to be attached to slots of Units. Typical examples are buttons, dials, graphs and histograms. The graphics are also implemented as Units via ''KEEPictures'' - a frame-based graphics library.\n\n==See also==\n* [[Expert system]]\n* [[Frame language]]\n* [[Inference engine]]\n* [[IntelliCorp (software)|IntelliCorp]]\n* [[Knowledge base]]\n* [[Knowledge-based system]]\n* [[Knowledge representation]]\n\n==References==\n<references/>\n\n==External links==\n* [http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625 An Assessment of Tools for Building Large Knowledge-Based Systems]\n\n[[Category:Knowledge engineering]]\n[[Category:Knowledge representation]]\n[[Category:Common Lisp software]]"]
['Babelfy', '43480298', "{{multiple issues|\n{{context|date=August 2016}}\n{{notability|Products|date=August 2016}}\n}}\n{{Infobox software\n |name = Babelfy\n |logo = [[File:Babelfy_logo.png|140px|Babelfy logo.]]\n |screenshot =\n |caption = Babelfy\n |developer = \n |released = \n |latest_release_version = Babelfy 1.0\n |latest_release_date = June 2014\n |genre = {{Flatlist|\n* [[Word sense disambiguation]]\n* [[Entity linking]]\n}}\n |programming language = \n |license = [[Attribution-NonCommercial-ShareAlike 3.0 Unported]]\n |website = {{URL|babelfy.org}}\n |alexa   = \n}}\n\n'''Babelfy''' is an [[algorithm]] for the disambiguation of text written in any language. Specifically, Babelfy performs the tasks of [[Multilinguality|multilingual]] [[Word Sense Disambiguation]] (i.e., the disambiguation of common nouns, verbs, adjectives and adverbs) and [[Entity Linking]] (i.e. the disambiguation of mentions to encyclopedic entities like people, companies, places, etc.).<ref>A. Moro, A. Raganato, R. Navigli. [http://wwwusers.di.uniroma1.it/~navigli/pubs/TACL_2014_Babelfy.pdf Entity Linking meets Word Sense Disambiguation: a Unified Approach]. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 231-244, 2014.</ref> Babelfy is based on the [[BabelNet]] multilingual semantic network and performs disambiguation and entity linking in three steps:\n\n* It associates with each [[Vertex (graph theory)|vertex]] of the BabelNet semantic network, i.e., either [[concept]] or [[named entity]], a semantic signature, that is, a set of related vertices. This is a preliminary step which needs to be performed only once, independently of the input text.\n* Given an input [[Written text|text]], it extracts all the linkable fragments from this text and, for each of them, lists the possible [[meaning (linguistics)|meanings]] according to the [[semantic network]].\n* It creates a [[Graph (data structure)|graph-based]] semantic interpretation of the whole text by linking the candidate meanings of the extracted fragments using the previously-computed semantic signatures. It then extracts a dense [[Glossary of graph theory#Subgraphs|subgraph]] of this representation and selects the best candidate meaning for each fragment.\n\nAs a result, the text, written in any of the 271 [[language]]s supported by BabelNet, is output with possibly overlapping semantic annotations.\n\n==See also==\n* [[BabelNet]]\n* [[Entity linking]]\n* [[Multilinguality]]\n* [[Word sense disambiguation]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{Official website|http://babelfy.org}}\n\n[[Category:Lexical semantics]]\n[[Category:Semantics]]\n[[Category:Knowledge representation]]\n[[Category:Computational linguistics]]\n[[Category:Artificial intelligence]]\n[[Category:Multilingualism]]\n\n\n{{prog-lang-stub}}"]
['Logic Programming Associates', '1899829', '{{multiple issues|\n{{notability|Companies|date=January 2012}}\n{{more footnotes|date=April 2013}}\n}}\n\n{{Infobox company\n|name   = Logic Programming Associates Ltd\n|type   = [[Private company|Private]]\n|foundation = 1981\n|directors= Diane Reeve <br />Clive Spenser <br />Brian Steel \n|location = London SW18 3SX\n|area_served = UK, United States, [[Europe, the Middle East and Africa|EMEA]]\n|industry             = [[Computer software]] \n|products = [[VisiRule]], [[Flex expert system|Flex expert system toolkit]], [[Flint toolkit]], LPA Prolog for Windows\n|website = [http://www.lpa.co.uk www.lpa.co.uk]}}\n\n\'\'\'Logic Programming Associates\'\'\' (\'\'\'LPA\'\'\') is a company specializing in [[logic programming]] and [[artificial intelligence]] software. LPA was founded in 1980 and is widely known for its range of [[Prolog]] compilers and more recently for [[VisiRule]].\n\nLPA was established to exploit research at Imperial College, London into [[logic programming]] carried out under the supervision of [[Robert Kowalski|Prof Robert Kowalski]]. One of the first implementations made available by LPA was micro-PROLOG<ref name = "Prolog implementations">{{citation |url=http://www.berghel.com/publications/micropro/micropro_ncc87.pdf | title= Microcomputer PROLOG implementations | accessdate=2013-04-29}}</ref> which ran on popular 8-bit home computers such as the [[Sinclair Spectrum]]<ref name = "micro-PROLOG for Sinclair Spectrum">{{citation |url=http://www.worldofspectrum.org/infoseekid.cgi?id=0008429 | title= micro-PROLOG for Sinclair Spectrum | accessdate=2013-04-29}}</ref> and [[Apple II]]. This was followed by micro-PROLOG Professional one of the first Prolog implementations for MS-DOS.\n\nAs well as continue with Prolog compiler technology development, LPA has a track record of creating innovative associated tools and products to address specific challenges and opportunities.\n\nIn 1989, LPA developed the [[Flex expert system|Flex expert system toolkit]], which incorporated [[Frame language|frame-based]] reasoning with inheritance, [[rule-based programming]] and data-driven procedures. Flex has its own English-like Knowledge Specification Language (KSL) which means that knowledge and rules are defined in an easy-to-read and understand way.\n\nIn 1992, LPA helped set up the Prolog Vendors Group,<ref name = "PVG launched">{{citation |url=http://iospress.metapress.com/content/c1p1351212770518/fulltext.pdf | title=Prolog Vendors Group Launched | accessdate=2013-04-29}}</ref> a not-for-profit organization whose aim was to help promote Prolog by making people aware of its usage in industry.\n\nIn 2000, LPA helped set up [[Business Integrity]], now a leading supplier of document assembly and contract creation software solutions for the legal market.\n\nLPA\'s core product is LPA Prolog for Windows,<ref name = "WIN-PROLOG">{{citation |url=http://www.lpa.co.uk/win.htm | title= LPA Prolog for Windows | accessdate=2013-04-29}}</ref> a compiler and development system for the Microsoft Windows platform. The current LPA software range comprises an integrated AI toolset which covers various aspects of [[Artificial Intelligence]] including Logic Programming, [[Expert Systems]], [[Knowledge-based Systems]], Data Mining, Agents and [[Case-based reasoning]] etc.\n\nIn 2004, LPA launched [[VisiRule]] <ref name = "VisiRule">{{citation |url=http://www.lpa.co.uk/vsr.htm | title= LPA VisiRule | accessdate=2013-04-29}}</ref> a graphical tool for developing knowledge-based and decision support systems. VisiRule has been used in various sectors, to build [[legal expert systems]], machine diagnostic programs, medical and financial advice systems, etc.\n\n==Customers==\nFor many years, LPA has worked closely with [[Valdis Krebs]], an American-Latvian researcher, author, and consultant in the field of social and organizational network analysis. Valdis is the founder and chief scientist of Orgnet, and the creator of the popular Inflow <ref name = "InFlow">[http://www.orgnet.com/inflow3.html InFlow]</ref> software package.\n\n==External links==\n*[http://www.lpa.co.uk/ind_pro.htm LPA home page]\n*[http://www.lpa.co.uk/abo_lpa.htm About LPA]\n*[[:es:Micro-PROLOG|Micro-PROLOG (in Spanish)]]\n*[http://www.teamethno-online.org.uk/Issue2/Rouchy.pdf Aspects of PROLOG History]\n*[http://www.lpa.co.uk/vrs_dem.htm VisiRule demos]\n*[http://dssresources.com/news/83.php VisiRule: a new graphical business rules tool from LPA]\n*[http://dl.acm.org/citation.cfm?id=297981 A flex-based expert system for sewage treatment works support]\n*[http://www.lamsade.dauphine.fr/~tsoukias/papers/esse.pdf ESSE: An Expert System for Software Evaluation]\n\n== References ==\n{{Reflist}}\n\n[[Category:Information technology organisations]]\n[[Category:Software companies of the United Kingdom]]\n[[Category:Expert systems]]\n[[Category:Knowledge engineering]]\n[[Category:Knowledge representation]]\n\n\n{{compu-ai-stub}}']
['Semantic network', '29109', '{{Network Science}}\n\nA \'\'\'semantic network\'\'\', or \'\'\'frame network\'\'\',  is a network that represents [[Semantics|semantic]] relations between [[concept]]s. This is often used as a form of [[Knowledge representation and reasoning|knowledge representation]]. It is a [[directed graph|directed]] or [[undirected graph]] consisting of [[vertex (graph theory)|vertices]], which represent [[concept]]s, and [[graph theory|edges]], which represent semantic relations between concepts.<ref name = \'Sowa\'/>\n\nTypical standardized semantic networks are expressed as [[semantic triple]]s.\n\n== History ==\n[[Image:Semantic Net.svg|thumb|320px|Example of a semantic network]]\n"Semantic Nets" were first invented for [[computers]] by [[Richard H. Richens]] of the Cambridge Language Research Unit in 1956 as an "[[Pivot language|interlingua]]" for [[machine translation]] of [[natural language]]s.{{citation needed|date=October 2013}}\n\nThey were independently developed by Robert F. Simmons,<ref name=\'Simmons1963\'>{{cite journal | title=Synthetic language behavior | journal=Data Processing Management | year=1963 | last=Robert F. Simmons |volume=5 |issue=12 |pages=11–18}}</ref> Sheldon Klein, Karen McConologue, M. Ross Quillian<ref name=\'Quillian1963\'>Quillian, R. A notation for representing conceptual information: An application to semantics and mechanical English para- phrasing. SP-1395, System Development Corporation, Santa Monica, 1963.</ref> and others at [[System Development Corporation]] in the early 1960s as part of the SYNTHEX project. It later featured prominently in the work of [[Allan M. Collins]] and Quillian (e.g., Collins and Quillian;<ref name=\'Collins1969\'>{{cite journal | title=Retrieval time from semantic memory | journal=Journal of verbal learning and verbal behavior | year=1969 | last1=Allan M. Collins |author2= M. R. Quillian |volume=8 |issue=2 |pages=240–247 |doi=10.1016/S0022-5371(69)80069-1  }}</ref><ref name=\'Collins1970\'>{{cite journal |title=Does category size affect categorization time? |journal=Journal of verbal learning and verbal behavior |year=1970 |first= |last=Allan M. Collins\n|author2=M. Ross Quillian  |volume=9 |issue=4 |pages=432–438 |doi=10.1016/S0022-5371(70)80084-6 }}</ref> Collins and Loftus<ref name=\'Collins1975\'>{{cite journal |title=A spreading-activation theory of semantic processing |journal=Psychological Review |year=1975 |last=Allan M. Collins |author2=Elizabeth F. Loftus |volume=82 | doi = 10.1037/0033-295x.82.6.407 |pages=407–428}}</ref> Quillian<ref>Quillian, M. R. (1967). Word concepts: A theory and simulation of some basic semantic capabilities. Behavioral Science, 12(5), 410-430.</ref><ref>Quillian, M. R. (1968). Semantic memory. Semantic information processing, 227–270.</ref><ref>{{cite journal | last1 = Quillian | first1 = M. R. | year = 1969 | title = The teachable language comprehender: a simulation program and theory of language | url = | journal = Communications of the ACM | volume = 12 | issue = 8| pages = 459–476 | doi=10.1145/363196.363214}}</ref><ref>Quillian, R. Semantic Memory. Unpublished doctoral dissertation, Carnegie Institute of Technology, 1966.</ref>)\n\nIn the late 1980s, two [[Netherlands]] universities, [[University of Groningen|Groningen]] and [[University of Twente|Twente]], jointly began a project called \'\'Knowledge Graphs\'\', which are semantic networks but with the added constraint that edges are restricted to be from a limited set of possible relations, to facilitate algebras on the graph.<ref>{{cite book |last=Van de Riet |first=R. P. |date=1992 |title=Linguistic Instruments in Knowledge Engineering |url=http://www.stokman.org/artikel/92Jame.KnowGraphs.LIKE.pdf |publisher=Elsevier Science Publishers |page=98 |isbn=0444883940}}</ref> In the subsequent decades, the distinction between semantic networks and knowledge graphs was blurred.<ref>{{cite conference |url=https://books.google.com/books?id=15PDCgAAQBAJ&pg=PA444 |title=Path-Based Semantic Relatedness on Linked Data and Its Use to Word and Entity Disambiguation |last1=Hulpus |first1=Ioana |last2=Prangnawarat |first2=Narumol |date=2015 |publisher=Springer International Publishing |book-title=The Semantic Web - ISWC 2015: 14th International Semantic Web Conference, Bethlehem, PA, USA, October 11-15, 2015, Proceedings, Part 1 |pages=444 |conference=[[International Semantic Web Conference]] 2015}}</ref><ref>{{cite web |url=https://www.authorea.com/users/6341/articles/107281 |title=What is a Knowledge Graph? |last1=McCusker |first1=James P. |last2=Chastain |first2=Katherine |date=April 2016 |website=authorea.com |access-date=15 June 2016 |quote="usage [of the term \'knowledge graph\'] has evolved"}}</ref> In 2012, [[Google]] gave their knowledge graph the name [[Knowledge Graph]].\n\n== Basics of semantic networks ==\nA semantic network is used when one has knowledge that is best understood as a set of concepts that are related to one another.\n\nMost semantic networks are cognitively based. They also consist of arcs and nodes which can be organized into a taxonomic hierarchy. Semantic networks contributed ideas of [[spreading activation]], [[inheritance]], and nodes as proto-objects.\n\n== Examples ==\n\n=== Semantic Net in [[Lisp (programming language)|Lisp]] ===\nUsing an association list.\n<source lang="lisp">\n(defun *database* ()\n\'((canary  (is-a bird)\n           (color yellow)\n           (size small))\n  (penguin (is-a bird)\n           (movement swim))\n  (bird    (is-a vertebrate)\n           (has-part wings)\n           (reproduction egg-laying))))\n</source>\n\nYou would use the "assoc" function with a key of "canary" to extract all the information about the "canary" type.<ref>{{cite web|last=Swigger|first=Kathleen|title=Semantic.ppt|url=http://zeus.csci.unt.edu/swigger/csci3210/semantic.ppt|accessdate=23 March 2011}}</ref>\n\n=== WordNet ===\n{{Main|WordNet}}\nAn example of a semantic network is [[WordNet]], a [[lexicon|lexical]] database of [[English language|English]]. It groups English words into sets of synonyms called [[synsets]], provides short, general definitions, and records the various semantic relations between these synonym sets. Some of the most common semantic relations defined are [[meronymy]] (A is part of B, i.e. B has A as a part of itself), [[holonymy]] (B is part of A, i.e. A has B as a part of itself), [[hyponym]]y (or [[troponymy]])  (A is subordinate of B; A is kind of B), [[hypernym]]y (A is superordinate of B), [[synonym]]y (A denotes the same as B) and [[antonym]]y (A denotes the opposite of B).\n\nWordNet properties have been studied from a [[Graph theory|network theory]] perspective and compared to other semantic networks created from [[Roget\'s Thesaurus]] and [[word association]] tasks.  From this perspective the three of them are a [[Small-world network|small world structure]].<ref name=Steyvers2005>{{cite journal\n | author = Steyvers, M.\n |author2=Tenenbaum, J.B.\n  | year = 2005\n | title = The Large-Scale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth\n | journal = Cognitive Science\n | volume = 29\n | issue = 1\n | pages = 41–78\n | doi = 10.1207/s15516709cog2901_3\n}}</ref>\n\n=== Other examples ===\nIt is also possible to represent logical descriptions using semantic networks such as the [[existential graph]]s of [[Charles Sanders Peirce]] or the related [[conceptual graph]]s of [[John F. Sowa]].<ref name=\'Sowa\'>{{cite encyclopedia\n|author=John F. Sowa\n|editor=Stuart C Shapiro\n|encyclopedia=Encyclopedia of Artificial Intelligence\n|title=Semantic Networks\n|url=http://www.jfsowa.com/pubs/semnet.htm\n|accessdate=2008-04-29\n|year=1987\n|authorlink=John F. Sowa}}</ref> These have expressive power equal to or exceeding standard [[first-order predicate calculus|first-order predicate logic]].  Unlike WordNet or other lexical or browsing networks, semantic networks using these representations can be used for reliable automated logical deduction.  Some automated reasoners exploit the graph-theoretic features of the networks during processing.\n\nOther examples of semantic networks are [[Gellish]] models. [[Gellish English]] with its [[Gellish English dictionary]], is a [[formal language]] that is defined as a network of relations between concepts and names of concepts. Gellish English is a formal subset of natural English, just as Gellish Dutch is a formal subset of Dutch, whereas multiple languages share the same concepts. Other Gellish networks consist of knowledge models and information models that are expressed in the Gellish language. A Gellish network is a network of (binary) relations between things. Each relation in the network is an expression of a fact that is classified by a relation type. Each relation type itself is a concept that is defined in the Gellish language dictionary. Each related thing is either a concept or an individual thing that is classified by a concept. The definitions of concepts are created in the form of definition models (definition networks) that together form a Gellish Dictionary. A Gellish network can be documented in a Gellish database and is computer interpretable.\n\n[[SciCrunch]] is a collaboratively edited knowledge base for scientific resources. It provides unambiguous identifiers (Research Resource IDentifiers or RRIDs) for software, lab tools etc. and it also provides options to create links between RRIDs and from communities.\n\nAnother example of semantic networks, based on [[category theory]], is [[olog]]s. Here each type is an object, representing a set of things, and each arrow is a morphism, representing a function. [[Commutative diagrams]] also are prescribed to constrain the semantics.\n\nIn the social sciences people sometimes use the term semantic network to refer to [[co-occurrence networks]].<ref name=\'Atteveldt\'>{{cite book\n|author=Wouter Van Atteveldt\n|title=Semantic Network Analysis: Techniques for Extracting, Representing, and Querying Media Content\n|publisher=BookSurge Publishing\n|year=2008}}</ref> The basic idea is that words that co-occur in a unit of text, e.g. a sentence, are semantically related to one another. Ties based on co-occurrence can then be used to construct semantic networks.\n\n== Software tools ==\nThere are also elaborate types of semantic networks connected with corresponding sets of software tools used for [[Lexicon|lexical]] [[knowledge engineering]], like the Semantic Network Processing System ([[SNePS]]) of Stuart C. Shapiro<ref>[http://www.cse.buffalo.edu/~shapiro/ Stuart C. Shapiro]</ref> or the [[MultiNet]] paradigm of Hermann Helbig,<ref>[http://pi7.fernuni-hagen.de/helbig/index_en.html Hermann Helbig]</ref> especially suited for the semantic representation of natural language expressions and used in several [[Natural language processing|NLP]] applications.\n\nSemantic networks are used in specialized information retrieval tasks, such as [[plagiarism]] detection. They provide information on hierarchical relations in order to employ [[semantic compression]] to reduce language diversity and enable the system to match word meanings, independently from sets of words used.\n\n== See also ==\n{{Div col}}\n* [[Abstract semantic graph]]\n* [[Chunking (psychology)]]\n* [[Network diagram]]\n* [[Ontology (information science)]]\n* [[Repertory grid]]\n* [[Semantic lexicon]]\n* [[Semantic neural network]]\n* [[SemEval]] - an ongoing series of evaluations of [[Semantic analysis (computational)|computational semantic analysis]] systems\n* [[Sparse distributed memory]]\n* [[Taxonomy (general)]]\n* [[Unified Medical Language System]] (UMLS)\n* [[Word-sense disambiguation]] (WSD)\n{{Div col end}}\n\n=== Other examples ===\n* [[Cognition Network Technology]]\n* [[Lexipedia]]\n* [[Open Mind Common Sense]] (OMCS)\n* [[Schema.org]]\n* [[SNOMED CT]]\n* [[Universal Networking Language]] (UNL)\n* [[Wikidata]]\n\n== References ==\n{{reflist|30em}}\n\n== Further reading ==\n* Allen, J. and A. Frisch (1982). "What\'s in a Semantic Network". In: \'\'Proceedings of the 20th. annual meeting of ACL\'\', Toronto, pp.&nbsp;19–27.\n* John F. Sowa, Alexander Borgida (1991). \'\'Principles of Semantic Networks: Explorations in the Representation of Knowledge\'\'.\n\n== External links ==\n{{Commons category|Semantic networks}}\n* [http://www.jfsowa.com/pubs/semnet.htm "Semantic Networks"] by John F. Sowa\n* [http://www.knowledgegrid.net/~H.Zhuge/SLN.htm "Semantic Link Network" ] by Hai Zhuge\n\n{{Semantic Web}}\n{{Use dmy dates|date=August 2011}}\n\n{{Authority control}}\n\n[[Category:Knowledge representation]]\n[[Category:Networks]]']
['Faceted metadata', '47571988', '#REDIRECT [[Faceted classification]]\n\n{{R with possibilities}}\n\n[[Category:Metadata|Faceted]]\n[[Category:Knowledge representation]]']
['Mind map', '19688', '{{about|the visual diagram|the geographical concept|Mental mapping}}\n[[File:Tennis-mindmap.png|thumb|upright=1.8|A mind map about the sport of [[tennis]]]]\n\nA \'\'\'mind map\'\'\' is a  [[diagram]] used to visually organize information. A mind map is hierarchical and shows relationships among pieces of the whole.<ref>Carolyn H. Hopper, Practicing College Learning Strategies, 7th Edition, ISBN 9781305109599, Ch. 7</ref> It is often created around a single concept, drawn as an image in the center of a blank page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas branch out from those.\n\nMind maps can be drawn by hand, either as "rough notes" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available. Mind maps are considered to be a type of [[spider diagram]].<ref>{{cite web|url=http://dictionary.cambridge.org/dictionary/british/mind-map?q=mind+map |title=Mind Map noun - definition in the British English Dictionary & Thesaurus - Cambridge Dictionaries Online |publisher=Dictionary.cambridge.org |accessdate=2013-07-10}}</ref> A similar concept in the 1970s was "idea [[sunburst chart|sun bursting]]".<ref>{{cite web|url=http://www.mind-mapping.org/mindmapping-learning-study-memory/who-invented-mind-mapping.html |title=Who invented mind mapping |publisher=Mind-mapping.org |accessdate=2013-07-10}}</ref>\n\n== Origins ==\n\nAlthough the term "mind map" was first popularized by British [[popular psychology]] author and television personality [[Tony Buzan]], the use of diagrams that visually "map" information using branching and [[Radial tree|radial maps]] traces back centuries. These pictorial methods record knowledge and model systems, and have a long history in learning, [[brainstorming]], [[memory]], [[visual thinking]], and [[problem solving]] by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by [[Porphyry of Tyros]], a noted thinker of the 3rd century, as he graphically visualized the concept [[Categories (Aristotle)|categories of Aristotle]]. Philosopher [[Ramon Llull]] (1235–1315) also used such techniques.\n\nThe [[semantic network]] was developed in the late 1950s as a theory to understand human learning and developed further by [[Allan M. Collins]] and [[M. Ross Quillian]] during the early 1960s. Mind maps are similar in radial structure to [[concept map]]s, developed by learning experts in the 1970s, but differ in that the former are simplified by focusing around a single central key concept.\n\n== Popularisation of the term "mind map" ==\n\nBuzan\'s specific approach, and the introduction of the term "mind map" arose during a 1974 BBC TV series he hosted, called \'\'Use Your Head\'\'.<ref>{{cite web|url=http://www.mind-mapping.org/blog/mapping-history/roots-of-visual-mapping/ |title=Roots of visual mapping - The mind-mapping.org Blog |publisher=Mind-mapping.org |date=2004-05-23 |accessdate=2013-07-10}}</ref><ref>Buzan, Tony 1974. Use your head. London: BBC Books.</ref> In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure.<ref>[http://www.knowledgeboard.com/item/2980 Buzan claims mind mapping his invention in interview.] \'\'KnowledgeBoard\'\' retrieved Jan. 2010.</ref>\n\nBuzan says the idea was inspired by [[Alfred Korzybski]]\'s [[general semantics]] as popularized in science fiction novels, such as those of [[Robert A. Heinlein]] and [[A. E. van Vogt]]. He argues that while "traditional" outlines force readers to scan left to right and top to bottom, readers actually tend to scan the entire page in a non-linear fashion. Buzan\'s treatment also uses then-popular assumptions about the functions of [[cerebral hemispheres]] in order to explain the claimed increased effectiveness of mind mapping over other forms of note making.\n\n==Mind map guidelines==\nBuzan suggests the following guidelines for creating mind maps:\n\n# Start in the center with an image of the topic, using at least 3 colors.\n# Use images, symbols, codes, and dimensions throughout your mind map.\n# Select key words and print using upper or lower case letters.\n# Each word/image is best alone and sitting on its own line.\n# The lines should be connected, starting from the central image. The lines become thinner as they radiate out from the center.\n# Make the lines the same length as the word/image they support.\n# Use multiple colors throughout the mind map, for visual stimulation and also for encoding or grouping.\n# Develop your own personal style of mind mapping.\n# Use emphasis and show associations in your mind map.\n# Keep the mind map clear by using radial hierarchy or outlines to embrace your branches.\n\n== Uses ==\n\n[[File:Mindmap.gif|thumb|Rough mindmap notes taken during a course session]]\nAs with other diagramming tools, mind maps can be used to [[generation|generate]], [[creative visualization|visualize]], [[structure]], and [[taxonomic classification|classify]] ideas, and as an aid to [[study skills|studying]]<ref>\'Mind maps as active learning tools\', by Willis, CL. Journal of computing sciences in colleges. {{ISSN|1937-4771}}. 2006. Volume:\n21 Issue: 4</ref> and [[organization|organizing]] information, [[problem solving|solving problems]], [[decision making|making decisions]], and writing.\n\nMind maps have many applications in personal, family, [[education]]al, and [[business]] situations, including [[notetaking]], brainstorming (wherein ideas are inserted into the map radially around the center node, without the implicit prioritization that comes from hierarchy or sequential arrangements, and wherein grouping and organizing is reserved for later stages), summarizing, as a [[mnemonic technique]], or to sort out a complicated idea. Mind maps are also promoted as a way to collaborate in color pen creativity sessions.\n\nIn addition to these direct use cases, data retrieved from mind maps can be used to enhance several other applications; for instance [[expert system|expert search systems]], [[search engine]]s and search and tag query recommender.<ref name=Beel2009>{{Cite journal| first=Jöran | last=Beel | first2=Bela| last2=Gipp | first3=Jan-Olaf |last3= Stiller | contribution=Information Retrieval On Mind Maps - What Could It Be Good For? | contribution-url=http://www.sciplore.org/publications_en.php | title=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom\'09) | year=2009 | publisher=IEEE | place=Washington | postscript=. -->}}</ref> To do so, mind maps can be analysed with classic methods of [[information retrieval]] to classify a mind map\'s author or documents that are linked from within the mind map.<ref name=Beel2009 />\n\n==Differences from other visualizations==\n\n* \'\'\'Concept maps\'\'\' - Mind maps differ from [[concept maps]] in that mind maps focus on \'\'only\'\' one word or idea, whereas concept maps connect multiple words or ideas. Also, concept maps typically have text labels on their connecting lines/arms. Mind maps are based on radial hierarchies and [[tree structure]]s denoting relationships with a central governing concept, whereas concept maps are based on connections between concepts in more diverse patterns.  However, either can be part of a larger [[personal knowledge base]] system.\n* \'\'\'Modelling graphs\'\'\' - There is no rigorous right or wrong with mind maps, relying on the arbitrariness of [[mnemonic]] systems. A [[UML diagram]] or a [[semantic network]] has structured elements modelling relationships, with lines connecting objects to indicate relationship. This is generally done in black and white with a clear and agreed iconography. Mind maps serve a different purpose: they help with memory and organization. Mind maps are collections of words structured by the mental context of the author with visual mnemonics, and, through the use of colour, icons and visual links, are informal and necessary to the proper functioning of the mind map.\n\n==Research==\n\n\'\'\'Effectiveness\'\'\' - Cunningham (2005) conducted a user study in which 80% of the students thought "mindmapping helped them understand concepts and ideas in science".<ref name="Cunningham05">{{cite thesis| type=Ph.D.| author={G}lennis {E}dge {C}unningham| title=Mindmapping: Its Effects on Student Achievement in High School Biology| year=2005| publisher=The University of Texas at Austin| accessdate=1 November 2013}}</ref> Other studies also report positive effects through the use of mind maps.<ref name="Holland2004">{{cite journal| author={B}rian {H}olland, {L}ynda {H}olland, {J}enny {D}avies| title=An investigation into the concept of mind mapping and the use of mind mapping software to support and improve student academic performance| year=2004| accessdate=1 November 2013}}</ref><ref name="Antoni2006">{{cite journal| author=D\'Antoni, A.V., Zipp, G.P.| title=Applications of the Mind Map Learning Technique in Chiropractic Education: A Pilot Study and Literature| year=2006| accessdate=1 November 2013}}</ref> Farrand, Hussain, and Hennessy (2002) found that [[spider diagram]]s (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline).<ref name= Farrand2002>{{cite journal |author=Farrand, P. |author2=Hussain, F. |author3=Hennessy, E.  |year=2002 |title=The efficacy of the mind map study technique |journal=Medical Education |volume=36 |issue=5 |pages=426–431 |url=http://www3.interscience.wiley.com/journal/118952400/abstract |accessdate=2009-02-16 |doi=10.1046/j.1365-2923.2002.01205.x |pmid=12028392}}</ref> This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects\' preferred methods of note taking. A meta study about [[concept map]]ping concluded that concept mapping is more effective than "reading text passages, attending lectures, and participating in class discussions".<ref name="Nesbit06">{{cite journal| author={N}esbit, {J}.{C}., {A}desope, {O}.{O}.| title=Learning with concept and knowledge maps: A meta-analysis| journal=Review of Educational Research| year=2006| volume=76| number=3| pages=413| publisher=Sage Publications| doi=10.3102/00346543076003413}}</ref> The same study also concluded that concept mapping is slightly more effective "than other constructive activities such as writing summaries and outlines". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students.\n\n\'\'\'Features of Mind Maps\'\'\' - Beel & Langer (2011) conducted a comprehensive analysis of the content of mind maps.<ref name="Beel2011d">{{cite book| author={J}oeran {B}eel, {S}tefan {L}anger| chapter=An Exploratory Analysis of Mind Maps| title=Proceedings of the 11th ACM Symposium on Document Engineering (DocEng\'11)| year=2011| publisher=ACM| url=http://docear.org/papers/An%20Exploratory%20Analysis%20of%20Mind%20Maps%20--%20preprint.pdf | accessdate=1 November 2013}}</ref> They analysed 19,379 mind maps from 11,179 users of the mind mapping applications [[SciPlore MindMapping]] (now [[Docear]]) and [[MindMeister]]. Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about 3 words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7500 words. The study also showed that between different mind mapping applications ([[Docear]] vs [[MindMeister]]) significant differences exist related to how users create mind maps.\n\n\'\'\'Automatic Creating of Mind Maps\'\'\' - There have been some attempts to create mind maps automatically. Brucks & Schommer created mind maps automatically from full-text streams.<ref name="Brucks2008">{{cite journal| author={C}laudine {B}rucks, {C}hristoph {S}chommer| title=Assembling Actor-based Mind-Maps from Text Stream| journal=CoRR| year=2008| volume=abs/0810.4616| accessdate=1 November 2013}}</ref> Rothenberger et al. extracted the main story of a text and presented it as mind map.<ref name="Rothenberger2008">{{cite journal| author=Rothenberger, T, Oez, S, Tahirovic, E, Schommer, Christoph| title=Figuring out Actors in Text Streams: Using Collocations to establish Incremental Mind-maps| journal= | arxiv=0803.2856| year=2008}}</ref> And there is a patent about automatically creating sub-topics in mind maps.<ref name="Plotkin09">{{cite journal|year=2009|title=Software tool for creating outlines and mind maps that generates subtopics automatically|journal=USPTO Application: 20090119584|volume=|author={R}obert {P}lotkin|accessdate=1 November 2013}}</ref>\n\n\'\'\'Pen and Paper vs Computer\'\'\' - There are two studies that analyze whether electronic mind mapping or pen based mind mapping is more effective.<ref name="Mahler09">{{cite book| author={M}ahler, {T}., {W}eber, {M}.| chapter=Dimian-Direct Manipulation and Interaction in Pen Based Mind Mapping| title=Proceedings of the 17th World Congress on Ergonomics, IEA 2009| year=2009| accessdate=1 November 2013}}</ref><ref name="Shih09">{{cite journal| author={S}hih, {P}.{C}., {N}guyen, {D}.{H}., {H}irano, {S}.{H}. and {R}edmiles, {D}.{F}., {H}ayes, {G}.{R}.| title=Groupmind: supporting idea generation through a collaborative mind-mapping tool| year=2009| pages=139–148| accessdate=1 November 2013}}</ref>\n\n==Tools==\n[[List of concept- and mind-mapping software|Mind-mapping software]] can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites and images.<ref>{{cite web|url=http://www.imdevin.com/top-10-totally-free-mind-mapping-software-tools/|title=Top 10 Totally Free Mind Mapping Software Tools|last=Santos|first=Devin|date=15 February 2013|publisher=IMDevin|accessdate=10 July 2013}}</ref> It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional [[note-taking]].<ref>\n{{cite journal\n  | last = Farrand\n  | first = Paul |author2=Hussain, Fearzana |author3=Hennessy, Enid\n  | title = The efficacy of the \'mind map\' study technique\n  | journal = Medical Education\n  | volume = 36\n  | issue = 5\n  | pages = 426–431\n  | date = May 2002\n  | doi = 10.1046/j.1365-2923.2002.01205.x\n  | pmid = 12028392\n}}</ref>\n\n==See also==\n{{Portal|Education}}\n* [[Brainstorming]]\n* [[Graph (discrete mathematics)]]\n* [[Idea]]\n* [[List of concept mapping and mind mapping software]]\n* [[Mental literacy]]\n* [[Personal wiki]]\n\n; Related diagrams\n* [[Argument map]]\n* [[Cognitive map]]\n* [[Concept map]]\n* [[Nodal organizational structure]]\n* [[Radial tree]]\n* [[Rhizome (philosophy)]]\n* [[Semantic network]]\n* [[Social map]]\n* [[Spider mapping]]\n* [[Tree structure]]\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n* {{cite journal |last= Novak |first= J.D. |date= 1993 |title= How do we learn our lesson?: Taking students through the process |journal= [[The Science Teacher]] |volume= 60 |number= 3 |pages= 50–55 |issn= 0036-8555 }}\n\n==External links==\n*{{Commons category-inline|Mind maps}}\n\n{{Mindmaps}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Mind Map}}\n[[Category:Articles with inconsistent citation formats]]\n[[Category:Knowledge representation]]\n[[Category:Games of mental skill]]\n[[Category:Creativity]]\n[[Category:Design]]\n[[Category:Educational technology]]\n[[Category:Diagrams]]\n[[Category:Thought]]\n[[Category:Note-taking]]\n[[Category:Reading (process)]]\n[[Category:Zoomable user interfaces]]\n[[Category:Educational devices]]']
['Group concept mapping', '1539290', '\'\'\'Group concept mapping\'\'\' is a structured methodology for organizing the ideas of a group on any topic of interest and representing those ideas visually in a series of interrelated maps.<ref name="ref1">Kane M, Trochim WM (2007). Concept mapping for planning and evaluation. Thousand Oaks, CA: Sage Publications.</ref><ref name="ref2">Trochim W (1989). An introduction to concept mapping for evaluation and planning. Evaluation and Program Planning, 12(1), 1-16.</ref>  It is a type of integrative mixed method,<ref>Caracelli VW, Greene JC (1993). Data analysis strategies for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 15(2), 195-207.</ref><ref>Greene JC, Caracelli VJ, Graham WF (1989). Toward a conceptual framework for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 11, 255-274.</ref> combining qualitative and quantitative approaches to [[data collection]] and [[analysis]].  Group concept mapping allows for a collaborative group process with groups of any size, including a broad and diverse array of participants.<ref name="ref1" />  Since its development in the late 1980s by William M.K. Trochim at [[Cornell University]], it has been applied to various fields and contexts, including community and public health,<ref>Rao JK, Alongi J, Anderson LA, Jenkins L, Stokes GA, Kane M (2005). Development of public health priorities for end-of-life initiatives. American Journal of Preventive Medicine, 29(5), 453-460.</ref><ref>Risisky D, Hogan VK, Kane M, Burt B, Dove C, Payton M (2008). Concept mapping as a tool to engage a community in health disparity identification. Ethnicity & Disease, 18, 77-83.</ref><ref>Trochim W, Milstein B, Wood B, Jackson S, Pressler V (2004). Setting objectives for community and systems change: an application of concept mapping for planning a statewide health improvement initiative. Health Promotion Practice, 5, 8–19.</ref><ref>Trochim WM, Cabrera DA, Milstein B, Gallagher RS, Leischow SJ (2006). Practical challenges of systems thinking and modeling in public health. American Journal of Public Health, 96(3), 538-546.</ref> social work,<ref>Petrucci C, Quinlan KM (2007). Bridging the research-practice gap: concept mapping as a mixed-methods strategy in practice-based research and evaluation. Journal of Social Science Research, 34(2), 25-42.</ref><ref>Ridings JW, Powell DM, Johnson JE, Pullie CJ, Jones CM, Jones RL, Terrell KJ (2008). Using concept mapping to promote community building: the African American Initiative at Roseland. Journal of Community Practice, 16(1), 39-63.</ref> health care,<ref>Trochim WM, Kane M (2005). Concept mapping: an introduction to structured conceptualization in health care. International Journal for Quality in Health Care, 7(3),187-191.</ref> human services,<ref>Pammer W, Haney M, Wood BM, Brooks RG, Morse K, Hicks P, Handler EG, Rogers H, Jennett P (2001). Use of telehealth technology to extend child protection team services. Pediatrics, 108(3), 584-590.</ref><ref>Paulson BL, Truscott D, Stuart J (1999). Clients’ perceptions of helpful experiences in counseling. Journal of Counseling Psychology, 46(3), 317-324.</ref> and biomedical research and evaluation.<ref>Kagan JM, Kane M, Quinlan KM, Rosas S, Trochim WMK (2009). Developing a conceptual framework for an evaluation system for the NIAID HIV/AIDS clinical trials networks. Health Research Policy and Systems, 7, 12.</ref><ref>Robinson JM, Trochim WMK (2007). An examination of community members’, researchers’ and health professionals’ perceptions of barriers to minority participation in medical research: an application of concept mapping. Ethnicity & Health, 12(5), 521-539.</ref><ref>Trochim WM, Markus SE, Masse LC, Moser RP, and Weld PC (2008). The evaluation of large research initiatives: a participatory integrative mixed-methods approach. American Journal of Evaluation, 29(1), 8–28.</ref>\n\n==Overview==\nGroup concept mapping integrates qualitative group processes with [[multivariate analysis]] to help a group organize and visually represent its ideas on any topic of interest through a series of related maps.<ref name="ref1" /><ref name="ref2" />  It combines the ideas of diverse participants to show what the group thinks and values in relation to the specific topic of interest. It is a type of structured conceptualization used by groups to develop a conceptual framework, often to help guide evaluation and planning efforts.<ref name="ref2" />  Group concept mapping is participatory in nature, allowing participants to have an equal voice and to contribute through various methods.<ref name="ref1" /> A group concept map visually represents all the ideas of a group and how they relate to each other, and depending on the scale, which ideas are more relevant, important, or feasible.\n\n==Process==\n\nGroup concept mapping involves a structured multi-step process, including [[brainstorming]], sorting and rating, [[multidimensional scaling]] and [[cluster analysis]], and the generation and interpretation of multiple maps.<ref name="ref1" /><ref name="ref2" />  The first step requires participants to brainstorm a large set of statements relevant to the topic of interest, usually in response to a focus prompt.  Participants are then asked to individually sort those statements into categories based on their perceived similarity and rate each statement on one or more scales, such as importance or feasibility.\n\nThe data is then analyzed using The Concept System® software, which creates a series of interrelated maps using [[multidimensional scaling]] (MDS) of the sort data, [[hierarchical clustering]] of the MDS coordinates applying [[Ward\'s method|Ward’s method]], and the computation of average ratings for each statement and cluster of statements.<ref name="ref3">Rosas SR, Camphausen (2007).  The use of concept mapping for scale development and validation in evaluation.  Evaluation and Program Planning, 30, 125-135.</ref>  The resulting maps display the individual statements in two-dimensional space with more similar statements located closer to each other, and grouped into clusters that partition the space on the map.  The Concept System® software also creates other maps that show the statements in each cluster rated on one or more scales, and absolute or relative cluster ratings between two cluster sets.  As a last step in the process, participants are led through a structured interpretation session to better understand and label all the maps.\n\n==History==\nGroup concept mapping was developed as a methodology in the late 1980s by William M.K. Trochim at [[Cornell University]].  Trochim is considered to be a leading evaluation expert, and he has taught evaluation and research methods at Cornell since 1980.<ref name="ref4">Cornell University, College of Human Ecology (2012).  William Trochim biographical statement. http://www.human.cornell.edu/bio.cfm?netid=wmt1.</ref>  Originally called "concept mapping", the methodology has evolved since its inception with the maturation of the field and the continued advancement of the software, which is now a Web application.\n\n==Uses==\nGroup concept mapping can be used with any group for any topic of interest.  It is often used by government agencies, academic institutions, national associations, not-for-profit and community-based organizations, and private businesses to help turn the ideas of the group into measurable actions.  This includes in the areas of organizational development, strategic planning, needs assessment, curriculum development, research, and evaluation.<ref name="ref1" />  Group concept mapping is well-documented, well-established methodology, and it has been used in hundreds of published papers.\n\n==Group concept mapping versus concept mapping and mind mapping==\nConcept mapping is any process used for visually representing relationships between ideas in pictures or maps.<ref name="ref1" />  The technique was originally developed in the 1970s by [[Joseph D. Novak]] at [[Cornell University]]. <ref name="ref5">Novak JD, Gowin DB (1984). Learning How to Learn. Cambridge: Cambridge University Press.</ref>  A [[concept map]] is typically a diagram of multiple ideas, often represented as boxes or circles, linked in a hierarchical structure through arrows and words where each idea is connected to each other and linked back to the original idea.<ref name="ref5" />  Concept mapping tends to be more free form, and may involve an individual or group.  Unlike other forms of concept mapping, group concept mapping is purposefully designed to work with groups and has a more structured process for organizing and visually representing the ideas of a group through a series of specific steps.<ref name="ref1" />\n\nA [[mind map]] is a diagram used to visually represent information, centering on one word or idea with categories and sub-categories radiating off of it.<ref>Buzan T (2010).  The Mind Map Book: Unlock Your Creativity, Boost Your Memory, Change Your Life. Essex: British Broadcasting Company.</ref>  Popularized by [[Tony Buzan]] in the 1970s, mind mapping is often a spontaneous exercise done by an individual or group to gather information about what they think around a single topic.  In contrast to mind mapping, group concept mapping represents multiple ideas and it has a less flexible and more structured process.  Group concept mapping is also specific to groups.\n\n==See also==\n* [[Knowledge representation and reasoning]]\n* [[List of concept- and mind-mapping software]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.socialresearchmethods.net/mapping/mapping.htm Concept mapping research guide]\n\n[[Category:Diagrams]]\n[[Category:Knowledge representation]]\n[[Category:Survey methodology]]']
['Consistency (knowledge bases)', '25154802', "A [[knowledge base]] KB is '''consistent''' ''[[iff]]'' its negation is not a [[Tautology (logic)|tautology]].\n\nI.e., a knowledge base KB is inconsistent (not consistent) [[iff]] there is no [[Interpretation (logic)|interpretation]] which [[entailment|entails]] KB.\n\nExample of an inconsistent knowledge base:\n\nKB := { a, ¬a }\n\nConsistency in terms of knowledge bases is mostly the same as the natural understanding of [[consistency]].\n\n[[Category:Knowledge representation]]\n{{logic-stub}}\n{{database-stub}}"]
["Hallin's spheres", '31244175', '\'\'\'Hallin\'s spheres\'\'\' is a theory of media objectivity posited by journalism historian [[Daniel C. Hallin]] in his book \'\'The Uncensored War\'\' to explain the coverage of the Vietnam war.<ref name="DH">{{cite book|last=Hallin|first=Daniel|title=The Uncensored War: The Media and Vietnam.|year=1986|publisher=Oxford University press|location=New York|pages=116–118|isbn=978-0-19-503814-9}}</ref> Hallin divides the world of political discourse into three concentric spheres: consensus, legitimate controversy, and deviance. In the sphere of consensus, journalists assume everyone agrees. The sphere of legitimate controversy includes the standard political debates, and journalists are expected to remain neutral. The sphere of deviance falls outside the bounds of legitimate debate, and journalists can ignore it. These boundaries shift, as public opinion shifts.<ref>[http://www.cjr.org/analysis/trump_inspires_murrow_moment_for_journalism.php For journalists covering Trump, a Murrow moment], By David Mindich, Columbia Journalism Review, July 15, 2016</ref>\n\nHallin\'s spheres, which deals with the media, are similar to the [[Overton window]], which deals with public opinion generally, and posits a sliding scale of public opinion on any given issue ranging from conventional wisdom to unacceptable. \n\nHallin used the concept of [[Framing (social sciences)|framing]] to describe the presentation and reception of issues in public. For example, framing the use of drugs as criminal activity can encourage the public to consider that behavior anti-social. Hallin also used the concept of an [[opinion corridor]], in which the range of public opinion narrows, and opinion outside that corridor moves from legitimate controversy into deviance. \n\n== Description ==\n\n=== Sphere of consensus ===\nThis sphere contains those topics on which there is widespread agreement, or at least the perception thereof. Within the sphere of consensus, \'journalists feel free to invoke a generalized "we" and to take for granted shared values and shared assumptions\'<ref>Schudson 2002, p. 40</ref> Example include such things as free speech, the abolition of slavery, or human rights.  For topic in this sphere "journalists do not feel compelled to present an opposing view point or to remain disinterested observers."<ref name="DH"/>\n\n=== Sphere of legitimate controversy ===\nFor topics in this sphere rational and informed people hold differing views. These topics are therefore the most important to cover, and also ones upon which journalists are obliged to remain disinterested reporters, rather than advocating for or against a particular view.<ref>Hallin, 1986, p. 116;</ref> Schudson notes that Hallin, in his influential study of the US media during the Vietnam War, argues that journalism\'s commitment to objectivity has always been compartmentalized. That is, within a certain sphere—the sphere of legitimate controversy—journalists seek conscientiously to be balanced and objective.<ref>Schudson, M (2002) \'What\'s unusual about covering politics as usual\', in Zelizer, B., & Allan, S. (Eds.). Journalism after September 11. London: Routledge, p. 40</ref>\n\n=== Sphere of deviance ===\nTopics in this sphere are rejected by journalists as being unworthy of general consideration.  Such views are perceived as being either unfounded, taboo, or of such minor consequence that they are not news worthy.  Hallin argues that in the sphere of deviance, \'journalists also depart from standard norms of objective reporting and feel authorized to treat as marginal, laughable, dangerous, or ridiculous individuals and groups who fall far outside a range of variation taken as legitimate.\'<ref>Schudson 2002, 40</ref> For example, a person claiming that aliens are manipulating college basketball scores might have difficulty finding media coverage for such a claim.<ref>Hallin, 1986, p. 117</ref>\n\n== Uses of the terms ==\nCraig Watkins (2001, pp.&nbsp;92–4) makes use of the Hallin\'s spheres in a paper examining ABC, CBS, and NBC television network television news coverage of the "Million Man March", a demonstration that took place in Washington, DC on October 16, 1995. Watkins analyzes the dominant framing practices-problem definition, rhetorical devices, use of sources, and images-employed by journalists to make sense of this particular expression of political protest. He argues that Hallins three spheres are a way for media framing practices to develop specific reportorial contexts, each sphere develops its own distinct style of news reporting resources by different rhetorical tropes and discourses.<ref>Watkins, S. C. (2001). Framing protest: News media frames of the Million Man March. Critical Studies in Media Communication, 18(1), 83-101.</ref>\n\nPiers Robinson (2001, p.&nbsp;536) uses the concept in relation to debate that have emerged over the extent to which the mass media serves elite interests or, alternatively, plays a powerful role in shaping political outcomes. His articles reviews Hallin\'s spheres as an example of media-state relations, that highlights theoretical and empirical shortcomings in the \'manufacturing consent\' thesis (Chomsky McChesney).<ref>Herman, E. S., & Chomsky, N. (2010). Manufacturing consent: The political economy of the mass media. Random House.</ref> Robinson argues that a more nuanced and bi-directional understanding is needed of the direction of influence between media and the state that builds upon, rather than rejecting, existing theoretical accounts.<ref>Robinson, P. (2001). Theorizing the Influence of Media on World Politics Models of Media Influence on Foreign Policy. European Journal of Communication, 16(4), 523-544.</ref>\n\nHallin\'s theory assumed a relatively homogenized media environment, where most producers were trying to reach most consumers. A more fractured media landscape can challenge this assumption.<ref>{{cite web|title=Does NPR Have A Liberal Bias?|url=http://www.onthemedia.org/2012/sep/14/|work=On The Media from NPR|publisher=WNYC|accessdate=11 February 2013}}</ref> because different audiences may place topics in different spheres, a concept related to the [[filter bubble]], which posits that many members of the public choose to limit their media consumption to the areas of consensus and deviance that they personally prefer.\n\n==See also==\n* [[Ambit claim]]\n* [[Argument to moderation]]\n* [[Creeping normality]]\n* [[Cultural hegemony]]\n* [[Door-in-the-face technique]]\n* [[Political suicide]]\n* [[Slippery slope]]\n* [[Spiral of silence]]\n* [[Third rail of politics]]\n\n== References ==\n{{reflist}}\n\n==External links==\n* [http://archive.pressthink.org/2009/01/12/atomization.html Audience Atomization Overcome: Why the Internet Weakens the Authority of the Press], Rosen, Jay. PressThink.org, January 12, 2009\n* [http://wnymedia.net/smith/2009/03/the-sphere-of-deviance/ The Sphere of Deviance] Smith, Christopher. WNYMedia.net, 2009.\n* [http://www.onthemedia.org/2012/sep/14/ Does NPR have a Liberal Bias?], On The Media from NPR. Retrieved 11 February 2013\n\n{{Media culture}}\n{{Media manipulation}}\n{{Propaganda}}\n\n[[Category:Framing (social sciences)]]\n[[Category:Knowledge representation]]\n[[Category:Propaganda techniques]]\n[[Category:Journalism]]']
