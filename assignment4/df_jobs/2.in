['Information needs', '11016342', 'information need is an uncertainty that arises in an individual, and which the individual is believed to be satisfied by information.{{more footnotes|date=June 2015}}\nThe concept \'\'\'Information need\'\'\' is seldom, if ever, mentioned in the general literature about [[needs]], but is a common term in the literature of [[information science]]. According to Hjørland (1997) it is closely related to the concept of [[relevance]]: If something is relevant for a person in relation to a given task, we might say that the person needs the information for that task. \n\nIt is often understood as an individual or group\'s desire to locate and obtain [[information]] to satisfy a conscious or unconscious [[need]]. The ‘information’ and ‘need’ in ‘information need’ are an inseparable interconnection. Needs and interests call forth information. The objectives of studying information needs are:\n# The explanation of observed phenomena of information use or expressed need;\n# The prediction of instances of information uses;\n# The control and thereby improvement of the utilization of information manipulation of essentials conditions.\n\nInformation needs are related to, but distinct from [[information requirements]].  An example is that a need is hunger; the requirement is food.\n\n== Background ==\n\nThe concept of information needs was coined by an American information journalist [http://www.libsci.sc.edu/BOB/ISP/taylor2.htm Robert S. Taylor] in his article [http://doi.wiley.com/10.1002/asi.5090130405 "The Process of Asking Questions"] published in American Documentation (renamed Journal of the American Society of Information Science and Technology).\n\nIn this paper, Taylor attempted to describe how an inquirer obtains an answer from an [[information system]], by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system.\n\nAccording to Taylor, information need has four levels:\n# The conscious and unconscious need for information not existing in the remembered experience of the investigator. In terms of the query range, this level might be called the “ideal question” — the question which would bring from the ideal system exactly what the inquirer, if he could state his need. It is the actual, but unexpressed, need for information\n# The conscious mental description of an ill-defined area of in decision. In this level, the inquirer might talk to someone else in the field to get an answer.\n# A researcher forms a rational statement of his question. This statement is a rational and unambiguous description of the inquirer’s doubts.\n# The question as presented to the information system.\n\nThere are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback).\n\nHerbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles:\n# When approached from the point of view of the scientist or technologists, these are studies of scientists’ communication behaviour;\n# When approached from the point of view of any communication medium, they are use studies;\n# When approached from the science communication system, they are studies in the flow of information among scientists and technologists.\n\nWilliam J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research. In the framework, he places the information users at the centre of ten systems, which are:\n# The scientist within his culture.\n# The scientist within a political system.\n# The scientist within a membership group.\n# The scientist within a reference group.\n# The scientist within an invisible college.\n# The scientist within a formal organization.\n# The scientist within a work team.\n# The scientist within his own head.\n# The scientist within a legal/economical system.\n# The scientist within a formal.\n\n==See also==\n* [[Information retrieval]]\n* [[Needs]]\n\n==References==\n* Hjørland, Birger (1997). Information seeking and subject representation. An activity-theoretical approach to information science. Westport, CO: Greenwood Press. \n* Menzel, Herbert. “Information Needs and Uses in Science and Technology.” Annual Review of Information Science and Technology, Vol. 1, Interscience Publishers 1966, pp 41-69.\n* Paisley, William J. “Information Needs and Uses.” Annual Review of Information Science and Technology, Vol.3, Encyclopædia Britannica, Inc. Chicago 1968, pp.1-30.\n* Taylor, Robert S. “The Process of Asking Questions” American Documentation, Vol.13, No. 4, October 1962, pp.391-396, DOI: 10.1002/asi.5090130405.\n* Wilson, T.D. “On User Studies and Information Needs.” Journal of Documentation, Vol. 37, No. 1, 1981, pp.3-15\n\n[[Category:Information retrieval]]']
['Category:Internet search', '8321034', '{{Cat main|Internet search}}\n\n[[Category:Web services]]\n[[Category:Information retrieval]]\n[[Category:World Wide Web|Search]] <!-- searching is a web function. Note that [[Internet search]] redirects to [[Web search engine]] -->']
['Category:Information retrieval genres', '46965446', '[[Category:Information retrieval]]']
['Wiener connector', '45655492', '{{Orphan|date=February 2016}}\n\nIn mathematics applied to the study of networks, the \'\'\'Wiener connector\'\'\', named in honor of chemist [[Harry Wiener]] who first introduced the [[Wiener Index]], is a means of maximizing efficiency in connecting specified "query vertices" in a network. Given a [[connected graph|connected]], [[undirected graph]] and a set of query vertices in a graph, the \'\'\'minimum Wiener connector\'\'\' is an [[induced subgraph]] that connects the query vertices and minimizes the sum of [[shortest path]] distances among all pairs of vertices in the subgraph. In [[combinatorial optimization]], the \'\'\'minimum Wiener connector problem\'\'\' is the problem of finding the minimum Wiener connector. It can be thought of as a version of the classic [[Steiner tree problem]] (one of [[Karp\'s 21 NP-complete problems]]), where instead of minimizing the size of the tree, the objective is to minimize the distances in the subgraph.<ref name="steiner">{{cite journal|last1=Hwang|first1=Frank|last2=Richards|first2=Dana|last3=Winter|first3=Dana|last4=Winter|first4=Pawel|title=The Steiner Tree Problem|journal=Annals of Discrete Mathematics|date=1992|url=http://www.sciencedirect.com/science/bookseries/01675060/53}}</ref><ref name="dimacs">[http://dimacs11.cs.princeton.edu/ DIMACS Steiner Tree Challenge]</ref>\n\nThe minimum Wiener connector was first presented by Ruchansky, et al. in 2015.<ref name="sigmod">{{cite journal|last2=Bonchi|first2=Francesco|last3=Garcia-Soriano|first3=David|last4=Gullo|first4=Francesco|last5=Kourtellis|first5=Nicolas|date=2015|year=|title=The Minimum Wiener Connector|url=https://arxiv.org/abs/1504.00513|journal=SIGMOD|volume=|pages=|via=|last1=Ruchansky|first1=Natali}}</ref>\n\nThe minimum Wiener connector has applications in many domains where there is a graph structure and an interest in learning about connections between sets of individuals. For example, given a set of patients infected with a viral disease, which other patients should be checked to find the culprit? Or given a set of proteins of interest, which other proteins participate in pathways with them?\n\n==Problem definition==\nThe [[Wiener index]] is the sum of shortest path distances in a (sub)graph. Using <math>d(u,v)</math> to denote the shortest path between <math>u</math> and <math>v</math>, the Wiener index of a (sub)graph <math>S</math>, denoted <math>W(S)</math>, is defined as\n: <math>W(S) = \\sum_{(u, v) \\in S} d(u,v)</math>.\n\nThe minimum Wiener connector problem is defined as follows. Given an undirected and unweighted graph with vertex set <math>V</math> and edge set <math>E</math> and a set of query vertices <math>Q\\subseteq V</math>, find a connector <math>H\\subseteq V</math> of minimum Wiener index. More formally, the problem is to compute\n: <math>\\operatorname*{arg\\,min}_H W(H\\cup Q)</math>,\nthat is, find a connector <math>H</math> that minimizes the sum of shortest paths in <math>H</math>.\n\n==Relationship to Steiner tree==\n[[File:SteinerExample nicer.pdf|thumb|upright=2.0|The optimal solutions to the Steiner tree problem and the minimum Wiener connector can differ. Define the set of query vertices \'\'Q\'\' by \'\'Q\'\' = {\'\'v\'\'<sub>1</sub>, &hellip;, \'\'v\'\'<sub>10</sub>}. The unique optimal solution to the Steiner tree problem is \'\'Q\'\' itself, which has Wiener index 165, whereas the optimal solution for the minimum Wiener connector problem is \'\'Q\'\' ∪ {\'\'r\'\'<sub>1</sub>, \'\'r\'\'<sub>2</sub>}, which has Wiener index 142.]]\nThe minimum Wiener connector problem is related to the [[Steiner tree problem]]. In the former, the [[objective function]] in the minimization is the Wiener index of the connector, whereas in the latter, the objective function is the sum of the weights of the edges in the connector. The optimum solutions to these problems may differ, given the same graph and set of query vertices. In fact, a solution for the Steiner tree problem may be arbitrarily bad for the minimum Wiener connector problem; the graph on the right provides an example.\n\n== Computational complexity ==\n\n===Hardness===\nThe problem is [[NP-hard]], and does not admit a [[polynomial-time approximation scheme]] unless [[P = NP|\'\'\'P\'\'\' = \'\'\'NP\'\'\']].<ref name="sigmod"/> This can be proven using the [[inapproximability]] of [[vertex cover]] in bounded degree graphs.<ref name="dinursafra">{{cite journal|last1=Dinur|first1=Irit|last2=Safra|first2=Samuel|title=On the hardness of approximating minimum vertex cover|journal=Annals of Mathematics|date=2005}}</ref> Although there is no polynomial-time approximation scheme, there is a polynomial-time constant-factor approximation—an algorithm that finds a connector whose Wiener index is within a constant multiplicative factor of the Wiener index of the optimum connector. In terms of [[complexity class]]es, the minimum Wiener connector problem is in \'\'\'[[APX]]\'\'\' but is not in \'\'\'PTAS\'\'\' unless \'\'\'P\'\'\' = \'\'\'NP\'\'\'.\n\n=== Exact algorithms ===\nAn exhaustive search over all possible subsets of vertices to find the one that induces the connector of minimum Wiener index yields an algorithm that finds the optimum solution in <math>2^{O(n)}</math> time (that is, [[exponential time]]) on graphs with \'\'n\'\' vertices. In the special case that there are exactly two query vertices, the optimum solution is the [[shortest path]] joining the two vertices, so the problem can be solved in [[polynomial time]] by computing the shortest path. In fact, for any fixed constant number of query vertices, an optimum solution can be found in polynomial time.\n\n=== Approximation algorithms ===\nThere is a constant-factor approximation algorithm for the minimum Wiener connector problem that runs in time <math>O(q (m \\log n + n \\log^2 n))</math> on a graph with \'\'n\'\' vertices, \'\'m\'\' edges, and \'\'q\'\' query vertices, roughly the same time it takes to compute shortest-path distances from the query vertices to every other vertex in the graph.<ref name="sigmod"/> The central approach of this algorithm is to reduce the problem to the vertex-weighted Steiner tree problem, which admits a constant-factor approximation in particular instances related to the minimum Wiener connector problem.\n\n==Behavior==\n\nThe minimum Wiener connector behaves like [[Centrality#Betweenness centrality|betweenness centrality]].\n\nWhen the query vertices belong to the same community, the non-query vertices that form the minimum Wiener connector tend to belong to the same community and have high centrality within the community.  Such vertices are likely to be [[influential]] vertices playing leadership roles in the community. In a [[social network]], these influential vertices might be good users for spreading information or to target in a viral marketing campaign.<ref name="viral">{{cite journal | first1=Oliver | last1=Hinz | first2=Bernd | last2=Skiera | first3=Christian | last3=Barrot | first4=Jan U. | last4=Becker | title=Seeding Strategies for Viral Marketing: An Empirical Comparison | journal=Journal of Marketing | volume=75 | number=6 | pages=55–71 | year = 2011 | doi=10.1509/jm.10.0088}}</ref>\n\nWhen the query vertices belong to different communities, the non-query vertices that form the minimum Wiener connector contain vertices adjacent to edges that bridge the different communities. These vertices span a [[Social Network#Structural holes|structural hole]] in the graph and are important.<ref name="structhole">{{cite conference|last1=Lou|first1=Tiancheng|last2=Tang|first2=Jie|title=Mining Structural Hole Spanners Through Information Diffusion in Social Networks|booktitle=Proceedings of the 22nd International Conference on World Wide Web|date=2013|isbn=9781450320351|location=Rio de Janeiro, Brazil|pages=825–836|url=http://dl.acm.org/citation.cfm?id=2488388.2488461|publisher=International World Wide Web Conferences Steering Committee}}</ref>\n\n==Applications==\nThe minimum Wiener connector is useful in applications in which one wishes to learn about the relationship between a set of vertices in a graph. For example,\n* in [[biology]], it provides insight into how a set of proteins in a [[protein–protein interaction]] network are related,\n* in [[social network]]s (like [[Twitter]]), it demonstrates the communities to which a set of users belong and how these communities are related,\n* in [[computer network]]s, it may be useful in identifying an efficient way to route a [[multicast]] message to a set of destinations.\n\n==References==\n{{reflist}}\n\n[[Category:NP-complete problems]]\n[[Category:Trees (graph theory)]]\n[[Category:Computational problems in graph theory]]\n[[Category:Geometric algorithms]]\n[[Category:Geometric graphs]]\n[[Category:Graph algorithms]]\n[[Category:Data mining]]\n[[Category:Social networks]]\n[[Category:Computational biology]]\n[[Category:Computer science]]\n[[Category:Algorithms]]\n[[Category:Information retrieval]]\n__INDEX__']
['Rollback (data management)', '1015240', '{{Other uses|Rollback (disambiguation)}}\n{{Selfref|For the Wikipedia tool, see [[Wikipedia:Rollback feature]].}}\n{{no footnotes|date=June 2009}}\n\nIn [[database]] technologies, a \'\'\'rollback\'\'\' is an operation which returns the database to some previous state. Rollbacks are important for database [[data integrity|integrity]], because they mean that the database can be restored to a clean copy even after erroneous operations are performed. They are crucial for recovering from database server crashes; by rolling back any [[Database transaction|transaction]] which was active at the time of the crash, the database is restored to a consistent state.\n\nThe rollback feature is usually implemented with a [[Database log|transaction log]], but can also be implemented via [[multiversion concurrency control]].\n\n==Cascading rollback==\nA \'\'cascading rollback\'\' occurs in database systems when a transaction (T1) causes a failure and a rollback must be performed. Other transactions dependent on T1\'s actions must also be rollbacked due to T1\'s failure, thus causing a cascading effect. That is, one transaction\'s failure causes many to fail.\n\nPractical database recovery techniques guarantee cascadeless rollback, therefore a cascading rollback is not a desirable result.\n\n==SQL==\nIn [[SQL]], <code>ROLLBACK</code> is a command that causes all data changes since the last <code>[[Begin work (SQL)|BEGIN WORK]]</code>, or <code>[[Start transaction (SQL)|START TRANSACTION]]</code> to be discarded by the [[relational database management systems]] (RDBMS), so that the state of the data is "rolled back" to the way it was before those changes were made.\n\nA <code>ROLLBACK</code> statement will also release any existing [[savepoint]]s that may be in use.\n\nIn most SQL dialects, <code>ROLLBACK</code>s are connection specific.  This means that if two connections are made to the same database, a <code>ROLLBACK</code> made in one connection will not affect any other connections.  This is vital for proper [[Concurrent programming|concurrency]].\n\n==See also==\n*[[Savepoint]]\n*[[Commit (data management)|Commit]]\n*[[Undo]]\n*[[Schema migration]]\n\n==References==\n*{{cite book |author = [[Ramez Elmasri]] |title= Fundamentals of Database Systems |publisher= [[Pearson Addison Wesley]] |year= 2007|isbn= 0-321-36957-2 }}\n*[http://msdn2.microsoft.com/en-us/library/ms181299.aspx "ROLLBACK Transaction"], Microsoft SQL Server.\n*[http://www.pantz.org/software/mysql/mysqlcommands.html "Sql Commands"], MySQL.\n\n{{Databases}}\n{{Web syndication}}\n\n[[Category:Data management]]\n[[Category:Database theory]]\n[[Category:SQL]]\n[[Category:Transaction processing]]\n[[Category:Reversible computing]]\n\n\n{{compu-prog-stub}}']
['Data bank', '40990', "In [[telecommunication]]s, a '''data bank''' is a repository of information on one or more subjects that is organized in a way that facilitates local or remote information retrieval. A data bank may be either centralized or decentralized.\nIn computers the data bank is the same as in telecommunication (i.e. it is the repository of data. The data in the data bank can be things such as credit card transactions or it can be any data base of a company where large quantities of queries are being processed on daily bases). \n \n'''Data bank''' may also refer to an organization primarily concerned with the construction and maintenance of a [[database]].\n\n== See also ==\n\n* [[Star Wars Databank]]\n* [[Protein Data Bank]]\n* [[National Trauma Data Bank]]\n* [[memory bank]]\n* [[International Tree-Ring Data Bank]]\n* [[Hazardous Substances Data Bank]]\n* [[electron microscopy data bank]]\n* [[Dortmund Data Bank]]\n* [[Casio Databank]]\n* [[conformational dynamics data bank]]\n* [[Databank Systems Limited]] a former New Zealand banking agency\n\n==Sources==\n*{{FS1037C MS188}}\n*''[[The American Heritage Dictionary of the English Language]], Fourth Edition''. [[Houghton Mifflin]], 2000.\n\n==External links==\n{{wiktionary}}\n\n[[Category:Data management]]\n\n\n{{telecomm-stub}}"]
['Universal Data Element Framework', '2570284', 'The \'\'\'Universal Data Element Framework\'\'\' (\'\'\'UDEF\'\'\') provides the foundation for building an enterprise-wide [[controlled vocabulary]]. It is a standard way of indexing enterprise information that can produce big cost savings. UDEF simplifies information management through consistent classification and assignment of a global standard identifier to the data names and then relating them to similar data element concepts defined by other organizations. Though this approach is a small part of the overall picture, it is potentially a crucial enabler of semantic [[interoperability]].\n\n== How UDEF works ==\nUDEF provides semantic links, through assigning an intelligent, derived ID as an attribute of the data element, essentially labeling the element as a specific data element concept. When this UDEF ID exists in both source and target formats, it can then be used as an easy analysis point via a [[match report]], and then as the primary pivot point for transformations between source and target.\n\nUDEF takes a list of high-level root object classes and assigns an integer to each class plus alpha characters to each specialization modifier.  It then also assigns integers to property word plus integers to each specialization modifier.  These object class alpha-integers are concatenated together with the property integers to form a dewey-decimal like code for each data element concept.\n\n===Examples===\nFor the following examples, go to http://www.opengroup.org/udefinfo/htm/en_defs.htm and expand the applicable UDEF object and property trees\n\nAssuming an application used by a hospital needs to map the data element concepts to the UDEF, the last name and first name (within UDEF you will find Family Name and Given Name under the UDEF property Name) of several people that are likely to appear on a medical record that could include the following example data element concepts –\n\n*Patient Person Family Name – find the word “Patient” under the UDEF object “Person” and find the word “Family” under the UDEF property “Name”\n*Patient Person Given Name – find the word “Patient” under the UDEF object “Person” and find the word “Given” under the UDEF property “Name”\n*Doctor Person Family Name – find the word “Doctor” under the UDEF object “Person” and find the word “Family” under the UDEF property “Name”\n*Doctor Person Given Name – find the word “Doctor” under the UDEF object “Person” and find the word “Given” under the UDEF property “Name”\n\nThe associated UDEF IDs for the above are derived by walking up each tree respectively and using an underscore to separate the object from the property. For the examples above, the following data element concepts are available within the current UDEF – see http://www.opengroup.org/udefinfo/htm/en_ob5.htm and http://www.opengroup.org/udefinfo/htm/en_pr10.htm\n\n*“Patient Person Family Name” the UDEF ID is “au.5_11.10”\n*“Patient Person Given Name” the UDEF ID is “au.5_12.10”\n*“Doctor Person Family Name” the UDEF ID is “aq.5_11.10”\n*“Doctor Person Given Name” the UDEF ID is “aq.5_12.10”\n\n== Six basic steps to map enterprise data to the UDEF ==\n\nThere are six basic steps to follow when mapping data element concepts to the UDEF.\n\n1. Identify the applicable UDEF property word that characterizes the dominant attribute (property) of the data element concept. For example: Name, Identifier, Date, etc.\n\n2. Identify the dominant UDEF object word that the dominant property (selected in step 1) is describing. For example, Person_Name, Product_Identifier, Document_Date, etc.\n\n3. By reviewing the UDEF tree for the selected property identified in step 1, identify applicable qualifiers that are necessary to describe the property word term unambiguously. For example, Family Name.\n\n4. By reviewing the UDEF tree for the selected object identified in step 2, identify applicable qualifiers that are necessary to describe the object word term unambiguously. For example, Customer Person.\n\n5. Concatenate the object term and the property term to create a UDEF naming convention compliant name where it is recognized that the name may seem artificially long. For example, Customer Person_Family Name.\n\n6. Derive a structured ID based on the UDEF taxonomy that carries the UDEF inherited indexing scheme. For example <CustomerPersonFamilyName UDEFID=”as.5_11.10”>.\n\n== The Open Group UDEF Project objectives ==\n\nThe UDEF Project aims to establish the Universal Data Element Framework (UDEF) as the universally-used classification system for data element concepts. It focuses on developing and maintaining the UDEF as an open standard, advocating and promoting it, putting in place a technical infrastructure to support it, implementing a Registry for it, and setting up education programs to train information professionals in its use.\n\nOrganizations that implement UDEF will likely realize the greatest benefit by defining their controlled vocabulary based on the UDEF. To help an organization manage its UDEF based controlled vocabulary, it should seriously consider a metadata registry that is based on ISO/IEC 11179-5.\n\n== History of UDEF ==\n\nRon Schuldt, Sr. Enterprise Data Architect, Lockheed Martin, originated the UDEF concept based on [[ISO/IEC 11179]] Metadata standards in the early 1990s. Currently, he is a Senior Partner with Femto-Data LLC\n\n== Ownership of UDEF intellectual property ==\n\nThe Open Group assumed from the [[Association for Enterprise Information]] (AFEI) the right to grant public use licensing of the UDEF.\n\nThe Supplier Management Council Electronic Enterprise Working Group of the [[Aerospace Industry Association]] (AIA) supports the UDEF as the naming convention solution to [[XML]] interoperability between standards that include all functions throughout a product\'s life-cycle and is working through a well defined process to obtain approval of this position from AIA and its member companies.\n\n== Criticism ==\n\nClassification in UDEF is sometimes hampered by ad hoc decisions that might produce problems. \nExample: \n* b.be.5 is "United-Kingdom Citizen Person" and\n* c.be.5 is "European Union Citizen Person"\nAs the United Kingdom is part of the European Union, the classification is not unique.\nResponse:\nThe UDEF is flexible and is designed to match the semantics and behaviour of existing systems. Therefore, if one system has a table for United Kingdom Citizens and a different system has a table for European Union Citizens, the UDEF can handle both situations.\n\nSome of the concepts in UDEF are not as universal as it is claimed. They show a lot of bias to Anglo-American tradition and way of thinking and are not easily transferable to other languages.\nExample: The following part of the hierarchy shows the concept of an officer.\n\n* j.5        Officer.Person\n* a.j.5      Contracting.Officer.Person\n* a.a.j.5    Procuring.Contracting.Officer.Person\n* a.a.a.j.5  Government.Procuring.Contracting.Officer.Person\n* b.a.j.5    Administrative.Contracting.Officer.Person\n* b.j.5      Police.Officer.Person\n* c.j.5      Military.Officer.Person\nIn many cultures, the part of the tree below "a.j.5 Contracting Officer Person" would not be placed under j.5 (see [[Officer (disambiguation)|officer]]) as b.j.5 (see [[Law enforcement officer]]) or c.j.5 (see [[Officer (armed forces)]]).\n\n==See also==\n* [[Data integration]]\n* [[ISO/IEC 11179]]\n* [[National Information Exchange Model]]\n* [[Metadata]]\n* [[Naming conventions (programming)]]\n* [[Semantics]]\n* [[Semantic web]]\n* [[Semantic equivalency]]\n* [[Data element]]\n* [[Representation term]]\n* [[Controlled vocabulary]]\n\n== External links ==\n* [http://www.opengroup.org/udefinfo/ UDEF Project of The Open Group]\n* [https://www.youtube.com/embed/y6hID5qzAzQ YouTube - UDEF Tutorial, Part 1]\n* [https://www.youtube.com/embed/d6dH_U8TqhY YouTube - UDEF Tutorial, Part 2]\n* [http://www.opengroup.org/udefinfo/faq.htm UDEF Frequently Asked Questions]\n* [https://udef-it.com/UDEF_Tools.html - Obtain Enhanced UDEF Gap Analysis Tool in English, Dutch, or French]\n\n== Further reading ==\n* {{cite book |title=UDEF - Six Steps to Cost Effective Data Integration |author=Ronald Schuldt |publisher=CreateSpace |date=November 15, 2011 |isbn=978-1-4664-6762-0 |url=https://www.createspace.com/3711806}}\n* {{cite book |title=UDEF - Six Steps to Cost Effective Data Integration |author=Ronald Schuldt and Roberta Shauger |publisher=Amazon Digital Services |date=January 16, 2012 |isbn=1-4664-6762-2 |url=http://www.amazon.com/dp/B006YK6YOQ}}\n* {{cite book |title=UDEF Concepts Defined - Reference Guide |author=Roberta Shauger |publisher=CreateSpace |date=December 20, 2011 |isbn=978-1-4681-1483-6 |url=https://www.createspace.com/3753707}}\n* {{cite book |title=UDEF Concepts Defined - Reference Guide |author=Roberta Shauger and Ronald Schuldt |publisher=Amazon Digital Services |date=January 14, 2012 |isbn=1-4681-1483-2 |url=http://www.amazon.com/dp/B006XXMLQE}}\n\n[[Category:Data management]]\n[[Category:Interoperability]]\n[[Category:Knowledge representation]]\n[[Category:Metadata]]\n[[Category:Open Group standards]]\n[[Category:Software that uses Motif]]\n[[Category:Technical communication]]']
['Distributed database', '41054', '{{multiple issues|\n{{refimprove|date=August 2010}}\n{{Cleanup|date=June 2009}}\n}}\n\nA \'\'\'distributed database\'\'\' is a [[database]]  in which [[computer data storage|storage devices]] are not all attached to a common [[Processor (computing)|processor]].<ref>http://www.its.bldrdoc.gov/fs-1037/dir-012/_1750.htm</ref> It may be stored in multiple [[computers]], located in the same physical location; or may be dispersed over a [[computer network|network]] of interconnected computers. Unlike [[Parallel computing|parallel systems]], in which the processors are tightly coupled and constitute a single database system, a distributed database system consists of loosely coupled sites that share no physical components.\n\nSystem administrators can distribute collections of data (e.g. in a database) across multiple physical locations. A distributed database can reside on organized [[network servers]] or [[blockchain (database)|decentralized independent computers]] on the [[Internet]], on corporate [[intranets]] or [[extranets]], or on other organization [[Computer network|networks]]. Because they store data across multiple computers, distributed databases may improve performance at [[end-user]] worksites by allowing transactions to be processed on many machines, instead of being limited to one.<ref name="obrien">\nO\'Brien, J. & Marakas, G.M.(2008) Management Information Systems (pp. 185-189). New York, NY: McGraw-Hill Irwin</ref>\n\nTwo processes ensure that the distributed databases remain up-to-date and current: [[Replication (computing)|replication]] and [[Data transmission|duplication]].\n\n# Replication involves using specialized software that looks for changes in the distributive database. Once the changes have been identified, the replication process makes all the databases look the same. The replication process can be complex and time-consuming depending on the size and number of the distributed databases. This process can also require a lot of time and computer resources.  \n# Duplication, on the other hand, has less complexity. It basically identifies one database as a [[master-slave (technology)|master]] and then duplicates that database. The duplication process is normally done at a set time after hours.  This is to ensure that each distributed location has the same data.  In the duplication process, users may change only the master database. This ensures that local data will not be overwritten.\n\nBoth replication and duplication can keep the data current in all distributive locations.<ref name="obrien" />\n\nBesides distributed database replication and fragmentation, there are many other distributed database design technologies. For example, local autonomy, synchronous and asynchronous distributed database technologies. These technologies\' implementations can and do depend on the needs of the business and the sensitivity/[[confidentiality]] of the data stored in the database, and the price the business is willing to spend on ensuring [[data security]], [[data consistency|consistency]] and [[data integrity|integrity]].\n\nWhen discussing access to distributed databases, [[Microsoft]] favors the term \'\'\'distributed query\'\'\', which it defines in protocol-specific manner as "[a]ny SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources".<ref>\n{{cite web\n |url          = http://technet.microsoft.com/en-us/library/cc966484.aspx\n |title        = TechNet Glossary\n |publisher    = Microsoft\n |accessdate   = 2013-07-16\n |quote        = distributed query[:] Any SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources.\n}}\n</ref>\n[[Oracle Database|Oracle]] provides a more language-centric view in which distributed queries and [[distributed transaction]]s form part of \'\'\'distributed SQL\'\'\'.<ref>\n{{cite web\n |url          = http://docs.oracle.com/cd/E11882_01/server.112/e25789/toc.htm\n |title        = Oracle Database Concepts, 11g Release 2 (11.2)\n |last1        = Ashdown\n |first1       = Lance\n |last2        = Kyte\n |first2       = Tom\n |date=September 2011\n |publisher    = Oracle Corporation\n |accessdate   = 2013-07-17\n |quote        = Distributed SQL synchronously accesses and updates data distributed among multiple databases. [...] Distributed SQL includes distributed queries and distributed transactions. \n}}\n</ref>\n\nToday the distributed [[DBMS]] market is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and [[NoSQL]] DBMS engines, as well as [[XML database]]s and [[NewSQL|NewSQL databases]]. These databases are increasingly supporting distributed database architecture that provides [[high availability]] and [[fault tolerance]] through [[replication (computing)|replication]] and scale out ability.  Some examples are [[Aerospike database|Aerospike]],<ref>{{cite web|title=Aerospike distributed database|url=http://www.aerospike.com|website=Aerospike}}</ref> [[Apache Cassandra|Cassandra]],<ref>{{cite web |url=http://cassandra.apache.org/ |title=Apache Cassandra database menagement system |publisher=Apache.org}}</ref> [[Clusterpoint]],<ref>{{cite web |url=http://www.clusterpoint.com |title=Clusterpoint XML distributed database |publisher=Clusterpoint}}</ref> [[Clustrix|ClustrixDB]],<ref>{{cite web|title=Frequently Asked Questions about ClustrixDB - Clustrix Documentation|url=http://docs.clustrix.com/display/CLXDOC/Frequently+Asked+Questions+about+ClustrixDB#FrequentlyAskedQuestionsaboutClustrixDB-WhatisClustrixDB?|publisher=Clustrix, Inc.}}</ref> [[Couchbase Server|Couchbase]],<ref>{{cite web|title=Couchbase distributed database|url=http://www.couchbase.com|website=Couchbase}}</ref> [[Druid (open-source data store)]],<ref>{{cite web |url=http://druid.io |title=Druid distributed datastore/database |publisher=The Druid Community}}</ref> [[FoundationDB]],<ref>\n{{cite web |url=https://foundationdb.com |title=FoundationDB database |publisher=FoundationDB}}</ref> [[NuoDB]],<ref>Clark, Jack. [http://www.theregister.co.uk/2014/02/26/nuodb_funding/ "NuoDB slurps European cash for database expansion"] The Register. Feb. 26, 2014</ref> [[Riak]]<ref>\n{{cite web |url=http://www.basho.com |title=Basho Riak Distributed database |publisher=Basho}}</ref> and [[OrientDB]].<ref>\n{{cite web |url=http://www.orientdb.com |title=OrientDB database |publisher=OrientDB}}</ref> The [[Blockchain (database)|blockchain]] technology popularised by [[bitcoin]] is an implementation of a distributed database.<ref>{{cite news|last1=Margaret|first1=Alyson|title=How Bitcoin and the blockchain are a transformative technology |url=http://blog.blockchain.com/2015/06/23/how-bitcoin-and-the-block-chain-are-a-transformative-technology/|accessdate=23 July 2015|date=23 June 2015}}</ref>\n\n== Architecture ==\nA database user accesses the distributed database through:\n;Local applications\n:applications which do not require data from other sites.\n;Global applications\n:applications which do require data from other sites.\n\nA \'\'\'homogeneous distributed database\'\'\' has identical software and hardware running all databases instances, and may appear through a single interface as if it were a single database.  A \'\'\'heterogeneous distributed database\'\'\' may have different hardware, operating systems, database management systems, and even data models for different databases.\n\n===Homogeneous Distributed Databases Management System===\nIn homogeneous distributed database, all sites have identical software and are aware of each other and agree to cooperate in processing user requests. Each site surrenders part of its autonomy in terms of right to change schema or software. A homogeneous DBMS appears to the user as a single system. The homogeneous system is much easier to design and manage. The following conditions must be satisfied for homogeneous database:\n*The operating system used at each location must be same or compatible.{{According to whom|date=March 2013}}{{Elucidate|date=March 2013}}\n*The data structures used at each location must be same or compatible.\n*The database application (or DBMS) used at each location must be same or compatible.\n\n===Heterogeneous DDBMS===\n{{See also|Heterogeneous database system}}\nIn a heterogeneous distributed database, different sites may use different schema and software. Difference in schema is a major problem for query processing and transaction processing. Sites may not be aware of each other and may provide only limited facilities for cooperation in transaction processing. In heterogeneous systems, different nodes may have different hardware & software and data structures at various nodes or locations are also incompatible. Different computers and operating systems, database applications or data models may be used at each of the locations. For example, one location may have the latest relational database management technology, while another location may store data using conventional files or old version of database management system. Similarly, one location may have the Windows 10 operating system, while another may have UNIX. Heterogeneous systems are usually used when individual sites use their own hardware and software. On heterogeneous system, translations are required to allow communication between different sites (or DBMS). In this system, the users must be able to make requests in a database language at their local sites. Usually the SQL database language is used for this purpose. If the hardware is different, then the translation is straightforward, in which computer codes and word-length is changed. The heterogeneous system is often not technically or economically feasible. In this system, a user at one location may be able to read but not update the data at another location.\n\n== Important considerations ==\nCare with a distributed database must be taken to ensure the following:\n* The distribution is transparent — users must be able to interact with the system as if it were one logical system.  This applies to the system\'s performance, and methods of access among other things.\n* [[Database transaction|Transaction]]s are transparent — each transaction must maintain [[database integrity]] across multiple databases.  Transactions must also be divided into sub-transactions, each sub-transaction affecting one database system.\n\nThere are two principal approaches to store a relation r in a distributed database system:\n\n:A) [[database replication|Replication]]\n:B) Fragmentation/[[Partition (database)|Partitioning]]\n\nA) Replication: In replication, the system maintains several identical replicas of the same relation r in different sites.\n:* Data is more available in this scheme.\n:* Parallelism is increased when read request is served.\n:* Increases overhead on update operations as each site containing the replica needed to be updated in order to maintain consistency.\n:* Multi-datacenter replication provides geographical diversity, like in [[Clusterpoint]]<ref>\n{{cite web |url=http://www.clusterpoint.com/solutions/distributed-storage |title=Clusterpoint database distributed storage multi-datacenter replication|publisher=Clusterpoint}}</ref> or [[Riak]].<ref>\n{{cite web |url=http://basho.com/tag/multi-datacenter-replication/ |title=Riak database multi-datacenter replication|publisher=Basho}}</ref>\n\nB) Fragmentation: The relation r is fragmented into several relations r<sub>1</sub>, r<sub>2</sub>, r<sub>3</sub>....r<sub>n</sub> in such a way that the actual relation could be reconstructed from the fragments and then the fragments are scattered to different locations. There are basically two schemes of fragmentation:\n\n:* Horizontal fragmentation - splits the relation by assigning each tuple of r to one or more fragments.\n:* Vertical fragmentation - splits the relation by decomposing the schema R of relation r.\n\nA distributed database can be run by independent or even competing parties as, for example, in [[bitcoin]] or [[Hasq]].\n\n== Advantages ==\n* Management of distributed data with different levels of transparency like network transparency, fragmentation transparency, replication transparency, etc.\n* Increase reliability and availability\n* Easier expansion\n* Reflects organizational structure — database fragments potentially stored within the departments they relate to\n* Local autonomy or site autonomy — a department can control the data about them (as they are the ones familiar with it)\n* Protection of valuable data — if there were ever a catastrophic event such as a fire, all of the data would not be in one place, but distributed in multiple locations\n* Improved performance — data is located near the site of greatest demand, and the database systems themselves are parallelized, allowing load on the databases to be balanced among servers.  (A high load on one module of the database won\'t affect other modules of the database in a distributed database)\n* Economics — it may cost less to create a network of smaller computers with the power of a single large computer\n* Modularity — systems can be modified, added and removed from the distributed database without affecting other modules (systems)\n* Reliable transactions - due to replication of the database\n* Hardware, operating-system, network, fragmentation, DBMS, replication and location independence\n* Continuous operation, even if some nodes go offline (depending on design)\n* Distributed query processing can improve performance\n* Single-site failure does not affect performance of system.\n* For those systems that support full distributed transactions, operations enjoy the [[ACID]] properties:\n** A-atomicity, the transaction takes place as a whole or not at all\n** C-consistency, maps one consistent DB state to another\n** I-isolation, each transaction sees a consistent DB\n** D-durability, the results of a transaction must survive system failures\n\nThe Merge Replication Method is popularly used to consolidate the data between databases.{{citation needed|date=July 2013}}\n\n== Disadvantages ==\n* Complexity — [[Database administrator|DBAs]] may have to do extra work to ensure that the distributed nature of the system is transparent.  Extra work must also be done to maintain multiple [[disparate system]]s, instead of one big one.  Extra database design work must also be done to account for the disconnected nature of the database — for example, joins become prohibitively expensive when performed across multiple systems.\n* Economics — increased complexity and a more extensive infrastructure means extra labour costs\n* Security — remote database fragments must be secured, and they are not centralized so the remote sites must be secured as well.  The infrastructure must also be secured (for example, by encrypting the network links between remote sites).\n* Difficult to maintain integrity — but in a distributed database, enforcing integrity over a network may require too much of the network\'s resources to be feasible\n* Inexperience — distributed databases are difficult to work with, and in such a young field there is not much readily available experience in "proper" practice\n* Lack of standards — there are no tools or methodologies yet to help users convert a centralized DBMS into a distributed DBMS{{citation needed|date=July 2013}}\n* Database design more complex — In addition to traditional database design challenges, the design of a distributed database has to consider fragmentation of data, allocation of fragments to specific sites and data replication\n* Additional software is required\n* Operating system should support distributed environment\n* [[Concurrency control]] poses a major issue. It can be solved by [[Lock (database)|locking]] and [[timestamp]]ing.\n* Distributed access to data\n* Analysis of distributed data\n\n==See also==\n*[[Centralized database]]\n*[[Data grid]]\n*[[Distributed data store]]\n*[[Distributed cache]]\n*[[Routing protocol]]\n*[[Distributed hash table]]\n\n==References==\n{{Reflist|30em}}\n{{more footnotes|date=April 2013}}\n*M. T. Özsu and P. Valduriez, \'\'Principles of Distributed Databases\'\' (3rd edition) (2011), Springer, ISBN 978-1-4419-8833-1\n*Elmasri and Navathe, \'\'Fundamentals of database systems\'\' (3rd edition), Addison-Wesley Longman, ISBN 0-201-54263-3\n*\'\'Oracle Database Administrator\'s Guide 10g\'\' (Release 1), http://docs.oracle.com/cd/B14117_01/server.101/b10739/ds_concepts.htm\n\n{{Databases}}\n\n[[Category:Data management]]\n[[Category:Types of databases]]\n[[Category:Distributed computing architecture]]\n[[Category:Applications of distributed computing]]\n[[Category:Database management systems]]']
['Data archaeology', '2588620', '{{for|the computer-based analysis of archaeological data|Computational archaeology}}\n\n\'\'\'Data archaeology\'\'\' refers to the art and science of recovering [[computer]] [[data]] encoded and/or encrypted in now obsolete [[Computer media|media]] or [[content format|formats]]. Data archaeology can also refer to recovering information from damaged electronic formats after natural or man made disasters.\n\nThe term originally appeared in 1993 as part of the [[Global Oceanographic Data Archaeology and Rescue Project]] (GODAR). The original impetus for data archaeology came from the need to recover computerized records of climatic conditions stored on old computer tape, which can provide valuable evidence for testing theories of [[climate change]]. These approaches allowed the reconstruction of an image of the Arctic that had been captured by the [[Nimbus program|Nimbus 2]] satellite on September 23, 1966, in higher resolution than ever seen before from this type of data.<ref>[http://nsidc.org/monthlyhighlights/january2010.html Techno-archaeology rescues climate data from early satellites] U.S. National Snow and Ice Data Center (NSIDC), January 2010 [http://www.webcitation.org/5xN1sNyDp Archived]</ref>\n\n[[NASA]] also utilizes the services of data archaeologists to recover information stored on 1960s era vintage computer tape, as exemplified by the [[Lunar Orbiter Image Recovery Project]] (LOIRP).<ref>[http://www.nasa.gov/topics/moonmars/features/LOIRP/ LOIRP Overview] NASA website November 14, 2008 [http://www.webcitation.org/5xN1DjLG4 Archived]</ref>\n\n==Recovery==\nIt is also important to make the distinction in data archaeology between data recovery, and data intelligibility. You may be able to recover the data, but not understand it. For data archaeology to be effective the data must be intelligible.<ref name="www.ukoln.ac.uk"> [http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf] Study on website October 23, 2011 </ref>\n\n===Disaster recovery===\nData archaeologists can also use [[data recovery]] after natural disasters such as fires, floods, earthquakes, or even hurricanes. For example, in 1995 during [[Hurricane Marilyn]] the National Media Lab assisted the [[National Archives and Records Administration]] in recovering data at risk due to damaged equipment. The hardware was damaged from rain, salt water, and sand, yet it was possible to clean some of the disks and refit them with new cases thus saving the data within.<ref name="www.ukoln.ac.uk"/>\n\n===Recovery techniques===\nWhen deciding whether or not to try and recover data, the cost must be taken into account. If there is enough time and money, most data will be able to be recovered. In the case of [[magnetic media]], which are the most common type used for data storage, there are various techniques that can be used to recover the data depending on the type of damage.<ref name="www.ukoln.ac.uk"/>{{rp|17}}\n\nHumidity can cause tapes to become unusable as they begin to deteriorate and become sticky.  In this case, a heat treatment can be applied to fix this problem, by causing the oils and residues to either be reabsorbed into the tape or evaporate off the surface of the tape.  However, this should only be done in order to provide access to the data so it can be extracted and copied to a medium that is more stable.<ref name="www.ukoln.ac.uk"/>{{rp|17–18}}\n\nLubrication loss is another source of damage to tapes. This is most commonly caused by heavy use, but can also be a result of improper storage or natural evaporation.  As a result of heavy use, some of the lubricant can remain on the read-write heads which then collect dust and particles.  This can cause damage to the tape.  Loss of lubrication can be addressed by re-lubricating the tapes.  This should be done cautiously, as excessive re-lubrication can cause tape slippage, which in turn can lead to media being misread and the loss of data.<ref name="www.ukoln.ac.uk"/>{{rp|18}}\n\nWater exposure will damage tapes over time.  This often occurs in a disaster situation.  If the media is in salty or dirty water, it should be rinsed in fresh water.  The process of cleaning, rinsing, and drying wet tapes should be done at room temperature in order to prevent heat damage.  Older tapes should be recovered prior to newer tapes, as they are more susceptible to water damage.<ref name="www.ukoln.ac.uk"/>{{rp|18}}\n\n==Prevention==\nTo prevent the need of data archaeology, creators and holders of digital documents should take care to employ [[digital preservation]].\n\n==See also==\n* [[Data degradation|Bit rot]]\n* [[Digital dark age]]\n* [[Knowledge discovery]]\n\n==References==\n<references />\n*[http://www.worldwidewords.org/turnsofphrase/tp-dat1.htm World Wide Words: Data Archaeology]\n*O\'Donnell, James Joseph.  \'\'Avatars of the Word:  From Papyrus to Cyperspace\'\'  Harvard University Press, 1998.\n* {{cite book | last1  = Ross | first1 = Seamus | last2  = Gow | first2 = Ann| lastauthoramp = yes| title = Digital archaeology : rescuing neglected and damaged data resources| publisher = British Library and Joint Information Systems Committee| place     = London & Bristol| series = Electronic libraries programme studies\n| year    = 1999| | language = EN| url = http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf| isbn = 1-90050-851-6}}\n\n[[Category:Data management|Archaeology]]\n[[Category:Digital preservation]]\n[[Category:Archaeological sub-disciplines]]']
['Online complex processing', '7577329', "'''Online complex processing''' ('''OLCP''') is a class of realtime data processing involving complex queries, lengthy queries and/or simultaneous reads and writes to the same records.\n{{software-stub}}\n==Sources==\n*http://www.pcmag.com/encyclopedia_term/0,2542,t=online+complex+processing&i=48345,00.asp\n\n==See also==\n*[[Online transaction processing]] \n*[[OLAP]]\n*[[Transaction processing]]\n\n\n[[Category:Data management]]\n[[Category:Databases]]"]
['Distributed data store', '870094', "{{Essay-like|date=May 2012}}\n\nA '''distributed data store''' is a [[computer network]] where information is stored on more than one [[Node (networking)|node]], often in a [[Replication (computing)|replicated]] fashion.<ref>{{Citation\n|author = Yaniv Pessach\n|url = http://openlibrary.org/books/OL25423189M/Distributed_Storage_Concepts_Algorithms_and_Implementations\n|title = Distributed Storage\n|edition = Distributed Storage: Concepts, Algorithms, and Implementations\n}}</ref> It is usually specifically used to refer to either a [[distributed database]] where users store information on a ''number of nodes'', or a [[computer network]] in which users store information on a ''number of peer network nodes''.\n\n==Distributed databases==\n[[Distributed database]]s are usually [[non-relational database]]s that make a quick access to data over a large number of nodes possible. Some distributed databases expose rich query abilities while others are limited to a [[key-value store]] semantics. Examples of limited distributed databases are [[Google]]'s [[BigTable]], which is much more than a [[distributed file system]] or a [[peer-to-peer network]],<ref>{{cite web\n| accessdate = 2011-04-05\n| location = http://the-paper-trail.org/\n| publisher = Paper Trail\n| title = BigTable: Google's Distributed Data Store\n| quote = Although GFS provides Google with reliable, scalable distributed file storage, it does not provide any facility for structuring the data contained in the files beyond a hierarchical directory structure and meaningful file names. It’s well known that more expressive solutions are required for large data sets. Google’s terabytes upon terabytes of data that they retrieve from web crawlers, amongst many other sources, need organising, so that client applications can quickly perform lookups and updates at a finer granularity than the file level. [...] The very first thing you need to know about BigTable is that it isn’t a relational database. This should come as no surprise: one persistent theme through all of these large scale distributed data store papers is that RDBMSs are hard to do with good performance. There is no hard, fixed schema in a BigTable, no referential integrity between tables (so no foreign keys) and therefore little support for optimised joins.\n| url = http://the-paper-trail.org/blog/?p=86}}</ref> [[Amazon.com|Amazon]]'s [[Dynamo (storage system)|Dynamo]]<ref>{{cite web\n| accessdate = 2011-04-05\n| author = Sarah Pidcock\n| date = 2011-01-31\n| location = http://www.cs.uwaterloo.ca/\n| page = 2/22\n| publisher = WATERLOO – CHERITON SCHOOL OF COMPUTER SCIENCE\n| title = Dynamo: Amazon’s Highly Available Key-value Store\n| quote = Dynamo: a highly available and scalable distributed data store\n| url = http://www.cs.uwaterloo.ca/~kdaudjee/courses/cs848/slides/sarah1.pdf}}</ref>\nand [[Azure Services Platform|Windows Azure Storage]].<ref>{{cite web \n| url= http://www.microsoft.com/windowsazure/features/storage/ |title= Windows Azure Storage |author= |date=2011-09-16 |work= |publisher= |accessdate=6 November 2011}}</ref>\n\nAs the ability of arbitrary querying is not as important as the [[availability]], designers of distributed data stores have increased the latter at an expense of consistency. But the high-speed read/write access results in reduced consistency, as it is not possible to have both [[Consistency (database systems)|consistency]], availability, and partition tolerance of the network, as it has been proven by the [[CAP theorem]].\n\n==Peer network node data stores==\nIn peer network data stores, the user can usually reciprocate and allow other users to use their computer as a storage node as well. Information may or may not be accessible to other users depending on the design of the network.\n\nMost [[peer-to-peer]] networks do not have distributed data stores in that the user's data is only available when their node is on the network. However, this distinction is somewhat blurred in a system such as [[BitTorrent (protocol)|BitTorrent]], where it is possible for the originating node to go offline but the content to continue to be served. Still, this is only the case for individual files requested by the redistributors, as contrasted with a network such as [[Freenet]] where all computers are made available to serve all files.\n\nDistributed data stores typically use an [[error detection and correction]] technique.\nSome distributed data stores (such as [[Parchive]] over NNTP) use [[forward error correction]] techniques to recover the original file when parts of that file are damaged or unavailable.\nOthers try again to download that file from a different mirror.\n\n==Examples==\n\n===Distributed non-relational databases===\n* [[Aerospike database|Aerospike]]\n* [[Apache Cassandra]], former data store of [[Facebook]]\n* [[BigTable]], the data store of [[Google]]\n* [[CrateIO]]\n* [[Druid (open-source data store)]], used by [[Netflix]], [[Yahoo]] and others\n* [[Dynamo (storage system)|Dynamo]] of [[Amazon.com|Amazon]]\n* [[Hazelcast]]\n* [[HBase]], current data store of Facebook's Messaging Platform\n* [[Couchbase]], data store used by [[LinkedIn]], [[Paypal]], [[Ebay]] and others.\n* [[MongoDB]]\n* [[Riak]]\n* [[Hypertable]], from [[Baidu]]\n* [[Voldemort (distributed data store)|Voldemort]], data store used by [[LinkedIn]]\n\n===Peer network node data stores===\n* [[BitTorrent (protocol)|BitTorrent]]\n* [[Blockchain (database)]]\n* [[Chord project]]\n* [[GNUnet]]\n* [[Freenet]]\n* Unity, of the software [[Perfect Dark (P2P)|Perfect Dark]]\n* [[Mnet (Computer program)|Mnet]]\n* [[Network News Transfer Protocol|NNTP]] (the distributed data storage protocol used for [[Usenet]] news)\n* [[Storage@home]]\n* [[Tahoe-LAFS]]\n\n==See also==\n{{Portal|Computer Science}}\n* [[Data store]]\n* [[Distributed file system]]\n* [[Keyspace (distributed data store)|Keyspace]], the DDS [[Schema (database)|schema]]\n* [[Peer-to-peer]]\n* [[Distributed hash table]]\n* [[Distributed cache]]\n\n==References==\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:Distributed data storage| ]]\n[[Category:Distributed data stores| ]]\n\n[[ja:分散ファイルシステム#分散データストア]]"]
['Data field', '10082565', "{{Unreferenced|date=March 2007}}\nA '''data field''' is a place where you can store [[data]].  Commonly used to refer to a column in a [[database]] or a field in a [[Data entry clerk|data entry]] form or web form.\n\nThe field may contain data to be entered as well as data to be displayed.  \n\n==See also== \n{{wiktionary|Data}}\n*[[Data dictionary]]\n*[[Data element]]\n*[[Data acquisition]]\n*[[Data hierarchy]]\n\n[[Category:Data management]]\n\n[[it:Campo (informatica)]]"]
['Category:Transaction processing', '11300280', '{{Cat main|Transaction processing}}\n\n[[Category:Concurrency control]]\n[[Category:Data management]]\n[[Category:Utility software by type]]\n[[Category:Databases]]']
['Category:Semantic desktop', '12251078', '{{Cat main|Semantic desktop}}\n\n[[Category:Knowledge representation]]\n[[Category:Data management]]']
['Tagsistant', '12989031', '{{POV|date=October 2012}}\n\n{{Infobox software\n| name                   = Tagsistant\n| logo                   = [[file:Tagsistant logo.png|300px]]\n| developer              = Tx0 <tx0@strumentiresistenti.org>\n| latest release version = 0.6\n| frequently_updated     = yes<!-- Release version update? Don\'t edit this page, just click on the version number! -->\n| programming language   = [[C (programming language)|C]]\n| operating system       = Linux kernel\n| language               = English\n| genre                  = [[Semantic file system]]\n| license                = [[GNU General Public License|GNU GPL]]\n| website                = http://www.tagsistant.net/\n}}\n{{Infobox filesystem\n| name = Tagsistant\n| developer = Tx0\n| full_name =\n| introduction_date =\n| introduction_os =\n| partition_id =\n| directory_struct =\n| file_struct =\n| bad_blocks_struct =\n| max_file_size =\n| max_files_no =\n| max_filename_size =\n| max_volume_size =\n| dates_recorded =\n| date_range =\n| date_resolution =\n| forks_streams =\n| attributes =\n| file_system_permissions =\n| compression =\n| encryption =\n| OS =\n}}\n\'\'\'Tagsistant\'\'\' is a [[semantic file system]] for the [[Linux kernel]], written in [[C (programming language)|C]] and based on [[Filesystem in Userspace|FUSE]]. Unlike traditional [[file systems]] that use hierarchies of directories to locate objects, Tagsistant introduces the concept of [[Tag (metadata)|tags]].\n\n==Design and differences with hierarchical file systems==\n\nIn computing, a [[file system]] is a type of data store which could be used to store, retrieve and update [[Computer file|files]]. Each file can be uniquely located by its [[Path (computing)|path]]. The user must know the path in advance to access a file and the path does not necessarily include any information about the content of the file.\n\nTagsistant uses a complementary approach based on [[Tag (metadata)|tags]]. The user can create a set of tags and apply those tags to files, [[File directory|directories]] and other objects ([[Device file|devices]], [[Named pipe|pipes]], ...). The user can then search all the objects that match a subset of tags, called a query. This kind of approach is well suited for managing user contents like pictures, audio recordings, movies and text documents but is incompatible with system files (like libraries, commands and configurations) where the univocity of the path is a [[Computer security|security]] requirement to prevent the access to a wrong content.\n\n==The tags/ directory==\n\nA Tagsistant file system features four main directories:\n\n:archive/\n:relations/\n:stats/\n:tags/\n\nTags are created as sub directories of the <code>tags/</code> directory and can be used in queries complying to this syntax:\n\n:<code>tags/subquery/[+/subquery/[+/subquery/]]/@/</code><ref>{{cite web|title=tags/ and relations/ directories|url=http://www.tagsistant.net/documents-about-tagsistant/0-6-howto?showall=&start=3}}</ref>\n\nwhere a subquery is an arbitrarily long list of tags, concatenated as directories:\n\n:<code>tag1/tag2/tag3/.../tagN/</code>\n\nThe portion of a path delimited by <code>tags/</code> and <code>@/</code> is the actual query. The <code>+/</code> operator joins the results of different sub-queries in one single list. The <code>@/</code> operator ends the query.\n\nTo be returned as a result of the following query:\n\n:<code>tags/t1/t2/+/t1/t4/@/</code>\n\nan object must be tagged as both <code>t1/</code> and <code>t2/</code> or as both <code>t1/</code> and <code>t4/</code>. Any object tagged as <code>t2/</code> or <code>t4/</code>, but not as <code>t1/</code> will not be retrieved.\n\nThe query syntax deliberately violates the [[POSIX]] file system semantics by allowing a path token to be a descendant of itself, like in <code>tags/t1/t2/+/t1/t4/@</code> where <code>t1/</code> appears twice. As a consequence a recursive scan of a Tagsistant file system  will exit with an error or endlessly loop, as done by [[UNIX]] <code>[[Find|find]]</code>:\n\n<syntaxhighlight lang="bash">\n~/tagsistant_mountpoint$ find tags/\ntags/\ntags/document\ntags/document/+\ntags/document/+/document\ntags/document/+/document/+\ntags/document/+/document/+/document\ntags/document/+/document/+/document/+\n[...]\n</syntaxhighlight>\n\nThis drawback is balanced by the possibility to list the tags inside a query in any order. The query <code>tags/t1/t2/@/</code> is completely equivalent to <code>tags/t2/t1/@/</code> and <code>tags/t1/+/t2/t3/@/</code> is equivalent to <code>tags/t2/t3/+/t1/@/</code>.\n\nThe <code>@/</code> element has the precise purpose of restoring the POSIX semantics: the path <code>tags/t1/@/directory/</code> refers to a traditional directory and a recursive scan of this path will properly perform.\n\n==The reasoner and the relations/ directory==\n\nTagsistant features a simple [[Semantic Reasoner|reasoner]] which expands the results of a query by including objects tagged with related tags. A relation between two tags can be established inside the <code>relations/</code> directory following a three level pattern:\n\n:<code>relations/tag1/rel/tag2/</code>\n\nThe <code>rel</code> element can be \'\'includes\'\' or \'\'is_equivalent\'\'. To include the \'\'rock\'\' tag in the \'\'music\'\' tag, the UNIX command <code>mkdir</code> can be used:\n\n:<code>mkdir -p relations/music/includes/rock</code>\n\nThe reasoner can recursively resolve relations, allowing the creation of complex structures:\n\n:<code>mkdir -p relations/music/includes/rock</code>\n:<code>mkdir -p relations/rock/includes/hard_rock</code>\n:<code>mkdir -p relations/rock/includes/grunge</code>\n:<code>mkdir -p relations/rock/includes/heavy_metal</code>\n:<code>mkdir -p relations/heavy_metal/includes/speed_metal</code>\n\nThe web of relations created inside the <code>relations/</code> directory constitutes a basic form of [[Ontology (information science)|ontology]].\n\n==Autotagging plugins==\n\nTagsistant features an autotagging plugin stack which gets called when a file or a symlink is written.<ref>{{cite web|title=How to write a plugin for Tagsistant?|url=http://www.tagsistant.net/documents-about-tagsistant/coding-and-debugging/7-how-to-write-a-plugin-for-tagsistant}}</ref> Each plugin is called if its declared [[Mime type|MIME type]] matches\n\nThe list of working plugins released with Tagsistant 0.6 is limited to:\n\n* text/html: tags the file with each word in <code><title></code> and <code><keywords></code> elements and with \'\'document\'\', \'\'webpage\'\' and \'\'html\'\' too\n* image/jpeg: tags the file with each [[Exchangeable image file format|Exif]] tag\n\n==The repository==\n\nEach Tagsistant file system has a corresponding repository containing an <code>archive/</code> directory where the objects are actually saved and a <code>tags.sql</code> file holding tagging information as an [[SQLite]] database. If the [[MySQL]] database engine was specified with the <code>--db</code> argument, the <code>tags.sql</code> file will be empty. Another file named <code>repository.ini</code> is a [[GLib]] ini store with the repository configuration.<ref>{{cite web|title=Key-value file parser|url=https://developer.gnome.org/glib/2.32/glib-Key-value-file-parser.html}}</ref>\n\nTagsistant 0.6 is compatible with the MySQL and Sqlite dialects of SQL for tag reasoning and tagging resolution. While porting its logic to other SQL dialects is possible, differences in basic constructs (especially the INTERSECT SQL keyword) must be considered.\n\n==The archive/ and stats/ directories==\n\nThe <code>archive/</code> directory has been introduced to provide a quick way to access objects without using tags. Objects are listed with their inode number prefixed.<ref>{{cite web|title=Tagsistant 0.6 howto - Inodes|url=http://www.tagsistant.net/documents-about-tagsistant/0-6-howto?showall=&start=6}}</ref>\n\nThe <code>stats/</code> directory features some read-only files containing usage statistics. A file <code>configuration</code> holds both compile time information and current repository configuration.\n\n==Main criticisms==\n\nIt has been highlighted that relying on an external database to store tags and tagging information could cause the complete loss of metadata if the database gets corrupted.<ref>{{cite web|title=Extended attributes and tag file systems|url=http://www.lesbonscomptes.com/pages/tagfs.html}}</ref>\n\nIt has been highlighted that using a flat namespace tends to overcrowd the <code>tags/</code> directory.<ref>{{cite web|title=The major problem with this approach is scalability|publisher=https://news.ycombinator.com/item?id=2573318}}</ref> This could be mitigated introducing [[Tag (metadata)#Triple tags|triple tags]].\n\n==See also==\n{{Portal|Free software}}\n[[Semantic file system]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Official website|http://www.tagsistant.net/}}\n* [https://aur.archlinux.org/packages.php?ID=54644 Arch Linux package]\n* [https://news.ycombinator.com/item?id=2573318 Discussion on Hacker News]\n* [http://www.lesbonscomptes.com/pages/tagfs.html Extended attributes and tag file systems]\n* [http://lakm.us/logit/2010/03/tagsistant-on-production-2/ Tagsistant On Production]\n\n[[Category:Computer file systems]]\n[[Category:Data management]]\n[[Category:Semantic file systems]]']
['Distributed concurrency control', '13329119', '{{POV|Commitment ordering|date=November 2011}}\n\'\'\'Distributed concurrency control\'\'\' is the [[concurrency control]] of a system [[Distributed computing|distributed]] over a [[computer network]] ([[#Bern87|Bernstein et al. 1987]], [[#Weikum01|Weikum and Vossen 2001]]). \n\nIn \'\'[[database systems]]\'\' and \'\'[[transaction processing]]\'\' (\'\'transaction management\'\') distributed concurrency control refers primarily to the concurrency control of a [[distributed database]]. It also refers to the concurrency control in a multidatabase (and other multi-[[transactional object]]) environment (e.g., [[federated database]], [[grid computing]], and [[cloud computing]] environments. A major goal for distributed concurrency control is distributed [[serializability]] (or [[global serializability]] for multidatabase systems). Distributed concurrency control poses special challenges beyond centralized one, primarily due to communication and computer [[latency (engineering)|latency]]. It often requires special techniques, like [[distributed lock manager]] over fast [[computer network]]s with low latency, like [[switched fabric]] (e.g., [[InfiniBand]]). [[commitment ordering]] (or commit ordering) is a general serializability technique that achieves distributed serializability (and global serializability in particular) effectively on a large scale, without concurrency control information distribution (e.g., local precedence relations, locks, timestamps, or tickets), and thus without performance penalties that are typical to other serializability techniques ([[#Raz92|Raz 1992]]).\n\nThe most common distributed concurrency control technique is \'\'strong strict two-phase locking\'\' ([[two phase locking#Strong strict two-phase locking|SS2PL]], also named \'\'rigorousness\'\'), which is also a common centralized concurrency control technique. SS2PL provides both the \'\'serializability\'\', \'\'[[schedule (computer science)#Strict|strictness]]\'\', and \'\'commitment ordering\'\' properties. Strictness, a special case of recoverability, is utilized for effective recovery from failure, and commitment ordering allows participating in a general solution for global serializability. For large-scale distribution and complex transactions, distributed locking\'s typical heavy performance penalty (due to delays, latency) can be saved by using the [[atomic commitment]] protocol, which is needed in a distributed database for (distributed) transactions\' [[Atomicity (database systems)|atomicity]] (e.g., [[Two-phase commit protocol|two-phase commit]], or a simpler one in a reliable system), together with some local commitment ordering variant (e.g., local [[two phase locking#Strong strict two-phase locking|SS2PL]]) instead of distributed locking, to achieve global serializability in the entire system. All the commitment ordering theoretical results are applicable whenever atomic commitment is utilized over partitioned, distributed recoverable (transactional) data, including automatic \'\'distributed deadlock\'\' resolution. Such technique can be utilized also for a large-scale [[parallel database]], where a single large database, residing on many nodes and using a distributed lock manager, is replaced with a (homogeneous) multidatabase, comprising many relatively small databases (loosely defined; any process that supports transactions over partitioned data and participates in atomic commitment complies), fitting each into a single node, and using commitment ordering (e.g., SS2PL, strict CO) together with some appropriate atomic commitment protocol (without using a distributed lock manager).\n\n==See also==\n*[[Global concurrency control]]\n\n==References==\n*<cite id=Bern87>[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  \'\'Concurrency Control and Recovery in Database Systems\'\'], Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5 </cite>\n*<cite id=Weikum01>[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  \'\'Transactional Information Systems\'\'], Elsevier, ISBN 1-55860-508-8 </cite>\n*<cite id=Raz92>[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html  "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment."]  \'\'Proceedings of the Eighteenth International Conference on Very Large Data Bases\'\' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) </cite>\n\n[[Category:Data management]]\n[[Category:Distributed computing problems]]\n[[Category:Databases]]\n[[Category:Concurrency control]]\n[[Category:Transaction processing]]']
['Category:Data synchronization', '7645825', '[[Category:Synchronization]]\n[[Category:Data management|Synchronization]]\n[[Category:Distributed computing problems]]\n[[Category:Fault-tolerant computer systems]]\n[[Category:Data quality]]']
['Category:Computer-aided software engineering tools', '15189720', '{{Cat main|Computer-aided software engineering}}\n\n[[Category:Data management]]\n[[Category:Computer programming tools]]']
['Operational database', '14190258', '{{more footnotes|date=March 2013}}\n\nOperational database management systems (also referred to as [[OLTP]] On Line Transaction Processing databases), are used to manage dynamic data in real-time. These types of databases allow you to do more than simply view archived data. Operational databases allow you to modify that data (add, change or delete data), doing it in [[Real-time computing|real-time]].\n\nSince the early 90\'s, the operational database software market has been largely taken over by [[SQL]] engines. Today, the operational [[DBMS]] market (formerly [[OLTP]]) is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and [[NoSQL]] DBMS engines, as well as [[XML database]]s and [[NewSQL|NewSQL databases]]. Operational databases are increasingly supporting [[distributed database]] architecture that provides [[high availability]] and [[fault tolerance]] through [[replication (computing)|replication]] and scale out ability.\n\nRecognizing the growing role of operational databases in the IT industry that is fast moving from legacy databases to real-time operational databases capable to handle distributed web and mobile demand and to address [[Big data]] challenges, in October 2013 [[Gartner]] started to publish the [[Magic Quadrant]] for Operational Database Management Systems.<ref name="Gartner Magic Quadrant for Operational Database Management Systems">{{cite web|url=https://www.gartner.com/doc/2610218/magic-quadrant-operational-database-management |title=Gartner Magic Quadrant for Operational Database Management Systems|publisher=Gartner.com}}</ref>\n\n== List of Operational Databases ==\n\n{| style="text-align: left;" class="wikitable sortable"\n|-\n! Database platform !! Database model !! [[SQL]] Support !! [[NoSQL]] Support !! Managed objects !! ACID-transactions\n|-\n| [[Aerospike database|Aerospike]] || Key–Value Store ||  No || \'\'\'Yes\'\'\' || key-value pairs || None\n|-\n| [[Altibase]] || Relational database || \'\'\'Yes\'\'\' || NO || tabular data || Real-time ACID transactions\n|-\n| [[Apache Cassandra]] || Key-value store || No || \'\'\'Yes\'\'\' || key-value pairs || None\n|-\n| [[Cloudant]] || Document-Oriented Database || No || \'\'\'Yes\'\'\' || JSON || None\n|-\n| [[Clusterpoint]] || Document-Oriented Database || \'\'\'Yes\'\'\' (essential SQL)  || \'\'\'Yes\'\'\' || XML, JSON, text data || Distributed ACID-transactions \n|-\n| [[Clustrix]] || Relational Database || \'\'\'Yes\'\'\' (newSQL) || No || tabular data || ACID-transactions \n|-\n| [[Couchbase]] || Document-Oriented Database || \'\'\'Yes\'\'\' (N1QL) || \'\'\'Yes\'\'\' || JSON || None\n|-\n| [[CouchDB]] || Document-Oriented Database || No || \'\'\'Yes\'\'\' || JSON || None \n|-\n| [[EnterpriseDB]] || Relational Database || \'\'\'Yes\'\'\' || No || tabular data || ACID-transactions \n|-\n| [[FoundationDB]] || Key-value store || \'\'\'Yes\'\'\' || No || key-value pairs || ACID-transactions \n|-\n| [[IBM DB2]] || Relational Database || \'\'\'Yes\'\'\' || No || tabular data || ACID-transactions \n|-\n| [[Ingres_(database)|Ingres]] || Relational Database || \'\'\'Yes\'\'\' || No || tabular data || ACID-transactions \n|-\n| [[MarkLogic]] || Document-Oriented Database || No || \'\'\'Yes\'\'\' (XQuery) || XML || ACID-transactions\n|-\n| [[Microsoft SQL Server]] || Relational Database || \'\'\'Yes\'\'\' || No || tabular data || ACID-transactions\n|-\n| [[MongoDB]] || Document-Oriented Database || No || \'\'\'Yes\'\'\' || BSON || None\n|-\n| [[NuoDB]] || Relational Database || \'\'\'Yes\'\'\' (newSQL) || No || tabular data || ACID-compliant\n|-\n| [[Oracle Database|Oracle]] || Relational Database || \'\'\'Yes\'\'\' || No || tabular data || ACID-transactions\n|-\n| [[OrientDB]] || Document-oriented Database || \'\'\'Yes\'\'\' || Yes || key-value pairs || ACID-transactions<ref>http://orientdb.com/docs/last/Transactions.html</ref>\n|-\n| [[Riak]] || Key-value store || No || \'\'\'Yes\'\'\' || key-value pairs || None\n|-\n| [[SAP HANA]] || Relational Database || \'\'\'Yes\'\'\' || No || tabular data || ACID-transactions \n|-\n| [[VoltDB]] || Relational Database || \'\'\'Yes\'\'\' (newSQL) || No || tabular data || ACID-transactions\n\n|}\n\n== Use in business ==\n\nOperational databases are used to store, manage and track real-time business information. For example, a company might have an operational database used to track warehouse/stock quantities. As customers order products from an online web store, an operational database can be used to keep track of how many items have been sold and when the company will need to reorder stock.  An \'\'\'operational database\'\'\' stores information about the activities of an [[organization]], for example [[customer relationship management]] transactions or financial operations, in a computer [[database]].\n\nOperational databases allow a business to enter, gather, and retrieve large quantities of specific information, such as company legal data, financial data, call data records, personal employee information, sales data, customer data, data on assets and many other information.  An important feature of storing information in an operational database is the ability to share information across the company and over the Internet.  Operational databases can be used to manage mission-critical business data, to monitor activities, to audit suspicious transactions, or to review the history of dealings with a particular customer.  They can also be part of the actual process of making and fulfilling a purchase, for example in [[e-commerce]].\n\n==Data warehouse terminology==\n\nIn [[Data warehouse|data warehousing]], the term is even more specific: the operational database is the one which is accessed by an [[operational system]] (for example a customer-facing website or the application used by the customer service department) to carry out regular operations of an organization. Operational databases usually use an [[online transaction processing]] database which is optimized for faster transaction processing ([[create, read, update and delete]] operations).\n\n== See also ==\n* [[Document database|Document-oriented databases]]\n* [[NewSQL|NewSQL databases]]\n* [[NoSQL|NoSQL databases]]\n* [[XML|XML databases]]\n* [[SQL|SQL databases]]\n* [[Distributed database]]s\n\n== References ==\n{{Reflist|33em}}\n* O’Brien, Jason., and Marakas, Gorila., (2008).  Management Information Technology Systems.  Computer Software (pp.&nbsp;185). New York, New York:  McGraw-Hill\n\n[[Category:Data warehousing]]\n[[Category:Data management]]\n[[Category:Information technology management]]\n[[Category:Business intelligence]]\n[[Category:Types of databases]]']
['Holistic Data Management', '17377283', "'''Holistic Data Management''' (HDM) framework is AHISDATA indigenous standard for implementing software implementations within an organization network. This framework extends the existing data management solutions such as [[data quality]], [[data governance]], [[data integration]], [[data processing]], [[master data management]] and [[data validation]] solutions.\n\nThe HDM framework specifies that:\n*All data objects must exist as a child data object or a parent data object.\n*Only one unique parent data object must exist within a data network scope (DNS).\n*All child data objects must have a data-mapping link defined within a data network scope.\n*A data object relationship must exist at least in one of the following four data management modules:\n''Data mapping'', ''data validation'', ''[[data integration]]'',''[[data processing]]''\n\n== HDM framework ==\nThe following entities are specified in the HDM framework.\n\n*''Data network scope (DNS)''\nThe data network scope (DNS) is the logical boundary that a software application database system of record (SOR) exists within an enterprise network. There can be multiple DNS within an enterprise network.\n\n*''Data network domain (DND)''\nThe data network domain (DND) is the logical boundary representing a collection of multiple data network scope (DNS). There can be multiple DND within an enterprise network.\n\n*''System of record (SOR)''\nA system of record applies to the master or principal database system that a parent data objects resides on.\nThere can only be one SOR within a data network scope.\n\n*''Parent data object (PDO)''\nA parent data object (PDO) is the system of record schema object name. Only one unique parent data object must exist within a data network scope.\n\n*''Child data object (CDO)''\nA child data object (CDO) is a schema object name that derives its data from one or more parent data object(s).\n\n*''Data-mapping link (DML)''\nA data-mapping link (DML) is the data requirement specification applied to the relationship between multiple database schema objects where one data object derives its data from one or more data objects. DML is only applicable for a data-mapping data management module.\n\n*''Data–object relationship (DOR)''\nThe data–object relationship (DOR) is the data requirement, business rule, program function that applies to one or multiple data objects. DOR can be applied on data-mapping links for each data management modules. Only one DOR can exist on a DML within a data management module.\n\n*''Data management modules (DMM)''\nData management modules are the common user interface (UI) programs that defines and manage the data object relationship(s) within a data network scope.\n\nThere are four data management modules:\n\n''Data mapping'' – This is the base data management user interface module. The data-mapping module provide the functionalities for managing data-mapping links and data object relationships for all database schemas within a data network scope. A data network scope must have at least one data-mapping design defined.\n\n''Data validation'' – This user interface module provides the functionalities for defining and managing validation events on data object relationships. Validation events include auditing, reporting, scheduler, logger, triggers and DNS health check. Data validation events requires a data-mapping design defined within a data network scope.\n\n''[[Data integration]]'' – This user interface module provides the functionalities for defining and managing interface configurations on data object relationships. The interface configurations include scheduler, transmission mode, listener, interface API and reporting. The interface APIs would allow third-party systems to transfer data using the data object relationship defined within a data network scope. Data Integration interface configuration requires a Data Mapping design defined within a data network scope.\n\n''[[Data processing]]'' – This user interface module provides the functionalities for defining and managing interface configurations and batch runtime engines on data object relationships. The interface configurations include scheduler, transmission mode, multi-batch transmission, user-defined DOR API and reporting. Data Processing interface configuration requires a data-mapping design defined within a data network scope.\n\n== Implementing the HDM framework ==\nThe HDM framework presents a standard for software implementations within an organization. The objective is to shed visibility, increase efficiency and centralized management of all other software implementations within an organization.\n\nThe HDM framework should be implemented as a major organization project that is supervised by the project management office. This would require a project charter developed and a project manager assigned for managing the implementation process. There are several phases involve in implementing the HDM framework:\n\n*''Choose a data management module (DMM)'' – This exercise requires the acquisition of a data management module software application to be used for implementing the rest of the HDM framework. AHISDATA iNTEGRITY software is an integrated solution that provides DMM functionalities.\n*''Scrub (inventory of existing applications and data sources)'' – This exercise identifies all applications within an organization and the data sources that they are connected to.\n*''Formation (applications and data schema relation)'' – This exercise is to align all applications in relation to the data schemas within the data sources. The applications are grouped in the order of the data schemas that they access.\n*''First axe (applications eligible for decommission)'' – This exercise is to identify all applications that are rogue, obsolete and completely redundant. These applications are eligible for removal.\n*''Second axe (application eligible for consolidation)'' – This exercise is to identify all applications that have some functional similarities and some uniqueness in the data requirement. These applications are eligible for consolidation. The functionalities that are similar are left intact on one application and turned off or disabled on the other(s).\n*''Define data network domain (DND)'' – This exercise is to define the data network domain for all the approved applications within the enterprise network.\n*''Define Data Network Scope (DNS)'' – This exercise is to define the data network scope(s) required for each DND.\n*''Define system of records (SOR)'' – This exercise is to define the SOR for each DNS.\n*''Define parent data objects (PDO)'' – This exercise is to define all PDOs in each DNS.\n*''Define child data objects (CDO)'' – This exercise is to define all CDOs in each DNS.\n*''Define data mapping links (DML)'' – This exercise is to define all data-mapping links and object relationship in all DNS.\n*''Define data object relationships (DOR)'' – This exercise is to define the DOR requirement for each data management module implemented.\n\n==See also==\n*[[Reference data]]\n*[[Master data]]\n*[[Customer data integration]]\n*[[Product information management]]\n*[[Identity resolution]]\n\n==External links==\n* [http://www.ahisdata.com/eHDMS/AHISDATA_HDM_WhitePaper_v35.pdf AHISDATA HDM WhitePaper]\n* [http://msdn2.microsoft.com/en-us/library/bb190163.aspx#mdm04_topic4 The What, Why, and How of Master Data Management]\n\n{{Data warehouse}}\n\n[[Category:Business intelligence]]\n[[Category:Data management]]\n[[Category:Data warehousing products]]\n[[Category:Information technology management]]"]
['Vector-field consistency', '18477184', '\'\'\'Vector-Field Consistency\'\'\'<ref group="nb"><sub>Designation coined by L. Veiga.</sub></ref> is a [[consistency model]] for replicated data (for example, objects), initially described in a paper<ref>{{cite conference |author1=Nuno Santos |author2=Luís Veiga |author3=Paulo Ferreira | year=2007 | title=Vector-Field Consistency for Adhoc Gaming| booktitle = ACM/IFIP/Usenix Middleware Conference 2007 | url=http://www.gsd.inesc-id.pt/~pjpf/middleware07vector.pdf | format=PDF}}</ref> which was awarded the best-paper prize in the ACM/IFIP/Usenix Middleware Conference 2007. It has since been enhanced for increased scalability and fault-tolerance in a recent paper.<ref>{{cite conference |author1=Luís Veiga |author2=André Negrão |author3=Nuno Santos |author4=Paulo Ferreira | year=2010 | title=Unifying Divergence Bounding and Locality Awareness in Replicated Systems with Vector-Field Consistency \n| booktitle = JISA, Journal of Internet Services and Applications, Volume 1, Number 2, 95-115, Springer, 2010 | url=http://www.gsd.inesc-id.pt/~lveiga/vfc-JISA-2010.pdf | format=PDF}}</ref>\n\n== Description ==\nThis consistency model was initially designed for replicated [[data management]] in adhoc gaming in order to minimize bandwidth usage without sacrificing playability. Intuitively, it captures the notion that although players require, wish, and take advantage of information regarding the whole of the game world (as opposed to a restricted view to rooms, arenas, etc. of limited size employed in many [[multiplayer game]]s), they need to know information with greater freshness, frequency, and accuracy as other game entities are located closer and closer to the player\'s position.\n\nIt prescribes a multidimensional divergence bounding scheme, based on a [[vector field]] that employs consistency vectors k=(θ,σ,ν), standing for maximum allowed \'\'\'t\'\'\'ime - or replica staleness, \'\'\'s\'\'\'equence - or missing updates, and \'\'\'v\'\'\'alue<ref group="nb"><sub>Since in the [[Greek alphabet]] there was no letter for the \'\'vee\'\' sound, the \'\'nu\'\' letter was preferred for its resemblance with the roman V, for \'\'v\'\'alue, instead of β (\'\'beta\'\') for the \'\'vee\'\' sound in contemporary Greek speaking.</sub></ref> - or user-defined measured replica divergence, applied to all space coordinates in game scenario or world.\n\nThe consistency vector-fields emanate from field-generators designated as pivots (for example, players) and [[Field strength|field intensity]] attenuates as distance grows from these pivots in concentric or square-like regions. This consistency model unifies locality-awareness techniques employed in message routing and consistency enforcement for multiplayer games, with divergence bounding techniques traditionally employed in replicated database and web scenarios.\n\n== Notes ==\n<references group="nb"/>\n\n== References ==\n<references/>\n\n[[Category:Data management]]']
['Data exchange', '10231058', '{{more footnotes|date=February 2014}}\n\'\'\'Data exchange\'\'\' is the process of taking [[data]] structured under a \'\'source\'\' [[Database schema|schema]] and transforming it into data structured under a \'\'target\'\' schema, so that the target data is an accurate representation of the source data.<ref>A. Doan, A. Halevy, and Z. Ives. "Principles of data integration", Morgan Kaufmann, 2012 pp. 276</ref> Data exchange allows data to be [[cross-platform|shared between]] different [[computer program]]s. It is similar to the related concept of [[data integration]] except that data is actually restructured (with possible loss of content) in data exchange. There may be no way to transform an [[Instance (computer science)|instance]] given all of the constraints. Conversely, there may be numerous ways to transform the instance (possibly infinitely many), in which case a "best" choice of solutions has to be identified and justified.\n\n== Single-domain data exchange ==\n\nOften there are a few dozen different source and target schema (proprietary data formats) in some specific domain.\nOften people develop an \'\'\'exchange format\'\'\' or \'\'\'interchange format\'\'\' for some single domain, and then write a few dozen different routines to (indirectly) translate each and every source schema to each and every target schema by using the interchange format as an intermediate step.\nThat requires a lot less work than writing and debugging the hundreds of different routines that would be required to directly translate each and every source schema directly to each and every target schema.\n(For example,\n[[Standard Interchange Format]] for geospatial data,\n[[Data Interchange Format]] for spreadsheet data,\n[[GPS eXchange Format]] or [[Keyhole Markup Language]] for indicating GPS coordinates on the globe,\n[[Quicken Interchange Format]] for financial data,\n[[GDSII]] for integrated circuit layout,\netc.){{Citation needed|date=September 2016}}\n\n== Data exchange languages == <!-- redirect target, if you change this, fix the redirect, too! -->\n{{merge to|Modeling language|date=May 2016}}\nA data exchange language{{citation needed|date=May 2016}} is a language that is domain-independent and can be used for any kind of data. Its semantic expression capabilities and qualities are largely determined by comparison with the capabilities of natural languages. The term is also applied to any [[file format]] that can be read by more than one program, including proprietary formats such as [[Microsoft Office]] documents. However, a file format is not a real language as it lacks a grammar and vocabulary.\n\nPractice has shown that certain types of [[formal language]]s are better suited for this task than others, since their specification is driven by a formal process instead of a particular software implementation needs. For example, [[XML]] is a [[markup language]] that was designed to enable the creation of dialects (the definition of domain-specific sublanguages) and a popular choice now in particular on the internet. However, it does not  contain domain specific dictionaries or fact types. Beneficial to a reliable data exchange is the availability of standard dictionaries-taxonomies and tools libraries such as [[parser]]s, schema [[validator]]s and transformation tools.{{Citation needed|date=September 2016}}\n\n=== Popular languages used for data exchange ===\nThe following is a partial list of popular generic languages used for data exchange in multiple domains.\n\n<!-- this currently is very rough and ad-hoc - feel free to extend and change it! -->\n<!-- Please verify definitions for the column headers of the table! -->\n{| class="wikitable sortable" style="font-size: 85%; text-align: center; width: auto;"\n!\n! Schemas\n! Flexible\n! Semantic verification\n! Dictionary\n! Information Model\n! Synonyms and homonyms\n! Dialecting\n! Web standard\n! Transformations\n! Lightweight\n! Human readable\n! Compatibility\n|-\n| {{rh}}|[[Resource Description Framework|RDF]]\n| {{yes}}{{Ref label|feat-rdf|1}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{partial}}\n| Subset of [[Semantic web]]\n|-\n| {{rh}}|[[XML]]\n| {{yes}}{{Ref label|feat-schema|1}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| subset of [[SGML]], [[HTML]]\n|-\n| {{rh}}|[[Atom (file format)|Atom]]\n| {{yes}}\n| {{unk}}\n| {{unk}}\n| {{unk}}\n| {{no}}\n| {{unk}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| [[XML]] dialect\n|-\n| {{rh}}|[[JSON]]\n| {{no}}\n| {{unk}}\n| {{unk}}\n| {{unk}}\n| {{no}}\n| {{unk}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| subset of [[YAML]]\n|-\n| {{rh}}|[[YAML]]\n| {{no}}{{Ref label|feat-ext|2}}\n| {{unk}}\n| {{unk}}\n| {{unk}}\n| {{no}}\n| {{unk}}\n| {{no}}\n| {{no}}\n| {{no}}{{Ref label|feat-ext|2}}\n| {{yes}}\n| {{yes}}{{Ref label|feat-yaml-readable|3}}\n| superset of [[JSON]]\n|-\n| {{rh}}|[[REBOL]]\n| {{yes}}{{Ref label|feat-rebol-parse|6}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}{{Ref label|feat-rebol-parse|6}}\n| {{yes}}\n| {{yes}}{{Ref label|feat-rebol-readable|4}}\n| \n|-\n| {{rh}}|[[Gellish]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}{{Ref label|feat-gellish-dict|7}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| ISO\n| {{no}}\n| {{yes}}\n| {{partial}}{{Ref label|feat-gellish-readable|5}}\n| SQL, RDF/XML, OWL\n|}\n\n\'\'\'Nomenclature\'\'\'\n* Schemas - Whether the language definition is available in a computer interpretable form.\n* Flexible - Whether the language enables extension of the semantic expression capabilities without modifying the schema.\n* Semantic verification - Whether the language definition enables semantic verification of the correctness of expressions in the language.\n* Dictionary-Taxonomy - Whether the language includes a dictionary and a taxonomy (subtype-supertype hierarchy) of concepts with inheritance.\n* Synonyms and homonyms - Whether the language includes and supports the use of synonyms and homonyms in the expressions.\n* Dialecting - Whether the language definition is available in multiple natural languages or dialects.\n* Web or ISO standard - Organization that endorsed the language as a standard.\n* Transformations - Whether the language includes a translation to other standards.\n* Lightweight - Whether a lightweight version is available, in addition to a full version.\n* Human readable - Whether expressions in the language are [[human-readable]]—readable by humans without training.{{Citation needed|date=September 2016}}\n* Compatibility - Which other tools are possible or required when using the language.{{Citation needed|date=September 2016}}\n\n\'\'\'Notes:\'\'\'\n\n# {{note|feat-rdf}} RDF is a schema flexible language.\n# {{note|feat-schema}} The schema of XML contains a very limited grammar and vocabulary.\n# {{note|feat-ext}} Available as extension.\n# {{note|feat-yaml-readable}} in the default format, not the compact syntax.\n# {{note|feat-rebol-readable}} the syntax is fairly simple (the language was designed to be human readable); the dialects may require domain knowledge.\n# {{note|feat-gellish-readable}} the standardized fact types are denoted by standardized English phrases, which interpretation and use needs some training.\n# {{note|feat-rebol-parse}} the [[REBOL#parse|Parse dialect]] is used to specify, validate, and transform dialects.\n# {{note|feat-gellish-dict}} the English version includes a Gellish English Dictionary-Taxonomy that also includes standardized fact types (= kinds of relations).\n\n=== XML for data exchange ===\nThe popularity of [[XML]] for data exchange on the [[World Wide Web]] has several reasons. First of all, it is closely related to the preexisting standards [[Standard Generalized Markup Language]] (SGML) and [[Hypertext Markup Language]] (HTML), and as such a parser written to support these two languages can be easily extended to support XML as well. For example, [[XHTML]] has been defined as a format that is formal XML, but understood correctly by most (if not all) HTML parsers. This led to quick adoption of XML support in web browsers and the toolchains used for generating web pages.{{Citation needed|date=September 2016}}\n\n=== YAML for data exchange ===\n[[YAML]] is a language that was designed to be human-readable (and as such to be easy to edit with any standard text editor). Its notion often is similar to [[reStructuredText]] or a Wiki syntax, who also try to be readable both by humans and computers. YAML 1.2 also includes a shorthand notion that is compatible with JSON, and as such any JSON document is also valid YAML; this however does not hold the other way.{{Citation needed|date=September 2016}}\n\n=== REBOL for data exchange ===\n\n[[REBOL]] is a language that was designed to be human-readable and easy to edit using any standard text editor. To achieve that it uses a simple free-form syntax with minimal punctuation, and a rich set of datatypes. REBOL datatypes like URLs, e-mails, date and time values, tuples, strings, tags, etc. respect the common standards. REBOL is designed to not need any additional meta-language, being designed in a metacircular fashion. The metacircularity of the language is the reason why e.g. the Parse dialect used (not exclusively) for definitions and transformations of REBOL dialects is also itself a dialect of REBOL. REBOL was used as a source of inspiration by the designer of JSON.{{Citation needed|date=September 2016}}\n\n=== Gellish for data exchange ===\n[[Gellish English]] is a formalized subset of natural English, which includes a simple grammar and a large extensible [[English dictionary|English Dictionary-Taxonomy]] that defines the general and domain specific terminology (terms for concepts), whereas the concepts are arranged in a subtype-supertype hierarchy (a Taxonomy), which supports inheritance of knowledge and requirements. The Dictionary-Taxonomy also includes standardized fact types (also called relation types). The terms and relation types together can be used to create and interpret expressions of facts, knowledge, requirements and other information. Gellish can be used in combination with [[SQL]], [[RDF/XML]], [[Web Ontology Language|OWL]] and various other meta-languages. The Gellish standard is being adopted as ISO 15926-11.{{Citation needed|date=September 2016}}\n\n== See also ==\n* [[Atom (file format)]]\n* [[Lightweight markup language]]\n* [[RSS]]\n\n== References ==\n\n{{reflist}}\n\n{{refbegin}}\n\n*R. Fagin, P. Kolaitis, R. Miller, and L. Popa. "Data exchange: semantics and query answering." Theoretical Computer Science, 336(1):89–124, 2005.\n*P. Kolaitis. "Schema mappings, data exchange, and metadata management." Proceedings of the twenty- fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 61–75, 2005\n{{refend}}\n\n{{Data Exchange}}\n\n[[Category:Data management]]']
['Locks with ordered sharing', '21064035', "In [[databases]] and [[transaction processing]] the term '''Locks with ordered sharing''' comprises several variants of the ''[[Two phase locking]]'' (2PL) [[concurrency control]] protocol generated by changing the blocking semantics of locks upon [[Serializability#View and conflict serializability|conflicts]]. One variant is identical to [[Commitment ordering#Strict CO (SCO)|Strict commitment ordering (SCO)]].\n\n==References==\n\n*D. Agrawal, A. El Abbadi, A. E. Lang: [http://portal.acm.org/citation.cfm?id=627615   ''The Performance of Protocols Based on Locks with Ordered Sharing''], IEEE Transactions on Knowledge and Data Engineering, Volume 6, Issue 5, October 1994, PP. 805 – 818, {{ISSN|1041-4347}}\n\n[[Category:Data management]]\n[[Category:Databases]]\n[[Category:Concurrency control]]\n[[Category:Transaction processing]]\n\n\n{{database-stub}}"]
['Control flow diagram', '21084005', '{{this|flow diagrams in business process modeling{{clarify|reason=From the lead, I\'m unable to give a more informative characterization of \'Control flow diagram\'.|date=January 2014}}|directed graphs representing the control flow of imperative computer programs|control flow graph}}\n[[File:Performance seeking control flow diagram.jpg|thumb|240px|Example of a "performance seeking" control flow diagram.<ref name="GO92"> Glenn B. Gilyard and John S. Orme (1992) [http://www.nasa.gov/centers/dryden/pdf/88262main_H-1808.pdf \'\'Subsonic Flight Test Evaluationof a Performance Seeking ControlAlgorithm on an F-15 Airplane\'\'] NASA Technical Memorandum 4400.</ref>]]\nA \'\'\'control flow diagram\'\'\' (\'\'\'CFD\'\'\') is a [[diagram]] to describe the [[control flow]] of a [[business process]], [[process (engineering)|process]] or review\n\nControl flow diagrams were developed in the 1950s, and are widely used in multiple [[engineering]] disciplines. They are one of the classic [[business process modeling]] methodologies, along with [[flow chart]]s, [[data flow diagram]]s, [[functional flow block diagram]], [[Gantt chart]]s, [[PERT]] diagrams, and [[IDEF]].<ref name="TD03"> Thomas Dufresne & James Martin (2003). [http://mason.gmu.edu/~tdufresn/paper.doc "Process Modeling for E-Business"]. INFS 770 Methods for Information Systems Engineering:  Knowledge Management and E-Business. Spring 2003</ref>\n\n== Overview ==\nA control flow diagram can consist of a subdivision to show sequential steps, with if-then-else conditions, repetition, and/or case conditions. Suitably annotated geometrical figures are used to represent operations, data, or equipment, and arrows are used to indicate the sequential flow from one to another.<ref>[http://www.fda.gov/ora/Inspect_ref/igs/gloss.html FDA glossary of terminology applicable to software development and computerized systems]. Accessed 14 Jan 2008.</ref>\n\nThere are several types of control flow diagrams, for example:\n* Change control flow diagram, used in [[project management]]\n* Configuration decision control flow diagram, used in [[configuration management]]\n* [[Process control]] flow diagram, used in [[process management]]  \n* Quality control flow diagram, used in [[quality control]].\n\nIn software and systems development control flow diagrams can be used in [[control flow analysis]], [[data flow analysis]], [[algorithm analysis]], and [[simulation]]. Control and data are most applicable for real time and data driven systems. These flow analyses transform logic and data requirements text into graphic flows which are easier to analyze than the text. PERT, state transition, and transaction diagrams are examples of control flow diagrams.<ref>Dolores R. Wallace et al. (1996). [http://hissa.nist.gov/HHRFdata/Artifacts/ITLdoc/234/val-proc.html \'\'Reference Information for the Software Verification and Validation Process\'\'], NIST Special Publication 500-234.</ref>\n\n== Types of Control Flow Diagrams ==\n=== Process Control Flow Diagram ===\nA flow diagram can be developed for the process [[control system]] for each critical activity. Process control is normally a closed cycle in which a [[sensor]] provides information to a process control [[software application]] through a [[communications system]]. The application determines if the sensor information is within the predetermined (or calculated) data parameters and constraints. The results of this comparison are fed to an actuator, which controls the critical component. This [[feedback]] may control the component electronically or may indicate the need for a manual action.<ref name="NIoJ02"> National Institute of Justice (2002). [http://www.ncjrs.gov/txtfiles1/nij/195171.txt \'\' A Method to Assess the Vulnerability of U.S. Chemical Facilities]\'\'. Series: Special Report.</ref> \n\nThis closed-cycle process has many checks and balances to ensure that it stays safe. The investigation of how the process control can be subverted is likely to be extensive because all or part of the process control may be\noral instructions to an individual monitoring the process. It may be fully computer controlled and automated, or it may be a hybrid in which only the sensor is automated and the action requires manual intervention. Further, some process control systems may use prior generations of hardware and software, while others are state of the art.<ref name="NIoJ02"/>\n\n=== Performance seeking control flow diagram ===\nThe figure presents an example of a performance seeking control [[flow diagram]] of the algorithm. The control law consists of estimation, modeling, and optimization processes. In the [[Kalman filter]] estimator, the inputs, outputs, and residuals were recorded. At the compact propulsion system modeling stage, all the estimated inlet and engine parameters were recorded.<ref name="GO92"/>  \n\nIn addition to temperatures, pressures, and control positions, such estimated parameters as stall margins, thrust, and drag components were recorded. In the optimization phase, the operating condition constraints, optimal solution, and linear programming health status condition codes were recorded. Finally, the actual commands that were sent to the engine through the DEEC were recorded.<ref name="GO92"/>\n\n== See also ==\n* [[Data flow diagram]]\n* [[Control flow graph]]\n* [[DRAKON]]\n* [[Flow process chart]]\n\n== References ==\n{{NIST-PD}}\n{{reflist}}\n\n[[Category:Information systems]]\n[[Category:Data management]]\n[[Category:Diagrams]]\n[[Category:Systems analysis]]\n{{DEFAULTSORT:Control Flow Diagram}}']
['Semantic warehousing', '20420236', "{{Multiple issues|\n{{unreferenced|date=November 2008}}\n{{confusing|date=August 2009}}\n{{cleanup-rewrite|date=August 2009}}\n}}\n\nIn [[data management]],  '''semantic warehousing''' is a methodology of digitalized text data using similar functions to [[Data warehousing]] (DW), such as ETL([[Extract, transform, load]]), ODS([[Operational data store]]), and MODEL. [[Value (computer science)|Key value]] operation is less useful for the digitalized text. Semantic warehousing is different from DW in that semantic information base from text(semantic) data.\n\nSemantic warehousing is different from search engine in that semantic information base from text data is stored in the database.([[DBMS]])\n\nThough data is most important word in computing era, it can not explain human knowledge well yet.\nData(numeric data) is key element of computing systems for certain organization (especially companies, enterprises), but no performance oriented organization needs something to gather and use knowledge or human feeling.\nSemantic warehousing will be equally or more important than data warehousing in the future.\n\n==Definition==\nSemantic warehousing is a conceptual and functional term meaning to gather from a source, semantically defining and providing information from digitalized text type data.\n\n==Background==\n\nData warehousing (DW) is popular these days. Gathering data from systems that generate transactions, data warehouses become a base of [[information]]. Key of data warehouse is a model (called [[datamart]]) and that model is made up of dimensions(key) and measures(value). Users get information from the models by doing certain operations. [[Online analytical processing]] (OLAP) is most the important operation for the users to get information from the DW models. Handling dimensions with pivoting, drilling, slice & dice operations users get numeric values like sales amounts, growth rates, etc.\nVarious areas of this world defined and appeared on the World Wide Web(Internet), eager to present their contents in a semantic way. \nBriefly speaking semantic warehousing has datawarehousing boby and search head and ontology features.\n\nData warehousing contributed to companies' business values and lots of solutions and tools are commercially successful. Analysis of internal data delivers a certain level of business values, on the contrary to this Semantic warehousing environment has not yet matured. Capacity of social data is increasing rapidly and various efforts of finding value from that data are made widely known as Big data, etc. Semantic warehousing can be the mainstream of treat data and intelligence of social world in the future though it is defined with other keywords.\n\nAt the Big data era, semantic processing is going to become major IT process. Semantic warehousing is digital infra of Intelligence.\n\n== Practices ==\n\n'''▣ Medical area (Clinical Information)'''\n\nSome hospital implement semantic warehousing for [[clinic]]al information (SWCI). Medical information is now knowledge network level. [[UMLS]] define semantic knowledge network of medical language. Currently medical information stored in database and not fully used for clinic. Semantic warehousing is next stage of digitalized medical information.\n\nSWCI is a name of conceptual system of clinical information.<br />\nNamed by Juhan Kim (SNUH, [[Seoul National University Hospital]]) and Bohyon Hwang, YongChan Keum in 2008.\n\nDefined architecture on SWCI ;<br />\n1. Semantic-oriented cleansing<br />\n2. Semantic-oriented meta management<br />\n3. Clinical(Medical) knowledge basement<br />\n4. Semantic-oriented user intelligence\n\n'''▣ Intelligence Area'''\n\nAt the point of Big data usage, intelligence reporting can be valuable results.\n\n1. Source information<br />\n2. Manage intelligence & Semantic data<br />\n3. Intelligence service & use\n\nhttp://www.globalintelligence.kr/gibigdata/\n\n== Connected area ==\n\n- [[Big data]] <br />\n- [[Semantic web]] <br />\n- [[Ontology]] <br />\n- [[Knowledge]] <br />\n- [[Medical]] and [[healthcare]] : EMR [[Electronic medical record|(Electronic Medical Record)]], EHR [[Electronic health record|(Electronic Health Record)]]<br />\n- [[Data warehouse]] <br />\n- AI ([[artificial intelligence]])\n\n== References ==\n*[http://www.snubi.org/ '''BI''' Laboratory of Seoul National University Hospital]\n*Smith, Barry Kumar, Anand and Schulze-Kremer, Steffen (2004) [http://ontology.buffalo.edu/medo/UMLS_SN.pdf Revising the UMLS Semantic Network], in M. Fieschi, et al. (eds.), Medinfo 2004, Amsterdam: IOS Press, 1700.\n* Foundations of Data Warehouse Quality :\n  Data Quality article mentioning that semantically rich DW.\n  http://www.cs.brown.edu/courses/cs227/Papers/Projects/iq97_dwq.pdf\n* An Integrative and Uniform Model for Metadata Management in Data Warehousing Environment.\n  Semantic metadata and technical metadata.\n  http://ftp.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-19/paper12.pdf\n\n* Effective Query Expansion using Condensed UMLS Metathesaurus for Medical Information Retrieval\nhttp://www.e-hir.org/journal/view.html?uid=201&start=&sort=&scale=&key=all&oper=&key_word=UMLS&year1=&year2=&Vol=&Num=&PG=&book=&mod=vol&sflag=&sub_box=Y&aut_box=Y&sos_box=&pub_box=Y&key_box=&abs_box=&year=\n\n* A Study of Effective Unified Medical Language System Concept Indexing in Radiology Reports\nhttp://www.e-hir.org/journal/view.html?uid=226&start=&sort=&scale=&key=all&oper=&key_word=UMLS&year1=&year2=&Vol=&Num=&PG=&book=&mod=vol&sflag=&sub_box=Y&aut_box=Y&sos_box=&pub_box=Y&key_box=&abs_box=&year=\n\n* Developing a Reference Terminology Model for Health Care Using an Object-Oriented Approach\nhttp://www.e-hir.org/journal/view.html?uid=311&start=&sort=&scale=&key=all&oper=&key_word=UMLS&year1=&year2=&Vol=&Num=&PG=&book=&mod=vol&sflag=&sub_box=Y&aut_box=Y&sos_box=&pub_box=Y&key_box=&abs_box=&year=\n\n* UMLS(Unified Medical Language System)의 증상용어와 국내의무기록에서 사용되는 증상용어와의 비교연구\nhttp://www.e-hir.org/journal/view.html?uid=922&start=&sort=&scale=&key=all&oper=&key_word=UMLS&year1=&year2=&Vol=&Num=&PG=&book=&mod=vol&sflag=&sub_box=Y&aut_box=Y&sos_box=&pub_box=Y&key_box=&abs_box=&year=\n\n[[Category:Data management]]"]
['Flat file database', '573973', '{{distinguish|Flat file system}}\n{{refimprove|date=March 2015}}\n{{originalresearch|date=March 2015}}\n\n[[Image:Flat File Model.svg|thumb|280px|Example of a flat file model<ref name="USDT01">[http://knowledge.fhwa.dot.gov/tam/aashto.nsf/All+Documents/4825476B2B5C687285256B1F00544258/$FILE/DIGloss.pdf Data Integration Glossary] {{webarchive |url=https://web.archive.org/web/20090320001015/http://knowledge.fhwa.dot.gov/tam/aashto.nsf/All+Documents/4825476B2B5C687285256B1F00544258/$FILE/DIGloss.pdf |date=March 20, 2009 }}, U.S. Department of Transportation, August 2001.</ref>]]\n\nA \'\'\'flat file database\'\'\' is a [[database]] which is stored on its host computer system as an ordinary unstructured file called a "flat file". To access the structure of the data and manipulate it, the file must be read in its entirety into the computer\'s memory. Upon completion of the database operations, the file is again written out in its entirety to the host\'s file system. In this stored mode the database is said to be "flat", meaning that it has no structure for indexing and there are usually no structural relationships between the records. A flat file can be a [[plain text]] file or a [[binary file]].\n\nThe term has generally implied a small, simple database. As computer memory has become cheaper, more sophisticated databases can now be entirely held in memory for faster access. These newer databases would not generally be referred to as flat-file databases.\n\n==Overview==\n\nPlain text files usually contain one [[Record (computer science)|record]] per line,<ref>{{Citation\n | last = Fowler\n | first = Glenn\n | year = 1994\n | title = cql: Flat file database query language\n | periodical = WTEC\'94: Proceedings of the USENIX Winter 1994 Technical Conference on USENIX Winter 1994 Technical Conference\n | url = http://www.research.att.com/~astopen/publications/cql-1994.pdf\n}}</ref> There are different conventions for depicting data. In [[comma-separated values]] and [[delimiter-separated values]] files, [[field (computer science)|field]]s can be separated by [[delimiters]] such as [[Comma-separated values|comma]] or [[Tab separated values|tab]] characters. In other cases, each field may have a fixed length; short values may be padded with [[space character]]s. Extra formatting may be needed to avoid [[delimiter collision]]. More complex solutions are [[markup language]]s and [[programming language]]s.\n\nUsing delimiters incurs some [[Computational overhead|overhead]] in locating them every time they are processed (unlike fixed-width formatting), which may have [[Computer performance|performance]] implications. However, use of character delimiters (especially commas) is also a crude form of [[data compression]] which may assist overall performance by reducing data volumes&nbsp;— especially for [[data transmission]] purposes. Use of character delimiters which include a length component ([[String literal#Declarative notation|Declarative notation]]) is comparatively rare but vastly reduces the overhead associated with locating the extent of each field.\n\nTypical examples of flat files are <code>[[/etc/passwd]]</code> and <code>[[/etc/group]]</code> on [[Unix-like]] operating systems. Another example of a flat file is a name-and-address list with the fields \'\'Name\'\', \'\'Address\'\', and \'\'Phone Number\'\'.\n\nA list of names, addresses, and phone numbers written by hand on a sheet of paper is a flat file database. This can also be done with any [[typewriter]] or [[word processor]]. A [[spreadsheet]] or [[text editor]] program may be used to implement a flat file database, which may then be printed or used [[online]] for improved search capabilities.\n\n==History==\n[[Herman Hollerith]] conceived the idea that data could be represented by holes punched in paper cards then tabulated by machine. He implemented this concept for the [[United States Census Bureau|US Census Bureau]]; thus the [[1890 United States Census]] processing created the first database—consisting of thousands of boxes full of [[punched card]]s.\n\nHollerith\'s enterprise grew into the computer giant [[IBM]], which dominated the data processing market for most of the 20th century. IBM\'s fixed-length field, 80-column punch cards became the ubiquitous means of inputting  electronic data until the 1970s.\n\nIn the 1980s, configurable flat-file database [[computer application]]s were popular on [[DOS]] and the [[Apple Macintosh|Macintosh]]. These programs were designed to make it easy for individuals to design and use their own databases, and were almost on par with [[word processors]] and [[spreadsheet]]s in popularity.{{citation needed|date=September 2011}} Examples of flat-file database products were early versions of [[FileMaker]] and the [[shareware]] [[PC-File]].  Some of these, like [[dBase II]], offered limited [[relational database|relational]] capabilities, allowing some data to be shared between files.\n\nIn the 2010s flat file databases were used in [[content management system]]s. Instead of using a database, web developers were able to change the content directly in the file system or at the command line.\n\n===Contemporary implementations===\nFairCom\'s [[c-tree]] is an example of a modern enterprise-level solution, and [[spreadsheet]] software and [[text editor]]s can be used for this purpose.  [[WebDNA]] is a scripting language designed for the World Wide Web, with a hybrid flat file in-memory database system making it easy to build resilient database-driven websites. With the in-memory concept, WebDNA searches and database updates are almost realtime while the data is stored as text files within the website itself. Otherwise, flat file database is implemented in [[Microsoft Works]] and [[Apple Works]]. Over time, products like [[Borland]]\'s Paradox, and [[Microsoft]]\'s [[Microsoft Access|Access]] started offering some relational capabilities, as well as built-in programming languages.  Database Management Systems ([[DBMS]]) like [[MySQL]] or [[Oracle database|Oracle]] generally require programmers to build applications.\n\nFaceless flat file database engines are used internally by [[Mac OS X]], [[Firefox]], and other computer software to store configuration data. Programs to manage collections of books or appointments and [[address book]] are essentially single-purpose flat file database applications, allowing users to store and retrieve information from flat files using a predefined set of fields.\n\n==Data transfer operations==\nFlat files are used not only as data storage tools in DB and [[Content_management_system|CMS]] systems, but also as data transfer tools to remote servers (in which case they become known as information streams).\n\nIn recent years, this latter implementation has been replaced with [[XML]] files, which not only contain but also describe the data.  Those still using flat files to transfer information are mainframes employing specific procedures which are too expensive to modify.\n\nOne criticism often raised against the XML format as a way to perform mass data transfer operations is that file size is significantly larger than that of flat files, which is generally reduced to the bare minimum.  The solution to this problem consists in XML file compression (a solution that applies equally well to flat files), which has nowadays gained [[Efficient XML Interchange|EXI]] standards (i.e., Efficient XML Interchange, which is often used by mobile devices).\n\nIt is advisable that transfer data be performed via EXI rather than flat files because defining the compression method is not required, because libraries reading the file contents are readily available, and because there is no need for the two communicating systems to preliminarily establish a protocol describing data properties such as position, alignment, type, and format. However, in those circumstances where the sheer mass of data and/or the inadequacy of legacy systems becomes a problem, the only viable solution remains the use of flat files.  In order to successfully handle those problems connected with data communication, format, validation, control and much else (be it a flat file or an XML file data source), it is advisable to adopt a [[Data Quality Firewall]].\n\n==Terminology==\n"Flat file database" may be defined very narrowly, or more broadly. The narrower interpretation is correct in [[Database|database theory]]; the broader covers the term as generally used.\n\nStrictly, a flat file database should consist of nothing but data and, if records vary in length, delimiters. More broadly, the term refers to any database which exists in a single file in the form of rows and columns, with no relationships or links between records and fields except the table structure.\n\nTerms used to describe different aspects of a database and its tools differ from one implementation to the next, but the concepts remain the same. FileMaker uses the term "Find", while MySQL uses the term "Query"; but the concept is the same. FileMaker "files", in version 7 and above, are equivalent to MySQL "databases", and so forth. To avoid confusing the reader, one consistent set of terms is used throughout this article.\n\nHowever, the basic terms "record" and "field" are used in nearly every flat file database implementation.\n\n==Example database==\nThe following example illustrates the basic elements of a flat-file database. The [[data]] arrangement consists of a series of columns and rows organized into a [[table (information)|tabular format]]. This specific example uses only one table.\n\nThe columns include: \'\'name\'\' (a person\'s name, second column); \'\'team\'\' (the name of an athletic team supported by the person, third column); and a numeric \'\'unique ID\'\', (used to uniquely identify records, first column).\n\nHere is an example textual representation of the described data:\n\n id    name    team\n 1     Amy     Blues\n 2     Bob     Reds\n 3     Chuck   Blues\n 4     Richard Blues\n 5     Ethel   Reds\n 6     Fred    Blues\n 7     Gilly   Blues\n 8     Hank    Reds\n 9     Hank    Blues\n\nThis type of data representation is quite standard for a flat-file database, although there are some additional considerations that are not readily apparent from the text:\n* \'\'\'Data types:\'\'\' each column in a database table such as the one above is ordinarily restricted to a specific [[data type]]. Such restrictions are usually established by convention, but not formally indicated unless the data is transferred to a [[relational database]] system.\n* \'\'\'Separated columns:\'\'\' In the above example, individual columns are separated using [[Whitespace (computer science)|whitespace]] characters. This is also called indentation or "fixed-width" data formatting. Another common convention is to separate columns using one or more [[delimiter]] characters. More complex solutions are markup and programming languages.\n* \'\'\'Relational algebra:\'\'\' Each row or record in the above table meets the standard definition of a [[tuple]] under [[relational algebra]] (the above example depicts a series of 3-tuples). Additionally, the first row specifies the [[Tuple#Field_names|field names]] that are associated with the values of each row.\n* \'\'\'Database management system:\'\'\' Since the formal operations possible with a text file are usually more limited than desired, the text in the above example would ordinarily represent an intermediary state of the data prior to being transferred into a [[database management system]].\n\n==References==\n{{Commons category|Flat file models}}\n{{reflist}}\n\n{{Database models}}\n\n{{DEFAULTSORT:Flat File Database}}\n[[Category:Data management]]\n[[Category:Computer file formats]]\n[[Category:Database models]]\n\n[[it:Flat file]]']
['Data extraction', '12097860', "'''Data extraction''' is the act or process of retrieving [[data]] out of (usually [[unstructured data|unstructured]] or poorly structured) data sources for further [[data processing]] or [[data storage device|data storage]] ([[data migration]]). The [[data import|import]] into the intermediate extracting system is thus usually followed by [[data transformation]] and possibly the addition of [[metadata]] prior to [[data export|export]] to another stage in the data [[workflow]].<ref>[http://www.extractingdata.com Definition of data extraction.]</ref>\n\nUsually, the term data extraction is applied when ([[experiment]]al) data is first imported into a computer from primary sources, like [[measuring device|measuring]] or [[recording device]]s. Today's [[electronic device]]s will usually present an [[electrical connector]] (e.g. [[USB]]) through which '[[raw data]]' can be [[data stream|streamed]] into a [[personal computer]].\n\nTypical unstructured data sources include web pages, emails, documents, PDFs, scanned text, mainframe reports, spool files, classifieds, etc. Which is further used for sales / marketing leads.<ref>[http://www.suntecdata.com/data-extraction-services.html Data Extraction Services] Retrieved, April 4, 2016</ref>  Extracting data from these unstructured sources has grown into a considerable technical challenge where as historically data extraction has had to deal with changes in physical hardware formats, the majority of current data extraction deals with extracting data from these unstructured data sources, and from different software formats.  This growing process of data extraction<ref>[http://www.loginworks.com/blogs/web-scraping-blogs/209-web-data-extraction/ data extraction.]</ref> from the web is referred to as [[Web scraping]].\n\nThe act of adding structure to unstructured data takes a number of forms\n* Using text pattern matching such as [[regular expression]]s to identify small or large-scale structure e.g. records in a report and their associated data from headers and footers; \n* Using a table-based approach to identify common sections within a limited domain e.g. in emailed resumes, identifying skills, previous work experience, qualifications etc. using a standard set of commonly used headings (these would differ from language to language), e.g. Education might be found under Education/Qualification/Courses;\n* Using text analytics to attempt to understand the text and link it to other information\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.etltools.org/extraction.html Data Extraction] as a part of the ETL process in a Data Warehousing environment\n\n{{Data warehouse}}\n\n{{DEFAULTSORT:Data Extraction}}\n[[Category:Data management]]\n[[Category:Data warehousing]]"]
['Data conditioning', '24540689', "{{Multiple issues|\n{{POV|date=August 2015}}\n{{One source|date=August 2015}}\n{{Notability|date=August 2015}}\n}}\n\n'''Data conditioning''' is the use of data management and optimization techniques which result in the intelligent routing, optimization and protection of data for storage or [[data movement]] in a computer system.  Data conditioning features enable enterprise and cloud [[data center]]s to dramatically improve system utilization and increase application performance lowering both [[capital expenditures]] and [[operating cost]]s.\n\nData conditioning technologies delivered through a Data Conditioning Platform optimize data as it moves through a computer’s I/O ([[Input/Output]]) path or I/O bus—the data path between the main processor complex and storage subsystems.  The functions of a Data Conditioning Platform typically reside on a storage controller add-in card inserted into the [[PCI-e]] slots of a server.  This enables easy integration of new features in a server or a whole data center.\n\nData conditioning features delivered via a Data Conditioning Platform are designed to simplify system integration, and minimize implementation risks associated with deploying new technologies by ensuring seamless compatibility with all leading server and  storage hardware, operating systems and applications, and meeting all current commercial/off-the-shelf (COTS) standards.  By delivering optimization features via a Data Conditioning Platform, data center managers can improve system efficiency and reduce cost with minimal disruption and avoid the need to modify existing applications or operating systems, and leverage existing hardware systems.\n\n== Summary ==\n\nData conditioning builds on existing data storage functionality delivered in the I/O path including [[RAID]] (Redundant Arrays of Inexpensive Disks), intelligent I/O-based [http://www.adaptec.com/en-us/_common/greenpower?refURL=/greenpower/ power management], and [[Solid-state drive|SSD]] (Solid-State Drive) performance caching techniques.  Data conditioning is enabled both by advanced [[ASIC]] controller technology and intelligent software.  New data conditioning capabilities can be designed into and delivered via storage controllers in the I/O path  or to achieve the data center’s technical and business goals.\n\nData Conditioning strategies can also be applied to improving server and storage utilization and for better managing a wide range of hardware and system-level capabilities.\n\n== Background and Purpose ==\n\nData conditioning principles can be applied to any demanding computing environment to create significant cost, performance and system utilization efficiencies, and are typically deployed by data center managers, system integrators, and storage and server OEMs seeking to optimize hardware and software utilization, simplified, non-intrusive technology integration, and minimal risks and performance hits traditionally associated with incorporating new data center technologies.\n\n== References ==\n\n[http://www.adaptec/maxIQ Adaptec MaxIQ]\n\n[[Category:Data management]]"]
['Database engine', '209503', 'A \'\'\'database engine\'\'\' (or \'\'\'storage engine\'\'\') is the underlying software component that a [[database management system]] (DBMS) uses to [[create, read, update and delete]] (CRUD) [[data]] from a [[database]]. Most database management systems include their own [[application programming interface]] (API) that allows the user to interact with their underlying engine without going through the user interface of the DBMS.\n\nThe term "database engine" is frequently used interchangeably with "[[database server]]" or "database management system". A \'database instance\' refers to the processes and memory structures of the running \'\'\'database engine\'\'\'.\n\n==Storage engines==\nMany of the modern DBMS support multiple storage engines within the same database. For example, [[MySQL]] supports [[InnoDB]] as well as [[MyISAM]].\n\nSome storage engines are [[Database transaction|transactional]].\n\n{| class="wikitable"\n|-\n! Name !! License !! Transactional\n|-\n| [[Aria (storage engine)|Aria]] || GPL || {{No}}\n|-\n| BlitzDB || GPL || {{No}}\n|-\n| [[Falcon (storage engine)|Falcon]] || GPL || {{Yes}}\n|-\n| [[InnoDB]] || GPL || {{Yes}}\n|-\n| [[MyISAM]] || GPL || {{No}}\n|-\n| [[InfiniDB]] || CPL || {{No}}\n|-\n| [[TokuDB]] || GPL || {{Yes}}\n|-\n| [[WiredTiger]] || GPL || {{Yes}}\n|-\n| [[XtraDB]] || GPL || {{Yes}}\n|}\n\nAdditional engine types include:\n*[[Embedded database]] engines\n*[[In-memory database]] engines\n\n==Design considerations==\nDatabase bits are laid out in storage in data structures and groupings that can take advantage of both known effective algorithms to retrieve and manipulate them and the storage own properties. Typically the storage itself is designed to meet requirements of various areas that extensively utilize storage, including databases. A DBMS in operation always simultaneously utilizes several storage types (e.g., memory, and external storage), with respective layout methods.\n\nIn principle the database storage can be viewed as a [[linear address space]], where every bit of data has its unique address in this address space. In practice, only a very small percentage of addresses are kept as initial reference points (which also requires storage); most data is accessed by indirection using displacement calculations (distance in bits from the reference points) and data structures which define access paths (using pointers) to all needed data in an effective manner, optimized for the needed data access operations.\n\n===Database storage hierarchy===\nA database, while in operation, resides simultaneously in several types of storage, forming a [[storage hierarchy]]. By the nature of contemporary computers most of the database part inside a computer that hosts the DBMS resides (partially replicated) in volatile storage. Data (pieces of the database) that are being processed/manipulated reside inside a processor, possibly in [[CPU cache|processor\'s caches]]. These data are being read from/written to memory, typically through a computer [[Bus (computing)|bus]] (so far typically volatile storage components). Computer memory is communicating data (transferred to/from) external storage, typically through standard storage interfaces or networks (e.g., [[fibre channel]], [[iSCSI]]). A [[Disk array|storage array]], a common external storage unit, typically has storage hierarchy of its own, from a fast cache, typically consisting of (volatile and fast) [[DRAM]], which is connected (again via standard interfaces) to drives, possibly with different speeds, like [[USB flash drive|flash drives]] and magnetic [[disk drive]]s (non-volatile). The drives may be connected to [[magnetic tape]]s, on which typically the least active parts of a large database may reside, or database backup generations.\n\nTypically a correlation exists currently between storage speed and price, while the faster storage is typically volatile.\n\n===Data structures===\n{{Main|Database storage structures}}\nA data structure is an abstract construct that embeds data in a well defined manner. An efficient data structure allows to manipulate the data in efficient ways. The data manipulation may include data insertion, deletion, updating and retrieval in various modes. A certain data structure type may be very effective in certain operations, and very ineffective in others. A data structure type is selected upon DBMS development to best meet the operations needed for the types of data it contains. Type of data structure selected for a certain task typically also takes into consideration the type of storage it resides in (e.g., speed of access, minimal size of storage chunk accessed, etc.). In some DBMSs database administrators have the flexibility to select among options of data structures to contain user data for performance reasons. Sometimes the data structures have selectable parameters to tune the database performance.\n\nDatabases may store data in many data structure types.<ref name="Physical Database Design">{{harvnb|Lightstone|Teorey|Nadeau|2007}}</ref> Common examples are the following:\n*ordered/unordered [[flat file database|flat files]]\n*[[hash table]]s\n*[[B+ tree]]s\n*[[ISAM]]\n*[[heap (data structure)|heaps]]\n\n===Data orientation and clustering===\nIn contrast to conventional row-orientation, relational databases can also be [[Column-oriented DBMS|column-oriented]] or [[Correlational database|correlational]] in the way they store data in any particular structure.\n\nIn general, substantial performance improvement is gained if different types of database objects that are usually utilized together are laid in storage in proximity, being "clustered". This usually allows to retrieve needed related objects from storage in minimum number of input operations (each sometimes substantially time consuming). Even for in-memory databases clustering provides performance advantage due to common utilization of large caches for input-output operations in memory, with similar resulting behavior.\n\nFor example, it may be beneficial to cluster a record of an "item" in stock with all its respective "order" records. The decision of whether to cluster certain objects or not depends on the objects\' utilization statistics, object sizes, caches sizes, storage types, etc.\n\n===Database indexing===\n{{Main|Database index}}\nIndexing is a technique some storage engines use for improving database performance. The many types of indexes share the common property that they reduce the need to examine every entry when running a query. In large databases, this can reduce query time/cost by orders of magnitude. The simplest form of index is a sorted list of values that can be searched using a [[binary search]] with an adjacent reference to the location of the entry, analogous to the index in the back of a book. The same data can have multiple indexes (an employee database could be indexed by last name and hire date).\n\nIndexes affect performance, but not results. Database designers can add or remove indexes without changing application logic, reducing maintenance costs as the database grows and database usage evolves.  Indexes can speed up data access, but they consume space in the database, and must be updated each time the data is altered. Indexes therefore can speed data access but slow data maintenance. These two properties determine whether a given index is worth the cost.\n\n==See also==\n{{cleanup-merge}} <!--Into chart -->\n*[[Architecture of Btrieve#Micro-Kernel Database Engine|Btrieve\'s Micro-Kernel Database Engine]]\n*[[Berkeley DB]]\n*[[c-treeACE|c-treeACE Database Engine]]\n*[[FLAIM Database Engine]]\n*[[Microsoft Jet Database Engine]]\n*[[MySQL Cluster]], on the NDB storage engine of [[MySQL]]\n*[[NuoDB]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*http://dev.mysql.com/tech-resources/articles/storage-engine/part_3.html\n*[https://books.google.com/books?id=PqZ6QytCemcC&pg=PT287&dq=storage+engines MySQL Administrator\'s Bible] Chapter 11 "Storage Engines"\n\n{{DEFAULTSORT:Database Engine}}\n[[Category:Data management]]\n[[Category:Database engines| ]]\n[[Category:Database management systems]]']
['Category:Open data', '25128034', '{{cat main}}\n[[Category:Open content]]\n[[Category:Free software|Data]]\n[[Category:Data management]]']
['Linear medium', '676562', "{{Unreferenced|date=December 2009}}\nA '''linear medium''' is any medium which is intended to be written to or accessed in a [[linear]] fashion, literally meaning ''in a line''. \n\nThis means that the information is written to or read from the medium in a given order, so for example a book containing a [[novel]] is intended to be read from front to back, beginning to end, and is therefore a linear medium. It may be written in the same way, but would not necessarily need to be, to be considered a linear medium. \nA book containing an [[encyclopedia]] however is a non-linear medium, as it is not necessary for the articles to be accessed (or written) in any particular order. Even though both non-linear and linear mediums have perimeters to which they are restricted, linear mediums have a set path of how to get from point A to point B, whereas non-linear mediums do not. \n\nExamples in technology are a pre-recorded [[videocassette]] which is usually accessed one item after another, compared with a pre-recorded [[DVD]] which can be accessed in any order.\n\n==Types of linear medium==\n* [[Scroll]]\n* [[Magnetic tape data storage]]\n* [[Paper tape]]\n* [[Photographic film]]\n* [[Novel|story book]]s\n* [[Compact cassette]]s\n\n==See also==\n*[[Sequential access]] \n*[[Random access]]\n\n{{DEFAULTSORT:Linear Medium}}\n[[Category:Data management]]"]
['Uniform data access', '1610890', "{{Unreferenced stub|auto=yes|date=December 2009}}\n'''Uniform data access''' is a computational concept describing an even-ness of connectivity and controllability across numerous target data sources.  \n\nNecessary to fields such as [[Enterprise Information Integration]] (EII) and [[Electronic Data Interchange]] (EDI), it is most often used regarding analysis of disparate data types and data sources, which must be rendered into a [[uniform information representation]], and generally must appear [[wiktionary:Homogenous|homogenous]] to the analysis tools—when the data being analyzed is typically [[heterogeneous]] and widely varying in size, type, and original representation.{{DEFAULTSORT:Uniform Data Access}}\n[[Category:Data management]]\n\n\n{{Comp-sci-stub}}"]
['Comparison of OLAP Servers', '24523966', 'The following tables compare general and technical information for a number of [[online analytical processing]] (OLAP) servers supporting MDX language. Please see the individual products articles for further information.\n\n==General information==\n{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"\n|-\n! OLAP Server\n! Company\n! Website\n! Latest stable version\n! [[Software license]]\n! License Pricing\n|-\n! [[TM1|IBM Cognos TM1]]\n| [[IBM]]\n|<ref>{{cite web|url=http://www-01.ibm.com/software/data/cognos/index.html|title=Cognos Business Intelligence and Financial Performance Management}}</ref>\n| 10.2.2 FP4\n| [[Proprietary software|Proprietary]]\n| -\n|-\n! [[Essbase]]\n| [[Oracle Corporation|Oracle]]\n|<ref>{{cite web|url=http://www.oracle.com/us/solutions/ent-performance-bi/business-intelligence/essbase/index.html|title=Oracle Essbase}}</ref>\n| 11.1.2.4\n| [[Proprietary software|Proprietary]]\n| [http://www.oracle.com/us/corporate/pricing/index.htm]\n|-\n! [[icCube]]\n| [[icCube]]\n|<ref>{{cite web|url=http://www.icCube.com|title=icCube OLAP Server}}</ref>\n| 6.0\n| [[Proprietary software|Proprietary]]\n| community/[http://www.iccube.com//prices]\n|-\n! [[Jedox|Jedox OLAP Server]]\n| [[Jedox]]\n|<ref>{{cite web|url=http://www.jedox.com/en/home/overview.html |title=Jedox AG Business Intelligence |deadurl=yes |archiveurl=https://web.archive.org/web/20100514124342/http://www.jedox.com:80/en/home/overview.html |archivedate=2010-05-14 |df= }}</ref>\n| 7.0\n| [[GNU General Public License|GPL]] v2 or [[EULA]], [[Proprietary software|Proprietary]]\n| -\n|-\n ! Infor BI OLAP Server\n| [[Infor]]\n|<ref>{{cite web|url=http://www.infor.com|title=Infor}}</ref>\n| 10.6.0\n| [[Proprietary software|Proprietary]]\n| -\n|-\n! [[Microsoft Analysis Services]]\n| [[Microsoft]]\n|<ref>{{cite web|url=http://www.microsoft.com/Sqlserver/2008/en/us/analysis-services.aspx|title=Microsoft SQL Server 2008 Analysis Services}}</ref>\n| 2016\n| [[Proprietary software|Proprietary]]\n| [http://www.microsoft.com/sqlserver/2008/en/us/pricing.aspx]\n|-\n! MicroStrategy Intelligence Server\n| [[MicroStrategy]]\n|<ref>{{cite web|url=http://www.microstrategy.com/Software/Products/Intelligence_Server/|title=MicroStrategy Intelligence Server}}</ref>\n| 9\n| [[Proprietary software|Proprietary]]\n| -\n|-\n! [[Mondrian OLAP server]]\n| [[Pentaho]]\n|<ref>{{cite web|url=http://mondrian.pentaho.org|title=Pentaho Analysis Services: Mondrian Project}}</ref>\n| 3.7\n| [[Eclipse Public License|EPL]]\n| free\n|-\n! [[Oracle OLAP|Oracle Database OLAP Option]]\n| [[Oracle Corporation|Oracle]]\n|<ref>{{cite web|url=http://www.oracle.com/technology/documentation/olap.html|title=Oracle OLAP Documentation}}</ref>\n| 11g R2\n| [[Proprietary software|Proprietary]]\n| [http://www.oracle.com/us/corporate/pricing/index.htm]\n|-\n! [[SAS System|SAS OLAP Server]]\n| [[SAS Institute]]\n|<ref>{{cite web|url=http://www.sas.com/technologies/dw/storage/mddb/index.html|title=SAS OLAP Server}}</ref>\n| 9.4\n| [[Proprietary software|Proprietary]]\n| -\n|-\n! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]\n| [[SAP AG|SAP]]\n|<ref>{{cite web|url=http://www.sap.com/usa/platform/netweaver/components/businesswarehouse/index.epx |title=Components & Tools}}</ref>\n| 7.30\n| [[Proprietary software|Proprietary]]\n| -\n|-\n! [[Cubes (OLAP server)|Cubes]]\n| [[Open source|Open source community]]\n|<ref>{{cite web|url=http://cubes.databrewery.org|title=Cubes – Lightweight OLAP Python Toolkit}}</ref>\n| 1.0.1\n| [[MIT License|MIT]]\n| -\n|}\n\n==Data storage modes==\n{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"\n|-\n! OLAP Server\n! [[MOLAP]]\n! [[ROLAP]]\n! [[HOLAP]]\n! In-Memory\n! Offline\n|-\n! [[TM1|IBM Cognos TM1]]\n| {{Yes}}\n| {{No}}\n| {{No}}\n| {{No}}\n| {{Yes| [http://www-01.ibm.com/support/knowledgecenter/SSVJ22_10.2.2/com.ibm.swg.ba.cognos.dsk_ug.10.2.2.doc/t_dsk_maintain_offline.html%23t_dsk_maintain_offline Cognos Insight Distributed mode]}}\n|-\n! [[Essbase]]\n| {{Yes}}\n| {{No}}\n| {{No}}\n| {{No}}\n|\n|-\n! [[icCube]]\n| {{Yes}}\n| {{No}}\n| {{No}}\n| \n| {{Yes | [http://www.iccube.com/support/documentation/user_guide/using/offline_cubes.html Offline Cubes]}}\n|-\n! Infor BI OLAP Server\n| {{Yes}}\n| {{No}}\n| {{No}}\n| {{Yes}}\n|Local cubes\n|-\n! [[Jedox|Jedox OLAP Server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{No}}\n|-\n! [[Microsoft Analysis Services]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes|Local cubes,<br /> [[PowerPivot|PowerPivot for Excel]],<br />[[Power BI|Power BI Desktop]]}}\n|-\n! [[MicroStrategy|MicroStrategy Intelligence Server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{Yes|[http://www.microstrategy.com/Software/Products/User_Interfaces/Office/ MicroStrategy Office],<br /> [http://www.microstrategy.com/Software/Products/Service_Modules/Report_Services/ Dynamic Dashboards]}}\n|-\n! [[Mondrian OLAP server]]\n| {{No}}\n| {{Yes}}\n| {{No}}\n| {{No}}\n|\n|-\n! [[Oracle OLAP|Oracle Database OLAP Option]]\n| {{No}}\n| {{Yes}}\n| {{No}}\n| {{No}}\n|\n|-\n! [[SAS System|SAS OLAP Server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n|\n|-\n! [[IBM Cognos BI]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n|\n|-\n! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{No}}\n|\n|-\n! [[Cubes (OLAP server)]]\n| {{No}}\n| {{Yes}}\n| {{No}}\n|\n|\n|-\n|}\n\n==APIs and query languages==\nAPIs and query languages OLAP servers support.\n{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"\n|-\n! OLAP Server\n! [[XML for Analysis]]\n! [[OLE DB for OLAP]]\n! [[Multidimensional Expressions|MDX]]\n! [[Stored procedures]]\n! Custom functions\n! [[SQL]]\n! [[LINQ]]<ref name="linq">{{cite web|url=http://agiledesignllc.com/products|title=SSAS Entity Framework Provider}}</ref>\n! Visualization\n! [[JSON]]\n! [[REST API]]\n|-\n! [[Essbase]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{Yes}}\n| SmartView (Excel-AddIn), WebAnalysis, Financial Reports\n| {{dunno}}\n| {{dunno}}\n|-\n! [[icCube]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes|[[Java (programming language)|Java]],<ref>{{cite web|url=http://www.iccube.com/support/documentation/mdx_integration/java_integration.html|title=icCube Java integration documentation|publisher=[[icCube]]}}</ref> [[R (programming language)|R]]<ref>{{cite web|url=http://www.iccube.com/support/documentation/mdx_integration/r_integration.html|title=icCube R language integration documentation|publisher=[[icCube]]}}</ref>}}\n| {{Yes}}\n| {{No}}\n| {{Yes}}\n| {{Yes|[[Java (programming language)|Java]], [[Javascript]]}}\n| {{dunno}}\n| {{dunno}}\n|-\n! Infor BI OLAP Server\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes|OLAP Rules, Push Rules, Application Engine}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{Yes|Application Studio}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Jedox|Jedox OLAP Server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes|Cube Rules, SVS Triggers}}\n| {{Yes}}\n| {{No}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Microsoft Analysis Services]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes|[[.NET framework|.NET]]}}<ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/ms176113.aspx|title=SQL Server 2008 Books Online (October 2009)Defining Stored Procedures|publisher=[[MSDN]]}}</ref>\n| {{Yes}}<ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/ms145486.aspx|title=SQL Server 2008 Books Online (October 2009)Using Stored Procedures|publisher=[[MSDN]]}}</ref>\n| {{Yes}}<ref>{{cite web|url=http://support.microsoft.com/kb/218592/en-gb|title=How to perform a SQL Server distributed query with OLAP Server|publisher=[[MSDN]]}}</ref>\n| {{Yes}}\n| {{Yes|Microsoft Excel, SharePoint, Microsoft Power BI, and 70+ other visualization tools}}<ref>{{cite web|url=http://www.ssas-info.com/analysis-services-client-tools-frontend|title=A collection of SSAS frontend tools|publisher=[[SSAS-info.com]]}}</ref>\n| {{dunno}}\n| {{dunno}}\n|-\n! [[MicroStrategy|MicroStrategy Intelligence Server]]\n| {{Yes}}\n| {{No}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Mondrian OLAP server]]\n| {{Yes}}\n| {{Yes}}<ref>{{cite web|url=http://www.simba.com/news/Pentaho-Simba-Partner-for-Excel-Connectivity.htm|title=Pentaho and Simba Technologies Partner to Bring World\'s Most Popular Open Source OLAP Project to Microsoft Excel Users}}</ref>\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}<ref>{{cite web|url=http://mondrian.pentaho.org/documentation/schema.php#User-defined_function|title=How to Define a Mondrian Schema|publisher=Pentaho}}</ref>\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Oracle OLAP|Oracle Database OLAP Option]]\n| {{No}}\n| {{Yes}}<ref name="oraclemdx">{{cite web|url=http://www.oracle.com/us/corporate/press/036550|title=Oracle and Simba Technologies Introduce MDX Provider for Oracle OLAP}}</ref>\n| {{Yes}}<ref name="oraclemdx"/>\n| {{Yes|[[Java (programming language)|Java]], PL/SQL, [[OLAP DML]]}}\n| {{Yes}}\n| {{Yes}}<ref>{{cite web|url=http://www.oracle.com/technology/products/bi/olap/11g/demos/olap_sql_demo.html|title=Querying Oracle OLAP Cubes: Fast Answers to Tough Questions Using Simple SQL}}</ref>\n| {{No}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[SAS System|SAS OLAP Server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{No}}\n| {{No}}\n| {{Yes}}\n| {{Yes|Web Report Studio}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{Yes}}\n| {{No}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[TM1|Cognos TM1]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{Yes}}\n| TM1 Web/TM1 Contributor, IBM Cognos Insight, IBM Performance Modeler, IBM Cognos Cafe for Excel, Cognos BI, TM1 Perspectives for Excel\n| {{dunno}}\n| {{Yes}}\n|-\n! [[Cubes (OLAP server)|Cubes]]\n| {{No}}\n| {{No}}\n| {{No}}\n| {{No}}\n| {{Yes}}\n| {{No}}\n| {{No}}\n| Cubes Viewer<ref>{{cite web|url=https://github.com/jjmontesl/cubesviewer|title=Cubes Viewer|publisher=jjmontes}}</ref>\n| {{Yes}}\n| {{dunno}}\n|}\n\n==OLAP distinctive features==\n\nA list of OLAP features that are not supported by all vendors. All vendors support features such as parent-child, multilevel hierarchy, drilldown.\n\nData processing, management and performance related features:\n\n{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"\n|-\n!OLAP Server\n!Real Time\n!Write-back\n!Partitioning\n!Usage Based Optimizations\n!Load Balancing and Clustering\n|-\n! [[Essbase]]\n| {{No}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[icCube]]\n| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/user_guide/walkthrough/walkthrough_rt.html|title=icCube Real Time walkthrough}}</ref>\n| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/mdx/Update%20Cube.html|title=icCube Writeback/Update Cube}}</ref>\n| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/user_guide/reference/partitioning_edition.html|title=icCube Partitioning}}</ref>\n| {{dunno}}\n| {{dunno}}\n|-\n! Infor BI OLAP Server\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Jedox|Jedox OLAP Server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Microsoft Analysis Services]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[MicroStrategy|MicroStrategy Intelligence Server]]\n| {{dunno}}\n| {{Yes}}<ref>{{cite web|url=http://www.microstrategy.com/Software/Products/Dev_Tools/SDK/extensions.asp|title=Common Extensions of the MicroStrategy Platform}}</ref>\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Mondrian OLAP server]]\n| {{Yes}}\n| {{Yes2 | Planned}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Oracle OLAP|Oracle Database OLAP Option]]\n| {{dunno}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{dunno}}\n|-\n! [[IBM Cognos TM1]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[IBM Cognos BI]]\n| {{Yes}}\n| {{No}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n|-\n! [[SAS OLAP Server]]\n| ?\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]\n| ?\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Cubes (OLAP server)|Cubes]]\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|}\n\nData modeling features:\n{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"\n|-\n!OLAP Server\n!Semi-additive measures\n!Many-to-Many \n!Multi-Cube Model\n!Perspectives\n!KPI\n!Translations\n!Named Sets\n!Multi-attribute Hierarchies\n!Actions\n|-\n! [[Essbase]]\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n|-\n! [[icCube]]\n| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/user_guide/schemas_cubes/facts_aggregation.html|title=icCube Aggregatin types}}</ref>\n| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/user_guide/schemas_cubes/facts_many2many.html|title=icCube Many-to-Many}}</ref>\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! Infor BI OLAP Server\n| {{Yes}}\n| {{dunno}}\n| {{Yes}}\n| {{dunno}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Jedox|Jedox OLAP Server]]\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Microsoft Analysis Services]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[MicroStrategy|MicroStrategy Intelligence Server]]\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Mondrian OLAP server]]\n| {{Yes}}<ref>{{cite web|url=http://jira.pentaho.com/browse/MONDRIAN-962|title=Support for Non-Additive and Semi-Additive Measures}}</ref>\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Oracle OLAP|Oracle Database OLAP Option]]\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[IBM Cognos TM1]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[IBM Cognos BI]]\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n|-\n! [[SAS OLAP Server]]\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n! [[Cubes (OLAP server)|Cubes]]\n| {{dunno}}\n| {{Yes}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|}\n\n==System limits==\n{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"\n|-\n!OLAP Server\n!# cubes\n!# measures\n!# dimensions\n!# dimensions in cube\n!# hierarchies in dimension\n!# levels in hierarchy\n!# dimension members\n|-\n! [[Essbase]]<ref>{{cite web|url=http://docs.oracle.com/cd/E57185_01/epm.1112/essbase_db/frameset.htm?limits.html|title=Essbase Server Limits|publisher=Oracle}}</ref>\n| ?\n| ?\n| ?\n| 255\n| 255\n| ?\n| 20,000,000 (ASO), 1,000,000 (BSO)\n|-\n! [[icCube]]<!-- Java Integer, 32 bits -->\n| 2,147,483,647\n| 2,147,483,647\n| 2,147,483,647\n| ?\n| 2,147,483,647\n| 2,147,483,647\n| 2,147,483,647\n|-\n! Infor BI OLAP Server\n| ?\n| 10,000,000\n| ?\n| 30\n| ?\n| ?\n| 10,000,000\n|-\n! [[Jedox|Jedox OLAP Server]]\n| 2^32 (32 bits) / 2^64 (64 bits)\n| 2^32\n| 2^32 (32 bits) / 2^64 (64 bits)\n| 250\n| 2^32\n| 2^32\n| 2^32\n|-\n! [[Microsoft Analysis Services]]<ref>{{cite web|url=http://technet.microsoft.com/en-us/library/ms365363.aspx|title=SQL Server 2008 Books Online (October 2009)Maximum Capacity Specifications (Analysis Services - Multidimensional Data)|publisher=Microsoft}}</ref>\n| 2,147,483,647\n| 2,147,483,647\n| 2,147,483,647\n| 2,147,483,647 (max. number of dimensions in a database)\n| 2,147,483,647\n| 2,147,483,647\n| 2,147,483,647 (xOLAP)\nUnrestricted (In-memory)\n|-\n! [[MicroStrategy|MicroStrategy Intelligence Server]]<!-- Unrestricted by server - based on hardware limits, infinite it\'s not possible ;-) -->\n| Unrestricted{{efn|name=fn0}}\n| Unrestricted{{efn|name=fn0}}\n| Unrestricted{{efn|name=fn0}}\n| ?\n| Unrestricted{{efn|name=fn0}}\n| Unrestricted{{efn|name=fn0}}\n| Unrestricted{{efn|name=fn0}}\n|-\n! [[SAS System|SAS OLAP Server]]<ref>{{cite web|url=http://support.sas.com/documentation/cdl/en/olapug/63148/HTML/default/viewer.htm#p0m66bhcbgqwjen1jyfhf6woysu3.htm|title=SAS OLAP Cube Size Specifications}}</ref>\n| Unrestricted{{efn|name=fn0}}\n| 1024\n| 128\n| ?\n| 128\n| 19\n| 4,294,967,296\n|-\n! [[IBM Cognos TM1]]\n| Unrestricted{{efn|name=fn0}}\n| Unrestricted\n| Unrestricted{{efn|name=fn0}}\n| 256\n| Unrestricted{{efn|name=fn0}}\n| Unrestricted\n| Unrestricted\n|}\n{{notelist|notes=\n{{efn|name=fn0|Please update as \'unrestricted\', is just not possible}}\n}}\n\n==Security==\n{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"\n|-\n!rowspan="2"| OLAP Server\n!rowspan="2"| Authentication\n!rowspan="2"| Network encryption\n!rowspan="2"| On-the-Fly{{efn|name=fn1}}\n!colspan="3"| Data access\n|-\n!Cell security\n!Dimension security\n!Visual totals\n|-\n! [[Essbase]]\n| {{Yes|Essbase authentication, [[LDAP]] authentication, [[Microsoft Active Directory]]}}\n| {{Yes|[[Transport Layer Security|SSL]]}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n|-\n! [[icCube]]\n| {{Yes|HTTP Basic/Form Authentication, Windows SSO (NTLM,Kerberos)}}\n| {{Yes|[[Transport Layer Security|SSL]]}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! Infor BI OLAP Server\n| {{Yes|OLAP authentication, Infor Federation Services, [[LDAP]], [[Microsoft Active Directory]]}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n|-\n! [[Jedox|Jedox OLAP Server]]\n| {{Yes|Jedox authentication, [[LDAP]], [[Microsoft Active Directory]]}}\n| {{Yes|[[Transport Layer Security|SSL]]}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n|-\n! [[Microsoft Analysis Services]]\n| {{Yes|[[NTLM]], [[Kerberos (protocol)|Kerberos]]}}\n| {{Yes|[[Transport Layer Security|SSL]] and [[SSPI]]}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[MicroStrategy|MicroStrategy Intelligence Server]]\n| {{Yes|Host authentication, database authentication, [[LDAP]], <br />[[Microsoft Active Directory]], [[NTLM]], SiteMinder, Tivoli, SAP, [[Kerberos (protocol)|Kerberos]]}}\n| {{Yes|[[Transport Layer Security|SSL]], AES<ref>[http://latam.microstrategy.com/Software/Products/Intelligence_Server/features.asp MicroStrategy Intelligence Server Features]</ref>}}\n| ?\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[Oracle OLAP|Oracle Database OLAP Option]]\n| {{Yes|Oracle Database authentication}}\n| {{Yes|[[Transport Layer Security|SSL]]}}\n| ?\n| {{Yes}}\n| {{Yes}}\n| {{dunno}}\n|-\n! [[SAS System|SAS OLAP Server]]<ref>{{cite web|url=http://support.sas.com/documentation/cdl/en/mdxag/59575/HTML/default/a003230130.htm|title=SAS OLAP Security Totals and Permission Conditions}}</ref>\n| {{Yes|Host authentication,SAS token authentication, [[LDAP]], [[Microsoft Active Directory]]}}\n| {{Yes}}<ref>{{cite web|url=http://support.sas.com/documentation/cdl/en/bisecag/61133/HTML/default/a003275910.htm|title=How to Change Over-the-Wire Encryption Settings for SAS Servers}}</ref>\n| ?\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[IBM Cognos TM1]]\n| {{Yes|Builtin, [[LDAP]], [[Microsoft Active Directory]], [[NTLM]], IBM Cognos BI authentication}}\n| {{Yes|[[Transport Layer Security|SSL]]}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|}\n{{notelist|notes=\n{{efn|name=fn1|On-the-Fly : The ability to define authentication dynamically via programmatic interfaces. New users do not require restarting the server or redefining the security.}}\n}}\n\n==Operating systems==\nThe OLAP servers can run on the following [[operating system]]s:\n{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"\n|-\n! OLAP Server\n! Windows\n! Linux\n! UNIX\n! z/OS\n! AIX\n|-\n! [[Essbase]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n|-\n! [[icCube]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! Infor BI OLAP Server\n| {{Yes}}\n| {{No}}\n| {{No}}\n| {{No}}\n|-\n! [[Jedox|Jedox OLAP Server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n|-\n! [[Microsoft Analysis Services]]\n| {{Yes}}\n| {{No}}\n| {{No}}\n| {{No}}\n|-\n! [[MicroStrategy|MicroStrategy Intelligence Server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n|-\n! [[Mondrian OLAP server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[Oracle OLAP|Oracle Database OLAP Option]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[SAS System|SAS OLAP Server]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n|-\n! [[IBM Cognos TM1]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n| {{Yes}}\n|-\n! [[Cubes (OLAP server)|Cubes]]\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}\n| {{No}}\n|}\n<cite id="os_java">Note (1):</cite>The server availability depends on [[JVM|Java Virtual Machine]] not on the [[operating system]]</cite>\n\n==Support information==\n{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"\n|-\n! OLAP Server\n! Issue Tracking System\n! Forum/Blog\n! Roadmap\n! Source code\n|-\n! [[Essbase]]\n| {{Yes|[http://support.oracle.com myOracle Support]}}\n| [http://forums.oracle.com/forums/main.jspa?categoryID=84]\n| [http://communities.ioug.org/Portals/2/Oracle_Essbase_Roadmap_Sep_09.pdf]\n| Closed\n|-\n! [[icCube]]\n| {{Yes | [http://issues.iccube.com/ YouTrack]}}\n| |[http://www.iccube.com/forum]\n| \n| Open\n|-\n! Infor BI OLAP Server\n| {{Yes|Infor Xtreme}}\n| \n| Available upon request\n| Closed\n|-\n! [[Jedox|Jedox OLAP Server]]\n| {{Yes|[http://bugs.palo.net/mantis/main_page.php Mantis]}}\n| [http://www.jedox.com/community/palo-forum/board.php?boardid=9]\n|\n| Open\n|-\n! [[Microsoft Analysis Services]]\n| {{Yes|[https://connect.microsoft.com/SQLServer Connect]}}\n| [http://social.msdn.microsoft.com/Forums/en-US/sqlanalysisservices/threads]\n| -\n| Closed\n|-\n! [[MicroStrategy|MicroStrategy Intelligence Server]]\n| {{Yes | [https://resource.microstrategy.com/Support/MainSearch.aspx MicroStrategy Resource Center]}}\n| [https://resource.microstrategy.com/Forum/]\n| -\n| Closed\n|-\n! [[Mondrian OLAP server]]\n| {{Yes|[http://jira.pentaho.com/browse/MONDRIAN Jira]}}\n| [http://forums.pentaho.org/forumdisplay.php?f=79]\n| [http://mondrian.pentaho.org/documentation/roadmap.php]\n| Open\n|-\n! [[Oracle OLAP|Oracle Database OLAP Option]]\n| {{Yes|[http://support.oracle.com myOracle Support]}}\n| [http://forums.oracle.com/forums/main.jspa?categoryID=84]\n|\n| Closed\n|-\n! [[SAS System|SAS OLAP Server]]\n| {{Yes|[http://support.sas.com/forums/index.jspa Support]}}\n| [http://blogs.sas.com/]\n|\n| Closed\n|-\n! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]\n| {{Yes | [http://service.sap.com/ OSS]}}\n| [http://forums.sdn.sap.com/index.jspa]\n| [http://esworkplace.sap.com/socoview(bD1lbiZjPTAwMSZkPW1pbg==)/render.asp?id=2270EAD629814D05A7ECECECECC8D002&fragID=&packageid=DEE98D07DF9FA9F1B3C7001A64D3F462]\n| Closed\n|-\n! [[IBM Cognos TM1]]\n| {{Yes | [http://ibm.com/support/servicerequest/ IBM Service Request]}}\n| [http://www.tm1forum.com/viewforum.php?f=3]\n|\n| Closed\n|-\n! [[Cubes (OLAP server)|Cubes]]\n| {{Yes|[https://github.com/databrewery/cubes/issues Cubes – Github Issues]}}\n| [https://groups.google.com/forum/#!forum/cubes-discuss]\n| [https://github.com/DataBrewery/cubes/wiki/Roadmap]\n| [https://github.com/DataBrewery/cubes Open]\n|}\n\n==See also==\n* [[Cubes (OLAP server)|Cubes]] (light-weight open-source OLAP server)\n* [[icCube]]\n* [[Palo (OLAP database)]]\n\n==References==\n{{reflist}}\n\n{{Data warehouse}}\n\n{{DEFAULTSORT:Comparison Of Olap Servers}}\n[[Category:Online analytical processing| ]]\n[[Category:Software comparisons|OLAP Servers]]\n[[Category:Data management]]\n[[Category:Data warehousing products]]']
['Jenks natural breaks optimization', '25397242', 'The \'\'\'Jenks optimization method\'\'\', also called the \'\'\'Jenks natural breaks classification method\'\'\', is a [[data clustering]] method designed to determine the best arrangement of values into different classes. This is done by seeking to minimize each class’s average deviation from the class mean, while maximizing each class’s deviation from the means of the other groups. In other words, the method seeks to reduce the [[variance]] within classes and maximize the variance between classes.<ref name="Jenks">Jenks, George F. 1967. "The Data Model Concept in Statistical Mapping", International Yearbook of Cartography 7: 186–190.</ref><ref name="McMaster">McMaster, Robert, "In Memoriam: George F. Jenks (1916–1996)". Cartography and Geographic Information Science. 24(1) p.56-59.</ref>\n\n==History==\n\n=== George Jenks ===\nGeorge Frederick Jenks was a 20th Century American [[cartography|cartographer]]. Graduating with his Ph.D. in agricultural geography from [[Syracuse University]] in 1947, Jenks began his career under the tutelage of [[Richard Edes Harrison|Richard Harrison]], cartographer for [[Time (magazine)|TIME]] and Fortune magazine.<ref name="McMaster2">McMaster, Robert and McMaster, Susanna. 2002. “A History of Twentieth-Century American Academic Cartography”, Cartography and Geographic Information Science. 29(3) p.312-315.</ref> He joined the faculty of the [[University of Kansas]] in 1949 and began to build the cartography program. During his 37-year tenure at KU, Jenks developed the Cartography program into one of three programs renowned for their graduate education in the field; the others being the [[University of Wisconsin]] and the [[University of Washington]]. Much of his time was spent developing and promoting improved cartographic training techniques and programs. He also spent significant time investigating three-dimensional maps, eye-movement research, [[thematic map]] communication, and [[geostatistics]].<ref name="McMaster" /><ref name="McMaster2" /><ref name="CSUN">CSUN Cartography Specialty Group, [http://www.csun.edu/~hfgeg003/csg/winter97.html Winter 1997 Newsletter]</ref>\n\n===Development===\nJenks was a cartographer by profession. His work with [[statistics]] grew out of a desire to make [[choropleth map]]s more visually accurate for the viewer. In his paper, \'\'The Data Model Concept in Statistical Mapping\'\', he claims that by visualizing data in a three dimensional model cartographers could devise a “systematic and rational method for preparing choroplethic maps”.<ref name="Jenks" /> Jenks used the analogy of a “blanket of error” to describe the need to use elements other than the mean to generalize data. The three dimensional models were created to help Jenks visualize the difference between data classes. His aim was to generalize the data using as few planes as possible and maintain a constant “blanket of error”.\n\n==Method==\nThe method requires an iterative process. That is, calculations must be repeated using different breaks in the dataset to determine which set of breaks has the smallest in-class [[variance]]. The process is started by dividing the ordered data into groups. Initial group divisions can be arbitrary. There are four steps that must be repeated:\n#Calculate the sum of squared deviations between classes (SDBC).\n#Calculate the sum of squared deviations from the array mean (SDAM).\n#Subtract the SDBC from the SDAM (SDAM-SDBC). This equals the sum of the squared deviations from the class means (SDCM).\n#After inspecting each of the SDBC, a decision is made to move one unit from the class with the largest SDBC toward the class with the lowest SDBC.\n\nNew class deviations are then calculated, and the process is repeated until the sum of the within class deviations reaches a minimal value.<ref name="Jenks" /><ref name="ESRI">ESRI FAQ, [http://support.esri.com/index.cfm?fa=knowledgebase.techarticles.articleShow&d=26442 What is the Jenks Optimization method]</ref>\n\nAlternatively, all break combinations may be examined, SDCM calculated for each combination, and the combination with the lowest SDCM selected. Since all break combinations are examined, this guarantees that the one with the lowest SDCM is found.\n\nFinally, the GVF statistic (goodness of variance fit) is calculated. GVF is defined as (SDAM - SDCM) / SDAM. GVF ranges from 0 (worst fit) to 1 (perfect fit).\n\n==Uses==\n\n{{main article|Choropleth map}}\nJenks’ goal in developing this method was to create a map that was absolutely accurate, in terms of the representation of data’s spatial attributes. By following this process, Jenks claims, the “blanket of error” can be uniformly distributed across the mapped surface. He developed this with the intention of using relatively few data classes, less than seven, because that was the limit when using monochromatic shading on a choroplethic map.<ref name="Jenks" />\n\n==Alternative methods==\n{{Main article|Cluster analysis}}\n\nOther methods of data classification include [[Head/tail Breaks]], Natural Breaks (without Jenks Optimization), Equal Interval, Quantile, and Standard Deviation.\n\n==See also==\n* [[k-means clustering]], a generalization for multivariate data (Jenks natural breaks optimization seems to be one dimensional k-means<ref>[http://www.quantdec.com/SYSEN597/GTKAV/section1/chapter_9.htm]</ref>).\n\n==References==\n{{Reflist}}\n\n==External links==\n* ESRI FAQ, [http://support.esri.com/index.cfm?fa=knowledgebase.techarticles.articleShow&d=26442 What is the Jenks Optimization method]\n* Volunteered Geographic Information, Daniel Lewis, [http://danieljlewis.org/2010/06/07/jenks-natural-breaks-algorithm-in-python/ Jenks Natural Breaks Algorithm with an implementation in python]\n* Object Vision wiki, [http://wiki.objectvision.nl/index.php/Fisher%27s_Natural_Breaks_Classification Fisher\'s Natural Breaks Classification, a O(k*n*log(n)) algorithm]\n* [http://www.ehdp.com/vitalnet/breaks-1.htm What is Jenks Natural Breaks?]\n\n[[Category:Data management]]\n[[Category:Cartography]]']
['Distributed transaction', '619053', "{{POV|Commitment ordering|date=November 2011}}\nA '''distributed transaction''' is a [[database transaction]] in which two or more network hosts are involved. Usually, hosts provide '''transactional resources''', while the '''transaction manager''' is responsible for creating and managing a global transaction that encompasses all operations against such resources. Distributed transactions, as any other [[Database transaction|transactions]], must have all four [[ACID|ACID (atomicity, consistency, isolation, durability)]] properties, where atomicity guarantees all-or-nothing outcomes for the unit of work (operations bundle).\n\nOpen Group, a vendor consortium, proposed the [[X/Open XA|X/Open Distributed Transaction Processing (DTP) Model]] (X/Open XA), which became a de facto standard for behavior of transaction model components.\n\nDatabase are common transactional resources and, often, transactions span a couple of such databases. In this case, a distributed transaction can be seen as a [[database transaction]] that must be [[Synchronization|synchronized]] (or provide [[ACID]] properties) among multiple participating [[database]]s which are [[distributed computing|distributed]] among different physical locations. The [[isolation (computer science)|isolation]] property (the I of ACID) poses a special challenge for multi database transactions, since the (global) [[serializability]] property could be violated, even if each database provides it (see also [[global serializability]]). In practice most commercial database systems use [[Two phase locking|strong strict two phase locking (SS2PL)]] for [[concurrency control]], which ensures global serializability, if all the participating databases employ it. (see also [[commitment ordering]] for multidatabases.)\n\nA common [[algorithm]] for ensuring [[correctness (computer science)|correct]] completion of a distributed transaction is the [[two-phase commit]] (2PC). This algorithm is usually applied for updates able to [[commit (data management)|commit]] in a short period of time, ranging from couple of milliseconds to couple of minutes.\n\nThere are also long-lived distributed transactions, for example a transaction to book a trip, which consists of booking a flight, a rental car and a hotel. Since booking the flight might take up to a day to get a confirmation, two-phase commit is not applicable here, it will lock the resources for this long. In this case more sophisticated techniques that involve multiple undo levels are used. The way you can undo the hotel booking by calling a desk and cancelling the reservation, a system can be designed to undo certain operations (unless they are irreversibly finished).\n\nIn practice, long-lived distributed transactions are implemented in systems based on [[Web Services]]. Usually these transactions utilize principles of [[Compensating transaction]]s, Optimism and Isolation Without Locking. X/Open standard does not cover long-lived DTP.\n\nSeveral modern technologies, including [[Enterprise Java Beans]] (EJBs) and [[Microsoft Transaction Server]] (MTS) fully support distributed transaction standards.\n\n==See also==\n* [[Java Transaction API|Java Transaction API (JTA)]]\n* [[Enduro/X|Enduro/X Open source X/Open XA and XATMI implementation]]\n\n==References==\n* {{cite web | title=Web-Services Transactions | work=Web-Services Transactions | url=http://xml.sys-con.com/read/43755.htm | accessdate=May 2, 2005 }}\n* {{cite web | title=Nuts And Bolts Of Transaction Processing | work=Article about Transaction Management | url=http://www.subbu.org/articles/transactions/NutsAndBoltsOfTP.html\n| accessdate=May 3, 2005 }}\n* {{cite web | title=A Detailed Comparison of Enterprise JavaBeans (EJB) & The Microsoft Transaction Server (MTS) Models\n | url=http://gsraj.tripod.com/misc/ejbmtscomp.html }}\n\n==Further reading==\n* Gerhard Weikum, Gottfried Vossen, ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, 2002, ISBN 1-55860-508-8\n\n{{DEFAULTSORT:Distributed Transaction}}\n[[Category:Data management]]\n[[Category:Transaction processing]]"]
['Lean integration', '28252181', '\'\'\'Lean integration\'\'\' is a [[management system]] that emphasizes creating value for customers, continuous improvement, and eliminating waste as a sustainable [[data integration]] and [[system integration]] practice.  Lean integration has parallels with other lean disciplines such as [[lean manufacturing]], [[lean IT]], and [[lean software development]].  It is a specialized collection of tools and techniques that address the unique challenges associated with seamlessly combining information and processes from systems that were independently developed, are based on incompatible data models, and remain independently managed, to achieve a cohesive holistic operation.\n\n==History==\n\nLean integration was first introduced by John Schmidt in a series of blog articles starting in January 2009 entitled 10 Weeks To Lean Integration.<ref>[http://blogs.informatica.com/perspectives/index.php/2009/01/14/10-weeks-to-lean-integration/ Original Lean Integration Blog Series]</ref>  This was followed by a white paper<ref>[http://www.cloudyintegration.com/uploads/LEAN_INTEGRATION_AFE_-_John_Schmidt.pdf Lean Integration White Paper]</ref> on the topic in April 2009 and the book \'\'Lean Integration, An Integration Factory Approach to Business Agility\'\' <ref name="Schmidt">John G.Schmidt, David Lyle (2010) \'\'Lean Integration: An Integration Factory Approach to Business Agility\'\', Addison-Wesley Pearson Education, ISBN 0-321-71231-5</ref> in May 2010.\n\n==Overview==\n\nLean integration builds on the same set of principles that were developed for [[lean manufacturing]] and [[lean software development]] which is based on the [[Toyota Production System]]. Integration solutions can be broadly categorized as either Process Integration or Data Integration.  \n\nThe book<ref name="Schmidt"/> is based on the premise that Integration is an ongoing activity and not a one-time activity;  therefore integration should be viewed as a long term strategy for an organization.  John Schmidt and David Lyle initially articulated in their book the reasons for maintaining an efficient and sustainable integration team.  Lean integration as an integration approach must be \'\'sustainable\'\' and \'\'holistic\'\' unlike other integration approaches that either tackle only a part of the problem or tackle the problem for a short period of time.  Lean integration drives elimination of waste by adopting reusable elements, high automation and quality improvements.  Lean is a data-driven, fact-based methodology that relies on metrics to ensure that the quality and performance are maintained at a high level. \n\nAn organizational focus is required for the implementation of lean integration principles. The predominant organizational model is the [[Integration Competency Center]] which may be structured as a central group or a more loosely coupled federated team.\n\n==Lean integration principles==\n\nThe principles of Lean Integration may at first glance appear similar to that of [[Six Sigma]] but there are some very clear differences between them.  Six-Sigma is an \'\'analytical technique\'\' that focuses on quality and reduction of defects while Lean is a \'\'management system\'\' that focuses on delivering value to the end customer by continuously improving value delivery processes.  Lean provides a robust framework that facilitates improving efficiency and effectiveness by focusing on critical customer requirements.\n\nAs mentioned in lean integration there are seven core \'\'lean integration principles\'\' vital for deriving significant and sustainable business benefits. They are as below: \n\n# Focus on the customer and eliminate waste: Waste elimination should be viewed from the customer perspective and all activities that do not add value to the customer needs to be looked at closely and eliminated or reduced. In an integration context, the customer is often an internal sponsor or group within an organization that uses, benefits from, or pays for, the integrated capabilities.\n# Continuously improve: A data driven cycle of hypothesis-validation-implementation should be used to drive innovation and continuously improve the end-to-end process.  Adopting and institutionalizing lessons learned and sustaining integration knowledge are related concepts that assist in the establishment of this principle.\n# Empower the team: Creating cross-functional teams and sharing commitments across individuals empower the teams and individuals who have a clear understanding of their roles and the needs of their customers.  The team is also provided the support by senior management to innovate and try new ideas without fear of failure.\n# Optimize the whole: Adopt a big-picture perspective of the end-to-end process and optimize the whole to maximize the customer value.  This may at times require performing individual steps and activities that appear to be sub-optimal when viewed in isolation, but aid in streamlining the end-to-end process.\n# Plan for change: Application of mass customization techniques like leveraging automated tools, structured processes, and reusable and parameterized integration elements leads to reduction in cost and time in both the build and run stages of the integration life-cycle. Another key technique is a middleware services layer that presents applications with enduring abstractions of data through standardized interfaces, allowing the underlying data structures to change without necessarily impacting the dependent applications.\n# Automate processes: Automation of tasks increases the ability to respond to large integration projects as effectively as small changes. In its ultimate form, automation eliminates integration dependencies from the critical implementation path of projects.\n# Build quality in : Process excellence is emphasized and quality is built in rather than inspected in. A key metric for this principle is First Time Through (FTT) percentage which is a measure of the number of times an end-to-end process is executed without having to do any rework or repeat any of the steps.\n\n==Benefits of lean integration==\n\nThe Lean integration practices transforms integration from an \'\'art\'\' into a \'\'science\'\', a repeatable and teachable methodology that shifts the focus from integration as a point-in-time activity to integration as a sustainable activity that enables organizational agility.  Once an organization adopts the integration as a science it enhances the organization’s ability to change rapidly without comprising on the IT risk or quality thereby transforming the organization into an agile data driven enterprise.  The following are the advantages derived by adopting the lean integration practices:\n\n# Efficiency: typical improvements are in the scale of 50% labor productivity improvements and 90% lead-time reduction through continuous efforts to eliminate waste.\n# Agility: Reusable components, highly automated processes and self-service delivery models improve the agility of the organization.\n# Data quality: quality and reliability of data is enhanced and data becomes a real asset.\n# Governance: metrics are established that drive continuous improvement.\n# Innovation: innovation is facilitated by using fact-based approach.\n# Staff Morale: IT staff is kept engaged with high morale driving bottom-up improvements.\n\n==See also==\n\n* [[Integration Competency Center]]\n* [[Lean software development]]\n* [[Lean IT]]\n* [[Data Integration]]\n* [[Toyota Production System]]\n\n==References==\n\n<references/>\n\n==External links==\n* [http://www.integrationfactory.com Lean Integration book microsite]\n* [http://blogs.informatica.com/perspectives/index.php/2010/04/06/health-care-is-ready-for-lean-integration/ Application of Lean Integration to Health Care]\n* [http://www.informatica.com/news_events/press_releases/Pages/02082010_lean.aspx  Press Release about Lean Integration Book]\n* [http://www.linkedin.com/in/johnschmidt John Schmidt profile]\n* [http://www.linkedin.com/in/davelyle David Lyle profile]\n* [http://my.safaribooksonline.com/9780321712363 Lean Integration book publisher website]\n* [http://www.baselinemag.com/c/a/IT-Management/How-IT-Runs-Lean-419352/ Slide show overview of Lean Integration]\n* [http://www.linkedin.com/groups?gid=2302506 LinkedIn Group for Lean Integration Community]\n* [http://www.itbusinessedge.com/cm/blogs/vizard/making-the-case-for-lean-integration/?cs=42547 Book review by Mike Vizard of ITBusinessEdge]\n* [http://www.bcs.org/server.php?show=conBlogPost.1685 Book review by John Morris]\n* [http://www.itbusinessedge.com/cm/blogs/lawson/lean-principles-can-make-it-better-at-integration/?cs=42041&utm_source=itbe&utm_medium=email&utm_campaign=EEB&nr=EEB John Schmidt and David Lyle Interview by Loraine Lawson]\n* [http://www.insurancenetworking.com/blogs/insurance_technology_Lean_IT_manufacturing-25138-1.html Book review by Joe McKendrick]\n* [http://www.information-management.com/dmradio/-10017194-1.html David Lyle Interview on DM Radio]\n\n[[Category:Data management]]\n[[Category:Software development philosophies]]\n[[Category:Agile software development]]\n[[Category:Information technology]]\n[[Category:Quality]]']
['Virtual data room', '17879652', "A '''virtual data room''' (sometimes called a '''VDR''') is an online repository of information that is used for the storing and distribution of documents.  In many cases, a virtual data room is used to facilitate the [[due diligence]] process during an [[M&A]] transaction, [[loan syndication]], or private equity and venture capital transactions.  This due diligence process has traditionally used a physical [[data room]] to accomplish the disclosure of documents. For reasons of cost, efficiency and security, virtual data rooms have widely replaced the more traditional physical data room.<ref>http://www.inc.com/best-industries-2013/jeremy-quittner/virtual-data-rooms.html</ref><ref>http://www.forbes.com/pictures/fghj45fjl/2-virtual-data-rooms/</ref>\n\nA virtual data room is an [[extranet]] to which the bidders and their advisers are given access via the internet. An extranet is essentially a website with limited controlled access, using a secure log-on supplied by the vendor, which can be disabled at any time, by the vendor, if a bidder withdraws. Much of the information released is confidential and restrictions are applied to the viewer’s ability to release this to third parties (by means of forwarding, copying or printing). This can be effectively applied to protect the data using digital rights management.<ref>http://www.divestopedia.com/definition/836/virtual-data-room-vdr</ref> \n\nIn the process of [[mergers and acquisitions]] the data room is set up as part of the central repository of data relating to companies or divisions being acquired or sold. The data room enables the interested parties to view information relating to the business in a controlled environment where confidentiality can be preserved. Conventionally this was achieved by establishing a supervised, physical data room in secure premises with controlled access. In most cases, with a physical data room, only one bidder team can access the room at a time. A virtual data room is designed to have the same advantages as a conventional data room (controlling access, viewing, copying and printing, etc.) with fewer disadvantages. Due to their increased efficiency, many businesses and industries have moved to using virtual data rooms instead of physical data rooms. In 2006, a spokesperson for a company which sets up virtual deal rooms was reported claiming that the process reduced the bidding process by about thirty days compared to physical data rooms.<ref>{{cite news|last1=Buckler|first1=Grant|title=A virtual smoke-filled room|url=http://www.theglobeandmail.com/technology/a-virtual-smoke-filled-room/article1110056/|accessdate=4 July 2016|work=The Globe and Mail|date=21 November 2006}}</ref>\n\n==References==\n{{Reflist}}\n\n\n{{DEFAULTSORT:Virtual Data Room}}\n[[Category:Data management]]\n[[Category:Disclosure]]\n[[Category:Mergers and acquisitions]]"]
['DMAPI', '8947566', '\'\'\'Data Management API\'\'\' (\'\'\'DMAPI\'\'\') is the interface defined in the [[X/Open]] document "Systems Management: Data Storage Management (XDSM) API" dated February 1997. [[XFS]], IBM [[JFS (file system)|JFS]], [[VxFS]], [[AdvFS]], [[StorNext]] and [[GPFS]] file systems support DMAPI for [[Hierarchical storage management|Hierarchical Storage Management]] (HSM).\n\n== External links ==\n* [http://pubs.opengroup.org/onlinepubs/9657099/ Systems Management: Data Storage Management (XDSM) API]\n* [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/topic/com.ibm.cluster.gpfs34.dmapi.doc/bl1dmp_BookMap_xtoc.html GPFS V3.4 Data Management API Guide]\n* [http://oss.sgi.com/projects/xfs/ Open Source XFS Source code with DMAPI Implementation and Test Suite ]\n\n[[Category:Data management]]\n\n{{compu-storage-stub}}']
['Category:NoSQL', '29590205', '{{Cat main|NoSQL}}\n\n[[Category:Databases]] \n[[Category:Data management]]\n[[Category:Structured storage]]']
['Data custodian', '28192799', "{{more footnotes|date=August 2010}}\n{{merge|Data steward|date=February 2016}}\nIn [[Data governance|Data Governance]] groups, responsibilities for data management are increasingly divided between the business process owners and information technology (IT) departments.  Two functional titles commonly used for these roles are [[Data steward|Data Steward]] and Data Custodian. \n\nData Stewards are commonly responsible for data content, context, and associated business rules. Data Custodians are responsible for the safe custody, transport, storage of the data and implementation of business rules.<ref>Carnegie Mellon - Information Security Roles and Responsibilities, http://www.cmu.edu/iso/governance/roles/data-custodian.html</ref><ref>''Policies, Regulations and Rules: Data Management Procedures - REG 08.00.3 - Information Technology'', , NC State University, http://www.ncsu.edu/policies/informationtechnology/REG08.00.3.php</ref> Simply put, Data Stewards are responsible for what is stored in a data field, while Data Custodians are responsible for the technical environment and database structure. Common job titles for data custodians are Database Administrator (DBA), Data Modeler, and ETL Developer.\n\n==Data Custodian Responsibilities==\nA data custodian ensures:\n# Access to the data is authorized and controlled\n# Data stewards are identified for each data set\n# Technical processes sustain data integrity\n# Processes exist for data quality issue resolution in partnership with Data Stewards\n# Technical controls safeguard data\n# Data added to data sets are consistent with the common data model\n# Versions of Master Data are maintained along with the history of changes\n# Change management practices are applied in maintenance of the database\n# Data content and changes can be audited\n\n==See also==\n* [[Data governance]]\n* [[Data steward]]\n\n==References==\n<references></references>\n\n==Related Links==\n* ''Establishing data stewards'', by Jonathan G. Geiger, Teradata Magazine Online, September 2008, http://www.teradata.com/tdmo/v08n03/Features/EstablishingDataStewards.aspx\n* '' A Rose By Any Other Name – Titles In Data Governance'', by Anne Marie Smith, Ph.D., EIMInstitute.ORG Archives, Volume 1, Issue 13, March 2008, http://www.eiminstitute.org/library/eimi-archives/volume-1-issue-13-march-2008-edition/a-rose-by-any-other-name-2013-titles-in-data-governance\n\n{{DEFAULTSORT:Data Custodian}}\n[[Category:Information technology governance]]\n[[Category:Data management]]\n[[Category:Knowledge representation]]\n[[Category:Library occupations]]\n[[Category:Metadata]]\n[[Category:Technical communication]]\n\n[[ar:ميتاداتا]]\n[[cs:Metadata]]\n[[da:Metadata]]\n[[de:Data Steward]]\n[[et:Metaandmed]]\n[[es:Metadato]]\n[[eo:Meta-dateno]]\n[[fr:Métadonnée]]\n[[it:Metadata]]\n[[lv:Metadati]]\n[[hu:Metaadat]]\n[[nl:Metadata]]\n[[ja:メタデータ]]\n[[no:Metadata]]\n[[pl:Metadane]]\n[[pt:Metadados]]\n[[ru:Метаданные]]\n[[fi:Metatieto]]\n[[sv:Metadata]]\n[[th:เมทาดาตา]]\n[[vi:Metadata]]"]
['Project workforce management', '7217055', "'''Project workforce management''' is the practice of combining the coordination of all logistic elements of a project through a single [[software application]] (or [[workflow engine]]). This includes planning and tracking of schedules and mileposts, cost and revenue, resource allocation, as well as overall management of these project elements.  Efficiency is improved by eliminating manual processes, like [[spreadsheet]] tracking<ref>\n{{Cite web\n| author      = Seema Haji\n| title       = Business Intelligence Cures the Spreadsheet Problem\n| url         = http://www.refresher.com/asmhbi.html\n| publisher   = Refresher Publications Inc.\n| year        = 2009\n| accessdate  = October 30, 2009\n}}</ref>  to monitor project progress. It also allows for at-a-glance status updates and ideally integrates with existing legacy applications in order to unify ongoing projects, [[enterprise resource planning]] (ERP) and broader organizational goals.<ref>\n{{Cite web\n| author      = Rudolf Melik\n| title       = The Rise of the Project Workforce\n| url         = http://www.projectworkforcebook.com/\n| publisher   = Wiley: New York, NY\n| year        = 2007\n| accessdate  = October 30, 2009\n}}</ref> There are a lot of logistic elements in a project. Different team members are responsible for managing each element and often, the organisation may have a mechanism to manage some logistic areas as well.\n\nBy coordinating these various components of [[project management]], [[workforce management]] and financials through a single solution, the process of configuring and changing project and workforce details is simplified.\n\n== Introduction ==\n<ref>{{Citation|title = Project workforce management|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref> A project workforce management system defines project tasks, project positions, and assigns personnel to the project positions. The project tasks and positions are correlated to assign a responsible project position or even multiple positions to complete each project task. Because each project position may be assigned to a specific person, the qualifications and availabilities of that person can be taken into account when determining the assignment. By associating project tasks and project positions, a manager can better control the assignment of the workforce and complete the project more efficiently.\n\nWhen it comes to project workforce management, it is all about managing all the logistic aspects of a project or an organisation through a software application. Usually, this software has a workflow engine defined. Therefore, all the logistic processes take place in the workflow engine.\n\n== About ==\n\n=== Technical Field ===\nThis invention relates to project management systems and methods, more particularly to a software-based system and method for project and workforce management.<ref>{{Citation|title = Project workforce management Technical Field|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref>\n\n=== Software Usage ===\nDue to the software usage, all the project workflow management tasks can be fully automated without leaving many tasks for the project managers. This returns high efficiency to the project management when it comes to project tracking proposes. In addition to different tracking mechanisms, project workforce management software also offer a dashboard for the project team. Through the dashboard, the project team has a glance view of the overall progress of the project elements.\n\nMost of the times, project workforce management software can work with the existing legacy software systems such as ERP (enterprise resource planning) systems. This easy integration allows the organisation to use a combination of software systems for management purposes.<ref>{{Citation|title = Project workforce management Software Use|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref>\n\n=== Background ===\nGood project management is an important factor for the success of a project. A project may be thought of as a collection of activities and tasks designed to achieve a specific goal of the organisation, with specific performance or quality requirements while meeting any subject time and cost constraints. Project management refers to managing the activities that lead to the successful completion of a project. Furthermore, it focuses on finite deadlines and objectives. A number of tools may be used to assist with this as well as with assessment.\n\nProject management may be used when planning personnel resources and capabilities. The project may be linked to the objects in a professional services life cycle and may accompany the objects from the opportunity over quotation, contract, time and expense recording, billing, period-end-activities to the final reporting. Naturally the project gets even more detailed when moving through this cycle.<ref>{{Citation|title = Project workforce management background|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref>\n\nFor any given project, several project tasks should be defined. Project tasks describe the activities and phases that have to be performed in the project such as writing of layouts, customising, testing. What is needed is a system that allows project positions to be correlated with project tasks. Project positions describe project roles like project manager, consultant, tester, etc. Project-positions are typically arranged linearly within the project. By correlating project tasks with project positions, the qualifications and availability of personnel assigned to the project positions may be considered.\n\n== Benefits of Project Management ==\n<ref>{{Cite web|title = The advantages of project management and how it can help your business|url = https://www.nibusinessinfo.co.uk/content/advantages-project-management-and-how-it-can-help-your-business|website = nibusinessinfo.co.uk|accessdate = 2015-11-04|last = Migrator}}</ref> Good project management should:\n* Reduce the chance of a project failing\n* Ensure a minimum level of quality and that results meet requirements and expectations\n* Free up other staff members to get on with their area of work and increase efficiency both on the project and within the business\n* Make things simpler and easier for staff with a single point of contact running the overall project\n* Encourage consistent communications amongst staff and suppliers\n* Keep costs, timeframes and resources to budget\n\n== Workflow Engine ==\nWhen it comes to project workforce management, it is all about managing all the logistic aspects of a project or an organisation through a software application. Usually, this software has a workflow engine defined in them. So, all the logistic processes take place in the workflow engine.\n\nThe regular and most common types of tasks handled by project workforce management software or a similar workflow engine are:\n\n=== Planning and Monitoring the Project Schedules and Milestones ===\nRegularly monitoring your project’s schedule performance can provide early indications of possible activity-coordination problems, resource conflicts, and possible cost overruns. To monitor schedule performance. Collecting information and evaluating it ensure a project accuracy.<ref>{{Cite web|title = How to Monitor Project-Schedule Performance - For Dummies|url = http://www.dummies.com/how-to/content/how-to-monitor-schedule-performance.html|website = www.dummies.com|accessdate = 2015-11-04}}</ref>\n\n=== Tracking the Cost and Revenue aspects of Projects ===\nThe importance of tracking actual costs and resource usage in projects depends upon the project situation.\n\nTracking actual costs and resource usage is an essential aspect of the project control function.<ref>{{Cite web|title = Why Track Actual Costs and Resource Usage on Projects?|url = http://www.projecttimes.com/articles/why-track-actual-costs-and-resource-usage-on-projects.html|website = www.projecttimes.com|accessdate = 2015-11-04}}</ref>\n\n=== Resource Utilisation and Monitoring ===\nOrganisational profitability is directly connected to project management efficiency and optimal resource utilisation.To sum up, organisations that struggle with either or both of these core competencies typically experience cost overruns, schedule delays and unhappy customers.<ref>{{Cite web|title = Resource Utilization in Project Management|url = https://www.clarizen.com/work/resource-utilization-in-project-management|website = www.clarizen.com|accessdate = 2015-11-04}}</ref>\n\nThe focus for project management is the analysis of project performance to determine whether a change is needed in the plan for the remaining project activities to achieve the project goals.<ref>{{Cite web|title = Project Management Guru Monitoring and Controlling Tools|url = http://www.projectmanagementguru.com/controlling.html|website = www.projectmanagementguru.com|accessdate = 2015-11-04}}</ref>\n\n=== Other Management Aspects of the Project Management<ref>{{Cite web|title = Project Management Guide - How to Manage a Project {{!}} TeamGantt|url = http://teamgantt.com/guide-to-project-management/|website = teamgantt.com|accessdate = 2015-11-04}}</ref> ===\n\n==== Project risk management ====\nRisk identification consists of determining which risks are likely to affect the project and documenting the characteristics of each.\n\n==== Project communication management ====\nProject communication management is about how communication is carried out during the course of the project\n\n==== Project quality management ====\nIt is of no use completing a project within the set time and budget if the final product is of poor quality. The project manager has to ensure that the final product meets the quality expectations of the stakeholders. This is done by good: \x83\n\n===== ''Quality Planning:'' =====\nIdentifying what quality standards are relevant to the project and determining how to meet them.\n\n===== ''Quality Assurance:'' =====\nEvaluating overall project performance on a regular basis to provide confidence that the project will satisfy the relevant quality standards.\n\n===== ''Quality Control:'' =====\n\nMonitoring specific project results to determine if they comply with relevant quality standards and identifying ways to remove causes of poor performance.\n\n==Project Workforce Management vs. Traditional Management==\nThere are three main differences between Project Workforce Management and traditional [[project management]] and [[workforce management]] disciplines and solutions:<ref>{{Cite web\n|author = Rudolf Melik|title = The Rise of the Project Workforce|url = https://books.google.co.uk/books?id=0b2RB81RqyQC&pg=PA121&lpg=PA121&dq=the+rise+of+project+workforce+pdf&source=bl&ots=_Io_xYQd2Q&sig=4KO0i1Gr5m_XoybVwJHqfP0enHk&hl=en&sa=X&ved=0CDIQ6AEwBGoVChMImJOZ3a33yAIVQ70aCh1yGAq4#v=onepage&q=the%20rise%20of%20project%20workforce%20pdf&f=false|publisher = Wiley: New York, NY|year = 2007|accessdate = November 4, 2015}}</ref>\n\n=== Workflow-driven ===\nAll project and workforce processes are designed, controlled and audited using a built-in graphical workflow engine. Users can design, control and audit the different processes involved in the project. The graphical workflow is quite attractive for the users of the system and allows the users to have a clear idea of the workflow engine.<ref>{{Cite book|title = Flexibility of Data-Driven Process Structures|url = http://link.springer.com/chapter/10.1007/11837862_19|publisher = Springer Berlin Heidelberg|date = 2006-09-04|isbn = 978-3-540-38444-1|pages = 181–192|series = Lecture Notes in Computer Science|first = Dominic|last = Müller|first2 = Manfred|last2 = Reichert|first3 = Joachim|last3 = Herbst|editor-first = Johann|editor-last = Eder|editor-first2 = Schahram|editor-last2 = Dustdar}}</ref>\n\n=== Organisation and Work Breakdown Structures ===\nProject Workforce Management provides organization and work breakdown structures to create, manage and report on functional and approval hierarchies, and to track information at any level of detail. Users can create, manage, edit and report work breakdown structures. Work breakdown structures have different abstraction levels, so the information can be tracked at any level. Usually, project workforce management has approval hierarchies. Each workflow created will go through several records before it becomes an organisational or project standard. This helps the organisation to reduce the inefficiencies of the process, as it is audited by many stakeholders.<ref>{{Cite web|title = Organisational Breakdown Structure|url = http://www.successful-project-management.com/organisational-breakdown-structure.html|website = www.successful-project-management.com|accessdate = 2015-11-04}}</ref>\n\n=== Connected Project, Workforce and Financial Processes ===\nUnlike traditional disconnected project, workforce and billing management systems that are solely focused on tracking IT projects, internal workforce costs or billable projects, Project Workforce Management is designed to unify the coordination of all project and workforce processes, whether internal, shared (IT) or billable.\n\n== Summary ==\nA project workforce management system defines project tasks, project positions and assigns personnel to the project positions. The project tasks and project positions are correlated to assign a responsible project position or positions to complete each project task. Because each project position may be assigned to a specific person, the qualification and availabilities of the person can be taken into account when determining the assignment. By correlating the project tasks and project positions, a manager can better control the assignment of the workforce and complete projects more efficiently.<ref>{{Citation|title = Project workforce management abstract|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref>\n\nProject workflow management is one of the best methods for managing different aspects of project. If the project is complex, then the outcomes for the project workforce management could be more effective.\n\nFor simple projects or small organisations, project workflow management may not add much value, but for more complex projects and big organisations, managing project workflow will make a big difference. This is because that small organisations or projects do not have a significant overhead when it comes to managing processes. There are many project workforce management, but many organisations prefer to adopt unique solutions.\n\nTherefore, organisation gets software development companies to develop custom project workflow managing systems for them. This has proved to be the most suitable way of getting the best project workforce management system acquired for the company.\n\n==Literature==\n*{{Cite book\n | first = Rudolf\n | last = Melik\n | authorlink =\n | year = 2007\n | title = The Rise of the Project Workforce\n | edition =\n | publisher = Willey\n | location = New York, NY\n | isbn = 0-470-12430-X\n}}\n\n==References==\n{{Wikiquote}}\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:ERP software]]\n[[Category:Project management]]\n[[Category:Workflow technology]]"]
['Technical data management system', '20092666', '{{Orphan|date=February 2009}}\nA \'\'\'Technical Data Management System\'\'\' (TDMS) is essentially a [[Document management system]] (DMS) pertaining to the management of technical and [[engineering drawing]]s and documents. Often the data are contained in \'records\' of various forms, such as on paper, microfilms or digital media. Hence technical [[data management]] is also concerned with record management involving technical data. Proper Technical Document [[Management system|Management Systems]] are essential for executions within large organisations with large scale projects involving engineering. For example, TDMS is a vital function for the successful management of Integrated Steel Plants (ISP), Automobile factories, Aero-space facilities, Infrastructure companies, City Corporations, Research Organisations, etc. In such organisations, Technical Archives or Technical Documentation Centres are created as central facilities for effective management of technical data and records.\n[[File:Information processing system (english).svg|alt= A simplified example of information flow within a Technical Data Management System|thumb|A simplified example of information flow within a Technical Data Management System]]\nTDMS functions are similar to that of conventional archive functions in concepts, except that the archived materials in this case are essentially engineering drawings, survey maps, [[Specification|technical specifications]], plant and equipment data sheets, feasibility reports, project reports, operation and maintenance manuals, standards, etc.\n\nDocument registration, indexing, repository management, reprography, etc. are parts of TDMS.  Various kinds of sophisticated technologies such as document scanners, microfilming and digitization camera units, wide format printers, digital plotters, software, etc. are available now, making TDMS functions an easier process than previous times.\n\n== Crucial Constituents of a Technical Data Management System ==\nTechnical data refers to both scientific and technical information recorded and presented in any form or manner (excluding financial and management information).<ref>{{Cite web|url = http://www.businessdictionary.com/definition/technical-data.html|title = What is technical data? Definition and meaning|date = 2015-11-03|accessdate = 2015-11-03|website = BusinessDictionary.com|publisher = WebFinance, Inc|last = |first = }}</ref> A Technical Data Management System is created within an organisation for archiving and sharing information such as [[technical specifications]], datasheets and drawings. Similar to other types of data management system, a Technical Data Management System consists of the 4 crucial constituents mentioned below.\n\n=== Data planning ===\nData plans (long-term or short-term) are constructed as the first essential step of a proper and complete TDMS. It is created to ultimately help with the 3 other constituents, Data Acquisition, Data Management and Data sharing. A proper data plan should not exceed 2 pages and should address the following basics:<ref>{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-planning|title = Data planning|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}</ref>\n* Types of data (samples, experiment results, reports, drawings, etc.) and [[Metadata]] (Data that summarizes and describes other data. In this case, it refers to details such as sample sizes, experiment conditions and procedures, dates of reports, explanations of drawings, etc.)<ref>{{Cite web|url = http://whatis.techtarget.com/definition/metadata|title = metadata|date = July 2014|accessdate = 2015-11-03|website = WhatIs.com|publisher = Search engine optimization (SEO)|last = Rouse|first = Margaret}}</ref> \n* Means of researches and collections of data (field works, experiments in production lines, etc.)\n* Costs of researches\n* Policies for access, sharing (re-use within the organisation and re-distribution to the public)\n* Proposals for archiving data and maintaining access to it\n\n=== Data Acquisition ===\nRaw Data is collected from Primary Sites of the organisations through the use of modern Technologies.<ref name=":0">{{Cite web|url = http://sine.ni.com/cs/app/doc/p/id/cs-13019#|title = By using powerful default components, TDM, NI DataFinder, and DIAdem, and without using a database, we considerably reduced our creation and maintenance costs.|date = 2015-11-03|accessdate = 2015-11-03|website = National Instruments|publisher = a-solution GmbH|last = Finkl|first = Karl}}</ref> Please reference the table below for examples.<ref name=":0" />\n{| class="wikitable"\n!Organisations\n!Raw Data\n!Primary Sites\n!Technologies\n|-\n|Integrated steel plants, Automobile factories\n|Feasibility reports, Equipment datasheets, etc.\n|Test rigs and Controls\n|Transiting software to digitize data and Input software for recording report results and details on datasheets\n|-\n|Aero-space facilities\n|Engineering drawings, Operation manuals, maintenance logs, etc.\n|Engineering labs\n|Scanners for engineering drawings, Input software for maintenance logs\n|-\n|City corporations\n|Survey maps, Population reports, etc.\n|City to be mapped and City that involves the research\n|Digital cameras for survey maps, Input software for statistics of population\n|}\nThe data collected is then transferred to Technical Data Centres for Data Management.\n\n=== Data Management ===\nAfter Data Acquisition, data is sorted out, whilst useful data is archived, unwanted data is disposed. When managing and archiving data, the features below of the data are considered.<ref>{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-management|title = Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}</ref>\n* Names, labels, values and descriptions for variables and records. (In the case of TDMS, one example is names of equipments on an equipment datasheet)\n* Derived data from the original data, with code, algorithm or command file used to create them. (In the case of TDMS, one example is an expectation report derived from the analysis of an equipment datasheet)\n* [[Metadata]] associates with the data being archived\n\n=== Data Sharing ===\nArchived and managed data are accessible to rightful entities. A proper and complete TDMS should share data to a suitable extent, under suitable security, in order to achieve optimal usage of data within the organisation. It aims for easy access when reused by other researchers and hence it enhances other research processes. Data is often referred in other tests and [[Specification (technical standard)|technical specifications]], where new analysis is generated, managed and archived again. As a result, data is flowing within the organisation under effective management through the use of TDMS.<ref>{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-sharing|title = Data Sharing|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}</ref>\n\n== Advantages and disadvantages of usage of Technical Data Management Systems ==\nThere are strengths and weakness when using Technical Data Management Systems (TDMS) to archive data. Some of the advantages and disadvantages are listed below.<ref>{{Cite web|url = https://razorleaf.com/solutions/technologies/product-data-management/|title = Product Data Management / Technical Data Management (PDM/TDM)|date = 2015-11-03|accessdate = 2015-11-03|website = Razorleaf Solutions|publisher = Razorleaf Corporation|last = |first = }}</ref><ref>{{Cite web|url = http://arxiv.org/ftp/arxiv/papers/1008/1008.1321.pdf|title = Contributions of PDM Systems in Organiza- tional Technical Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = |publisher = Mechanical Engineering Informatics and Virtual Product Development Division (MIVP),  Vienna University of Technology|last1 = Ahmed|first1 = Zeeshan|last2 = Gerhard|first2 = Detlef}}</ref><ref>{{Cite web|url = http://www.flosim.com/calcium.aspx|title = Calcium - technical data management|date = 2015-11-03|accessdate = 2015-11-03|website = Flow Simulation|publisher = Flow Simulation Ltd.|last = |first = }}</ref>\n\n=== Advantages ===\n\n==== 1. Faster and easier data management ====\n\nSince TDMS is integrated into the organisation\'s systems, whenever workers develop data files (SolidWorks, AutoCAD, Microsoft Word, etc.), they can also archive and manage data, linking what they need to their current work, at the same time they can also update the archives with useful data. This speeds up working processes and makes them more efficient.\n\n==== 2. Increased security ====\n\nAll data files are centralized, hence internal and external data leakages are less likely to happen, and the data flow is more closely monitored. As a result, data in the organisation is more secured.\n\n==== 3. Increased collaboration within the organisation ====\n\nSince the data files are centralized and the data flow within the organisation increases, researchers and workers within the organisation are able to work on joint projects. More complex tasks can be performed for higher yields.\n\n==== 4. Compatible to various formats of data ====\n\nTDMS is compatible to many formats of data, from basic data like Microsoft Words to complex data like voice data. This enhances the quality of the management of data archived.\n\n=== Disadvantages ===\n\n==== 1. Higher financial costs ====\n\nImplementing TDMS into the organisation\'s systems involves monetary costs. Maintenance costs certain amount of human resources and money as well. These resources involve opportunity costs as they can be utilized in other aspects.\n\n==== 2. Lower stability ====\n\nSince TDMS manages and centralizes all the data the organisation processes, it links the working processes within the whole organisation together. It also increases the vulnerability of the organisation data network. If TDMS is not stable enough or when it is exposed to hacker and virus attacks, the organisation\'s data flow might shut down completely, affecting the work in an organisation-wide scale and leading to a lower stability as results.\n\n== Comparison between Traditional Data Management Approaches and Technical Data Management Systems ==\nTest engineers and researchers are facing great challenges in turning complex test results and simulation data into usable information for higher yields of firms. These challenges are listed below.<ref>{{Cite web|url = http://www.ni.com/white-paper/7389/en/|title = From Raw Data to Engineering Results: The NI Technical Data Management Solution|date = 2015-10-13|accessdate = 2015-11-03|website = |publisher = National Instruments|last = |first = }}</ref>\n* Increase in complication of designs\n* Reduced in time and budgets available\n* Higher quality is demanded\n[[File:Logo oracle.jpg|alt= A company logo for Oracle|thumb|A company logo for Oracle]]\n\n=== Traditional Data Management Approaches ===\nMany organisations are still applying the conventional file management systems, due to the difficulty in building a proper and complete archives for data management.\n\nThe first approach is the simple file-folder system. This costs the problem of ineffectiveness as workers and researchers have to manually go through numerous layers of systems and files for the target data. Moreover, the target data may contain files with different formats and these files may not be stored in the same machine. These files are also easily lost if renamed or moved to another location.\n\nThe second approach is conventional databases such as Oracle. These databases are capable of enabling easy search and access of data. However, a great drawback is that huge effort for preparing and modeling the data is required. For large-scale projects, huge monetary costs are induced, and extra IT human resources must be employed for constant handling, expanding and maintaining the inflexible system, which is custom for specific tasks, instead of all tasks. In the long-term, it is not cost-effective.\n\n=== Technical Data Management Systems(TDMS) ===\nTDMS is developed based on 3 principles, flexible and organized file storage, self-scaling hybrid data index, and an interactive post-processing environment. The system in practical, mainly consists of 3 components, data files with essential and relevant [[Metadata]], data finders for organizing and managing data regardless of files formats, and, a software of searching, analyzing and reporting. With [[Metadata]] attached to original data files, the data finder can identify different related data files during searches, even if they are in different file formats. TDMS hence allows researchers to search for data like browsing the Internet. Last but not least, it can adapt to changes and update itself according to the changes, unlike databases.\n\n== Comparison between Strong Information Systems and Weak Information Systems ==\nComplex organizations may need large amounts of technical information, which can be distributed among several independent archives. Existing approaches span from “no integration” to “strong integration”, that is based on a common database or product model. The so-called “Weak Information Systems” (WIS)<ref>{{cite conference |url=http://www.marcolazzari.it/publications/weak-information-systems-for-technical-data-management-preprint.pdf |title=Weak information systems for technical data management |first= |last1=Salvaneschi |first1=Paolo |last2=Lazzari |first2=Marco |year=1997 |conference=Worldwide ECCE Symposium on computers in the practice of building and civil engineering |location=Lahti, Finland |pages=310–314 |access-date=2015-11-29 }}</ref> lie somewhere in the middle. Their basic concept is to add to the pre-existing information a new layer of multiple partial models of products and processes, so that it is possible to reuse existing databases, to reduce the development from scratch, and to provide evolutionary paths relevant for the development of the WIS. Each partial model may include specific knowledge and it acts as a way to structure and access the information according to a specific user view.\nThe comparison between strong and weak information systems may be summarized as follows:\n{| class="wikitable"\n!Strong information systems\n!Weak information systems\n|-\n|Common data model\n|Multiple specific integration models\n|-\n|Database oriented architecture\n|Integration of multiple data sources by adding integration layers\n|-\n|One shot design\n|Growing process\n|-\n|Redesign of legacy systems\n|Integration of legacy systems\n|}\nThe architecture of a weak information  system is composed of:\n* information sources (databases, computational programs, ...);\n* the integration layer.\nThe integration layer comprises the following sub-layers:\n* abstraction layer (information models);\n* communication layer between models and information sources;\n* communication layer between models and humans (human-computer interface).\n\n== Technical Data Management Systems in terms of regulations in different countries ==\nIn some countries, such as in the US, record and document management are considered very vital functions, and much stress is given in the management of Technical Archives. Records and documents coming under the public domain are governed by appropriate laws.<ref>{{Cite web|url = http://apps.americanbar.org/lpm/lpt/articles/tch01093.shtml|title = Document Management in the Digital Law Office|date = January 2009|accessdate = 2015-11-03|website = Law Practice Today|publisher = American Bar Association|last1 = Best|last2 = Foster|first1 = Steven J.|first2 = Debbie}}</ref> However, this has not been so in many underdeveloped and [[Developing country|developing nations]]. For example, India enacted the \' Public Records Act\'<ref>{{Cite web|url = http://nationalarchives.nic.in/writereaddata/html_en_files/html/public_records93.html|title = THE PUBLIC RECORDS ACT, 1993 (India)|date = 1993-12-22|accessdate = 2015-11-03|website = |publisher = Government of India|last = MOHANPURIA|first = K.L.}}</ref> in 1993. However, many in the country are not aware of the existence of such a law or its importance.\n\n== Applications and Examples of Technical Data Management Systems ==\nTechnical Data Management Systems (TDMS) are widely applied across the globe, in different sectors. Some of the examples are listed below.\n* Voith Hydro tests models of the power plant turbines, including 4 main program parts, engine characteristics values, oscillation and cavitation, and transfer data from 1 program part to the next one using TDMS.<ref name=":0" />\n* Danburykline created a knowledge and data platform, SOROS, which is following the wiki based approach. It aims to represent data in accessible and simple forms.<ref>{{Cite web|url = http://danburykline.co.uk/DKWP/?page_id=967|title = Knowledge & Technical Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = |publisher = Danburykline|last = |first = }}</ref>\n* Berghof develops and provides a TDMS to simplify and manage data for development of firms including automobile firms. This TDMS enables reserve of data, centralization of data volumes on an online server. It is also compatible to Windows PC and many other systems.<ref>{{Cite web|url = http://www.berghof.com/en/products/test-engineering/technical-data-management/|title = Data availability|date = 2015-11-03|accessdate = 2015-11-03|website = Test engineering Technical data management|publisher = Berghof|last = |first = }}</ref>\n* This journal proposes the use of [[Cloud database|Cloud]] TDMS in third world countries for higher education purposes. Republic of Sudan is the model in this journal. Some of the solutions mentioned include online course delivery and online assignments and tests for greater class participation. Weaknesses mentioned include high financial costs and the fact that underdeveloped countries have not enough infrastructure to support such proposal.<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7315ijdms02.pdf|title = Cloud Computing Architecture for higher education in the third world countries (Republic of the Sudan as model)|last = Adrees|first = Mohmed Sirelkhtem|date = June 2015|journal = International Journal of Database Management Systems ( IJDMS )|doi = 10.5121/ijdms.2015.7302|pmid = |access-date = 2015-11-03|volume = 7|last3 = Sheta|last2 = Omer|first2 = Majzoob Kamal Aldein|first3 = Osama E.|issue = 3}}</ref>\n* This journal is about [[text simplification]]. The purpose of this text simplification project in the journal is to simplify high level knowledge in English, so that students in high level studies who do not have sufficient English foundations can learn about these knowledge and data more easily. The method to do so suggested by the journal is to introduce a TDMS that can transform complicated English words into easier words. A problem with this project is that the Internet is flooded with useless information and it is very difficult to sort out useful information for simplification.<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7415ijdms01.pdf|title = Software feasibility study to transform complex scientific written knowledge to a clear, rationale and simple language|last = Khandelwal|first = Manoj|date = August 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7401|pmid = |access-date = 2015-11-03|last2 = Jafarabad|first2 = Mohammad|issue = 4|volume = 7}}</ref>\n* This journal mentions about [[River basin|River Basin]] Information System (RBIS), which monitors data of different parts of a river basin, in order to identify which parts of the basin are gauging. Data is dynamic and lots of information has to be taken, which is impossible to do it manually. RBIS can help with this but one current weakness is that there are only 2 synoptic stations working (Kara and Niamtougou), whilst the rest are out of order.<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms02.pdf|title = An information for integrated land and water resources management in the Kara River Basin (Togo and Benin)|last = BADJANA|first = Hèou Maléki|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7102|pmid = |access-date = 2015-11-03|last2 = ZANDER|first2 = Franziska|issue = 1|volume = 7|last3 = KRALISCH|first3 = Sven|last4 = HELMSCHROT|first4 = Jörg|last5 = FLÜGEL|first5 = Wolfgang-Albert}}</ref>\n\n== Data mining ==\nData mining is an important criteria in constructing a technical Data Management System. For example, in building a E-commence platform, TDMS is needed to search and display information about the products.<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms01.pdf|title = Web-mining on Indonesia E-commerce site: Lazada and Rakuten|last = Simanjuntak|first = Humasak|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7101|pmid = |access-date = 2015-11-03|last2 = Sibarani|first2 = Novitasari|issue = 1|volume = 7|last3 = inaga|first3 = Bambang|last4 = Hutabarat|first4 = Novalina}}</ref> It indicates that it is essential to gather information from other systems, to archive and manage it properly, and finally, to share it to users.\n\n== See also ==\n[[Data management system|Data Management System]]\n\n[[Data mining]]\n\n[[Database]]\n\n[[Information Systems Research]]\n\n== Further reading ==\nhttp://airccse.org/journal/ijdms/papers/7115ijdms03.pdf<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms03.pdf|title = Mining closed sequential patterns in large sequence databases|last = Raju|first = V. Purushothama|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7103|pmid = |access-date = 2015-11-03|last2 = Varma|first2 = G.P. Saradhi|issue = 1|volume = 7}}</ref>\n\nhttps://seer.lcc.ufmg.br/index.php/jidm<ref>{{Cite journal|url = https://seer.lcc.ufmg.br/index.php/jidm|title = JOURNAL OF INFORMATION AND DATA MANAGEMENT|date = February 2015|journal = JOURNAL OF INFORMATION AND DATA MANAGEMENT|doi = |pmid = |access-date = 2015-11-03|volume = 6|issue = 1|editor-last = Traina Junior|editor-first = Caetano|editor2-last = Cordeiro|editor2-first = Robson L. F.|editor3-last = Amo|editor3-first = Sandra de|display-editors = 3 |editor4-last = Davis|editor4-first = Clodoveu|issn = 2178-7107}}</ref>\n\n== External links ==\n* http://airccse.org/journal/ijdms/Editorialboard.html\n\n==References==\n\n<references>\n</references>\n\n<nowiki/>\n\n[[Category:Data management]]\n[[Category:Document management systems]]\n[[Category:Systems engineering]]']
['Category:Data quality', '31206312', "{{Cat main|Data quality}}\n:''See also:'' [[:category:Data security]] ([[data loss]] prevention is in fact an assurance of data quality)\n\n\n[[Category:Data management|Quality]]\n[[Category:Quality]]"]
['State transition network', '31261582', "{{unreferenced|date=March 2011}}\n\nA '''state transition network''' is a [[diagram]] that is developed from a set of data and charts the [[data flow|flow of data]] from particular data points (called states or nodes) to the next in a probabilistic manner.\n\n==Use==\nState transition networks are used in both [[academic]] and [[industry|industrial]] fields. \n\n==Examples==\nState transition networks are a general construct, with more specific examples being augmented transition networks, recursive transition networks, and augmented recursive networks, among others.\n==See also==\n* [[State transition system]]\n* [[Markov network]]\n* [[History monoid]]\n\n==References==\n{{reflist}}\n\n[[Category:Data management]]"]
['Super column', '31220085', '[[Image:SuperColumn (data store).png|300px|thumb|The super column consists of a (unique) super column name, and a number of columns.]]\nA \'\'\'super column\'\'\' is a [[tuple]] (a pair) with a binary super column name and a value that maps it to many columns.<ref>{{cite web\n| accessdate = 2011-03-18\n| author = Arin Sarkissian\n| date = 2009-09-01\n| location = http://arin.me/post/40054651676/wtf-is-a-supercolumn-cassandra-data-model\n| publisher = Arin Sarkissian\n| title = WTF is a SuperColumn? An Intro to the Cassandra Data Model\n| quote = A SuperColumn is a tuple with a binary name & a value which is a map containing an unbounded number of Columns – keyed by the Column‘s name.\n| url = }}</ref> They consist of a key-value pairs, where the values are columns. Theoretically speaking, super columns are ([[Sorting algorithm|sorted]]) [[associative array]] of columns.<ref>{{cite web\n| accessdate = 2011-03-18\n| location = http://wiki.apache.org/cassandra/DataModel\n| publisher = Apache Cassandra\n| title = Cassandra wiki: Data Model: Super columns\n| url = http://wiki.apache.org/cassandra/DataModel}}</ref> Similar to a regular [[column family]] where a row is a sorted map of column names and column values, a row in a super column family is a sorted map of super column names that maps to column names and column values. \n\nA super column is part of a [[keyspace (data model)]] together with other super columns and column families, and columns.\n\n==Code example==\nWritten in the [[JSON]]-like syntax, a super column definition can be like this:\n\n<source lang="SQL">\n {\n   "mccv": {\n     "Tags": {\n       "cassandra": {\n         "incubator": {"url": "http://incubator.apache.org/cassandra/"},\n         "jira": {"url": "http://issues.apache.org/jira/browse/CASSANDRA"}\n       },\n       "thrift": {\n         "jira": {"url": "http://issues.apache.org/jira/browse/THRIFT"}\n       }\n     }\n   }\n }\n</source>\n\n==See also==\n* [[Column (data store)]]\n* [[Keyspace (NoSQL)]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://wiki.apache.org/cassandra/DataModel The Apache Cassandra data model]\n\n<!--Interwikies-->\n[[Category:Data_management]]\n<!--Categories-->\n\n\n{{database-stub}}']
['H-Store', '32670994', "{{Infobox software\n| name                   = H-Store\n| logo                   = [[File:H-Store-logo.png|80px|H-Store logo]]\n| screenshot             =\n| caption                =\n| developer              = [[Brown University|Brown]], [[Carnegie Mellon University|CMU]], [[Massachusetts Institute of Technology|MIT]], [[Yale University|Yale]]\n| latest release version = June 2016\n| latest release date    = {{Start date and age|2016|06|03}}\n| programming language   = [[C++]], [[Java (programming language)|Java]]\n| operating system       = [[Linux]], [[Mac OS X]]\n| genre                  = [[Database Management System]]\n| license                = [[BSD License]], [[GPL]]\n| website                = {{URL|hstore.cs.brown.edu}}\n}}\n\n'''H-Store''' is an experimental [[database management system]] (DBMS) designed for [[online transaction processing]] applications that is being developed by a team at [[Brown University]], [[Carnegie Mellon University]], the [[Massachusetts Institute of Technology]], and [[Yale University]].<ref>\n{{cite web \n| url = http://hstore.cs.brown.edu\n| title = H-Store - Next Generation OLTP DBMS Research\n| accessdate = 2011-08-07\n}}\n</ref><ref>\n{{cite web \n| url = http://www.dbms2.com/2008/02/18/mike-stonebraker-calls-for-the-complete-destruction-of-the-old-dbms-order/\n| title = Stonebraker's H-Store: There's something happenin' here\n| first = David\n| last = Van Couvering\n\n| date = 2008-02-18\n| <!-- is this one just plain wrong?:--> publication-date = 2011-03-11\n| accessdate = 2012-07-18\n}}\n</ref>\nThe system's design was developed in 2007 by database researchers [[Michael Stonebraker]], [[Samuel Madden (MIT)|Sam Madden]], Andy Pavlo and Daniel Abadi.<ref>\n{{cite conference\n| authorlink = Michael Stonebraker\n| first = Mike | last = Stonebraker\n| title = The end of an architectural era: (it's time for a complete rewrite)\n| booktitle = VLDB '07: Proceedings of the 33rd international conference on Very large data bases\n| location = Vienna, Austria\n| year = 2007\n| url = http://hstore.cs.brown.edu/papers/hstore-endofera.pdf\n| format = PDF |display-authors=etal}}</ref><ref>\n{{cite journal\n| last1 = Kallman\n| first1 = Robert\n| last2 = Kimura\n| first2 = Hideaki\n| last3 = Natkins\n| first3 = Jonathan\n| last4 = Pavlo\n| first4 = Andrew\n| last5 = Rasin\n| first5 = Alexander\n| last6 = Zdonik\n| first6 = Stanley\n| authorlink6 = Stan Zdonik\n| last7 = Jones\n| first7 = Evan P. C.\n| last8 = Madden\n| first8 = Samuel\n| authorlink8 = Samuel Madden (MIT)\n| last9 = Stonebraker\n| first9 = Michael\n| authorlink9 = Michael Stonebraker\n| last10 = Zhang\n| first10 = Yang\n| last11 = Hugg\n| first11 = John\n| last12 = Abadi\n| first12 = Daniel J.\n| title = H-Store: a high-performance, distributed main memory transaction processing system\n| journal = Proc. VLDB Endowment\n| year = 2008\n| volume = 1\n| series = 2\n| pages = 1496–1499\n| url = http://hstore.cs.brown.edu/papers/hstore-demo.pdf\n| issn = 2150-8097\n}}</ref><ref>\n{{cite web \n| url = http://www.dbms2.com/2008/02/18/mike-stonebraker-calls-for-the-complete-destruction-of-the-old-dbms-order/\n| title = Mike Stonebraker calls for the complete destruction of the old DBMS order\n| first = Curt\n| last = Monash\n| year = 2008\n| publication-date = 2008-02-18\n| accessdate  = 2012-07-18\n}}\n</ref>\n\n==Architecture==\nThe significance of the H-Store is that it is the first implementation of a new class of [[Parallel database|parallel database management systems]], called [[NewSQL]],<ref>{{cite web|url=http://www.cs.brown.edu/courses/cs227/papers/newsql/aslett-newsql.pdf |title=How Will The Database Incumbents Respond To NoSQL And NewSQL? |first=Matthew |last=Aslett |publisher=451 Group |publication-date=2011-04-04 |year=2010 |accessdate=2012-07-06 |deadurl=yes |archiveurl=https://web.archive.org/web/20120127202623/http://www.cs.brown.edu/courses/cs227/papers/newsql/aslett-newsql.pdf |archivedate=January 27, 2012 }}\n</ref><!-- good link, just not supporting H-Store directly, is supporting [[VoltDB]] that is related, but doesn not state the connection: <ref>\n{{cite web \n| url = http://cacm.acm.org/blogs/blog-cacm/109710-new-sql-an-alternative-to-nosql-and-old-sql-for-new-oltp-apps/fulltext\n| title = NewSQL: An Alternative to NoSQL and Old SQL for New OLTP Apps\n| first = Michael\n| last = Stonebraker\n| publisher = Communications of the ACM\n| publication-date = 2011-06-16\n| accessdate = 2012-07-06\n}}\n</ref> -->that provide the high-throughput and high-availability of [[NoSQL]] systems, but without giving up the [[ACID|transactional guarantees]] of a traditional DBMS.<ref>\n{{cite web \n| url = http://preferisco.blogspot.com/2008/03/h-store-new-architectural-era-or-just.html\n| title = H-Store - a new architectural era, or just a toy? \n| first = Nigel\n| last = Thomas\n\n| date = 2008-03-01\n| accessdate = 2012-07-05\n}}\n</ref>\nSuch systems are able to scale out horizontally across multiple machines to improve throughput, as opposed to moving to a more powerful, more expensive machine for a single-node system.<ref>\n{{cite web \n| url = http://blogs.the451group.com/information_management/2008/03/04/is-h-store-the-future-of-database-management-systems/\n| title = Is H-Store the future of database management systems?\n| first = Matthew\n| last = Aslett\n\n| date = 2008-03-04\n| accessdate  = 2012-07-05\n}}\n</ref>\n\nH-Store is able to execute [[transaction processing]] with high throughput by forgoing much of legacy architecture of [[IBM System R|System R]]-like systems. For example, H-Store was designed as a [[Parallel database|parallel]], row-storage relational DBMS that runs on a cluster of [[Shared nothing architecture|shared-nothing]], main memory executor nodes.<ref>\n{{cite web \n| url = http://hstore.cs.brown.edu/documentation/architecture-overview/\n| title = H-Store - Architecture Overview\n| accessdate  = 2011-08-07\n}}\n</ref>\nThe database is [[Partition (database)|partitioned]] into disjoint subsets that are assigned to a single-threaded execution engine assigned to one and only one [[Multi-core processor|core]] on a node. Each engine has exclusive access to all of the data at its partition. Because it is single-threaded, only one transaction at a time is able to access the data stored at its partition. Thus, there are no physical locks or latches in the system, and no transaction will stall waiting for another transaction once it is started.<ref>\n{{cite web \n| url = http://www.zdnet.com/blog/btl/h-store-complete-destruction-of-the-old-dbms-order/8055\n| title = H-Store: Complete destruction of the old DBMS order?\n| first = Larry\n| last = Dignan\n| year = 2008\n| accessdate  = 2012-07-05\n}}\n</ref>\n\n==Licensing==\nH-Store is licensed under the [[BSD license]] and [[GPL]] licenses. The commercial version of H-Store's design is [[VoltDB]].<ref>\n{{cite web\n| url         = http://www.dbms2.com/2009/06/22/h-store-horizontica-voltdb/\n| title       = H-Store is now VoltDB\n| first       = Curt\n| last        = Monash\n| year        = 2009\n| accessdate  = 2011-07-14\n| postscript  = \n}}\n</ref>\n\n==See also==\n{{Portal|Free software}}\n*[[VoltDB]]\n*[[C-Store]]\n*[[Transaction processing]]\n\n==References==\n{{Reflist}}\n\n==External links==\n\n[[Category:Data management]]\n[[Category:Distributed data stores]]\n[[Category:Free database management systems]]\n[[Category:NewSQL]]"]
['Information governance', '22723009', '{{Governance}}\n\n\'\'\'Information governance\'\'\', or \'\'\'IG\'\'\', is the set of multi-disciplinary structures, policies, procedures, processes and controls implemented to manage information at an enterprise level, supporting an organization\'s immediate and future regulatory, legal, risk, environmental and operational requirements. Information governance should determine the balance point between two potentially divergent organizational goals: extracting value from information and reducing the potential risk of information. Information governance reduces organizational risk in the fields of compliance, operational transparency, and reducing expenditures associated with e-discovery and litigation response. An organization can establish a consistent and logical framework for employees to handle data through their information governance policies and procedures. These policies guide proper behavior regarding how organizations and their employees handle electronically stored information ([[Electronically stored information (Federal Rules of Civil Procedure)|ESI]]).<ref>{{cite web|url=http://blogs.gartner.com/debra_logan/2010/01/11/what-is-information-governance-and-why-is-it-so-hard/|title=What is Information Governance? And Why is it So Hard? - Debra Logan|date=11 January 2010|publisher=}}</ref><ref>[Kooper, M., Maes, R., and Roos Lindgreen, E. (2011). On the governance of information: Introducing a new concept of governance to support the management of information. International Journal of Information Management, 31(3), 195-200]</ref>\n\nInformation governance encompasses more than traditional [[records management]].  It incorporates [[information security]] and protection, compliance, [[data governance]], [[electronic discovery]], [[risk management]], privacy, data storage and archiving, [[knowledge management]], business operations and management, audit, analytics, IT management, [[master data management]], [[enterprise architecture]], [[business intelligence]], [[big data]], [[data science]], and finance.<ref>{{cite web|url=http://iginitiative.com/igi-publishes-2014-annual-report/|title=IGI PUBLISHES 2014 ANNUAL REPORT - Information Governance Initiative|date=11 August 2014|publisher=}}</ref>\n\n==History==\n\n===Records management===\nRecords management deals with the creation, retention and storage and disposition of records.  A record can either be a physical, tangible object, or digital information such as a database, application data, and e-mail.  The [[records life-cycle|lifecycle]] was historically viewed as the point of creation to the eventual disposal of a record.  As data generation exploded in recent decades, and regulations and compliance issues increased, traditional records management failed to keep pace.  A more comprehensive platform for managing records and information became necessary to address all phases of the lifecycle, which led to the advent of information governance.<ref>http://www.arma.org/pdf/WhatIsRIM.pdf</ref>\n\nIn 2003 the Department of Health in England introduced the concept of broad-based information governance into the National Health Service, publishing version 1 of an online performance assessment tool with supporting guidance. The NHS IG Toolkit<ref>{{cite web|url=https://www.igt.hscic.gov.uk/|title=Home|publisher=}}</ref> is now used by over 30,000 NHS and partner organisations, supported by an e-learning platform with some 650,000 users.\n\nIn 2008, [[ARMA International]] introduced the Generally Accepted Recordkeeping Principles®, or "The Principles"<ref>{{cite web|url=http://www.arma.org/principles|title=Generally Accepted Recordkeeping Principles|publisher=}}</ref> and the subsequent "The Principles" Information Governance Maturity Model.<ref>http://www.arma.org/principles/metrics.cfm</ref> "The Principles" identify the critical hallmarks of information governance. As such, they apply to all sizes of organizations, in all types of industries, and in both the private and public sectors. Multi-national organizations can also use "The Principles" to establish consistent practices across a variety of business units. ARMA International recognized that a clear statement of "Generally Accepted Recordkeeping Principles®" ("The Principles") would guide:\n\n* CEOs in determining how to protect their organizations in the use of information assets;\n* Legislators in crafting legislation meant to hold organizations accountable; and\n* Records management professionals in designing comprehensive and effective records management programs.\n\nInformation governance goes beyond retention and disposition to include privacy, access controls, and other compliance issues.  In electronic discovery, or e-discovery, relevant data in the form of [[electronically stored information]] is searched for by attorneys and placed on [[legal hold]].  IG includes consideration of how this data is held and controlled for e-discovery, and also provides a platform for defensible disposition and compliance.  Additionally, [[metadata]] often accompanies electronically stored data and can be of great value to the enterprise if stored and managed correctly.\n\nWith all of these additional considerations that go beyond traditional records management, IG emerged as a platform for organizations to define policies at the enterprise level, across multiple jurisdictions.  IG then also provides for the enforcement of these policies into the various repositories of information, data, and records.\n\nA coalition of organizations known as Electronic Discovery Reference Model (EDRM), which was founded in 2005 to address issues related to electronic discovery and information governance, subsequently developed, as one of its projects, a resource called the Information Governance Reference Model (IGRM).<ref>{{cite web|author=EDRM|url=http://www.edrm.net/what-is-edrm|title=About EDRM|accessdate=2015-01-21}}</ref> In 2011, EDRM, in collaboration with ARMA International, published a white paper that describes \'\'How the Information Governance Reference Model (IGRM) Complements ARMA International’s Generally Accepted Recordkeeping Principles ("The Principles")\'\'<ref>{{cite book|last=White Paper|title=How the Information Governance Reference Model (IGRM)Complements ARMA International’s Generally Accepted Recordkeeping Principles|year=2011|publisher=EDRM and ARMA International|pages=15|url=http://www.edrm.net/wp-content/uploads/downloads/2011/12/White-Paper-EDRM-Information-Governance-Reference-Model-IGRM-and-ARMAs-GARP-Principles-12-7-2011.pdf|editor-last=Ledergerber|editor-first=Marcus}}</ref>  The IGRM illustrates the relationship between key stakeholders and the Information Lifecycle and highlights the transparency required to enable effective governance IGRM v3.0 Update: Privacy & Security Officers As Stakeholders.<ref>[http://www.edrm.net/download/all_projects/igrm/The-Final..-IGRM_v3.0Update-Whitepaper_Oct_2012.pdf IGRM v3.0 Update: Privacy & Security Officers As Stakeholders]</ref>\n\nUniversities and professional associations started to develop information governance training and education programmes. In 2010, Dr Elizabeth Lomas (who had been aligning RM with information security, assurance and risk management models throughout the 2000s) authored distance learning materials for Information Governance modules delivered internationally through Northumbria University. ARMA subsequently started to deliver an Information Governance certification. These initiatives have now been picked up by other Universities, e.g. San Jose State University offers a graduate certificate in information governance, information assurance, and cyber security, and has also incorporated a required course in information governance as part of their 100% online Master of Archives and Records Administration<ref>[http://ischool.sjsu.edu/programs/master-archives-records-administration-mara]]</ref> (MARA) degree program.\n\nIn 2014, [[John Wiley & Sons]] published the first textbook on information governance, "Information Governance: Concepts, Strategies, and Best Practices"<ref>{{cite book|url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118218302.html|title=Information Governance: Concepts, Strategies, and Best Practices|isbn=978-1-118-21830-3|date=April 2014|publisher=John Wiley & Sons}}</ref> by [[Robert Smallwood]]. Also in 2014, The Information Governance Conference,<ref>Information Governance Conference [http://www.infogovcon.com InfoGovCon.com]</ref> an annual conference on information governance best practices began and the Information Governance Model<ref>Information Governance Model [http://www.infogovmodel.com InfoGovModel.com]</ref> was launched at the inaugural event, it is now in use at over 1000 organizations worldwide.\n\n===Organizational structure===\nIn the past, records managers owned records management, perhaps within a compliance department at an enterprise.  In order to address the broader issues surrounding records management, several other key stakeholders must be involved.  Legal, IT, and Compliance tend to be the departments that touch information governance the most, though certainly other departments might seek representation.  Many enterprises create information governance committees to ensure that all necessary constituents are represented and that all relevant issues are addressed.<ref>{{cite web|url=http://www.law.com/jsp/cc/PubArticleFriendlyCC.jsp?id=1202533945005|title=From the Experts: Information Governance and Its Impact on Litigation|publisher=}}</ref>\n\n===Tools===\nTo address retention and disposition, Records Management and Enterprise Content Management applications were developed.  Sometimes detached search engines or homegrown policy definition tools were created.  These were often employed at a departmental or divisional level; rarely were tools used across the enterprise.  While these tools were used to define policies, they lacked the ability to enforce those policies.  Monitoring for compliance with policies was increasingly challenging. Since information governance addresses so much more than traditional records management, several software solutions have emerged to include the vast array of issues facing records managers.\n\nOther available tools include:\n* ARMA International [[Www.arma.org/nextlevel|Next Level Information Governance Assessment]] ( Based upon the Generally Accepted Recordkeeping Principles)\n* ARMA Generally Accepted Recordkeeping Principles<ref>ARMA International, [http://www.arma.org/r2/generally-accepted-br-recordkeeping-principles "The Principles"], \'\'ARMA International\'\'</ref>\n* EDRM Information Governance Reference Model<ref>EDRM, [http://www.edrm.net/projects/igrm "Information Governance Reference Model"], \'\'EDRM\'\'</ref>\n* Information Coalition Information Governance Model<ref>Information Coalition, [http://infocoalition.com/resources/models-methodologies/information-governance-model-infogovmodel "The Information Governance Model"], \'\'Information Coalition\'\'</ref>\n* NHS Information Governance Toolkit<ref>NHS, [https://www.igt.hscic.gov.uk/ "NHS Information Governance Toolkit"], \'\'NHS\'\'</ref>\n\n===Laws and regulations===\nKey to IG are the regulations and laws that help to define corporate policies.  Some of these regulations include:\n*The Foreign Account Tax Compliance Act, or [[Foreign Account Tax Compliance Act|FATCA]]<ref>{{cite web|url=http://www.irs.gov/businesses/corporations/article/0,,id=236667,00.html|title=Foreign Account Tax Compliance Act|publisher=}}</ref>\n*Payment Card Industry Data Security Standard, or [[Payment Card Industry Data Security Standard|PCI Compliance]]<ref>{{cite web|url=https://www.pcisecuritystandards.org/|title=Official PCI Security Standards Council Site - Verify PCI Compliance, Download Data Security and Credit Card Security Standards|publisher=}}</ref>\n*Health Insurance Portability and Accountability Act, or [[Health Insurance Portability and Accountability Act|HIPAA]]<ref>{{cite web|url=http://www.hhs.gov/hipaa/|title=Health Information Privacy|date=26 August 2015|publisher=}}</ref>\n*Financial Services Modernization Act of 1999, or [[Gramm–Leach–Bliley Act|GLBA]]<ref>{{cite web|url=https://www.congress.gov/bill/106th-congress/senate-bill/900|title=S.900 - Gramm-Leach-Bliley Act}}</ref>\n*Sarbanes–Oxley Act of 2002, or [[Sarbanes–Oxley|Sarbox or SOX]]<ref>{{cite web|url=https://www.sec.gov/about/laws/soa2002.pdf|title=Sarbanes–Oxley Act of 2002}}</ref>\n*[[Federal Rules of Civil Procedure]]\n\n===Guidelines===\n*[[MoReq2]]<ref>{{cite web|url=http://www.moreq2.eu/|title=Home - MoReq2|publisher=}}</ref>\n*MoReq2010<ref>{{cite web|url=http://moreq2010.eu/|title=Account Suspended|publisher=}}</ref>\n*[[ISO 15489 Information and documentation -- Records management|ISO 15489 Information and Documentation - Records Management]]<ref>{{cite web|url=http://www.iso.org/iso/catalogue_detail?csnumber=31908|title=ISO 15489-1:2001 - Information and documentation -- Records management -- Part 1: General|publisher=}}</ref>\n*DoD 5015.2, or [[Design Criteria Standard for Electronic Records Management Software Applications]]<ref>{{cite web|url=http://www.archives.gov/records-mgmt/initiatives/dod-standard-5015-2.html|title=DoD Standard 5015.2|publisher=}}</ref>\n\n==See also==\n*[[Data defined storage]]\n* [[Data governance]]\n*[[Electronic discovery]]\n*[[Enterprise content management]]\n*[[Information management]]\n*[[Information technology governance]]\n*[[Knowledge management]]\n*[[National archives]]\n*[[Records management]]\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [http://www.epa.gov/records/what/quest1.htm EPA 10 Reasons for RM]\n* [http://www.druva.com/resources/analyst-reports/governance-takes-central-role-enterprises-shift-to-mobile/]\n\n[[Category:Information governance|*]]\n[[Category:Information technology management]]\n[[Category:Content management systems]]\n[[Category:Public records]]\n[[Category:Data management]]']
['Electronic lab notebook', '1616185', 'An \'\'\'electronic lab notebook\'\'\' (also known as electronic laboratory notebook, or ELN) is a [[computer program]] designed to replace paper [[lab notebook|laboratory notebook]]s.  Lab notebooks in general are used by [[scientist]]s, [[engineer]]s, and [[technician]]s to document [[research]], [[experiment]]s, and procedures performed in a laboratory.  A lab notebook is often maintained to be a [[legal document]] and may be used in a [[court of law]] as [[evidence (law)|evidence]].  Similar to an [[inventor\'s notebook]], the lab notebook is also often referred to in [[patent]] prosecution and [[intellectual property]] [[litigation]].\n\nElectronic lab notebooks are a fairly new technology and offer many benefits to the user as well as organizations. For example: electronic lab notebooks are easier to search upon, simplify data copying and backups, and support collaboration amongst many users.<ref>{{cite conference |\ntitle = A Collaborative Electronic Notebook |\nfirst = James | \nlast = Myers |author2=Elena Mendoza |author3=Bonnie Hoopes |\njournal = Proceedings of the IASTED International Conference on Internet and Multimedia Systems and Applications |\nyear = 2001 \n}}</ref>  \nELNs can have fine-grained access controls, and can be more secure than their paper counterparts.<ref>{{\ncite conference | \nlast= Myers | \nfirst = James | \nyear = 2003 | \njournal = Proceedings of the 2003 International Symposium On Collaborative Technologies and Systems | \ntitle = Collaborative Electronic Notebooks as Electronic Records:Design Issues for the Secure Electronic Laboratory Notebook (ELN) | \nurl = http://collaboratory.emsl.pnl.gov/resources/publications/papers/seceln(final1)1-22Nov.pdf\n}}</ref>  They also allow the direct incorporation of data from instruments, replacing the practice of printing out data to be stapled into a paper notebook.<ref>{{Cite journal | last1 = Perkel | first1 = J. M. | title = Coding your way out of a problem | journal = Nature Methods | volume = 8 | issue = 7 | pages = 541–543 | year = 2011 | pmid = 21716280 | doi = 10.1038/nmeth.1631}}</ref>\n\n==Types==\nELNs can be divided into two categories:\n\n* "Specific ELNs" contain features designed to work with specific applications, scientific instrumentation or data types.\n* "[[Cross-disciplinary]] ELNs" or "Generic ELNs" are designed to support access to all data and information that needs to be recorded in a lab notebook.\n\nSolutions range from specialized programs designed from the ground up for use as an ELN, to modifications or direct use of more general programs.  Examples of using more general software include using [[OpenWetWare]], a [[MediaWiki]] install (running the same software that Wikipedia uses), as an ELN, or the use of general note taking software such as OneNote as an ELN.<ref>{{Cite journal | last1 = Perkel | first1 = J. M. | title = Coding your way out of a problem | journal = Nature Methods | volume = 8 | issue = 7 | pages = 541–543 | year = 2011 | pmid = 21716280 | doi = 10.1038/nmeth.1631}}.</ref>\n\nELN\'s come in many different forms. They can be standalone programs, use a client-server model, or be entirely web-based. Some use a lab-notebook approach, others resemble a blog.\n\nA good many variations on the "ELN" acronym have appeared.<ref>{{Cite web|url=http://cerf-notebook.com/articles/eln-glossary/|title=Lab Notebook (ELN) Glossary - CERF|date=2016-02-16|language=en-US|access-date=2016-08-20}}</ref> Differences between systems with different names are often subtle, with considerable functional overlap between them. Examples include "ERN" (Electronic Research Notebook), "ERMS" (Electronic Resource (or Research or Records) Management System (or Software) and SDMS (Scientific Data (or Document) Management System (or Software). Ultimately, these types of systems all strive to do the same thing: Capture, record, centralize and protect scientific data in a way that is highly searchable, historically accurate, and legally stringent, and which also promotes secure collaboration, greater efficiency, reduced mistakes and lowered total research costs.\n\n==Objectives==\nA good electronic laboratory notebook should offer a secure environment to protect the integrity of both data and process, whilst also affording the flexibility to adopt new processes or changes to existing processes without recourse to further software development. The package architecture should be a modular design, so as to offer the benefit of minimizing validation costs of any subsequent changes that you may wish to make in the future as your needs change.\n\nA good electronic laboratory notebook should be an "out of the box" solution that, as standard, has fully configurable forms to comply with the requirements of regulated analytical groups through to a sophisticated ELN for inclusion of structures, spectra, chromatograms, pictures, text, etc. where a preconfigured form is less appropriate. All data within the system may be stored in a database (e.g. MySQL, MS-SQL, Oracle) and be fully searchable. The system should enable data to be collected, stored and retrieved through any combination of forms or ELN that best meets the requirements of the user.\n\nThe application should enable secure forms to be generated that accept laboratory data input via PCs and/or laptops / palmtops, and should be directly linked to electronic devices such as laboratory balances, pH meters, etc.  Networked or wireless communications should be accommodated for by the package which will allow data to be interrogated, tabulated, checked, approved, stored and archived to comply with the latest regulatory guidance and legislation.  A system should also include a scheduling option for routine procedures such as equipment qualification and study related timelines. It should include configurable qualification requirements to automatically verify that instruments have been cleaned and calibrated within a specified time period, that reagents have been quality-checked and have not expired, and that workers are trained and authorized to use the equipment and perform the procedures.\n\n==Regulatory and legal aspects==\nThe laboratory accreditation criteria found in the [[ISO 17025]] standard needs to be considered for the protection and computer backup of electronic records. These criteria can be found specifically in clause 4.13.1.4 of the standard.<ref>"ISO/IEC 17025:2005 - General Requirements for the Competence of Testing and Calibration Laboratories." ISO - International Organization for Standardization. Web. 16 Nov. 2011. <http://www.iso.org/iso/Catalogue_detail?csnumber=39883>.</ref>\n\nElectronic lab notebooks used for development or research in regulated industries, such as medical devices or pharmaceuticals, are expected to comply with FDA regulations related to software validation.  The purpose of the regulations is to ensure the integrity of the entries in terms of time, authorship, and content.  Unlike ELNs for patent protection, FDA is not concerned with patent interference proceedings, but is concerned with avoidance of falsification.  Typical provisions related to software validation are included in the medical device regulations at 21 CFR 820 (et seq.)<ref>United States. Food and Drug Administration. Department of Health and Human Resources. 1 Food and Drugs - Subchapter H Medical Devices - Part 820 System RegCode of Federal Regulations - Title 2ulation. FDA.gov, 7 Oct. 1996. Web. <http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/cfrsearch.cfm?cfrpart=820>.</ref> and [[Title 21 CFR Part 11]].<ref>United States. Food and Drug Administration. Department of Health and Human Resources. Code of Federal Regulations - Title 21 Part 11 Electronic Records; Electronic Signatures. FDA.gov. Authority: 21 U.S.C. 321-393; 42 U.S.C. 262., 20 Mar. 1997. Web. 16 Nov. 2011. <http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/cfrsearch.cfm?cfrpart=11>.</ref>  Essentially, the requirements are that the software has been designed and implemented to be suitable for its intended purposes.  Evidence to show that this is the case is often provided by a Software Requirements Specification (SRS) setting forth the intended uses and the needs that the ELN will meet; one or more testing protocols that, when followed, demonstrate that the ELN meets the requirements of the specification and that the requirements are satisfied under worst-case conditions.  Security, audit trails, prevention of unauthorized changes without substantial collusion of otherwise independent personnel (i.e., those having no interest in the content of the ELN such as independent quality unit personnel) and similar tests are fundamental.  Finally, one or more reports demonstrating the results of the testing in accordance with the predefined protocols are required prior to release of the ELN software for use.  If the reports show that the software failed to satisfy any of the SRS requirements, then corrective and preventive action ("CAPA") must be undertaken and documented.  Such CAPA may extend to minor software revisions, or changes in architecture or major revisions.  CAPA activities need to be documented as well.\n\nAside from the requirements to follow such steps for regulated industry, such an approach is generally a good practice in terms of development and release of any software to assure its quality and fitness for use.  There are standards related to software development and testing that can be applied (see ref.).\n\n==See also==\n* [[List of ELN software packages]]\t\n* [[Data management]]\n* [[Laboratory informatics]]\n* [[Scientific management]]\n\n==References==\n{{reflist}}\n\n== Further reading ==\n* {{Cite journal \n| last1 = Taylor | first1 = K. T. \n| title = The status of electronic laboratory notebooks for chemistry and biology \n| journal = Current opinion in drug discovery & development \n| volume = 9 \n| issue = 3 \n| pages = 348–353 \n| year = 2006 \n| pmid = 16729731\n}}\n* {{Cite journal | last1 = Rubacha | first1 = M. | last2 = Rattan | first2 = A. K. | last3 = Hosselet | first3 = S. C. | doi = 10.1016/j.jala.2009.01.002 | title = A Review of Electronic Laboratory Notebooks Available in the Market Today | journal = Journal of Laboratory Automation | volume = 16 | issue = 1 | pages = 90–98 | year = 2011 | pmid =  21609689| pmc = }}\n\n{{DEFAULTSORT:Electronic Lab Notebook}}\n[[Category:Electronic lab notebook]]\n[[Category:Research]]\n[[Category:Science software]]\n[[Category:Scientific documents]]\n[[Category:Notebooks]]\n[[Category:Electronic documents]]\n[[Category:Data management]]\n[[Category:Content management systems]]\n[[Category:Data management software]]']
['Asset Description Metadata Schema', '35676267', '[[File:ADMSmodelv1.png|thumb|300px|ADMS UML model version 1.00]]\n\nThe \'\'\'Asset Description Metadata Schema\'\'\' (\'\'\'ADMS\'\'\') is a common [[metadata]] vocabulary to describe standards, so-called interoperability assets, on the Web.\n\nUsed in concert with [[Web Syndication|web syndication technology]] ADMS helps people make sense of the complex multi-publisher environment around standards and in particular the ones which are semantic assets such as [[ontologies]], [[data model]]s, [[Data dictionary|data dictionaries]], code lists, [[XML]] and [[Resource Description Framework|RDF]] schemas. In spite of their importance, standards are not easily discoverable on the web via search engines because [[metadata]] about them is seldom available. Navigating on the websites of the different publishers of standards is not efficient either.\n\n==Key terminology==\nA \'\'\'semantic asset\'\'\' is a specific type of standard which involves:\n\n highly reusable metadata\n (e.g. xml schemata, generic data models)\n and/or reference data\n (e.g. code lists, taxonomies, dictionaries, vocabularies)\n\nOrganisations use semantic assets to share information and knowledge (within themselves and with others). Semantic assets are usually very valuable and reusable elements for the development of Information Systems, in particular, as part of machine-to-machine interfaces. As enablers to interoperable information exchange, semantic assets are usually created, published and maintained by standardisation bodies. Nonetheless, ICT projects and groups of experts also create such assets. There are therefore many publishers of semantic assets with different degrees of formalism.\n\n==What is ADMS==\nADMS<ref name="ADMS">[http://joinup.ec.europa.eu/asset/adms/home], ADMS homepage on Joinup</ref> is a standardised metadata vocabulary created by the [[European Union|EU]]\'s Interoperability Solutions for European Public Administrations (ISA) Programme<ref name="ISA">[http://ec.europa.eu/isa/], Interoperability Solutions for European Public Administrations (ISA) Programme</ref> of the [[European Commission]] to help publishers of standards document what their standards are about (their name, their status, theme, version, etc.) and where they can be found on the Web. ADMS descriptions can then be published on different websites while the standard itself remains on the website of its publisher (i.e. syndication of content). ADMS embraces the multi-publisher environment and, at the same time, it provides the means for the creation of aggregated catalogues of standards and single points of access to them based on ADMS descriptions. The Commission will offer a single point of access to standards described using ADMS via its collaborative platform, Joinup.<ref name="Joinup">[https://joinup.ec.europa.eu/], Link to Joinup</ref> The Federation<ref name="Federation">[https://joinup.ec.europa.eu/elibrary/document/adms-enabled-federation-semantic-asset-repositories-brochure], Link to the brochure of the Federation of Semantic Asset Repositories</ref> service will increase the visibility of standards described with ADMS on the web. This will also stimulate their reuse by Pan-European initiatives.\n\n==ADMS Working Group==\nMore than 43 people of 20 EU Member States as well as from the US and Australia have participated in the [https://joinup.ec.europa.eu/asset/adms/document/adms-working-group ADMS Working Group]. Most of them were experts from standardisation bodies, research centres and the EU Commission. The working group used a methodology based on [[W3C]]’s processes and methods.<ref name="CoreVocsPM">{{cite web|url=http://joinup.ec.europa.eu/sites/default/files/D3.1-Process%20and%20Methodology%20for%20Core%20Vocabularies_v1.01.pdf |title=Archived copy |accessdate=2012-04-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20130430164940/https://joinup.ec.europa.eu/sites/default/files/D3.1-Process%20and%20Methodology%20for%20Core%20Vocabularies_v1.01.pdf |archivedate=2013-04-30 |df= }}, Link to ISA\'s Core Vocs methodology</ref>\n\n==How to download ADMS==\nADMS version 1 was officially released in April 2012.<ref name="ADMSrelease">[https://joinup.ec.europa.eu/news/adms-v100-officially-released], ADMS V1 release announcement</ref> Version 1.00 of ADMS is available for download on Joinup:<ref name="Joinup"/>\nhttps://joinup.ec.europa.eu/asset/adms/release/100<ref name="ADMS1">[https://joinup.ec.europa.eu/], Link to ADMS v1</ref>\n\nADMS is offered under ISA\'s Open Metadata Licence v1.1<ref name="OpenMetadataLicense">[https://joinup.ec.europa.eu/category/licence/isa-open-metadata-licence-v11], ISA Open Metadata Licence v1.1</ref>\n\n==Related work==\nThe ADMS specification reuses existing [[metadata]] vocabularies and core vocabularies including:\n* The [[Dublin Core]] Metadata Element Set (DCMES)<ref>http://dublincore.org/documents/dces/</ref>\n* The [[Data Catalog Vocabulary]] (DCAT) <ref>http://www.w3.org/TR/vocab-dcat/</ref>\n* The [[FOAF (software)|Friend of a Friend (FOAF) Ontology]]\n* The [[vCard]] Ontology <ref>[http://www.w3.org/TR/vcard-rdf/], Representing vCard Objects in RDF</ref>\n\n==The future of ADMS==\nADMS v1.00 will be contributed to<ref name="ADMScontributed">[https://joinup.ec.europa.eu/asset/adms/topic/adms-public-review-key-specifications-interoperability-developed-eus-isa-programme-], Announcement that key specifications for interoperability developed by the EU\'s ISA Programme will become W3C standards</ref> W3C’s Government Linked Data (GLD) Working Group.<ref name="W3C_GLD">[http://www.w3.org/2011/gld/wiki/Main_Page], W3 Government Linked Data (GLD) Working Group</ref> This means that ADMS will be published by the GLD Working Group as First Public Working Drafts for further consultation within the context of the typical W3C standardization process. The desired outcome of that process will be the publication of ADMS as a W3C Recommendation available under W3C\'s Royalty-Free License.\n\nThe ADMS RDFS Vocabulary already has a w3.org namespace: [http://www.w3.org/ns/adms http://www.w3.org/ns/adms#].\n\n==References==\n{{reflist|30em}}\n\n[[Category:Data management]]\n[[Category:Metadata]]\n[[Category:Technical communication]]']
['NewSQL', '37256799', '\'\'\'NewSQL\'\'\' is a class of modern [[relational database management system|relational]] [[database management system]]s that seek to provide the same scalable performance of [[NoSQL]] systems for [[online transaction processing]] (OLTP) read-write workloads while still maintaining the [[ACID]] guarantees of a traditional database system.<ref name="aslett2012">\n{{cite web \n| url = http://cs.brown.edu/courses/cs227/archives/2012/papers/newsql/aslett-newsql.pdf\n| title = How Will The Database Incumbents Respond To NoSQL And NewSQL?\n| first = Matthew\n| last = Aslett\n| publisher = 451 Group\n| publication-date = 2011-04-04\n| year = 2011\n| accessdate = 2012-07-06\n}}\n</ref><ref>\n{{cite web \n| url = http://cacm.acm.org/blogs/blog-cacm/109710-new-sql-an-alternative-to-nosql-and-old-sql-for-new-oltp-apps/fulltext\n| title = NewSQL: An Alternative to NoSQL and Old SQL for New OLTP Apps\n| first = Michael\n| last = Stonebraker\n| publisher = Communications of the ACM Blog\n| publication-date = 2011-06-16\n| accessdate  = 2012-07-06\n}}\n</ref><ref name="highscalability">\n{{cite web \n| url = http://highscalability.com/blog/2012/9/24/google-spanners-most-surprising-revelation-nosql-is-out-and.html\n| title = Google Spanner\'s Most Surprising Revelation: NoSQL is Out and NewSQL is In\n| first = Todd\n| last = Hoff\n| publication-date = 2012-09-24\n| accessdate  = 2012-10-07\n}}\n</ref>\n\n== History ==\nThe term was first used by 451 Group analyst Matthew Aslett in a 2011 research paper discussing the rise of new database systems as challengers to established vendors.<ref name="aslett2010" /> Many enterprise systems that handle high-profile data (e.g., financial and order processing systems) also need to be able to scale but are unable to use NoSQL solutions because they cannot give up strong transactional and consistency requirements.<ref name="aslett2010">{{cite web \n| url = http://blogs.the451group.com/information_management/2011/04/06/what-we-talk-about-when-we-talk-about-newsql/\n| title = What we talk about when we talk about NewSQL\n| first = Matthew\n| last = Aslett\n| publisher = 451 Group\n| publication-date = 2011-04-06\n| year = 2010\n| accessdate = 2012-10-07\n}}</ref><ref>\n{{cite web \n| url = http://berlinbuzzwords.de/sessions/keynote-0\n| title = Building Spanner\n| first = Alex\n| last = Lloyd\n| publisher = Berlin Buzzwords\n| publication-date = 2012-06-05\n| year = 2012\n| accessdate = 2012-10-07\n}}</ref> The only options previously available for these organizations were to either purchase a more powerful single-node machine or develop custom middleware that distributes queries over traditional DBMS nodes. Both approaches are prohibitively expensive and thus are not an option for many. Thus, in this paper, Aslett discusses how NewSQL upstarts are poised to challenge the supremacy of commercial vendors, in particular [[Oracle Database|Oracle]].\n\n== Systems ==\nAlthough NewSQL systems vary greatly in their internal architectures, the two distinguishing features common amongst them is that they all support the [[Relational model|relational data model]] and use [[SQL]] as their primary interface.<ref>{{Cite journal | last1 = Cattell | first1 = R. | title = Scalable SQL and NoSQL data stores | doi = 10.1145/1978915.1978919 | journal = ACM SIGMOD Record | volume = 39 | issue = 4 | pages = 12 | year = 2011 | url = http://cattell.net/datastores/Datastores.pdf| pmid =  | pmc = }}</ref>\nThe applications targeted by these NewSQL systems are characterized as having a large number of transactions that (1) are short-lived (i.e., no user stalls), (2) touch a small subset of data using index lookups (i.e., no full table scans or large distributed joins), and (3) are repetitive (i.e. executing the same queries with different inputs).<ref>\n{{cite conference\n| authorlink = Michael Stonebraker\n| first = Mike | last = Stonebraker\n| title = The end of an architectural era: (it\'s time for a complete rewrite\n| booktitle = VLDB \'07: Proceedings of the 33rd international conference on Very large data bases\n| location = Vienna, Austria\n| year = 2007\n| url = http://hstore.cs.brown.edu/papers/hstore-endofera.pdf\n| format = PDF |display-authors=etal}}</ref> These NewSQL systems achieve high performance and scalability by eschewing much of the legacy architecture of the original [[IBM System R]] design, such as heavyweight [[Algorithms for Recovery and Isolation Exploiting Semantics|recovery]] or [[concurrency control]] algorithms.<ref>{{Cite journal | last1 = Stonebraker | first1 = M. | last2 = Cattell | first2 = R. | doi = 10.1145/1953122.1953144 | title = 10 rules for scalable performance in \'simple operation\' datastores | journal = Communications of the ACM | volume = 54 | issue = 6 | pages = 72 | year = 2011 | pmid =  | pmc = }}</ref> One of the first known NewSQL systems is the [[H-Store]] [[Parallel database|parallel database system]].<ref>\n{{cite web \n| url = http://blogs.the451group.com/information_management/2008/03/04/is-h-store-the-future-of-database-management-systems/\n| title = Is H-Store the future of database management systems?\n| first = Matthew\n| last = Aslett\n| year = 2008\n| publication-date = 2008-03-04\n| accessdate  = 2012-07-05\n}}\n</ref><ref>\n{{cite web \n| url = http://www.zdnet.com/blog/btl/h-store-complete-destruction-of-the-old-dbms-order/8055\n| title = H-Store: Complete destruction of the old DBMS order?\n| first = Larry\n| last = Dignan\n| year = 2008\n| accessdate  = 2012-07-05\n}}\n</ref>\n\nNewSQL systems can be loosely grouped into three categories:\n<ref>\n{{cite web \n| url = http://www.linuxforu.com/2012/01/newsql-handle-big-data/\n| title = NewSQL - The New Way to Handle Big Data\n| first =  Prasanna\n| last = Venkatesh\n| year = 2012\n| publication-date = 2012-01-30\n| accessdate  = 2012-10-07\n}}\n</ref><ref>\n{{cite web \n| url = http://www.scalebase.com/the-story-of-newsql/\n| title = The NewSQL Market Breakdown\n| first = Doron\n| last = Levari\n| year = 2011\n| accessdate  = 2012-04-08\n}}\n</ref>\n\n=== New architectures ===\nThe first type of NewSQL systems are completely new database platforms. These are designed to operate in a distributed cluster of [[Shared nothing architecture|shared-nothing]] nodes, in which each node owns a subset of the data. These databases are often written from scratch with a distributed architecture in mind, and include components such as distributed concurrency control, flow control, and distributed query processing. Example systems in this category are [[Google Spanner]], [[Clustrix]], [[VoltDB]], [[MemSQL]], [[Pivotal Labs|Pivotal]]\'s GemFire XD, [[SAP HANA]],<ref>{{cite web|title=SAP HANA|url=http://www.sap.com/pc/tech/data-management/software/extreme-transaction-oltp/index.html|publisher=SAP|accessdate=17 July 2014}}</ref> [[NuoDB]], [[TiDB]], and [[Trafodion]].<ref>\n{{cite web \n| url = http://www.trafodion.org\n| title = Trafodion: Transactional SQL-on-HBase\n| year = 2014\n}}\n</ref>\n\n=== SQL engines ===\nThe second category are highly optimized [[Database engine|storage engines]] for [[SQL]]. These systems provide the same programming interface as SQL, but scale better than built-in engines, such as [[InnoDB]]. Examples of these new storage engines include [[MySQL Cluster]], [[Infobright]], [[TokuDB]] and the now defunct [[InfiniDB]].\n\n=== Transparent sharding ===\nThese systems provide a [[Shard (database architecture)|sharding]] [[middleware]] layer to automatically split databases across multiple nodes. [[ScaleBase]] is an example of this type of system.\n\n==See also==\n* [[Transaction processing]]\n* [[Partition (database)]]\n\n== References ==\n{{Reflist|30em}}\n\n{{Databases}}\n\n<!--Categories-->\n[[Category:Data management]]\n[[Category:Distributed data stores]]\n[[Category:NewSQL]]']
['Consistency (database systems)', '1140830', '{{merge from|Data consistency|date=November 2014}}{{About||Consistency in distributed systems as defined in the CAP Theorem|CAP theorem}}\n\n\'\'\'Consistency\'\'\' in [[database systems]] refers to the requirement that any given [[database transaction]] must change affected data only in allowed ways. Any data written to the database must be valid according to all defined rules, including [[Integrity constraints|constraints]], [[Cascading rollback|cascades]], [[Database trigger|triggers]], and any combination thereof.  This does not guarantee correctness of the transaction in all ways the application programmer might have wanted (that is the responsibility of application-level code) but merely that any programming errors cannot result in the violation of any defined rules.\n\n==As an ACID guarantee==\nConsistency is one of the four guarantees that define [[ACID]] [[database transaction|transactions]]; however, significant ambiguity exists about the nature of this guarantee. It is defined variously as:\n* The guarantee that any transactions started in the future necessarily see the effects of other transactions committed in the past<ref name="CAP Theorem Paper">http://webpages.cs.luc.edu/~pld/353/gilbert_lynch_brewer_proof.pdf "Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services"</ref><ref name="Ports et al">{{cite journal | url=http://drkp.net/papers/txcache-osdi10.pdf | title=Transactional Consistency and Automatic Management in an Application Data Cache |author1=Ports, D.R.K |author2=Clements, A.T |author3=Zhang, I |author4=Madden, S |author5=Liskov, B. | journal=MIT CSAIL}}</ref>\n* The guarantee that [[Relational database#Constraints|database constraints]] are not violated, particularly once a transaction commits<ref name="Haerder & Reuter">{{cite journal | url=http://www.minet.uni-jena.de/dbis/lehre/ws2005/dbs1/HaerderReuter83.pdf | title=Principles of Transaction-Oriented Database Recovery |author1=Haerder, T |author2=Reuter, A. | journal=Computing Surveys |date=December 1983  | volume=15 | issue=4 | pages=287–317}}</ref><ref>{{cite web|url=http://databases.about.com/od/specificproducts/a/acid.htm|title=The ACID Model|author=Mike Chapple|work=About}}</ref><ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/aa480356.aspx|title=ACID properties|publisher=}}</ref><ref>{{cite web|url=http://www.techopedia.com/definition/23949/atomicity-consistency-isolation-durability-acid|title=What is ACID in Databases? - Definition from Techopedia|author=Cory Janssen|work=Techopedia.com}}</ref>\n* The guarantee that operations in transactions are performed accurately, correctly, and with validity, with respect to application semantics<ref>{{cite web|url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=27614|title=ISO/IEC 10026-1:1998 - Information technology -- Open Systems Interconnection -- Distributed Transaction Processing -- Part 1: OSI TP Model|publisher=}}</ref>\n\nAs these various definitions are not mutually exclusive, it is possible to design a system that guarantees "consistency" in every sense of the word, as most [[relational database management system]]s in common use today arguably do.\n\n==As a CAP trade-off==\n\nThe [[CAP theorem]] is based on three trade-offs, one of which is "atomic consistency" (shortened to "consistency" for the acronym), about which the authors note, "Discussing atomic consistency is somewhat different than talking about an ACID database, as database consistency refers to transactions, while atomic consistency refers only to a property of a single request/response operation sequence. And it has a different meaning than the Atomic in ACID, as it subsumes the database notions of both Atomic and Consistent."<ref name="CAP Theorem Paper" />\n\n==See also==\n* [[Consistency model]]\n* [[CAP theorem]]\n* [[Eventual consistency]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Consistency (Database Systems)}}\n[[Category:Data management]]\n[[Category:Transaction processing]]']
['Inverted index', '3125116', 'In [[computer science]], an \'\'\'inverted index\'\'\' (also referred to as \'\'\'postings file\'\'\' or \'\'\'inverted file\'\'\') is an [[index (database)|index data structure]] storing a mapping from content, such as words or numbers, to its locations in a [[Table (database)|database file]], or in a document or a set of documents (named in contrast to a [[Forward Index]], which maps from documents to content).  The purpose of an inverted index is to allow fast [[full text search]]es, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its [[Index (database)|index]]. It is the most popular data structure used in [[document retrieval]] systems,<ref>{{Harvnb |Zobel|Moffat|Ramamohanarao|1998| Ref=none }}</ref> used on a large scale for example in [[search engine]]s.  Additionally, several significant general-purpose [[Mainframe computer|mainframe]]-based [[database management systems]] have used inverted list architectures, including [[ADABAS]], [[DATACOM/DB]], and [[Model 204]].\n\nThere are two main variants of inverted indexes: A \'\'\'record-level inverted index\'\'\' (or \'\'\'inverted file index\'\'\' or just \'\'\'inverted file\'\'\') contains a list of references to documents for each word. A \'\'\'word-level inverted index\'\'\' (or \'\'\'full inverted index\'\'\' or \'\'\'inverted list\'\'\') additionally contains the positions of each word within a document.<ref name="isbn0-201-39829-X-p192">{{Harvnb |Baeza-Yates|Ribeiro-Neto|1999| p=192 | Ref=BYR99 }}</ref> The latter form offers more functionality (like [[phrase search]]es), but needs more processing power and space to be created.\n\n==Applications==\n\nThe inverted index [[data structure]] is a central component of a typical [[Index (search engine)|search engine indexing algorithm]]. A goal of a search engine implementation is to optimize the speed of the query: find the documents where word X occurs. Once a [[Search engine indexing#The forward index|forward index]] is developed, which stores lists of words per document, it is next inverted to develop an inverted index. Querying the forward index would require sequential iteration through each document and to each word to verify a matching document. The time, memory, and processing resources to perform such a query are not always technically realistic.  Instead of listing the words per document in the forward index, the inverted index data structure is developed which lists the documents per word.\n\nWith the inverted index created, the query can now be resolved by jumping to the word ID (via [[random access]]) in the inverted index.\n\nIn pre-computer times, [[Concordance (publishing)|concordances]] to important books were manually assembled.  These were effectively inverted indexes with a small amount of accompanying commentary that required a tremendous amount of effort to produce.\n\nIn bioinformatics, inverted indexes are very important in the [[sequence assembly]] of short fragments of sequenced DNA. One way to find the source of a fragment is to search for it against a reference DNA sequence. A small number of mismatches (due to differences between the sequenced DNA and reference DNA, or errors) can be accounted for by dividing the fragment into smaller fragments—at least one subfragment is likely to match the reference DNA sequence. The matching requires constructing an inverted index of all substrings of a certain length from the reference DNA sequence. Since the human DNA contains more than 3 billion base pairs, and we need to store a DNA substring for every index and a 32-bit integer for index itself, the storage requirement for such an inverted index would probably be in the tens of gigabytes.\n\n==See also==\n* [[Index (search engine)]]\n* [[Reverse index]]\n* [[Vector space model]]\n\n== Bibliography ==\n*{{cite book |last= Knuth |first= D. E. |authorlink= Donald Knuth |title= [[The Art of Computer Programming]] |publisher= [[Addison-Wesley]] |edition= Third |year= 1997 |origyear= 1973 |location= [[Reading, Massachusetts]] |isbn= 0-201-89685-0 |ref= Knu97 |chapter= 6.5. Retrieval on Secondary Keys}}\n*{{cite journal|last1= Zobel |first1= Justin |last2=Moffat |first2=Alistair |last3=Ramamohanarao |first3=Kotagiri |date=December 1998 |title= Inverted files versus signature files for text indexing |journal= ACM Transactions on Database Systems |volume= 23 |issue= 4 |pages=453–490 |publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/296854.277632 |url= |accessdate= }}\n*{{cite journal|last1= Zobel |first1= Justin |last2=Moffat |first2=Alistair |date=July 2006 |title= Inverted Files for Text Search Engines |journal= ACM Computing Surveys |volume= 38 |issue= 2 |page=6|publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/1132956.1132959 |url= |accessdate= }}\n*{{cite book |last= Baeza-Yates | first = Ricardo |authorlink=Ricardo Baeza-Yates |author2=Ribeiro-Neto, Berthier |title= Modern information retrieval |publisher= Addison-Wesley Longman |location= [[Reading, Massachusetts]] |year= 1999 |isbn= 0-201-39829-X |oclc= |doi= |ref= BYR99 |page= 192 }}\n*{{cite journal |last= Salton | first = Gerard |author2=Fox, Edward A. |author3=Wu, Harry  |title= Extended Boolean information retrieval |publisher= ACM |year= 1983\n|journal = Commun. ACM |volume = 26 |issue = 11 |doi= 10.1145/182.358466 |page=1022 }}\n*{{cite book |title=Information Retrieval: Implementing and Evaluating Search Engines  |url=http://www.ir.uwaterloo.ca/book/ |publisher=MIT Press |year=2010 |location=Cambridge, Massachusetts |isbn= 978-0-262-02651-2 |author8=Stefan B&uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack}}\n\n==References==\n{{Reflist}}\n\n==External links==\n*[https://xlinux.nist.gov/dads/HTML/invertedIndex.html NIST\'s Dictionary of Algorithms and Data Structures: inverted index]\n*[http://mg4j.di.unimi.it Managing Gigabytes for Java] a free full-text search engine for large document collections written in Java.\n*[http://lucene.apache.org/java/docs/ Lucene] - Apache Lucene is a full-featured text search engine library written in Java.\n*[http://sphinxsearch.com/ Sphinx Search] - Open source high-performance, full-featured text search engine library used by craigslist and others employing an inverted index.\n*[http://rosettacode.org/wiki/Inverted_Index Example implementations] on [[Rosetta Code]]\n* [http://www.vision.caltech.edu/malaa/software/research/image-search/ Caltech Large Scale Image Search Toolbox]: a Matlab toolbox implementing Inverted File Bag-of-Words image search.\n\n[[Category:Data management]]\n[[Category:Search algorithms]]\n[[Category:Database index techniques]]\n[[Category:Substring indices]]']
['Write–write conflict', '217748', "In [[computer science]], in the field of [[database]]s, '''write–write conflict''', also known as '''overwriting [[commit (data management)|uncommitted]] data''' is a computational anomaly associated with interleaved execution of [[Database transaction|transactions]].\n\nGiven a [[Schedule (computer science)|schedule]] S\n\n<math>S = \\begin{bmatrix}\nT1 & T2 \\\\\nW(A) & \\\\\n & W(B) \\\\\nW(B) & \\\\\nCom. & \\\\\n & W(A)\\\\\n & Com. \\end{bmatrix}</math>\n\nnote that there is no read in this schedule. The writes are called '''''blind writes'''''.\n\nWe have a '''''lost update'''''.  Any attempts to make this schedule serial would give off two different results (either T1's version of A and B is shown, or T2's version of A and B is shown), and would not be the same as the above schedule.  This schedule would not be [[Serializability|serializable]].\n\n[[Strict two-phase locking|Strict 2PL]] overcomes this inconsistency by locking T1 out from B.  Unfortunately, [[deadlock]]s are something Strict 2PL does not overcome all the time.\n\n== See also ==\n\n* [[Concurrency control]]\n* [[Read–write conflict]]\n* [[Write–read conflict]]\n\n==References==\n{{reflist}}\n{{Unreferenced|date=August 2009}}\n\n{{DEFAULTSORT:Write-write conflict}}\n[[Category:Data management]]\n[[Category:Transaction processing]]"]
['Address space', '507144', '{{About|a concept used universally in computing|addressing specifically the main memory|Memory address}}\n{{refimprove|date=December 2011}}\n\nIn [[computing]], an \'\'\'address space\'\'\' defines a range of discrete addresses, each of which may correspond to a [[network host]], [[peripheral device]], [[disk sector]], a [[computer data storage|memory]] cell or other logical or physical entity.\n\nFor software programs to save and retrieve stored data, each unit of data must have an address where it can be individually located or else the program will be unable to find and manipulate the data. The number of address spaces available will depend on the underlying address structure and these will usually be limited by the computer architecture being used.\n\nAddress spaces are created by combining enough uniquely identified qualifiers to make an address unambiguous (within a particular address space). For a person\'s physical address, the \'\'address space\'\' would be a combination of locations, such as a neighborhood, town, city, or country. Some elements of an address space may be the same– but if any element in the address is different than addresses in said space will reference different entities. An example could be that there are multiple buildings at the same address of "32 Main Street" but in different towns, demonstrating that different towns have different, although similarly arranged, [[street address]] spaces.\n\nAn address space usually provides (or allows) a partitioning to several regions according to the [[mathematical structure]] it has. In the case of [[total order]], as for [[memory address]]es, these are simply [[interval (mathematics)|chunks]]. Some nested domains hierarchy appears in the case of [[arborescence (graph theory)|directed ordered tree]] as for the [[Domain Name System]] or a [[directory structure]]; this is similar to the hierarchical design of [[postal address]]es. In the [[Internet]], for example, the [[Internet Assigned Numbers Authority]] (IANA) allocates ranges of [[IP address]]es to various registries in order to enable them to each manage their parts of the global Internet address space.<ref>{{cite web|url=http://www.iana.org/assignments/ipv4-address-space/ |title= IPv4 Address Space Registry |date=March 11, 2009 |publisher=Internet Assigned Numbers Authority (IANA) |accessdate= September 1, 2011}}</ref>\n\n==Examples==\nUses of addresses include, but are not limited to the following:\n* [[Memory address]]es for [[main memory]], [[memory-mapped I/O]], as well as for [[virtual memory]];\n* Device addresses on an [[expansion bus]];\n* [[disk sector|Sector]] addressing for [[disk drive]]s;\n* [[File name]]s on a particular [[volume (computing)|volume]];\n* Various kinds of network host addresses in [[computer network]]s;\n* [[Uniform resource locator]]s in the Internet.\n\n== Address mapping and translation ==\n[[File:CNFTL9.JPG|thumb|Illustration of translation from logical block addressing to physical geometry]]\nAnother common feature of address spaces are [[map (mathematics)|mappings and translations]], often forming numerous layers. This usually means that some higher-level address must be translated to lower-level ones in some way. \nFor example, [[file system]] on a [[logical disk]] operates [[one-dimensional array|linear]] sector numbers, which have to be translated to \'\'absolute\'\' [[Logical block addressing|LBA]] sector addresses, in simple cases, via [[addition]] of the partition\'s first sector address. Then, for a disk drive connected via [[Parallel ATA]], each of them must be converted to \'\'logical\'\' (means fake) [[cylinder-head-sector]] address due to the interface historical shortcomings. It is converted back to LBA by the disk [[controller (computing)|controller]] and then, finally, to \'\'physical\'\' [[cylinder (disk drive)|cylinder]], [[disk head|head]] and [[track (disk drive)|sector]] numbers.\n\nThe [[Domain Name System]] maps its names to (and from) network-specific addresses (usually IP addresses), which in turn may be mapped to [[link layer]] network addresses via [[Address Resolution Protocol]]. Also, [[network address translation]] may occur on the edge of \'\'different\'\' IP spaces, such as a [[local area network]] and the Internet.\n\n[[File:Virtual address space and physical address space relationship.svg|thumb|Virtual address space and physical address space relationship]]\nAn iconic example of virtual-to-physical address translation is [[virtual memory]], where different [[Page (computer memory)|pages]] of [[virtual address space]] map either to [[paging|page file]] or to main memory [[physical address]] space. It is possible that several numerically different virtual addresses all refer to one physical address and hence to the same physical byte of [[Random access memory|RAM]]. It is also possible that a single virtual address maps to zero, one, [[CPU cache#Homonym and synonym problems|or more than one]] physical address.\n\n== See also ==\n* [[Linear address space]]\n* [[Name space]]\n* [[Virtualization]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Computing terminology]]\n[[Category:Data management]]\n[[Category:Computer architecture]]']
['Data thinking', '40598793', "{{unreferenced|date=September 2013}}\n'''Data thinking''' is the generic mental pattern observed during the processes of picking a subject to start with, identifying its parts or components, organizing and describing them in an informative fashion that is relevant to what motivated and initiated the whole processes.\n\nThe term was created by Mario Faria and Rogerio Panigassi in 2013 when they were writing a book about data science, [[data analysis|data analytics]], data management and how data practitioners were able to achieve their goals.\n\nMario Faria is one of the first [[Chief Data Officer]]s in the world.\n\n\n\n[[Category:Data management]]"]
['Quality of Data (QoD)', '42426440', '{{Orphan|date=January 2016}}\n\n\'\'\'Quality-of-Data (QoD)\'\'\' is a designation coined by L. Veiga, that specifies and describes the required Quality of Service of a distributed storage system from the Consistency point of view of its data.\n. It can be used to support [[Big data|Big Data]] management frameworks, Workflow management, and HPC systems (mainly for data replication and consistency). It takes into account data semantics, namely Time interval of data freshness, Sequence of tolerable number of outstanding versions of the data read before refresh, and Value divergence allowed before displaying it. Initially it was based in a model from an existing research work regarding vector-field Consistency,<ref>{{cite conference |author1=Nuno Santos |author2=Luís Veiga |author3=Paulo Ferreira | year=2007 | title=Vector-Field Consistency for Adhoc Gaming| booktitle = ACM/IFIP/Usenix Middleware Conference 2007 | url=http://www.gsd.inesc-id.pt/~lveiga/msc-08-09/vfc-middleware-07.pdf | format=PDF}}</ref> awarded the best-paper prize in the ACM/IFIP/Usenix Middleware Conference 2007 and later enhanced for increased scalability and fault-tolerance.<ref>{{cite conference |author1=Luís Veiga |author2=André Negrão |author3=Nuno Santos |author4=Paulo Ferreira | year=2010 | title=Unifying Divergence Bounding and Locality Awareness in Replicated Systems with Vector-Field Consistency | booktitle = JISA, Journal of Internet Services and Applications, Volume 1, Number 2, 95-115, Springer, 2010 | url=http://www.gsd.inesc-id.pt/~lveiga/vfc-JISA-2010.pdf | format=PDF}}</ref>\n\nThis consistency model has been successfully applied and proven in Big Data key/value store [[Apache HBase]],<ref group="nb">url=https://hbase.apache.org</ref> initially designed as a [[middleware]]<ref>{{cite conference |author1=Sergio Estéves |author2=João Silva |author3=Luís Veiga  |last-author-amp=yes | year=2013 | title=Quality-of-service for consistency of data geo-replication in cloud computing  | booktitle = Euro-Par 2012 Parallel Processing. Springer Berlin Heidelberg, 2012. 285-297 | url=http://www.gsd.inesc-id.pt/~sesteves/papers/vfc3-europar12.pdf | format=PDF}}</ref> module seating between clusters from separate data centres. The HBase-QoD coupling <ref>{{cite conference |author1=Álvaro García-Recuero |author2=Sergio Estéves |author3=Luís Veiga | year=2013 | title=Quality-of-Data for Consistency Levels in Geo-replicated Cloud Data Stores  | booktitle = IEEE CloudCom 2013 | url=http://www.inesc-id.pt/ficheiros/publicacoes/9253.pdf | format=PDF}}</ref> minimises bandwidth usage and optimises resources allocation during replication achieving the desired consistency level at a more fine-grained level.\n\nQoD is defined by the three-dimensions of vector k=(θ,σ,ν), but with a broader view of the issue, applicable also to large-scale data management techniques in regards to their timely delivery.<ref group="nb"><sub>url=http://www-01.ibm.com/software/data/quality/</sub></ref>\n\n== Other Descriptions ==\n\nQuality-of-Data should not be confused with other definitions for Data Quality such as\n<ref>{{cite conference |author1=Richard Y. Wang | year= 1992 | title=Toward quality data : an attribute-based approach | booktitle=Decision Support Systems 13, MIT  | url=http://web.mit.edu/tdqm/www/tdqmpub/Toward%20Quality%20Data.pdf | format=PDF}}</ref>\n<ref>{{cite conference |author1=George A. Mihaila |author2=Louiqa Raschid |author3=María-Esther Vidal | year= 2000 | title=Using Quality of Data Metadata for Source Selection and Ranking | booktitle =  | url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.9361 | format=}}</ref>\n- Completeness\n- Validity\n- Accuracy\n\n== Notes ==\n<references group="nb"/>\n\n== References ==\n<references/>\n\n[[Category:Data management]]']
['Data security', '1157832', "{{Refimprove|date=February 2012}}\n'''Data security''' means protecting data, such as a database, from destructive forces and from the unwanted actions of unauthorized users.<ref>Summers, G. (2004). Data and databases.  In: Koehne, H Developing Databases with Access: Nelson Australia Pty Limited. p4-5.</ref>\n\n== Data security technologies ==\n\n=== Disk encryption ===\n\n[[Disk encryption]] refers to encryption technology that encrypts data on a [[hard disk drive]]. Disk encryption typically takes form in either software (see [[disk encryption software]]) or hardware (see [[disk encryption hardware]]). Disk encryption is often referred to as [[on-the-fly encryption]] (OTFE) or transparent encryption.\n\n=== Software versus hardware-based mechanisms for protecting data ===\n\nSoftware-based security solutions encrypt the data to protect it from theft. However, a malicious program or a hacker could corrupt the data in order to make it unrecoverable, making the system unusable. Hardware-based security solutions can prevent read and write access to data and hence offer very strong protection against tampering and unauthorized access.\n\nHardware based security or assisted [[computer security]] offers an alternative to software-only computer security. [[Security token]]s such as those using [[PKCS#11]] may be more secure due to the physical access required in order to be compromised. Access is enabled only when the token is connected and correct [[Personal identification number|PIN]] is entered (see [[two-factor authentication]]). However, dongles can be used by anyone who can gain physical access to it. Newer technologies in hardware-based security solves this problem offering full proof security for data.\n\nWorking of hardware-based security: A hardware device allows a user to log in, log out and set different privilege levels by doing manual actions. The device uses biometric technology to prevent malicious users from logging in, logging out, and changing privilege levels. The current state of a user of the device is read by controllers in [[peripheral devices]] such as hard disks. Illegal access by a malicious user or a malicious program is interrupted based on the current state of a user by hard disk and DVD controllers making illegal access to data impossible. Hardware-based access control is more secure than protection provided by the operating systems as operating systems are vulnerable to malicious attacks by [[Computer virus|viruses]] and hackers. The data on hard disks can be corrupted after a malicious access is obtained. With hardware-based protection, software cannot manipulate the user privilege levels. It is impossible for a [[Hacker (computer security)|hacker]] or a malicious program to gain access to secure data protected by hardware or perform unauthorized privileged operations. This assumption is broken only if the hardware itself is malicious or contains a backdoor.<ref>{{Citation| last1 = Waksman  | first1 = Adam | last2 = Sethumadhavan | first2 = Simha | title = Silencing Hardware Backdoors | volume = | pages =  | periodical = Proceedings of the IEEE Symposium on Security and Privacy | location = Oakland, California  | url = http://www.cs.columbia.edu/~simha/preprint_oakland11.pdf | year = 2011  | issn =  | doi =  | isbn = }}</ref> The hardware protects the operating system image and file system privileges from being tampered. Therefore, a completely secure system can be created using a combination of hardware-based security and secure system administration policies.\n\n=== Backups ===\n\n[[Backup]]s are used to ensure data which is lost can be recovered from another source. It is considered essential to keep a backup of any data in most industries and the process is recommended for any files of importance to a user.\n\n===Data masking===\n{{main|Data masking}}\n[[Data masking]] of structured data is the process of obscuring (masking) specific data within a database table or cell to ensure that data security is maintained and sensitive information is not exposed to unauthorized personnel.<ref>{{cite web|title=What is Data Obfuscation|url=http://www.dataobfuscation.com.au|accessdate=1 March 2016}}</ref>  This may include masking the data from users (for example so banking customer representatives can only see the last 4 digits of a customers national identity number), developers (who need real production data to test new software releases but should not be able to see sensitive financial data), outsourcing vendors, etc.\n<ref>{{Cite web\n |url = http://searchsecurity.techtarget.com/definition/data-masking\n |title = data masking\n |accessdate = 29 July 2016\n}}</ref>\n\n===Data erasure===\n[[Data erasure]] is a method of software-based overwriting that completely destroys all electronic data residing on a hard drive or other digital media to ensure that no sensitive data is leaked when an asset is retired or reused...\n\n== International laws and standards ==\n\n=== International laws ===\n\nIn the [[United Kingdom|UK]], the [[Data Protection Act 1998|Data Protection Act]] is used to ensure that personal data is accessible to those whom it concerns, and provides redress to individuals if there are inaccuracies.<ref>{{Cite web\n |url = https://ico.org.uk/for-organisations/guide-to-data-protection/principle-1-fair-and-lawful/\n |title = data protection act\n |accessdate = 29 July 2016\n}}</ref> This is particularly important to ensure individuals are treated fairly, for example for credit checking purposes. The Data Protection Act states that only individuals and companies with legitimate and lawful reasons can process personal information and cannot be shared. [[Data Privacy Day]] is an international [[holiday]] started by the [[Council of Europe]] that occurs every January 28.<ref name=dataprivacyday>{{cite web|url=http://googleblog.blogspot.com/2008/01/celebrating-data-privacy.html|title=Celebrating data privacy |author=[[Peter Fleischer]], [[Jane Horvath]], [[Shuman Ghosemajumder]]|publisher=[[Google Blog]] |accessdate=12 August 2011 |year=2008}}</ref>\n\n=== International standards ===\n\nThe international standards ISO/IEC 27001:2013 and ISO/IEC 27002:2013 covers data security under the topic of [[information security]], and one of its cardinal principles is that all stored information, i.e. data, should be owned so that it is clear whose responsibility it is to protect and control access to that data.\n\nThe [[Trusted Computing Group]] is an organization that helps standardize computing security technologies.\n\nThe [[PCI DSS|Payment Card Industry Data Security Standard]] is a proprietary international information security standard for organizations that handle cardholder information for the major [[Debit card|debit]], [[Credit card|credit]], prepaid, [[e-purse]], [[Cash machine|ATM]] and POS cards.<ref>{{cite web|title=PCI DSS Definition|url=http://www.pcmag.com/encyclopedia/term/59104/pci-dss|accessdate=1 March 2016}}</ref>\n\n== Industry and software ==\nThere are several data security software available to be used by consumers and one of the most used data security software with a U.S issued patent is [[Folder lock|Folder Lock]].\n\n==See also==\n* [[Copy Protection]]\n* [[Data-centric security]]\n* [[Data erasure]]\n* [[Data masking]]\n* [[Data recovery]]\n* [[Digital inheritance]]\n* [[Disk encryption]]\n** [[Comparison of disk encryption software]]\n* [[Identity Based Security]]\n* [[Information security]]\n* [[IT network assurance]]\n* [[Pre-boot authentication]]\n* [[Privacy engineering]]\n* [[Secure USB drive]]\n* [[Security Breach Notification Laws]]\n* [[Single sign-on]]\n* [[Smart card]]\n* [[Trusted Computing Group]]\n\n== Notes and references ==\n{{reflist}}\n\n==External links==\n{{Commons category}}\n\n{{Data}}\n{{Privacy}}\n{{Portal bar|Computer security|Information technology}}\n\n[[Category:Data security| ]]\n[[Category:Data management]]"]
['NCSA Brown Dog', '43444201', '\n\'\'\'NCSA Brown Dog\'\'\' is a research project to develop a method for easily accessing historic research data stored in order to maintain the long-term viability of large bodies of scientific research. It is supported by the [[National Center for Supercomputing Applications]] (NCSA) that is funded by the [[National Science Foundation]] (NSF).<ref name=bdweb>{{cite web|title=Brown Dog|url=http://browndog.ncsa.illinois.edu|website=NCSA Brown Dog|accessdate=31 July 2014}}</ref> \n\n==History==\nBrown Dog is part of the [[Datanet|DataNet]] partners program funded by NSF in 2008. DataNet was conceived to address the increasingly digital and data-intensive nature of science, engineering and education. Brown Dog is part of a follow-on effort called [[Data Infrastructure Building Blocks (DIBBs)]], focused on building software to support DataNet. The project was proposed by researchers at NCSA and the [[University of Illinois Urbana-Champaign]] as well as researchers from [[Boston University]] and the [[University of North Carolina at Chapel Hill]].\n\n==Unstructured, uncurated, long tail data==\nMuch scientific data is smaller, [[Unstructured data|unstructured]] and uncurated and thus not easily shared. Such data is sometimes referred to as "long tail" data. This borrows a term from statistics and refers to the tail of the distribution of project sizes. The majority of smaller projects lack the resources to properly steward the data they produce. This so-called “long tail” data, both past and present, has the potential to inform future research in many study areas. Much of this data has become inaccessible due to obsolete software and file formats. The resulting impossibility of reviewing data from older research disrupts the overall scientific research project.<ref>{{cite web|title=DataUp—Data Curation for the Long Tail of Science|url=http://blogs.msdn.com/b/msr_er/archive/2012/10/02/dataup-data-curation-for-the-long-tail-of-science.aspx|website=Microsoft Research Connections Blog|publisher=Microsoft Research Connections Team|accessdate=7 August 2014}}</ref>\n\n==Approach==\nBrown Dog describes itself as the “super mutt” of software<ref>{{cite web|last1=Woodie|first1=Alex|title=NCSA Project Aims to Create a DNS-Like Service for Data|url=http://www.datanami.com/2014/01/06/ncsa_project_aims_to_create_a_dns-like_service_for_data/|website=datanami|accessdate=7 August 2014}}</ref> (thus the name “Brown Dog”), serving as a low-level data infrastructure to interface digital data content across the internet.  Its approach is to use every possible source of automated help (i.e., software) in existence in a robust and provenance-preserving manner to create a service that can deal with as much of this data as possible.<ref>{{cite web|last1=Pletz|first1=John|title=U of I researchers get millions for \'super mutt\' to sniff out big-data trends|url=http://www.chicagobusiness.com/article/20131202/blogs11/131129794/u-of-i-researchers-get-millions-for-super-mutt-to-sniff-out-big-data-trends|website=Chicago Business|publisher=Crain Communications, Inc.|accessdate=7 August 2014}}</ref> The project sees the broader impact of its work in its potential to serve the general public as a sort of “DNS for data”, with the goal of making all data and all file formats as accessible as webpages are today.\n\n==Technology==\nBrown Dog seeks to address problems involving the use of uncurated and unstructured data collections through the development of two services: the Data Access Proxy (DAP) to aid in the conversion of file formats and the Data Tilling Services (DTS) for the automatic extraction of metadata from file contents. Once developed, researchers and general public users will be able to download browser plugins and other tools from the Brown Dog tool catalog.<ref name="bdweb" /><ref>{{cite web|last1=Jewett|first1=Barbara|title=DATA SET FREE|url=http://www.ncsa.illinois.edu/news/stories/KentonMcHenry/|website=NCSA Access Magazine|publisher=NCSA|accessdate=7 August 2014}}</ref>\n\n===Data Tilling Service===\nData Tilling Service (DTS) will allow users to search data collections using an existing file to discover other similar files in a collection. A DTS search field will be appended to configured browsers where example files can be dropped. This tells DTS to search all the files under a given [[URL]] for files similar to the dropped file. For example, while browsing an online image collection, a user could drop an image of three people into the search field, and the DTS would return all images in the collection that also contain three people. If DTS encounters a foreign file format, it will utilize DAP to make the file accessible. DTS also indexes the data and extract and appends metadata to files and collections enabling users to gain some sense of the type of data they are encountering.\n\nThis service runs on port 9443.\n\n===Data Access Proxy===\nData Access Proxy (DAP) allows users to access data files that would otherwise be unreadable. Similar to an internet gateway or [[Domain Name System|Domain Name Service]], the DAP configuration would be entered into a user’s machine and browser settings. Data requests over [[HTTP]] would first be examined by DAP to determine if the native file format is readable on the client device. If not, DAP converts the file into the best available format readable by the client machine.  Alternatively, the user could specify the desired format themselves.\n\nThis service runs on port 8184.\n\n==Use cases==\nBrown Dog targets three [[use cases]] proposed by groups within the [http://earthcube.org EarthCube] research communities. Developers and researchers from these communities will work together on use cases that span [[geoscience]], [[engineering]], [[biology]] and [[social science]].\n\n===Long tail vegetation data in ecology and global change biology===\nThis use case is led by [http://people.bu.edu/dietze/ \'\'\'Michael Dietze\'\'\'], [http://www.bu.edu/ Boston University]\n\n<blockquote>Data on the abundance, species composition, and size structure of vegetation is critically important for a wide array of sub-disciplines in ecology, conservation, natural resource management, and global change biology. However, addressing many of the pressing questions in these disciplines will require that terrestrial biosphere and hydrologic models are able to assimilate the large amount of long-tail data that exists but is largely inaccessible. The Brown Dog team in cooperation with researches from Dietze\'s lab will facilitate the capture of a huge body of smaller research-oriented vegetation data sets collected over many decades and historical vegetation data embedded in Public Land Survey data dating back to 1785. This data will be used as initial conditions for models, to make sense of other large data sets and for model calibration and validation.<ref name=bdweb /><ref name=newswise>{{cite web|title=BU Scientist, Collaborators Get $10.5 Million Grant to Develop Software for un-Curated Data|url=http://www.newswise.com/articles/bu-scientist-collaborators-get-10-5-million-grant-to-develop-software-for-un-curated-data|website=www.newswise.com|publisher=Boston University College of Arts and Sciences|accessdate=7 August 2014}}</ref></blockquote>\n\n===Designing green infrastructure considering storm water and human requirements===\nThis use case is led by [http://eisa.ncsa.illinois.edu/ Barbara Minsker], [http://illinois.edu University of Illinois at Urbana-Champaign];  [http://willsull.net/research/ William Sullivan], University of Illinois at Urbana-Champaign; [http://cee.illinois.edu/faculty/arthurschmidt Arthur Schmidt], University of Illinois at Urbana-Champaign\n\n<blockquote>This case study involves developing novel green infrastructure design criteria and models that integrate requirements for storm water management and ecosystem and human health and well being. To address the scientific and social problems associated with the design of green spaces, data accessibility and availability is a major challenge.  This study will focus on identified areas of the Green Healthy Neighborhood Planning region within the City of Chicago where existing local sewer performance is most deficient and where changes in impervious area through green infrastructure would be beneficial to under served neighborhoods. Brown Dog will be used to extract long-tail experimental data on human landscape preferences and health impacts. This data will be used to develop a human health impacts model that will then be linked together with a terrestrial biosphere model and a storm water model using Brown Dog technology.<ref name=bdweb /></blockquote>\n\n===Development and application for critical zone studies===\nThis use case is led by [http://hydrocomplexity.net/index.html Praveen Kumar], University of Illinois at Urbana-Champaign\n\n<blockquote>[[Earth\'s Critical Zone|Critical Zone]] (CZ) is the “skin” of the earth that extends from the treetops to the bedrock that is created by life processes working at scales from microbes to biomes. The Critical Zone supports all terrestrial living systems. Its upper part is the bio-mantle. This is where terrestrial biota live, reproduce, use and expend energy, and where their wastes and remains accumulate and decompose. It encompasses the soil, which acts as a geomembrane through which water and solutes, energy, gases, solids, and organisms interact with the atmosphere, biosphere, hydrosphere, and lithosphere. A variety of drivers affect this bio-dynamic zone, ranging from climate and deforestation to agriculture, grazing and human development. Understanding and predicting these effects is central to managing and sustaining vital ecosystem services such as soil fertility, water purification, and production of food resources, and, at larger scales, global carbon cycling and carbon sequestration.\nThe CZ provides a unifying framework for integrating terrestrial surface and near-surface environments, and reflects an intricate web of biological and chemical processes and human impacts occurring at vastly different temporal and spatial scales. The nature of these data create significant challenges for inter-disciplinary studies of the CZ because integration of the variety and number of data products and models has been a barrier. On the other hand, CZ data provides an excellent opportunity for defining, testing and implementing Brown Dog technologies. In this context “unstructured” data is viewed broadly as consisting of a collection of heterogeneous data with formats that reflect temporal and disciplinary legacies, data from emerging low cost open hardware based sensors and embedded sensor networks that lack well defined metadata and sensor characteristics, as well as data that are available as maps, images and text.<ref name=bdweb /></blockquote>\n\n==NSF Award==\nCIF21 DIBBs: Brown Dog was awarded in the winter of 2013 with a start date of October 1, 2013. Estimated expiration date is September 30, 2018.<ref>{{cite web|title=Award#1261582 - CIF21 DIBBs: Brown Dog|url=http://www.nsf.gov/awardsearch/showAward?AWD_ID=1261582&HistoricalAwards=false|website=nsf.gov|accessdate=31 July 2014}}</ref>\n\nThe award amount was $10,519,716.00, the largest DIBB award. The principal investigator is Kenton McHenry of NCSA at the University of Illinois at Urbana-Champaign. Coleaders are Jong Lee NCSA/UIUC; Barbara Minsker, Civil and Environmental Engineering, University of Illinois at Urbana-Champaign; Praveen Kumar, Civil and Environmental Engineering, University of Illinois at Urbana-Champaign; Michael Dietze, Department of Earth and Environment, Boston University.\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* {{official website|http://browndog.ncsa.illinois.edu}}\n\n[[Category:Data management]]\n[[Category:National Science Foundation]]\n[[Category:Research projects]]']
['BBC Genome Project', '44172733', '[[File:BBC Genome Logo.png|thumb|BBC Genome Logo]]\n\nThe \'\'\'BBC Genome Project\'\'\' is a digitised searchable database of programme listings from the [[Radio Times]] from the first issue in 1923, to 2009.<ref name=About>{{cite web|title=About this project|url=http://genome.ch.bbc.co.uk/about|publisher=[[BBC]]|accessdate=21 October 2014}}</ref> \n\n==History==\n===Prior===\nBBC Genome is not the BBC\'s first online searchable database; in April 2006 the BBC gave the public access to Infax, the BBC\'s programme database. Infax contained around 900,000 entries, but not every programme ever broadcast, and it ceased operation in December 2007.<ref name="About Infax">{{cite web|title=About This Prototype|url=http://open.bbc.co.uk/cataloguemeta/2005/11/about_this_prototype.html|publisher=[[BBC]]|accessdate=2 February 2016|archiveurl=https://web.archive.org/web/20060613100552/http://open.bbc.co.uk/cataloguemeta/2005/11/about_this_prototype.html|archivedate=13 June 2006}}</ref> The front page of the website is still available to see via the [[Internet Archive]] [https://web.archive.org/web/20060512054648/http://open.bbc.co.uk/catalogue/infax here.] After Infax ceased, a message on the website said that it would be incorporating in the information into individual programme pages.<ref name="Prototype End">{{cite web|title=This experimental prototype trial has now concluded|url=http://www.bbc.co.uk/archive/catalogue_offline.shtml|publisher=[[BBC]]|accessdate=2 February 2016}}</ref> In 2012, it was replaced by the database Fabric but this is only for internal use within the BBC.\n\n==\'\'Radio Times\'\'== \n[[File:BBC Genome OCR error.jpg|thumb|Screenshot of an OCR error (since corrected) in Genome. The text, "Uza TarbuclC\'s Christmas", should read "[[Liza Tarbuck]]\'s Christmas".]]\nIn December 2012, the [[BBC]] completed a digitisation exercise, scanning the listings from [[Radio Times]] of all BBC programmes 1923-2009 from an entire run of about 4,500 copies of the magazine.<ref name="Kelion">{{cite web|url=http://www.bbc.co.uk/news/technology-20625884|title=BBC finishes Radio Times archive digitisation effort|last=Kelion|first=Leo|work=[[BBC Online]]|accessdate=20 January 2013}}</ref> They identified around five million programmes, involving 8.5 million actors, presenters, writers and technical staff.<ref name="Kelion" /> The listings are as published, in advance, and so do not include late changes or cancellations.\n\nThe issues were scanned at high resolution, producing [[TIF]] images and [[Optical Character Recognition]] (OCR) was then used to turn the text from the page into searchable text on the Genome database.<ref name=About/>\n\nBBC Genome was released for public use on 15 October 2014.<ref name="Hilbish">{{cite web|url=http://www.bbc.co.uk/blogs/aboutthebbc/posts/Genome-The-Radio-Times-Archive-is-now-live|title=Genome – Radio Times archive now live|last=Bishop|first=Hilary|work=[[BBC Online]]|accessdate=15 October 2014}}</ref><ref>{{cite news|last=Sweney|first=Mark|title=BBC digitises Radio Times back issues|url=https://www.theguardian.com/media/2014/oct/16/bbc-digitises-radio-times-back-issues-genome-project|newspaper=Guardian|date=16 October 2014}} </ref>\n\nThe aim of this project is to allow researchers to be able to find out information easier and to help [[BBC Archives]] to build up a picture of what exists and what is currently missing from the archive.<ref>{{cite web|title=BBC\'s Genome Project offers radio and TV archive listings|url=http://www.bbc.co.uk/news/technology-29643662|publisher=[[BBC]]|accessdate=21 October 2014|date=16 October 2014}}</ref><ref>{{cite web|last1=Sweney|first1=Mark|title=BBC digitises Radio Times back issues|url=https://www.theguardian.com/media/2014/oct/16/bbc-digitises-radio-times-back-issues-genome-project|publisher=[[The Guardian]]|accessdate=21 October 2014|date=16 October 2014}}</ref> Corrections to OCR errors and changes to advertised schedules are being [[Crowdsourcing|crowdsourced]],<ref name="Hilbish" /> with over 180,000 user generated edits accepted as of January 2017. <ref>{{Cite web|url=http://genome.ch.bbc.co.uk/schedules/bbcone/london/1964-04-20|title=BBC One London - 20 April 1964 - BBC Genome|website=genome.ch.bbc.co.uk|access-date=2017-01-09}}</ref> \n\nEach listing entry has a unique identifier which may be expressed as a URL. For example, the very first screening of \'\'[[Doctor Who]]\'\' is http://genome.ch.bbc.co.uk/8f81c193ba224e84981f353cae480d49 A broadcast programme may have more than one such identifier, if it was screened (and thus listed) on repeat occasions.\n\n==See also==\n*[[BBC Archives]]\n* [[Timeline of the BBC]]\n\n==References==\n{{Reflist|2}}\n\n==External links==\n\n{{Wikidata property|P1573|BBC Genome identifiers}}\n\n*[http://genome.ch.bbc.co.uk/ BBC Genome website]\n*[http://www.bbc.co.uk/blogs/genome BBC Genome blog] \n*[http://twitter.com/bbcgenome/ BBC Genome on Twitter] \n*[http://www.facebook.com/bbcgenome/ BBC Genome on Facebook]\n\n{{BBC}}\n\n[[Category:BBC]]\n[[Category:BBC New Media|Archives]]\n[[Category:Data management]]\n[[Category:Broadcasting websites]]\n[[Category:British websites]]\n[[Category:History of television in the United Kingdom]]\n[[Category:History of radio]]\n[[Category:BBC history]]\n[[Category:Databases in the United Kingdom]]']
['Relational data stream management system', '44046965', 'A \'\'\'relational data stream management system (RDSMS)\'\'\' is a distributed, in-memory [[data stream management system]] (DSMS) that is designed to use standards-compliant [[SQL]] queries to process unstructured and structured data streams in real-time. Unlike [[SQL]] queries executed in a traditional [[RDBMS]], which return a result and exit, SQL queries executed in a RDSMS do not exit, generating results continuously as new data become available. Continuous SQL queries in a RDSMS use the [[SQL]] Window function to analyze, join and aggregate data streams over fixed or sliding windows. Windows can be specified as time-based or row-based.\n\n== RDSMS SQL Query Examples ==\n\nContinuous [[SQL]] queries in a RDSMS conform to the [[ANSI]] [[SQL]] standards. The most common RDSMS SQL query is performed with the declarative <code>SELECT</code> statement. A continuous SQL <code>SELECT</code> operates on data across one or more data streams, with optional keywords and clauses that include <code>FROM</code> with an optional <code>JOIN</code> subclause to specify the rules for joining multiple data streams, the <code>WHERE</code> clause and comparison predicate to restrict the records returned by the query, <code>GROUP BY</code> to project streams with common values into a smaller set, <code>HAVING</code> to filter records resulting from a <code>GROUP BY</code>, and <code>ORDER BY</code> to sort the results.\n\nThe following is an example of a continuous data stream aggregation using a <code>SELECT</code> query that aggregates a sensor stream from a weather monitoring station. The <code>SELECT</code>query aggregates the minimum, maximum and average temperature values over a one-second time period, returning a continuous stream of aggregated results at one second intervals.\n \n<source lang="sql">\nSELECT STREAM\n    FLOOR(WEATHERSTREAM.ROWTIME to SECOND) AS FLOOR_SECOND,\n    MIN(TEMP) AS MIN_TEMP,\n    MAX(TEMP) AS MAX_TEMP,\n    AVG(TEMP) AS AVG_TEMP\nFROM WEATHERSTREAM\nGROUP BY FLOOR(WEATHERSTREAM.ROWTIME TO SECOND);\n</source>\n\nRDSMS SQL queries also operate on data streams over time or row-based windows. The following example shows a second continuous SQL query using the <code>WINDOW</code> clause with a one-second duration. The <code>WINDOW</code> clause changes the behavior of the query, to output a result for each new record as it arrives. Hence the output is a stream of incrementally updated results with zero result latency.\n\n<source lang="sql">\nSELECT STREAM\n    ROWTIME,\n    MIN(TEMP) OVER W1 AS WMIN_TEMP,\n    MAX(TEMP) OVER W1 AS WMAX_TEMP,\n    AVG(TEMP) OVER W1 AS WAVG_TEMP\nFROM WEATHERSTREAM\nWINDOW W1 AS ( RANGE INTERVAL \'1\' SECOND PRECEDING );\n</source>\n\n== See also ==\n* [[SQL]]\n* [[NoSQL]]\n* [[NewSQL]]\n\n== External links ==\n* [http://www.sqlstream.com/stream-processing/ Stream processing with SQL]\n* [http://researcher.watson.ibm.com/researcher/view_group.php?id=2531 IBM System S]\n* [http://www.mcjones.org/System_R/SQL_Reunion_95/sqlr95.html \'\'1995 SQL Reunion: People, Projects, and Politics\'\', by Paul McJones (ed.)]: transcript of a reunion meeting devoted to the personal history of relational databases, SQL System R.\n\n[[Category:Data management]]\n[[Category:Relational model]]']
['Multi-model database', '44971098', 'Most database management systems are organized around a single [[Database model|data model]] that determines how data can be organized, stored, and manipulated. In contrast, a \'\'\'multi-model database\'\'\' is designed to support multiple data models against a single, integrated backend.<ref name="neither">[http://blogs.the451group.com/information_management/2013/02/08/neither-fish-nor-fowl/ The 451 Group, "Neither Fish Nor Fowl: The Rise of Multi-Model Databases"]</ref> Document, graph, relational, and key-value models are examples of data models that may be supported by a multi-model database.\n\n== Background ==\n\nThe [[relational model|relational]] data model became popular after its publication by [[Edgar F. Codd]] in 1970. Due to increasing requirements for [[Scalability#Horizontal and vertical scaling|horizontal scalability]] and [[fault tolerance]], [[NoSQL]] databases became prominent after 2009. NoSQL databases use a variety of data models, with [[Document-oriented database|document]], [[Graph database|graph]], and key-value models being popular.<ref name="rise">[http://www.infoworld.com/article/2861579/database/the-rise-of-the-multimodel-database.html Infoworld, "The Rise of the Multi-Model Database"]</ref>\n\nA Multi-model database is a database that can store, index and query data in more than one model. For some time, databases have primarily supported only one model, such as: [[Relational database]], [[Document-oriented database]], [[Graph database]] or [[Triplestore]]. A database that combines many of these is multi-model.\n\nFor some time, it was all but forgotten (or considered irrelevant) that there were any other database models besides Relational. The Relational model and notion of [[Third normal form]] were the de facto standard for all data storage. However, prior to the dominance of Relational data modeling from about 1980 to 2005 the [[Hierarchical database model]] was commonly used, and since 2000 or 2010, many [[NoSQL]] models that are non-relational including Documents, triples, key-value stores and graphs are popular. Arguably, geospatial data, temporal data and text data are also separate models, though indexed, queryable text data is generally termed a "[[search engine]]" rather than a database.\n\nThe first time the word "Multi-Model" has been associated to the databases was on May 30, 2012 in Cologne, Germany, during the Luca Garulli\'s key note "\'\'NoSQL Adoption – What’s the Next Step?\'\'".<ref>{{Cite journal|date=2012-06-01|title=Multi-Model storage 1/2 one product,|url=http://www.slideshare.net/lvca/no-sql-matters2012keynote/47-MultiModel_storage_12_one_product}}</ref><ref>{{Cite web|url=https://2012.nosql-matters.org/cgn/index.html?p=1202.html#luca_garulli_keynote|title=Nosql Matters Conference 2012 {{!}} NoSQL Matters CGN 2012|website=2012.nosql-matters.org|access-date=2017-01-12}}</ref> Luca Garulli envisioned the evolution of the 1st generation NoSQL products into new products with more features able to be used by multiple use cases.\n\nA Multi-model database is most directly a response to the "[[Polyglot Persistence]]" approach of knitting together multiple database products, each handing a different model, to achieve a multi-model capability as described by Martin Fowler.<ref name="polyglot">[http://martinfowler.com/bliki/PolyglotPersistence.html Polyglot Persistence]</ref> This strategy has two major disadvantages: it leads to a significant increase in operational complexity, and there is no support for maintaining data consistency across the separate data stores, so Multi-model databases have begun to fill in this gap.\n\nSome stories of overcomplicated systems from un-necessary "frankenbeast" database integrations are found on the web.<ref name="frankenbeast">[http://www.marklogic.com/blog/polyglot-persistence-done-right/ MarkLogic, "Avoiding the Frankenbeast"]</ref><ref name="boring">[http://mcfunley.com/choose-boring-technology McKinley, "Choose Boring Technology"]</ref>\n\nMulti-model databases are intended to offer the data modeling advantages of [[Polyglot Persistence]],<ref name="polyglot"/> without its disadvantages. Operational complexity, in particular, is reduced through the use of a single data store.<ref name="rise"/>\n\n== Databases ==\nMulti-model databases include (in alphabetic order):\n* [[ArangoDB]] - Document (JSON), Graph, Key-value\n* [[CouchBase]] - Relational (SQL), Document\n* [[CrateDB]] - Relational (SQL), Document (Lucene)\n* [[MarkLogic]] - Document (XML and JSON), Graph (RDF with OWL/RDFS), text, geospatial, binary, SQL\n* [[OrientDB]] - Document (JSON), Graph, Key-value, Text, Geospatial, Binary, SQL, Reactive\n\nNote that the level of support for the various models varies widely, including the ability to query across models, fully index the internal structure of a model, transactional support, and optimization or query planning across models.\nThe first multi-model database was [[OrientDB]], created in 2010 as an answer to the fragmented NoSQL environment, with the goal of providing one product to replace multiple NoSQL databases.\n\n== Architecture ==\n\nThe main difference between the available multi-model databases is related to their architectures. Multi-model databases can support different models either within the engine or via different layers on top of the engine. Some products may provide an engine which supports documents and graphs while others provide layers on top of a key-key store.<ref>[http://blog.foundationdb.com/7-things-that-make-google-f1-and-the-foundationdb-sql-layer-so-strikingly-similar, "layer"]</ref> With a layered architecture, each data model is provided via its own [[Component-based software engineering|component]].\n\n== User-defined data models ==\n\nIn addition to offering multiple data models in a single data store, some databases allow developers to easily define custom data models. This capability is enabled by ACID transactions with high performance and scalability. In order for a custom data model to support concurrent updates, the database must be able to synchronize updates across multiple keys. ACID transactions, if they are sufficiently performant,  allow such synchronization.<ref name="multiple">[http://www.odbms.org/wp-content/uploads/2014/04/Multiple-Data-Models.pdf ODBMS, "Polyglot Persistence or Multiple Data Models?"]</ref> JSON documents, graphs, and relational tables can all be implemented in a manner that inherits the horizontal scalability and fault-tolerance of the underlying data store.\n\n== See also ==\n<!-- please do not list specific implementations here -->\n* [[Comparison of multi-model databases]]\n* [[ACID]]\n* [[NoSQL]]\n* [[Comparison of structured storage software]]\n* [[Database transaction]]\n* [[Distributed database]]\n* [[Distributed transaction]]\n* [[Document-oriented database]]\n* [[Graph database]]\n* [[Relational model]]\n\n== References ==\n{{Reflist|2}}\n\n== External links ==\n* [http://www.orientechnologies.com/docs/last/orientdb.wiki/Tutorial-Document-and-graph-model.html OrientDB Document and Graph Model]\n* [https://www.arangodb.com/key-features ArangoDB Key Features]\n* [https://foundationdb.com/try/multi-model FoundationDB Multi-Model Architecture]\n* [http://martinfowler.com/bliki/PolyglotPersistence.html Polyglot Persistence]\n* [http://blogs.the451group.com/information_management/2013/02/08/neither-fish-nor-fowl/ The 451 Group, "Neither Fish Nor Fowl: The Rise of Multi-Model Databases"]\n* [http://www.odbms.org/blog/2013/10/on-multi-model-databases-interview-with-martin-schonert-and-frank-celler/ ODBMS, "On Multi-Model Databases. Interview with Martin Schönert and Frank Celler."]\n* [http://www.odbms.org/wp-content/uploads/2014/04/Multiple-Data-Models.pdf ODBMS, "Polyglot Persistence or Multiple Data Models?"]\n* [http://www.infoworld.com/article/2861579/database/the-rise-of-the-multimodel-database.html Infoworld, "The Rise of the Multi-Model Database"]\n* [https://crate.io/docs/reference/storage_consistency.html, Crate.IO Storage and Consistency]\n* [http://www.marklogic.com/blog/tag/multi-model-database/, MarkLogic on Multi-model databases]\n\n{{DEFAULTSORT:Multi-model Database}}\n[[Category:Applications of distributed computing]]\n[[Category:Databases]]\n[[Category:Data management]]\n[[Category:Distributed computing architecture]]\n[[Category:Distributed data stores]]\n[[Category:NoSQL]]\n[[Category:Structured storage]]\n[[Category:Transaction processing]]']
['Software intelligence', '45124802', "{{one source|date=January 2015}}\n'''Software Intelligence''' ('''SI''') is [[software]] designed to analyze [[source code]] to better understand [[Information technology|Information Technology]] environments. Similarly to [[Business intelligence|Business Intelligence]] (BI), Software Intelligence is a set of software tools and techniques for the [[Data mining|mining of data]] into meaningful and useful information.<ref>{{cite web|last1=Hassan|first1=Ahmed|last2=Xie|first2=Tao|title=Software Intelligence: The Future of Mining Software Engineering Data|url=http://web.engr.illinois.edu/~taoxie/publications/foser10-si.pdf|website=http://web.engr.illinois.edu|accessdate=19 January 2015}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:Source code]]"]
['Data based decision making', '46235825', '{{more footnotes|date=May 2015}}\n\n\'\'\'Data based decision making\'\'\' or data driven decision making refers to educator’s  ongoing process of collecting and analyzing different types of data, including demographic, student achievement test, satisfaction, process data to guide decisions towards improvement of educational process. DDDM becomes more important in education since federal and state test-based  accountability policies. [[No Child Left Behind Act]] opens broader opportunities and incentives in using [[data]] by educational organizations by requiring schools and districts to analyze additional components of data, as well as pressing them to increase student test scores. Information makes schools accountable for year by year improvement various student groups. DDDM helps to recognize the problem and who is affected by the problem; therefore, DDDM can find a solution of the problem\n\n==Purpose==\n\nThe purpose of DDDM is to help educators, schools, districts, and states to use information they have to actionable knowledge to improve student outcomes. DDDM requires high-quality data and possibly technical assistance; otherwise, data can misinform and lead to unreliable inferences. [[Data management]] techniques can improve teaching and learning in schools. Test scores are used by many principals to identify “bubble kids”, students whose results are just below proficiency level in reading and mathematics.<ref name=r1>{{cite journal|last1=Mandinach|first1=Ellen|title=A perfect time for data use|journal=Educational Psychologist|date=April 23, 2012|volume=47|page=2|doi=10.1080/00461520.2012.667064}}</ref>\n\n==Types of data used in education==\n\nThere are 4 major types of data used in education: demographics data, perceptions data, student learning data, and school processes data.<ref name=Bernhardt>{{cite book|last1=Bernhardt|first1=Victoria|title=Data analysis for continuous school improvement|date=2013|publisher=Routledge|location=711 Third Avenue, New York, 10017|isbn=978-1-59667-252-9|pages=27–80}}</ref>\n\n1. Demographics data in educational organizations answers the question, "Who are we?". Demographics show the current context of the school and shows the trends. Trends help to predict and plan for the future, along with seeing measures where leaders work towards continuous school improvement. Thorough demographic data explains the structure of school, system, and the leadership. In education demographic data to the next items: number of students in the school, number of students with special needs, number of English learners, age or grade of students in cohorts, socio-economical status of students, attendance rates, [[ethnicity]]/[[race (human classification)|race]]/[[religious beliefs]], graduation rates, dropout rates, experience information of teachers, information about parents of students.<ref name="Bernhardt"/>\n\n2. Perception data tells us what students, staff, and parents think about a school and answers the question, "How do we do business?". School culture, climate, and organizational processes are assessed by perception data. Perception data includes values, beliefs, perceptions, opinions, observations. Perception data is collected mostly questionnaires. Perception data can be differentiate by two groups: 1- staff, 2 - students and parents. Staff are being asked if any changes in instruction or [[curriculum]] need to take place. Student and parent are questioned to report their interests, how difficult it take them to learn, how are they taught and treated.<ref name="Bernhardt"/>\n\n3. Student learning data answers two questions: How are our students doing? and Where are we now? Student learning data requires information from all subject areas, disaggregated by demographic groups, by teachers, by grade level, by cohorts over time, and individual student growth. This type of data helps to address additional help to students who are not proficient, deepening into what they know and what they don\'t know to become proficient. Student learning data connects with [[curriculum]], [[Teaching|instruction]], and [[Educational assessment|assessment]] in order to improve outcomes. Student learning data can clearly state the effectiveness of a single educator or the entire school. SLD can be gathered by looking at diagnostic tests, formative assessments, performance assessments, standardized tests, non-referenced tests, summative assessments, teacher-assigned tests, and others.<ref name="Bernhardt"/>\n\n4. School processes refer to actions of administrators and teachers to achieve the purpose of the school. Teachers\' habits, customs, knowledge, and professionalism are the things leading towards progress inside organizations. School processes data tell us what works, what doesn\'t, the results of educational process, and answers the question, "What are our processes are?". School processes produce school and class results. There are 4 major types of school processes: 1. instructional processes, 2. Organizational processes, 3. Administrative processes, 4. Continuous school improvement processes.   \n<ref name="Bernhardt"/>\n\n==Use in educational organizations==\n\n[[The U.S. Department of Education]] and the [[Institute of Education Sciences]] require to use data and DDDM in past decades to run educational organizations. Hard evidence and the use of data are emphasized to inform decisions. The data in educational organizations means more than analyzing test scores. Educational data movement is considered as a sociotechnical revolution. Educational data systems involve technologies and evidence to explain districts\', schools\', classrooms\' tendencies. DDDM is used to explain complexity of education, support collaboration, creating new designs of teaching. Student performance is central in DDDM. NCLB provided boost in the collection and use of educational information.<ref name=Piety>{{cite book|last1=Piety|first1=Philip|title=Assessing the educational data movement|date=2013|publisher=Teachers college press|location=New york|isbn=978-0-8077-5426-9|pages=1–20}}</ref>\n\nFor example, in a rural area educators tried to understand why a particular subset of students were struggling academically. Data analysts collected students performance data, medical records, behavioral data, attendance, and other data less qualitative information. After not finding direct correlation between collected data and student outcomes they decided to include transportation data into the research. As result, educators found that students who had longer way from houses to the school were struggling the most. According to the finding administrators modified transportation arrangements to make the way shorter for students as well as installing Internet access in buses so students could concentrate on doing homework. DDDM in this particular case helped to improve student results.<ref name="r1"/>\n\n==Effects on schools==\n\nEffective schools showing outstanding gains in academic measures report that the wide and wise use of data has a positive effect on student achievement and progress. DDDM is suggested to be a main tool to move educational organizations towards school improvement and [[educator effectiveness]]. Data can be used to measure growth over time, program evaluation along with identifying root causes of problems connected to education. Involving school teachers in data inquiry causes more collaborative work from staff. Data provides increasing communication and knowledge which has a positive effect on altering educator attitudes towards groups inside schools which are underperforming \n<ref>{{cite journal|last1=Wayman|first1=Jeffrey|title=Involving teachers in data driven decision making:Using computer data systems to support teacher inquiry and reflection|journal=Journal of education for students placed at risk|date=2005|pages=296–300}}</ref>\n\n==Notes==\n{{Reflist}}\n\n==General references==\n* {{cite journal|last1=Spillane|first1=James P.|title=Data in Practice: Conceptualizing the Data-Based Decision-Making Phenomena|journal=American Journal of Education|date=2012|volume=118|issue=2|pages=113–141|jstor=10.1086/663283|doi=10.1086/663283}}\n* {{cite journal|last1=Reeves|first1=Patricia L.|last2=Burt|first2=Walter L.|title=Challenges in Data-based Decision-making: Voices from Principals|journal=Educational Horizons|date=2006|volume=85|issue=1|pages=65–71|jstor=42925967}}\n\n[[Category:Data management]]\n[[Category:Standards-based education]]']
['Database administrator', '254789', '{{Infobox Occupation\n|caption= Database Administrator\n|official_names= Database administrator, database analyst\n|activity_sector=[[Information technology]], [[information system]]s\n|competencies= [[Database design|Databases design and implementation]], [[Computer programming|programming]] skills, [[database theory]], [[Computer network|networking]] basics, [[analytical skill]]s, [[critical thinking]]\n|formation=At least a [[Academic certificate|certificate]] with experience.\n}}\n\n\'\'\'Database administrators\'\'\' (\'\'\'DBAs\'\'\') use specialized software to store and organize data.<ref name="BLS-DBA">{{cite web | url=http://www.bls.gov/ooh/computer-and-information-technology/database-administrators.htm | title=Database Administrators | publisher=Bureau Of Labor Statistics | work=11/04/2015 | accessdate=4 November 2015}}</ref>\n\nThe role may include [[capacity planning]], [[Installation (computer programs)|installation]], [[Computer configuration|configuration]], [[database design]], [[Data migration|migration]], performance monitoring, [[Computer security|security]], [[troubleshooting]], as well as [[backup]] and [[data recovery]].<ref name="techrepublic">{{cite web | url=http://www.techrepublic.com/blog/the-enterprise-cloud/what-does-a-dba-do-all-day/ | title=What does a DBA do all day? | publisher=techrepublic.com | work=11/04/2015 | accessdate=4 November 2015}}</ref>\n\n==Skills==\nList of skills required to become database administrators are:<ref>{{cite web|last1=Spenik|first1=Mark|last2=Sledge|first2=Orryn|date=2001-03-20|url=http://www.developer.com/db/article.php/718491/What-Is-a-Database-Administrator.htm|title=What is a Database Administrator? (DBA)|publisher=Developer.com|accessdate=2012-02-06|archiveurl=https://web.archive.org/web/20110613101702/http://www.developer.com/db/article.php/718491/What-Is-a-Database-Administrator.htm|archivedate=2011-06-13}}</ref><ref>http://www.dba-oracle.com/oracle_tips_dba_job_skills.htm</ref><ref>http://www.orafaq.com/wiki/Roles_and_Responsibilities</ref>\n* [[Communication]] skills\n* Knowledge of [[database theory]]\n* Knowledge of [[database design]]\n* Knowledge about the [[Relational database management system|RDBMS]] itself, e.g. [[Microsoft SQL Server]] or [[MySQL]]\n* Knowledge of [[SQL|structured query language]] (SQL), e.g. [[SQL/PSM]] or [[Transact-SQL]]\n* General [[understanding]] of [[Distributed computing|distributed computing architectures]], e.g. [[Client–server model]]\n* General understanding of [[operating system]], e.g. [[Microsoft Windows|Windows]] or [[Linux]]\n* General understanding of [[Computer data storage|storage technologies]] and [[Computer network|networking]]\n* General understanding of routine maintenance, recovery, and handling failover of a database\n\nDatabase administrators benefit from a [[bachelor\'s degree]] or [[master\'s degree]] in [[computer science]]. An [[associate degree]] or a [[Academic certificate|certificate]] may be sufficient with work experience.<ref name="study.com">{{cite web | url=http://study.com/articles/Database_Administrator_Job_Description_and_Requirements.html | title=Database Administrator: Job Description and Requirements | publisher=study.com | work=11/4/2015 | accessdate=4 November 2015}}</ref>\n\n===Certification===\nThere are many certifications available for becoming a certified database administrator. Many of these certifications are offered by database vendors themselves. By passing a series of tests and sometimes other requirements, you can earn a database administrator certification. Schools offering Database Administration degrees can also be found.<ref name="learn.org">{{cite web | url=http://learn.org/articles/How_Do_I_Become_a_Certified_Database_Administrator.html | title=How Do I Become a Certified Database Administrator? | publisher=learn.org | work=learn.org | accessdate=4 November 2015}}</ref>\n\nFor example:\n* IBM Certified Advanced Database Administrator - DB2 10.1 for Linux, Unix and Windows<ref name="ibm.com">{{cite web |url=http://www-03.ibm.com/certify/index.shtml |title=IBM Professional Certification Program |work=ibm.com |publisher=[[IBM]] |accessdate=2014-08-10}}</ref>\n* IBM Certified Database Administrator - DB2 10.1 for Linux, Unix, and Windows<ref name="ibm.com"/>\n* Oracle Database 11g Administrator Certified Professional<ref>{{cite web |url=http://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=143&p_org_id=1001&lang=US |title=Oracle Certification Program |work=oracle.com |publisher=[[Oracle Corporation]] |accessdate=2011-06-18}}</ref>\n* Oracle MySQL 5.6 Database Administrator Certified Professional<ref>{{cite web |url=https://education.oracle.com/pls/web_prod-plq-dad/ou_product_category.getPage?p_cat_id=159&p_org_id=15941&lang=US#tabs-3 |title=Oracle Certified Professional, MySQL 5.6 Database Administrator |work=oracle.com |publisher=[[Oracle Corporation]] |accessdate=2016-09-18}}</ref>\n* MCSA SQL Server 2012<ref name=MCSASQL>{{cite web |url=https://www.microsoft.com/en-us/learning/mcsa-sql-certification.aspx |title=MCSA: SQL Server |work=microsoft.com |publisher=[[Microsoft]] |accessdate=2015-11-03}}</ref>\n* MCSE Data Platform Solutions Expert <ref name="microsoftsolutionsexpert">{{cite web | url=https://www.microsoft.com/en-us/learning/mcse-sql-data-platform.aspx | title=MCSE: Data Platform | publisher=microsoft.com | work=11/4/2015 | accessdate=4 November 2015}}</ref>\n\n==Duties==\nA database administrator\'s responsibilities can include the following tasks:<ref>{{cite web |url=http://docs.oracle.com/cd/B10501_01/server.920/a96521/dba.htm#852 |title=Oracle DBA Responsibilities |work=[[Oracle Corporation]] |accessdate=2012-02-06}}</ref>\n* [[Installation (computer programs)|Installing]] and [[upgrade|upgrading]] the database server and application tools\n* Allocating system storage and [[planning]] future storage requirements for the database system\n* Modifying the database structure, as necessary, from information given by application developers\n* Enrolling users and maintaining system [[Computer security|security]]\n* Ensuring compliance with database vendor [[license|license agreement]]\n* Controlling and [[System Monitoring|monitoring]] [[user (computing)|user]] access to the database\n* Monitoring and [[Program optimization|optimizing]] the performance of the database\n* Planning for [[backup]] and recovery of database information\n* Maintaining [[archive]]d data\n* Backing up and restoring databases\n* Contacting database [[vendor]] for [[technical support]]\n* Generating various reports by querying from database as per need\n\n==See also==\n* [[Comparison of database tools]]\n\n==References==\n{{Reflist}}\n\n==External links==\n"Database Administrators"\n\n{{Database}}\n\n{{Use British English|date=June 2012}}\n{{Use dmy dates|date=June 2012}}\n\n{{DEFAULTSORT:Database Administrator}}\n[[Category:Computer occupations]]\n[[Category:Data management]]\n[[Category:Database specialists| ]]']
['Copyright', '5278', '{{Redirect2|Copyrighting|Copyrights|the use of words to promote or advertise|Copywriting|the Wikipedia policy about copyright issues|Wikipedia:Copyrights}}\n{{pp-move-indef|small=yes}}\n{{Intellectual property}}\n{{Capitalism|Concepts}}\n{{Use dmy dates|date=January 2011}}\n{{Use American English|date=January 2014}}\n\n\'\'\'Copyright\'\'\' is a [[Natural and legal rights|legal right]] created by the law of a country that grants the creator of an original work [[exclusive right]]s for its use and distribution. This is usually only for a limited time. The exclusive rights are not absolute but limited by [[limitations and exceptions to copyright]] law, including fair use. A major limitation on copyright is that copyright protects only the original expression of ideas, and not the underlying ideas themselves.<ref>{{cite web|url=http://www.bitlaw.com/copyright/unprotected.html#ideas|title=Works Unprotected by Copyright Law|publisher=Bitlaw|author=Daniel A. Tysver}}</ref><ref>{{cite web|url=http://digital-law-online.info/lpdi1.0/treatise9.html |title=Legal Protection of Digital Information |page=\'\'Chapter 1: An Overview of Copyright\'\', Section II.E. Ideas Versus Expression.|author=Lee A. Hollaar}}</ref>\n\nCopyright is a form of [[intellectual property]], applicable to certain forms of creative work. Some, but not all jurisdictions require "fixing" copyrighted works in a tangible form. It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders.<ref>{{Citation|title=Copyright|publisher=University of California|year=2014|url=http://copyright.universityofcalifornia.edu/ownership/joint-works.html|accessdate=2014-12-15}}</ref><ref>http://www.jetlaw.org/publish/journal-conventions/</ref><ref>https://books.google.de/books?id=kz1F6uAHtaEC&pg=PA81&dq=%22rights+holder%22&hl=en&sa=X&ved=0ahUKEwiG4OnUo87RAhXqBcAKHQgZAD8Q6AEIHDAA#v=onepage&q=%22rights%20holder%22&f=false</ref><ref>https://books.google.de/books?id=xD_iBwAAQBAJ&pg=PT465&dq=%22rights+holder%22&hl=en&sa=X&ved=0ahUKEwiG4OnUo87RAhXqBcAKHQgZAD8Q6AEIKDAC#v=onepage&q=%22rights%20holder%22&f=false</ref> These rights frequently include reproduction, control over [[derivative work]]s, distribution, [[Performing rights|public performance]], and "[[moral rights]]" such as attribution.<ref>{{Citation|title=17 U.S.C. § 106|publisher=United States of America|year=2011|url=http://www.copyright.gov/title17/92chap1.html#106|accessdate=2014-12-15}}</ref>\n\nCopyrights are considered \'\'territorial rights\'\', which means that they do not extend beyond the territory of a specific jurisdiction.  While many aspects of national copyright laws have been standardized through [[international copyright agreements]], copyright laws vary by country.<ref name="International Copyright Law Survey">{{cite web|url=http://worldcopyrightlaw.com/copyrightsurvey|title=International Copyright Law Survey|publisher=Mincov Law Corporation}}</ref>\n\nTypically, the [[Copyright term|duration of a copyright]] spans the author\'s life plus 50 to 100 years (that is, copyright typically expires 50 to 100 years after the author dies, depending on the jurisdiction).  Some countries require certain copyright [[copyright formalities|formalities]] to establishing copyright, but most recognize copyright in any completed work, without formal registration. Generally, copyright is enforced as a [[civil law (common law)|civil]] matter, though some jurisdictions do apply [[criminal law|criminal]] sanctions.\n\nMost jurisdictions recognize copyright limitations, allowing "fair" exceptions to the creator\'s exclusivity of copyright and giving users certain rights. The development of digital media and computer network technologies have prompted reinterpretation of these exceptions, introduced new difficulties in enforcing copyright, and inspired additional challenges to copyright law\'s philosophic basis. Simultaneously, businesses with great economic dependence upon copyright, such as those in the music business, have advocated the extension and expansion of copyright and sought additional legal and technological enforcement.\n\n==History==\n{{main|History of copyright law}}\n===Background===\nCopyright came about with the invention of [[Printing press|the printing press]] and with wider literacy. As a legal concept, its origins in Britain were from a reaction to printers\' monopolies at the beginning of the 18th&nbsp;century. [[Charles II of England]] was concerned by the unregulated [[copying]] of books and passed the [[Licensing of the Press Act 1662]] by Act of Parliament,<ref>\'\'Copyright in Historical Perspective\'\', p. 136-137, Patterson, 1968, Vanderbilt Univ. Press</ref> which established a register of licensed books and required a copy to be deposited with the [[Worshipful Company of Stationers and Newspaper Makers|Stationers\' Company]], essentially continuing the licensing of material that had long been in effect.\n\nCopyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in [[Europe]] and not, for example, in Asia. In the [[Middle Ages]] in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which [[capitalism]] led to the [[commodification]] of many aspects of social life that earlier had no monetary or economic value per&nbsp;se.<ref>Bettig, Ronald V. (1996). \'\'Copyrighting Culture: The Political Economy of Intellectual Property. Westview Press\'\'. p. 9–17. ISBN 0-8133-1385-6.</ref>\n\nCopyright has grown from a legal concept regulating copying rights in the publishing of books and maps to one with a significant effect on nearly every modern industry, covering such items as [[Sound recording and reproduction|sound recordings]], films, photographs, software, and architectural works.\n\n===National copyrights===\n{{See also|Statute of Anne|History of US Copyright Law}}\n[[File:Statute of anne.jpg|thumb|left|The [[Statute of Anne]] (the Copyright Act 1709) came into force in 1710.]]\nOften seen as the first real copyright law, the 1709 British [[Statute of Anne]] gave the publishers rights for a fixed period, after which the copyright expired.<ref name="Rethinking copyright: history, theory, language">{{cite book|url=https://www.google.com/books?id=dMYXq9V1JBQC&dq=statute+of+anne+copyright&lr=&as_brr=3&source=gbs_navlinks_s |title=Rethinking copyright: history, theory, language |page=13 |author=Ronan, Deazley |isbn=978-1-84542-282-0 |year=2006 |publisher=Edward Elgar Publishing. |deadurl=yes |archiveurl=https://web.archive.org/web/20111119042246/https://www.google.com/books?id=dMYXq9V1JBQC&dq=statute+of+anne+copyright&lr=&as_brr=3&source=gbs_navlinks_s |archivedate=19 November 2011 }}</ref>\nThe act also alluded to individual rights of the artist. It began, "Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing... Books, and other Writings, without the Consent of the Authors... to their very great Detriment, and too often to the Ruin of them and their Families:".<ref>{{cite web|url=http://www.copyrighthistory.com/anne.html |title=Statute of Anne |publisher=Copyrighthistory.com |accessdate=2012-06-08}}</ref> A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work\'s creator appears in some countries\' copyright laws.\n\nThe [[Copyright Clause]] of the United States Constitution (1787) authorized copyright legislation: "To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries." That is, by guaranteeing them a period of time in which they alone could profit from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.\n\nThe original length of copyright in the United States was 14&nbsp;years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14‑year monopoly grant, but after that the work entered the [[public domain]], so it could be used and built upon by others.\n\nCopyright law was enacted rather [[Copyright in Germany|late in German states]], and the historian Eckhard Höffner argues that the absence of copyright laws in the early 19th century encouraged publishing, was profitable for authors, led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century.<ref name="thad">{{cite web |url=http://www.spiegel.de/international/zeitgeist/0,1518,710976,00.html | author=Frank Thadeusz |title=No Copyright Law: The Real Reason for Germany\'s Industrial Expansion? |publisher=[[Der Spiegel]] |date= 18 August 2010 |accessdate= 11 April 2015}}</ref>\n\n===International copyright treaties===\n{{See also|International copyright agreements|List of parties to international copyright agreements}}\n[[File:Joseph Ferdinand Keppler - The Pirate Publisher - Puck Magazine - Restoration by Adam Cuerden.jpg|\'\' The Pirate Publisher—An International Burlesque that has the Longest Run on Record\'\', from \'\'[[Puck (magazine)|Puck]]\'\', 1886, satirizes the then-existing situation where a publisher could profit by simply stealing newly published works from one country, and publishing them in another, and vice versa.|thumb|300px]]\nThe 1886 [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] first established recognition of copyrights among [[Sovereignty|sovereign nations]], rather than merely bilaterally. Under the Berne Convention, copyrights for [[creative works]] do not have to be asserted or declared, as they are automatically in force at creation: an author need not "register" or "apply for" a copyright in countries adhering to the Berne Convention.<ref name="Berne Convention for the Protection of Literary and Artistic Works Article 5">{{cite web |url=http://www.wipo.int/treaties/en/ip/berne/trtdocs_wo001.html#P109_16834 |title=Berne Convention for the Protection of Literary and Artistic Works Article 5 |accessdate=2011-11-18 |publisher=World Intellectual Property Organization}}</ref> As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100&nbsp;years later with the passage of the \'\'Copyright, Designs and Patents Act of 1988\'\'. The United States did not sign the Berne Convention until 1989.<ref>Garfinkle, Ann M; Fries, Janet; Lopez, Daniel; Possessky, Laura (1997). "Art conservation and the legal obligation to preserve artistic intent". [[JAIC]]   36 (2): 165–179.</ref>\n\nThe United States and most [[Latin America]]n countries instead entered into the [[Buenos Aires Convention]] in 1910, which required a copyright notice on the work (such as \'\'[[all rights reserved]]\'\'), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms.<ref>[http://www.copyright.gov/circs/circ38a.pdf "International Copyright Relations of the United States"], U.S.&nbsp;Copyright Office Circular No.&nbsp;38a, August&nbsp;2003.</ref><ref>[http://www.unesco.org/culture/copyright/html_eng/ucc52ms.pdf Parties to the Geneva Act of the Universal Copyright Convention] as of 2000-01-01: the dates given in the document are dates of ratification, not dates of coming into force. The Geneva Act came into force on 16 September 1955, for the first twelve to have ratified (which included four non-members of the Berne Union as required by Art.&nbsp;9.1), or three months after ratification for other countries. {{webarchive |url=https://web.archive.org/web/20080625003242/http://www.unesco.org/culture/copyright/html_eng/ucc52ms.pdf |date=25 June 2008 }}</ref><ref>[http://www.copyright.ht/en 165&nbsp;Parties to the Berne Convention for the Protection of Literary and Artistic Works] as of May 2012.</ref> The [[Universal Copyright Convention]] was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the [[Soviet Union]] and developing nations.\n\nThe regulations of the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] are incorporated into the [[World Trade Organization]]\'s [[Agreement on Trade-Related Aspects of Intellectual Property Rights|TRIPS]] agreement (1995), thus giving the Berne Convention effectively near-global application.<ref name="Contemporary Intellectual Property: Law and Policy">{{cite book |title=Contemporary Intellectual Property: Law and Policy|url= https://www.google.com/books?id=_Iwcn4pT0OoC&dq=contemporary+intellectual+property&source=gbs_navlinks_s |page=39 |author1=MacQueen, Hector L |author2=Charlotte Waelde |author3=Graeme T Laurie |isbn=978-0-19-926339-4 |year=2007 |publisher=Oxford University Press}}</ref> \n\nIn 1961, the [[United International Bureaux for the Protection of Intellectual Property]] signed the [[Rome Convention for the Protection of Performers, Producers of Phonograms and Broadcasting Organizations]]. In 1996, this organization was succeeded by the founding of the [[World Intellectual Property Organization]], which launched the 1996 [[WIPO Performances and Phonograms Treaty]] and the 2002 [[World Intellectual Property Organization Copyright Treaty|WIPO Copyright Treaty]], which enacted greater restrictions on the use of technology to copy works in the nations that ratified it. The [[Trans-Pacific Partnership]] includes [[Trans-Pacific Partnership Intellectual Property Provisions|intellectual Property Provisions]] relating to copyright.\n\nCopyright laws are standardized somewhat through these international conventions such as the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] and [[Universal Copyright Convention]]. These multilateral treaties have been ratified by nearly all countries, and [[international organizations]] such as the [[European Union]] or [[World Trade Organization]] require their member states to comply with them.\n\n==Obtaining protection==\n===Ownership===\nThe original holder of the copyright may be the employer of the author rather than the author himself, if the work is a "[[work for hire]]".<ref>17 U.S.C. § 201(b); Cmty. for Creative Non-Violence v. Reid, 490 U.S. 730 (1989)</ref> For example, in [[English law]] the \'\'Copyright, Designs and Patents Act\'\' 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a "Work for Hire."\n\n===Eligible works===\nCopyright may apply to a wide range of creative, intellectual, or artistic forms, or "works". Specifics vary by [[jurisdiction]], but these can include [[poem]]s, [[theses]], [[drama|plays]] and other [[book|literary works]], [[film|motion pictures]], [[choreography]], [[music|musical compositions]], [[sound recording]]s, [[painting]]s, [[drawing]]s, [[sculpture]]s, [[photography|photographs]], [[computer software]], [[radio]] and [[television]] [[Broadcasting|broadcasts]], and [[industrial design]]s. Graphic [[designs]] and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.<ref name="Intellectual Property and Information Wealth: Copyright and related rights">{{cite book |title=Intellectual Property and Information Wealth: Copyright and related rights|url=https://www.google.com/books?id=tgK9BzcF5WgC&dq=statute+of+anne+copyright&lr=&as_brr=3&source=gbs_navlinks_s|page=346 |author=Peter K, Yu |isbn=978-0-275-98883-8 |year=2007 |publisher=Greenwood Publishing Group}}</ref><ref>{{cite web |url= http://www.wipo.int/freepublications/en/intproperty/909/wipo_pub_909.pdf |format=PDF| last = World Intellectual Property Organization | title= Understanding Copyright and Related Rights|publisher=WIPO|accessdate=11 August 2016|page=8}}</ref>\n\nCopyright does not cover ideas and information themselves, only the form or manner in which they are expressed.<ref name="Art and copyright">{{cite book |title=Art and copyright|url=https://www.google.com/books?id=h-XBqKIryaQC&dq=idea-expression+dichotomy&lr=&as_brr=3&source=gbs_navlinks_s|pages=48–49 |author=Simon, Stokes |isbn=978-1-84113-225-9 |year=2001 |publisher=Hart Publishing }}</ref> For example, the copyright to a [[Mickey Mouse]] cartoon restricts others from making copies of the cartoon or creating [[derivative work]]s based on [[The Walt Disney Company|Disney\'s]] particular [[anthropomorphic]] mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney\'s.<ref name="Art and copyright"/> Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, [[Steamboat Willie]] is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection.\n\n===Originality===\n{{main|Threshold of originality}}\nTypically, a work must meet [[Threshold of originality|minimal standards of originality]] in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the [[United Kingdom]] there has to be some "skill, labour, and judgment" that has gone into it.<ref>\'\'Express Newspaper Plc v News (UK) Plc\'\', F.S.R. 36 (1991)</ref> In [[Australia]] and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a [[trademark]] instead.\n\nCopyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.\n\n===Registration===\n{{main|Copyright registration}}\n[[File:Fermat Last Theorem "proof" registered by Ukraine officials.jpg|thumb|right|A copyright certificate for proof of the Fermat theorem, issued by the State Department of Intellectual Property of Ukraine.]]\nIn all countries where the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] standards apply, copyright is automatic, and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce his or her exclusive rights.<ref name="Berne Convention for the Protection of Literary and Artistic Works Article 5"/> However, while registration isn\'t needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as \'\'[[prima facie]]\'\' evidence of a valid copyright and enables the copyright holder to seek [[statutory damages for copyright infringement|statutory damages]] and attorney\'s fees.<ref>{{cite web|title=Subject Matter and Scope of Copyright|url=http://copyright.gov/title17/92chap1.pdf|website=copyright.gov|accessdate=4 June 2015}}</ref> (In the USA, registering after an infringement only enables one to receive actual damages and lost profits.)\n\nA widely circulated strategy to avoid the cost of copyright registration is referred to as the [[poor man\'s copyright]]. It proposes that the creator send the work to himself in a sealed envelope by registered mail, using the [[postmark]] to establish the date. This technique has not been recognized in any published opinions of the United States courts. <!-- Note to editors: The previously-worded statement, "This technique has not been recognized by any United States court" is overbroad because not all such cases are reported, and it is impossible to know whether this is correct.--> The United States Copyright Office says the technique is not a substitute for actual registration.<ref>{{cite web|title=Copyright in General (FAQ)|url=http://www.copyright.gov/help/faq/faq-general.html#poorman|publisher=U.S Copyright Office|accessdate=11 Aug 2016}}</ref> The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original nor who the creator of the work is.<ref>[http://www.ipo.gov.uk/copy/c-claim/c-register.htm "Copyright Registers"], United Kingdom Intellectual Property Office</ref><ref>[http://www.ipo.gov.uk/types/copy/c-about/c-auto.htm "Automatic right"], United Kingdom Intellectual Property Office</ref>  <!-- Note to editors: The previously-worded statement, "The United Kingdom Intellectual Property Office discusses the technique but does not recommend its use." overstates the UK IPO position; the IPO does NOT recommend against the PMC approach.-->\n\n===Fixing===\nThe [[Berne Convention]] allows member countries to decide whether creative works must be "fixed" to enjoy copyright. Article 2, Section 2 of the Berne Convention states: "It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form." Some countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be "fixed in a tangible medium of expression" to obtain copyright protection.<ref name="cyber.law.harvard.edu">See Harvard Law School, [http://cyber.law.harvard.edu/copyrightforlibrarians/Module_3:_The_Scope_of_Copyright_Law#Fixation \'\'Module 3: The Scope of Copyright Law\'\']. See also Tyler T. Ochoa, [http://digitalcommons.law.scu.edu/chtlj/vol20/iss4/5 \'\'Copyright, Derivative Works and Fixation: Is Galoob a Mirage, or Does the Form(GEN) of the Alleged Derivative Work Matter?\'\'], 20 {{smallcaps|Santa Clara High Tech.}} L.J. 991, 999–1002 (2003) ("Thus, both the text of the Act and its legislative history demonstrate that Congress intended that a derivative work does not need to be fixed in order to infringe."). The legislative history of the 1976 Copyright Act says this difference was intended to address transitory works such as ballets, pantomimes, improvised performances, dumb shows, mime performances, and dancing.</ref> U.S. law requires that the fixation be stable and permanent enough to be "perceived, reproduced or communicated for a period of more than transitory duration." Similarly, Canadian courts consider fixation to require that the work be "expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance."<ref name="cyber.law.harvard.edu"/>\n\n===Copyright notice===\n{{main|Copyright notice}}\n[[File:Copyright.svg|thumb|upright|A copyright symbol used in copyright notice.]]\nBefore 1989, United States law required the use of a copyright notice, consisting of the [[copyright symbol]] (©, the letter \'\'\'C\'\'\' inside a circle), the abbreviation "Copr.", or the word "Copyright", followed by the year of the first publication of the work and the name of the copyright holder.<ref>Copyright Act of 1976, {{USPL|94|553}}, 90 Stat. 2541, § 401(a) (19 October 1976)</ref><ref>The Berne Convention Implementation Act of 1988 (BCIA), {{USPL|100|568}}, 102 Stat. 2853, 2857. One of the changes introduced by the BCIA was to section&nbsp;401, which governs copyright notices on published copies, specifying that notices "may be placed on" such copies; prior to the BCIA, the statute read that notices "shall be placed on all" such copies. An analogous change was made in section&nbsp;402, dealing with copyright notices on phonorecords.</ref> Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a [[sound recording copyright symbol]] (℗, the letter&nbsp;\'\'\'P\'\'\' inside a circle), which indicates a sound recording copyright, with the letter&nbsp;\'\'\'P\'\'\' indicating a "phonorecord". In addition, the phrase \'\'[[All rights reserved]]\'\' was once required to assert copyright, but that phrase is now legally obsolete.\n\nIn 1989 the United States enacted the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] Implementation Act, amending the 1976&nbsp;Copyright Act to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic.<ref>{{cite web|url=http://www.copyright.gov/circs/circ03.pdf |title=U.S. Copyright Office – Information Circular |format=PDF |accessdate=2012-07-07}}</ref> However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit&nbsp;– using notices of this form may reduce the likelihood of a defense of "innocent infringement" being successful.<ref>[[17 U.S.C.]]{{UnitedStatesCodeSec|17|401(d)}}</ref>\n\n==Enforcement==\nCopyrights are generally enforced by the holder in a [[Civil law (private law)|civil law]] court, but there are also criminal infringement statutes in some jurisdictions. While [[copyright registry|central registries]] are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily [[legal proof|prove]] that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the [[RIAA]] are increasingly targeting the [[file sharing]] home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (See: [[Legal aspects of file sharing]])\n\nIn most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative and or court costs. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.\n\n===Copyright infringement===\n{{main|Copyright infringement}}\nFor a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws and/or adheres to a bilateral treaty or established international convention such as the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] or [[World Intellectual Property Organization Copyright Treaty|WIPO Copyright Treaty]]. Improper use of materials outside of legislation is deemed "unauthorized edition", not copyright infringement.<ref>{{Cite journal | last1 = Owen | first1 = L. | doi = 10.1087/09531510125100313 | title = Piracy | journal = Learned Publishing | volume = 14 | pages = 67–70 | year = 2001 | pmid =  | pmc = }}</ref>\n\nCopyright infringement most often occurs to software, film and music. However, infringement upon books and other text works remains common, especially for educational reasons. Statistics regarding the effects of copyright infringement are difficult to determine. Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available.<ref>Butler, S. Piracy Losses "Billboard" 199(36)</ref> Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect.<ref>{{cite web|url=http://www.ejpd.admin.ch/content/ejpd/de/home/dokumentation/mi/2011/2011-11-30.html |title=Urheberrechtsverletzungen im Internet: Der bestehende rechtliche Rahmen genügt |publisher=Ejpd.admin.ch}}</ref> In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.<ref>{{cite journal|publisher=Social Science Electronic Publishing|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2425386|title=Video Killed the Radio Star? Online Music Videos and Digital Music Sales|ISSN=2042-2695|year=2014|authors=Tobias Kretschmer & Christian Peukert}}</ref>\n\n==Rights granted==\n===Exclusive rights===\nSeveral exclusive rights typically attach to the holder of a copyright:\n* to produce copies or reproductions of the work and to sell those copies (including, typically, electronic copies)\n* to import or export the work\n* to create [[derivative work]]s (works that adapt the original work)\n* to perform or display the work publicly\n* to sell or cede these rights to others\n* to transmit or display by radio or video.<ref name=autogenerated1>{{cite book |title=Intellectual Property and Information Wealth: Copyright and related rights |page=346 |author=Peter K, Yu |isbn=978-0-275-98883-8 |year=2007 |publisher=Greenwood Publishing Group}}</ref>\n\nThe phrase "exclusive right" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder\'s permission. Copyright is sometimes called a "negative right", as it serves to prohibit certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than permitting people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the [[unregistered design right]] in [[English law]] and [[European law]]. The rights of the copyright holder also permit him/her to not use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a [[Philosophy of copyright|philosophical interpretation of copyright law]] that is not universally shared. There is also debate on whether copyright should be considered a [[property right]] or a [[Moral rights (copyright law)|moral right]].<ref>Tom G. Palmer, [http://www.tomgpalmer.com/wp-content/uploads/papers/morallyjustified.pdf "Are Patents and Copyrights Morally Justified?"] Accessed 5 February 2013.</ref>\n\nIf a pictorial, graphic or sculptural work is a useful article, it is copyrighted only if its aesthetic features are separable from its utilitarian features. A useful article is an article having an intrinsic utilitarian function that is not merely to portray the appearance of the article or to convey information. They must be separable from the functional aspect to be copyrighted.<ref name="U.S Copyright Office - Copyright Law: Chapter 1">{{cite web |url=http://www.copyright.gov/title17/92chap1.pdf |title=U.S Copyright Office – Copyright Law: Chapter 1 |accessdate=2012-06-27}}</ref>\n\n===Duration===<!-- This section is linked from [[Little Nemo]] -->\n{{main|Copyright term|List of countries\' copyright length}}\n[[File:Tom Bell\'s graph showing extension of U.S. copyright term over time.svg|thumb|300px|Expansion of U.S. copyright law (currently based on the date of creation or publication).]]\nCopyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been [[Publication|published]], and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries\' laws (for example, the United States<ref>{{usc|17|305}}</ref> and the United Kingdom<ref>The Duration of Copyright and Rights in Performances Regulations 1995, [http://www.opsi.gov.uk/si/si1995/Uksi_19953297_en_3.htm part II], Amendments of the UK Copyright, Designs and Patents Act 1988</ref>), copyrights expire at the end of the calendar year in question.\n\nThe length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.<ref>{{cite book\n|title=Copyright: Sacred Text, Technology, and the DMCA|last=Nimmer|first=David |publisher=Kluwer Law International|year=2003|isbn=  978-90-411-8876-2|oclc=50606064|page=63|url= https://books.google.com/books?id=RYfRCNxgPO4C}}</ref>\n\nIn the United States, all books and other works published before 1923 have expired copyrights and are in the public domain.<ref>"[http://copyright.cornell.edu/resources/publicdomain.cfm Copyright Term and the Public Domain in the United States].", \'\'[[Cornell University]]\'\'.</ref> In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain. Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain.<ref>See Peter B. Hirtle, "Copyright Term and the Public Domain in the United States 1 January 2015" [https://copyright.cornell.edu/resources/publicdomain.cfm online at footnote 8]</ref>  Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.\n\nBut if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the U.S., the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.\n\nIn 1998, the length of a copyright in the United States was increased by 20 years under the [[Copyright Term Extension Act]]. This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point.<ref>Lawrence Lessig, \'\'Copyright\'s First Amendment\'\', 48 UCLA L. Rev. 1057, 1065 (2001)</ref>\n\n{{globalize/US|date=September 2016}}\n\n==Limitations and exceptions==\n{{main|Limitations and exceptions to copyright|Traditional safety valves}}\n\nIn many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses. It should be noted that US copyright does NOT cover names, title, short phrases or Listings (such as ingredients, recipes, labels, or formulas).<ref>[http://copyright.gov/circs/circ34.pdf (2012) \'\'Copyright Protection Not Available for Names, Titles, or Short Phrases\'\' U.S. Copyright Office]</ref> However, there are protections available for those areas copyright does not cover – such as [[trademark]]s and [[patent]]s.\n\nThere are some exceptions to what copyright will protect. Copyright will not protect:\n* Names of products\n* Names of businesses, organizations, or groups\n* Pseudonyms of individuals\n* Titles of works\n* Catchwords, catchphrases, mottoes, slogans, or short advertising expressions\n* Listings of ingredients in recipes, labels, and formulas, though the directions can be copyrighted\n\n===Idea–expression dichotomy and the merger doctrine===\n{{main|Idea–expression divide}}\n\nThe idea–expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of [[Baker v. Selden]], has since been codified by the [[Copyright Act of 1976]] at 17 U.S.C. § 102(b).\n\n===The first-sale doctrine and exhaustion of rights===\n\n{{main|First-sale doctrine|Exhaustion of rights}}\nCopyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or [[compact disc|CD]]. In the United States this is known as the [[first-sale doctrine]], and was established by the [[court]]s to clarify the legality of reselling books in second-hand [[bookstore]]s.\n\nSome countries may have [[parallel importation]] restrictions that allow the copyright holder to control the [[aftermarket (merchandise)|aftermarket]]. This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as [[exhaustion of rights]] in other countries and is a principle which also applies, though somewhat differently, to [[patent]] and [[trademark]] rights. It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved. It does not permit making or distributing additional copies.\n\nIn \'\'[[Kirtsaeng v. John Wiley & Sons, Inc.]]\'\',<ref>{{cite web|title=John Wiley & Sons Inc. v. Kirtsaeng |url= http://www.supremecourt.gov/opinions/12pdf/11-697_d1o2.pdf}}</ref> in 2013, the [[United States Supreme Court]] held in a 6-3 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner\'s permission and then imported into the US without such permission.  The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff\'s permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court\'s holding severely limits the ability of copyright holders to prevent such importation.\n\nIn addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying his or her own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement [[Moral rights (copyright law)|moral rights]], a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.\n\n===Fair use and fair dealing===\n{{main|Fair use|Fair dealing}}\nCopyright does not prohibit all copying or replication. In the United States, the [[Fair use|fair use doctrine]], codified by the [[United States Copyright Act of 1976|Copyright Act of 1976]] as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:\n# the purpose and character of one\'s use\n# the nature of the copyrighted work\n# what amount and proportion of the whole work was taken, and\n# the effect of the use upon the potential market for or value of the copyrighted work.<ref>{{cite web|url=http://www4.law.cornell.edu/uscode/17/107.html |title=US CODE: Title 17,107. Limitations on exclusive rights: Fair use |publisher=.law.cornell.edu |date=2009-05-20 |accessdate=2009-06-16}}</ref>\n\nIn the [[United Kingdom]] and many other [[Commonwealth of Nations|Commonwealth]] countries, a similar notion of fair dealing was established by the [[court]]s or through [[legislation]]. The concept is sometimes not well defined; however in [[Canada]], private copying for personal use has been expressly permitted by statute since 1999. In \'\'[[Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright)]]\'\', 2012 SCC 37, the [[Supreme Court of Canada]] concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the \'\'Copyright Act 1968\'\' (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder\'s consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. [[legal advice]]). Under current [[Law of Australia|Australian law]], although it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner, owners of a legitimate copy are permitted to “format shift” that work from one medium to another for personal, private use, or to “time shift” a broadcast work for later, once and only once, viewing or listening. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.\n\nIn the United States the AHRA ([[Audio Home Recording Act]] Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.\n\n:\'\'Section 1008. Prohibition on certain infringement actions\'\'\n\n:\'\'No action may be brought under this title alleging infringement of copyright based on the manufacture, importation, or distribution of a digital audio recording device, a digital audio recording medium, an analog recording device, or an analog recording medium, or based on the noncommercial use by a consumer of such a device or medium for making digital musical recordings or analog musical recordings.\'\'\n\nLater acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The [[Digital Millennium Copyright Act]] prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner.<ref name="Intellectual Property and Information Wealth: Copyright and related rights"/> An appellate court has held that fair use is not a defense to engaging in such distribution.\n\nThe [[copyright directive]] allows EU member states to implement a set of exceptions to copyright. Examples of those exceptions are:\n*photographic reproductions on paper or any similar medium of works (excluding sheet music) provided that the rightholders receives fair compensation,\n*reproduction made by libraries, educational establishments, museums or archives, which are non-commercial\n*archival reproductions of broadcasts,\n*uses for the benefit of people with a disability,\n*for demonstration or repair of equipment,\n*for non-commercial research or private study\n*when used in [[parody]]\n\n===Accessible copies===\nIt is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder.<ref>[http://www.copyright.gov/title17/92chap1.html#121 Copyright Law of the USA, Chapter 1 Section 121]</ref><ref>{{cite web|url=http://www.rnib.org.uk/xpedio/groups/public/documents/publicwebsite/public_cvipsact2002.hcsp|title=Copyright (Visually Impaired Persons) Act 2002 comes into force|publisher=Royal National Institute of Blind People|date=1 January 2011|accessdate=11 Aug 2016}}</ref>\n\n=={{anchor|Transfer and licensing, and assignment}} Transfer, assignment and licensing==\n{{see also|Collective rights management|extended collective licensing|Compulsory license|Copyright transfer agreement}}\n[[File:All rights reserved.jpg|thumb|right|300px|DVD: [[All Rights Reserved]].]]\nA copyright, or aspects of it (e.g. reproduction alone, all but moral rights), may be assigned or transferred from one party to another.<ref name="WIPO Guide on the Licensing of Copyright and Related Rights">{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&dq=copyright+transfer+and+licensing&as_brr=3&source=gbs_navlinks_s|isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization|page=15}}</ref> For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the [[Internet]]; however, the [[record industry]] attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy and/or distribute the work in a particular region or for a specified period of time.\n\nA transfer or licence may have to meet particular formal requirements in order to be effective,<ref name="WIPO Guide on the Licensing of Copyright and Related Rights(2)">{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&dq=copyright+transfer+and+licensing&as_brr=3&source=gbs_navlinks_s|page=8 |isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization}}</ref> for example under the Australian [[Copyright law of Australia#Copyright Act 1968|Copyright Act 1968]] the copyright itself must be expressly transferred in writing. Under the U.S. Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under [[Law of the United States|U.S. law]]. They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office\'s web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a [[real estate]] transaction.\n\nCopyright may also be [[license]]d.<ref name="WIPO Guide on the Licensing of Copyright and Related Rights"/> Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed [[statutory license]] (e.g. musical works in the United States used for radio broadcast or performance). This is also called a [[compulsory license]], because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made.<ref name="WIPO Guide on the Licensing of Copyright and Related Rights(3)">{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&dq=copyright+transfer+and+licensing&as_brr=3&source=gbs_navlinks_s|page=16 |isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization}}</ref> Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, [[copyright collective]]s or [[collecting societies]] and [[performance rights organisation|performing rights organizations]] (such as [[ASCAP]], [[Broadcast Music Incorporated|BMI]], and [[SESAC]]) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.\n\n===Free licences===\nCopyright licenses known as \'\'open\'\' or [[free license]]s seek to grant several rights to licensees, either for a fee or not, to an effect inspired by the [[public domain]]. \'\'Free\'\' in this context isn\'t much of a reference to price as it is to freedom. What constitutes free licensing has been characterised in a number of similar definitions, including by order of longevity the [[Free Software Definition]], the [[Debian Free Software Guidelines]], the [[Open Source Definition]] and the [[Definition of Free Cultural Works]]. Further refinements to these licenses have resulted in categories such as [[copyleft]] and [[permissive license|permissive]]. Common examples of free licences are the [[GNU General Public License]], [[BSD license]]s and some [[Creative Commons licenses]].\n\nFounded in 2001 by [[James Boyle (academic)|James Boyle]], [[Lawrence Lessig]], and [[Hal Abelson]], the [[Creative Commons]] (CC) is a non-profit organization<ref name="CC">{{cite web|url=http://creativecommons.org/ |title=Creative Commons Website|website=creativecommons.org|accessdate=24 October 2011}}</ref> which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of generic copyright license options to the public, [[gratis versus libre|gratis]]. These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.<ref name="CC" />\n\nTerms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available (although some of them aren\'t properly free per the above definitions and per Creative Commons\' own advice). These are based upon copyright holder stipulations such as whether he or she is willing to allow modifications to the work, whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work.<ref name="Rubin">Rubin, R. E. (2010) \'Foundations of Library and Information Science: Third Edition\', Neal-Schuman Publishers, Inc., New York, p. 341</ref> {{As of|2009}} approximately 130 million individuals had received such licenses.<ref name="Rubin" />\n\n==Criticism==\nSome sources are critical of particular aspects of the copyright system. This is known as a debate over [[copynorms]]. Particularly on the internet, there is discussion about the [[copyright aspects of downloading and streaming]], the [[copyright aspects of hyperlinking and framing]]. Such concerns are often couched in the language of [[digital rights]] and [[database right]]s. Discussions include \'\'[[Free Culture (book)|Free Culture]]\'\', a 2004 book by [[Lawrence Lessig]]. Lessig coined the term [[permission culture]] to describe a worst-case system. \'\'[[Good Copy Bad Copy]]\'\' (documentary) and [[RiP!: A Remix Manifesto]], discuss copyright. Some suggest an [[alternative compensation system]]. \n\nSome groups reject copyright altogether, taking an [[anti-copyright]] stance. The perceived inability to enforce copyright online leads some to advocate [[Crypto-anarchism|ignoring legal statutes when on the web]].\n\n==Public domain==\n{{main|Public domain}}\nCopyright, like other [[intellectual property rights]], is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be freely used or exploited by anyone. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a [[common law copyright]]. Public domain works should not be confused with works that are publicly available. Works posted in the [[internet]], for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author\'s copyright.\n\n==See also==\n{{Portal|Social and political philosophy|Law}}\n{{colbegin|colwidth=15em}}\n* [[Adelphi Charter]]\n* [[Artificial scarcity]]\n* [[Conflict of laws]]\n* [[Copyright Alliance]]\n* [[Copyright in architecture in the United States]]\n* [[Copyright on the content of patents and in the context of patent prosecution]]\n* [[Copyright for Creativity]]\n* [[Copyright infringement of software]]\n* [[Copyright on religious works]]\n* [[Creative Barcode]]\n* [[Digital rights management]]\n* [[Digital watermarking]]\n* [[Entertainment law]]\n* [[Freedom of panorama]]\n* [[Intellectual property education]]\n* [[Intellectual property protection of typefaces]]\n* [[List of Copyright Acts]]\n* [[List of copyright case law]]\n* [[Model release]]\n* [[Paracopyright]]\n* [[Photography and the law]]\n* [[Pirate Party]]\n* [[Private copying levy]]\n* [[Production music]]\n* [[Rent-seeking]]\n* [[Reproduction fees]]\n* [[Samizdat]]\n* [[Software copyright]]\n* [[Threshold pledge system]]\n{{colend}}\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n{{refbegin|30em}}\n* {{Cite book\n  |author=Dowd, Raymond J.\n  |title=Copyright Litigation Handbook\n  |publisher=Thomson West |edition=1st |year=2006\n  |isbn=0-314-96279-4\n  |ref=Dowd, Litigation handbook\n}}\n* Ellis, Sara R. \'\'Copyrighting Couture: An Examination of Fashion Design Protection and Why the DPPA and IDPPPA are a Step Towards the Solution to Counterfeit Chic\'\', 78 Tenn. L. Rev. 163 (2010), \'\'available at\'\' http://ssrn.com/abstract=1735745.\n* {{Cite book\n  |author1=Gantz, John  |author2=Rochester, Jack B.\n  |title=Pirates of the Digital Millennium\n  |publisher=Financial Times Prentice Hall\n  |year=2005\n  |isbn=0-13-146315-2\n  |ref=Gantz, Pirates\n}}\n* [[Shuman Ghosemajumder|Ghosemajumder, Shuman]]. \'\'[http://dspace.mit.edu/handle/1721.1/8438 Advanced Peer-Based Technology Business Models]\'\'. [[MIT Sloan School of Management]], 2002.\n* [[Bruce Lehman|Lehman, Bruce]]: \'\'Intellectual Property and the National Information Infrastructure\'\' (Report of the Working Group on Intellectual Property Rights, 1995)\n* Lindsey, Marc: \'\'Copyright Law on Campus.\'\' [[Washington State University]] Press, 2003. ISBN 978-0-87422-264-7.\n* Mazzone, Jason. \'\'[[Copyfraud]]\'\'. [http://ssrn.com/abstract=787244 SSRN]\n* McDonagh, Luke. \'\'Is Creative use of Musical Works without a licence acceptable under Copyright?\'\' International Review of Intellectual Property and Competition Law (IIC) 4 (2012) 401-426, available at [http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2521081 SSRN]\n* {{Cite book\n  | last = Nimmer | first = Melville |authorlink=Melville Nimmer |author2=David Nimmer | title = [[Nimmer on Copyright]] | publisher = Matthew Bender | year=1997| isbn = 0-8205-1465-9\n}}\n* {{Cite book\n  |title=Copyright in Historical Perspective\n  |author=Patterson, Lyman Ray\n  |year=1968|publisher=Vanderbilt University Press\n  |isbn=0-8265-1373-5\n  |version=Online Version\n}}\n* Rife, by Martine Courant. \'\'Convention, Copyright, and Digital Writing\'\'  (Southern Illinois University Press; 2013) 222 pages; Examines legal, pedagogical, and other aspects of online authorship.\n* {{cite book | last = Rosen | first = Ronald | title = Music and Copyright | publisher = Oxford University Press | location = Oxford Oxfordshire | year = 2008 | isbn = 0-19-533836-7 }}\n* Shipley, David E. [http://ssrn.com/abstract=1076789 Thin But Not Anorexic: Copyright Protection for Compilations and Other Fact Works] UGA Legal Studies Research Paper No. 08-001; Journal of Intellectual Property Law, Vol. 15, No. 1, 2007.\n* Silverthorne, Sean. \'\'[http://hbswk.hbs.edu/item.jhtml?id=4206&t=innovation Music Downloads: Pirates- or Customers?]\'\'. [[Harvard Business School|Harvard Business School Working Knowledge]], 2004.\n* Sorce Keller, Marcello. "Originality, Authenticity and Copyright", \'\'Sonus\'\', VII(2007), no. 2, pp.&nbsp;77–85.\n* {{Cite book\n  |author1=Steinberg, S.H.  |author2=Trevitt, John\n  |title=Five Hundred Years of Printing\n  |location=London and New Castle |publisher=The British Library and Oak Knoll Press\n  |edition=4th |year=1996\n  |isbn=1-884718-19-1\n  |ref=Steinberg, Five hundred years\n}}\n* {{Cite book\n  |title=The Copy/South Dossier: Issues in the Economics, Politics and Ideology of Copyright in the Global South \n  |url=http://copysouth.org/en/documents/csdossier.pdf\n  |editor1=Story, Alan |editor2=Darch, Colin |editor3=Halbert, Deborah |year=2006|publisher=Copy/South Research Group\n  |isbn=978-0-9553140-1-8\n}}\n* [http://whynotaskme.org/ WhyNotAskMe.org]: \'\'Organization demanding democratic participation in copyright legislation and a moratorium on secret and fast-tracked copyright negotiations\'\'\n{{refend}}\n\n==External links==\n{{Wikisource1911Enc|Copyright}}\n{{Wikisource|Wikisource:Copyright law|Copyright law}}\n{{Spoken Wikipedia|En-Copyright.ogg|2008-12-30}}\n{{Library resources box}}\n* {{Wikiquote-inline}}\n* {{Commons-inline|Copyright}}\n* {{dmoz|Society/Law/Legal_Information/Intellectual_Property/Copyrights}}\n* [http://www.wipo.int/clea/en/ Collection of laws for electronic access] from [[WIPO]] – intellectual property laws of many countries\n* [http://purl.fdlp.gov/GPO/gpo55676 Compendium of Copyright Practices] (3rd ed.) [[United States Copyright Office]]\n* [http://ucblibraries.colorado.edu/govpubs/us/copyrite.htm Copyright] from \'\'UCB Libraries GovPubs\'\'\n* [http://www.ipo.gov.uk/types/copy.htm About Copyright] at the UK Intellectual Property Office\n* [http://www.lawtech.jus.unitn.it/index.php/copyright-history/bibliography A Bibliography on the Origins of Copyright and Droit d\'Auteur]\n* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-912-introduction-to-copyright-law-january-iap-2006/ 6.912 Introduction to Copyright Law] taught by Keith Winstein, MIT OpenCourseWare January IAP 2006\n* [http://www.wipo.int/treaties/en/ShowResults.jsp?country_id=ALL&start_year=ANY&end_year=ANY&search_what=C&treaty_id=15 Copyright Berne Convention: Country List]  List of the 164 members of the Berne Convention for the protection of literary and artistic works\n* [http://www.copyrightservice.co.uk/copyright/p01_uk_copyright_law UK Copyright Law fact sheet] (April 2000) a concise introduction to UK Copyright legislation\n* [http://www.jisc.ac.uk/whatwedo/themes/content/contentalliance/reports/ipr.aspx IPR Toolkit – An Overview, Key Issues and Toolkit Elements] (September 2009) by Professor Charles Oppenheim and Naomi Korn at the [http://www.jisc.ac.uk/whatwedo/themes/content/contentalliance.aspx Strategic Content Alliance]\n* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-912-introduction-to-copyright-law-january-iap-2006/ MIT OpenCourseWare 6.912 Introduction to Copyright Law] Free self-study course with video lectures as offered during the January 2006, Independent Activities Period (IAP)\n* [http://www.loc.gov/rr/rarebook/coll/067.html Early Copyright Records] From the [http://www.loc.gov/rr/rarebook/ Rare Book and Special Collections Division at the Library of Congress]\n* [http://copyright.gov/title17/ Copyright Law of the United States Documents], US Government\n{{Copyright law by country}}\n{{Intellectual property activism}}\n\n{{Authority control}}\n\n[[Category:Copyright law| ]]\n[[Category:Data management]]\n[[Category:Intellectual property law]]\n[[Category:Metadata]]\n[[Category:Monopoly (economics)]]\n[[Category:Public records]]']
['Couchbase Server', '28366048', '{{Infobox software\n| name                   = Couchbase Server\n| logo                   = [[File:CouchbaseLogo.svg|224px]]\n| screenshot             = Couchbase Server Screenshot.jpg\n| caption                =\n| developer              = [[Couchbase, Inc.]]\n| released               = {{Start date|2010|08}}\n| latest release version = 4.5\n| latest release date    = {{release date|2016|06|22}}\n| status                 = active\n| programming language   = [[C++]], [[Erlang (programming language)|Erlang]], [[C (programming language)|C]],<ref>{{cite web |author= Damien Katz|url=http://damienkatz.net/2013/01/the_unreasonable_effectiveness_of_c.html |title=The Unreasonable Effectiveness of C |date= January 8, 2013 |accessdate= September 30, 2016 }}</ref> [[Go (programming language)|Go]]\n| operating system       = [[Cross-platform]]\n| genre                  = [[Multi-model database]] / [[Key-value database|Distributed Key-Value]] / [[Document-oriented database]]\n| license                = [[Apache License]] (Open Source edition), [[Proprietary software|Proprietary]] (Free Community edition and Paid Enterprise edition)\n| website                = {{URL|http://www.couchbase.com/}}\n| frequently updated     = yes\n}}\n\n\'\'\'Couchbase Server\'\'\', originally known as \'\'\'Membase\'\'\', is an [[open-source]], distributed ([[shared-nothing architecture]]) [[Multi-model database|multi-model]] [[NoSQL]] [[document-oriented database]] software package that is optimized for interactive applications. These applications may serve many [[concurrent user]]s by creating, storing, retrieving, aggregating, manipulating and presenting data. In support of these kinds of application needs, Couchbase Server is designed to provide easy-to-scale key-value or JSON document access with low latency and high sustained throughput. It is designed to be [[Cluster (computing)|clustered]] from a single machine to very large-scale deployments spanning many machines.\nA version originally called \'\'\'Couchbase Lite\'\'\' was later marketed as Couchbase Mobile combined with other software.\n\nCouchbase Server provided client protocol compatibility with [[memcached]],<ref>{{cite web|url=http://code.google.com/p/memcached/wiki/NewProtocols |title=NewProtocols - memcached - Klingon - Memcached - Google Project Hosting |publisher=Code.google.com |date=2011-08-22 |accessdate=2013-06-04}}</ref> but added disk [[Persistence (computer science)|persistence]], [[data replication]], live cluster reconfiguration, rebalancing and [[multitenancy]] with [[Partition (database)|data partitioning]].\n\n==Product history==\nMembase was developed by several leaders of the [[memcached]] project, who had founded a company, NorthScale, to develop a [[key-value store]] with the simplicity, speed, and scalability of memcached, but also the storage, persistence and querying capabilities of a database. The original membase source code was contributed by NorthScale, and project co-sponsors [[Zynga]] and [[Naver Corporation]] (then known as NHN) to a new project on membase.org in June 2010.<ref>{{Cite book |title= Professional NoSQL |author= Shashank Tiwari  |publisher= John Wiley & Sons |pages= 15–16 |isbn= 9781118167809 }}</ref>\n\nOn February 8, 2011, the Membase project founders and Membase, Inc. announced a merger with CouchOne (a company with many of the principal players behind [[CouchDB]]) with an associated project merger. The merged company was called [[Couchbase, Inc.]] In January 2012, Couchbase released Couchbase Server 1.8. \nIn September, 2012, [[Orbitz]] said it had changed some of its systems to use Couchbase.<ref>{{cite web |url= http://gigaom.com/cloud/balancing-oracle-and-open-source-at-orbitz/ |title= Balancing Oracle and open source at Orbitz |publisher=[[GigaOM]] |date= September 21, 2012 |accessdate= September 19, 2016 }}</ref>\nOn December 2012, \nCouchbase Server 2.0 (announced in July 2011) was released and included a new [[JSON]] document store, indexing and querying, incremental [[MapReduce]] and [[Replication (computing)|replication]] across [[data center]]s.<ref name="zd2">{{cite web |url= http://www.zdnet.com/couchbase-2-0-released-implements-json-document-store-7000008649/ |title= Couchbase 2.0 released; implements JSON document store |publisher= [[ZDNet]] |author=  Andrew Brust |date= December 12, 2012}}</ref><ref>{{Cite web |title= Couchbase goes 2.0, pushes SQL for NoSQL |author= Derrick Harris |date= July 29, 2011 |work= GigaOm |url= https://gigaom.com/2011/07/29/couchbase-2-0-unql-sql-nosql/ |accessdate= September 19, 2016 }}</ref>\n\n==Architecture==\nEvery Couchbase node consists of a data service, index service, query service, and cluster manager component. Starting with the 4.0 release, the three services can be distributed to run on separate nodes of the cluster if needed.\nIn the parlance of Eric Brewer’s [[CAP theorem]], Couchbase is normally a CP type system meaning it provides [[Consistency (database systems)|consistency]] and [[Network partitioning|partition tolerance]], or it can be set up as an AP system with multiple clusters.\n\n===Cluster manager===\nThe cluster manager supervises the configuration and behavior of all the servers in a Couchbase cluster. It configures and supervises inter-node behavior like managing replication streams and re-balancing operations. It also provides metric aggregation and consensus functions for the cluster, and a [[REST]]ful cluster management interface. The cluster manager uses the [[Erlang (programming language)|Erlang programming language]] and the [[Open Telecom Platform]].\n\n====Replication and fail-over====\n[[Data replication]] within the nodes of a cluster can be controlled with several parameters.\nIn December 2012, replication was also supported between different [[data center]]s.<ref name="zd2" />\n\n===Data manager===\nThe data manager stores and retries documents in response to data operations from applications.\nIt asynchronously writes data to disk after acknowledging to the client.  In version 1.7 and later, applications can optionally ensure data is written to more than one server or to disk before acknowledging a write to the client.\nParameters define item ages that affect when data is persisted, and how max memory and migration from main-memory to disk is handled.\nIt supports working sets greater than a memory quota per "node" or "bucket".\nExternal systems can subscribe to filtered data streams, supporting, for example, [[full text search]] indexing, [[data analytics]] or archiving.<ref>{{Cite web |url= http://blog.couchbase.com/want-know-what-your-memcached-servers-are-doing-tap-them |title= Want to know what your memcached servers are doing? Tap them |author= Trond Norbye |work= Couchbase blog |date= March 15, 2010}}</ref>\n\n====Data format====\nA document is the most basic unit of data manipulation in Couchbase Server. Documents are stored in JSON document format with no predefined schemas.\n\n====Object-managed cache====\nCouchbase Server includes a built-in multi-threaded object-managed cache that implements memcached compatible APIs such as get, set, delete, append, prepend etc.\n\n====Storage engine ====\nCouchbase Server has a tail-append storage design that is immune to data corruption, [[OOM killer]]s or sudden loss of power.  Data is written to the data file in an append-only manner, which enables Couchbase to do mostly sequential writes for update, and provide an optimized access patterns for disk I/O.\n\n=== Performance ===\nA performance benchmark done by [[Altoros]] in 2012, compared Couchbase Server with other technologies.<ref>{{cite web |url= http://www.couchbase.com/nosql-resources/presentations/benchmarking-couchbase%5B2%5D.html |title= Benchmarking Couchbase |author= Frank Weigel |publisher=Couchbase |date= October 30, 2012 |accessdate= September 30, 2016 }}</ref>\n[[Cisco Systems]] published a benchmark that measured the latency and throughput of Couchbase Server with a mixed workload in 2012.<ref>{{cite web |url= http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/white_paper_c11-708169.pdf |title=Cisco and Solarflare Achieve Dramatic Latency Reduction for Interactive Web Applications with Couchbase, a NoSQL Database |publisher=[[Cisco Systems]] |date= June 18, 2012 |archivedate=  August 13, 2012 |archiveurl= https://web.archive.org/web/20120813162214/http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/white_paper_c11-708169.pdf |accessdate= October 7, 2016 }}</ref>\n\n== Licensing and support ==\nCouchbase Server is a packaged version of Couchbase\'s [[open source software]] technology and is available in a community edition without recent bug fixes with Apache 2.0 license.<ref>{{cite web |url= http://developer.couchbase.com/open-source-projects |title=Couchbase Open Source Projects |work= Couchbase web site |accessdate= October 7, 2016 }}</ref> and an edition for commercial use.<ref>{{cite web|url=http://www.couchbase.com/couchbase-server/editions|title=Couchbase Server Editions|publisher= Couchbase }}</ref> \nCouchbase Server builds are available for Ubuntu, Debian, Red Hat, SUSE, Oracle Linux, [[Microsoft Windows]] and Mac OS X operating systems.\n\nCouchbase has supported software developers\' kits for the programming languages [[.NET Framework|.Net]], [[PHP]], [[Ruby (programming language)|Ruby]], [[Python (programming language)|Python]], [[C (programming language)|C]], [[Node.js]], [[Java (programming language)|Java]], and [[Go (programming language)|Go]].\n\n==N1QL==\nA [[query language]] called the non-first normal form query language, N1QL (pronounced nickel), is used for manipulating the JSON data in Couchbase, just like SQL manipulates data in RDBMS. It has SELECT, INSERT, UPDATE, DELETE, MERGE statements to operate on JSON data.\nIt was announced in March 2015 as "SQL for documents".<ref>{{Cite web |title= Ssssh!  don’t tell anyone but Couchbase is a serious contender: Couchbase Live Europe 2015 |author= Andrew Slater |date= March 24, 2015 |accessdate= September 19, 2016 }}</ref>\n\nThe N1QL [[data model]] is [[Database normalization#Non-first normal form .28NF.C2.B2 or N1NF.29|non-first normal form]] (N1NF) with support for nested attributes and domain-oriented [[Database normalization|normalization]].  The N1QL data model is also a proper superset and generalization of the [[relational model]].\n\n===Example===\n<source lang="json">\n{\n  "email":"testme@gmail.com",\n  "friends":[\n            {"name":"rick"},\n            {"name":"cate"}\n           ]\n}\n</source>\n\n;Like Query: {{code|2=sql|SELECT * FROM `bucket` WHERE LIKE "%@gmail.com";}}\n\n;Array Query: {{code|2=sql|1=SELECT * FROM `bucket` WHERE ANY x IN friends SATISFIES x.name = "cate" END;}}\n\n==Bibliography==\n* {{cite book |last=Brown |first=MC |editor-first=|editor-last=|title=Getting Started with Couchbase Server (1st edition) |publisher=O\'Reilly Media |date=June 22, 2012 |page=88 | isbn=978-1449331061}}\n* {{citation\n| first1    = David\n| last1     = Ostrovsky\n| first2   = Mohammed\n| last2   = Haji\n| first3  = Yaniv\n| last3   = Rodenski\n| date      = November 26, 2015\n| title     = Pro Couchbase Server 2nd ed.\n| edition   = 2nd\n| publisher = [[Apress]]\n| page     = 349\n| isbn      = 978-1484211861\n}}\n* {{citation\n| first1    = Henry\n| last1     = Potsangbam\n| date      = November 23, 2015\n| title     = Learning Couchbase\n| edition   = 1st\n| publisher = [[Packt]]\n| page     = 202\n| isbn      = 978-1785288593\n}}\n* {{citation\n| first1    = Deepak\n| last1     = Vohra\n| date      = August 3, 2015\n| title     = Pro Couchbase Development: A NoSQL Platform for the Enterprise\n| edition   = 1st\n| publisher = [[Apress]]\n| page     = 331\n| isbn      = 978-1484214350\n}}\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{Official website}}\n\n[[Category:Free database management systems]]\n[[Category:Distributed computing architecture]]\n[[Category:NoSQL]]\n[[Category:Cross-platform software]]\n[[Category:Structured storage]]\n[[Category:Client-server database management systems]]\n[[Category:Database-related software for Linux]]\n[[Category:Applications of distributed computing]]\n[[Category:Databases]]\n[[Category:Data management]]\n[[Category:Distributed data stores]]']
['Sedona Canada Principles', '48536375', '{{orphan|date=January 2016}}\n<!-- Don\'t mess with this line! --><!-- Write your article below this line -->\nThe \'\'\'Sedona Canada Principles\'\'\' are a set of authoritative guidelines published by The Sedona Conference ® to aid members of the Canadian legal community involved in the identification, collection, preservation, review and production of [[electronically stored information]] (ESI).  The principles were drafted by a small group of lawyers, judges and technologists called the Sedona Working Group 7 or \'\'Sedona Canada\'\'.  Sedona Canada is an offshoot of The Sedona Conference ® which is an American “non-profit…research and educational institute dedicated to the advanced study of law and policy in the areas of antitrust law, complex litigation, and intellectual property rights.”<ref>{{cite web|url=https://thesedonaconference.org/|title=The Sedona Conference® - "Moving the law forward in a reasoned and just way."|work=thesedonaconference.org}}</ref>\n\n==Background==\n[[Civil procedure in Canada]] is jurisdictional with each province following its own rules of civil procedure.<ref>{{cite web|url=https://en.wikibooks.org/wiki/Canadian_Civil_Procedure/Rules_by_Province|title=Canadian Civil Procedure/Rules by Province|work=wikibooks.org}}</ref> However, each province must address the fact that due to the advancement of technology the discovery process enshrined in the rules of civil procedure can be potentially derailed due to the sheer volume of [[electronically stored information]] (ESI). <ref name="mccarthy.ca">{{cite web|url=http://www.mccarthy.ca/article_detail.aspx?id=4068|title=McCarthy Tétrault - Taming the Beast of Electronic Discovery with Sedona Canada Principles - Article Detail|work=mccarthy.ca}}</ref> When dealing with litigation matters that involve [[electronically stored information]] (ESI), the discovery process is commonly called [[electronic discovery|e-discovery]].  The problems associated with [[electronic discovery|e-discovery]] in Canada led to the creation of the Sedona Canada Principles. <ref name="mccarthy.ca"/> Rule 29.1.03(4) of the [[wikibooks:Ontario Rules of Civil Procedure]] specifically refers to the Sedona Canada Principles in referencing Principles re Electronic Discovery although it has been reported that this rule has been largely ignored in practice.<ref name="canlii.org">{{cite web|url=http://www.canlii.org/en/on/onsc/doc/2010/2010onsc3670/2010onsc3670.html|title=CanLII - 2010 ONSC 3670 (CanLII)|work=canlii.org}}</ref>\n\n==Summary==\nThe Sedona Canada Principles largely refer to the processes found in the Electronic Discovery Reference Model.<ref>{{cite web|url=http://www.edrm.net/resources/edrm-stages-explained|title=EDRM Stages|work=edrm.net}}</ref>\n\nThe principles urge proportionality due to the potentially enormous volumes of documents that may be discoverable when dealing with ESI.  They also encourage [[good faith]] in the document preservation stage and regular meetings between parties to discuss the scope of the litigation.  Parties are urged to be aware of the potential costs involved in producing relevant ESI but are advised that only reasonably accessible ESI need be produced.  The principles stipulate that parties should not be required to search for or collect deleted material unless there is an agreement or court order related to those terms.  The use of electronic tools and processes such as data sampling and web harvesting are acceptable practices.  Parties are encouraged to agree early in the litigation process on production format required for the exchange of relevant documents as part of the discovery process (native files, [[pdf]], [[Tagged Image File Format|tiff]], [[metadata]] requirements etc).  Agreements or direction should be sought, if necessary, with respect to [[wikt:privilege|privilege]] or other confidential information related to production of electronic documents and data.  Parties should be aware that legal precedents can be formed as a result of [[e-discovery]] practices and sanctions can be considered for a party’s failure to meet their discovery obligations unless it can be demonstrated that the failure was not intentional.  All parties must bear the “reasonable” costs associated with [[e-discovery]] but other arrangements can be agreed upon by the parties or by court order.<ref>{{cite web|url=https://www.canlii.org/en/commentary/sedonacanada/principles_en.html|title=CanLII - The Sedona Canada Principles Addressing Electronic Discovery (Jan. 2008)|work=canlii.org}}</ref>\n\n==Caselaw==\n\nIn \'\'Warman v. National Post Company\'\' proportionality was at issue in a case where the plaintiff was suing the defendant for libel.  A motion was brought by the defendant to have the plaintiff provide a mirror image of his hard drive in an effort to prove an internet article was indeed authored by the plaintiff.  Issues of proportionality and the work of the Sedona Conference and Sedona Canada Principles were factored in to the decision to grant the defendant only limited access to the hard drive.<ref name="canlii.org"/>\n\nIn \'\'Innovative Health Group Inc. v. Calgary Health Region\'\' the plaintiff’s legal obligation to produce imaged hard drives is in question.  Justice Conrad refers to the advice of Sedona Canada on proportionality and problems associated with time and expense related to the difficulties associated with electronically stored information.<ref>{{cite web|url=http://www.canlii.org/en/ab/abca/doc/2008/2008abca219/2008abca219.html|title=CanLII - 2008 ABCA 219 (CanLII)|work=canlii.org}}</ref>\n\nIn \'\'York University v. Michael Markicevic\'\' Justice Brown specifically refers to the need for the parties to agree upon a formal e-discovery plan to be drafted in consultation with Sedona Canada Principles.<ref name="canlii.org1">{{cite web|url=http://www.canlii.org/en/on/onsc/doc/2013/2013onsc378/2013onsc378.html|title=CanLII - 2013 ONSC 378 (CanLII)|work=canlii.org}}</ref>\n\nIn \'\'Friends of Lansdowne v. Ottawa\'\' Master MacLeod refers to the need for Sedona Canada principles and states “This is particularly true in the current information age when e-mail is ubiquitous and multiple copies or variants of messages may be held on various kinds of data storage devices including individual hard drives, e-mail and Blackberry servers.  Even documents that ultimately exist in paper form normally begin their life on computers and negotiations frequently involve exchanges of electronic drafts.  To find every scrap of paper and every electronic trace of relevant information has become a nightmarish task that threatens to render any kind of litigation extravagantly expensive.”<ref name="canlii.org1"/>\n\n==Criticism==\n\nCritics of the Sedona Canada Principles believe they should address [[system integrity]] and that the true history of any file preserved cannot be identified without proof of the integrity of the electronic record systems management it comes from.<ref>{{cite web|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2530515|title=The Sedona Canada Principles are Very Inadequate on Records Management and for Electronic Discovery|work=ssrn.com}}</ref>\n\nOther criticism is more directed to the Sedona Canada working group and complaints that it is insular and irrelevant<ref>{{cite web|url=http://www.canadianlawyermag.com/5078/Sedona-Canada-is-alive-and-well.html|title=Sedona Canada is alive and well|author=Colin Campbell and James Swanson|work=canadianlawyermag.com}}</ref>\n\n==External links==\n\n[https://www.canlii.org/en/commentary/sedonacanada/principles_en.html/The Sedona Canada Principles]\n\n[http://www.canadianlawyermag.com/5078/Sedona-Canada-is-alive-and-well.html/ Sedona Canada is alive and well]\n\n[https://www.highbeam.com/doc/1G1-181488000.html/ Taming the beast of eDiscovery with Sedona Canada Principles]\n\n[https://www.highbeam.com/doc/1G1-400332555.html/ 2014 eDiscovery Year in Review includes Sedona Canada Principles]\n\n[http://www.canadianlawyermag.com/legalfeeds/469/ontario-judge-slams-dark-ages-court-system.html/Ontario Courts discuss Sedona Canada Principles]\n\n[http://www.canadianlawyermag.com/3988/What-is-predictive-coding-and-can-it-help-me.html/ Sedona Canada Principles and predictive coding]\n\n[http://www.canadianlawyermag.com/5019/Alternative-routes.html/ Document review using Sedona Canada Principles]\n\n\n==References==\n\n{{reflist}}\n<!-- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. -->\n*\n*\n*\n*\n\n<!-- STOP! Be warned that by using this process instead of Articles for Creation, this article is subject to scrutiny. As an article in "mainspace", it will be DELETED if there are problems, not just declined. If you wish to use AfC, please return to the Wizard and continue from there. -->\n\n\n\n[[Category:Data management]]']
['WCF Data Services', '10988544', '{{Primary sources|date=November 2010}}\n\'\'\'[[Windows Communication Foundation|WCF]] Data Services\'\'\' (formerly \'\'\'ADO.NET Data Services\'\'\',<ref>{{cite web|url=http://blogs.msdn.com/astoriateam/archive/2009/11/17/simplifying-our-n-tier-development-platform-making-3-things-1-thing.aspx|title=Simplifying our n-tier development platform: making 3 things 1 thing|date=2009-11-17|accessdate=2009-12-17|publisher=ADO.NET Data Services Team Blog}}</ref> codename \'\'\'"Astoria"\'\'\')<ref>{{cite web | url = http://blogs.msdn.com/data/archive/2007/12/10/ado-net-data-services-ctp-released.aspx | title = ADO.NET Data Services CTP Released! | accessdate = 2007-11-12}}</ref> is a platform for what [[Microsoft]] calls \'\'Data Services\'\'. It is actually a combination of the runtime and a [[web service]] through which the services are exposed. In addition, it also includes the \'\'\'Data Services Toolkit\'\'\' which lets Astoria Data Services be created from within [[ASP.NET]] itself. The Astoria project was announced at [[MIX (Microsoft)|MIX]] 2007, and the first developer preview was made available on April 30, 2007. The first [[Software release life cycle#Beta|CTP]] was made available as a part of the [[ASP.NET]] 3.5 Extensions Preview. The final version was released as part of [[Service Pack]] 1 of the [[.NET Framework 3.5]] on August 11, 2008.  The name change from ADO.NET Data Services to WCF data Services was announced at the 2009 [[Professional Developers Conference|PDC]].\n\n==Overview==\nWCF Data Services exposes data, represented as [[ADO.NET#Entity Framework|Entity Data Model]] (EDM) objects, via web services accessed over [[HTTP]]. The data can be addressed using a [[Representational State Transfer|REST]]-like [[URI]]. The data service, when accessed via the HTTP GET method with such a URI, will return the data. The web service can be configured to return the data in either plain [[XML]], [[JSON]] or [[Resource Description Framework|RDF+XML]]. In the initial release, formats like [[RSS]] and [[ATOM]] are not supported, though they may be in the future. In addition, using other HTTP methods like PUT, POST or DELETE, the data can be updated as well. POST can be used to create new entities, PUT for updating an entity, and DELETE for deleting an entity.\n\n==Description==\n\nWindows Communication Foundation (WCF) comes to the rescue when we find ourselves not able to achieve what we want to achieve using web services, i.e., other protocols support and even duplex communication. With WCF, we can define our service once and then configure it in such a way that it can be used via HTTP, TCP, IPC, and even Message Queues. We can consume Web Services using server side scripts (ASP.NET), JavaScript Object Notations (JSON), and even REST (Representational State Transfer).\n\n\'\'\'Understanding the basics\'\'\'\n\nWhen we say that a WCF service can be used to communicate using different protocols and from different kinds of applications, we will need to understand how we can achieve this. If we want to use a WCF service from an application, then we have three major questions:\n\n\'\'\'1.\'\'\'Where is the WCF service located from a client\'s perspective?\n\'\'\'2.\'\'\'How can a client access the service, i.e., protocols and message formats?\n\'\'\'3.\'\'\'What is the functionality that a service is providing to the clients?\n\nOnce we have the answer to these three questions, then creating and consuming the WCF service will be a lot easier for us. The WCF service has the concept of endpoints. A WCF service provides endpoints which client applications can use to communicate with the WCF service. The answer to these above questions is what is known as the ABC of WCF services and in fact are the main components of a WCF service. So let\'s tackle each question one by one.\n\n\'\'\'Address:\'\'\' Like a webservice, a WCF service also provides a URI which can be used by clients to get to the WCF service. This URI is called as the Address of the WCF service. This will solve the first problem of "where to locate the WCF service?" for us.\n\n\'\'\'Binding:\'\'\' Once we are able to locate the WCF service, we should think about how to communicate with the service (protocol wise). The binding is what defines how the WCF service handles the communication. It could also define other communication parameters like message encoding, etc. This will solve the second problem of "how to communicate with the WCF service?" for us.\n\n\'\'\'Contract:\'\'\' Now the only question we are left up with is about the functionalities that a WCF service provides. Contract is what defines the public data and interfaces that WCF service provides to the clients.\n\n\nThe URIs representing the data will contain the physical location of the service, as well as the service name. In addition, it will also need to specify an EDM Entity-Set or a specific entity instance, as in respectively\n <nowiki>http://dataserver/service.svc/MusicCollection</nowiki>\nor\n <nowiki>http://dataserver/service.svc/MusicCollection[SomeArtist]</nowiki>\nThe former will list all entities in the \'\'Collection\'\' set whereas the latter will list only for the entity which is indexed by \'\'SomeArtist\'\'.\n\nIn addition, the URIs can also specify a traversal of a relationship in the Entity Data Model. For example,\n <nowiki>http://dataserver/service.svc/MusicCollection[SomeSong]/Genre</nowiki>\ntraverses the relationship \'\'Genre\'\' (in SQL parlance, joins with the \'\'Genre\'\' table) and retrieves all instances of \'\'Genre\'\' that are associated with the entity \'\'SomeSong\'\'. Simple predicates can also be specified in the URI, like\n <nowiki>http://dataserver/service.svc/MusicCollection[SomeArtist]/ReleaseDate[Year eq 2006]</nowiki>\nwill fetch the items that are indexed by \'\'SomeArtist\'\' and had their \'\'release\'\' in \'\'2006\'\'. Filtering and partition information can also be encoded in the URL as\n <nowiki>http://dataserver/service.svc/MusicCollection?$orderby=ReleaseDate&$skip=100&$top=50</nowiki>\nIt is important to note that although the presence of skip and top keywords indicate paging support, in Data Services version 1 there is no method of determining the number of records available and thus impossible to determine how many pages there may be.  The [[Open Data Protocol|OData]] 2.0 spec adds support for the \'\'\'$count\'\'\' path segment (to return just a count of entities) and \'\'\'$inlineCount\'\'\' (to retrieve a page worth of entities and a total count without a separate round-trip....).<ref>http://msdn.microsoft.com/en-us/library/ee373845.aspx</ref>\n\n==References==\n{{Reflist}}\n{{Refbegin}}\n* {{cite web | url = http://blogs.msdn.com/pablo/archive/2007/04/30/codename-astoria-data-services-for-the-web.aspx | title = Codename "Astoria": Data Services for the Web | accessdate = 2007-04-30}}\n* [http://astoria.mslivelabs.com/ ADO.NET Data Services Framework (formerly "Project Astoria")]\n{{Refend}}\n\n==External links==\n*[http://msdn.microsoft.com/en-us/library/cc907912.aspx Using Microsoft ADO.NET Data Services]\n*[http://www.asp.net/downloads/3.5-extensions/ ASP.NET 3.5 Extensions Preview]\n*[http://blogs.msdn.com/astoriateam/ ADO.NET Data Services (Project Astoria) Team Blog]\n*[http://entmag.com/news/article.asp?EditorialsID=9105 Access Cloud Data with Astoria: ENT News Online]\n\n{{.NET Framework}}\n\n[[Category:Data management]]\n[[Category:Web services]]\n[[Category:ADO.NET Data Access technologies]]\n[[Category:.NET Framework]]']
['Secure Electronic Delivery', '1282406', "'''Secure Electronic Delivery'''  (SED) is a service created in 2003 and provided by the  [[British Library#Document Supply Service|British Library Document Supply Service]] (BLDSS). Its purpose is to enable faster delivery of digital materials as [[Encryption|encrypted]], copyright-compliant [[Portable Document Format| PDF Document]]s, to a personal e-mail address. These documents are supplied from the British Library via its On Demand service.<ref>{{cite web |url=http://www.bl.uk/sed   |title=Secure Electronic Delivery  |author=  |publisher=[[British Library]] |date=  |accessdate= }}</ref><ref>{{cite web |url=http://www.bl.uk/reshelp/atyourdesk/docsupply/help/receiving/deliveryoptions/electronic/sed/sedhelpsheetfinal.pdf  |title= Secure Electronic Delivery – Technical Helpsheet  |author=  |publisher=[[British Library]] |date=  |accessdate= }}</ref> When the British Library supplies articles electronically, it sends them securely in order to ensure its usage is permitted (research purposes) and copyright law is observed.\n\n==Methods==\nAs the [[publishing | publishing industry]], authors and creators become highly protective of their assets and [[intellectual property]], they impose strict rules on delivery methods to prevent [[copyright infringement]]. Nowadays, [[Digital rights management|DRM]]-enabled secure delivery appears to be the most widely used solution to address issues faced by libraries in supplying ebooks and digital materials to their users.<ref>{{cite news  | title=Secure E-mail Delivery Poised to Take Off  |url=https://books.google.com.mx/books?id=PWHbLjAQ57gC&pg=PA38&lpg=PA38&dq=document+secure+delivery+technology&source=bl&ots=YgfE16c0Yy&sig=qd0R1j9LtZI6_hJi6zqHCGOBzfQ&hl=en&sa=X&redir_esc=y#v=onepage&q=document%20secure%20delivery%20technology&f=false   |date=23 August 1999 |author=Dominique Deckmyn  |newspaper=[[Computerworld]] }}</ref><ref>{{cite web  |title= Practical problems for libraries distributing ebooks & secure electronic delivery |url=http://www.locklizard.com/libraries-secure-electronic-delivery/   |publisher=Locklizard Limited |date=  |accessdate= }}</ref>  SED, one of these solutions, is using [[Adobe LiveCycle]] Digital Rights Management (LCDRM) as an encryption method to deliver documents.<ref>{{cite web |url=http://www.lancaster.ac.uk/library/using-the-library/interlending-and-document-supply/secure-electronic-delivery/ |title=British Library On Demand Electronic Delivery  |author=  |publisher=[[Lancaster University]] Library  |date=  |accessdate= }}</ref>\n\n==Advantages==\nSED offers convenience, quality and speed as documents are delivered upon request at any location and on any device. Requested articles are scanned for high quality reproduction, opened anywhere on any machine, including mobile devices.<ref>{{cite web |url= http://www.brad.ac.uk/library/media/library/interlibraryloans/sed.pdf |title=SED – Secure Electronic Delivery  |author=  |publisher=[[University of Bradford]] |date=  |accessdate= }}</ref>\n\n== Restrictions==\nThe following are restrictions hold in a SED service implementation:\n*  The digital material is accessible only for 14 days via a link sent to a personal message.\n* Due to copyright reasons,<ref>{{cite journal |last=Eiblum |first= Paula   |last2= Ardito |first2= Stephanie    |date= September 1999 |title= Document Delivery & Copyright: Librarians Take the Fifth |url= |journal=Online (magazine) |publisher= |volume=23 |issue=5 |pages=74–77  |doi= |access-date=7 May 2016}}</ref>  the material can be opened only once, saved for 14 days and does not allow a copy-paste action.\n* Upon display, the material must be printed from the same device and reprinted only once.\n* The On Demand encryption technology works best on the default Safari browser although other browsers may accommodate it.\n\n==See also==\n* [[Digital rights management]]\n* [[Digital asset management]]\n\n==References==\n{{Reflist}}\n\n==External links==\n[http://www.bl.uk/sed SED Web page]\n\n{{DEFAULTSORT:Secure Electronic Delivery}}\n[[Category:Information technology management]]\n[[Category:Content management systems]]\n[[Category:Document management systems]]\n[[Category:Data management]]\n[[Category:Secure communication]]"]
['Category:Information governance', '50290347', '{{catmain}}\n\n[[Category:Data management]]\n[[Category:Data security]]\n[[Category:Information technology management]]']
['Machine-Readable Documents', '51558108', '\'\'\'Machine-readable documents\'\'\' are [[document]]s whose content can be readily processed by [[computer]]s.  Such documents are distinguished from [[machine-readable data]] by virtue of having sufficient structure to provide the necessary context to support the business processes for which they are created.  [[Data]] without [[context (language use)]] is meaningless and lacks the four essential [http://www.archives.gov/records-mgmt/policy/managing-web-records.html#1.0 characteristics] of trustworthy [[business record]]s specified in [[ISO 15489 Information and documentation -- Records management]]: \n* Reliability\n* Authenticity\n* Integrity\n* [[Usability]]\nThe vast bulk of information is [[unstructured data]] and, from a business perspective, that means it is "immature", i.e., Level 1 (chaotic) of the [[Capability Maturity Model]].  Such immaturity fosters inefficiency, diminishes quality, and limits effectiveness.  Unstructured information is also ill-suited for [[records management]] functions, provides inadequate [[evidence]] for legal purposes, drives up the cost of [[discovery (law)]] in [[litigation]], and makes access and usage needlessly cumbersome in routine, ongoing [[business process]]es.\n\nThere are at least four aspects to machine-readability:  \n* First, words or phrases should be discretely delineated (tagged) so that computer software and/or hardware logic can be applied to them as individual conceptual elements.  \n* Second, the semantics of each element should be specified so that computers can help human beings achieve a common understanding of their meanings and potential usages.  \n* Third, if the relationships among the individual elements are also specified, computers can automatically apply inferences to them, thereby further relieving human beings of the burden of trying to understand them, particularly for purposes of inquiry, discovery, and analysis. \n* Fourth, if the structures of the documents in which the elements occur are also specified, human understanding is further enhanced and the data becomes more reliable for legal and business-quality purposes.\n\nAs early as 1981, the U.S. [[Government Accountability Office]] (GAO) began reporting on the problem of inadequate record-keeping practices in the U.S. federal government.<ref>{{cite web\n | url=http://www.gao.gov/products/PLRD-81-2\n | title=FEDERAL RECORDS MANAGEMENT: A History of Neglect\n | work=gao.gov\n | date=1981-02-24\n | accessdate=2016-09-08}}\n</ref>  Such deficiencies are not unique to government and advances in information technology mean that most information is now "born digital" and thus potentially far more easily managed by automated means.<ref>{{cite web\n | url=http://www.oclc.org/content/dam/research/activities/hiddencollections/borndigital.pdf\n | title=Defining "Born Digital": An Essay by Ricky Erway, OCLC Research\n | work=oclc.org\n | date=2010-11-30\n | accessdate=2016-09-08}}\n</ref>  However, in testimony to Congress in 2010, GAO highlighted problems with managing electronic records, and as recently as 2015, GAO has continued to report inadequacies in the performance of Executive Branch agencies in meeting records management requirements.<ref>{{cite web\n | url=http://www.gao.gov/new.items/d10838t.pdf\n | title=INFORMATION MANAGEMENT: The Challenges of Managing Electronic Records, Statement of Valerie C. Melvin, Director, Information Management and Human Capital Issues \n | work=gao.gov\n | date=2010-06-17\n | accessdate=2016-09-08}}\n</ref>\n<ref>{{cite web\n | url=http://www.gao.gov/products/GAO-15-339\n | title=INFORMATION MANAGEMENT: Additional Actions Are Needed to Meet Requirements of the Managing Government Records Directive\n | work=gao.gov\n | date=2015-05-14\n | accessdate=2016-09-08}}\n</ref>  Moreover, more than two decades after a major and formerly highly respected auditing firm, [[Arthur Andersen]], met its demise due to a records destruction scandal, record-keeping practices became a central issue in the 2016 Presidential election.\n\nOn January 4, 2011, President Obama signed H.R. 2142, the [[Government Performance and Results Act]] (GPRA) Modernization Act of 2010 (GPRAMA), into law as P.L. 111-352. Section 10 of GPRAMA requires U.S. federal agencies to publish their strategic and performance plans and reports in searchable, machine-readable format.<ref>{{cite web\n | url=http://xml.fido.gov/stratml/references/PL111-532StratML.htm#SEC10\n | title=GPRAMA SEC. 10. FORMAT OF PERFORMANCE PLANS AND REPORTS.\n | work=congress.gov\n | date=2011-01-04\n | accessdate=2016-09-08}}\n</ref>\nAdditionally, in 2013, he issued [[Executive Order]] 13642, Making Open and Machine Readable the New Default for Government Information in general.<ref>{{cite web\n | url=http://xml.fido.gov/stratml/carmel/EOOMRDwStyle.xml\n | title=Executive Order 13642 in open, standard, machine-readable Strategy Markup Language format\n | work=whitehouse.gov\n | date=2013-05-09\n | accessdate=2016-09-08}}\n</ref>\nOn July 28, 2016, the [[Office of Management and Budget]] (OMB) followed up by including in the revised issuance of Circular A-130 direction for agencies to use [http://xml.fido.gov/stratml/carmel/iso/A130wStyle.xml#_3f7d15f0-5799-11e6-8d37-8523b3fa12e0 open, machine-readable formats] and to publish "[http://xml.fido.gov/stratml/carmel/iso/A130wStyle.xml#_3f7d449e-5799-11e6-8d37-8523b3fa12e0 public information online in a manner that promotes analysis and reuse for the widest possible range of purposes]", meaning that the information is both publicly accessible and machine-readable.\n\nIn support of such policy direction, technological advancement is enabling more efficient and effective management and use of machine-readable electronic records.  [[Document-oriented database]]s have been developed for storing, retrieving, and managing document-oriented information, also known as semi-structured data.  Extensible Markup Language ([[XML]]) is a World Wide Web Consortium ([[W3C]]) [[World Wide Web Consortium#W3C recommendation .28REC.29|Recommendation]] setting forth rules for encoding documents in a format that is both [[human-readable]] and machine-readable.  Many [[XML editor]] tools have been developed and most, if not all major information technology applications support XML to greater or lesser degrees.  The fact that XML itself is an open, standard, machine-readable format makes it relatively easy for application developers to do so.\n\nThe W3C\'s accompanying XML Schema ([[XSD]]) Recommendation specifies how to formally describe the elements in an XML document.  With respect to the specification of XML schemas, the [[Organization for the Advancement of Structured Information Standards]] (OASIS) is a leading [[standards-developing organization]]. [[JSON#JSON Schema|JSON Schema]] was proposed by the [[Internet Engineering Task Force]] (IETF) but was allowed to expire in 2013 and thus is less mature and a riskier alternative to XSD, the most recent version of which was approved by the W3C in 2012.\n\nThe W3C\'s Extensible Stylesheet Language ([[XSL]]) family of languages provides for the transformation and rendering of XML documents for human-readable presentation.  Machine-readable documents can be automatically rendered in human-readable format but documents formatted primarily for attractiveness of presentation cannot easily be processed by computers to support [[usability]] by human beings.  \n\nThe [[Portable Document Format]] (PDF) is a file format used to present documents in a manner independent of application software, hardware, and operating systems. Each PDF file encapsulates a complete description of the presentation of the document, including the text, fonts, graphics, and other information needed to display it.  [[PDF/A]] is an ISO-standardized version of the PDF specialized for use in the archiving and long-term preservation of electronic documents.  PDF/A-3 allows embedding of other file formats, including XML, into PDF/A conforming documents, thus potentially providing the best of both human- and machine-readability.  The W3C\'s [[XSL-FO]] (XSL Formatting Objects) markup language is commonly used to generate PDF files\n\n[[Metadata]], data about data, can be used to organize electronic resources, provide digital identification, and support the archiving and preservation of resources.  In well-structured, machine-readable electronic records, the content can be [[Repurposing|repurposed]] as both data and metadata.  In the context of electronic record-keeping systems, the terms "management" and "metadata" are virtually synonymous.  Given proper metadata, records management functions can be automated, thereby reducing the risk of [[spoliation of evidence]] and other fraudulent manipulations of records.  Moreover, such records can be used to automate the process of [[audit]]ing data maintained in [[database]]s, thereby reducing the risk of single points of failure associated with the [[Machiavellianism#In the workplace|Machiavellian]] concept of a [[single source of truth]].\n\n[[Blockchain (database)]] is a new technology for maintaining continuously-growing lists of records secured from tampering and revision.  A key feature is that every node in a decentralized system has a copy of the blockchain so there is no [[single point of failure]] subject to manipulation and [[fraud]].\n\n==See also==\n\n* [[Budapest Declaration on Machine Readable Travel Documents]]\n* [[Comparison of XML editors]]\n* [[Integrity]] and particularly [[Data integrity]]\n* [[Linked data]]\n* [[Machine-readable passport]]\n* [[Open data]]\n* [[Data reliability|Reliability]], particularly [[Reliability (statistics)]], [[Data reliability]], [[Reliability (computer networking)]], and [[Reliability (research methods)]]\n* [[Strategy Markup Language]] (StratML)\n* [[Structured document]]\n* [[Tag (metadata)]]\n* [[Universal Business Language]] (UBL)\n* [[XBRL]] (eXtensible Business Reporting Language)\n\n==References==\n{{reflist}}\n\n==External  links==\n* [http://xml.fido.gov/stratml/carmel/M-13-13wStyle.xml#_78e85ef4-b91c-11e2-bf2b-79d279ad226c OMB M-13-13], Open Data Policy: Managing Information as an Asset, which requires agencies to use open, machine-readable, data format standards\n* [http://ambur.net/CaponeConsultancyMethod.pdf Driving a Stake in the Heart of the Capone Consultancy Method of Records Management: Best Practices for Correcting Non-Records Non-Policy Nonsense], March 9, 2015\n* The U.S. Code, which includes [http://uscode.house.gov/search.xhtml?searchString=machine-readable&pageNumber=1&itemsPerPage=100&sortField=CODE_ORDER&action=search&q=bWFjaGluZS1yZWFkYWJsZQ%3D%3D%7C%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%7C%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%7Cfalse%7C%5B%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%5D%7C%5B%3A%5D 51 references] to the term "machine-readable" as of September 10, 2016\n\n[[Category:Data management]]\n[[Category:Records management]]']
['Object storage', '40572678', '\'\'\'Object storage\'\'\' (also known as \'\'\'object-based storage\'\'\'<ref>{{cite journal|last=Mesnier|first=Mike|author2=Gregory R. Ganger |author3=Erik Riedel |title=Object-Based Storage|journal=IEEE Communications Magazine|date=August 2003|pages=84–90|url=http://www.storagevisions.com/White%20Papers/MesnierIEEE03.pdf|accessdate=27 October 2013|doi=10.1109/mcom.2003.1222722 }}</ref>) is a storage architecture that manages data as objects, as opposed to other storage architectures like [[file systems]] which manage data as a file hierarchy and [[block storage]] which manages data as blocks within sectors and tracks.<ref>{{cite web|last=Porter De Leon|first=Yadin|author2=Tony Piscopo|title=Object Storage versus Block Storage: Understanding the Technology Differences|url=http://www.druva.com/blog/object-storage-versus-block-storage-understanding-technology-differences/|publisher=Druva.com|accessdate=19 January 2015}}</ref> Each object typically includes the data itself, a variable amount of [[metadata]], and a globally unique identifier. Object storage can be implemented at multiple levels, including the device level (object storage device), the system level, and the interface level. In each case, object storage seeks to enable capabilities not addressed by other storage architectures, like interfaces that can be directly programmable by the application, a namespace that can span multiple instances of physical hardware, and data management functions like data replication and data distribution at object-level granularity.\n\nObject storage systems allow relatively inexpensive, scalable and self-healing retention of massive amounts of [[unstructured data]]. Object storage is used for diverse purposes such as storing photos on [[Facebook]], songs on [[Spotify]], or files in online collaboration services, such as [[Dropbox (service)|Dropbox]].<ref>{{cite web|authors=Chandrasekaran, Arun, Dayley, Alan|title=Critical Capabilities for Object Storage|publisher=Gartner Research|date=11 February 2014|url=http://www.gartner.com/technology/reprints.do?id=1-1R78PJ9&ct=140226&st=sb}}</ref>\n\n==History==\n\n===Origins===\nIn 1995, new research by Garth Gibson, \'\'et al.\'\' on [[Network Attached Secure Disks]] first promoted the concept of splitting less common operations, like namespace manipulations, from common operations, like reads and writes, to optimize the performance and scale of both.<ref name="NASD">{{cite web|title=File Server Scaling with Network-Attached Secure Disks|url=http://www.pdl.cmu.edu/ftp/NASD/Sigmetrics97.pdf|publisher=Proceedings of the ACM International Conference on Measurement and Modeling of Computer Systems (Sigmetrics ‘97)|accessdate=27 October 2013|author=Garth A. Gibson |author2=Nagle D. |author3=Amiri K. |author4=Chan F. |author5=Feinberg E. |author6=Gobioff H. |author7=Lee C. |author8=Ozceri B. |author9=Riedel E. |author10=Rochberg D. |author11=Zelenka J.}}</ref>  In the same year, 1995, a Belgium company - FilePool - was established to build the basis for archiving functions by using those and own concepts. Object storage was proposed  at [[Carnegie Mellon University|Carnegie Mellon University\'s]] Parallel Data Lab as a research project in 1996 .<ref>{{cite web|last1=Factor|first1=Michael|last2=Meth|first2=K.|last3=Naor|first3=D.|last4=Rodeh|first4=O.|last5=Satran |first5=J.|title=Object Storage: The Future Building Block for Storage Systems|url=http://webhdd.ru/library/files/PositionOSD.pdf|publisher=IBM Haifa Research Labs|accessdate=26 September 2013}}</ref>   Another key concept was abstracting the writes and reads of data to more flexible data containers (objects). Fine grained access control through object storage architecture<ref>{{cite web|title=Security for Network Attached Storage Devices (CMU-CS-97-185)|url=http://repository.cmu.edu/cgi/viewcontent.cgi?article=1147&context=pdl|publisher=Parallel Data Laboratory|accessdate=7 November 2013|author=Gobioff, Howard|author2=Gibson, Garth A. |author3= Tygar, Doug |date=1 October 1997}}</ref>  was further described by one of the NASD team, Howard Gobioff, who later was one of the inventors of the [[Google File System]].<ref>{{cite web|title=The Google File System|url=http://research.google.com/archive/gfs-sosp2003.pdf|publisher=Google|accessdate=7 November 2013|author=Sanjay Ghemawat |author2=Howard Gobioff |author3=Shun-Tak Leung|date=October 2003}}</ref>  Other related work includes the [[Coda (file system)|Coda]] filesystem project at [[Carnegie Mellon]], which started in 1987, and spawned the [[Lustre (file system)|Lustre file system]].<ref name="Lustre">{{cite web|last=Braam|first=Peter|title=Lustre: The intergalactic ﬁle system|url=http://ols.fedoraproject.org/OLS/Reprints-2002/braam-reprint.pdf|accessdate=17 September 2013}}</ref> There is also the OceanStore project at UC Berkeley,<ref>{{cite web|title=OceanStore|url=http://oceanstore.cs.berkeley.edu/|accessdate=18 September 2013}}</ref> which started in 1999.<ref>{{cite journal|last1=Kubiatowicz|first1=John|last2=Bindel|first2=D.|last3=Chen|first3=Y.|last4=Czerwinski|first4=S.|last5=Eaton|first5=P.|last6=Geels|first6=D.|last7=Gummadi|first7=R.|last8=Rhea|first8=S.|last9=Weatherspoon|first9=H.|last10=Weimer |first10=W.|last11=Wells|first11=C.|last12=Zhao|first12=B.|title=OceanStore: An Architecture for Global-Scale Persistent Storage|journal=Proceedings of the Ninth international Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2000)|date=November 2000|url=http://oceanstore.cs.berkeley.edu/publications/papers/pdf/asplos00.pdf|accessdate=18 September 2013}}</ref>\n\nOne of the earliest and best-known object storage products, EMC\'s Centera, debuted in 2002.<ref>{{cite news|title=EMC Unveils Low-Cost Data-Storage Product|url=http://articles.latimes.com/2002/apr/30/business/fi-techbriefs30.3|accessdate=17 September 2013|newspaper=LA Times|date=April 30, 2002}}</ref> [[Content-addressable storage|Centera\'s technology]]  has been developed at Filepool and the company had been acquired  by EMC² in 2002.\n\n===Development===\nOverall industry investment in object storage technology has been sustained for over a decade. From 1999 to 2013, there has been at least $300 million of venture financing related to object storage, including vendors like Amplidata, Bycast, Cleversafe, Cloudian, Nirvanix, and Scality.<ref>{{cite web|last=Leung|first=Leo|title=After 10 years, object storage investment continues and begins to bear significant fruit|url=http://blog.oxygencloud.com/2013/09/16/after-10-years-object-storage-investment-continues-and-begins-to-bear-significant-fruit/|accessdate=17 September 2013|date=16 September 2013}}</ref> This doesn\'t include millions of dollars of private engineering from systems vendors like DataDirect Networks (WOS), [http://www.emc.com/en-us/storage/ecs/index.htm#collapse=&tab14=0 Dell EMC Elastic Cloud Storage], Centera, [[Atmos]], HDS (HCP), HP ([[HP OpenStack]]), IBM, NetApp (StorageGRID), Redhat GlusterFS and [http://www.keepertech.com Keeper Technology] ([http://www.keepertech.com/products/keepersafe/ keeperSAFE]), cloud services vendors like Amazon ([[AWS S3]]), Microsoft ([[Microsoft Azure]]) and Google ([[Google Cloud Storage]]), or the many man years of open source development at [[Lustre (file system)|Lustre]], OpenStack ([[OpenStack#Object Storage .28Swift.29|Swift]]), MogileFS, [[Ceph (file system)|Ceph]], [[Skylable SX (object storage)|Skylable SX]] and OpenIO.<ref name="Mellor">{{cite web|last=Mellor|first=Chris (Dec. 2, 2015)|title=Openio\'s objective is opening up object storage space|url=http://www.theregister.co.uk/2015/12/02/openio_object_storage_upstart/}}</ref><ref name="Nicolas">{{cite web|last=Nicolas|first=Philippe (Oct. 2, 2015)|title=OpenIO, ready to take off|url=http://filestorage.blogspot.fr/2015/10/openio-ready-to-take-off.html/}}</ref><ref name="Raffo">{{cite web|last=Raffo|first=Dave (May 20, 2016)|title=OpenIO joins object storage cloud scrum|url=http://searchcloudstorage.techtarget.com/news/450296765/OpenIO-joins-object-storage-cloud-scrum/}}</ref><ref name="Maleval">{{cite web|last=Maleval|first=Jean-Jacques (Apr. 25, 2016)|title=Start-Up Profile: OpenIO|url=http://www.storagenewsletter.com/rubriques/start-ups/start-up-profile-openio/}}</ref>\n\nA great article written by Philippe Nicolas illustrating products\' timeline was published in July 2016 on The Register with all players, pioneers, mergers and acquisitions and of course genesis with CAS included.<ref>{{cite web|last=Nicolas|first=Philippe (July 15, 2016)|title=The History Boys: Object storage ... from the beginning|url=http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/}}</ref>\n\n==Architecture==\n[[File:High level object storage architecture.png|thumb]]\n\n===Abstraction of storage===\nOne of the design principles of object storage is to abstract some of the lower layers of storage away from the administrators and applications. Thus, data is exposed and managed as objects instead of files or blocks. Objects contain additional descriptive properties which can be used for better indexing or management. Administrators do not have to perform lower level storage functions like constructing and managing [[Logical unit number|logical volumes]] to utilize disk capacity or setting [[RAID]] levels to deal with disk failure.\n\nObject storage also allows the addressing and identification of individual objects by more than just file name and file path. Object storage adds a unique identifier within a bucket, or across the entire system, to support much larger namespaces and eliminate name collisions.\n\n=== Inclusion of rich custom metadata within the object ===\nObject storage explicitly separates file metadata from data to support additional capabilities:\nAs opposed to fixed metadata in file systems (filename, creation date, type, etc.), object storage provides for full function, custom, object-level metadata in order to:\n* Capture application-specific or user-specific information for better indexing purposes\n* Support data management policies (e.g. a policy to drive object movement from one storage tier to another)\n* Centralize management of storage across many individual nodes and clusters\n* Optimize metadata storage (e.g. encapsulated, database or key value storage) and caching/indexing (when authoritative metadata is encapsulated with the metadata inside the object) independently from the data storage (e.g. unstructured binary storage)\n\nAdditionally, in some object-based file system implementations:\n* The file system clients only contact metadata servers once when the file is opened and then get content directly via object storage servers (vs. block-based file systems which would require constant metadata access)\n* Data objects can be configured on a per-file basis to allow adaptive stripe width, even across multiple object storage servers, supporting optimizations in bandwidth and I/O\n\n\'\'\'Object-based storage devices\'\'\' (\'\'\'OSD\'\'\') as well as some software implementations (e.g., Caringo Swarm) manage metadata and data at the storage device level:\n* Instead of providing a block-oriented interface that reads and writes fixed sized blocks of data, data is organized into flexible-sized data containers, called objects\n* Each object has both data (an uninterpreted sequence of bytes) and metadata (an extensible set of attributes describing the object); physically encapsulating both together benefits recoverability.\n* The command interface includes commands to create and delete objects, write bytes and read bytes to and from individual objects, and to set and get attributes on objects\n* Security mechanisms provide per-object and per-command access control\n\n===Programmatic data management===\nObject storage provides programmatic interfaces to allow applications to manipulate data. At the base level, this includes [[CRUD]] functions for basic read, write and delete operations. Some object storage implementations go further, supporting additional functionality like object versioning, object replication, and movement of objects between different tiers and types of storage. Most API implementations are [[Representational state transfer|ReST]]-based, allowing the use of many standard [[HTTP]] calls.\n\n==Implementation==\n\n===Object-based storage devices===\nObject storage at the protocol and device layer was proposed 20 years ago and approved for the [[SCSI]] command set nearly 10 years ago as "Object-based Storage Device Commands" (OSD),<ref>{{cite web|last=Riedel|first=Erik|title=Object Storage and Applications|url=https://www.usenix.org/legacy/event/lsf07/tech/riedel.pdf|accessdate=3 November 2013|author2=Sami Iren |date=February 2007}}</ref> but has not been productized until the development of the Seagate Kinetic Open Storage platform.<ref>{{cite web|title=The Seagate Kinetic Open Storage Vision|url=http://www.seagate.com/tech-insights/kinetic-vision-how-seagate-new-developer-tools-meets-the-needs-of-cloud-storage-platforms-master-ti/|publisher=Seagate|accessdate=3 November 2013}}</ref><ref>{{cite news|last=Gallagher|first=Sean|title=Seagate introduces a new drive interface: Ethernet|url=http://arstechnica.com/information-technology/2013/10/seagate-introduces-a-new-drive-interface-ethernet/|accessdate=3 November 2013|newspaper=Arstechnica.com|date=27 October 2013}}</ref>  The [[SCSI]] command set for Object Storage Devices was developed by a working group of the [[Storage Networking Industry Association]] (SNIA) for the T10 committee of the [[International Committee for Information Technology Standards]] (INCITS).<ref>{{cite web|last=Corbet|first=Jonathan|title=Linux and object storage devices|url=https://lwn.net/Articles/305740/|accessdate=8 November 2013|newspaper=LWN.net|date=4 November 2008}}</ref>  T10 is responsible for all SCSI standards.\n\n===Object-based file systems===\nSome distributed file systems use an object-based architecture, where file metadata is stored in metadata servers and file data is stored in object storage servers. File system client software interacts with the distinct servers, and abstracts them to present a full file system to users and applications. [[IBM General Parallel File System|IBM Spectrum Scale (also known as GPFS)]], [http://www.emc.com/en-us/storage/ecs/index.htm#collapse=&tab14=0 Dell EMC Elastic Cloud Storage], [[Ceph (software)|Ceph]], [[XtreemFS]], and [[Lustre (file system)|Lustre]] are examples of this type of object storage.\n\n===Archive storage===\nSome early incarnations of object storage were used for archiving, as implementations were optimized for data services like immutability, not performance. [[Content-addressable storage|EMC Centera]] and Hitachi HCP (formerly known as HCAP) are two commonly cited object storage products for archiving. Another example is Quantum Lattus Object Storage Platform.\n\n===Cloud storage===\nThe vast majority of cloud storage available in the market leverages an object storage architecture. Two notable examples are [[AWS S3|Amazon Web Services S3]], which debuted in 2005, and [[Rackspace]] Files (whose code was released as [[OpenStack#Swift|OpenStack Swift]]). Other major cloud storage services include Microsoft Azure, Google Cloud Storage, Alibaba Cloud OSS, Oracle Elastic Storage Service and DreamHost based on Ceph.\n\n==="Captive" object storage===\nSome large internet companies developed their own software when object storage products were not commercially available or use cases were very specific. Facebook famously invented their own object storage software, code-named Haystack, to address their particular massive scale photo management needs efficiently.<ref name="haystack">{{cite web|last=Vajgel|first=Peter|title=Needle in a haystack: efficient storage of billions of photos|url=https://www.facebook.com/note.php?note_id=76191543919|accessdate=17 September 2013}}</ref>\n\n===Hybrid storage===\nA few object storage systems, such as [[Ceph (software)|Ceph]], [[GlusterFS]], [[Cloudian]],<ref name="Primesberger">{{cite web|last=Primesberger|first=Chris (27 October 2016)|title=Cloudian Raises $41 Million VC for Hybrid Cloud Object Storage|url=http://www.eweek.com/storage/cloudian-raises-41-million-vc-for-hybrid-cloud-object-storage.html}}</ref> and [[Scality]] support Unified File and Object (UFO) storage, allowing some clients to store objects on a storage system while simultaneously other clients store files on the same storage system. While "hybrid storage" is not a widely accepted term for this concept, interoperable interfaces to the same set of data is becoming available in some object storage products.\n\n===Virtual object storage===\nIn addition to object storage systems that own the managed files, some systems provide an object abstraction on top of one or more traditional filesystem based solutions. These solutions do not own the underlaying raw storage, but instead actively mirror the filesystem changes and replicate them in their own object catalog, alongside any metadata that can be automatically extracted from the files. Users can then contribute additional metadata through the virtual object storage APIs. A global namespace and replication capabilities both inside and across filesystems are typically supported.\n\nNotable examples in this category are [[Nirvana (software)|Nirvana]], and its open-source cousin iRODS.\n\nMost products in this category have recently extended their capabilities to support other Object Store solutions as well.\n\n===Object storage systems===\nMore general purpose object storage systems came to market around 2008. Lured by the incredible growth of "captive" storage systems within web applications like Yahoo Mail and the early success of cloud storage, object storage systems promised the scale and capabilities of cloud storage, with the ability to deploy the system within an enterprise, or at an aspiring cloud storage service provider. Notable examples of object storage systems include [[EMC Atmos]], [[OpenStack#Object Storage (Swift)|OpenStack Swift]], [[Scality|Scality RING]], Caringo Swarm<ref>{{cite web|last=Nicolas|first=Philippe (Sept. 21, 2009)|title=Caringo FileFly, back to the future|url=http://continuousdataprotection.blogspot.fr/2015/09/caringo-filefly-back-to-future.html}}</ref> (formerly CAStor), [[Cloudian]],<ref name="Primesberger"/> and OpenIO.<ref name="Mellor"/>\n\n==Market adoption==\n[[File:Titan supercomputer at the Oak Ridge National Laboratory.jpg|thumb|The Titan supercomputer at Oak Ridge National Laboratory]]\nOne of the first object storage products, Lustre, is used in 70% of the Top 100 supercomputers and ~50% of the [[Top 500]].<ref>{{cite web|last=Dilger|first=Andreas|title=Lustre Future Development|url=http://storageconference.org/2012/Presentations/M04.Dilger.pdf|publisher=IEEE MSST|accessdate=27 October 2013}}</ref> As of June 16, 2013, this includes 7 of the top 10, including the current fastest system on the list - China\'s Tianhe-2 and the second fastest, the [[Titan (supercomputer)|Titan supercomputer]] at [[Oak Ridge National Laboratory]] (pictured on the right).<ref>{{cite web|title=Datadirect Networks to build world\'s fastest storage system for Titan, the world\'s most powerful supercomputer|url=http://www.multivu.com/mnr/60497-datadirect-networks-titan-supercomputer-storage-system-ornl|accessdate=27 October 2013}}</ref>\n\nObject storage systems had good adoption in the early 2000s as an archive platform, particularly in the wake of compliance laws like [[Sarbanes-Oxley]]. After five years in the market, EMC\'s Centera product claimed over 3,500 customers and 150 [[petabytes]] shipped by 2007.<ref>{{cite web|title=EMC Marks Five Years of EMC Centera Innovation and Market Leadership|url=http://www.emc.com/about/news/press/us/2007/04182007-5028.htm|publisher=EMC|accessdate=3 November 2013|date=18 April 2007}}</ref> Hitachi\'s HCP product also claims many [[petabyte]]-scale customers.<ref>{{cite web|title=Hitachi Content Platform Supports Multiple Petabytes, Billions of Objects|url=http://www.techvalidate.com/portals/hitachi-content-platform-customers-with-more-than-1pb-of-data-stored|publisher=Techvalidate.com|accessdate=19 September 2013}}</ref> Newer object storage systems have also gotten some traction, particularly around very large custom applications like eBay\'s auction site, where EMC Atmos is used to manage over 500 million objects a day.<ref>{{cite news|last=Robb|first=Drew|title=EMC World Continues Focus on Big Data, Cloud and Flash|url=http://www.infostor.com/backup-and_recovery/cloud-storage/emc-world-continues-focus-on-big-data-cloud-and-flash-.html|accessdate=19 September 2013|newspaper=Infostor|date=11 May 2011}}</ref> As of March 3, 2014, EMC claims to have sold over 1.5 exabytes of Atmos storage.<ref>{{cite web|last=Hamilton|first=George|title=In it for the Long Run: EMC\'s Object Storage Leadership|url=http://www.rethinkstorage.com/in-it-for-the-long-run-emcs-object-storage-leadership#.UyEzj9yllFI|accessdate=15 March 2014}}</ref> On July 1, 2014, [[Los Alamos National Lab]] chose the [[Scality|Scality RING]] as the basis for a 500 petabyte storage environment, which would be among the largest ever.<ref>{{cite news|last1=Mellor|first1=Chris|title=Los Alamos National Laboratory likes it, puts Scality\'s RING on it|url=http://www.theregister.co.uk/2014/07/01/scalitys_ring_goes_faster/|accessdate=26 January 2015|publisher=The Register|date=1 July 2014}}</ref>\n\n"Captive" object storage systems like Facebook\'s Haystack have scaled impressively. In April 2009, Haystack was managing 60 billion photos and 1.5 petabytes of storage, adding 220 million photos and 25 terabytes a week.<ref name="haystack" /><ref>{{cite web|last=Nicolas|first=Philippe (Sept. 13, 2009)|title=Haystack chez Facebook|url=http://filestorage.blogspot.com/2009/09/haystack-chez-facebook.html}}</ref> Facebook more recently stated that they were adding 350 million photos a day and were storing 240 billion photos.<ref>{{cite news|last=Miller|first=Rich|title=Facebook Builds Exabyte Data Centers for Cold Storage|url=http://www.datacenterknowledge.com/archives/2013/01/18/facebook-builds-new-data-centers-for-cold-storage/|accessdate=6 November 2013|newspaper=Datacenterknowledge.com|date=13 January 2013}}</ref> This could equal as much as 357 petabytes.<ref>{{cite web|last=Leung|first=Leo|title=How much data does x store?|url=http://techexpectations.org/2014/05/17/how-much-data-does-x-store/|publisher=Techexpectations.org|accessdate=23 May 2014|date=17 May 2014}}</ref>\n\nCloud storage has become pervasive as many new web and mobile applications choose it as a common way to store [[binary data]].<ref>{{cite web|last=Leung|first=Leo|title=Object storage already dominates our days (we just didn’t notice)|url=http://blog.oxygencloud.com/2012/01/11/object-storage-already-dominates/|accessdate=27 October 2013|date=January 11, 2012}}</ref>  As the storage backend to many popular applications like [[Smugmug]] and [[Dropbox (service)|Dropbox]], AWS S3 has grown to massive scale, citing over 2 trillion objects stored in April 2013.<ref>{{cite news|last=Harris|first=Derrick|title=Amazon S3 goes exponential, now stores 2 trillion objects|url=http://gigaom.com/2013/04/18/amazon-s3-goes-exponential-now-stores-2-trillion-objects/|accessdate=17 September 2013|newspaper=Gigaom|date=18 April 2013}}</ref> Two months later, Microsoft claimed that they stored even more objects in Azure at 8.5 trillion.<ref>{{cite news|last=Wilhelm|first=Alex|title=Microsoft: Azure powers 299M Skype users, 50M Office Web Apps users, stores 8.5T objects|url=http://thenextweb.com/microsoft/2013/06/27/microsoft-our-cloud-powers-hundreds-of-millions/|accessdate=18 September 2013|newspaper=thenextweb.com|date=27 June 2013}}</ref> By April 2014, Azure claimed over 20 trillion objects stored.<ref>{{cite news|last1=Nelson|first1=Fritz|title=Microsoft Azure\'s 44 New Enhancements, 20 Trillion Objects|url=http://www.tomsitpro.com/articles/microsoft-azure-paas-iaas-cloud-computing,1-1841.html|accessdate=3 September 2014|publisher=Tom\'s IT Pro|date=4 April 2014}}</ref> Windows Azure Storage manages Blobs (user files), Tables (structured storage), and Queues (message delivery) and counts them all as objects.<ref>{{cite web|last=Calder|first=Brad|title=Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency|url=http://sigops.org/sosp/sosp11/current/2011-Cascais/printable/11-calder.pdf|publisher=Microsoft|accessdate=6 November 2013|location=23rd ACM Symposium on Operating Systems Principles (SOSP)}}</ref>\n\n==Market analysis==\n[[International Data Corporation|IDC]] has begun to assess the object-based storage market annually using its MarketScape methodology. IDC describes the MarketScape as: "...a quantitative and qualitative assessment of the characteristics that assess a vendor\'s current and future success in the said market or market segment and provide a measure of their ascendancy to become a Leader or maintain a leadership. IDC MarketScape assessments are particularly helpful in emerging markets that are often fragmented, have several players, and lack clear leaders."<ref>{{cite web|last1=Nadkarni|first1=Ashish|title=IDC MarketScape: Worldwide Object-Based Storage 2013 Vendor Assessment|url=http://www.idc.com/getdoc.jsp?containerId=244081|website=http://www.idc.com|publisher=IDC|accessdate=26 January 2015}}</ref>\n\nIn 2013, IDC rated [[Cleversafe]], [[Scality]], [[DataDirect Networks]], [[Amplidata]], and [[EMC Corporation|EMC]] as leaders.<ref>{{cite news|last1=Mellor|first1=Chris|title=IDC\'s explicit snapshot: Everyone who\'s anyone in object storage: In 3D|url=http://www.theregister.co.uk/2013/11/27/idcs_objectscape_pretty_as_a_picture/|accessdate=26 January 2015|publisher=The Register|date=27 November 2013}}</ref> In 2014, it rated [[Scality]], [[Cleversafe]], [[DataDirect Networks]], [[Hitachi Data Systems]], [[Amplidata]], [[EMC Corporation|EMC]], and [[Cloudian]]<ref>{{cite web|last=Nicolas|first=Philippe (Sept. 14, 2015)|title=Cloudian shakes the object storage market|url=http://filestorage.blogspot.fr/2015/09/cloudian-shakes-object-storage-market.html}}</ref><ref>{{cite web|last=Mellor|first=Chris (June. 21, 2016)|title=Cloudian clobbers car drivers with targeted ads|url=http://www.theregister.co.uk/2016/06/21/cloudian_could_clobber_car_drives_with_targeted_ads/}}</ref><ref>{{cite web|last=Nicolas|first=Philippe (June. 22, 2016)|title=Cloudian is the real S3 leader|url=http://filestorage.blogspot.fr/2016/06/cloudian-is-real-s3-leader.html}}</ref> as leaders.<ref>{{cite news|last1=Mellor|first1=Chris|title=IDC: Who\'s HOT and who\'s NOT (in object storage) in 2014|url=http://www.theregister.co.uk/2015/01/06/idc_shows_emcs_object_presence_shrinking/|accessdate=26 January 2015|publisher=The Register|date=6 January 2015}}</ref><ref>{{cite web|last=Mellor|first=Chris (Nov. 24, 2015)|title=We pick storage brains: Has object storage endgame started?|url=http://www.channelregister.co.uk/2015/11/24/object_storage_endgame/}}</ref><ref>{{cite web|last=Nicolas|first=Philippe (Oct. 19, 2015)|title=Red alert for Object Storage vendors|url=http://filestorage.blogspot.com/2015/10/red-alert-for-object-storage-vendors.html}}</ref>\n\n==Standards==\n\n===Object-based storage device standards===\n\n====OSD version 1====\nIn the first version of the OSD standard,<ref>{{cite web|title=INCITS 400-2004|url=http://www.techstreet.com/cgi-bin/detail?product_id=1204555|publisher=InterNational Committee for Information Technology Standards|accessdate=8 November 2013}}</ref> objects are specified with a 64-bit partition ID and a 64-bit object ID. Partitions are created and deleted within an OSD, and objects are created and deleted within partitions. There are no fixed sizes associated with partitions or objects; they are allowed to grow subject to physical size limitations of the device or logical quota constraints on a partition.\n\nAn extensible set of attributes describe objects. Some attributes are implemented directly by the OSD, such as the number of bytes in an object and the modify time of an object. There is a special policy tag attribute that is part of the security mechanism. Other attributes are uninterpreted by the OSD. These are set on objects by the higher-level storage systems that use the OSD for persistent storage. For example, attributes might be used to classify objects, or to capture relationships among different objects stored on different OSDs.\n\nA list command returns a list of identifiers for objects within a partition, optionally filtered by matches against their attribute values. A list command can also return selected attributes of the listed objects.\n\nRead and write commands can be combined, or piggy-backed, with commands to get and set attributes. This ability reduces the number of times a high-level storage system has to cross the interface to the OSD, which can improve overall efficiency.\n\n====OSD version 2====\nA second generation of the SCSI command set, "Object-Based Storage Devices - 2" (OSD-2) added support for snapshots, collections of objects, and improved error handling.<ref>{{cite web|title=INCITS 458-2011|url=http://www.techstreet.com/products/1801667|publisher=InterNational Committee for Information Technology Standards|accessdate=8 November 2013|date=15 March 2011}}</ref>\n\nA [[snapshot (computer storage)|snapshot]] is a point in time copy of all the objects in a partition into a new partition. The OSD can implement a space-efficient copy using [[copy-on-write]] techniques so that the two partitions share objects that are unchanged between the snapshots, or the OSD might physically copy the data to the new partition. The standard defines clones, which are writeable, and snapshots, which are read-only.\n\nA collection is a special kind of object that contains the identifiers of other objects. There are operations to add and delete from collections, and there are operations to get or set attributes for all the objects in a collection. Collections are also used for error reporting.  If an object becomes damaged by the occurrence of a media defect (i.e., a bad spot on the disk) or by a software error within the OSD implementation, its identifier is put into a special error collection. The higher-level storage system that uses the OSD can query this collection and take corrective action as necessary.\n\n==Differences between Key-Value and Object Stores==\n{{Disputed|date=December 2015}}\nLet’s first clarify what a key/value store and an object store are. Using the traditional block storage interface, one has a series of fixed size blocks which are numbered starting at 0. Data must be that exact fixed size and can be stored in a particular block which is identified by its logical block number (LBN). Later, one can retrieve that block of data by specifying its unique LBN.\n\nWith a key/value store, data is identified by a key rather than a LBN. A key might be "cat" or "olive" or "42". It can be an arbitrary sequence of bytes of arbitrary length. Data (called a value in this parlance) does not need to be a fixed size and also can be an arbitrary sequence of bytes of arbitrary length. One stores data by presenting the key and data (value) to the data store and can later retrieve the data by presenting the key. You’ve seen this concept before in programming languages. Python calls them dictionaries, Perl calls them hashes, Java and C++ call them maps, etc. Several data stores also implement key/value stores such as Memcached, Redis and CouchDB.\n\nObject stores are similar to key/value stores except that the key must be a positive integer like a LBN. However, unlike a LBN, the key can be any positive integer; it does not have to map to an existing logical block number. In practice, it is usually limited to 64 bits. More like a key/value store than the traditional block storage interface, data is not limited to a fixed size block but may be an arbitrary size. Object stores also allow one to associate a limited set of attributes with each piece of data. The key, value and set of attributes is referred to as an object. To add more confusion, sometimes key/value stores are loosely referred to as object stores but technically there is a difference.<ref>http://blog.gigaspaces.com/were-flash-keyvalue-and-object-stores-made-for-each-other-guest-post-by-johann-george-sandisk/</ref>\n\n==See also==\n*[[Cloud storage]]\n*[[Clustered file system]]\n*[[Object access method]]\n\n==References==\n{{Reflist|2}}\n<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\n*\n*\n*\n*\n\n==External links==\n*[http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html AWS S3 API Documentation]\n*[https://developers.google.com/storage/ Google Cloud Storage API Documentation]\n*[http://docs.openstack.org/developer/swift/ Openstack Swift API Documentation]\n*[https://developers.seagate.com/display/KV/Kinetic+Open+Storage+Documentation+Wiki Seagate Kinetic Open Storage Documentation]\n*[http://msdn.microsoft.com/en-us/library/windowsazure/dd179355.aspx Windows Azure Storage API Documentation]\n*[https://nkolayofis.com a Saas solution in Turkey]\n*[https://quictransfer.com a Cloud storage]\n\n\n[[Category:Data management]]\n[[Category:Data management software]]\n[[Category:Computer file systems]]\n[[Category:Computer data storage]]\n[[Category:Network file systems]]\n[[Category:Cloud storage]]']
['Lambda architecture', '43539426', '[[File:Diagram of Lambda Architecture (generic).png|thumb|Flow of data through the processing and serving layers of a generic lambda architecture]]\n\'\'\'Lambda architecture\'\'\' is a [[data processing|data-processing]] architecture designed to handle massive quantities of data by taking advantage of both [[batch processing|batch]]- and [[stream processing|stream-processing]] methods. This approach to architecture attempts to balance [[latency (engineering)|latency]], [[throughput]], and [[fault-tolerance]] by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation. The rise of lambda architecture is correlated with the growth of [[big data]], real-time analytics, and the drive to mitigate the latencies of [[map-reduce]].<ref>{{cite web|last1=Schuster|first1=Werner|title=Nathan Marz on Storm, Immutability in the Lambda Architecture, Clojure|url=http://www.infoq.com/interviews/marz-lambda-architecture|website=www.infoq.com}} Interview with Nathan Marz, 6 April 2014</ref>\n\nLambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record.<ref name=bijnens-slide>Bijnens, Nathan. [http://lambda-architecture.net/architecture/2013-12-11-a-real-time-architecture-using-hadoop-and-storm-devoxx/ "A real-time architecture using Hadoop and Storm"]. 11 December 2013.</ref>{{rp|32}} It is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them. State is determined from the natural time-based ordering of the data.\n\n==Overview==\nLambda architecture describes a system consisting of three layers: batch processing, speed (or real-time) processing, and a serving layer for responding to queries.<ref name=big-data>Marz, Nathan; Warren, James. \'\'Big Data: Principles and best practices of scalable realtime data systems\'\'. Manning Publications, 2013.</ref>{{rp|13}} The processing layers ingest from an immutable master copy of the entire data set.\n\n===Batch layer===\nThe batch layer precomputes results using a distributed processing system that can handle very large quantities of data. The batch layer aims at perfect accuracy by being able to process \'\'all\'\' available data when generating views. This means it can fix any errors by recomputing based on the complete data set, then updating existing views. Output is typically stored in a read-only database, with updates completely replacing existing precomputed views.<ref name=big-data />{{rp|18}}\n\n[[Hadoop|Apache Hadoop]] is the de facto standard batch-processing system used in most high-throughput architectures.<ref>Kar, Saroj. [http://cloudtimes.org/2014/05/28/hadoop-sector-will-have-annual-growth-of-58-for-2013-2020/ "Hadoop Sector will Have Annual Growth of 58% for 2013-2020"], 28 May 2014. \'\'Cloud Times\'\'.</ref>\n\n===Speed layer===\n[[File:Diagram of Lambda Architecture (named components).png|thumb|Diagram showing the flow of data through the processing and serving layers of lambda architecture. Example named components are shown.]]\nThe speed layer processes data streams in real time and without the requirements of fix-ups or completeness. This layer sacrifices throughput as it aims to minimize latency by providing real-time views into the most recent data. Essentially, the speed layer is responsible for filling the "gap" caused by the batch layer\'s lag in providing views based on the most recent data. This layer\'s views may not be as accurate or complete as the ones eventually produced by the batch layer, but they are available almost immediately after data is received, and can be replaced when the batch layer\'s views for the same data become available.<ref name=big-data />{{rp|203}}\n\nStream-processing technologies typically used in this layer include [[Storm (event processor)|Apache Storm]], [[Sqlstream|SQLstream]] and [[Apache Spark]]. Output is typically stored on fast NoSQL databases.<ref name=kinley>Kinley, James. [http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for-architecting "The Lambda architecture: principles for architecting realtime Big Data systems"], retrieved 26 August 2014.</ref><ref>Ferrera Bertran, Pere. [http://www.datasalt.com/2014/01/lambda-architecture-a-state-of-the-art/ "Lambda Architecture: A state-of-the-art"]. 17 January 2014, Datasalt.</ref>\n\n===Serving layer===\n[[File:Diagram of Lambda Architecture (Druid data store).png|thumb|Diagram showing a lambda architecture with a Druid data store.]]\nOutput from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data.\n\nExamples of technologies used in the serving layer include [[Druid (open-source data store)|Druid]], which provides a single cluster to handle output from both layers.<ref name=metamarkets-lambda>Yang, Fangjin, and Merlino, Gian. [https://speakerdeck.com/druidio/real-time-analytics-with-open-source-technologies-1 "Real-time Analytics with Open Source Technologies"]. 30 July 2014.</ref> Dedicated stores used in the serving layer include [[Apache Cassandra]] or [[Apache HBase]] for speed-layer output, and [https://github.com/nathanmarz/elephantdb Elephant DB] or [[Cloudera Impala]] for batch-layer output.<ref name=bijnens-slide />{{rp|45}}<ref name=kinley />\n\n==Optimizations==\nTo optimize the data set and improve query efficiency, various rollup and aggregation techniques are executed on raw data,<ref name=metamarkets-lambda />{{rp|23}} while estimation techniques are employed to further reduce computation costs.<ref>Ray, Nelson. [https://metamarkets.com/2013/histograms/ "The Art of Approximating Distributions: Histograms and Quantiles at Scale"]. 12 September 2013. Metamarkets.</ref> And while expensive full recomputation is required for fault tolerance, incremental computation algorithms may be selectively added to increase efficiency, and techniques such as \'\'partial computation\'\' and resource-usage optimizations can effectively help lower latency.<ref name=big-data />{{rp|93,287,293}}\n\n==Lambda architecture in use==\nMetamarkets, which provides analytics for companies in the programmatic advertising space, employs a version of the lambda architecture that uses [[Druid (open-source data store)|Druid]] for storing and serving both the streamed and batch-processed data.<ref name=metamarkets-lambda />{{rp|42}}\n\nFor running analytics on its advertising data warehouse, [[Yahoo]] has taken a similar approach, also using [[Storm (event processor)|Apache Storm]], [[Hadoop|Apache Hadoop]], and [[Druid (open-source data store)|Druid]].<ref name=yahoo-lambda>Rao, Supreeth; Gupta, Sunil. [http://www.slideshare.net/Hadoop_Summit/interactive-analytics-in-human-time?next_slideshow=1 "Interactive Analytics in Human Time"]. 17 June 2014</ref>{{rp|9,16}}\n\nThe [[Netflix]] Suro project has separate processing paths for data, but does not strictly follow lambda architecture since the paths may be intended to serve different purposes and not necessarily to provide the same type of views.<ref name=netflix>Bae, Jae Hyeon; Yuan, Danny; Tonse, Sudhir. [http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html "Announcing Suro: Backbone of Netflix\'s Data Pipeline"], \'\'[[Netflix]]\'\', 9 December 2013</ref> Nevertheless, the overall idea is to make selected real-time event data available to queries with very low latency, while the entire data set is also processed via a batch pipeline. The latter is intended for applications that are less sensitive to latency and require a map-reduce type of processing.\n\n==Criticism==\nCriticism of lambda architecture has focused on its inherent complexity and its limiting influence. The batch and streaming sides each require a different code base that must be maintained and kept in sync so that processed data produces the same result from both paths. Yet attempting to abstract the code bases into a single framework puts many of the specialized tools in the batch and real-time ecosystems out of reach.<ref>{{cite web|last1=Kreps|first1=Jay|title=Questioning the Lambda Architecture|url=http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html|website=radar.oreilly.com|publisher=Oreilly|accessdate=15 August 2014|ref=kreps}}</ref>\n\nIn a technical discussion over the merits of employing a pure streaming approach, it was noted that using a flexible streaming framework such as [[Apache Samza]] could provide some of the same benefits as batch processing without the latency.<ref>[https://news.ycombinator.com/item?id=7976785 Hacker News] retrieved 20 August 2014</ref> Such a streaming framework could allow for collecting and processing arbitrarily large windows of data, accommodate blocking, and handle state.\n\n== References ==\n<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using <ref></ref> tags, these references will then appear here automatically -->\n{{Reflist}}\n\n== External links ==\n* [http://lambda-architecture.net/ Repository of Information on Lambda of Architecture]\n\n<!--- Categories --->\n[[Category:Articles created via the Article Wizard]]\n[[Category:Data processing]]\n[[Category:Big data]]\n[[Category:Data management]]\n[[Category:Free software projects]]\n[[Category:Software architecture]]']
['Business intelligence', '168387', '{{Use dmy dates|date=March 2012}}\n\n{{Business administration}}\n\n\'\'\'Business Intelligence\'\'\' (\'\'\'BI\'\'\') are the set of strategies, processes, [[application software|applications]], [[data]], products, technologies and technical architectures which are used to support the collection, analysis, presentation and dissemination of business information.<ref>Dedić N. & Stanier C. (2016). Measuring the Success of Changes to Existing Business Intelligence Solutions to Improve Business Intelligence Reporting. Lecture Notes in Business Information Processing. Springer International Publishing. Volume 268, pp. 225-236.</ref> BI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies are [[Business reporting|reporting]], [[online analytical processing]], [[analytics]], [[data mining]], [[process mining]], [[complex event processing]], [[business performance management]], [[benchmarking]], [[text mining]], [[Predictive Analysis|predictive analytics]] and [[Prescriptive Analytics|prescriptive analytics]] and are capable of handling large amounts of structured and sometimes unstructured data to help identify, develop and otherwise create new strategic business opportunities. The goal is to allow for the easy interpretation of these [[big data]]. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.<ref>({{cite book |title= Business Intelligence Success Factors: Tools for Aligning Your Business in the Global Economy |last= Rud|first= Olivia |year= 2009|publisher= Wiley & Sons|location= Hoboken, N.J|isbn= 978-0-470-39240-9 |page= |pages= |url= |accessdate=}})</ref>\n\nBusiness intelligence can be used to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions include priorities, goals and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a more complete picture which, in effect, creates an "intelligence" that cannot be derived by any singular set of data.<ref>{{cite book| last1= Coker| first1= Frank| title= Pulse: Understanding the Vital Signs of Your Business| publisher= Ambient Light Publishing\n| publication-date= 2014| pages= 41–42| isbn= 978-0-9893086-0-1}}</ref> Amongst myriad uses, business intelligence tools empower organisations to gain insight into new markets, assess demand and suitability of products and services for different market segments and gauge the impact of marketing efforts.<ref name=":0">Chugh, R & Grandhi, S 2013, ‘Why Business Intelligence? Significance of Business Intelligence tools and integrating BI governance with corporate governance’, International Journal of E-Entrepreneurship and Innovation, vol. 4, no.2, pp. 1-14. https://www.researchgate.net/publication/273861123_Why_Business_Intelligence_Significance_of_Business_Intelligence_Tools_and_Integrating_BI_Governance_with_Corporate_Governance</ref>\n\n==Components==\nBusiness intelligence is made up of an increasing number of components including:\n* Multidimensional aggregation and allocation\n* [[Denormalization]], tagging and standardization\n* Realtime reporting with analytical alert\n* A method of interfacing with [[unstructured data]] sources\n* Group consolidation, budgeting and [[rolling forecast]]s\n* [[Statistical inference]] and probabilistic simulation\n* [[Key performance indicator]]s optimization\n* Version control and process management\n* Open item management\n\n==History==\nThe earliest known use of the term "Business Intelligence" is in Richard Millar Devens’ in the ‘Cyclopædia of Commercial and Business Anecdotes’ from 1865. Devens used the term to describe how the banker, Sir Henry Furnese, gained profit by receiving and acting upon information about his environment, prior to his competitors. “\'\'Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the [[Siege of Namur (1695)|fall of Namur]] added to his profits, owing to his early receipt of the news\'\'.” (Devens, (1865), p.&nbsp;210).  The ability to collect and react accordingly based on the information retrieved, an ability that Furnese excelled in, is today still at the very heart of BI.<ref name="Miller Devens">{{cite book|last=Miller Devens|first=Richard|title=Cyclopaedia of Commercial and Business Anecdotes; Comprising Interesting Reminiscences and Facts, Remarkable Traits and Humors of Merchants, Traders, Bankers Etc. in All Ages and Countries|url=https://books.google.dk/books?id=9MspAAAAYAAJ&pg=PA210&dq=%22business+intelligence%22&hl=en&ei=a5EPTdaRIsOWnAeVyYHQDg&sa=X&oi=book_result&ct=result&redir_esc=y#v=onepage&q=%22business%20intelligence%22&f=false|publisher=D. Appleton and company|accessdate=15 February 2014|page=210}}</ref>\n\nIn a 1958 article, [[IBM]] researcher [[Hans Peter Luhn]] used the term business intelligence. He employed the Webster\'s dictionary definition of intelligence: "the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal."<ref>\n{{cite journal|url= http://www.research.ibm.com/journal/rd/024/ibmrd0204H.pdf|doi=10.1147/rd.24.0314|title= A Business Intelligence System|author=H P Luhn |authorlink= Hans Peter Luhn |year= 1958 |journal= IBM Journal|volume= 2|issue= 4|pages= 314}}\n</ref>\n\nBusiness intelligence as it is understood today is said to have evolved from the [[decision support system]]s (DSS) that began in the 1960s and developed throughout the mid-1980s. DSS originated in the computer-aided models created to assist with [[decision making]] and planning. From DSS, [[data warehouse]]s, [[Executive Information System]]s, [[Online analytical processing|OLAP]] and business intelligence came into focus beginning in the late 80s.\n\nIn 1989, Howard Dresner (later a [[Gartner]] analyst) proposed "business intelligence" as an umbrella term to describe "concepts and methods to improve business decision making by using fact-based support systems."<ref name=power>{{cite web |url= http://dssresources.com/history/dsshistory.html\n|title= A Brief History of Decision Support Systems, version 4.0 |accessdate=10 July 2008\n|author= D. J. Power |date= 10 March 2007|publisher= DSSResources.COM }}\n</ref> It was not until the late 1990s that this usage was widespread.<ref>{{cite web |url=http://dssresources.com/history/dsshistory.html |title=A Brief History of Decision Support Systems |last=Power |first=D. J. |accessdate=1 November 2010 }}</ref>\n\nCritics see BI as evolved from mere [[business reporting]] together with the advent of increasingly powerful and easy-to-use [[data analysis]] tools. In this respect it has also been criticized as a marketing buzzword in the context of the "[[big data]]" surge.<ref>{{cite web|title=Decoding big data buzzwords|year=2015|quote=BI refers to the approaches, tools, mechanisms that organizations can use to keep a finger on the pulse of their businesses. Also referred by unsexy versions -- “dashboarding”, “MIS” or “reporting.”|publisher=cio.com|url=http://www.cio.com/article/2919082/big-data/what-are-they-talking-about-decoding-big-data-buzzwords.html}}</ref>\n\n==Data warehousing==\nOften BI applications use data gathered from a [[data warehouse]] (DW) or from a [[data mart]], and the concepts of BI and DW sometimes combine as "\'\'\'BI/DW\'\'\'"<ref>\n{{cite book\n| last1                 = Golden\n| first1                = Bernard\n| title                 = Amazon Web Services For Dummies\n| url                   = https://books.google.com/books?id=xSVwAAAAQBAJ\n| series                = For dummies\n| publisher             = John Wiley & Sons\n| publication-date      = 2013\n| page                  = 234\n| isbn                  = 9781118652268\n| accessdate            = 2014-07-06\n| quote                 = [...] traditional business intelligence or data warehousing tools (the terms are used so interchangeably that they\'re often referred to as BI/DW) are extremely expensive [...]\n}}\n</ref>\nor as "\'\'\'BIDW\'\'\'". A data warehouse contains a copy of analytical data that facilitates decision support. However, not all data warehouses serve for business intelligence, nor do all business intelligence applications require a data warehouse.\n\nTo distinguish between the concepts of business intelligence and data warehouses, [[Forrester Research]] defines business intelligence in one of two ways:\n\n# Using a broad definition: "Business Intelligence is a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making."<ref>{{cite web |url=http://www.forrester.com/rb/Research/topic_overview_business_intelligence/q/id/39218/t/2 |title=Topic Overview: Business Intelligence |last=Evelson |first=Boris |date=21 November 2008}}</ref> Under this definition, business intelligence also includes technologies such as data integration, data quality, data warehousing, master-data management, text- and content-analytics, and many others that the market sometimes lumps into the "[[Information Management]]" segment. Therefore, Forrester refers to \'\'data preparation\'\' and \'\'data usage\'\' as two separate but closely linked segments of the business-intelligence architectural stack.\n# Forrester defines the narrower business-intelligence market as, "...referring to just the top layers of the BI architectural stack such as reporting, analytics and [[Dashboards (management information systems)|dashboards]]."<ref>{{cite web\n|url=http://blogs.forrester.com/boris_evelson/10-04-29-want_know_what_forresters_lead_data_analysts_are_thinking_about_bi_and_data_domain\n|title=Want to know what Forrester\'s lead data analysts are thinking about BI and the data domain? |last=Evelson |first=Boris |date=29 April 2010}}</ref>\n\n==Comparison with competitive intelligence==\nThough the term business intelligence is sometimes a synonym for [[competitive intelligence]] (because they both support [[decision making]]), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can include the subset of competitive intelligence.<ref>{{cite web |url=http://blogs.forrester.com/james_kobielus/10-04-30-what%E2%80%99s_not_bi_oh_don%E2%80%99t_get_me_startedoops_too_latehere_goes |title=What’s Not BI? Oh, Don’t Get Me Started....Oops Too Late...Here Goes.... |last=Kobielus |first=James |date=30 April 2010 |quote=“Business” intelligence is a non-domain-specific catchall for all the types of analytic data that can be delivered to users in reports, dashboards, and the like. When you specify the subject domain for this intelligence, then you can refer to “competitive intelligence,” “market intelligence,” “social intelligence,” “financial intelligence,” “HR intelligence,” “supply chain intelligence,” and the like.}}</ref>\n\n==Comparison with business analytics==\nBusiness intelligence and [[business analytics]] are sometimes used interchangeably, but there are alternate definitions.<ref>{{cite web|url=http://timoelliott.com/blog/2011/03/business-analytics-vs-business-intelligence.html |title=Business Analytics vs Business Intelligence? |publisher=timoelliott.com |date=2011-03-09 |accessdate=2014-06-15}}</ref>  One definition contrasts the two, stating that the term business intelligence refers to collecting business data to find information primarily through asking questions, reporting, and online analytical processes. Business analytics, on the other hand, uses statistical and quantitative tools for explanatory and [[predictive modelling]].<ref>{{cite web|url=http://www.businessanalytics.com/difference-between-business-analytics-and-business-intelligence/ |title=Difference between Business Analytics and Business Intelligence |publisher=businessanalytics.com |date=2013-03-15 |accessdate=2014-06-15}}</ref>\n\nIn an alternate definition, [[Thomas H. Davenport|Thomas Davenport]], professor of information technology and management at [[Babson College]] argues that business intelligence should be divided into [[Information retrieval|querying]], [[Business reporting|reporting]], [[Online analytical processing]] (OLAP), an "alerts" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.<ref>{{Cite interview |url=http://www.informationweek.com/news/software/bi/222200096 |title=Analytics at Work: Q&A with Tom Davenport |last=Henschen |first=Doug |date=4 January 2010}}</ref>\n\n==Applications in an enterprise==\nBusiness intelligence can be applied to the following business purposes, in order to drive business value.{{Citation needed|date=October 2010}}\n# [[Measurement]]&nbsp;– program that creates a hierarchy of [[performance metrics]] (see also [[Metrics Reference Model]]) and [[benchmarking]] that informs business leaders about progress towards business goals ([[business process management]]).\n# [[Analytics]]&nbsp;– program that builds quantitative processes for a business to arrive at optimal decisions and to perform business knowledge discovery. Frequently involves: [[data mining]], [[process mining]], [[statistical analysis]], [[predictive analytics]], [[predictive modeling]], [[business process modeling]], [[data lineage]], [[complex event processing]] and [[Prescriptive Analytics|prescriptive analytics]].\n# [[Business reporting|Reporting]]/[[enterprise reporting]]&nbsp;– program that builds infrastructure for strategic reporting to serve the strategic management of a business, not operational reporting. Frequently involves [[data visualization]], [[executive information system]] and [[OLAP]].\n# [[Collaboration]]/[[collaboration platform]]&nbsp;– program that gets different areas (both inside and outside the business) to work together through [[data sharing]] and [[electronic data interchange]].\n# [[Knowledge management]]&nbsp;– program to make the company data-driven through strategies and practices to identify, create, represent, distribute, and enable adoption of insights and experiences that are true business knowledge. Knowledge management leads to [[learning management]] and [[regulatory compliance]].\n\nIn addition to the above, business intelligence can provide a pro-active approach, such as alert functionality that immediately notifies the end-user if certain conditions are met. For example, if some business metric exceeds a pre-defined threshold, the metric will be highlighted in standard reports, and the business analyst may be alerted via e-mail or another monitoring service. This end-to-end process requires data governance, which should be handled by the expert.{{Citation needed|date=January 2012}}\n\n==Prioritization of projects==\nIt can be difficult to provide a positive business case for business intelligence initiatives, and often the projects must be prioritized through strategic initiatives. BI projects can attain higher prioritization within the organization if managers consider the following:\n* As described by Kimball<ref>Kimball et al., 2008: 29</ref> the BI manager must determine the tangible benefits such as eliminated cost of producing legacy reports.\n* Data access for the entire organization must be enforced.<ref>{{cite web|url= http://content.dell.com/us/en/enterprise/d/large-business/ready-business-intelligence.aspx|title= Are You Ready for the New Business Intelligence?|publisher=Dell.com | accessdate=19 June 2012}}</ref> In this way even a small benefit, such as a few minutes saved, makes a difference when multiplied by the number of employees in the entire organization.\n* As described by Ross, Weil & Roberson for Enterprise Architecture,<ref>[[Jeanne W. Ross]], [[Peter Weill]], [[David C. Robertson]] (2006) \'\'Enterprise Architecture As Strategy\'\', p. 117 ISBN 1-59139-839-8.</ref> managers should also consider letting the BI project be driven by other business initiatives with excellent business cases. To support this approach, the organization must have enterprise architects who can identify suitable business projects.\n* Using a structured and quantitative methodology to create defensible prioritization in line with the actual needs of the organization, such as a weighted decision matrix.<ref>{{cite web|last=Krapohl|first=Donald|title=A Structured Methodology for Group Decision Making|url=http://www.augmentedintel.com/wordpress/index.php/a-structured-methodology-for-group-decision-making/|publisher=AugmentedIntel|accessdate=22 April 2013}}</ref>\n\n==Success factors of implementation==\nAccording to Kimball et al., there are three critical areas that organizations should assess before getting ready to do a BI project:<ref>Kimball et al. 2008: p. 298</ref>\n\n# The level of commitment and sponsorship of the project from senior management.\n# The level of business need for creating a BI implementation.\n# The amount and quality of business data available.\n\n===Business sponsorship===\n\nThe commitment and [[:wikt:sponsor|sponsor]]ship of senior management is according to Kimball \'\'et al.\'\', the most important criteria for assessment.<ref>Kimball et al., 2008: 16</ref> This is because having strong management backing helps overcome shortcomings elsewhere in the project. However, as Kimball \'\'et al.\'\' state: “even the most elegantly designed DW/BI system cannot overcome a lack of business [management] sponsorship”.<ref>Kimball et al., 2008: 18</ref>\n\nIt is important that personnel who participate in the project have a vision and an idea of the benefits and drawbacks of implementing a BI system. The best business sponsor should have organizational clout and should be well connected within the organization. It is ideal that the business sponsor is demanding but also able to be realistic and supportive if the implementation runs into delays or drawbacks. The management sponsor also needs to be able to assume accountability and to take responsibility for failures and setbacks on the project. Support from multiple members of the management ensures the project does not fail if one person leaves the steering group. However, having many managers work together on the project can also mean that there are several different interests that attempt to pull the project in different directions, such as if different departments want to put more emphasis on their usage. This issue can be countered by an early and specific analysis of the business areas that benefit the most from the implementation. All stakeholders in the project should participate in this analysis in order for them to feel invested in the project and to find common ground.\n\nAnother management problem that may be encountered before the start of an implementation is an overly aggressive business sponsor. Problems of [[scope creep]] occur when the sponsor requests data sets that were not specified in the original planning phase.\n\n===Business needs===\n\nBecause of the close relationship with senior management, another critical thing that must be assessed before the project begins is whether or not there is a business need and whether there is a clear business benefit by doing the implementation.<ref name="Kimball et al., 2008: 17">Kimball et al., 2008: 17</ref>\nThe needs and benefits of the implementation are sometimes driven by competition and the need to gain an advantage in the market. Another reason for a business-driven approach to implementation of BI is the acquisition of other organizations that enlarge the original organization it can sometimes be beneficial to implement DW or BI in order to create more oversight.\n\nCompanies that implement BI are often large, multinational organizations with diverse subsidiaries.<ref>{{cite web|title=How Companies Are Implementing Business Intelligence Competency Centers |url=http://www.computerworld.com/pdfs/SAS_Intel_BICC.pdf |publisher=Computer World |deadurl=yes |accessdate=1 April 2014 |archiveurl=https://web.archive.org/web/20130528054421/http://www.computerworld.com/pdfs/SAS_Intel_BICC.pdf |archivedate=28 May 2013 }}</ref> A well-designed BI solution provides a consolidated view of key business data not available anywhere else in the organization, giving management visibility and control over measures that otherwise would not exist.\n\n===Amount and quality of available data===\n\nWithout proper data, or with too little quality data, any BI implementation fails; it does not matter how good the management sponsorship or business-driven motivation is. Before implementation it is a good idea to do [[data profiling]]. This analysis identifies the “content, consistency and structure [..]”<ref name="Kimball et al., 2008: 17"/> of the data. This should be done as early as possible in the process and if the analysis shows that data is lacking, put the project on hold temporarily while the IT department figures out how to properly collect data.\n\nWhen planning for business data and business intelligence requirements, it is always advisable to consider specific scenarios that apply to a particular organization, and then select the business intelligence features best suited for the scenario.\n\nOften, scenarios revolve around distinct business processes, each built on one or more data sources. These sources are used by features that present that data as information to knowledge workers, who subsequently act on that information. The business needs of the organization for each business process adopted correspond to the essential steps of business intelligence. These essential steps of business intelligence include but are not limited to:\n#Go through business data sources in order to collect needed data\n#Convert business data to information and present appropriately\n#Query and analyze data\n#Act on the collected data\nThe \'\'\'quality aspect\'\'\' in business intelligence should cover all the process from the source data to the final reporting. At each step, the \'\'\'quality gates\'\'\' are different:\n# Source Data:\n#* Data Standardization: make data comparable (same unit, same pattern...)\n#* [[Master data management|Master Data Management:]] unique referential\n# [[Operational data store|Operational Data Store (ODS)]]:\n#* [[Data cleansing|Data Cleansing:]] detect & correct inaccurate data\n#* Data Profiling: check inappropriate value, null/empty\n# [[Data warehouse]]:\n#* Completeness: check that all expected data are loaded\n#* [[Referential integrity]]: unique and existing referential over all sources\n#* Consistency between sources: check consolidated data vs sources\n# Reporting:\n#* Uniqueness of indicators: only one share dictionary of indicators\n#* Formula accuracy: local reporting formula should be avoided or checked\n\n==User aspect==\n\nSome considerations must be made in order to successfully integrate the usage of business intelligence systems in a company. Ultimately the BI system must be accepted and utilized by the users in order for it to add value to the organization.<ref name = kimball>Kimball</ref><ref name = swain>Swain Scheps \'\'Business Intelligence for Dummies\'\', 2008, ISBN 978-0-470-12723-0</ref> If the [[usability]] of the system is poor, the users may become frustrated and spend a considerable amount of time figuring out how to use the system or may not be able to really use the system. If the system does not add value to the users´ mission, they simply don\'t use it.<ref name = swain />\n\nTo increase user acceptance of a BI system, it can be advisable to consult business users at an early stage of the DW/BI lifecycle, for example at the requirements gathering phase.<ref name = kimball /> This can provide an insight into the [[business process]] and what the users need from the BI system. There are several methods for gathering this information, such as questionnaires and interview sessions.\n\nWhen gathering the requirements from the business users, the local IT department should also be consulted in order to determine to which degree it is possible to fulfill the business\'s needs based on the available data.<ref name = kimball />\n\nTaking a user-centered approach throughout the design and development stage may further increase the chance of rapid user adoption of the BI system.<ref name = swain />\n\nBesides focusing on the user experience offered by the BI applications, it may also possibly motivate the users to utilize the system by adding an element of competition. Kimball<ref name = kimball /> suggests implementing a function on the Business Intelligence portal website where reports on system usage can be found. By doing so, managers can see how well their departments are doing and compare themselves to others and this may spur them to encourage their staff to utilize the BI system even more.\n\nIn a 2007 article, H. J. Watson gives an example of how the competitive element can act as an incentive.<ref name = watson>{{cite journal|title=The Current State of Business Intelligence|year=2007|doi=10.1109/MC.2007.331|last1=Watson|first1=Hugh J.|last2=Wixom|first2=Barbara H.|journal=Computer|volume=40|issue=9|pages=96}}</ref> Watson describes how a large call centre implemented performance dashboards for all call agents, with monthly incentive bonuses tied to performance metrics. Also, agents could compare their performance to other team members. The implementation of this type of performance measurement and competition significantly improved agent performance.\n\nBI chances of success can be improved by involving senior management to help make BI a part of the [[organizational culture]], and by providing the users with necessary tools, training, and support.<ref name = watson /> Training encourages more people to use the BI application.<ref name = kimball />\n\nProviding user support is necessary to maintain the BI system and resolve user problems.<ref name = swain /> User support can be incorporated in many ways, for example by creating a website. The website should contain great content and tools for finding the necessary information. Furthermore, helpdesk support can be used. The help desk can be manned by power users or the DW/BI project team.<ref name = kimball />\n\n==BI Portals==\nA \'\'\'Business Intelligence portal\'\'\' (BI portal) is the primary access interface for [[Data warehouse|Data Warehouse]] (DW) and Business Intelligence (BI) applications. The BI portal is the user\'s first impression of the DW/BI system. It is typically a browser application, from which the user has access to all the individual services of the DW/BI system, reports and other analytical functionality.\nThe BI portal must be implemented in such a way that it is easy for the users of the DW/BI application to call on the functionality of the application.<ref name="Ralph">\'\'The Data Warehouse Lifecycle Toolkit (2nd ed.). Ralph Kimball (2008).\'\'</ref>\n\nThe BI portal\'s main functionality is to provide a navigation system of the DW/BI application. This means that the portal has to be implemented in a way that the user has access to all the functions of the DW/BI application.\n\nThe most common way to design the portal is to custom fit it to the business processes of the organization for which the DW/BI application is designed, in that way the portal can best fit the needs and requirements of its users.<ref name="Wiley">\'\'Microsoft Data Warehouse Toolkit. Wiley Publishing. (2006)\'\'</ref>\n\nThe BI portal needs to be easy to use and understand, and if possible have a look and feel similar to other applications or web content of the organization the DW/BI application is designed for ([[consistency]]).\n\nThe following is a list of desirable features for [[web portal]]s in general and BI portals in particular:\n\n;Usable: User should easily find what they need in the BI tool.\n;Content Rich: The portal is not just a report printing tool, it should contain more functionality such as advice, help, support information and documentation.\n;Clean: The portal should be designed so it is easily understandable and not over-complex as to confuse the users\n;Current: The portal should be updated regularly.\n;Interactive: The portal should be implemented in a way that makes it easy for the user to use its functionality and encourage them to use the portal. Scalability and customization give the user the means to fit the portal to each user.\n;Value Oriented: It is important that the user has the feeling that the DW/BI application is a valuable resource that is worth working on.\n\n==Marketplace==\nThere are a number of business intelligence vendors, often categorized into the remaining independent "pure-play" vendors and consolidated "megavendors" that have entered the market through a recent trend<ref>{{cite news|url=http://www.zdnet.com/gartner-releases-2013-bi-magic-quadrant-7000011264/ |title=Gartner releases 2013 BI Magic Quadrant |publisher=ZDNet |author=Andrew Brust| date= 2013-02-14|accessdate=21 August 2013}}</ref> of acquisitions in the BI industry.<ref>{{cite web |url=http://www.bi-verdict.com/fileadmin/FreeAnalyses/consolidations.htm |title=Consolidations in the BI industry |date=7 March 2008 |last=Pendse |first=Nigel |work=The OLAP Report}}</ref> The business intelligence market is gradually growing. In 2012 business intelligence services brought in $13.1 billion in revenue.<ref>{{cite web|title=Why Business Intelligence Is Key For Competitive Advantage|url=https://cisonline.bu.edu/news-resources/why-business-intelligence-is-key-for-competitive-advantage/|website=Boston University|accessdate=23 October 2014}}</ref>\n\nSome companies adopting BI software decide to pick and choose from different product offerings (best-of-breed) rather than purchase one comprehensive integrated solution (full-service).<ref>{{cite web |url=http://www.b-eye-network.com/view/2608 |title=Three Trends in Business Intelligence Technology |last=Imhoff |first=Claudia |date=4 April 2006}}</ref>\n\n===Industry-specific===\nSpecific considerations for business intelligence systems have to be taken in some sectors such as [[Bank regulation|governmental banking regulations]] or healthcare.<ref>{{cite journal |vauthors=Mettler T, Vimarlund V |title=Understanding business intelligence in the context of healthcare |journal=Health Informatics Journal |volume=15 |issue=3 |pages=254–264 |year=2009 |doi=10.1177/1460458209337446 }}</ref> The information collected by banking institutions and analyzed with BI software must be protected from some groups or individuals, while being fully available to other groups or individuals. Therefore, BI solutions must be sensitive to those needs and be flexible enough to adapt to new regulations and changes to existing law.{{citation needed|date=May 2016}}\n\n==Semi-structured or unstructured data==\nBusinesses create a huge amount of valuable information in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material and news. According to Merrill Lynch, more than 85% of all business information exists in these forms. These information types are called either \'\'[[Semi-structured data|semi-structured]]\'\' or \'\'[[Unstructured data|unstructured]]\'\' data. However, organizations often only use these documents once.<ref name = rao>{{cite journal|doi=10.1109/MITP.2003.1254966 |url=http://www.ramanarao.com/papers/rao-itpro-2003-11.pdf|title=From unstructured data to actionable intelligence|year=2003|last1=Rao|first1=R.|journal=IT Professional|volume=5|issue=6|pages=29}}</ref>\n\nThe managements of semi-structured data is recognized as a major unsolved problem in the information technology industry.<ref name = blumberg>{{cite journal|url=http://soquelgroup.com/Articles/dmreview_0203_problem.pdf|author1=Blumberg, R.  |author2=S. Atre  |lastauthoramp=yes |title=The Problem with Unstructured Data|journal=DM Review |year=2003|pages=42–46}}</ref> According to projections from Gartner (2003), white collar workers spend anywhere from 30 to 40 percent of their time searching, finding and assessing unstructured data. BI uses both structured and unstructured data, but the former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision making.<ref name = blumberg /><ref name = negash>{{cite journal|url=http://site.xavier.edu/sena/info600/businessintelligence.pdf|author=Negash, S |title=Business Intelligence|journal= Communications of the Association of Information Systems|volume=13|year= 2004|pages=177–195}}</ref> Because of the difficulty of properly searching, finding and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task or project. This can ultimately lead to poorly informed decision making.<ref name = rao />\n\nTherefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.<ref name = negash />\n\n===Unstructured data vs. semi-structured data===\nUnstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered [[columns]] and [[Row (database)|rows]]. One type of unstructured data is typically stored in a [[BLOB]] (binary large object), a catch-all data type available in most [[relational database]] management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row within each file or document.{{citation needed|date=May 2016}}\n\nMany of these data types, however, like e-mails, word processing text files, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database.\nTherefore, it may be more accurate to talk about this as semi-structured documents or data,<ref name = blumberg /> but no specific consensus seems to have been reached.\n\nUnstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.\n\n===Problems with semi-structured or unstructured data===\nThere are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,<ref name = inmon>Inmon, B. & A. Nesavich, "Unstructured Textual Data in the Organization" from "Managing Unstructured data in the organization", Prentice Hall 2008, pp. 1–13</ref> some of those are:\n\n# Physically accessing unstructured textual data&nbsp;– unstructured data is stored in a huge variety of formats.\n# [[Terminology]]&nbsp;– Among researchers and analysts, there is a need to develop a standardized terminology.\n# Volume of data&nbsp;– As stated earlier, up to 85% of all data exists as semi-structured data. Couple that with the need for word-to-word and semantic analysis.\n# Searchability of unstructured textual data&nbsp;– A simple search on some data, e.g. apple, results in links where there is a reference to that precise search term. (Inmon & Nesavich, 2008)<ref name = inmon /> gives an example: “a search is made on the term felony. In a simple search, the term felony is used, and everywhere there is a reference to felony, a hit to an unstructured document is made. But a simple search is crude. It does not find references to crime, arson, murder, embezzlement, vehicular homicide, and such, even though these crimes are types of felonies.”\n\n===The use of metadata===\nTo solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of [[metadata]].<ref name = rao /> Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content&nbsp;– e.g. summaries, topics, people or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and [[information extraction]].\n\n==2009 predictions==\nA 2009 paper predicted<ref>[http://www.gartner.com/it/page.jsp?id=856714 Gartner Reveals Five Business Intelligence Predictions for 2009 and Beyond]. gartner.com. 15 January 2009</ref> these developments in the business intelligence market:\n* Because of lack of information, processes, and tools, through 2012, more than 35 percent of the top 5,000 global companies regularly fail to make insightful decisions about significant changes in their business and markets.\n* By 2012, business units will control at least 40 percent of the total budget for business intelligence.\n* By 2012, one-third of analytic applications applied to business processes will be delivered through [[Granularity|coarse-grained]] application [[mashup (web application hybrid)|mashups]].\n* BI has a huge scope in Entrepreneurship however majority of new entrepreneurs ignore its potential.<ref>[http://brighterkashmir.com/role-of-business-intelligence-in-entrepreneurship/ huge scope in Entrepreneurship]</ref>\n\nA 2009 \'\'Information Management\'\' special report predicted the top BI trends: "[[green computing]], [[social networking service]]s, [[data visualization]], [[Mobile business intelligence|mobile BI]], [[predictive analytics]], [[composite application]]s, [[cloud computing]] and [[Multi-touch|multitouch]]".<ref>{{cite web |url=http://www.information-management.com/specialreports/2009_148/business_intelligence_data_vizualization_social_networking_analytics-10015628-1.html |title=10 Red Hot BI Trends |last=Campbell |first=Don |date=23 June 2009 |work=Information Management}}</ref> Research undertaken in 2014 indicated that employees are more likely to have access to, and more likely to engage with, cloud-based BI tools than traditional tools.<ref>{{cite web |url=http://www.aberdeen.com/Aberdeen-Library/8906/RR-analytics-cloud-saas-bi.aspx |title=Cloud Analytics in 2014: Infusing the Workforce with Insight |last=Lock|first=Michael|date=27 March 2014 }}</ref>\n\nOther business intelligence trends include the following:\n\n* Third party SOA-BI products increasingly address [[Extract, transform, load|ETL]] issues of volume and throughput.\n* Companies embrace in-memory processing, 64-bit processing, and pre-packaged analytic BI applications.\n* Operational applications have callable BI components, with improvements in response time, scaling, and concurrency.\n* Near or real time BI analytics is a baseline expectation.\n* Open source BI software replaces vendor offerings.\n\nOther lines of research include the combined study of business intelligence and uncertain data.<ref>{{Cite journal\n | last=Rodriguez | first=Carlos\n | last2=Daniel | first2=Florian\n | last3=Casati | first3=Fabio\n | last4=Cappiello | first4=Cinzia\n | year=2010\n | title=Toward Uncertain Business Intelligence: The Case of Key Indicators |doi=10.1109/MIC.2010.59\n | journal=IEEE Internet Computing\n | volume=14\n | issue=4\n | pages=32 }}</ref><ref>{{citation |author=Rodriguez, C. |author2=Daniel, F. |author3=Casati, F. |author4=Cappiello, C. |last-author-amp=yes |url=http://mitiq.mit.edu/ICIQ/Documents/IQ%20Conference%202009/Papers/3-C.pdf |title=Computing Uncertain Key Indicators from Uncertain Data  |pages=106–120 | conference = ICIQ\'09 | year = 2009}}</ref> In this context, the data used is not assumed to be precise, accurate and complete. Instead, data is considered uncertain and therefore this uncertainty is propagated to the results produced by BI.\n\nAccording to a study by the Aberdeen Group, there has been increasing interest in [[Software-as-a-Service]] (SaaS) business intelligence over the past years, with twice as many organizations using this deployment approach as one year ago&nbsp;– 15% in 2009 compared to 7% in 2008.<ref>{{cite web|last1=Julian|first1=Taylor|title=Business intelligence implementation according to customer\'s needs|url=http://apro-software.com/services/software-development/business-intelligence|publisher=APRO Software|accessdate=16 May 2016|date=10 January 2010}}</ref>\n\nAn article by InfoWorld’s Chris Kanaracus points out similar growth data from research firm IDC, which predicts the SaaS BI market will grow 22 percent each year through 2013 thanks to increased product sophistication, strained IT budgets, and other factors.<ref>[http://infoworld.com/d/cloud-computing/saas-bi-growth-will-soar-in-2010-511 SaaS BI growth will soar in 2010 | Cloud Computing]. InfoWorld (2010-02-01). Retrieved 17 January 2012.</ref>\n\nAn analysis of top 100 Business Intelligence and Analytics scores and ranks the firms based on several open variables<ref>{{cite web|url=http://www.appsbi.com/top-100-analytics-companies-ranked-and-scored-by-mattermark|title=Top 100 analytics companies ranked and scored by Mattermark -  Business Intelligence - Dashboards - Big Data|publisher=}}</ref>\n\n==See also==\n{{colbegin|3}}\n* [[Accounting intelligence]]\n* [[Analytic applications]]\n* [[Artificial intelligence marketing]]\n* [[Business Intelligence 2.0]]\n* [[Business process discovery]]\n* [[Business process management]]\n* [[Business activity monitoring]]\n* [[Business service management]]\n* [[Comparison of OLAP Servers]]\n* [[Customer dynamics]]\n* [[Data Presentation Architecture]]\n* [[Data visualization]]\n* [[Decision engineering]]\n* [[Enterprise planning systems]]\n* [[Infonomics]]\n* [[Intelligent document|Document intelligence]]\n* [[Integrated business planning]]\n* [[Location intelligence]]\n* [[Media intelligence]]\n* [[Meteorological intelligence]]\n* [[Mobile business intelligence]]\n* [[Multiway Data Analysis]]\n* [[Operational intelligence]]\n* [[Business Information Systems]]\n* [[Business intelligence tools]]\n* [[Process mining]]\n* [[Real-time business intelligence]]\n* [[Runtime intelligence]]\n* [[Sales intelligence]]\n* [[Test and learn]]\n\n{{colend}}\n\n==References==\n{{Reflist|30em}}\n\n==Bibliography==\n*Ralph Kimball \'\'et al.\'\' "The Data warehouse Lifecycle Toolkit" (2nd ed.) Wiley ISBN 0-470-47957-4\n*Peter Rausch, Alaa Sheta, Aladdin Ayesh : \'\'Business Intelligence and Performance Management: Theory, Systems, and Industrial Applications\'\', Springer Verlag U.K., 2013, ISBN 978-1-4471-4865-4.\n\n==External links==\n* [http://online.sju.edu/resource/engineering-technology/key-role-hadoop-plays-in-business-intelligence "The Key Role Hadoop Plays in Business Intelligence and Data Warehousing" - St. Joseph\'s University]\n* {{cite journal\n|url=http://cacm.acm.org/magazines/2011/8/114953-an-overview-of-business-intelligence-technology/fulltext\n|title=An Overview Of Business Intelligence Technology\n|date=August 2011 | accessdate=26 October 2011\n|first1=Surajit |last1=Chaudhuri |first2=Umeshwar |last2=Dayal |first3=Vivek |last3=Narasayya\n|journal=Communications of the ACM\n|volume =54 |issue= 8 |pages=88–98\n|doi=10.1145/1978542.1978562 }}\n\n{{Data warehouse}}\n\n{{DEFAULTSORT:Business Intelligence}}\n[[Category:Business intelligence| ]]\n[[Category:Financial data analysis]]\n[[Category:Data management]]\n[[Category:Financial technology]]\n[[Category:Information management]]']
['Cut, copy, and paste', '157115', '{{Redirect|Copy & Paste|the album|Hurricane Venus}}\n{{other uses|Cut and paste (disambiguation)}}\n{{Refimprove|date=August 2008}}\nIn [[human–computer interaction]], \'\'\'cut\'\'\', \'\'\'copy\'\'\' and \'\'\'paste\'\'\' are related [[Command (computing)|commands]] that offer a [[user interface|user-interface]] [[interprocess communication]] technique for transferring [[data (computing)|data]]. The \'\'\'cut\'\'\' command removes the [[Selection (user interface)|selected data]] from its original position, while the \'\'\'copy\'\'\' command creates a duplicate; in both cases the selected data is kept in a temporary storage tool called the [[Clipboard (software)|clipboard]]. The data in the clipboard is later inserted in the position where the \'\'\'paste\'\'\' command is issued.\n\nThe command names are an [[interface metaphor]] based on the physical procedure used in [[manuscript]] editing to create a [[page layout]].\n\nThis [[interaction technique]] has close associations with related techniques in [[graphical user interface]]s that use [[pointing device]]s such as a [[computer mouse]] (by [[drag and drop]], for example).\n\nThe capability to replicate information with ease, changing it between contexts and applications, involves [[privacy]] concerns because of the risks of disclosure when handling [[Information sensitivity|sensitive information]]. Terms like \'\'cloning\'\', \'\'copy forward\'\', \'\'carry forward\'\', or \'\'re-use\'\' refer to the dissemination of such information through documents, and may be subject to regulation by [[administrative body|administrative bodies]].<ref name="Laubach">{{cite web|url=http://hcca-info.org/portals/0/pdfs/resources/conference_handouts/regional_conference/2012/seattle/laubachwakefieldprint2.pdf|title=Cloning and Other Compliance Risks in Electronic Medical Records|last1=Laubach|first1=Lori|last2=Wakefield|first2=Catherine|date=June 8, 2012|publisher=[[Moss Adams LLP]], [[MultiCare]]|accessdate=April 23, 2014}}</ref>\n\n==History==\n\n===Origins===\nThe term "\'\'cut and paste\'\'" comes from the traditional practice in manuscript-editings whereby people would cut paragraphs from a page with [[scissors]] and [[Adhesive|paste]] them onto another page. This practice remained standard into the 1980s. Stationery stores formerly sold "editing scissors" with blades long enough to cut an 8½"-wide page. The advent of [[photocopier]]s made the practice easier and more flexible.\n\nThe act of copying/transferring text from one part of a computer-based document ("[[Data buffer|buffer]]") to a different location within the same or different computer-based document was a part of the earliest on-line computer editors. As soon as computer data entry moved from punch-cards to online files (in the mid/late 1960s) there were "commands" for accomplishing this operation. This mechanism was often used to transfer frequently-used commands or text snippets from additional buffers into the document, as was the case with the [[QED (text editor)|QED]] editor.<ref name="communications1967">{{citation|doi=10.1145/363848.363863|last1=Deutsch|first1=L. Peter|authorlink1=L. Peter Deutsch|last2=Lampson|first2=Butler W.|authorlink2=Butler Lampson|title=An online editor|journal=Communications of the ACM |volume=10|issue=12|year=1967|pages=793–799, 803|url=http://research.microsoft.com/en-us/um/people/blampson/04-OnlineEditor/04-OnlineEditor.htm<!-- http://portal.acm.org/citation.cfm?id=363848.363863&coll=ACM&dl=ACM&CFID=15669714&CFTOKEN=68334085 -->}}, p. 793.</ref>\n\n===Early methods===\nThe earliest editors, since they were designed for [[teleprinter]] terminals, provided [[computer keyboard|keyboard]] commands to delineate contiguous regions of text, remove such regions, or move them to some other location in the file.  Since moving a region of text required first removing it from its initial location and then inserting it into its new location various schemes had to be invented to allow for this multi-step process to be specified by the user.\n\nOften this was done by the provision of a \'move\' command, but some text editors required that the text be first put into some temporary location for later retrieval/placement. In 1983, the [[Apple Lisa]] became the first text editing system to call that temporary location "the clipboard".\n\nEarlier control schemes such as [[NLS (computer system)|NLS]] used a [[Linguistic typology#Subject.E2.80.93verb.E2.80.93object positioning|verb-object command structure]], where the command name was provided first and the object to be copied or moved was second. The inversion from [[Subject–verb–object|verb-object]] to [[Subject–object–verb|object-verb]] on which copy and paste are based, where the user selects the object to be operated before initiating the operation, was an innovation crucial for the success of the desktop metaphor as it allowed copy and move operations based on [[direct manipulation]].<ref>{{cite paper|title=Metaphors create theories for users|author=Kuhn, Werner|journal=Spatial Information Theory A Theoretical Basis for GIS|pages=366–376|year=1993|publisher=Springer}}</ref>\n\n===Popularization===\nInspired by early line and character editors that broke a move or copy operation into two steps—between which the user could invoke a preparatory action such as navigation—[[Lawrence G. Tesler]] (Larry Tesler) proposed the names "cut" and "copy" for the first step and "paste" for the second step. Beginning in 1974, he and colleagues at [[Xerox PARC|Xerox Corporation Palo Alto Research Center (PARC)]] implemented several text editors that used cut/copy-and-paste commands to move/copy text.<ref>{{cite web|url=http://www.designinginteractions.com/ |title=Bill Moggridge, Designing Interactions, MIT Press 2007, pp. 63–68 |publisher=Designinginteractions.com |date= |accessdate=2011-11-25}}</ref>\n\n[[Apple Computer]] widely popularized the computer-based cut/copy-and-paste paradigm through the [[Apple Lisa|Lisa]] (1983) and [[Apple Macintosh|Macintosh]] (1984) operating systems and applications. Apple mapped the functionalities to key combinations consisting of the [[Command key]] (a special [[modifier key]]) held down while typing the letters X (for cut), C (for copy), and V (for paste), choosing a handful of [[keyboard shortcuts]] to control basic editing operations. The keys involved all cluster together at the left end of the bottom row of the standard [[QWERTY]] keyboard, and each key is combined with a special [[modifier key]] to perform the desired operation:\n* [[control-Z|Z]] to [[undo]]\n* [[control-X|X]] to cut\n* [[control-C|C]] to copy\n* [[control-V|V]] to paste\nThe [[IBM Common User Access]] (CUA) standard also uses combinations of the [[Insert key|Insert]], [[Del key|Del]], [[Shift key|Shift]] and [[Control key]]s.  Early versions of [[Microsoft Windows|Windows]]{{Dubious|date=March 2014}} used the IBM standard. [[Microsoft]] later also adopted the Apple key combinations with the introduction of [[Microsoft Windows|Windows]]{{Dubious|date=January 2016}}, using the [[control key]] as [[modifier key]]. For users migrating to Windows from [[MS-DOS]] this was a big change as MS-DOS users used the "copy" and "move" commands.\n\nSimilar patterns of key combinations, later borrowed by others, remain widely available {{As of|2007|alt= today}} in most GUI text editors, word processors, and file system browsers.\n\n== Cut and paste ==\nComputer-based editing can involve very frequent use of cut-and-paste operations. Most software-suppliers provide several methods for performing such tasks, and this can involve (for example)  key combinations, pulldown menus, pop-up menus, or [[toolbar]] buttons.\n# The user selects or "highlights" the text or file for moving by some method, typically by [[dragging]] over the text or file name with the pointing-device or holding down the [[Shift key]] while using the [[arrow keys]] to move the [[Cursor (computers)|text cursor]].\n# The user performs a "cut" operation via key combination [[Control key|Ctrl]]+x ([[Command key|⌘]]+x for [[Macintosh]] users), menu, or other means.\n# Visibly, "cut" text immediately disappears from its location.  "Cut" files typically change color to indicate that they will be moved.\n# Conceptually, the text has now moved to a location often called the [[Clipboard (software)|clipboard]]. The clipboard typically remains invisible. On most systems only one clipboard location exists, hence another cut or copy operation overwrites the previously stored information. Many [[Unix|UNIX]] text-editors provide multiple clipboard entries, as do some Macintosh programs such as Clipboard Master,<ref>{{cite web |title=Clipboard Master |work=Clipboard Master 2.0 by In Phase Consulting, July 1994|url=http://forums.info-mac.org/viewtopic.php?f=243&t=14244&sid=739ce1119f88340c52dc2aed3c788fff |accessdate=14 September 2009}}</ref> and Windows [[clipboard manager|clipboard-manager]] programs such as the one in [[Microsoft Office]].\n# The user selects a location for insertion by some method, typically by clicking at the desired insertion point.\n# A \'\'paste\'\' operation takes place which visibly inserts the clipboard text at the insertion point. (The paste operation does not typically destroy the clipboard text: it remains available in the clipboard and the user can insert additional copies at other points).\nWhereas cut-and-paste often takes place with a mouse-equivalent in Windows-like GUI environments, it may also occur entirely from the keyboard, especially in [[Unix|UNIX]] [[text editor]]s, such as [[Pico (text editor)|Pico]] or [[vi]]. Cutting and pasting without a mouse can involve a selection (for which Ctrl+x is pressed in most graphical systems) or the entire current line, but it may also involve text after the [[cursor (computers)|cursor]] until the end of the line and other more sophisticated operations.\n\nWhen a software environment provides \'\'cut\'\' and \'\'paste\'\' functionality, a nondestructive operation called \'\'copy\'\'  usually accompanies them; \'\'copy\'\' places a copy of the selected text in the clipboard without removing it from its original location.\n\nThe clipboard usually stays invisible, because the operations of cutting and pasting, while actually independent, usually take place in quick succession, and the user (usually) needs no assistance in understanding the operation or maintaining mental context. Some application programs provide a means of viewing, or sometimes even editing, the data on the clipboard.\n\n== Copy and paste ==\nThe term "copy-and-paste" refers to the popular, simple method of reproducing [[Character (computing)|text]] or other [[data]] from a source to a destination. It differs from \'\'\'cut and paste\'\'\' in that the original source text or data does not get deleted or removed. The popularity of this method stems from its simplicity and the ease with which users can move data between various applications visually – without resorting to [[Disk storage|permanent storage]].\n\nOnce one has copied data into the [[clipboard]], one may \'\'\'paste\'\'\' the contents of the clipboard into a destination document.\n\nThe [[X Window System]] maintains an additional clipboard containing the most recently selected text; middle-clicking pastes the content of this "selection" clipboard into whatever the [[pointer (computing WIMP)|pointer]] is on at that time.\n\nMost [[terminal emulator]]s and some other applications support the key combinations Ctrl-Insert to copy and Shift-Insert to paste. This is in accordance with the [[IBM Common User Access]] (CUA) standard.\n\n== Find and go ==\nThe [[NeXTStep]] operating system extended the concept of having a single copy buffer by adding a second system-wide \'\'\' Find buffer\'\'\' used for searching. The Find buffer is also available in [[OSX|Mac OS X]].\n\nText can be placed in the Find buffer by either using the Find panel or by selecting text and hitting {{key press|⌘E}}.\n\nThe text can then be searched with \'\'\'Find Next\'\'\' {{key press|⌘G}} and \'\'\'Find Previous\'\'\' {{key press|⌘D}}.\n\nThe functionality comes in handy when for example editing [[source code]]. To find the occurrence of a variable or function name elsewhere in the file, simply select the name by double clicking, hit {{key press|⌘E}} and then jump to the next or previous occurrence with {{key press|⌘G}} / {{key press|⌘D}}.\n\nNote that this does \'\'not\'\' destroy your copy buffer as with other [[User interface|UIs]] like [[Windows]] or the [[X Window System]].\n\nTogether with copy and paste this can be used for quick and easy replacement of repeated text:\n* select the text that you want to replace (i.e. by double clicking)\n* put the text in the Find buffer with {{key press|⌘E}}\n* overwrite the selected text with your replacement text\n* select the replacement text (try {{key press| ⎇⇧←}} to avoid lifting your hands from the keyboard)\n* copy the replacement text {{key press|⌘C}}\n* find the next or previous occurrence {{key press|⌘G}} / {{key press|⌘D}}\n* paste the replacement text {{key press|⌘V}}\n* repeat the last two steps as often as needed\nor in short:\n* select {{key press|⌘ E}}  {{key press|replstr}}  {{key press| ⎇⇧←}}  {{key press|⌘C}}  {{key press|⌘G}}{{key press|⌘V}} {{key press|⌘G}}{{key press|⌘V}} ...\nWhile this might sound a bit complicated at first, it is often \'\'much\'\' faster than using the find panel, especial when only a few occurrences shall be replaced or when only some of the occurrences shall be replaced. When a text shall not be replaced, simply hit {{key press|⌘G}} again to skip to the next occurrence.\n\nThe find buffer is system wide. That is, if you enter a text in the find panel (or with {{key press|⌘E}}) in one application and then switch to another application you can immediately start searching without having to enter the search text again.\n\n== Common keyboard shortcuts ==\n{| class="wikitable"\n|-\n! &nbsp;\n! Cut\n! Copy\n! Paste\n|-\n! Apple\n| Command+X\n| Command-C\n| Command-V\n|-\n! Windows/GNOME/KDE\n| Control-X / Shift-Delete\n| Control-C / Control-Insert\n| Control-V / Shift-Insert\n|-\n! GNOME/KDE terminal emulators\n| <!-- cut -->\n| Shift-Control-C / Control-Insert\n| Shift-Control-V / Shift-Control-Insert (Shift-Insert for pasting selected text)\n|-\n! BeOS\n| Alt-X\n| Alt-C\n| Alt-V\n|-\n! Common User Access\n| Shift+Delete\n| Control+Insert\n| Shift+Insert\n|-\n! Emacs\n| Control-W (to mark)<br />Control-K (to end of line)\n| [[Meta key|meta]]-W (to mark)\n| Control-Y\n|-\n! vi\n| d (delete)\n| y (yank)\n| p (put)\n|-\n! X Window System\n| <!-- cut -->\n| click-and-drag to highlight\n| middle mouse button\n|}\n\n== Copy and paste automation ==\nCopying data one by one from one application to another, such as from [[Microsoft Excel|Excel]] to a [[Form (HTML)|web form]], might involve a lot of manual work. Copy and paste can be automated with the help of a [[Computer program|program]] that would iterate through the values list and paste them to the active [[Window (computing)|application window]]. Such programs might come in the form of [[Macro (computer science)|macros]] or dedicated programs which involve more or less scripting. Alternatively, applications supporting [[simultaneous editing]] may be used to copy or move collections of items.\n\n== Additional differences between moving and copying ==<!-- This section is linked from [[Spreadsheet]] -->\nIn a spreadsheet, moving (cut and paste) need not equate to copying (copy and paste) and then deleting the original: when moving, references to the moved cells may move accordingly.\n\n[[Windows Explorer]] also differentiates moving from merely copy-and-delete: a "cut" file will not actually disappear until pasted elsewhere and cannot be pasted more than once. The icon fades to show the transient "cut" state until it is pasted somewhere. Cutting a second file while the first one is cut will release the first from the "cut" state and leave it unchanged. Shift+Delete cannot be used to cut files; instead it deletes them without using the Recycle bin.\n\n== Multiple clipboards ==\nSeveral editors allow copying text into or pasting text from specific clipboards, typically using a special keystroke-sequence to specify a particular clipboard-number.\n\n[[Clipboard manager]]s can be very convenient productivity-enhancers by providing many more features than system-native clipboards. Thousands of clips from the clip history are available for future pasting, and can be searched, edited, or deleted. Favorite clips that a user frequently pastes (for example, the current date, or the various fields of a user\'s contact info) can be kept standing ready to be pasted with a few clicks or keystrokes.\n\nSimilarly, a \'\'\'kill ring\'\'\' provides a [[LIFO (computing)|LIFO]] [[stack (data structure)|stack]] used for cut-and-paste operations as a type of clipboard capable of storing multiple pieces of data.<ref>{{cite web|url=http://www.ai.sri.com/~gkb/general.html#kill-ring |title=GKB (Generic Knowledge Base) Editor user\'s manual |work=[[Artificial Intelligence Center]] |publisher=[[SRI International]] |accessdate=2011-11-25}}</ref>\nFor example, the [[GNU Emacs]] text editor provides a kill ring.<ref>{{cite web|url=https://www.gnu.org/software/emacs/manual/html_mono/emacs.html#Kill-Ring |title=GNU Emacs manual |publisher=Gnu.org |date= |accessdate=2011-11-25}}</ref>\nEach time a user performs a cut or copy operation, the system adds the affected text to the ring. The user can then access the contents of a specific (relatively numbered) buffer in the ring when performing a subsequent paste-operation. One can also give kill-buffers individual names, thus providing another form of multiple-clipboard functionality.\n\n==Use in healthcare==\nConcerns have been raised over the use of copy and paste functions in healthcare documentation and [[electronic health records]]. There is potential for the introduction of [[medical error|errors]], [[information overload]], and [[fraud]].<ref name="Laubach" /><ref>{{cite web|url=http://library.ahima.org/xpedio/groups/public/documents/ahima/bok1_050621.pdf|title=Appropriate Use of the Copy and Paste Functionality in Electronic Health Records|date=March 17, 2014|publisher=[[American Health Information Management Association]]|accessdate=April 23, 2014}}</ref>\n\n==Use in software development==\n[[Copy and paste programming]] is an [[antipattern]] arising from the blind pasting of pre-existing code into another [[source code]] file.\n\n== See also ==\n* [[Clipboard (software)|Clipboard]]\n* [[Control key]]\n* [[Cut and paste job]]\n* [[Drag and drop]]\n* [[Photomontage]]\n* [[Publishing Interchange Language]]\n* [[Simultaneous editing]]\n* [[X Window selection]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://tronche.com/gui/x/icccm/sec-2.html 2. Peer-to-Peer Communication by Means of Selections] in the [[ICCCM]]\n\n[[Category:User interface techniques]]\n[[Category:Data management]]\n[[Category:Clipboard (computing)]]']
['Category:Information architecture', '52912241', '{{cat main}}\n\nConcepts, methodologies and topics related to the practice and theory of information architecture.\n\n[[Category:Data management]]\n[[Category:Enterprise architecture]]\n[[Category:Information architects]]\n[[Category:Information governance]]\n[[Category:Information science]]\n[[Category:Information technology management]]\n[[Category:Information technology]]\n[[Category:Records management]]\n[[Category:Technical communication]]']
['Document retrieval', '731640', '\'\'\'Document retrieval\'\'\' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.\n\nDocument retrieval is sometimes referred to as, or as a branch of, \'\'\'text retrieval\'\'\'. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.\n\n==Description==\nDocument retrieval systems find information to given criteria by matching text records (\'\'documents\'\') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.\n\nA document retrieval system has two main tasks:\n# Find relevant documents to user queries\n# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].\n\nInternet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.\n\n==Variations==\nThere are two main classes of indexing schemata for document retrieval systems: \'\'form based\'\' (or \'\'word based\'\'), and \'\'content based\'\' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.\n\n===Form based===\nForm based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.\n\n===Content based===\nThe content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.\n\nA \'\'signature file\'\' is a technique that creates a \'\'quick and dirty\'\' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and \'\'hopefully\'\' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.\n\n==Example: PubMed==\nThe [[PubMed]]<ref>{{cite journal |vauthors=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=319–23 |year=2001 |pmid=11825203 |pmc=2243528 }}\n</ref> form interface features the "related articles" search which works through a comparison of words from the documents\' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.<ref>{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}</ref><ref>{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}</ref>\n\n== See also ==\n\n* [[Compound term processing]]\n* [[Document classification]]\n* [[Enterprise search]]\n* [[Full text search]]\n* [[Information retrieval]]\n* [[Latent semantic indexing]]\n* [[Search engine]]\n\n== References ==\n\n<references/>\n\n==Further reading==\n* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems|volume=2|issue=4|year=1984|pages=267–288|doi=10.1145/2275.357411}}\n* {{cite journal|author1=Justin Zobel |author2=Alistair Moffat |author3=Kotagiri Ramamohanarao |title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems|volume=23|issue=4|year=1998|pages= 453–490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}\n* {{cite journal|author1=Ben Carterette |author2=Fazli Can |title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613–633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}\n\n== External links ==\n* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College\n\n[[Category:Information retrieval genres]]\n[[Category:Electronic documents]]\n[[Category:Substring indices]]\n[[Category:Search engine software]]\n\n[[zh:文本信息检索]]']
['Transaction document', '17788286', '\'\'\'Transaction documents\'\'\'  refers to legally relevant [[documents]] that are either printed, inserted and mailed, or electronically presented.<ref>[http://www.outputlinks.com/html/news/acadami_xplor_best_practices_progam_canada_043008.shtml Transaction documents]<blockquote>"...The course focuses on the concepts, technologies, and best practices associated with automated transaction document production."</blockquote></ref> They consist of a mixture of fixed and variable data. \n\nThese [[documents]] are usually created by organizations through their financial computing system and then delivered to other parties (such as clients) through the [[post office]] or through an [[electronic billing]] system. The printed transaction documents, once delivered to the [[post office]], conform to the [[mail box rule]]. \n\nCommon examples of transaction documents are:\n* bills\n* [[bank statements]] (and credit card, financial services, etc.)\n* insurance policies\n* notices\n* other legally relevant correspondence, etc.\n\n[[Xplor international]] is a technical association that focuses on the best practices and technologies associated with these documents.\n\n==References==\n{{Reflist}}\n\n[[Category:Electronic documents]]\n[[Category:Contract law]]\n\n\n{{law-stub}}']
['Digital object identifier', '422994', '{{Selfref|For the use of digital object identifiers on Wikipedia, see [[Wikipedia:Digital Object Identifier]].}}\n{{Use dmy dates|date=February 2011}}\n{{Infobox identifier\n| name          = Digital object identifier\n| image         = DOI logo.svg\n| image_size    = 130px\n| image_caption = \n| image_alt     = \n| image_border  = no\n| full_name     = \n| acronym       = DOI\n| number        = \n| start_date    = {{Start date|2000}}\n| organisation  = International DOI Foundation\n| digits        = \n| check_digit   = \n| example       = \n| website       = {{URL|doi.org}}\n}}\nIn computing, a \'\'\'Digital Object Identifier\'\'\' or \'\'\'DOI\'\'\' is a [[persistent identifier]] or [[handle (computing)|handle]] used to uniquely identify objects. An implementation of the [[Handle System]]<ref>{{cite web|url= http://handle.net/|title=The Handle System}}</ref><ref>{{cite web |url=http://www.doi.org/factsheets.html |title=Factsheets}}</ref> and standardized by the [[ISO]], DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they can be used to identify other objects, such as commercial videos.\n\nDOI means "digital identifier of an object" rather than "identifier of a digital object".<ref name = "iso">{{cite web | url = https://www.iso.org/obp/ui/#iso:std:iso:26324:ed-1:v1:en | title = ISO 26324:2012(en), Information and documentation — Digital object identifier system | publisher = [[ISO]] | date = | accessdate = 2016-04-20 | quote = DOI is an acronym for \'digital object identifier\', meaning a \'digital identifier of an object\' rather than an \'identifier of a digital object\'.}} "Introduction", paragraph 2.</ref> Thus \'\'DOI\'\' stands for "digital object-identifier" rather than "digital-object identifier".\n\n[[Metadata]] about the object is stored in association with the DOI name. It may include a location, such as a [[URL]], indicating where the object can be found. The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply using its URL, because if its URL changes, the publisher only needs to update the metadata for the DOI to link to the new URL.<ref>{{cite book|author= Witten, Ian H.|author2= David Bainbridge|author3= David M. Nichols|last-author-amp= yes |date= 2010|title= How to Build a Digital Library|edition= 2nd|location= Amsterdam; Boston|publisher= Morgan Kaufmann|pages= 352–253|isbn= 978-0-12-374857-7}}</ref><ref>{{Cite journal|first1= Marc|last1= Langston|first2= James|last2= Tyler|title= Linking to journal articles in an online teaching environment: The persistent link, DOI, and OpenURL|journal= The Internet and Higher Education|volume= 7|issue= 1|date= 2004|pages= 51–58|doi= 10.1016/j.iheduc.2003.11.004}}</ref><ref>{{Cite journal |url=http://www.bloomberg.com/bw/stories/2001-07-22/online-extra-how-the-digital-object-identifier-works |title= How the \'Digital Object Identifier\' works |date= 23 July 2001 |work= BusinessWeek |accessdate= 20 April 2010 |quote= Assuming the publishers do their job of maintaining the databases, these centralized references, unlike current Web links, should never become outdated or broken. |publisher= [[BusinessWeek]]}}</ref>\n\nA DOI name differs from standard identifier registries such as the [[ISBN]] and [[ISRC]]. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable.\n\nThe DOI system began in 2000 and is managed by the International DOI Foundation.<ref>{{citation|last= Paskin|first= Norman|chapter= Digital Object Identifier (DOI®) System|title= Encyclopedia of Library and Information Sciences|date= 2010|publisher= Taylor and Francis|pages= 1586–1592|edition= 3rd}}</ref> Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.<ref name="dd">{{Cite journal|title= Digital Object Identifiers: Promise and problems for scholarly publishing|first1= Lloyd A.|last1= Davidson|first2= Kimberly|last2= Douglas|date= December 1998|journal= Journal of Electronic Publishing|volume= 4|issue= 2|doi= 10.3998/3336451.0004.203}}</ref> The DOI system is implemented through a federation of registration agencies coordinated by the International DOI Foundation,<ref>{{cite web|url= https://doi.org/ |title= Welcome to the DOI System |publisher= Doi.org |date= 28 June 2010 |accessdate= 7 August 2010}}</ref> which developed and controls the system. The DOI system has been developed and implemented in a range of publishing applications since 2000; by late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations.<ref>{{Cite web|url= https://doi.org/news/DOINewsApr11.html#1 |title= DOI® News, April 2011: 1. DOI System exceeds 50 million assigned identifiers |publisher= Doi.org |date= 20 April 2011 |accessdate= 3 July 2011}}</ref> By April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.\n\n==Nomenclature==\nA DOI name takes the form of a [[character string]] divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the name, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal [[Unicode]] characters are allowed in these strings, which are interpreted in a [[case-insensitive]] manner. The prefix usually takes the form <code>10.NNNN</code>, where <code>NNNN</code> is a series of at least 4 numbers greater than or equal to <code>1000</code>, whose limit depends only on the total number of registrants.<ref name="CrossRefDOI">{{cite web |url=http://www.crossref.org/01company/15doi_info.html |access-date=10 June 2016 |title=doi info & guidelines |website=CrossRef.org |publisher=Publishers International Linking Association, Inc. |date=2013 |quote=All DOI prefixes begin with "10" to distinguish the DOI from other implementations of the Handle System followed by a four-digit number or string (the prefix can be longer if necessary).}}</ref><ref name="DOIKeyFacts">{{cite web |url=https://doi.org/factsheets/DOIKeyFacts.html |access-date=10 June 2016 |title=Factsheet—Key Facts on Digital Object Identifier System |website=doi.org |publisher=International DOI Foundation |date=June 6, 2016 |quote=Over 18,000 DOI name prefixes within the DOI System}}</ref> The prefix may be further subdivided with periods, like <code>10.NNNN.N</code>.<ref name="2.2.2">{{cite web |url=https://doi.org/doi_handbook/2_Numbering.html#2.2.2 |access-date=10 June 2016 |title=DOI Handbook—2 Numbering |website=doi.org |publisher=International DOI Foundation |date=February 1, 2016 |quote=The registrant code may be further divided into sub-elements for administrative convenience if desired. Each sub-element of the registrant code shall be preceded by a full stop.}}</ref>\n\nFor example, in the DOI name <code>10.1000/182</code>, the prefix is <code>10.1000</code> and the suffix is <code>182</code>. The "10." part of the prefix identifies the DOI registry,{{efn-ua|Other registries are identified by other strings at the start of the prefix. Handle names that begin with "100." are also in use, as for example in the following citation: {{cite journal|hdl=100.2/ADA013939 |url=http://handle.dtic.mil/100.2/ADA013939 |title=Development of a Transmission Error Model and an Error Control Model l |journal=<!-- --> |volume=<!-- --> | date=May 1975 |last1=Hammond |first1=Joseph L., Jr. |last2=Brown |first2=James E. |last3=Liu |first3=Shyan-Shiang S. |bibcode=1975STIN...7615344H|publisher=Rome Air Development Center|series=Technical Report RADC-TR-75-138}}}} and the characters <code>1000</code> in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. <code>182</code> is the suffix, or item ID, identifying a single object (in this case, the latest version of the \'\'DOI Handbook\'\').\n\nDOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, [[performance]]s, and abstract works<ref name=doifaq2>{{Cite journal |url=https://doi.org/faq.html#1 |title=Frequently asked questions about the DOI system: 2. What can be identified by a DOI name? | accessdate = 23 April 2010 | date = 17 February 2010|origyear=update of earlier version |publisher=International DOI Foundation}}\n</ref> such as licenses, parties to a transaction, etc.\n\nThe names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the [[indecs Content Model]].\n\n===Display===\nThe official \'\'DOI Handbook\'\' explicitly states that DOIs should display on screens and in print in the format "doi:10.1000/182".<ref name="C4WDefault-2811140">{{cite web |url=https://doi.org/doi_handbook/2_Numbering.html#2.6.1 |title=DOI Handbook – Numbering |date=13 February 2014 |accessdate=30 June 2014 |work=doi.org |author1=<!--Staff writer(s); no by-line.--> |archiveurl=https://web.archive.org/web/20140630181440/http://www.doi.org/doi_handbook/2_Numbering.html |archivedate=30 June 2014 |deadurl=no |at=Section 2.6.1 Screen and print presentation}}</ref> Contrary to the \'\'DOI Handbook\'\', [[CrossRef]], a major DOI registration agency, recommends displaying a URL (for example, <code><nowiki>https://doi.org/10.1000/182</nowiki></code>) instead of the officially specified format (for example, <code>[https://doi.org/10.1000/182 doi:10.1000/182]</code>)<ref>{{Cite web| title=DOI Display Guidelines|url=http://www.crossref.org/02publishers/doi_display_guidelines.html}}</ref><ref>{{Cite web| title=New Crossref DOI display guidelines are on the way|url=http://blog.crossref.org/2016/09/new-crossref-doi-display-guidelines.html}}</ref> This URL provides the location of an [[HTTP proxy]] server which will redirect web accesses to the correct online location of the linked item.<ref name="dd"/><ref>{{Cite journal | first=Andy |last=Powell |title=Resolving DOI Based URNs Using Squid: An Experimental System at UKOLN |journal = D-Lib Magazine|date=June 1998|url=http://www.dlib.org/dlib/june98/06powell.html| issn=1082-9873}}</ref> This recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.\n\n==Applications==\nMajor applications of the DOI system currently include:\n* [[Scientific literature|scholarly materials]] (journal articles, books, ebooks, etc.) through [[CrossRef]], a consortium of around 3,000 publishers;\n* research datasets through [[DataCite]], a consortium of leading research libraries, technical information providers, and scientific data centers;\n* [[European Union]] official publications through the [[Publications Office (European Union)|EU publications office]];\n* Permanent global identifiers for commercial video content through the Entertainment ID Registry, commonly known as [[EIDR]].\n\nIn the [[Organisation for Economic Co-operation and Development]]\'s publication service [[OECD iLibrary]], each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.<ref>{{cite journal|doi=10.1787/603233448430 |title=We Need Publishing Standards for Datasets and Data Tables |date=2009|journal=Research Information |last=Green|first=T.}}</ref>\n\nA multilingual European DOI registration agency activity, [http://www.mEDRA.org \'\'m\'\'EDRA], Traditional Chinese content thru [http://doi.airiti.com/ Airiti Inc.] and a Chinese registration agency, [http://www.wanfangdata.com/ Wanfang Data], are active in non-English language markets. Expansion to other sectors is planned by the International DOI Foundation.{{Citation needed|date=May 2010}}\n\n==Features and benefits==\nThe DOI system was designed to provide a form of [[Persistent identifier|persistent identification]], in which each DOI name permanently and unambiguously identifies the object to which it is associated. And, it associates [[metadata]] with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the [[Handle System]] and the [[indecs Content Model]] with a social infrastructure.\n\nThe Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the [[Uniform Resource Identifier|URI]] specification. The DOI name resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on [[open architecture]]s, incorporates [[Computational trust|trust mechanisms]], and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.<ref>{{cite web|url=http://arstechnica.com/science/2010/03/dois-and-their-discontents-1/|title=DOIs and their discontents|last=Timmer|first=John|date=6 March 2010|work=[[Ars Technica]]|accessdate=5 March 2013}}</ref> DOI name resolution may be used with [[OpenURL]] to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.<ref>{{Cite journal|first1=Susanne|last1=DeRisi|first2=Rebecca|last2=Kennison|first3=Nick|last3=Twyman|title=Editorial: The what and whys of DOIs|journal=[[PLoS Biology]]|volume=1|issue=2|page=e57|date=2003|doi=10.1371/journal.pbio.0000057|pmid=14624257|pmc=261894}} {{open access}}</ref> However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.<ref>{{Cite book|contribution=Open access to scientific and technical information: the state of the art|first=Jack|last=Franklin|title=Open access to scientific and technical information: state of the art and future trends|editor1-first=Herbert|editor1-last=Grüttemeier|editor2-first=Barry|editor2-last=Mahon|publisher=IOS Press|date=2003|page=74|url=https://books.google.com/?id=2X3gW1lUvN4C&pg=PA74#v=onepage&q|isbn=978-1-58603-377-4}}</ref>\n\nThe [[indecs Content Model]] is used within the DOI system to associate metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.\n\nThe International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as [[GS1]] and [[ISBN]].\n\n==Comparison with other identifier schemes==\nA DOI name differs from commonly used Internet pointers to material, such as the [[Uniform Resource Locator]] (URL), in that it identifies an object itself as a [[First class (computing)|first-class entity]], rather than the specific place where the object is located at a certain time. It implements the [[Uniform Resource Identifier]] ([[Uniform Resource Name]]) concept and adds to it a data model and social infrastructure.<ref>{{cite web|url=https://doi.org/factsheets/DOIIdentifierSpecs.html |title=DOI System and Internet Identifier Specifications |publisher=Doi.org |date=18 May 2010 |accessdate=7 August 2010}}</ref>\n\nA DOI name also differs from standard identifier registries such as the [[International Standard Book Number|ISBN]], [[International Standard Recording Code|ISRC]], etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.<ref>{{cite web|url=https://doi.org/factsheets/DOIIdentifiers.html |title=DOI System and standard identifier registries |publisher=Doi.org |accessdate=7 August 2010}}</ref>\n\nThe DOI system offers persistent, [[Semantic interoperability|semantically-interoperable]] resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn\'t mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include [[Persistent Uniform Resource Locator]] (PURL), URLs, [[Globally Unique Identifier]]s (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., [[Archival Resource Key|ARK]]).\n\nA DOI name does not depend on the object\'s location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.\n\n==Resolution==\nDOI name resolution is provided through the [[Handle System]], developed by [[Corporation for National Research Initiatives]], and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <code><type></code> field, which defines the syntax and semantics of its data.\n\nTo resolve a DOI name, it may be input to a DOI resolver (e.g. [https://doi.org/ doi.org]) or may be represented as an HTTP string by preceding the DOI name by the string <code><nowiki>https://doi.org/</nowiki></code> (preferred)<ref>{{cite web|author1=International DOI Foundation|title=Resolution|url=https://doi.org/doi_handbook/3_Resolution.html#3.7.3|website=DOI Handbook|accessdate=19 March 2015|date=2014-08-07}}</ref> or <code><nowiki>https://dx.doi.org/</nowiki></code>. For example, the DOI name <code>10.1000/182</code> can be resolved at the address "<nowiki>https://doi.org/10.1000/182</nowiki>". Web pages or other hypertext documents can include hypertext links in this form. Some browsers allow the direct resolution of a DOI (or other handles) with an add-on, e.g., [http://www.handle.net/hs-tools/extensions/firefox_hdlclient.html CNRI Handle Extension for Firefox]. The CNRI Handle Extension for Firefox enables the browser to access handle or DOI URIs like hdl:4263537/4000 or doi:10.1000/1 using the native Handle System protocol. It will even replace references to web-to-handle proxy servers with native resolution.\n\nAlternative DOI resolvers include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/ and http://doai.io. The last is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher\'s version.<ref>{{cite web|url=http://doai.io/|title=DOAI|publisher=CAPSH (Committee for the Accessibility of Publications in Sciences and Humanities)|accessdate=6 August 2016}}</ref><ref>{{Cite web| last = Schonfeld| first = Roger C.| title = Co-opting \'Official\' Channels through Infrastructures for Openness |work = The Scholarly Kitchen| accessdate = 2016-10-17| date = 2016-03-03| url = https://scholarlykitchen.sspnet.org/2016/03/03/coopting-official-channels/}}</ref>\n\n==Organizational structure==\nThe International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.<ref>{{cite book|url=https://doi.org/doi_handbook/7_IDF.html#7.5 |title=DOI Handbook |chapter=Chapter 7: The International DOI Foundation |publisher=Doi.org |accessdate=8 July 2015}}</ref> It safeguards all [[intellectual property|intellectual property rights]] relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.\n\nThe IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.\n\nRegistration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation.\n\nRegistration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a [[not-for-profit]] cost recovery basis.\n\n==Standardization==\nThe DOI system is an international standard developed by the [[International Organization for Standardization]] in its technical committee on identification and description, TC46/SC9.<ref>{{cite web|url=http://www.iso.org/iso/pressrelease.htm?refid=Ref1561 |title=Digital object identifier (DOI) becomes an ISO standard |publisher=iso.org |date=10 May 2012 |accessdate=10 May 2012}}</ref> The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,<ref>{{cite web|url=https://doi.org/about_the_doi.html#TC46 |title=about_the_doi.html DOI Standards and Specifications |publisher=Doi.org |date=28 June 2010 |accessdate=7 August 2010}}</ref> which was approved by 100% of those voting in a ballot closing on 15 November 2010.<ref>{{cite web|url=https://doi.org/about_the_doi.html#TC46 |title=Overviews & Standards – Standards and Specifications: 1. ISO TC46/SC9 Standards |publisher=Doi.org |date=18 November 2010 |accessdate=3 July 2011}}</ref> The final standard was published on 23 April 2012.<ref>{{cite web|url=http://www.iso.org/iso/catalogue_detail?csnumber=43506 |title=ISO 26324:2012 |publisher=iso.org |date=23 April 2012 |accessdate=10 May 2012}}</ref>\n\nDOI is a registered URI under the [[info URI scheme]] specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.<ref>{{cite web|url=http://info-uri.info/registry/docs/misc/faq.html#which_namespaces |title=About "info" URIs – Frequently Asked Questions |publisher=Info-uri.info |accessdate=7 August 2010}}</ref>\n\nThe DOI syntax is a [[NISO]] standard, first standardised in 2000, ANSI/NISO Z39.84{{hyphen}}2005 Syntax for the Digital Object Identifier.<ref>{{cite web|url=http://www.techstreet.com/standards/niso-z39-84-2005-r2010?product_id=1262088 |title=ANSI/NISO Z39.84{{hyphen}}2000 Syntax for the Digital Object Identifier |publisher=Techstreet.com |accessdate=7 August 2010}}</ref>\n\n==See also==\n{{columns-list|3|\n* [[Bibcode]]\n* [[Digital identity]]\n* [[Metadata standards]]\n* [[Object identifier]]\n* [[ORCID]]\n* [[PubMed#PubMed identifier|PMID]]\n* [[Publisher Item Identifier]] (PII)\n* [[Permalink]]\n* [[Scientific literature]]\n* [[Universally Unique Identifier]] (UUID)\n}}\n\n==Notes==\n{{notelist-ua}}\n\n==References==\n{{Reflist|colwidth=30em}}\n\n==External links==\n{{Wikidata property|P356}}\n* {{official website|https://doi.org/}}\n* [http://shortdoi.org Short DOI] – DOI Foundation service for converting long DOIs to shorter equivalents\n* [https://doi.org/factsheets/DOIIdentifierSpecs.html Factsheet: DOI System and Internet Identifier Specifications]\n* [http://search.crossref.org/ CrossRef DOI lookup]\n\n{{Audiovisual works|state=uncollapsed}}\n{{ISO standards}}\n{{Authority control}}\n\n{{DEFAULTSORT:Digital Object Identifier}}\n[[Category:Academic publishing]]\n[[Category:Electronic documents]]\n[[Category:Identifiers]]\n[[Category:Index (publishing)]]']
['DataCite', '26964279', '[[File:DataCite logo.png|thumb|DataCite\'s logo.]]\n\n\'\'\'DataCite\'\'\' is an international [[not-for-profit]] organization which aims to improve [[data citation]] in order to:\n*establish easier access to research data on the Internet\n*increase acceptance of research data as legitimate, citable contributions to the scholarly record\n*support data archiving that will permit results to be verified and re-purposed for future study.<ref>{{cite web|publisher=DataCite|title=What is DataCite?|url=http://www.datacite.org/whatisdatacite|accessdate=17 March 2014}}</ref> \n\n==Background==\nIn August 2009 a paper was published laying out an approach for a global registration agency for research data.<ref>{{cite web|title=Approach for a joint global registration agency for research data|doi=10.3233/ISU-2009-0595}}<!--| accessdate=2011-05-23--></ref> DataCite was subsequently founded in London on 1 December 2009<ref>{{cite journal|last1=Neumann|first1=Janna|last2=Brase|first2=Jan|title=DataCite and DOI names for research data|journal=Journal of Computer-Aided Molecular Design|date=20 July 2014|volume=28|issue=10|pages=1035–1041|doi=10.1007/s10822-014-9776-5}}</ref> by organisations from 6 countries: the [[British Library]]; the Technical Information Center of Denmark (DTIC); the [[TU Delft]] Library from the Netherlands; the National Research Council’s [[Canada Institute for Scientific and Technical Information]] (NRC-CISTI); the [[California Digital Library]] (University of California Curation Center);<ref>{{cite web|url=http://webarchives.cdlib.org/sw1st7gf68/http://www.universityofcalifornia.edu/news/article/23055 |title=University of California becomes founding member of Datacite |accessdate=2014-05-05 }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> [[Purdue University]] (USA);<ref>{{cite web |url= http://blogs.lib.purdue.edu/news/2010/02/16/purdue-libraries-a-founding-member-of-international-cooperative-to-advance-research/ | title=Purdue Libraries becomes founding member of Datacite | accessdate=2010-04-15}}</ref> and the German National Library of Science and Technology (TIB).<ref>{{cite web|url=http://www.tib-hannover.de/en/the-tib/news/news/id/133/ | title=TIB becomes founding member of Datacite | accessdate=2010-04-15}}</ref>\n\nAfter the founding of DataCite, leading research libraries and information centres converged for the first official members’ convention in Paris on 5 February 2010. The inclusion of five further members was approved in the office of the International Council for Science (ICSU): Australian National Data Service (ANDS);<ref>{{cite web|url=http://ands.org.au/guides/doi.html | title=ANDS joins Datacite | accessdate=2010-04-15}}</ref> Deutsche Zentralbibliothek für Medizin (ZB MED); GESIS - Leibniz-Institut für Sozialwissenschaften; French Institute for Scientific and Technical Information (INIST);<ref>{{cite web|url=http://www.inist.fr/spip.php?article66 |title=Inist join Datacite consortium |accessdate=2010-04-15 |deadurl=yes |archiveurl=https://web.archive.org/web/20100309010702/http://www.inist.fr:80/spip.php?article66 |archivedate=2010-03-09 |df= }}</ref> and Eidgenössische Technische Hochschule (ETH) Zürich.\n\n== Technical ==\n\nThe primary means of establishing easier access to research data is by DataCite members assigning persistent identifiers, such as [[digital object identifier]]s (DOIs), to data sets. Although currently leveraging the well-established DOI infrastructure, DataCite takes an open approach to identifiers, and considers other systems and services that help forward its objectives.<ref>{{cite web|publisher=DataCite |title=What do we do? |url=http://www.datacite.org/whatdowedo |accessdate=17 March 2014 |deadurl=yes |archiveurl=https://web.archive.org/web/20140317195125/http://www.datacite.org/whatdowedo |archivedate=17 March 2014 |df= }}</ref> \n\nDataCite\'s recommended format for a data citation is: \n*Creator (PublicationYear): Title. Publisher. Identifier\nOR\n*Creator (PublicationYear): Title. Version. Publisher. ResourceType. Identifier\n\nDataCite recommends that DOI names are displayed as linkable, permanent URLs.<ref>{{cite web|publisher=DataCite|title=Why cite data?|url=http://www.datacite.org/whycitedata|accessdate=17 March 2014}}</ref>\n\nThird-party tools allow the migration of content to and from other services such as ODIN, for [[ORCID]]<ref name="ODIN">{{cite web |url=http://odin-project.eu/2013/05/13/new-orcid-integrated-data-citation-tool/|title=New ORCID-integrated data citation tool |last=Thorisson |first=Gudmundur |date=2013-05-13 |publisher=ODIN Project |accessdate=7 May 2014}}</ref>\n\n==Members==\n* Australia:\n** [[Australian National Data Service]] - ANDS\n* Canada:\n** [[National Research Council (Canada)|National Research Council Canada]] - NRC-CNRC\n* China:\n** [[Beijing Genomics Institute]] - BGI \n* Denmark:\n** Technical Information Center of Denmark (DTU Library)\n* Estonia:\n** [[University of Tartu]] (/UT)\n* France:\n** [[Institut de l\'information scientifique et technique]] - INIST-CNRS\n* Germany:\n** [[German National Library of Economics]] - ZBW\n** [[German National Library of Medicine]] - ZB MED\n** [[German National Library of Science and Technology]] - TIB\n** Leibniz Institute for the Social Sciences - GESIS\n** Göttingen State and University Library - [[Göttingen State and University Library|SUB]]\n** Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen - GWDG \n* Hungary:\n** Library and Information Centre, Hungarian Academy of Sciences - MTA KIK\n* International:\n** [[World Data System|ICSU World Data System]] - ICSU-WDS \n* Italy:\n** [[Conference of Italian University Rectors]] - CRUI\n* Japan:\n** Japan Link Center - JaLC\n* Netherlands:\n** [[TU Delft]] Library\n* Norway:\n** [[BIBSYS]]\n* Republic of Korea:\n** [[Korea Institute of Science and Technology Information]] - KISTI \n* South Africa:\n** South African Environmental Observation Network - SAEON\n* Sweden:\n** Swedish National Data Service - SND\n* Switzerland:\n** [[CERN]] - European Organization for Nuclear Research\n** [[Swiss Federal Institute of Technology Zurich]] - ETH\n* Thailand:\n** National Research Council of Thailand - NRCT\n* United Kingdom:\n** [[The British Library]] - BL\n** [[Digital Curation Centre]] \n* United States:\n** [[California Digital Library]] - CDL\n** [[OSTI|Office of Scientific and Technical Information, US Department of Energy]] - OSTI\n** [[Purdue University|Purdue University Libraries]] - PUL\n** [[Inter-university Consortium for Political and Social Research]] - ICPSR \n** [[Harvard University Library]] \n** [[Institute of Electrical and Electronics Engineers]] - IEEE \n\n==References==\n<references />\n\n== External links ==\n* [http://www.datacite.org/ Official \'\'\'DataCite\'\'\' website]\n\n[[Category:Academic publishing]]\n[[Category:Data publishing]]\n[[Category:Electronic documents]]\n[[Category:Identifiers]]\n[[Category:Index (publishing)]]\n[[Category:Non-profit organizations]]\n[[Category:Non-profit technology]]']
['Apache Wave', '22992426', '{{Infobox software\n|name                       = Apache Wave\n|logo                       = Apache Wave logo.png\n|screenshot                 = Google Wave.png\n|caption                    = Google Wave, the previous incarnation of Apache Wave\n|collapsible                = \n|author                     = [[Google]]\n|developer                  = [[Apache Software Foundation]], Google\n|released                   = {{start date|2009|5|27}}\n|latest release version     =\n|latest release date        = <!-- {{Start date and age|YYYY|MM|DD}} -->\n|latest preview version     =\n|latest preview date        = <!-- {{Start date and age|YYYY|MM|DD}} -->\n|frequently updated         =\n|programming language       = [[Java (programming language)|Java]]<ref>{{cite web |url=http://www.lextrait.com/Vincent/implementations.html |title=The Programming Languages Beacon, v10.0 |first=Vincent |last =Lextrait |date=January 2010 |accessdate=14 March 2010}}</ref>\n|operating system           =\n|platform                   = [[Web application]]\n|size                       =\n|language                   =\n|status                     =\n|genre                      = [[Collaborative real-time editor]]\n|license                    = [[Apache License]]\n|website                    = {{URL|incubator.apache.org/wave/}}\n|repo                       = {{URL|https://git-wip-us.apache.org/repos/asf/incubator-wave.git}}\n}}\n\'\'\'Apache Wave\'\'\' is a software framework for [[Collaborative real-time editor|real-time collaborative editing]] online. [[Google]] originally developed it as \'\'\'Google Wave\'\'\'.<ref>{{cite web|url=http://wave.google.com/about.html\n|title=Google Wave Overview|author=Google Inc.\n|quote=[A] new web application for real-time communication and collaboration.\n|year=2009|accessdate=May 2010| archiveurl= https://web.archive.org/web/20100427183005/http://wave.google.com/about.html| archivedate= 27 April 2010 <!--DASHBot-->| deadurl= no}}</ref>\nIt was announced at the [[Google I/O]] conference on May 27, 2009.<ref>[[TechCrunch]] (May 28, 2009):\n[http://www.techcrunch.com/2009/05/28/google-wave-drips-with-ambition-can-it-fulfill-googles-grand-web-vision/ Google Wave Drips With Ambition.  A New Communication Platform For A New Web.]</ref><ref name="iokeynote">{{cite web\n|url=https://www.youtube.com/watch?v=v_UyVmITiYQ\n|title=I/O Conference Google Wave Keynote|author=Google Inc.}}</ref>\n\nWave is a [[web application|web-based]] [[computing platform]] and [[communications protocol]] designed to merge key features of [[Media (communication)|communications media]] such as [[email]], [[instant messaging]], [[wiki]]s, and [[Social networking service|social networking]].<ref name="aboutgw">{{cite web\n|url=http://wave.google.com/help/wave/about.html|title=About Google Wave|author=Google Inc.}}</ref> Communications using the system can be [[Synchronization|synchronous]] or [[Asynchronous communication#Electronically mediated communication|asynchronous]]. Software extensions provide contextual [[spell checker|spelling and grammar checking]], [[machine translation|automated language translation]]<ref name="iokeynote" /> and other features.<ref name="gwdevblog">{{cite web|url=http://googlewavedev.blogspot.com/2009/05/introducing-google-wave-apis-what-can.html|title=Google Wave Developer Blog|publisher=Google}}</ref>\n\nInitially released only to developers, a preview release of Google Wave was extended to 100,000 users in September 2009, each allowed to invite additional users. Google accepted most requests submitted starting November 29, 2009, soon after the September extended release of the technical preview. On May 19, 2010, it was released to the general public.<ref>Shankland, Stephen. (2010-05-19) [http://news.cnet.com/8301-30685_3-20005394-264.html Google Wave: Now open to the public | Deep Tech – CNET News]. News.cnet.com. Retrieved on 2010-12-14.</ref>\n\nOn August 4, 2010, Google announced the suspension of stand-alone Wave development and the intent of maintaining the web site at least for the remainder of the year,<ref>[http://googleblog.blogspot.com/2010/08/update-on-google-wave.html Official Google Blog: Update on Google Wave]. Googleblog.blogspot.com (2010-04-08). Retrieved on 2010-12-14.</ref> and on November 22, 2011, announced that existing Waves would become read-only in January 2012 and all Waves would be deleted in April 2012.<ref>{{cite web|url=http://googleblog.blogspot.com/2011/11/more-spring-cleaning-out-of-season.html |title=Official Blog: More spring cleaning out of season |publisher=Googleblog.blogspot.com |date=2011-11-22 |accessdate=2013-06-15}}</ref> Development was handed over to the [[Apache Software Foundation]] which started to develop a server-based product called \'\'\'Wave in a Box\'\'\'.<ref>Meyer, David. (2010-09-03) [http://www.zdnet.co.uk/news/application-development/2010/09/03/google-puts-open-source-wave-in-a-box-40089999/ Google puts open-source Wave in a \'box\' | Application Development | ZDNet UK]. Zdnet.co.uk. Retrieved on 2010-12-14.</ref><ref>[http://www.idg.se/2.1085/1.355483/google-wave-inte-ute-ur-leken Google Wave inte ute ur leken]. IDG.se. Retrieved on 2010-12-14.</ref><ref>Murphy, David. (1970-01-01) [http://www.pcmag.com/article2/0,2817,2368730,00.asp Google Spins Wave Into \'Wave in a Box\' for Third-Party Use | News & Opinion]. PCMag.com. Retrieved on 2010-12-14.</ref>\n\n==History==\n[[File:Googlewave.svg|thumb|upright|The original logo while owned by Google]]\n\n===Origin of name===\nThe science fiction television series \'\'[[Firefly (TV series)|Firefly]]\'\' provided the inspiration for the project\'s name.<ref name=itnewsau>{{cite news\n|first=Nate\n|last=Cochrane\n|url=http://www.itnews.com.au/News/104396,opinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx\n|title=Opinion: Google\'s wave drowns the bling in Microsoft\'s Bing\n|agency=IT News Australia\n|date=2009-05-29\n|accessdate=2009-06-03\n| archiveurl= https://web.archive.org/web/20090603041903/http://www.itnews.com.au/News/104396,opinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx| archivedate= 3 June 2009 <!--DASHBot-->| deadurl= no}}</ref> In the series, a \'\'wave\'\' is an electronic communication, often consisting of a [[video call]] or video message.<ref name=itnewsau/>  During the developer preview, a number of references were made to the series, such as [[Lars Rasmussen (Software Developer)|Lars Rasmussen]] replying to a message with "shiny", a word used in the series to mean \'\'cool\'\' or \'\'good\'\', and the crash message of Wave being a popular quotation from the series: "Curse your sudden but inevitable betrayal!"<ref name="iokeynote" /><ref>Originally said by [[List of characters in the Firefly universe#Hoban Washburne|Wash]] at 6:36, in \'\'[[Serenity (Firefly episode)|Serenity]]\'\'; [[Firefly (TV series)|Firefly]]: The Complete Series (Blu-ray), 2008, 20th Century Fox.</ref> Another common error message, "Everything\'s shiny, Cap\'n. Not to fret!" is a quote from [[Kaylee Frye]] in the 2005 motion-picture \'\'Firefly\'\' continuation, \'\'[[Serenity (film)|Serenity]]\'\', and it is matched with a sign declaring that "This wave is experiencing some turbulence and might explode. If you don\'t want to explode..." which is another reference to the opening of the film.\n\nDuring an event in [[Amsterdam]], [[Netherlands]],<ref name="nextweb">{{cite news | first = Ralf | last = Rottmann | url = http://thenextweb.com/appetite/2009/10/30/breaking-google-wave-opened-federation-today-host/ | title = Google Wave to be opened for federation today! | agency = The Next Web | date = October 30, 2009}}</ref> it became apparent that the 60-strong team that was currently working on Wave in [[Sydney, Australia]] use  [[Joss Whedon]]-related references to describe, among others, the sandbox version of Wave called \'\'[[Dollhouse (TV series)|Dollhouse]]\'\' after the TV-series by \'\'Firefly\'\' producer Joss Whedon, which was aired on Fox in the U.S. The development of external extensions is codenamed "Serenity", after the spaceship used in \'\'Firefly\'\' and \'\'Serenity\'\'.\n\n===Open source===\nGoogle released most of the source code as [[open source software]],<ref name="iokeynote"/> allowing the public to develop its features through extensions.<ref name="iokeynote" />  Google allowed third parties to build their own Wave services (be it private or commercial) because it wanted the [[Google Wave Federation Protocol|Wave protocol]] to replace the [[e-mail]] protocol.<ref name="iokeynote"/><ref name="gwarchitecture" /><ref name="wpcsmodel" /> Initially, Google was the only Wave service provider, but it was hoped that other service providers would launch their own Wave services, possibly designing their own unique web-based clients as is common with many email service providers.  The possibility also existed for native Wave clients to be made, as demonstrated with their [[command-line interface|CLI]]-based console client.<ref name="osreleasenext">{{cite web|url=https://groups.google.com/group/wave-protocol/browse_thread/thread/618ff4e9ef477e80?pli=1|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}</ref>\n\nGoogle released initial open-source components of Wave:<ref name="osrelease1">{{cite web|url=http://googlewavedev.blogspot.com/2009/07/google-wave-federation-protocol-and.html|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}</ref>\n# the [[operational transformation]] (OT) code,\n# the underlying wave model, and\n# a basic client/server prototype that uses the wave protocol\n\nIn addition, Google provided some detail about later phases of the open-source release:<ref name="osreleasenext" />\n# wave model code that is a simplified version of Google\'s production code and is tied to the OT code; this code will evolve into the shared code base that Google will use and expects that others will too\n# a testing and verification suite for people who want to do their own implementation (for example, for porting the code to other languages)\n\n===Reception===\n{{Wikinews|Google to discontinue social networking application Google Wave}}\nDuring the initial launch of Google Wave, invitations were widely sought by users and were sold on auction sites.<ref>[http://mashable.com/2009/09/30/google-wave-invite/ Google Wave Invite Selling for $70 on eBay]</ref><!--  However, people were confused as to how to use it.<ref>[http://www.csmonitor.com/Innovation/Horizons/2009/1124/so-youve-got-google-wave-now-what Christian Science Monitor: So you\'ve got Google wave, now what?]</ref> Google Wave was called an "over-hyped disappointment for the first generation of users"<ref>[http://www.linux-mag.com/id/7653 Linux Mag 2009 Review]</ref> with "dismal usability"<ref>[http://themilwaukeeseo.com/2009/12/14/the-google-wave-failure/ Google Wave Failure on Milwaukee SEO]</ref> that "humans may not be able to comprehend."<ref name="EngagetWave">[http://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]</ref> -->\nThose who received invitations and decided to test Google Wave could not communicate with their contacts on their regular email accounts. The initial spread of Wave was very restricted.\n\n===End of development of \'\'Google Wave\'\'===\nGoogle Wave initially received positive press coverage for its design<ref>[http://news.bbc.co.uk/1/hi/technology/8282687.stm B.B.C. report introducing Google Wave in September 2009]</ref> and potential uses.<ref name="EngagetWave">[http://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]</ref><ref>[http://asia.cnet.com/blogs/tokyo-shift/post.htm?id=63015591 CNET Predictions for 2010]</ref> On August 4, 2010, Google announced Wave would no longer be developed as a stand-alone product due to a lack of interest.<ref name="ZDNet on GW\'s death">[http://www.zdnet.com/blog/google/how-will-google-wave-be-reincarnated/2344 ZDNet on GW\'s death]</ref> Google\'s statement surprised many in the industry and user community.\n\nGoogle later clarified the Wave service would be available until [[Google Docs]] was capable of accessing saved waves.<ref>{{cite web|url=http://www.google.com/support/wave/bin/answer.py?answer=1083134 |title=Status of Google Wave - Google Help |publisher=Google.com |date= |accessdate=2013-06-15}}</ref>\n\nResponse to the news of the end of development came from Wave users in the form of a website.<ref>[http://www.webpronews.com/topnews/2010/08/09/save-google-wave-site-forms \'"Save Google Wave" Site Forms\']</ref> Since their announcement in early August, the website has recorded over 49,000 supporter registrations urging Google Wave\'s continuation.<ref>[http://www.savegooglewave.com Save Google Wave!]. Retrieved on 2011-05-14.</ref>\n\nIn retrospect, the lack of success of Google Wave was attributed among other things to its complicated user interface resulting in a product that merged features of email, instant messengers and wikis but ultimately failed to do anything significantly better than the existing solutions.<ref>[http://arstechnica.com/software/news/2010/08/google-wave-why-we-didnt-use-it.ars Google Wave: why we didn\'t use it], [[Ars Technica]]</ref>\n\nChris Dawson of online technology magazine [[Zdnet]] discussed inconsistencies in the reasoning of Google in deciding to end support for Wave,<ref name="ZDNet on GW\'s death"/> mentioning its "deep involvement" in developing social media networks, to which many of Wave\'s capabilities are ideally suited. Perhaps Google Wave was ended to clear the stage for their new social network [[Google+]] that tried to compete with Facebook but has not gained the reach as a similar comprehensive social networking site.<ref>[http://www.utalkmarketing.com/pages/Article.aspx?ArticleID=21768&Title=Can_Google+_really_challenge_Facebook_and_be_an_asset_to_brands "Can Google+ really challenge Facebook and be an asset to brands?" utalkmarketing.com]</ref>\n\n===Apache Wave===\nGoogle Wave was accepted by the [[Apache Software Foundation]]\'s Incubator program under the project name Apache Wave. The Google Wave Developer blog was updated with news of the change on December 6, 2010.<ref>North, Alex. (2010-12-06) [http://googlewavedev.blogspot.com/2010/12/introducing-apache-wave.html Google Wave Developer Blog: Introducing Apache Wave]. Googlewavedev.blogspot.com. Retrieved on 2010-12-14.</ref> A Wave Proposal page with details on the project\'s goals was created on the Apache Foundation\'s Incubator Wiki.<ref>[http://wiki.apache.org/incubator/WaveProposal WaveProposal – Incubator Wiki]. Wiki.apache.org (2010-11-24). Retrieved on 2010-12-14.</ref>\n\n====Wave in a Box====\n[[File:Wave in a Box logo.png|thumb|75px|The logo for Wave in a Box]]\nWave in a Box is the current server implementation of Apache Wave.  Currently, there are not any demo servers available.<ref name=demo_servers>{{cite web|title=Wave in a Box demo servers|url=http://incubator.apache.org/wave/demo-servers.html|publisher=Apache Software Foundation|accessdate=10 October 2012}}</ref>\n\n==Features==\nGoogle Wave was a new [[Internet]] [[communications]] platform. It was written in [[Java (programming language)|Java]] using [[OpenJDK]] and its web interface used the [[Google Web Toolkit]]. Google Wave works like previous messaging systems such as [[email]] and [[Usenet]], but instead of sending a message along with its entire thread of previous messages, or requiring all responses to be stored in each user\'s inbox for context, message documents (referred to as \'\'waves\'\') that contain complete threads of multimedia messages (blips) are perpetually stored on a central server. Waves are shared with collaborators who can be added or removed from the wave at any point during a wave\'s existence.\n\nWaves, described by Google as "\'\'equal parts conversation and document\'\'", are hosted [[XML]] documents that allow seamless and low latency concurrent modifications.<ref name="ot">[http://www.waveprotocol.org/whitepapers/operational-transform Google Wave Operational Transformation – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.</ref> Any participant of a wave can reply anywhere within the message, edit any part of the wave, and add participants at any point in the process. Each edit/reply is a blip and users can reply to individual blips within waves. Recipients are notified of changes/replies in all waves in which they are active and, upon opening a wave, may review those changes in chronological order. In addition, waves are live. All replies/edits are visible in real-time, letter-by-letter, as they are typed by the other collaborators. Multiple participants may edit a single wave simultaneously in Google Wave. Thus, waves can function not only as e-mails and [[threaded discussion|threaded conversations]] but also as an [[instant messaging]] service when many participants are online at the same time. A wave may repeatedly shift roles between e-mail and instant messaging depending on the number of users editing it concurrently. The ability to show messages as they are typed can be disabled, similar to conventional instant messaging.<ref name="aboutgw"/>\n\nThe ability to modify a wave at any location lets users create collaborative documents, [[collaborative editing|edited]] in a manner akin to [[wiki]]s. Waves can easily link to other waves. In many respects, it is a more advanced forum.<ref>[http://variableghz.com/2009/10/google-wave-review/ Google Wave Review]. VariableGHz (2009-10-13). Retrieved on 2010-12-14.</ref> It can be read and known to exist by only one person, or by two or more and can also be public, available for reading \'\'and\'\' writing to everyone on the Wave.\n\nThe history of each wave is stored within it. Collaborators may use a playback feature to observe the order in which it was edited, blips that were added, and who was responsible for what in the wave.<ref name="aboutgw"/><ref name="gwdevblog"/> The history may also be searched by a user to view and/or modify specific changes, such as specific kinds of changes or messages from a single user.<ref name="iokeynote" />\n\n==Extension programming interface==\n{{anchor|Google Wave extensions}}\nGoogle Wave is extensible through an [[application programming interface]] (API). It provides extensions in the form of \'\'Gadgets\'\' and \'\'Robots\'\', and is embeddable by dropping interactive windows into a given wave on external sites, such as [[blog]] sites.<ref name="iokeynote" /><ref name="codepage" />\n\nThe last version of robots API is 2.0.<ref name="robotsapiv2">{{cite web|url=http://googlewavedev.blogspot.com/2010/03/introducing-robots-api-v2-rise-of.html|title=Introducing Robots API v2: The Rise of Active Robots|publisher=Google}}</ref>\n\nGoogle Wave also supports extension installers, which bundle back-end elements (robots and gadgets) and front-end user interface elements into an integrated package. Users may install extensions directly within the Wave client using an extension installer.\n\n===Extensions===\nGoogle Wave extensions are [[Plug-in (computing)|add-ins]] that may be installed on Google Wave to enhance its functionality. They may be [[Internet bot]]s (robots) to automate common tasks, or gadgets to extend or change user interaction features, e.g., posting blips on [[microblog]] feeds or providing RSVP recording mechanisms.<ref name="iokeynote" /><ref name="aboutgw" /><ref name="codepage">{{cite web|url=https://code.google.com/apis/wave/|title=Google Wave API – Google Code|publisher=Google}}</ref>\n\nOver 150 Google Wave extensions have been developed either in the form of Gadgets or Robots.<ref>[http://wave-samples-gallery.appspot.com/ Google Wave Samples Gallery]. Wave-samples-gallery.appspot.com. Retrieved on 2010-12-14.</ref>\n\n====Robots====\nA robot is an automated participant on a wave. It can read the contents of a wave in which it participates, modify its contents, add or remove participants, and create new blips or new waves. Robots perform actions in response to events. For example, a robot might publish the contents of a wave to a public [[blog]] site and update the wave with user comments.\n\nRobots may be added as participants to the Wave itself. In theory, a robot can be added anywhere a human participant can be involved.\n\n====Gadgets====\nGadget extensions are applications that run within the wave, and to which all participants have access. Robots and Gadgets can be used together, but they generally serve different purposes. A gadget is an application users could participate with, many of which are built on Google’s [[OpenSocial]] platform. A good comparison would be iGoogle gadgets or Facebook applications.\n\nThe gadget is triggered based on the user action. They can be best described as applications installed on a mobile phone. For example, a wave might include a [[sudoku]] gadget that lets the wave participants compete to see who can solve the puzzle first.\n\nGadgets may be added to individual waves and all the participants share and interact with the gadget.\n\n==Federation protocol==\n{{Main article|Google Wave Federation Protocol}}\nGoogle Wave provides [[Federation (information technology)|federation]] using an extension of [[XMPP|Extensible Messaging and Presence Protocol]] (XMPP), the [[open standard|open]] [[Google Wave Federation Protocol|Wave Federation Protocol]]. Being an open protocol, anyone can use it to build a custom Wave system and become a wave provider.<ref>{{cite web|url=http://www.waveprotocol.org/|title=Google Wave Federation Protocol|publisher=Google}}</ref>  The use of an open protocol is intended to parallel the openness and ease of adoption of the [[e-mail]] protocol and, like e-mail, allow communication regardless of provider. Google hoped that waves would replace e-mail as the dominant form of Internet communication.<ref name="iokeynote"/><ref name="gwarchitecture">[http://www.waveprotocol.org/whitepapers/google-wave-architecture Google Wave Federation Architecture – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.</ref><ref name="wpcsmodel">[http://www.waveprotocol.org/whitepapers/internal-client-server-protocol Google Wave Client-Server Protocol – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.</ref>  In this way, Google intended to be only one of many wave providers<ref name="iokeynote"/><ref name="gwarchitecture" /><ref name="wpcsmodel" />  and to also be used as a supplement to [[e-mail]], [[instant messaging]], [[FTP]], etc.\n\nA key feature of the protocol is that waves are stored on the service provider\'s servers instead of being sent between users. Waves are federated; copies of waves and wavelets are distributed by the wave provider of the originating user to the providers of all other participants in a particular wave or wavelet so all participants have immediate access to up-to-date content. The originating wave server is responsible for hosting, processing, and concurrency control of waves.<ref name="gwarchitecture" /><ref name="wpcsmodel" />  The protocol allows private reply wavelets within parent waves, where other participants have no access or knowledge of them.<ref name="gwarchitecture" /><ref name="wpcsmodel" />\n\nSecurity for the communications is provided via [[Transport Layer Security]] authentication, and encrypted connections and waves/wavelets are identified uniquely by a service provider\'s [[domain name]] and ID strings. User-data is not federated, that is, not shared with other wave providers.\n\n===Adoption of Wave Protocol and Wave Federation Protocol===\nBesides Apache Wave itself, there are other open-source variants of servers and clients with different percentage of Wave Federation and Wave Protocol support. Wave has been adopted for corporate applications by Novell for [[Novell Pulse]],<ref>[http://www.novell.com/products/pulse/ Novell Vibe cloud service]. Novell.com. Retrieved on 2010-12-14.</ref> or by [[SAP AG|SAP]] for Cloudave,<ref>Elliott, Timo. (2009-10-19) [http://www.cloudave.com/link/sap-gravity-prototype-business-collaboration-using-google-wave SAP\'s Gravity Prototype: Business Collaboration Using Google Wave]. Cloudave.com. Retrieved on 2010-12-14.</ref> and community projects such as PyOfWave or [[Kune (software)|Kune]].\n\n====Compatible third-party servers====\nThe following servers are compatible with the Google Wave protocol:\n* \'\'\'[[Kune (software)|Kune]]\'\'\'<ref>{{cite web|title=Kune Homepage|url=http://kune.ourproject.org|accessdate=22 April 2012}}</ref> is a free/open source platform for social networking, collaborative work and web publishing, focusing on work groups and organizations rather than in individuals. It provides lists, tasks, documents, galleries, etc., while using waves underneath. It focuses on [[free culture movement|free culture]] and [[social movements]] needs.\n* \'\'\'[[Novell Vibe]]\'\'\', formerly known as Novell Pulse<ref>[http://www.novell.com/promo/vibe.html Novell Vibe]. Novell.com (2009-12-31). Retrieved on 2010-12-14.</ref>\n* \'\'\'Rizzoma\'\'\'<ref>{{cite web|title=Rizzoma Homepage|url=http://rizzoma.com|accessdate=9 May 2012}}</ref> is a platform for collaborative work in real time. It allows communication within a certain context permitting a chat to instantly become a document where topics of a discussion organized into branches of mind-map diagram and minor details are collapsed to avoid distraction. The user is able to sign in using a Google or Facebook account and choose whether your topics are private or public.\n* \'\'\'[[SAP StreamWork]]\'\'\' is a collaboration decision making service.<ref>Williams, Alex. (2010-05-17) [http://www.readwriteweb.com/cloud/2010/05/sap-streamworks-integrates-wit.php SAP StreamWork Integrates With Google Wave – ReadWriteCloud]. Readwriteweb.com. Retrieved on 2010-12-14.</ref><ref>[http://www.sapstreamwork.com/how-it-works/ How It Works | SAP® StreamWork™]. Sapstreamwork.com. Retrieved on 2010-12-14.</ref>\n\n==See also==\n{{Portal|Software}}\n* [[Microsoft Sharepoint Workspace]]\n* [[Real-time text]]\n* [[Opera Unite]]\n\n==References==\n{{Reflist|colwidth=30em}}\n\n==External links==\n{{Commons category|Google Wave}}\n* [http://incubator.apache.org/wave/ Apache Wave]\n* [http://incubator.apache.org/wave/demo-servers.html Wave in a Box]\n* [http://wave.google.com/ Google Wave]\n* [https://code.google.com/apis/wave/ Google Wave API]\n* [http://googlewavedev.blogspot.com/ Google Wave Developer Blog]\n* [https://www.youtube.com/watch?v=v_UyVmITiYQ Full Video of the Developer Preview at Google IO on ]\n* [https://www.youtube.com/watch?v=p6pgxLaDdQw Google Wave overview video]\n* [http://www.waveprotocol.org/ Google Wave Federation Protocol]\n\n{{Google Inc.}}\n{{Apache}}\n\n[[Category:Discontinued Google services|Wave]]\n[[Category:Web applications]]\n[[Category:Computing platforms]]\n[[Category:Electronic documents]]\n[[Category:Instant messaging]]\n[[Category:Online chat]]\n[[Category:Social information processing]]\n[[Category:Groupware]]\n[[Category:Wikis]]\n[[Category:Internet protocols]]\n[[Category:Internet Protocol based network software]]\n[[Category:Self-organization]]\n[[Category:Blogging]]\n[[Category:Collaborative real-time editors]]\n[[Category:2009 software]]\n[[Category:2010 disestablishments]]\n[[Category:Discontinued software]]\n[[Category:Discontinued Google software]]\n[[Category:Software using the Apache license]]\n[[Category:Social networking services]]\n[[Category:Apache Software Foundation|Wave]]']
['Kune (software)', '32895691', '{{Infobox software\n|name                       = Kune\n|logo                       = [[File:Kune-logo.svg|frameless|upright]]\n|screenshot                 = [[File:Concurrent-edit-and-chat.png|frameless|center]]\n|caption                    =\n|collapsible                = yes\n|author                     = [[Comunes Collective]]\n|developer                  = [[Comunes Collective]], IEPALA Foundation\n|released                   = {{start date and age|2007}} \n|discontinued               = \n|latest release version     = 1.0.0 (Codename "free-riders")<ref name=release1.0.0>{{cite web|title=Released Kune Version 1.0.0 Codename "free-riders"|url=http://kune.ourproject.org/2015/03/released-kune-version-1-0-0-codename-free-riders/|accessdate=2015-06-23|date=2015-03-18|website=Kune Blog}}</ref>\n|latest release date        = {{release date and age|2015|3|18}} \n|latest preview version     =\n|latest preview date        = <!-- {{Start date and age|YYYY|MM|DD}} -->\n|frequently updated         =\n|programming language       = Java-based [[Google Web Toolkit]]\n|operating system           = \n|platform                   = [[Cross-platform]]\n|size                       =\n|language                   = Multi-language (more than 10)\n|status                     = Active (as of 2015-05)\n|genre                      = [[Web application]] [[Collaborative software]] [[Distributed social network]]\n|license                    = [[Affero General Public License|AGPLv3]] \n|website                    = {{URL|http://kune.ourproject.org/}} {{URL|https://kune.cc/}}\n}}\n\n\'\'\'Kune\'\'\' is a [[free software|free/open source]] distributed social network focused on collaboration rather than just on communication.<ref name="kune.op.org">{{cite web|title=Kune development site|url=http://kune.ourproject.org|accessdate=3 February 2011}}</ref> That is, it focuses on online [[Collaborative real-time editor|real-time collaborative editing]], [[Distributed social network|decentralized social networking]] and web publishing, while focusing on workgroups rather than just on individuals.<ref>{{cite news|title=Presentando el proyecto Kune, redes sociales y colaboración libre para grupos|url=http://barrapunto.com/article.pl?sid=11/08/21/2240235|language= Spanish|accessdate=28 August 2011|newspaper=Barrapunto (Spanish Slashdot)|date=22 August 2011}}</ref><ref>{{cite news|title=Presentando el proyecto Kune, redes sociales y colaboración libre para grupos|url=http://www.meneame.net/story/presentando-proyecto-kune-redes-sociales-colaboracion-libre|language= Spanish|accessdate=28 August 2011|newspaper=Menéame (Spanish Digg)|date=23 August 2011}}</ref> It aims to allow for the creation of online spaces for collaborative work where organizations and individuals can build projects online, coordinate common agendas, set up virtual meetings, publish on the web, and join organizations with similar interests. It has a special focus on [[Free culture movement|Free Culture]] and [[social movements]] needs.<ref>{{cite web|title=Kune FAQ|url=http://kune.ourproject.org/faq|accessdate=7 July 2012}}</ref><ref>{{cite news\n| title       = Das neue Internet\n| first       = Niels\n| last        = Boeing\n| authorlink  = \n| url         = http://www.zeit.de/zeit-wissen/2012/05/Das-alternative-Netz/komplettansicht\n| format      = \n| agency      = \n| newspaper   = [[Die Zeit]]\n| publisher   = \n| location    = Germany\n| isbn        = \n| issn        = \n| oclc        = \n| pmid        = \n| pmd         = \n| bibcode     = \n| doi         = \n| id          = \n| date        = 31 August 2012\n| page        = \n| pages       = \n| at          = \n| accessdate  = 5 September 2012\n| language    = German\n| trans_title = The new internet\n| quote       = \n| archiveurl  = \n| archivedate =\n| deadurl     =\n| ref         = \n}}</ref> Kune is a project of the [[Comunes Collective]].\n\n== Technical details ==\nKune is programmed using the [[Java (programming language)|Java]]-based [[Google Web Toolkit|GWT]] in the client-side, integrating [[Apache Wave]] (formerly [[Google Wave]]) and using mainly the open protocols [[XMPP]] and [[Wave Federation Protocol]]. GWT Java sources on the client side generates [[Code obfuscation|obfuscated]] and deeply optimized [[JavaScript]] conforming a [[single page application]]. Wave extensions (gadgets, bots) run on top of Kune (as in [[Facebook apps]]) and can be programmed in Java+GWT, JavaScript or Python.\n\nThe current version has been under development since 2007,<ref name="video2008">{{cite video |people= |date= 26 January 2008|title= Video: Status of Kune development (Jan 2008)|url=http://kune.ourproject.org/2008/01/status-jan08/|format= AVI |medium= |trans_title= |publisher= |location= |archiveurl= |archivedate= |accessdate=28 August 2011|time= |id= |isbn= |oclc= |quote= |ref= }}</ref> with a constant, stable growth and an established codebase.<ref>{{cite web|title=Kune project in Ohloh|url=http://www.ohloh.net/p/kune|author=[[Ohloh]]|accessdate=28 August 2011}}</ref> Nowadays the code is hosted in the GIT of [[Gitorious]],<ref>{{Cite web\n|title=Kune repository in Gitorious\n|url=https://gitorious.org/kune\n| accessdate = 2 September 2012\n| author = \n| last = \n| first = \n| authorlink = \n| work = \n| publisher = [[Gitorious]]\n| pages = \n| format = \n| quote = \n| archiveurl = \n| archivedate = \n}}</ref> it has a development site<ref name="kune.op.org" /> and its main node<ref>{{Cite web\n|title=Kune node "Kune.cc"\n|url=http://kune.cc\n| accessdate = 5 September 2012\n| author = \n| last = \n| first = \n| authorlink = \n| work = \n| publisher = Maintained by [[Comunes Collective]]\n| pages = \n| format = \n| quote = \n| archiveurl = \n| archivedate = \n}}</ref> maintained by the [[Comunes Collective]].\n\nKune is 100% free software and was built only using free software. Its software is licensed under the [[Affero GPL]] license while the art is under a [[Creative Commons]] BY-SA.\n\n== Philosophy ==\n\nKune was born in order to face a growing concern from the community behind it. Nowadays, groups (a group of friends, activists, a NGO, a small start-up) that need to work together typically will use multiple [[Free like beer|free (like beer)]] commercial centralized for-profit services (e.g. [[Google Docs]], [[Google Groups]], [[Facebook]], [[Wordpress.com]], [[Dropbox (service)|Dropbox]], [[Flickr]], [[eBay]] ...) in order to communicate and collaborate online. However, "If you\'re not paying for it, you\'re the product".<ref>{{cite news|title=If You’re Not Paying for It; You’re the Product|url=http://lifehacker.com/5697167/if-youre-not-paying-for-it-youre-the-product|accessdate=7 July 2012|newspaper=Lifehacker|date=23 November 2010}}</ref> In order to avoid that, such groups of users may ask a technical expert to build them mailing lists, a webpage and maybe to set up an [[etherpad]]. However, technicians are needed for any new list (as they cannot configure e.g. [[GNU Mailman]]), configuration change, etc., creating a strong dependency and ultimately a bottle-neck.<ref>{{cite web|title=Kune 0.0.9 published (codename "15M")|url=http://kune.ourproject.org/2011/08/kune-0-0-9-published-codename-15m/|accessdate=12 April 2012|publisher = Kune Blog|date=4 August 2011}}</ref>\n\nKune aims to cover all those needs of groups to communicate and collaborate, in an usable way and thus without depending on technical experts\n.<ref>{{cite news|title=Software libre, hardware libre, ¿servicios libres?|url=http://libertonia.escomposlinux.org/story/2009/5/27/12014/3120|accessdate=3 February 2011|newspaper=Libertonia News|date=27 May 2009}}</ref> It aims to be a free/libre web service (and thus in [[Internet|the cloud]]), but decentralized as email so a user can choose the server they want and still interoperate transparently with the rest.\n\nOpposite to most distributed social networks, this software focuses on collaboration and building, not only on communication and sharing. Thus, Kune does not aim to ultimately replace Facebook, but also all the above-mentioned commercial services. Kune has a strong focus on the construction of [[Free culture movement|Free Culture]] and eventually facilitate [[Commons-based peer production]].<ref>\n{{Cite book\n| publisher = IOS Press\n| isbn = 9781614990642\n|last1= Mass Araya|first1= Elizabeth Roxana |last2= Borsetti Gregorio Vidotti|first2= Silvana Aparecida\n|editor1-first=  Ana Alice|editor1-last=Baptista\n|editor2-first= Peter|editor2-last= Linde\n|editor3-first= Niklas|editor3-last= Lavesson\n|editor4-first=Miguel |display-editors = 3 |editor4-last=  Abrunhosa de Brito\n| title = Social Shaping of Digital Publishing: Exploring the Interplay Between Culture and Technology\n|url= http://www.booksonline.iospress.nl/Content/View.aspx?piid=30613\n|chapter= Creative Commons: a Convergence Model Between the Ideal of Commons and the Possibilities of Creation in Contemporary TimesOpposed to Copyright Impediments\n| date = 15 July 2012\n|accessdate= 19 August 2012\n|pages= 3–11\n}}</ref>\n\n== History ==\n{| class="wikitable" style="float:right; text-align:center; margin-left:1em; margin-right:0"\n|-\n! rowspan=1 | Version\n! rowspan=1 | Code name\n! rowspan=1 | Release date\n|-\n| 0.0.1\n| --\n| colspan="2" {{Version |o | 2007}}\n|-\n| 0.0.9\n| [[15-M Movement|15M]]\n| colspan="2" {{Version |o | 2011-08-04}}\n|-\n| 0.1.0\n| [[We are the 99%|99%]]<ref>{{cite news|title=Kune new release "99%" & production site|url=https://tech.occupy.net/2012/04/24/kune-new-release-99-production-site/|accessdate=9 June 2012|date=24 April 2012|newspaper= #Occupy Tech News}}</ref>\n| colspan="2" {{Version |o| 2012-04-13}}\n|-\n| 0.2.0\n| [[Elinor Ostrom|Ostrom]]<ref name=releaseOstrom>{{cite news|title=New release of collaborative distributed social network Kune: "Ostrom"|url=https://tech.occupy.net/2012/11/26/new-release-of-collaborative-distributed-social-network-kune-ostrom/|accessdate=26 November 2012|date=26 November 2012|newspaper= #Occupy Tech News}}</ref>\n| colspan="2" {{Version |o | 2012-10-22}}\n|-\n| 1.0.0\n| "Free-riders"<ref name=release1.0.0 />\n| colspan="2" {{Version |c | 2015-03-18}}\n\n|-\n| colspan="99" | <small>{{Version |l |show=011101}}</small>\n|}\n\nThe origin of Kune relies on the community behind [[Ourproject.org]]. Ourproject<ref>{{cite news|title=There\'s Life after Microsoft - Free Software Advocates|url=http://www.ipsnews.net/interna.asp?idnews=22073|accessdate=3 February 2011|newspaper=Inter Press Service News Agency|date=24 January 2004}}</ref> aimed to provide for [[Free culture movement|Free Culture]] (social/cultural projects) what [[Sourceforge]] and other [[software forge]]s meant for [[free software]]: a collection of communication and collaboration tools that would boost the emergence of community-driven free projects.<ref>{{Cite book\n| last = Camino\n| first = S.\n|author2=F. Javier |author3=M. Jiménez Gañán |author4=S. Frutos Cid\n | chapter = Collaborative Development within Open Source Communities\n| title =Encyclopedia of Networked and Virtual Organizations\n|publisher= IGI Global, Information Science Reference\n|isbn = 978-1-59904-885-7\n| year = 2008\n}}</ref> However, although Ourproject was relatively successful, it was far from the original aims. The analysis of the situation in 2005<ref>{{cite press release\n | title = Towards a new manager of free projects (Hacia un nuevo gestor de proyectos libres)\n | publisher = [[Ourproject.org]]\n | date = 6 December 2005\n | url = http://ourproject.org/moin/Hacia_un_nuevo_gestor_de_Proyectos_Libres\n | accessdate = 22 April 2012\n}}</ref> concluded that only the groups that had a [[geek|techie]] among them (who would manage [[GNU Mailman|Mailman]] or install a [[Content Management System|CMS]]) were able to move forward, while the rest would abandon the service. Thus, new free collaborative tools were needed, more usable and suitable for anyone, as the available free tools required a high degree of technical expertise. This is why Kune, whose name means "together" in [[Esperanto]], was developed.\n\nThe first prototypes of Kune were developed using [[Ruby on Rails]] and [[Pyjamas (software)|Pyjamas]]. However, with the [[Java (software platform)#Licensing|release of Java]] and the [[Google Web Toolkit]] as free software, the community embraced these technologies since 2007.<ref name="video2008" /> In 2009, with a stable codebase and about to release a major version of Kune,<ref>{{cite news|title=¡Colabora con Kune! Llamado a desarrolladores/as|url=http://www.apesol.org/news/341|publisher=Peru Free Software Association|date=5 May 2009|accessdate=3 February 2011}}</ref> Google announced the [[Google Wave]] project and promised it would be released as free software. Wave was using the same technologies of Kune (Java + GWT, Guice, XMPP protocol) so it would be easy to integrate after its release. Besides, Wave was offering an open federated protocol, easy extensibility (through gadgets), easy control versioning, and very good real-time edition of documents. Thus, the community decided to halt the development of Kune, and wait for its release... in the meanwhile developing gadgets that would be integrated in Kune later on.<ref>{{cite web\n | last =\n | first =\n | authorlink =\n | title = MassMob: Meetings and Smart Mobs \n | work =\n | publisher = [[Comunes Collective]]\n | year = 2009\n | url = http://massmob.ourproject.org/\n | format =\n | doi =\n | accessdate = 22 April 2012 }}</ref><ref>{{cite web\n | last =\n | first =\n | authorlink =\n | title = Troco project: an experimental peer-to-peer currency\n | work = \n | publisher = [[Comunes Collective]]\n | origyear =2009| year =2010\n | url = http://troco.ourproject.org/\n | format =\n | doi =\n | accessdate = 22 April 2012 }}</ref><ref>{{cite web\n | last =\n | first =\n | authorlink =\n | title = Karma: A Reputation Rating System\n | work =\n | publisher = [[Comunes Collective]]\n | origyear = 2009| year = 2010\n | url = http://karma.ourproject.org/\n | format =\n | doi =\n | accessdate = 22 April 2012 }}</ref> In this same period, the community established the [[Comunes Association]] (with an acknowledged inspiration in [[Software in the Public Interest]]) as a non-profit legal umbrella for free software tools for encouraging the [[Commons]] and facilitating the work of [[social movements]].<ref>{{cite interview |last =  |first =  |subjectlink = Interview to [[Comunes Collective]] |interviewer = Serotonina EH |title = |url = http://ondaexpansiva.net/?p=1001  |work = Free Culture Forum 2011 |publisher = Radio Onda Expansiva |location = [[Burgos]], [[Spain]] |date = 9 November 2011 |accessdate =11 April 2012 }}</ref> The umbrella covered Ourproject, Kune and Move Commons,<ref>{{cite news\n|title=Move Commons, crowdfunding y etiquetado de proyectos sociales\n|url=http://www.misapisportuscookies.com/2011/12/move-commons/\n|accessdate=11 April 2012\n|newspaper=Mis APIs por tus Cookies\n|date=1 December 2012\n}}</ref> together with some other minor projects.\n\nIn November 2010, the free [[Apache Wave]] (previously Wave-in-a-Box) was released, under the umbrella of the [[Apache Foundation]]. Since then, the community began integrating its source code within the Kune previous codebase,<ref>{{Cite web\n| url = http://ecosistemaurbano.org/castellano/move-commons-y-kune-herramientas-libres-para-el-activismo-y-la-colaboracion/\n| title = Move Commons & Kune: free tools for activism and collaboration (Move Commons y Kune: herramientas libres para el activismo y la colaboración)\n| accessdate = 11 April 2012\n| author = \n| last = Toledo\n| first = Jorge\n| authorlink = \n| date = 14 February 2012\n| work = \n| publisher = Ecosistema Urbano\n| pages = \n| format = \n| quote = \n| archiveurl = \n| archivedate = \n}}</ref> and with the support of the IEPALA Foundation.<ref>{{cite web\n | last =\n | first =\n | authorlink =\n | title = Presenting status of Kune development Jan-2011\n | work =\n | publisher =\n | date = 24 January 2011\n | url =http://kune.ourproject.org/2011/01/status-jan2011/\n | format =\n | doi =\n | accessdate = 22 April 2012 }}</ref> Kune released its Beta and moved to production in April 2012.\n\nSince then, Kune has been catalogued as "activism 2.0"<ref>{{Cite web\n| url = https://pilargonzalo.wordpress.com/2011/11/04/activismo-2-0-y-empoderamiento-ciudadano-en-red-i/\n| title = Activism 2.0 and citizen empowerment in the net (I) (Activismo 2.0 y empoderamiento ciudadano en red (I))\n| accessdate = 11 April 2012\n| author = \n| last = Gonzalo\n| first = Pilar\n| authorlink = \n| date = 4 November 2011\n| work = \n| publisher = \n| pages = \n| format = \n| quote = \n| archiveurl = \n| archivedate = \n}}</ref> and citizen tool,<ref>{{cite journal|title=Free Knowledge: Collective intelligence for developing free tools and community resources (Conocimiento libre: Inteligencia colectiva para desarrollar herramientas libres y recursos comunitarios)|journal=¡Rebelaos!|year=2012|volume=1|pages=10|accessdate=11 April 2012}}</ref><ref>{{cite news\n|title= Cooperation, Collaboration and citizen power (Cooperación, colaboración y poder ciudadano)\n|url=http://www.sindikos.com/2012/01/cooperacion-colaboracion-y-poder-ciudadano/\n|accessdate=11 April 2012\n|newspaper=Sindikos\n|date=20 January 2012\n}}</ref> a tool for NGOs,<ref>{{Cite web\n| url = http://www.democraciaycooperacion.net/contenidos-sitio-web/espanol/fidc/entre-foros/iii-taller-internacional-del/informacion-398/article/las-redes-de-organizaciones\n| title = Las redes de organizaciones sociales del CIS generan propuestas para la internacionalización de la acción\n| accessdate = 11 April 2012\n| author = \n| last = \n| first = \n| authorlink = \n| date = 5 March 2011\n| work = \n| publisher = Foro Internacional Democracia y Cooperación\n| pages = \n| format = \n| quote = \n| archiveurl = \n| archivedate = \n}}</ref><ref>{{Cite report\n | author     = <!-- or |last= and |first= -->\n | authorlink =\n | coauthors  = \n | date       = February 2012\n | title      = Document Summary of the Rapporteur of Second Regional Workshop Latin America and the Caribbean\n | url        = http://www.democraciaycooperacion.net/IMG/pdf/Summary_Rapporteur_and_context.pdf\n | publisher  = [[Ministry of Foreign Affairs and Cooperation (Spain)]]\n | format     =\n | others     =\n | edition    =\n | location   = [[Mexico City]]\n | chapter    =\n | section    =\n | page       =\n | pages      = 15\n | docket     =\n | accessdate = 12 April 2012\n | quote      =\n}}</ref> multi-tool for general purpose<ref>{{Cite web\n| url = http://www.contenidosenred.com/blog/kune/\n| title = Kune\n| accessdate = 11 April 2012\n| author = \n| last = \n| first = \n| authorlink = \n| authors = Lucrecia Baquero, Clara Alba\n| date = 17 February 2012\n| work = \n| publisher = Contenidos en Red\n| pages = \n| format = \n| quote = \n| archiveurl = \n| archivedate = \n}}</ref> (and following that, criticized for the risk of falling on the [[second-system effect]]<ref>{{Cite web\n| url = http://jotarp.org/2011/10/internet/contra-las-redes-sociales.html\n| title = Against social networks (Contra las redes sociales)\n| accessdate = 11 April 2012\n| author = \n| last = Palacios\n| first = J. Ramón\n| authorlink = \n| date = 24 October 2011\n| work = \n| publisher = Jotarp\n| pages = \n| quote = \n| archiveurl = \n| archivedate = \n}}</ref>) and example of the new paradigm.<ref>{{Cite web\n| url = https://semillasdeinnovacion.wordpress.com/2012/03/13/sobre-la-necesidad-de-acercar-la-ciudad-al-campo-y-viceversa/\n| title = On the need to bring closer city and country (Sobre la necesidad de acercar la ciudad al campo y viceversa)\n| accessdate = 11 April 2012\n| author = \n| last = \n| first = \n| authorlink = \n| authors = Lucrecia Baquero, Clara Alba\n| date = 13 March 2012\n| work = \n| publisher = Semillas de Innovación\n| pages = \n| format = \n| quote = \n| archiveurl = \n| archivedate = \n}}</ref> It was selected as "open website of the week" by the [[Open University of Catalonia]]<ref>{{cite news\n|title= Open website of the week: Kune\n|url=http://mentesabiertas.uoc.edu/webabiertas/webabiertadelasemanakune?lang=en\n|accessdate=11 April 2012\n|newspaper=Open Minds, [[Open University of Catalonia]] \n|date=5 March 2012\n}}</ref> and as one of the [[Occupy movement|#Occupy]] Tech projects.<ref>{{cite web\n | last =\n | first =\n | authorlink =\n | title = #Occupy Tech projects\n | work =\n | publisher =\n | url =https://tech.occupy.net/projects/\n | format =\n | doi =\n | accessdate = 22 April 2012}}</ref> Nowadays there are plans of another federated social network, Lorea (based on [[Elgg (software)|Elgg]]), to connect with Kune.<ref>{{cite news\n|title= Radical Community Manager\n|url=https://ncomuneszgz.wordpress.com/2012/01/08/radical-community-manager/\n|accessdate=11 April 2012\n|newspaper=Nociones Comunes\n|date=17 March 2012\n}}</ref>\n\n<!--\n<ref>{{cite web\n | last =\n | first =\n | authorlink =\n | title =\n | work =\n | publisher =\n | url =\n | format =\n | doi =\n | accessdate = }}</ref>\n\n<ref>{{cite press release\n | title =\n | publisher =\n | url =\n | accessdate = }}</ref>\n\n<ref>{{Cite book\n| last = Camino\n| first = S.\n|author2=F. Javier |author3=M. Jiménez Gañán |author4=S. Frutos Cid\n | chapter = Collaborative Development within Open Source Communities\n| title =Encyclopedia of Networked and Virtual Organizations\n|publisher= IGI Global, Information Science Reference\n|isbn = 978-1-59904-885-7\n| year = 2008\n}}</ref>\n\n<ref>{{Cite journal\n| volume = 32\n| issue = 3\n| pages = 1–1\n| last = Machado\n| first = H.\n|author2=A. Suset |author3=GJ Martín |author4=FR Funes-Monzote\n | title = From the reductionist approach to the system approach in Cuban agriculture: a necessary change of vision\n| journal = Pastos y Forrajes\n| year = 2009\n}}</ref>\n\n<ref>{{cite news\n|title=\n|url=\n|accessdate=11 April 2012\n|newspaper=\n|date=29 January 2004\n}}</ref>\n\n-->\n\n== Feature list ==\n\n* All the functionalities of [[Apache Wave]], that is collaborative federated real-time editing, plus\n* Communication\n** Chat and chatrooms compatible with Gmail and Jabber through XMPP (with several XEP extensions), as it integrates Emite<ref>{{cite web\n | last =\n | first =\n | authorlink =\n | title = Emite: XMPP & GWT\n | work =\n | publisher =\n | url =http://emite.googlecode.com/\n | format =\n | doi =\n | accessdate = 22 April 2012 }}</ref>\n** Social networking (federated)\n* Real-time collaboration for groups in:\n** Documents: as in [[Google Docs]]\n** Wikis\n** Lists: as in [[Google Groups]] but minimizing emails, through waves\n** Group Tasks\n** Group Calendar: as in [[Google Calendar]], with ical export\n** Group Blogs\n** Web-creation: aiming to publish contents directly on the web (as in [[WordPress]], with a dashboard and public view) (in development)\n** Bartering: aiming to decentralize bartering as in [[eBay]]\n* Advanced email\n** Waves: aims to replace most uses of email\n** Inbox: as in email, all your conversations and documents in all kunes are controlled from your inbox\n** Email notifications (Projected: replies from email)\n* Multimedia & Gadgets\n** Image or Video galleries integrated in any doc\n** Maps, mindmaps, Twitter streams, etc.\n** Polls, voting, events, etc.\n** and more via Apache Wave extensions, easy to program (as in [[Facebook apps]], they run on top of Kune)\n* Federation\n** Distributed Social Networking the same way as e-mail: from one inbox you control all your activity in all kunes, and you can collaborate with anyone or any group regardless of the kune where they were registered.\n** Interoperable with any Kune server or Wave-based system\n** Chat interoperable with any XMPP server\n* Usability\n** Strong focus on usability for any user\n** Animated tutorials for each tool\n** [[Drag and drop|Drag&Drop]] for sharing contents, add users to a doc, change roles, delete contents, etc.\n** Shortcuts\n* Free culture\n** Developed using free software and released under [[Affero General Public License|AGPL]]\n** Easy assistant for choosing content licenses for groups. Default license is [[Creative Commons]] BY-SA.\n* Developer-friendly\n** Debian/Ubuntu package for easy installation\n** Wave Gadgets can be programmed in Java+GWT, [[JavaScript]] or [[Python (programming language)|Python]]\n\n== Supporters and adopters ==\nKune has the active support of several organizations and institutions:\n* [[Comunes Association]], whose community is behind Kune development. It hosts a Kune server for free projects: [https://kune.cc/#! https://kune.cc]\n* IEPALA Foundation,<ref>{{cite web|title=IEPALA Foundation homepage|url=http://www.iepala.es|accessdate=22 April 2012}}</ref> which is supporting the development with economical and technical resources. It hosts a Kune server for [[non-governmental organizations]]: [http://social.gloobal.net "Social Gloobal"] (previously EuroSur).\n* Grasia Software Agent Research Group<ref>{{cite web|title=Grasia Research Group homepage|url=http://grasia.fdi.ucm.es/main/|accessdate=22 April 2012}}</ref> of the [[Complutense University of Madrid]] has provided technical resources. It seeks to host a Kune server for academic article collaboration.\n* Interns from the Master of Free Software from the [[King Juan Carlos University]] are participating in the development.\n* Trainees from the [[American University of Science and Technology]] (Lebanon) participate in the system administration.\n* [[Paulo Freire Institute]] in Brazil participated in the early design and prototypes.\n* The Kune workgroup of the Medialab Prado<ref>{{cite web|title=Medialab-Prado (Madrid) homepage|url=http://medialab-prado.es|accessdate=22 April 2012}}</ref> are participating in the beta-testing.<ref>{{cite web|title=Comunes profile in Medialab-Prado|url=http://medialab-prado.es/person/comunes|accessdate=22 April 2012}}</ref>\n\n== See also ==\n* [[Apache Wave]]\n* [[Comunes Collective]]\n* [[Distributed social network]]\n* [[Comparison of software and protocols for distributed social networking]]\n* [[List of AGPL web applications]]\n* [[Ourproject.org]]\n* [[Wave Federation Protocol]]\n\n== References ==\n{{Reflist|2}}\n\n== External links ==\n* [https://kune.cc/ Kune.cc main site]\n* [http://kune.cc/?locale=de#!kune.wiki.17.1678 Sites using kune]\n* [http://kune.ourproject.org Kune information webpage]\n\n{{Cleanup|reason=[[WP:OVERCAT]]|date=May 2016}}\n\n<!--- Categories --->\n[[Category:Articles created via the Article Wizard]]\n[[Category:Project hosting websites]]\n[[Category:Creative Commons-licensed websites]]\n[[Category:Collaborative projects]]\n[[Category:Virtual communities]]\n[[Category:Online communities for social change]]\n[[Category:Free groupware]]\n[[Category:Free project management software]]\n[[Category:Multilingual websites]]\n[[Category:Community websites]]\n[[Category:Social networking services]]\n[[Category:Web applications]]\n[[Category:Instant messaging]]\n[[Category:Online chat]]\n[[Category:Social information processing]]\n[[Category:Groupware]]\n[[Category:Wikis]]\n[[Category:Blog software]]\n[[Category:Collaborative real-time editors]]\n[[Category:2012 software]]\n[[Category:Electronic documents]]\n[[Category:Free software]]\n[[Category:Free software programmed in Java (programming language)]]\n[[Category:Cross-platform free software]]\n[[Category:Internet properties established in 2007]]\n[[Category:Software using the GNU AGPL license]]']
['Category:Open-access archives', '43515750', '{{cat main|Open archive}}\n{{cat see also|Eprint archives}}\n \n[[Category:Open access (publishing)|Archives]]\n[[Category:Online archives]]\n[[Category:Academic publishing]]\n[[Category:Digital libraries]]\n[[Category:Electronic documents]]\n[[Category:Full text scholarly online databases]]']
['Category:Customer communications management', '47769116', '[[Category:Software]]\n[[Category:Document management systems]]\n[[Category:Electronic documents]]\n[[Category:Customer relationship management software]]\n[[Category:Information technology management]]\n{{Commons category|Customer communications management}}']
['Digital signature', '59644', 'A \'\'\'digital signature\'\'\'  is a mathematical scheme for demonstrating the authenticity of digital messages or documents. A valid digital signature gives a recipient reason to believe that the message was created by a known sender ([[authentication]]), that the sender cannot deny having sent the message ([[non-repudiation]]), and that the message was not altered in transit ([[Data integrity|integrity]]).\n\nDigital signatures are a standard element of most [[cryptographic protocol]] suites, and are commonly used for software distribution, financial transactions, [[contract management software]], and in other cases where it is important to detect forgery or tampering.\n\n== Explanation ==\nDigital signatures are often used to implement [[electronic signature]]s, a broader term that refers to any electronic data that carries the intent of a signature,<ref>[http://frwebgate.access.gpo.gov/cgi-bin/getdoc.cgi?dbname=106_cong_public_laws&docid=f:publ229.106.pdf US ESIGN Act of 2000]</ref> but not all electronic signatures use digital signatures.<ref>[http://enterprise.state.wi.us/home/strategic/esig.htm State of WI]</ref><ref>[http://www.naa.gov.au/recordkeeping/er/Security/6-glossary.html National Archives of Australia] {{webarchive |url=https://web.archive.org/web/20141109/http://www.naa.gov.au/recordkeeping/er/Security/6-glossary.html |date=November 9, 2014 }}</ref> In some countries, including the United States, [[Turkey]], [[India]], Brazil, Indonesia, Saudi Arabia,<ref>{{cite book|first=Government of India|title=The Information Technology Act, 2000|url=http://www.dot.gov.in/sites/default/files/itbill2000_0.pdf}}</ref> [[Switzerland]] and the countries of the [[European Union]],<ref name=Cryptomathic_MajorStandardsDigSig>{{cite web|last1=Turner|first1=Dawn|title=Major Standards and Compliance of Digital Signatures - A World-Wide Consideration|url=http://www.cryptomathic.com/news-events/blog/major-standards-and-compliance-of-digital-signatures-a-world-wide-consideration|publisher=Cryptomathic|accessdate=7 January 2016}}</ref><ref name=CryptomathicDigSigServicesAshiqJA>{{cite web|last1=JA|first1=Ashiq|title=Recommendations for Providing Digital Signature Services|url=http://www.cryptomathic.com/news-events/blog/recommendations-for-providing-digital-signature-services|publisher=Cryptomathic|accessdate=7 January 2016}}</ref> electronic signatures have legal significance.\n\nDigital signatures employ [[asymmetric key algorithm|asymmetric cryptography]]. In many instances they provide a layer of validation and security to messages sent through a nonsecure channel: Properly implemented, a digital signature gives the receiver reason to believe the message was sent by the claimed sender.  Digital seals and signatures are equivalent to handwritten signatures and stamped seals.<ref>[http://www.arx.com/industries/engineering/regulatory-compliance/ Regulatory Compliance: Digital signatures and seals are legally enforceable ESIGN (Electronic Signatures in Global and National Commerce) Act]</ref> Digital signatures are equivalent to traditional handwritten signatures in many respects, but properly implemented digital signatures are more difficult to forge than the handwritten type. Digital signature schemes, in the sense used here, are cryptographically based, and must be implemented properly to be effective. Digital signatures can also provide [[non-repudiation]], meaning that the signer cannot successfully claim they did not sign a message, while also claiming their [[private key]] remains secret; further, some non-repudiation schemes offer a time stamp for the digital signature, so that even if the private key is exposed, the signature is valid. Digitally signed messages may be anything representable as a [[bitstring]]: examples include [[electronic mail]], [[contract]]s, or a message sent via some other [[cryptographic protocol]].\n\n==Definition of Digital Signature==\n{{Main article|Public-key cryptography}}\nA digital signature scheme typically consists of three algorithms;\n* A \'\'[[key generation]]\'\' algorithm that selects a \'\'private key\'\' [[Uniform distribution (discrete)|uniformly at random]] from a set of possible private keys. The algorithm outputs the private key and a corresponding \'\'public key\'\'.\n* A \'\'signing\'\' algorithm that, given a message and a private key, produces a signature.\n* A \'\'signature verifying\'\' algorithm that, given the message, public key and signature, either accepts or rejects the message\'s claim to authenticity.\n\nTwo main properties are required.  First, the authenticity of a signature generated from a fixed message and fixed private key can be verified by using the corresponding public key. Secondly, it should be computationally infeasible to generate a valid signature for a party without knowing that party\'s private key.\nA digital signature is an authentication mechanism that enables the creator of the message to attach a code that acts as a signature. \nThe [[Digital Signature Algorithm]] (DSA), developed by the [[National Institute of Standards and Technology]], is one of [[Digital signature#Some digital signature algorithms|many examples]] of a signing algorithm.\n\nIn the following discussion, 1<sup>\'\'n\'\'</sup> refers to a [[Unary numeral system|unary number]].\n\nFormally, a \'\'\'digital signature scheme\'\'\' is a triple of probabilistic polynomial time algorithms, (\'\'G\'\', \'\'S\'\', \'\'V\'\'), satisfying:\n* \'\'G\'\' (key-generator) generates a public key, \'\'pk\'\', and a corresponding private key, \'\'sk\'\', on input 1<sup>\'\'n\'\'</sup>, where \'\'n\'\' is the security parameter.\n* \'\'S\'\' (signing) returns a tag, \'\'t\'\', on the inputs: the private key, \'\'sk\'\', and a string, \'\'x\'\'.\n* \'\'V\'\' (verifying) outputs \'\'accepted\'\' or \'\'rejected\'\' on the inputs: the public key, \'\'pk\'\', a string, \'\'x\'\', and a tag, \'\'t\'\'. \nFor correctness, \'\'S\'\' and \'\'V\'\' must satisfy\n\n: Pr [ (\'\'pk\'\', \'\'sk\'\') ← \'\'G\'\'(1<sup>\'\'n\'\'</sup>), \'\'V\'\'( \'\'pk\'\', \'\'x\'\', \'\'S\'\'(\'\'sk\'\', \'\'x\'\') ) = \'\'accepted\'\' ] = 1.<ref>Pass, def 135.1</ref>\n\nA digital signature scheme is \'\'\'secure\'\'\' if for every non-uniform probabilistic polynomial time [[Adversary (cryptography)|adversary]], \'\'A\'\'\n\n: Pr [ (\'\'pk\'\', \'\'sk\'\') ← \'\'G\'\'(1<sup>\'\'n\'\'</sup>), (\'\'x\'\', \'\'t\'\') ← \'\'A\'\'<sup>\'\'S\'\'(\'\'sk\'\', · )</sup>(\'\'pk\'\', 1<sup>\'\'n\'\'</sup>), \'\'x\'\' ∉ \'\'Q\'\', \'\'V\'\'(\'\'pk\'\', \'\'x\'\', \'\'t\'\') = \'\'accepted\'\'] < [[Negligible function|negl]](\'\'n\'\'),\n\nwhere \'\'A\'\'<sup>\'\'S\'\'(\'\'sk\'\', · )</sup> denotes that \'\'A\'\' has access to the [[Oracle machine|oracle]], \'\'S\'\'(\'\'sk\'\', · ), and \'\'Q\'\' denotes the set of the queries on \'\'S\'\' made by \'\'A\'\', which knows the public key, \'\'pk\'\', and the security parameter, \'\'n\'\'. Note that we require any adversary cannot directly query the string, \'\'x\'\', on \'\'S\'\'.<ref>Goldreich\'s FoC, vol. 2, def 6.1.2. Pass, def 135.2</ref>\n\n==History of Digital Signature==\nIn 1976, [[Whitfield Diffie]] and [[Martin Hellman]] first described the notion of a digital signature scheme, although they only conjectured that such schemes existed.<ref>"New Directions in Cryptography", IEEE Transactions on Information Theory, IT-22(6):644–654, Nov. 1976.</ref><ref name=lysythesis>"[http://theory.lcs.mit.edu/~cis/theses/anna-phd.pdf Signature Schemes and Applications to Cryptographic Protocol Design]", Anna Lysyanskaya, PhD thesis, [[Massachusetts Institute of Technology|MIT]], 2002.</ref>  Soon afterwards, [[Ronald Rivest]], [[Adi Shamir]], and [[Len Adleman]] invented the [[RSA (algorithm)|RSA]] algorithm, which could be used to produce primitive digital signatures<ref name="rsa">\n{{cite journal\n | first = R. | last = Rivest\n | author2 = A. Shamir; L. Adleman\n | url = http://people.csail.mit.edu/rivest/Rsapaper.pdf\n | title = A Method for Obtaining Digital Signatures and Public-Key Cryptosystems\n | journal = Communications of the ACM\n | volume = 21  | issue = 2 | pages = 120–126 | year = 1978\n | doi = 10.1145/359340.359342\n}}</ref> (although only as a proof-of-concept &ndash; "plain" RSA signatures are not secure<ref>For example any integer, \'\'r\'\', "signs" \'\'m\'\'=\'\'r\'\'<sup>\'\'e\'\'</sup> and the product, \'\'s\'\'<sub>1</sub>\'\'s\'\'<sub>2</sub>, of any two valid signatures, \'\'s\'\'<sub>1</sub>, \'\'s\'\'<sub>2</sub> of \'\'m\'\'<sub>1</sub>, \'\'m\'\'<sub>2</sub> is a valid signature of the product, \'\'m\'\'<sub>1</sub>\'\'m\'\'<sub>2</sub>.</ref>). The first widely marketed software package to offer digital signature was [[Lotus Notes]] 1.0, released in 1989, which used the RSA algorithm.<ref>{{cite web|title=The History of Notes and Domino|url=http://www.ibm.com/developerworks/lotus/library/ls-NDHistory/|website=developerWorks|accessdate=17 September 2014}}</ref>\n\nOther digital signature schemes were soon developed after RSA, the earliest being [[Lamport signature]]s,<ref>"Constructing digital signatures from a one-way function.", [[Leslie Lamport]], Technical Report CSL-98, SRI International, Oct. 1979.</ref> [[Merkle tree|Merkle signatures]] (also known as "Merkle trees" or simply "Hash trees"),<ref>"A certified digital signature", Ralph Merkle, In Gilles Brassard, ed., Advances in Cryptology – [[CRYPTO]] \'89, vol. 435 of Lecture Notes in Computer Science, pp. 218&ndash;238, Spring Verlag, 1990.</ref> and [[Rabin signature]]s.<ref>"Digitalized signatures as intractable as factorization."  [[Michael O. Rabin]], Technical Report MIT/LCS/TR-212, MIT Laboratory for Computer Science, Jan. 1979</ref>\n\nIn 1988, [[Shafi Goldwasser]], [[Silvio Micali]], and [[Ronald Rivest]] became the first to rigorously define the security requirements of digital signature schemes.<ref name="SJC 17(2)">"A digital signature scheme secure against adaptive chosen-message attacks.", Shafi Goldwasser, Silvio Micali, and Ronald Rivest. SIAM Journal on Computing, 17(2):281&ndash;308, Apr. 1988.</ref> They described a hierarchy of attack models for signature schemes, and also presented the [[GMR (cryptography)|GMR signature scheme]], the first that could be proved to prevent even an existential forgery against a chosen message attack.<ref name="SJC 17(2)"/>\n\n==How they work==\nTo create RSA signature keys, generate a RSA key pair containing a modulus, \'\'N\'\', that is the product of two large primes, along with integers, \'\'e\'\' and \'\'d\'\', such that \'\'e&nbsp;d\'\'&nbsp;[[Modular arithmetic|≡]]&nbsp;1&nbsp;(mod&nbsp;φ(\'\'N\'\')), where φ is the [[Euler\'s totient function|Euler phi-function]]. The signer\'s public key consists of \'\'N\'\' and \'\'e\'\', and the signer\'s secret key contains \'\'d\'\'.\n\nTo sign a message, \'\'m\'\', the signer computes a signature, σ, such that σ ≡ \'\'m\'\'<sup>\'\'d\'\'</sup> (mod \'\'N\'\'). To verify, the receiver checks that σ<sup>\'\'e\'\'</sup> ≡ \'\'m\'\' (mod \'\'N\'\').\n\nAs noted earlier, this basic scheme is not very secure. To prevent attacks, one can first apply a [[cryptographic hash function]] to the message, \'\'m\'\', and then apply the RSA algorithm described above to the result. This approach is secure assuming the hash function is a [[random oracle model|random oracle]].\n\nMost early signature schemes were of a similar type: they involve the use of a [[trapdoor permutation]], such as the RSA function, or in the case of the Rabin signature scheme, computing square modulo composite, \'\'n.\'\' A trapdoor permutation family is a family of [[permutation]]s, specified by a parameter, that is easy to compute in the forward direction, but is difficult to compute in the reverse direction without already knowing the private key ("trapdoor").  Trapdoor permutations can be used for digital signature schemes, where computing the reverse direction with the secret key is required for signing, and computing the forward direction is used to verify signatures.\n\nUsed directly, this type of signature scheme is vulnerable to a key-only existential forgery attack. To create a forgery, the attacker picks a random signature σ and uses the verification procedure to determine the message, \'\'m\'\', corresponding to that signature.<ref>"Modern Cryptography: Theory & Practice", Wenbo Mao, Prentice Hall Professional Technical Reference, New Jersey, 2004, pg. 308.  ISBN 0-13-066943-1</ref> In practice, however, this type of signature is not used directly, but rather, the message to be signed is first [[cryptographic hash function|hashed]] to produce a short digest that is then signed. This forgery attack, then, only produces the hash function output that corresponds to σ, but not a message that leads to that value, which does not lead to an attack. In the random oracle model, this [[Full domain hash|hash-then-sign]] form of signature is existentially unforgeable, even against a [[chosen-plaintext attack]].<ref name=lysythesis />{{Clarify|reason=Please give a page number or theorem number.|date=September 2010}}\n\nThere are several reasons to sign such a hash (or message digest) instead of the whole document.\n\n;For efficiency: The signature will be much shorter and thus save time since hashing is generally much faster than signing in practice.\n;For compatibility: Messages are typically bit strings, but some signature schemes operate on other domains (such as, in the case of RSA, numbers modulo a composite number \'\'N\'\'). A hash function can be used to convert an arbitrary input into the proper format.\n;For integrity: Without the hash function, the text "to be signed" may have to be split (separated) in blocks small enough for the signature scheme to act on them directly. However, the receiver of the signed blocks is not able to recognize if all the blocks are present and in the appropriate order.\n\n==Notions of security==\nIn their foundational paper, Goldwasser, Micali, and Rivest lay out a hierarchy of attack models against digital signatures:<ref name="SJC 17(2)"/>\n\n# In a \'\'key-only\'\' attack, the attacker is only given the public verification key.\n# In a \'\'known message\'\' attack, the attacker is given valid signatures for a variety of messages known by the attacker but not chosen by the attacker.\n# In an \'\'adaptive chosen message\'\' attack, the attacker first learns signatures on arbitrary messages of the attacker\'s choice.\n\nThey also describe a hierarchy of attack results:<ref name="SJC 17(2)"/>\n\n# A \'\'total break\'\' results in the recovery of the signing key.\n# A [[universal forgery]] attack results in the ability to forge signatures for any message.\n# A [[selective forgery]] attack results in a signature on a message of the adversary\'s choice.\n# An [[existential forgery]] merely results in some valid message/signature pair not already known to the adversary.\n\nThe strongest notion of security, therefore, is security against existential forgery under an adaptive chosen message attack.\n\n==Applications of digital signatures==\n\nAs organizations move away from paper documents with ink signatures or authenticity stamps, digital signatures can provide added assurances of the evidence to provenance, identity, and status of an electronic document as well as acknowledging informed consent and approval by a signatory.  The United States Government Printing Office (GPO) publishes electronic versions of the budget, public and private laws, and congressional bills with digital signatures.  Universities including Penn State, [[University of Chicago]], and Stanford are publishing electronic student transcripts with digital signatures.\n\nBelow are some common reasons for applying a digital signature to communications:\n\n===Authentication===\nAlthough messages may often include information about the entity sending a message, that information may not be accurate.  Digital signatures can be used to authenticate the source of messages. When ownership of a digital signature secret key is bound to a specific user, a valid signature shows that the message was sent by that user. The importance of high confidence in sender authenticity is especially obvious in a financial context. For example, suppose a bank\'s branch office sends instructions to the central office requesting a change in the balance of an account. If the central office is not convinced that such a message is truly sent from an authorized source, acting on such a request could be a grave mistake.\n\n===Integrity===\nIn many scenarios, the sender and receiver of a message may have a need for confidence that the message has not been altered during transmission. Although encryption hides the contents of a message, it may be possible to \'\'change\'\' an encrypted message without understanding it. (Some encryption algorithms, known as [[Malleability (cryptography)|nonmalleable]] ones, prevent this, but others do not.) However, if a message is digitally signed, any change in the message after signature invalidates the signature. Furthermore, there is no efficient way to modify a message and its signature to produce a new message with a valid signature, because this is still considered to be computationally infeasible by most cryptographic hash functions (see [[collision resistance]]).\n\n===Non-repudiation===\nNon-repudiation,<ref name="Cryptomathic_MajorStandardsDigSig" /> or more specifically \'\'non-repudiation of origin\'\', is an important aspect of digital signatures. By this property, an entity that has signed some information cannot at a later time deny having signed it. Similarly, access to the public key only does not enable a fraudulent party to fake a valid signature.\n\nNote that these authentication, non-repudiation etc. properties rely on the secret key \'\'not having been revoked \'\'prior to its usage.  Public revocation of a key-pair is a required ability, else leaked secret keys would continue to implicate the claimed owner of the key-pair. Checking revocation status requires an "online" check; e.g., checking a [[certificate revocation list]] or via the <ref name="CryptomathicDigSigServicesAshiqJA" />[[Online Certificate Status Protocol]].   Very roughly this is analogous to a vendor who receives credit-cards first checking online with the credit-card issuer to find if a given card has been reported lost or stolen.   Of course, with stolen key pairs, the theft is often discovered only after the secret key\'s use, e.g., to sign a bogus certificate for espionage purpose.\n\n==Additional security precautions==\n\n===Putting the private key on a smart card===\nAll public key / private key cryptosystems depend entirely on keeping the private key secret. A private key can be stored on a user\'s computer, and protected by a local password, but this has two disadvantages:\n\n* the user can only sign documents on that particular computer\n* the security of the private key depends entirely on the [[computer insecurity|security]] of the computer\n\nA more secure alternative is to store the private key on a [[smart card]]. Many smart cards are designed to be tamper-resistant (although some designs have been broken, notably by [[Ross J. Anderson (professor)|Ross Anderson]] and his students). In a typical digital signature implementation, the hash calculated from the document is sent to the smart card, whose CPU signs the hash using the stored private key of the user, and then returns the signed hash. Typically, a user must activate his smart card by entering a [[personal identification number]] or PIN code (thus providing [[two-factor authentication]]). It can be arranged that the private key never leaves the smart card, although this is not always implemented. If the smart card is stolen, the thief will still need the PIN code to generate a digital signature. This reduces the security of the scheme to that of the PIN system, although it still requires an attacker to possess the card. A mitigating factor is that private keys, if generated and stored on smart cards, are usually regarded as difficult to copy, and are assumed to exist in exactly one copy. Thus, the loss of the smart card may be detected by the owner and the corresponding certificate can be immediately revoked. Private keys that are protected by software only may be easier to copy, and such compromises are far more difficult to detect.\n\n===Using smart card readers with a separate keyboard===\nEntering a PIN code to activate the smart card commonly requires a [[numeric keypad]]. Some card readers have their own numeric keypad. This is safer than using a card reader integrated into a PC, and then entering the PIN using that computer\'s keyboard. Readers with a numeric keypad are meant to circumvent the eavesdropping threat where the computer might be running a [[keystroke logging|keystroke logger]], potentially compromising the PIN code. Specialized card readers are also less vulnerable to tampering with their software or hardware and are often [[Evaluation Assurance Level|EAL3]] certified.\n\n===Other smart card designs===\nSmart card design is an active field, and there are smart card schemes which are intended to avoid these particular problems, though so far with little security proofs.\n\n===Using digital signatures only with trusted applications===\nOne of the main differences between a digital signature and a written signature is that the user does not "see" what he signs. The user application presents a hash code to be signed by the digital signing algorithm using the private key. An attacker who gains control of the user\'s PC can possibly replace the user application with a foreign substitute, in effect replacing the user\'s own communications with those of the attacker. This could allow a malicious application to trick a user into signing any document by displaying the user\'s original on-screen, but presenting the attacker\'s own documents to the signing application.\n\nTo protect against this scenario, an authentication system can be set up between the user\'s application (word processor, email client, etc.) and the signing application. The general idea is to provide some means for both the user application and signing application to verify each other\'s integrity. For example, the signing application may require all requests to come from digitally signed binaries.\n\n===Using a network attached [[hardware security module]]===\nOne of the main differences between a [[cloud]] based digital signature service and a locally provided one is risk.  Many risk averse companies, including governments, financial and medical institutions,  and payment processors require more secure standards, like [[FIPS 140-2]] level 3 and [[FIPS 201]] certification, to ensure the signature is validated and secure.<ref>[http://www.arx.com/products/privateserver-hsm/overview/ PrivateServer HSM Overview]</ref> <!--To finish: current and future applications, actual algorithms, standards, why not as adopted as widely as expected, etc.-->\n\n===WYSIWYS===\n{{Main article|WYSIWYS}}\nTechnically speaking, a digital signature applies to a string of bits, whereas humans and applications "believe" that they sign the semantic interpretation of those bits. In order to be semantically interpreted, the bit string must be transformed into a form that is meaningful for humans and applications, and this is done through a combination of hardware and software based processes on a computer system. The problem is that the semantic interpretation of bits can change as a function of the processes used to transform the bits into semantic content. It is relatively easy to change the interpretation of a digital document by implementing changes on the computer system where the document is being processed. From a semantic perspective this creates uncertainty about what exactly has been signed. [[WYSIWYS]] (What You See Is What You Sign) <ref name=WYSIWYS_SeminalPaper>{{cite journal|last1=Landrock|first1=Peter|last2=Pedersen|first2=Torben|title=WYSIWYS? -- What you see is what you sign?|journal=Information Security Technical Report|date=1998|volume=3|issue=2|pages=55–61}}</ref> means that the semantic interpretation of a signed message cannot be changed. In particular this also means that a message cannot contain hidden information that the signer is unaware of, and that can be revealed after the signature has been applied. WYSIWYS is a necessary requirement for the validity of digital signatures, but this requirement is difficult to guarantee because of the increasing complexity of modern computer systems. The term WYSIWYS was coined by [[Peter Landrock]] and [[Cryptomathic|Torben Pedersen]] to describe some of the principles in delivering secure and legally binding digital signatures for Pan-European projects.<ref name=WYSIWYS_SeminalPaper />\n\n===Digital signatures versus ink on paper signatures===\n\nAn ink signature could be replicated from one document to another by copying the image manually or digitally, but to have credible signature copies that can resist some scrutiny is a significant manual or technical skill, and to produce ink signature copies that resist professional scrutiny is very difficult.\n\nDigital signatures cryptographically bind an electronic identity to an electronic document and the digital signature cannot be copied to another document. Paper contracts sometimes have the ink signature block on the last page, and the previous pages may be replaced after a signature is applied.  Digital signatures can be applied to an entire document, such that the digital signature on the last page will indicate tampering if any data on any of the pages have been altered, but this can also be achieved by signing with ink and numbering all pages of the contract.\n\n==Some digital signature algorithms==\n*[[RSA (algorithm)|RSA]]-based signature schemes, such as [[RSA-PSS]]\n*[[Digital Signature Algorithm|DSA]] and its [[elliptic curve cryptography|elliptic curve]] variant [[Elliptic Curve Digital Signature Algorithm|ECDSA]]\n*[[EdDSA|Edwards-curve Digital Signature Algorithm]] and its [[EdDSA#Ed25519|Ed25519]] variant.\n*[[ElGamal signature scheme]] as the predecessor to DSA, and variants [[Schnorr signature]] and [[Pointcheval–Stern signature algorithm]]\n*[[Rabin signature algorithm]]\n*[[Pairing]]-based schemes such as [[Boneh–Lynn–Shacham|BLS]]\n*[[Undeniable signature]]s\n*[[Aggregate signature]] - a signature scheme that supports aggregation: Given n signatures on n  messages from n users, it is possible to aggregate all these signatures into a single signature whose size is constant in the number of users. This single signature will convince the verifier that the n users did indeed sign the n original messages.\n*[[Signatures with efficient protocols]] - are signature schemes that facilitate efficient cryptographic protocols such as [[zero-knowledge proofs]] or [[secure computation]].\n\n==The current state of use &ndash; legal and practical ==\n{{Globalize|section|date=November 2009}}\nAll digital signature schemes share the following basic prerequisites regardless of cryptographic theory or legal provision:\n\n#;Quality algorithms: Some public-key algorithms are known to be insecure, as practical attacks against them having been discovered.\n#\n#; Quality implementations: An implementation of a good algorithm (or [[cryptographic protocol|protocol]]) with mistake(s) will not work.\n#\n#; Users (and their software) must carry out the signature protocol properly.\n#\n#; The private key must remain private: If the private key becomes known to any other party, that party can produce \'\'perfect\'\' digital signatures of anything whatsoever.\n#\n#; The public key owner must be verifiable: A public key associated with Bob actually came from Bob. This is commonly done using a [[public key infrastructure]] (PKI) and the public key↔user association is attested by the operator of the PKI (called a [[certificate authority]]). For \'open\' PKIs in which anyone can request such an attestation (universally embodied in a cryptographically protected  [[identity certificate]]), the possibility of mistaken attestation is non-trivial. Commercial PKI operators have suffered several publicly known problems. Such mistakes could lead to falsely signed, and thus wrongly attributed, documents. \'Closed\' PKI systems are more expensive, but less easily subverted in this way.\n\nOnly if all of these conditions are met will a digital signature actually be any evidence of who sent the message, and therefore of their assent to its contents. Legal enactment cannot change this reality of the existing engineering possibilities, though some such have not reflected this actuality.\n\nLegislatures, being importuned by businesses expecting to profit from operating a PKI, or by the technological avant-garde advocating new solutions to old problems, have enacted statutes and/or regulations in many jurisdictions authorizing, endorsing, encouraging, or permitting digital signatures and providing for (or limiting) their legal effect. The first appears to have been in [[Utah]] in the United States, followed closely by the states [[Massachusetts]] and [[California]]. Other countries have also passed statutes or issued regulations in this area as well and the UN has had an active model law project for some time. These enactments (or proposed enactments) vary from place to place, have typically embodied expectations at variance (optimistically or pessimistically) with the state of the underlying [[cryptographic engineering]], and have had the net effect of confusing potential users and specifiers, nearly all of whom are not cryptographically knowledgeable. Adoption of technical standards for digital signatures have lagged behind much of the legislation, delaying a more or less unified engineering position on [[interoperability]], [[algorithm]] choice, [[key length]]s, and so on what the engineering is attempting to provide.\n\n{{see also|ABA digital signature guidelines}}\n\n==Industry standards==\n{{unreferenced section|date=January 2015}}\nSome industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators.  These include the [[Automotive Network Exchange]] for the automobile industry and the [[SAFE-BioPharma Association]] for the healthcare industry.\n\n===Using separate key pairs for signing and encryption===\nIn several countries, a digital signature has a status somewhat like that of a traditional pen and paper signature, like in the [http://europa.eu/legislation_summaries/information_society/l24118_en.htm EU digital signature legislation].<ref name=Cryptomathic_MajorStandardsDigSig /> Generally, these provisions mean that anything digitally signed legally binds the signer of the document to the terms therein. For that reason, it is often thought best to use separate key pairs for encrypting and signing. Using the encryption key pair, a person can engage in an encrypted conversation (e.g., regarding a real estate transaction), but the encryption does not legally sign every message he sends. Only when both parties come to an agreement do they sign a contract with their signing keys, and only then are they legally bound by the terms of a specific document. After signing, the document can be sent over the encrypted link.  If a signing key is lost or compromised, it can be revoked to mitigate any future transactions.  If an encryption key is lost, a backup or [[key escrow]] should be utilized to continue viewing encrypted content.  Signing keys should never be backed up or escrowed unless the backup destination is securely encrypted.\n\n==See also==\n* [[21 CFR 11]]\n* [[Blind signature]]\n* [[Detached signature]]\n* [[Digital certificate]]\n* [[Digital signature in Estonia]]\n* [[Electronic lab notebook]]\n* [[Electronic signature]]\n* [[Electronic signatures and law]]\n* [[eSign (India)]]\n* [[GNU Privacy Guard]]\n* [[Global Trust Center]]\n* [[PAdES]]\n* [[Public key infrastructure]]\n* [[Server-based signatures]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{citation|last1=Goldreich|first1=Oded|title=Foundations of cryptography I: Basic Tools|date=2001|publisher=Cambridge University Press|location=Cambridge|isbn=978-0-511-54689-1}}\n*{{citation|last1=Goldreich|first1=Oded|title=Foundations of cryptography II: Basic Applications|date=2004|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=978-0-521-83084-3|edition=1. publ.}}\n*{{citation|last1=Pass|first1=Rafael|title=A Course in Cryptography|url=https://www.cs.cornell.edu/courses/cs4830/2010fa/lecnotes.pdf|accessdate=31 December 2015}}\n\n==Further reading==\n* J. Katz and Y. Lindell, "Introduction to Modern Cryptography" (Chapman & Hall/CRC Press, 2007)\n* Stephen Mason, Electronic Signatures in Law (4th edition, Institute of Advanced Legal Studies for the SAS Digital Humanities Library, School of Advanced Study, University of London, 2016). ISBN 978-1-911507-00-0.\n* Lorna Brazell, Electronic Signatures and Identities Law and Regulation (2nd edn, London: Sweet & Maxwell, 2008);\n* Dennis Campbell, editor, E-Commerce and the Law of Digital Signatures (Oceana Publications, 2005).\n* M. H. M Schellenkens, Electronic Signatures Authentication Technology from a Legal Perspective, (TMC Asser Press, 2004).\n* Jeremiah S. Buckley, John P. Kromer, Margo H. K. Tank, and R. David Whitaker, The Law of Electronic Signatures (3rd Edition, West Publishing, 2010).\n* [http://journals.sas.ac.uk/deeslr/ \'\'Digital Evidence and Electronic Signature Law Review\'\'] Free open source\n\n{{Cryptography navbox | public-key}}\n\n{{DEFAULTSORT:Digital Signature}}\n[[Category:Public-key cryptography]]\n[[Category:Electronic documents]]\n[[Category:Key management]]\n[[Category:Notary]]\n[[Category:Signature]]\n[[Category:Records management technology]]']
['Arts and Humanities Citation Index', '2209985', '{{ infobox bibliographic database\n| image       = \n| caption     = \n| producer    =Thomson Reuters \n| country     =United States \n| history     = \n| languages   = \n| providers   =Web of Science, Dialog Bluesheets \n| cost        =Subscription \n| disciplines =Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, Religion, Television, Theater, and Radio \n| depth       =Index, abstract, citation indexing, author \n| formats     =original research articles, reviews, editorials, chronologies, abstracts,   scripts, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, book reviews, films, music, and theatrical performances \n| temporal    =1975 to present \n| geospatial  =global \n| number      = \n| updates     = \n| p_title     = \n| p_dates     = \n| ISSN        = \n| web         = \n| titles      =  \n}}\n\nThe \'\'\'\'\'Arts & Humanities Citation Index\'\'\'\'\' (\'\'\'A&HCI\'\'\'), also known as \'\'\'\'\'Arts & Humanities Search\'\'\'\'\', is a [[citation index]], with abstracting and indexing for more than 1,700 arts and humanities journals, and coverage of disciplines that includes social and natural science journals. Part of this database is derived from [[Current Contents]] records. Furthermore, the print counterpart is Current Contents.\n\nSubjects covered are the Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, History, Religion, Television, Theater, and Radio.\n\nAvailable citation (source) coverage includes articles, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, as well as citations to reviews of books, films, music, and theatrical performances.\n\nThis database can be accessed online through \'\'[[Web of Science]]\'\'. It provides access to current and retrospective bibliographic information and cited references. It also covers individually selected, relevant items from approximately 1,200 titles, mostly arts and humanities journals but with an unspecified number of titles from other disciplines.\n\nAccording to Thomson Reuters, the \'\'Arts & Humanities Search\'\', can be accessed via Dialog, DataStar, and OCLC, with weekly updates and backfiles to 1980.<ref name=dialog-blue>\n{{Cite web\n  | title =Arts & Humanities Search (File 255) \n  | publisher =Dialog bluesheets  \n  | date = \n  | url =http://library.dialog.com/bluesheets/html/bl0439.html \n  | format =Online web page \n  | accessdate =2011-07-03}}</ref><ref name=Iowa>\nDescription of Arts & Humanities Search. \n{{Cite web\n  | title =e-Library catalog\n  | publisher =Iowas State University  \n  | year =2008 \n  | url =http://www.lib.iastate.edu/collections/db/artshm.html\n  | format =Online web page \n  | accessdate =2011-07-03}}</ref><ref name=Iowa-wos>\nDescription of Web of Science coverage.  \n{{Cite web\n  | title =e-Library catalog\n  | publisher =Iowas State University  \n  | year =2008 \n  | url =http://www.lib.iastate.edu/collections/db/websci.html\n  | format =Online web page \n  | accessdate =2011-07-03}}</ref><ref name=TR>\nSee the page entitled "Tech Specs" \n{{Cite web\n  | title =Database description\n  | publisher =Thomson Reuters  \n  | year = \n  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/#tab3\n  | format =Online web page \n  | accessdate =2011-07-03}}</ref>\n\n==History==\nThe index was originally developed by the [[Institute for Scientific Information]], which was later acquired by [[Thomson Scientific]]. It is now published by [[Thomson Reuters]]\' IP & Science division.\n\n==See also==\n* [[Science Citation Index]]\n* [[Social Sciences Citation Index]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n* {{Official website|http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/}} at Thomson Reuters.\n* [http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=H Subject categories] of the Arts and Humanities Citation Index.\n\n{{Thomson Reuters}}\n\n{{DEFAULTSORT:Arts And Humanities Citation Index}}\n[[Category:Citation indices]]\n[[Category:Thomson Reuters]]\n[[Category:Arts journals| ]]\n[[Category:Humanities journals| ]]']
['SPIN bibliographic database', '28010203', "{{Infobox Bibliographic Database\n|title =SPIN  (Searchable Physics Information Notices)  \n|image = \n|caption = \n|producer =[[American Institute of Physics]] (AIP) \n|country =USA, Russia, Ukraine\n|history = \n|languages =English, [[Russian language|Russian]], [[Ukrainian language|Ukrainian]] \n|providers =[[Dialog (online database)|Dialog]], [[American Institute of Physics|AIP website]], [[SPIE|SPIE Digital Library]] \n|cost = \n|disciplines =Physics, Astronomy, Mathematics, Geophysics, Geosciences, Nuclear Science, Science & Technology \n|depth =Word, Phrase, Abstract, Author and Author affiliations, Descriptor, Errata (coden, or date, or volume) Identifier, Title, Astronomical objects, CODEN, Conference (location, or title, or year), Journal name, and more...   \n|formats =Journal Articles, Book Reviews, Conferences, Meetings, Patents, Symposia\n|temporal =1975 to the present  \n|geospatial =International \n|number =over 1.5 million \n|updates =Weekly \n|p_title =No print counterparts \n|p_dates = \n|ISSN =\n|web =https://scitation.aip.org/jhtml/scitation/coverage.jsp \n|titles =  \n}}\n\n'''SPIN''' (Searchable Physics Information Notices) '''bibliographic database''' is an indexing and abstracting service produced by the [[American Institute of Physics]] (AIP). The content focus of SPIN is described as the most significant areas of [[physics]] [[research]]. This type of [[scientific literature|literature coverage]] spans the major [[scientific journal|physical science journals]] and magazines. Major [[conference proceedings]] that are reported by the American Institute of Physics, member societies, as well as affiliated organizations are also included as part of this database. References, or citations, provide access to more than 1.5 million articles as of 2010. ''SPIN''  has no print counterpart.<ref name=DialogSpin/><ref name=AIP-SPIN/>\n\n==Journals==\nDelivery of timely indexing and abstracting is for, what are deemed to be, the significant or important [[physics]] and [[astronomy]] journals from the [[United States]], [[Russia]], and the [[Ukraine]]. Citations for journal articles are derived from original publications of the ''AIP'', which includes published translated works. At the same time, citations are included from member societies, and selectively chosen American journals. Citations become typically available online on the same date as the corresponding journal article.<ref name=DialogSpin/><ref name=AIP-SPIN> {{Cite web\n  | title =What is the SPIN database? \n  | work =Information about SPIN \n  | publisher =[[American Institute of Physics]] \n  | date =July 2010 \n  | url =http://scitation.aip.org/servlet/HelpSystem?KEY=SCI&TYPE=HELP/FAQ#ques3 \n  | format = \n  | accessdate =2010-07-12}}</ref>\n\n==Sources==\nOverall, the source citations are derived from material published by the AIP and member societies,  which are English-speaking, Russian, and Ukrainian journals and conference proceedings. Certain American physics-related articles are also sources of citations. About 60 journals have cover to cover indexing, and about 100 journals, overall, are indexed.<ref name=DialogSpin/><ref name=pub-coverage>{{Cite web\n  | title =SPIN Publication Coverage \n  | work =Complete list of publications covered and coverage years. \n  | publisher =American Institute of Physics \n  | date =July 2010 \n  | url =http://scitation.aip.org/jhtml/scitation/spincodens.jsp \n  | format = \n  | accessdate =2010-07-12}}</ref>  \n\n==Scope==\nSubject coverage encompasses the following: <ref name=DialogSpin>  {{Cite web\n  | title =Indexes and Databases \n  | work =SPIN: Searchable Physics Information Notices\n  | publisher =Raymond H. Fogler Library, The University of Maine\n  | date =October 2010 \n  | url =http://www.library.umaine.edu/indexesdb/dbdetails.asp?field=Name&search=SPIN:+Searchable+Physics+Information+Notices \n  | format = \n  | accessdate =2010-07-12}}</ref>\n\n*[[Applied physics]], [[Electromagnetic spectrum|Electromagnetic]] technology, [[Microelectronics]] \n*[[Atomic physics]] and [[Molecular physics]] \n*[[Biological physics]] and [[Medical physics]] \n*[[Classical physics]] and [[Quantum physics]] \n*[[Condensed matter physics]] \n*[[Elementary particle physics]] \n*[[Physics|General physics]], [[Optics]], [[Acoustics]], and [[Fluid dynamics]] \n*[[Geophysics]], [[Astronomy]], [[Astrophysics]] \n*[[Materials science]] \n*[[Nuclear physics]] \n*[[Plasma physics]] \n*[[Physical chemistry]]\n\n==See also==\n*[[List of academic databases and search engines]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.aip.org/press_release/spin.html AIP'S SPIN Database Reaches One Million Records].  American Institute of Physics. March 1, 2002.\n*[http://scholarlykitchen.sspnet.org/2009/06/17/physics-papers-and-the-arxiv/ Can everything published in physics can be found in the [[arXiv]]?]. The Scholarly Kitchen. [[Society for Scholarly Publishing]]. June, 2010.\n*[http://www.pub4stm.org/ AIP partnerships] (society publishing). July 2010.\n\n\n[[Category:Bibliographic databases and indexes]]\n[[Category:Citation indices]]\n[[Category:Scientific databases]]"]
['Chinese Science Citation Database', '47180611', "{{Infobox bibliographic database \n |title=Chinese Science Citation Database\n}}\nThe '''Chinese Science Citation Database''' (CSCD) is a [[bibliographic database]] and [[citation index]] produced by the [[Chinese Academy of Sciences]].\n\nIt is hosted by [[Thomson Reuters]], and it was the first database in its [[Web of Science]] product in a language other than English.<ref>[http://wokinfo.com/products_tools/multidisciplinary/cscd/]</ref>\n\n==See also==\n*[[Chinese Social Sciences Citation Index]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://thomsonreuters.com/content/dam/openweb/documents/pdf/scholarly-scientific-research/methodology/cscd-journal-list.pdf CSCD Journal List]\n\n[[Category:Bibliographic databases and indexes]]\n[[Category:Citation indices]]\n[[Category:Science and technology in China]]"]
['Thomson Directories', '1322148', "{{Advert|date=July 2009}}\n{{Refimprove|date=July 2009}}\n'''Thomson Directories''', more commonly referred to as Thomson Local, is a local business directory company based in [[Farnborough, Hampshire|Farnborough]], [[Hampshire]], [[England]], and offers business listings both in print and online following the launch of ThomsonLocal.com in 2003.\n\n174 regional editions of the Thomson Local are produced and delivered free of charge to residential and commercial addresses throughout the UK.\n \nThe Chief Executive Officer is currently Elio Shiavo.<ref>CEO http://www.answers.com/topic/thomson-directories-1</ref> \n\nThe company was purchased by [[US West]], a telecommunications company in the United States, in 1994.<ref>{{cite news|url=http://www.independent.co.uk/news/business/us-west-pays-70m-pounds-for-thomson-directories-american-telephone-company-continues-to-develop-multimedia-in-uk-1437180.html|title=US West pays 70m pounds for Thomson Directories: American telephone company continues to develop multimedia in UK |last=Fagan|first=Mary|date=20 May 1994|work=The Independent|accessdate=2009-08-15}}</ref> In 1999, the company was sold by [[3i]] to [[TDL Infomedia]], a subsidiary of [[Apax Partners]].<ref>{{cite news|url=http://www.independent.co.uk/news/business/3i-sells-thomson-guides-for-pounds-220m-1109799.html|title=  3i sells Thomson guides for pounds 220m |last=Baker|first=Lucy|date=31 July 1999|work=The Independent|accessdate=2009-08-15}}</ref>\n\nThe company was placed in [[Administration (law)]] in August 2013, and acquired by Corporate Media Partners.<ref>{{cite news|url=http://www.bbc.co.uk/news/uk-england-hampshire-23710958|title= Thomson Local directory firm goes into administration|work=BBC News|accessdate=2013-09-11}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.thomsondirectories.com/ Official Thomson Directory Site]\n\n[[Category:Companies of the United Kingdom]]\n[[Category:Directories]]\n[[Category:Apax Partners companies]]\n[[Category:3i Group companies]]\n\n\n{{UK-company-stub}}"]
['Thomas Register', '4755498', '[[Image:ThomasRegister.png|frame|1905 \'\'Thomas\' Register of American Manufacturers\'\']]\nThe \'\'\'\'\'Thomas Register of American Manufacturers\'\'\'\'\', now \'\'\'ThomasNet\'\'\', is an online platform for supplier discovery and product sourcing in the USA and Canada. It was once known  as the "big green books" and "Thomas Registry", and was a multi-volume [[Yellow Pages|directory]] of [[Industry|industrial]] product information covering 650,000 [[distributors]], [[manufacturers]] and service companies within 67,000-plus [[industry|industrial]] categories that is now published on ThomasNet.\n\n==History==\nThe books were first published in 1898 by Harvey Mark Thomas as \'\'Hardware and Kindred Trades. \'\'In their heyday, \'\'\'\'\'Thomas Register of American Manufacturers \'\'\'\'\' was a 34-volume, 3 section buying guide offering sourcing information on industrial products and services, along with comprehensive specifications and detailed product information from thousands of manufacturers. The Thomas Regional Directory Company began as a division of Thomas Publishing in 1976. Thomas Regional Regional Industrial Buying Guides provided information in print and on CD-ROM,  on local OEMs, distributors, MRO services and other custom manufacturing services in 19 regional editions covering much of the United States. Thomas Register and Thomas Regional were available online from the mid 1990s. The company stopped publishing its print products in 2006.\n\nThomas has moved its database [[online]] as ThomasNet, published and maintained by Thomas Industrial Network, one of Thomas’ five business units. ThomasNet has expanded to provide not only product and company information, but also [[Online shopping|online catalog]]s, [[computer-aided design]] ([[Computer-aided design|CAD]]) drawings, [[news]], [[press releases]] and [[blogs]].\n\n==Thomas Publishing Company, LLC==\n\nThomas Publishing Company, LLC of [[New York City]] has been [[privately held]] since its inception. It used independent representatives to sell advertising space around its listings in print products like the Thomas Register and the Thomas Industrial Regional Directories, and these representatives continue to sell Internet related products to manufacturers, distributors, and other companies.\n\n==ThomasNet==\n\nThomasNet is an information and technology company based in New York City. In April 2006 the [[New York Public Library]] named ThomasNet.com as one of its [http://www.nypl.org/branch/books/index2.cfm?ListID=300 25 Best of Reference] sources for the [[reference librarian]], and is currently listed in their [http://www.nypl.org/weblinks/1382 Best of the Web] list for Industry Information.\n\nSince November 2010, ThomasNet has been a founding partner of GlobalTrade.net, a marketplace for international trade service providers.\n\n==ThomasNet News==\nThomasNet News is a product of Thomas Publishing Company, LLC. ThomasNet News was introduced with “the mission of delivering timely, new industrial product information covering the whole range of products …” It manually reviews press releases submitted through the website and publishes with a small description in one of 51 different categories.\n\nIn 2000, ThomasNet News released Industry Market Trends (IMT), its first Journal. In the IMT, editors published editorials, interviews, and long form journalism on issues ranging from career skills, developments in the industry, and discussions with leading experts. Soon after, IMT Green & Clean was launched in response to the growing interest in green technology and its impact on the world. In 2011, the IMT Machining Journal was launched followed by the IMT Fluid & Gas Flow Journal, the IMT Career Journal, and the IMT Procurement Journal.\n\n==Research==\nStarting in 2010, ThomasNet began reaching out to its database of manufacturers to get a better understanding of where the community was, where their shortcomings were, and where they saw the landscape going in the future. This yearly survey is called the Industry Market Barometer.\n\n== External links ==\n* {{Official website}}\n\n[[Category:Promotion and marketing communications]]\n[[Category:Marketing books]]\n[[Category:Directories]]']
['Vsya Moskva', '11017700', '{{italic title}}\n{{unreferenced|date=December 2008}}\n\'\'\'\'\'Vsya Moskva\'\'\'\'\' (literally translated "\'\'All Moscow\'\'" or "\'\'The Entire Moscow\'\'") was a series of [[city directories]] of [[Moscow]], Russia, published on a yearly basis from 1872 to 1936 by [[Aleksei Sergeevich Suvorin]]. \nThe directories contained detailed lists of private residents, names of streets and squares across the city with the details of their occupants and owners, government offices,  public services and medium and large businesses present in the city. Each volume was anywhere between 500 and 1500 pages long.  They are often used by [[genealogists]] for family research in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historian]]s use them to research the [[social histories]] of the city.\n\n==List of residents==\nEach directory was written exclusively in Russian Cyrillic only, and contains various sections among which was an [[alphabetical]] list of residents in the city. Those listed usually were the head of their respective household and so spouses and minors are not listed. \n\nThe following information can be found:\n*Person\'s surname and first name\n*[[Patronymic]]\n*Street address with apartment number\n*[[Profession]]\n*Telephone numbers (few private residents could afford a [[telephone]] before 1918)\n\n==List of occupants of each building on every street and square==\nA section immediately preceding or following that listing residents in alphabetical order was a directory of all streets, houses and flats with the names of their owners and occupants. In this way readers could determine all those people who lived on a particular street of in a certain apartment block.\n\n==Other sections==\nThe following information can also be found in each directory:\n\n*Maps of the city\n*Interior theater seating plan layouts\n*Lists of personnel in state, public and private institutions\n*Original Advertising\n\n== Interruption in the series ==\n\nNo volumes were published in the following years:\n*1918\n*1919\n*1920\n*1921\n\nThis was due to the events of the [[Russian revolution of 1917]] and the subsequent [[Russian civil war]].\n\n== Termination of series ==\n\nPublication came to a halt after the edition of 1936, coinciding with the time of [[Joseph Stalin]]\'s [[great purge]]s and [[Moscow Trials]].\n\n==Historical and genealogical value==\nBecause numerous residents emigrated from Moscow after the [[Russian Revolution of 1917]] and tens of thousands more were either arrested, shot, or sent to the [[gulag]] by the [[Cheka]] and the [[NKVD]] after 1918 the section detailing residents names is especially useful in determining until when a certain person was still living in the city, and under which address.\n\n==Availability==\nMany original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the [[United States]], [[Europe]] (including [[The Baltics]], [[Finland]] the [[United Kingdom]] and [[Germany]]) however most only have an incomplete collection.\n\n==Other city directories in Russia==\nSuvorin also published city directories for [[Saint Petersburg]] under the title \'\'[[Ves Petersburg]]\'\' (\'\'All Petersburg\'\') for the years 1894 to 1940 and for the whole country under the titles \'\'[[Vsya Rossiya]]\'\' (\'\'All Russia\'\') from 1895 to 1923 and continued under than name \'\'[[Ves SSSR]]\'\' (\'\'All USSR\'\') from 1924 to 1931.\n\n== See also ==\n\n*\'\'[[Ves Petersburg]]\'\'\n*\'\'[[Vsya Rossiya]]\'\'\n\n== External links ==\n*[http://surname.litera-ru.ru/ A Russian website offering a search engine in Cyrillic for some city directories.]\n\n[[Category:Directories]]\n[[Category:History of Moscow]]\n[[Category:Russian non-fiction books]]\n[[Category:Media in Moscow]]\n[[Category:1872 books]]']
['Yellowikis', '1430812', '{{Infobox Website\n| name = Yellowikis\n| favicon =\n| logo = [[Image:Yelwiki.png]]\n| screenshot =\n| url = [http://yellowikis.wikia.com/wiki/Main_Page yellowikis.wikia.com]\n| commercial =\n| alexa =\n| type = [[MediaWiki]]\n| registration = Optional\n| owner = [[Wikia]]\n| author =\n| launch date = 2005\n| current status = Inactive\n| revenue =\n}}\n\'\'\'Yellowikis\'\'\' was a [[MediaWiki]] [[website]] collecting basic information about businesses. This information included basic contact details such as company name, address, websites, and telephone numbers, as well as internal Yellowiki [[wikilink]]s to competitors. Yellowikis was launched in January 2005. {{As of|2011|3}}, the Yellowikis main page had been translated into more than 25 different languages.{{citation needed|date=April 2015}}\n\nSome users also entered a number of codes including a two letter country code as well as an [[International Standard Industrial Classification]], [[North American Industry Classification System|North American Industry Classification]] or US [[Standard Industrial Classification]]. Some users are also adding [[geocode]]s and [[Skype]] ids.\n\n==Legal issues==\nA commercial business listing company, [[Yell Limited]], requested that the founders of Yellowikis, Paul Youlten and Rosa Blaus, amend their site, claiming that Yellowikis was "passing itself off" as being associated with Yell.com and that people would confuse the two organisations.<ref>{{cite news\n | title =Legal threat to wiki listing site\n | work =BBC News\n | date = 12 July 2006\n | url =http://news.bbc.co.uk/1/hi/technology/5169674.stm\n | accessdate =2006-07-12 }}</ref><ref>{{cite news|title=Teenager faces action over listings website|author=Bobbie Johnson|date=2006-08-02|work=[[The Guardian]]|url=https://www.theguardian.com/uk_news/story/0,,1835233,00.html|publisher=Guardian News and Media Ltd | location=London}}</ref><ref>{{cite news|title=Yell threatens to sue wiki rival|author=Jane Hoskyn|work=vnunet.com|date=2006-07-14|url=http://www.vnunet.com/vnunet/news/2160380/yell-threatens-sue-wiki-rival|publisher=VNU Business Publications Ltd|archiveurl=https://web.archive.org/web/20070930195439/http://www.vnunet.com/vnunet/news/2160380/yell-threatens-sue-wiki-rival|archivedate=2007-09-30}}</ref> This might be considered to be anti-competitive behaviour/anti-competitive in the eyes of certain commentators, however, such claim is unlikely to carry water from a legal perspective. Yell\'s claim is given considerable weight by the slogan on Yellowiki\'s front page that they are "Yellow Pages for the 21st Century" although in their public protestations, Yellowikis claim that they are not trying to create association between themselves and Yellow Pages.<ref>{{cite web\n | last =The Yellowikis Community\n | title =Response to Yell\n | work =\n | publisher =Yellowikis\n | year =2006\n | url =http://www.yellowikis.org/wiki/index.php/Response_to_Yell\n | accessdate =2006-07-14 }}</ref>\n\n[[Yellow Pages]] is a registered [[trade mark]] in many countries including the UK. In some territories, however, the mark has lost its distinctiveness as a source of origin of goods and services.\n\nFrom 9 to 14 October 2006, the domain address redirected to the new [http://www.owikis.org.uk/ Owikis] website, which stated "The trademark dispute between Yell Limited and Paul Youlten concerning the Yellowikis website has been satisfactorily resolved".\n\nOn 15 October 2006, the Yellowikis website reappeared, with the explanation that [[United Kingdom]] users would have to use Owikis, with the word \'\'Yell\'\' from the domain name and the color yellow from the logo; international users could continue to use Yellowikis. {{As of|2008|5}}, the Owikis site is not yet available.\n\nAs of at least May 2014, the [http://yellowikis.wikia.com/wiki/Main_Page Wikia page] is dead.<ref>{{cite web|url=http://yellowikis.wikia.com/wiki/Main_Page |title=Web Archive |accessdate=2014-05-28 |deadurl=yes |archiveurl=https://web.archive.org/web/20140109015130/http://yellowikis.wikia.com/wiki/Main_Page |archivedate=January 9, 2014 }}</ref>\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{cite news|url=http://www.researchbuzz.org/2005/06/business_information_in_wiki_f.shtml|title=Business Information in Wiki Format|date=2005-06-22|publisher=ResearchBuzz}}\n* {{cite news|url=http://competia.com/competia_w/site/fiche/1954|title=Yellowikis|date=2005-07-26|publisher=Competia}}\n* {{cite web|url=http://alina_stefanescu.typepad.com/totalitarianism_today/2005/05/a_wiki_worth_wa.html|title=A wiki worth watching|work=totalitarianism today|accessdate=2005-10-07}}\n* {{cite web|url=http://www.stabani.com/index.php?s=yellowikis|title=Why I think Yellowikis is a good idea |work=site spotlight|accessdate=2006-01-16|author=S.Tabani}}\n* [[n:Emily Chang|Emily Chang]] {{cite web|url=http://www.emilychang.com/go/ehub/|title=Emily Chang\'s eHub|work=eHub|accessdate=2005-10-09}} \n* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=58|title=Yellowikis - A Case Study of a Web 2.0 Business, Part 1|date=2005-10-15}}\n* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=59|title=Yellowikis: a Web 2.0 Case Study, Part 2 - Industry Disruption and The Competition|date=2005-10-16}}\n* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=62|title=Yellowikis: Demonstrating Web 2.0 principles|date=2005-10-17}}\n* {{cite news|url=http://news.independent.co.uk/media/article1096343.ece|title=New Media: Who are the real winners now we\'ve all gone Wiki-crazy?|date=2006-06-26|publisher=[[The Independent]] | location=London | first=Kathy | last=Marks}}\n* [[n:Yell threatens to shut down Yellowikis|Yell threatens to shut down Yellowikis]] from [[Wikinews]], 2006-07-05\n\n==External links==\n* [http://yellowikis.wikia.com/wiki/Main_Page Yellowikis] on [[Wikia]]\n\n[[Category:Directories]]\n[[Category:MediaWiki websites]]\n[[Category:Internet properties established in 2005]]\n[[Category:Yellow pages]]']
['Annuario Pontificio', '2940988', '{{italic title}}\n{{Refimprove|date=April 2010}}\n{{Infobox book\n| name = Annuario Pontificio \n| title_orig =\n| translator =\n| image = Annuario Pontificio 2008 (MK).jpg\n| caption =\n| author = [[Libreria Editrice Vaticana]], Secretary of State\n| illustrator =\n| cover_artist =\n| country = [[Vatican City|Vatican]]\n| language = Italian\n| series =\n| subject =\n| genre = [[Annual publication]], [[Reference]]\n| publisher = [[Holy See]]\n| pub_date = December 2, 2014\n| media_type = Printed Book\n| pages = \n| isbn = 9788820997472\n| isbn_note = <ref>[http://www.libreriaeditricevaticana.va/content/libreriaeditricevaticana/it/news-ed-eventi/annuario-pontificio-2016.html 2016]</ref>\n| oclc= \n| preceded_by = Annuario Pontificio 2015\n| followed_by = Annuario Pontificio 2016\n}}\nThe \'\'\'\'\'Annuario Pontificio\'\'\'\'\' ([[Italian language|Italian]] for \'\'Pontifical Yearbook\'\') is the annual directory of the [[Holy See]]. It [[List of popes|lists all the popes]] to date and all officials of the Holy See\'s [[dicastery|departments]]. It also gives complete lists, with contact information, of the [[Cardinal (Catholicism)|cardinals]] and [[Catholic Church|Catholic]] bishops throughout the world, the [[diocese]]s (with statistics about each), the departments of the [[Roman Curia]], the Holy See\'s [[diplomatic mission]]s abroad, the [[embassy|embassies]] accredited to the Holy See, the headquarters of [[religious institute]]s (again with statistics on each), certain academic institutions, and other similar information. The index includes, along with all the names in the body of the book, those of all priests who have been granted the title of "[[Monsignor]]".\nAs the title suggests, the red-covered yearbook, compiled by the Central Statistics Office of the Church and published by [[Libreria Editrice Vaticana]], is mostly in Italian.\n\nThe 2015 edition has more than 2,400 pages and costs {{€|78}}.<ref>{{cite web |url=http://www.vaticanum.com/en/annuario-pontificio-2015-book-2|access-date=April 5, 2016|title=Annuario Pontificio 2015|publisher=Città del Vaticano}}</ref> According to the \'\'Pontifical Yearbook of 2010\'\', the number of Catholics in the world increased from 1,147 million to 1,166 million between 2007 and 2008, a growth of 1.7 percent.<ref>{{cite web| url=http://www.zenit.org/article-28425?l=english |title=Number of Catholics Increases Worldwide: 2010 "Annuario" Shows Growth in Asia and Africa |publisher=Zenit |date=February 21, 2010 |accessdate=April 11, 2010}}</ref> By the \'\'Yearbook of 2016\'\' it was 1,272,281,000 at the end of 2014.\n\n==History==\nA [[yearbook]] of the Catholic Church was published, with some interruptions, from 1716 to 1859 by the Cracas printing firm in Rome, under the title (in Italian) \'\'Information for the Year ...\'\' From 1851, a department of the Holy See began producing a different publication called (in Italian) \'\'Hierarchy of the Holy Catholic Apostolic Church Worldwide and in Every Rite, with historical notes\'\', which took the title \'\'Annuario Pontificio\'\' in 1860, but ceased publication in 1870. This was the first yearbook published by the Holy See itself, but its compilation was entrusted to the newspaper \'\'[[Giornale di Roma]]\'\'. The publishers "Fratelli Monaldi" (Monaldi Brothers) began in 1872 to produce their own yearbook entitled (in Italian) \'\'The Catholic Hierarchy and the Papal Household for the Year ... with an appendix of other information concerning the Holy See\'\'.\n\nThe [[Holy See Press Office|Vatican Press]] took this over in 1885, thus making it a semi-official publication.  It bore the indication "official publication" from 1899 to 1904, but this ceased when, giving the word "official" a more restricted sense, the \'\'Acta Sanctae Sedis\'\', forerunner of the \'\'[[Acta Apostolicae Sedis]]\'\', was declared the only "official" publication of the Holy See. In 1912, it resumed the title \'\'Annuario Pontificio\'\'. From 1912 to 1924, it included not only lists of names, but also brief illustrative notes on departments of the Roman Curia and on certain posts within the [[papal court]], a practice to which it returned in 1940.\n\nFor some years, beginning in 1898, the \'\'Maison de [[la Bonne Presse]]\'\' publishing house of [[Paris]] produced a similar yearbook in [[French language|French]] called \'\'Annuaire Pontifical Catholique\'\', not compiled by the Holy See. This contained much additional information, such as detailed historical articles on the [[Swiss Guards]] and the [[Apostolic Palace|Papal Palace]] at the [[Vatican City|Vatican]].\n\n== Statistical data ==\nAccording to the \'\'Annuario Pontificio 2012\'\' the statistical data given in the yearbook regarding [[archdiocese]]s and [[diocese]]s are furnished by the diocesan curias concerned and reflect the diocesan situation on 31 December of the year prior to the date on the yearbook, unless there is another indication.  The data recorded are shown in the following order next to these abbreviations:\n* Su – area in square kilometers of the diocesan territory\n* pp – population of the diocese\n* ct – number of Catholics\n* pr – parishes and quasi-parishes\n* ch – churches or mission stations\n* sd – secular priests resident in the diocese\n* dn – diocesan priests ordained during the year\n* sr – religious priests resident in the diocese\n* rn – religious priests ordained during the year\n* dp – permanent deacons\n* sm – seminarians taking courses of philosophy and theology\n* rm – members of men\'s religious institutes\n* rf – members of women\'s religious institutes\n* ie – educational institutes\n* ib – charitable institutes\n* ba – baptisms\n\n== See also ==\n{{Portal|Catholicism}}\n* [[Catholic Church by country]]\n* [[History of the papacy]]\n* [[Oldest popes]]\n* [[Vatican Publishing House]]\n\n==References==\n{{reflist}}\n\n==Bibliography==\n* Secretary of State, \'\'Annuario Pontificio 2010.\'\' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8355-0\n* Secretary of State, \'\'Annuario Pontificio 2009.\'\' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8191-4\n* Secretary of State, \'\'Annuario Pontificio 2008.\'\' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8021-4\n* Secretary of State, \'\'Annuario Pontificio 2007.\'\' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7908-9\n* Secretary of State, \'\'Annuario Pontificio 2006.\'\' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7806-8\n* Secretary of State, \'\'Annuario Pontificio 2005.\'\' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7678-1\n\n==External links==\n* [http://www.catholic-hierarchy.org/ CatholicHierarchy.org]\n* [http://www.gcatholic.org/ GCatholic.org]\n\n[[Category:Documents of the Catholic Church]]\n[[Category:Directories]]\n[[Category:Holy See]]']
['Boston Directory', '25348863', '{{italic title}}\n[[Image:1807 BostonDirectory title page.png|100px|thumb|1807 Boston Directory [[title page]]]]\n\'\'\'\'\'The Boston Directory\'\'\'\'\' of  [[Boston]], [[Massachusetts]], was first published in 1789. It contained "a list of the merchants, mechanics, traders, and others, of the town of Boston; in order to enable strangers to find the residence of any person." Also included were listings for public officials, doctors, bank directors, and firemen.<ref>Boston Directory. 1789.</ref> The directory was issued annually after 1825; previously it had appeared irregularly.\n\nThe number of listings in each directory reflected fluctuations in the population size of Boston. In 1789, the directory included some 1,474 listings; by 1875, there were 126,769.<ref name="auto">Advertisement for Boston Directory. Boston Almanac, 1875.</ref>\n\nPublishers included John Norman (1789); John West (1796-1803); Edward Cotton (1805-1818); Charles Stimpson (1820-1846); George Adams (1846-1857);<ref>{{citation |url=https://books.google.com/books?id=Ors-AAAAYAAJ&pg=PA87 |year=1866 |title=New England Historical & Genealogical Register }}</ref> Adams, Sampson & Co. (1858-1865); Sampson, Davenport & Co. (1865-1884); Sampson, Murdock & Co. (1885-1903); Sampson & Murdock Co. (1904-ca.1930); [[R.L. Polk & Co.]] (1944-ca.1980).<ref name="auto"/><ref>{{cite web|url=http://www.worldcat.org/oclc/228685309|title=The Boston directory ... including all localities within the city limits, as Allston, Brighton, Charlestown, Dorchester, Hyde Park, Roslindale, Roxbury, West Roxbury ...|work=worldcat.org}}</ref>\n{{TOC right}}\n\n==Boston Directories==\n\n===18th century===\n{| class="wikitable" style="font-size: 95%;"\n!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other\n|-\n| Boston Directory\n| <!-- PUBLISHER -->\n| 1789\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectory00sampgoog#page/n10/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston Directory\n| <!-- PUBLISHER -->\n| 1796\n| [https://books.google.com/books?id=CJFIAAAAYAAJ&client=safari&pg=RA1-PA215#v=onepage&q=&f=false reprint via Google Books, p.&nbsp;215-302]\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/1 via Boston Athenæum]\n|-\n| Boston Directory\n| <!-- PUBLISHER -->\n| 1798\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/2 via Boston Athenæum]\n|}\n\n===19th century===\n\n====1800-1829====\n{| class="wikitable" style="font-size: 95%;"\n!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other\n|-\n| Boston Directory\n| <!-- PUBLISHER -->\n| 1800\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/3 via Boston Athenæum]\n|-\n| Boston Directory\n| John West\n| 1803\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/4 via Boston Athenæum]\n|-\n| Boston Directory\n| Edward Cotton\n| 1805\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/010363295 via HathiTrust]\n| [https://archive.org/stream/bostondirectory00inbost#page/n9/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Edward Cotton\n| 1806\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/5 via Boston Athenæum]\n|-\n| Boston Directory\n| Edward Cotton\n| 1807\n| <!-- GOOGLE -->\n| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Edward Cotton\n| 1809\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/7 via Boston Athenæum]\n|-\n| Boston Directory\n| Edward Cotton\n| 1810\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/8 via Boston Athenæum]\n|-\n| Boston Directory\n| Edward Cotton\n| 1813\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/9 via Boston Athenæum]\n|-\n| Boston Directory\n| Edward Cotton\n| 1816\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/10 via Boston Athenæum]\n|-\n| Boston Directory\n| Edward Cotton\n| 1818\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/11 via Boston Athenæum]\n|-\n| Boston Directory\n| Frost and Stimpson\n| 1820\n| <!-- GOOGLE -->\n| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Frost and Stimpson\n| 1821\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/12 via Boston Athenæum]\n|-\n| Boston Directory\n| Frost and Stimpson\n| 1822\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/15 via Boston Athenæum]\n|-\n| Boston Directory\n| Frost and Stimpson\n| 1823\n| [https://books.google.com/books?id=nY4vAAAAYAAJ via Google Books]\n| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Frost and Stimpson\n| 1825\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/010363295 via HathiTrust]\n| [https://archive.org/stream/bostondirectorys1825bost via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Frost and Stimpson\n| 1826\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/17 via Boston Athenæum]\n|-\n| Boston Directory\n| Hunt, Stimpson, and Frost\n| 1827\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/18 via Boston Athenæum]\n|-\n| Boston Directory\n| Hunt and Stimpson\n| 1828\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/19 via Boston Athenæum]\n|-\n| Boston Directory\n| Charles Stimpson, Jr.\n| 1829\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/20 via Boston Athenæum]\n|-\n|}\n\n====1830-1849====\n{| class="wikitable" style="font-size: 95%;"\n!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other\n|-\n| Boston Directory\n| <!-- PUBLISHER -->\n| 1830\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/41 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1831\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/stimpsonsbostond3132adam#page/n29/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1832\n| [https://books.google.com/books?id=raQtAAAAYAAJ via Google Books]\n| [http://catalog.hathitrust.org/Record/010363296 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1833\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/27 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1834\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/010363296 via HathiTrust]\n| [https://archive.org/stream/bostondirectory01bost#page/n5/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1835\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectory03bost#page/n5/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1836\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/010363296 via HathiTrust]\n| [https://archive.org/stream/stimpsonsbostond1836adam#page/n21/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1837\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/34 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1838\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/35 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1839\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/40 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1840\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/39 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1841\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/37 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1842\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/36 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1843\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/38 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1844\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/45 via Boston Athenæum]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1845\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/44 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]\n|-\n| Stimpson\'s Boston Directory\n| Stimpson & Clapp\n| 1846\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/43 via Boston Athenæum]\n|-\n| Adams\'s New Directory of the City of Boston\n| George Adams\n| 1846-47\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/25 via Boston Athenæum]\n|-\n| Adams\'s Boston Directory\n| French and Stimpson\n| 1847-48\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/24 via Boston Athenæum]\n|-\n| Boston Directory\n| French and Stimpson\n| 1848-49\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]\n| [https://archive.org/stream/bostondirectory4849bost#page/n7/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston Directory\n| George Adams\n| 1849-50\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| [https://archive.org/stream/bostondirectory00bost#page/n7/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n|}\n\n====1850-1869====\n{| class="wikitable" style="font-size: 95%;"\n!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other\n|-\n| Directory of the City of Boston\n| George Adams\n| 1850\n| [https://books.google.com/books?id=UHDPAAAAMAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| George Adams\n| 1851\n| [https://books.google.com/books?id=C6UqAAAAYAAJ via Google Books]\n| [http://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| George Adams\n| 1852\n| [https://books.google.com/books?id=2tsCAAAAYAAJ via Google Books]\n| <!-- HATHI -->\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| George Adams\n| 1853\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/48 via Boston Athenæum]\n|-\n| Boston Directory\n| George Adams\n| 1854\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/81 via Boston Athenæum]\n|-\n| Boston Directory\n| George Adams\n| 1855\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/82 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]\n|-\n| Boston Directory\n| George Adams\n| 1856\n| [https://books.google.com/books?id=zYMqAAAAYAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| George Adams\n| 1857\n| [https://books.google.com/books?id=nYIqAAAAYAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Adams, Sampson, & Co.\n| 1858\n| [https://books.google.com/books?id=En4qAAAAYAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Adams, Sampson, & Co.\n| 1859\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Adams, Sampson, & Co.\n| 1860\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/87 via Boston Athenæum]\n|-\n| Boston Directory\n| Adams, Sampson, & Co.\n| 1861\n| [https://books.google.com/books?id=hHwqAAAAYAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/singleitem/collection/p16057coll32/id/93/rec/57 via Boston Athenaeum]\n|-\n| Boston Directory\n| Adams, Sampson, & Co.\n| 1862\n| [https://books.google.com/books?id=tH4qAAAAYAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/91 via Boston Athenæum]\n|-\n| Boston Directory\n| Adams, Sampson, & Co.\n| 1863\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/92 via Boston Athenæum]\n|-\n| Boston Directory\n| Adams, Sampson, & Co.\n| 1864\n| [https://books.google.com/books?id=8IEqAAAAYAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Adams, Sampson, & Co.\n| 1865\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/95 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1866\n| [https://books.google.com/books?id=_A5FAQAAMAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/96 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1867\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/97 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1868\n| [https://books.google.com/books?id=SFwJAQAAIAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1869\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/69 via Boston Athenæum]\n|-\n|}\n\n====1870-1889====\n{| class="wikitable" style="font-size: 95%;"\n!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1870\n| [https://books.google.com/books?id=GytFAQAAMAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/98 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1871\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/99 via Boston Athenæum]\n|-\n| Boston Commercial Directory\n| Wentworth & Co.\n| 1871\n| [https://books.google.com/books?id=xfACAAAAYAAJ via Google Books]\n| <!-- HATHI -->\n| <!-- IA -->\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1872\n| <!-- GOOGLE -->\n| [http://babel.hathitrust.org/cgi/pt?id=hvd.32044092998012;view=1up;seq=19 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/65 December] supplement via Boston Athenæum<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1873\n| [https://books.google.com/books?id=NqHNAAAAMAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/67 November], [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/66 December] supplements via Boston Athenæum\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1874\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/108 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1875\n| [https://books.google.com/books?id=EC5FAQAAMAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/104 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1876\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/105 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1877\n| [https://books.google.com/books?id=RTBFAQAAMAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/103 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1878\n| <!-- GOOGLE -->\n| [http://hdl.handle.net/2027/uc1.c047888986 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/110 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1879\n| <!-- GOOGLE -->\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/111 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1880\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/107 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1881\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/109 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1882\n| [https://books.google.com/books?id=NSFFAQAAMAAJ via Google Books]\n| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/112 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1883\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/113 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Davenport, & Co.\n| 1884\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/114 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1885\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/116 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1886\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/118 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1887\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/117 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1888\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/119 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1889\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/121 via Boston Athenæum]\n|-\n|}\n\n====1890-1899====\n{| class="wikitable" style="font-size: 95%;"\n!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1890\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/123 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1891\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/124 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1892\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/125 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1893\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/126 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1894\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/127 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1895\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/128 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1896\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/129 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1897\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/130 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1898\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/131 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1899\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/132 via Boston Athenæum]\n|-\n|}\n\n===20th century===\n\n====1900-1949====\n{| class="wikitable" style="font-size: 95%;"\n!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1900\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/133 via Boston Athenæum]\n|-\n| Boston Directory\n| Sampson, Murdock, & Co.\n| 1905\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]\n|-\n| Boston Directory\n| Sampson & Murdock Co.\n| 1916\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostonmassachuse1916112samp#page/n9/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston Register and Business Directory\n| Sampson & Murdock Co.\n| 1922\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostonregisterbu1922bost#page/n13/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston Directory\n| Sampson & Murdock Co.\n| 1925\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| <!-- IA -->\n| [http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]\n|-\n|}\n\n====1950-1999====\n{| class="wikitable" style="font-size: 95%;"\n!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other\n|-\n| Boston City Directory\n| R.L. Polk & Co.\n| 1955\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectoryi00bost#page/n7/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston City Directory\n| R.L. Polk & Co.\n| 1956\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectoryi56bost#page/n3/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston City Directory\n| R.L. Polk & Co.\n| 1959\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectoryi002bost via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston City Directory\n| R.L. Polk & Co.\n| 1961\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectoryi11961bost#page/n3/mode/2up v.1], [https://archive.org/details/bostondirectoryi261bost v.2] via Internet Archive\n| <!-- OTHER -->\n|-\n| Boston City Directory\n| R.L. Polk & Co.\n| 1962\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectoryi162bost#page/n3/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston City Directory\n| R.L. Polk & Co.\n| 1965\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectoryi11965bost#page/n3/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston City Directory\n| R.L. Polk & Co.\n| 1966\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectoryi1966bost#page/n3/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston City Directory\n| R.L. Polk & Co.\n| 1969\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectoryi169bost#page/n3/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n| Boston City Directory\n| R.L. Polk & Co.\n| 1970\n| <!-- GOOGLE -->\n| <!-- HATHI -->\n| [https://archive.org/stream/bostondirectoryi170bost#page/n3/mode/2up via Internet Archive]\n| <!-- OTHER -->\n|-\n|}\n\n==See also==\n* \'\'[[Boston Almanac|Boston Almanac and Business Directory]]\'\'\n* \'\'[[Boston Almanac|Boston Register and Business Directory]]\'\'\n* \'\'[[Massachusetts Register]]\'\'\n\n==References==\n{{reflist}}\n\n==Further reading==\n* [https://books.google.com/books?id=ALIUAAAAYAAJ New England historical and genealogical register]. Oct. 1862; p.&nbsp;387+\n* [https://books.google.com/books?id=CJFIAAAAYAAJ Report of the record commissioners of the city of Boston], Volume 10. Rockwell and Churchill, 1886; p.&nbsp;163+\n* {{Citation |publisher = Williams Directory Co. |publication-place = Cincinnati, Ohio |author = A.V. Williams |title = The Development and Growth of City Directories |publication-date = 1913 |chapter=Boston, Massachusetts |chapterurl=http://hdl.handle.net/2027/nyp.33433082423645?urlappend=%3Bseq=61 }}\n\n==External links==\n* HathiTrust. [https://catalog.hathitrust.org/Record/010363295 1805 etc]; [https://catalog.hathitrust.org/Record/000499337 1849-1883]\n* [http://www.damrellsfire.com/cgi-bin/directory_search.pl damrellsfire.com]\n* [http://cdm.bostonathenaeum.org/cdm/landingpage/collection/p16057coll32 Boston Athenæum: The Boston Directory 1789-1900 (Ongoing Project), Digital Collection].\n\n[[Category:History of Boston]]\n[[Category:Directories]]\n[[Category:Publications established in 1789]]\n[[Category:18th century in Boston]]\n[[Category:19th century in Boston]]\n[[Category:20th century in Boston]]\n[[Category:1789 establishments in Massachusetts]]']
['Category:Directory assistance services', '28706812', '[[Category:Directories]]\n[[Category:Telephone services]]\n[[Category:Telephone service enhanced features]]\n[[Category:Information by telephone]]']
['Who Owns Whom', '24167579', "'''''Who Owns Whom''''' is a set of annual directories published by GAP Books in association with [[Dun & Bradstreet]] (D&B).  They provide the relationship between companies worldwide showing who is the ultimate [[parent company]] and who are their [[subsidiaries]].  Details include parent name, address and telephone number, country of incorporation and [[SIC code]] for each ultimate parent company along with the names of the subsidiaries, where they are based, and who owns whom for each subsidiary.  The set of directories are broken down into seven geographic regions: UK & Ireland; West Europe; North Europe; South, Central & East Europe; North & South America; Australasia, Asia, Africa & Middle East.\n\n''Who Owns Whom'' was first published in 1958 by D&B, who still own the rights to the data.  Within the set of ten directories are listed approximately 2,500,000 [[corporate groups]], ranging from companies with hundreds of members across all continents to single-country groups with only a handful of members.\n\n==Criteria for entry==\nIn order to maintain full coverage, all entries in ''Who Owns Whom'' are published entirely [[free of charge]].  Every effort is made by D&B, who supply the data, to include all corporate groups within the coverage area.  The corporate group may be a vast [[multinational corporation|multinational]] with a range of subsidiaries spanning many different countries and industries, or it may consist of two companies – a parent and a subsidiary.  There is no size criterion for entry.  All corporate groups, which come to the attention of D&B, are included regardless of their [[revenue|sales turnover]] or number of employees.  Companies and organisations are only included if ownership is greater than 50% by other companies or organisations.\n\n==Coverage==\nAll types of industries are covered, including [[agriculture]], [[forestry]] and [[fishing]], [[mining]], [[construction]], [[manufacturing]], [[transportation]], [[communication]] and [[public utilities]], [[wholesale]], [[retail]], [[finance]] and [[insurance]], [[public services|services]] and [[public administration]].  Both public and private companies are covered, along with many companies owned by official bodies such as governments, nationalised industries and state holding companies which have subsidiaries, but are not themselves independently registered.\n\n==External links==\n* http://library.dialog.com/bluesheets/html/bl0522.html\n* http://www.gapbooks.com/shop/dandb-who-owns-whom/\n* http://www.loc.gov/rr/business/duns/duns30.html\n\n[[Category:Directories]]\n[[Category:Yearbooks]]\n[[Category:Corporate subsidiaries]]\n[[Category:Books about multinational companies]]\n\n\n{{globalization-book-stub}}\n{{Ref-book-stub}}"]
['Major Information Technology Companies of the World', '33232338', "The '''Major Information Technology Companies of the World''' is a directory of [[information technology]] companies published by [[Graham & Whiteside]] annually since 1997. The directory contains over 8000 companies.<ref>{{cite web|url=http://www.gale.cengage.com/servlet/BrowseSeriesServlet?region=9&imprint=000&cf=ps&titleCode=MITCW|title=Major Information Technology Companies of the World|publisher=Cengage|accessdate=27 September 2011}}</ref> The directory is also available online as part of the Gale Directory Library.<ref>{{cite web|url=http://www.gale.cengage.com/pdf/facts/GML25909_MITCOW_Major_GDL.pdf|title=Major Information Technology Companies of the World|publisher=Gale|accessdate=27 September 2011|year=2009}}</ref>\n\n==See also==\n*[[Corporate Technology Directory]]\n\n==References==\n{{reflist}}\n\n[[Category:Directories]]\n\n\n{{technology-stub}}"]
['Mobile social address book', '20893498', "\nA '''mobile social address book''' is a [[phonebook]] on a [[mobile device]] that enables [[subscribers]] to build and grow their [[social networks]]. The mobile social address book transforms the phone book on any standard mobile phone into a social networking platform that makes it easier for subscribers to exchange contact information.<ref>[http://www.wirelessweek.com/article.aspx?id=163626 Wireless Week, retrieved 2008-12-29]</ref> The mobile social address book is the convergence of [[personal information management]] (PIM) and social networking on a mobile device. While standard mobile phonebooks force users to manually enter contacts, mobile social address books automate this process by enabling subscribers to exchange contact information following a call or SMS.<ref>[http://www.computerworld.com/action/article.do?command=viewArticleBasic&articleId=9115165 Computerworld, retrieved 2008-11-5]</ref> The contact information exchange occurs instantaneously and the user’s phonebook updates automatically. Mobile social address books also provide dynamic updates of contacts if their numbers change over time.\n\n== History ==\nMobile social address books began appearing in 2007 as a parallel social trend to the emergence of Internet-based social networking sites like [[Facebook]], [[MySpace]] and [[LinkedIn]], establishing a new paradigm for interpersonal contact and communication. Mobile social address books sought to bring the connectivity of social networking to the in-the-moment experience of the mobile phone. Users can easily exchange contact information regardless of their handset, mobile carrier, or social networking application they use.<ref>[http://latestgeeknews.blogspot.com/2008/02/social-address-booknext-killer-app-part.html Latest Geek News, retrieved 2008-11-5]</ref>\n\nExamples of emerging companies providing technology to support mobile social address books include: [[PicDial]] (which dynamically augments the existing address book with pictures and status from Facebook, MySpace and Twitter, integrates with the call screen so during every call you see the latest picture and status of whoever is calling.  It is a network address book so everything can be managed from Windows or Mac as well and lastly you can also set your one callerID picture and status for your friends to see when you call them) FusionOne (whose backup and synchronization solutions lets users easily transfer and update mobile content, including contact information, among different devices); [[Loopt]] (whose Loopt service provides a social compass alerting users when friends are near); OnePIN (whose CallerXchange person-to-person contact exchange service lets users share contact info with one click on the mobile phone); and VoxMobili (whose Phone Backup and Synchronized Address Book solutions let users safeguard and synchronize their contact information among different devices).\n\n== References ==\n<references />\n\n==External links==\n* [http://www.loopt.com Loopt website]\n* [http://www.onepin.com OnePIN website]\n* [http://www.voxmobili.com VoxMobili website]\n* [http://www.picdial.com PicDial website]\n\n{{Mobile phones}}\n\n[[Category:Social networks]]\n[[Category:Directories]]"]
["Novel & Short Story Writer's Market", '7520269', '{{italic title}}\n[[File:Mosko.jpg|thumb|200px|right|\'\'Novel & Short Story Writer\'s Market\'\']]\n\n\'\'\'\'\'Novel & Short Story Writer\'s Market\'\'\'\'\' (\'\'NSSWM\'\') is an annual resource guide for fiction writers that compiles hundreds of listings for book publishers, magazines literary agents, writing contests, and conferences. \'\'NSSWM\'\' is published by [[Writer\'s Digest Books]] and usually hits bookstores around August of each year.\n\n==The Market Listings==\nFor 26 years, \'\'NSSWM\'\' has listed hundreds of U.S. and international magazines and book publishers who are open to submissions from fiction writers. Listings provide current contact information, editorial needs, schedules, submission guidelines, and payment and contract terms. All listings are updated annually.\n\n==The Articles==\nIn addition to the market listings, the book contains interviews with and essays by best-selling and award-winning writers, as well as editors and agents.\n\n==Writer\'s Digest Books==\n[[File:TheFaulknerPortable.jpg|350px|right|thumb|A copy of the 1939 edition of \'\'Writer\'s Market\'\' rests next to William Faulkner\'s [[Underwood Typewriter Company|Underwood]] Universal Portable typewriter in his office at his home, [[Rowan Oak]], which is now maintained by the [[University of Mississippi]] in [[Oxford, Mississippi|Oxford]] as a museum.]]\'\'Novel & Short Story Writer\'s Market\'\' is one of eight "[[Market (economics)|market]] books" published each year by [[Writer\'s Digest Books]] - the most famous of which is \'\'[[Writer\'s Market]]\'\', a book that lists thousands of magazine and book publishers listings for writers. Others include: \'\'Photographer\'s Market\'\', \'\'Children\'s Writer\'s and Illustrator\'s Market\'\', \'\'Guide to Literary Agents\'\', \'\'Artist and Graphic Designer\'s Market\'\', \'\'Poet\'s Market\'\' and \'\'Songwriter\'s Market\'\'. Each book is designed to give creatives instructions on how to submit work for publication.\n\n==See also==\n* [[Publishing]] \n* \'\'[[Writer\'s Digest]]\'\'\n* \'\'[[Writer\'s Market]]\'\'\n* \'\'[[Writers\' & Artists\' Yearbook]]\'\'\n* [[Literary agent#Querying|query]]\n* [[royalties]]\n* [[Authors Guild]]\n\n==External links==\n* [http://www.writersdigest.com/competitions Official site for the competitions of Writer\'s Digest Books]\n* [http://www.writersdigest.com \'\'Writer\'s Digest\'\' magazine official site]\n* [http://www.fwpublications.com F+W Publications - parent company of Writer\'s Digest Books]\n\n{{DEFAULTSORT:Novel and Short Story Writer\'s Market}}\n[[Category:Directories]]']
["Slater's Directory", '40452652', '#REDIRECT[[Isaac Slater]]\n\n[[Category:Directories]]']
["Trow's Directory", '41863504', '#REDIRECT[[John Fowler Trow]]\n\n[[Category:Directories]]\n[[Category:Books about New York City]]\n[[Category:History of New York City]]']
['Category:Library cataloging and classification', '7117782', '{{Commons cat|Library cataloging and classification}}\n\n{{Cat main|Library catalog|Library classification|Inventory (library)}}\n\n[[Category:Library science|Cataloging and classification]]\n[[Category:Classification systems]]\n[[Category:Metadata]]\n[[Category:Directories]]']
['Category:Public records', '20252940', '{{Cat main|Public records}}\n\n[[Category:Directories]]\n[[Category:Documents]]\n[[Category:Government information]]\n[[Category:Privacy]]']
['Telephone directory', '162263', '{{Redirect2|Phone book|White pages|a contact list|Contact list|other uses|White pages (disambiguation)}}\n{{Use dmy dates|date=June 2012}}\n{{refimprove|date=November 2008}}\n[[File:Telefonkatalog 1928.jpg|thumb|[[Gothenburg]] telephone directory, 1928.]]\n\nA \'\'\'telephone directory\'\'\', also known as a \'\'\'telephone book\'\'\', \'\'\'telephone address book\'\'\', \'\'\'phone book\'\'\', or the \'\'\'white/yellow pages\'\'\', is a listing of telephone [[subscriber]]s in a geographical area or subscribers to services provided by the organization that publishes the directory. Its purpose is to allow the telephone number of a subscriber identified by name and address to be found.\n\nThe advent of the Internet and [[smart phones]] in the 21st century greatly reduced the need for a paper phone book.  Some communities, such as [[Seattle]] and [[San Francisco]], sought to ban their unsolicited distribution as wasteful, unwanted and harmful to the environment.<ref name=SF>[http://www.sfgate.com/bayarea/article/Yellow-Pages-ruling-endangers-SF-ban-3951477.php Yellow Pages ruling endangers SF ban], Heather Knight, \'\'[[San Francisco Chronicle]]\'\', 15 October 2012; accessed 19 March 2013</ref><ref>[http://seattletimes.com/html/localnews/2019441687_yellowpages16m.html Appeals court rules against Seattle\'s curbs on yellow pages], Emily Heffter, \'\'[[Seattle Times]]\'\', 15 October 2012; accessed 19 March 2013</ref>\n\n== Content ==\nSubscriber names are generally listed in alphabetical order, together with their postal or street address and [[telephone number]].  In principle every subscriber in the geographical coverage area is listed, but subscribers may request the exclusion of their number from the directory, often for a fee; their number is then said to be "unlisted" ([[American English]]), "ex-directory" ([[British English]]), "private" or \'\'\'private number\'\'\' (Australia and New Zealand), or "non-published" (Canada).<ref>{{cite web|url=http://support.bell.ca/Home_phone/Phone_line/How_to_unlist_my_Bell_Home_phone_number |title=How to get a non-published Bell Home phone number |publisher=Support.bell.ca |date=2013-06-17 |accessdate=2014-04-16}}</ref>\n\nA telephone directory may also provide instructions: how to use the [[Local telephone service|telephone service]], how to dial a particular number, be it local or international, what numbers to access important and [[emergency services]], utilities, hospitals, doctors, and organizations who can provide support in times of crisis. It may also have [[civil defense]] or [[emergency management]] information. There may be transit maps, postal code/zip code guides, international dialing codes or stadium seating charts, as well as advertising.\n\nIn the US, under current rules and practices, mobile phone and [[voice over IP]] listings are not included in telephone directories.  Efforts to create cellular directories have met stiff opposition from several fronts, including those who seek to avoid [[telemarketer]]s.{{Citation needed|date=January 2011}}\n\n== Types ==\n[[File:Telefonbog ubt-1.JPG|thumb|White pages.]]\nA telephone directory and its content may be known by the color of the paper it is printed on.\n* White pages<!--redirects here--> generally indicates personal or alphabetic listings.\n* [[Yellow pages]], golden pages, A2Z, or classified directory is usually a "business directory", where businesses are listed alphabetically within each of many classifications (e.g., "lawyers"), almost always with paid advertising.\n* [[Reverse telephone directory|Grey pages]], sometimes called a "reverse telephone directory", allowing subscriber details to be found for a given number. Not available in all jurisdictions.{{citation needed|date=March 2014}}  (These listings are often published separately, in a city directory, [[Polk directory]], or under another name, for a price, and made available to commercial and government agencies.)\n\nOther colors may have other meanings; for example, information on [[government agencies]] is often printed on [[blue pages]] or green pages.{{Citation needed|date=September 2011}}\n\n== Publication ==\n[[File:New haven directory 1878.jpg|thumb|upright|New Haven directory, November, 1878.]]Telephone directories can be published in [[hard copy]] or in electronic form. In the latter case, the directory can be provided as an online service through proprietary terminals or over the Internet, or on physical media such as CD-ROM. In many countries directories are both published in book form and also available over the Internet. Printed directories were usually supplied free of charge.\n\n== History ==\n[[File:Unused Phonebooks.JPG|thumb|Recently delivered 2013–2014 phone books in the trash unopened; in the 21st century some communities have tried to stop the unsolicited distribution of the books<ref name=SF/>]]\n{{Expand section|date=September 2011}}\nTelephone directories are a type of [[city directory]]. Books listing the inhabitants of an entire city were widely published starting in the 18th century, before the invention of the telephone. \n\nThe first telephone directory, consisting of a single piece of cardboard, was issued on 21 February 1878; it listed 50 individuals, businesses, and other offices in [[New Haven, Connecticut]] that had telephones.<ref>{{cite web| title= The Phone Book | url= http://failuremag.com/feature/article/the_phone_book/ | author=Jason Zasky | work=Failure Magazine |accessdate=2013-12-31}}</ref>\n\nThe first British telephone directory was published on 15 January 1880  by The Telephone Company. It contained 248 names and addresses of individuals and businesses in London; telephone numbers were not used at the time as subscribers were asked for by name at the exchange.<ref>Records of the Telephone Company Limited (Bell\'s Patents), BT Archives reference TPA</ref> The directory is preserved as part of the British phone book collection by [[BT Archives]].\n\nIn 1938, AT&T commissioned the creation of a new type font, known as [[Bell Gothic|BELL GOTHIC]], the purpose of which was to be readable at very small font sizes when printed on newsprint where small imperfections were common.\n\nIn 1981 France was the first country to have an electronic directory<ref>{{cite web|url=http://whitepages.fr/minitel/ |title=Telephone History in France by |publisher=Whitepages.fr |date= |accessdate=2014-04-16}}</ref> on a system called [[Minitel]]. The directory is called "11" after its telephone access number.\n\nIn 1991 the [[U.S. Supreme Court]] ruled (in \'\'[[Feist v. Rural]]\'\') that telephone companies do not have a [[copyright]] on telephone listings, because copyright protects creativity and not the mere labor of collecting existing information.\n\n1996 is the year the first telephone directories go online in the USA. [[Yellowpages.com]] and [[Whitepages.com]] both see their start in April.<ref>[http://www.whitepages.fr/telecom-history-ft-late-with-internet.html Telephone Directory History by Whitepages.fr]</ref>\n\nIn 1999, the first online telephone directories and people finding sites such as [[LookupUK.com]] go online in the UK. In 2003, more advanced UK searching including Electoral Roll become available on [[LocateFirst.com]].\n\nIn the 21st century, printed telephone directories are increasingly criticized as waste. In 2012, after some North American cities passed laws banning the distribution of telephone books, an industry group sued and obtained a court ruling permitting the distribution to continue.<ref name=SF/> Manufacture and distribution of telephone directories produces over 1,400,000 metric tons of [[greenhouse gases]] and consumes over 600,000 tons of paper annually.<ref>{{cite web|last= Paster |first= Pablo |url=http://www.treehugger.com/culture/ask-pablo-what-is-the-impact-of-all-those-unwanted-phone-books.html |title=Ask Pablo: What Is The Impact Of All Those Unwanted Phone Books? |publisher=TreeHugger |date=2010-01-11 |accessdate=2014-04-16}}</ref>\n\n== Reverse directories ==\n{{main|Reverse telephone directory}}\nA reverse telephone directory is sorted by number, which can be looked up to give the name and address of the subscriber.\n\n== In popular culture ==\nRipping phone books in half has often been considered a [[Feats of strength|feat of strength]]. The Guinness World Record for ripping the most telephone directories is 27; the record for French telephone directories is 29, held by [[Georges Christen]].{{citation needed|date=October 2012}}\n\n== See also ==\n* [[Domain Name System|DNS]]\n* [[Lightweight Directory Access Protocol|LDAP]]\n* [[Silent number]]\n* [[Whois]]\n* [[City directory]]\n\n== References ==\n{{reflist|colwidth=30em}}\n\n== Further reading ==\n* {{cite book|title=The Phone Book: The Curious History of the Book That Everyone Uses But No One Reads| last= Shea|first=Ammon|publisher=Perigee Trade|year=2010|ISBN=978-0-399-53593-2}}\n\n== External links ==\n*{{Commonscat-inline|Phone books}}\n*{{wikt-inline}}\n* {{dmoz|Reference/Directories/Address_and_Phone_Numbers}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Telephone Directory}}\n[[Category:Telephone numbers]]\n[[Category:Directories]]\n[[Category:History of the telephone]]\n[[Category:American inventions]]\n[[Category:1878 introductions]]']
['Category:Australian directories', '45583078', "{{portal|Australia}}\n''Directories - of businesses, addresses and people in Australia''\n[[Category:Directories]]\n[[Category:Books about Australia]]"]
['Whitepages (company)', '25901032', '{{pp-protect|small=yes}}\n{{good article}}\n{{Infobox dot-com company\n| name             = Whitepages\n| logo     = [[File:White-Pages-Logo.png|175px]]\n| caption          =\n| type             = Private \n| industry         = \n| foundation       = 1997\n| founder          = [[Alex Algard]]\n| defunct          = <!-- {{End date|YYYY|MM|DD}} -->\n| location_city    = Seattle, Washington, US\n| location_country = US\n| locations        = <!-- Number of locations, stores, offices, etc. -->\n| area_served      = Worldwide\n| key_people       =  Rob Eleveld (CEO)<ref name="newCEO"/>\n| products         = People search, contact data, mobile apps\n| production       = \n| services         = \n| revenue          = $70 million (2015)<ref name="recentsource"/>\n| operating_income = \n| net_income       = \n| aum              = <!-- Only used with financial services companies -->\n| assets           = \n| equity           = \n| owner            = \n| num_employees    = 120 (2016)<ref>{{citation|publisher=Whitepages|title=Careers|url=http://whitepagesinc.com/about/careers.html|accessdate=August 19, 2013}}</ref>\n| parent           = \n| divisions        = \n| subsid           = \n| website = {{URL|http://www.whitepages.com}}\n| footnotes        = \n| intl             =\n| bodystyle        =\n| website_type          = Directory\n| current_status        = Active\n}}\'\'\'Whitepages\'\'\' is a provider of online directory services, fraud screening and identity verification for businesses, public record background checks, and other products, based on its database of contact information for people and businesses. It has the largest database available of contact information on US residents.<ref name="VB"/>\n\nWhitepages was founded in 1997 as a hobby for then-[[Stanford]] student [[Alex Algard]]. It was incorporated in 2000 and received $45 million in funding in 2005. Investors were later bought-out by Algard in 2013. From 2008 to 2013, Whitepages released several mobile apps, a re-design in 2009, the ability for consumers to control their contact information, and other features. From 2010 to 2016, the company shifted away from advertising revenue and began focusing more on selling business services and subscription products.\n\n==History==\nThe idea for Whitepages was conceived by Alex Algard, while studying at [[Stanford]] in 1996. Algard was searching for a friend\'s contact information and the phone company gave him the wrong number.<ref name="ppp"/> He thought of an online email directory as an easier to way to find people.<ref name="seven"/><ref name="two">{{cite news|title=WhitePages.com has number for fast growth|url=http://community.seattletimes.nwsource.com/archive/?date=20031013&slug=btinterface13|newspaper=The Seattle Times|accessdate=August 7, 2013|date=October 13, 2003}}</ref> Algard bought the Whitepages.com domain for nine hundred dollars,<ref name="four">{{cite news|first=Nicholas|last=Carlson|date=January 24, 2007|url=http://www.internetnews.com/xSP/article.php/3655611|publisher=InternetNews|title=WhitePages.com: Reach out and search someone|accessdate=December 2, 2013}}</ref><ref name="recentsource"/> which he says was all of his savings at the time.<ref name="seven"/> He continued operating the website as a hobby while working as an investment banker for [[Goldman Sachs]].<ref name="dakfhukajehf"/> He expanded the database of contact information using data licensed from American Business Information (now a part of Infogroup).<ref name="recentsource"/> Eventually WhitePages was producing more ad-revenue than Algard was earning at Goldman Sachs.<ref name="recentsource"/> In 1998, Algard left his job to focus on the website; he incorporated Whitepages in 2000.<ref name="dakfhukajehf">{{citation|publisher=Private Equity Growth Capital Council|url=http://www.pegcc.org/wordpress/wp-content/uploads/pec_cs_whitepages_020309a.pdf|title=WhitePages.com: From hobby to number one people search destination|accessdate=August 6, 2013}}</ref>\n\nThe site grew and attracted more advertisers. The company brokered deals with Yellowpages and Superpages, whereby Whitepages earned revenue for sending them referral traffic. By 2005, $15 million in annual revenues was coming from these contracts.<ref name="recentsource"/> In 2003, Algard stepped down as CEO to focus on CarDomain.com, which he had also founded<ref name="ppp">{{cite news|first=Brad|last=Broberg|title=Founder returns to WhitePages.com|publisher=Puget Sound Business Journal|date=September 30, 2007|url=http://www.bizjournals.com/seattle/stories/2007/10/01/focus10.html|accessdate=August 7, 2013}}</ref> and Max Bardon took his place as CEO temporarily.<ref name="recentsource"/> In 2005, Technology Crossover Ventures and Providence Equity Partners invested $45 million in the company.<ref name="recentsource"/><ref name="one"/> That same year, MSN adopted Whitepages\' directory data for its "Look it up" feature.<ref>{{cite news|title=MSN Replaces InfoSpace with WhitePages.com|url=http://www.mediapost.com/publications/article/28828/#axzz2bIuB3tM1|first=Shankar|last=Gupta|date=April 5, 2005|accessdate=August 7, 2013|publisher=MediaPost}}</ref> Algard returned to the company in 2007.<ref name="ppp"/> By the end of that year, the Whitepages database had grown to 180 million records<ref>{{cite news|title=WhitePages.com coverage expands from 40 to 80 percent|url=http://seattletimes.com/html/businesstechnology/2004062675_btbriefs10.html|newspaper=The Seattle Times|date=December 10, 2007|accessdate=August 7, 2013}}</ref> and the company was listed as one of [[Deloitte]]\'s 500 fastest growing technology companies in North America three times.<ref name="seven"/><ref>{{cite news|title=WhitePages hires new CTO|first=Rebecca|last=Collins|url=http://www.bizjournals.com/seattle/blog/techflash/2010/11/whitepages-taps-new-cto.html|publisher=Puget Sound Business Journal|date=November 17, 2010|accessdate=August 8, 2013}}</ref> By 2008 the company had $66 million in annual revenues.<ref name="recentsource"/>\n\nIn 2008, Whitepages said it would start working on options for users to control their information on the site.<ref>{{cite news|first=Steven|last=Vaughan-Nichols|newspaper=Computerworld|url=http://www.computerworld.com.au/article/216557/whitepages_com_grapples_privacy_web_2_0_world/?|title=WhitePages.com grapples with privacy in Web 2.0 world|date=May 19, 2008|accessdate=August 7, 2013}}</ref> That same year, it acquired [[Voice over Internet Protocol|VoIP]] developer [[Snapvine]]<ref name="one">{{cite news|first=Angel|last=Gonzalez|url=http://seattletimes.com/html/businesstechnology/2004458452_whitepages05.html|newspaper=The Seattle Times|title=WhitePages.com to buy Snapvine|accessdate=August 7, 2013|date=June 5, 2008}}</ref> in order to add features where users could be called through the website without giving out their phone number.<ref>{{cite news|title=WhitePages.com to buy Snapvine for around $20 million|first=Michael|last=Arrington|date=June 4, 2008|accessdate=August 7, 2013|url=http://techcrunch.com/2008/06/04/whitepagescom-to-buy-snapvine-for-around-20-million/|publisher=TechCrunch}}</ref> It also introduced an [[api]],  which gave third-party developers access to Whitepages\' data.<ref>{{cite news|first=Mike|last=Gunderloy|date=March 31, 2008|url=http://gigaom.com/2008/03/31/open-phone-data-whitepages/|accessdate=August 7, 2013|title=Open Phone Data from WhitePages.com|publisher=Giga Om}}</ref> Whitepages released an iOS app that August, followed by the Whitepages Caller ID app for Android devices  in February 2009<ref>{{cite news|publisher=VentureBeat|first=MG|last=Siegler|date=February 27, 2009|accessdate=August 7, 2013|url=http://venturebeat.com/2009/02/27/caller-id-a-paid-android-app-to-better-screen-my-phone-calls/|title=Caller ID: A paid Android app to better screen my phone calls}}</ref> and for Blackberry that May.<ref name="plp">{{cite news|publisher=VentureBeat|title=The background-check scams: Is WhitePages really better than Intelius?|url=http://venturebeat.com/2009/05/07/the-background-check-scams-is-whitepages-really-better-than-intelius/|first=Matt|last=Marshall|date=May 7, 2009|accessdate=August 7, 2013}}</ref> \n\nThe app displays information on callers, such as their latest social media posts, local weather at the caller\'s location and the identity of the caller.<ref name="eightlyy">{{cite news|first=Austin|last=Carr|newspaper=Fast Company|url=http://www.fastcompany.com/3000252/whitepages-launches-caller-id-social-mobile-age|title=WhitePages Launches Caller ID for the Social, Mobile Age|date=August 7, 2012|accessdate=August 7, 2013}}</ref><ref>{{cite news|newspaper=Time Magazine|first=Doug|last=Aamoth|url=http://techland.time.com/2012/12/04/top-10-tech-lists/slide/current-caller-id-android/|date=December 4, 2012|accessdate=August 7, 2013|title=Current Caller ID (Android)}}</ref><ref name="twenty">{{cite news|title=WhitePages\' new Current Caller ID App is the future of smartphone calling|url=http://venturebeat.com/2012/08/08/whitepages-current-caller-id-android/|date=August 8, 2012|first=Devindra|last=Hardawar|accessdate=August 7, 2013|publisher=VentureBeat}}</ref> It originally had the ability to display information on callers, such as their latest social media posts, local weather at the caller\'s location and the identity of the caller.<ref name="eightlyy">{{cite news|first=Austin|last=Carr|newspaper=Fast Company|url=http://www.fastcompany.com/3000252/whitepages-launches-caller-id-social-mobile-age|title=WhitePages Launches Caller ID for the Social, Mobile Age|date=August 7, 2012|accessdate=August 7, 2013}}</ref><ref>{{cite news|newspaper=Time Magazine|first=Doug|last=Aamoth|url=http://techland.time.com/2012/12/04/top-10-tech-lists/slide/current-caller-id-android/|date=December 4, 2012|accessdate=August 7, 2013|title=Current Caller ID (Android)}}</ref><ref name="twenty">{{cite news|title=WhitePages\' new Current Caller ID App is the future of smartphone calling|url=http://venturebeat.com/2012/08/08/whitepages-current-caller-id-android/|date=August 8, 2012|first=Devindra|last=Hardawar|accessdate=August 7, 2013|publisher=VentureBeat}}</ref> The ability for consumers to add themselves to the directory was added in the summer of 2009 and being able to edit existing entries was added that October.<ref>{{cite news|title=WhitePages Now Lets you control your own listings|first=Erick|last=Schonfeld|date=October 14, 2009|accessdate=August 8, 2013|url=http://techcrunch.com/2009/10/14/whitepages-now-lets-you-control-your-own-listings/|publisher=TechCrunch}}</ref>\n\nWhitepages.com underwent a re-design in 2009.<ref name="three">{{cite news|title=WhitePages launches $2.5 million overhaul|first=Brier|last=Dudley|url=http://seattletimes.com/html/technologybrierdudleysblog/2009467080_whitepagescom_launches_25_mill.html|date=July 14, 2009|accessdate=August 7, 2013|newspaper=The Seattle Times}}</ref> According to VentureBeat reporter Matt Marshall, the redesign made the advertising "cleaner" and made it more obvious when someone was going to a third-party website like US Search.<ref name="VB">{{cite news|date=July 14, 2009|first=Matt|last=Marshall|url=http://venturebeat.com/2009/07/14/whitepages-now-the-largest-database-of-american-people-cleans-up-act/|publisher=VentureBeat|title=WhitePages, now the largest database of American people, cleans up act|accessdate=August 7, 2013}}</ref> Marshall had previously criticized Whitepages, because website users that clicked on US Search ads and purchased data from US Search were sent through perpetual advertisements for other services that made it difficult to access the information they paid for.<ref name="VB"/><ref>{{cite news|title=The background-check scams: Is WhitePages really better than Intelius?|url=http://venturebeat.com/2009/05/07/the-background-check-scams-is-whitepages-really-better-than-intelius/|date=May 7, 2009|first=Matt|last=Marshall|accessdate=August 7, 2013}}</ref> A local business lookup feature called "Store Finder" was added in June 2010.<ref>{{cite news|title=WhitePages upgrades business search, adds "store finder"|url=http://seattletimes.com/html/technologybrierdudleysblog/2012197459_whitepages_upgrades_business_s.html|first=Brier|last=Dudley|newspaper=The Seattle Times|date=June 24, 2010}}</ref> The following month, Whitepages.com launched a deal site, Dealpop.com,<ref>{{cite news|title=Local shops join forces with coupon websites to sweeten sales|first=Melissa|last=Allison|author2=Amy Martinez |url=http://seattletimes.com/html/retailreport/2012259556_retailreport02.html|newspaper=The Seattle Times|date=July 1, 2010|accessdate=August 6, 2013}}</ref> which differed from [[Groupon]] by offering short-term deals on nationally available products.<ref>{{cite news|first=Amy|last=Martinez|date=October 20, 2010|accessdate=August 7, 2013|newspaper=The Seattle Times|url=http://seattletimes.com/html/businesstechnology/2013209878_dealpopweb21.html|title=WhitePages\' DealPop to try national approach as it takes on Groupon, other coupon websites}}</ref> Dealpop was sold to [[Martin Tobias#Tippr.com|Tippr]] the following year.<ref>{{cite news|title=Tippr Grabs Sales & Tech Talent in DealPop Acquisition, Continuing Daily Deals Dogfight for Third Place|url=http://www.xconomy.com/seattle/2011/06/01/tippr-grabs-sales-tech-talent-in-dealpop-acquisition-continuing-daily-deals-dogfight-for-third-place/|newspaper=Xconomy|date=July 1, 2011|accessdate=August 7, 2013|first=Curt|last=Wooodward}}</ref>\n\nIn 2010, Superpages and Yellowpages cut back spending with Whitepages from $33 million to $7 million, causing a substantial decline in revenues and a tense relationship with investors. Algard spent $50 million in cash the company had on-hand and $30 million from a bank loan, to buyout the investors in 2013. He also used his personal house, savings account and personal belongings as collateral for the loan.<ref name="recentsource"/> Algard began shifting the company\'s business model to reduce its reliance on advertising and instead focus on business users and paid subscriptions.<ref name="recentsource"/><ref name="Carlson 2013">{{cite web | last=Carlson | first=Nicholas | title=With Buyback, 16-Year-Old Startup WhitePages Is Doing Something Very Rare With $80 Million | website=Business Insider | date=October 21, 2013 | url=http://www.businessinsider.com/whitepages-stock-buyback-2013-10 | accessdate=August 18, 2016}}</ref> \n\nWhitepages released the Localicious app in July 2011. The app was released on Android first, because Whitepages was frustrated with Apple\'s approval process for iPhone apps.<ref name="agiu">{{cite news|title=WhitePages goes Android first with latest app|url=http://news.cnet.com/8301-1023_3-20079150-93/whitepages-goes-android-first-with-latest-app/|date=July 13, 2011|first=Ina|last=Fried|accessdate=August 7, 2013|publisher=All Things Digital}}</ref> Whitepages PRO was also introduced that same year.<ref name="cardnotpresent">{{cite news|url=http://pro.whitepages.com/sites/pro.whitepages.com/files/Marketing_Documents/CardNotPresent%20Article%2010.24.12.pdf|publisher=CNP Report|first=D.J.|last=Murphy|date=October 24, 2012|accessdate=September 24, 2013|title=WhitePages PRO Taps Phone Data and More to Identify CNP Fraud}}</ref> An updated Android app called  Current Caller ID was released in August 2012.<ref name="eightlyy"/> Within a year of its release, 5 billion calls and texts had been transmitted using the app. It was updated in July 2013 with new features, such as the ability to customize the layout of caller information for each caller and the ability to "Like" Facebook posts from within the app.<ref name="fgy">{{cite news|title=WhitePages\' Current Caller ID app powers more than 5B calls & texts, adds new customization features|url=http://venturebeat.com/2013/07/25/whitepages-current-caller-id-app-powers-more-than-5b-calls-texts-adds-new-customization-features/|publisher=VentureBeat|first=Devindra|last=Hardawar|date=July 25, 2013|accessdate=August 7, 2013}}</ref> In June 2013, Whitepages acquired Mr. Number, an Android app for blocking unwanted callers.<ref>{{cite news|title=WhitePages Scoops up Mr. Number, an Android App for Blocking Unwanted Calls|date=June 1, 2013|first=Ina|last=Fried|url=http://allthingsd.com/20130601/whitepages-scoops-up-mr-number-an-android-app-for-blocking-unwanted-calls/|newspaper=The Wall Street Journal|accessdate=August 7, 2013}}</ref>\n\nIn August 2013 Whitepages purchased all the interests in the company owned by investors for $80 million.<ref name="dafhybniub">{{cite news|title=With Buyback, 16-Year-Old Startup WhitePages Is Doing Something Very Rare With $80 Million|first=Nicholas|last=Carlson|date=October 21, 2013|url=http://www.businessinsider.com/whitepages-stock-buyback-2013-10#ixzz2qRETXgXX|publisher=Business Insider|accessdate=October 30, 2014}}</ref><ref>{{cite news|title=Nextcast: WhitePages CEO Alex Algard on the distraction of outside investors and keeping your startup zeal|first=Jeff|last=Dickey|date=April 5, 2014|accessdate=May 2, 2014|url=http://www.geekwire.com/2014/nextcast-whitepages-ceo-alex-algard-distraction-outside-investors-keep-startup-zeal/|publisher=Geekwire}}</ref> In 2015, WhitePages acquired San Francisco-based NumberCorp to improve the database of phone numbers used for scams in the Caller ID app.<ref name="Perez 2015">{{cite web | last=Perez | first=Sarah | title=Whitepages Acquires NumberCop To Improve Its Scam-Detecting Caller ID App | website=TechCrunch | date=June 10, 2015 | url=http://social.techcrunch.com/2015/06/10/whitepages-acquires-numbercop-to-improve-its-scam-detecting-caller-id-app/ | accessdate=August 12, 2016}}</ref> In April 2016, Whitepages spun-off its caller ID business into a separate company called Hiya<ref name="Lunden 2016">{{cite web | last=Lunden | first=Ingrid | title=Whitepages spins out its caller-ID business as Hiya to take on TrueCaller | website=TechCrunch | date=April 27, 2016 | url=http://social.techcrunch.com/2016/04/27/whitepages-hiya/ | accessdate=July 8, 2016}}</ref> with a staff of 40 in Seattle.<ref name="Flynn 2016">{{cite web | last=Flynn | first=Kerry | title=Meet Hiya: Whitepages Spins Off Caller ID Business With Mission To Fight Robocalls, Spam Texts Worldwide | website=International Business Times | date=April 27, 2016 | url=http://www.ibtimes.com/meet-hiya-whitepages-spins-caller-id-business-mission-fight-robocalls-spam-texts-2360298 | accessdate=July 8, 2016}}</ref> In September 2016, Alex Algard stepped down as CEO of WhitePages, in order to focus on the mobile spam-blocking spin-off Hiya. He appointed Rob Eleveld as the new WhitePages CEO.<ref name="newCEO">{{cite web | title=Whitepages Founder Alex Algard Gives Up CEO Slot To Focus On Caller ID Startup Hiya | newspaper=Forbes | date=September 16, 2016 | url=http://www.forbes.com/sites/amyfeldman/2016/09/16/whitepages-founder-alex-algard-gives-up-ceo-slot-there-to-focus-on-caller-id-spinoff-hiya/#2db4ba761803 | accessdate=September 20, 2016}}</ref>\n\n==Services==\nWhitepages has the largest database of contact information on Americans.<ref name="VB"/> As of 2008, it had data on about 90 percent of the US adult population,<ref>{{cite news|publisher=IntoMobile|first=Dusan|last=Belic|date=May 8, 2012|accessdate=September 24, 2013|url=http://www.intomobile.com/2012/05/08/whitepages-ios-app-gets-nearby-search-capability/|title=WhitePages\' iOS app gets nearby search capability}}</ref> including 200 million records on people and 15 million business listings.<ref name="seven">{{cite news|title=A Directory of Success: WhitePages CEO Alex Algard|date=February 2, 2011|newspaper=Examiner|first=Paul|last=Kim}}</ref> Whitepages\' data is collected from property deeds,<ref name="five"/> telecom companies, and public records.<ref name="ll">{{cite news|title=WhitePages IDs Growth in the Explosion of Personal Data|date=August 20, 2012|first=Curt|last=Woodward|accessdate=August 7, 2013|url=http://www.xconomy.com/seattle/2012/08/20/whitepages/}}</ref> Privacy is a common concern regarding Whitepages\' publishing of personal contact information.<ref name="StairReynolds2008">{{cite book|author1=Ralph M. Stair|author2=George Reynolds|author3=George Walter Reynolds|title=Fundamentals of Information Systems|url=https://books.google.com/books?id=J85RP4YmBTYC&pg=PA253|accessdate=7 August 2013|date=December 2008|publisher=Cengage Learning|isbn=978-1-4239-2581-1|pages=253–}}</ref> The Whitepages.com website has features that allow users to remove themselves from the directory or correct and update information.<ref name="five">{{cite news|title=Connecticut may let residents remove directory information|url=http://www.scmagazine.com/connecticut-may-let-residents-remove-directory-data/article/100267/#|date=December 28, 2007|first=Dan|last=Kaplan|newspaper=SC Magazine}}</ref><ref name="StairReynolds2008"/> WhitePages.com has about 50 million unique visitors per month<ref>{{cite news|publisher=VentureBeat|title=WhitePages acquires Mr. Number, the phone-spam Android app with 7M downloads, to reduce phone spam|url=http://www.reuters.com/article/2013/05/31/idUS27174982720130531|first=John|last=Koetsier|date=May 31, 2013|accessdate=December 2, 2013}}</ref> and performs two billion searches per month.<ref name="cardnotpresent"/>\n\nWhitePages started developing features for business users around 2010.<ref name="recentsource"/> WhitePages Pro is used for things like verifying the identity of a sales lead, find fake form data in online forms and to check form data from consumers making a purchase against common indicators of fraud, like shipping to a mailbox at an unoccupied building.<ref name="recentsource">{{cite news| first=Amy|last=Feldman |title=Alex Algard Risked Everything To Turn His Struggling Firm, Whitepages, Into A Growing Tech Company | newspaper=Forbes | date=August 23, 2016 | url=http://www.forbes.com/sites/amyfeldman/2016/08/03/alex-algard-risked-everything-to-turn-his-struggling-firm-whitepages-into-a-growing-tech-company/#165b97ae73d0 | accessdate=August 10, 2016}}</ref><ref name="cardnotpresent"/><ref name="Whitepages Pro">{{cite web | title=Whitepages Pro – Mobile Identity Data for Businesses | website=Whitepages Pro | url=http://pro.whitepages.com/ | accessdate=August 15, 2016}}</ref> In 2016, advertising on WhitePages.com was turned off in favor of selling monthly subscriptions that give users unlimited background checks and other records.<ref name="recentsource"/>\n\nAs of 2013 Whitepages provides its data and related services through seven web properties, ten mobile apps<ref>{{citation|url=http://whitepagesinc.com/about/|publisher=WhitePages|title=About Us|accessdate=December 2, 2013}}</ref> and  through multiple web properties, including 411.com and Switchboard.com.<ref name="SuiElwood2012">{{cite book|author1=Daniel Zhi Sui|author2=Sarah Elwood|author3=Michael F. Goodchild|title=Crowdsourcing Geographic Knowledge: Volunteered Geographic Information (VGI) in Theory and Practice|url=https://books.google.com/books?id=SSbHUpSk2MsC&pg=PA267|accessdate=7 August 2013|date=10 August 2012|publisher=Springer|isbn=978-94-007-4587-2|pages=267–}}</ref> The Hiya app (previously known as WhitePages Caller ID) checks incoming calls against a database of phone numbers known for spam or scam calls and helps users report scams to the Federal Trade Commission.<ref name="Stern 2016">{{cite web | last=Stern | first=Joanna | title=How to Stop Robocalls … or at Least Fight Back | website=WSJ | date=June 28, 2016 | url=http://www.wsj.com/articles/how-to-stop-robocalls-or-at-least-fight-back-1467138771 | accessdate=July 8, 2016}}</ref><ref name="Lerman 2016">{{cite web | last=Lerman | first=Rachel | title=Whitepages spins out mobile caller-ID startup Hiya | website=The Seattle Times | date=April 27, 2016 | url=http://www.seattletimes.com/business/technology/whitepages-spins-out-mobile-caller-id-startup-ceo-takes-on-dual-roles/ | accessdate=July 8, 2016}}</ref> Hiya mobile app replaces the Android user interface for making and receiving phone calls.<ref name="fgy"/>\n\n==References==\n{{reflist|2}}\n\n==External links==\n*[http://www.whitepages.com/ Official website]\n\n{{DEFAULTSORT:Whitepages.Com}}\n[[Category:Directories]]\n[[Category:Internet properties established in 1997]]\n[[Category:Privately held companies based in Washington (state)]]\n[[Category:Companies based in Seattle, Washington]]\n[[Category:Online person databases]]']
['Web query classification', '16350490', '{{Cleanup|date=March 2011}}\n\'\'\' \nA Web query topic classification/categorization is a problem in [[information science]]. The task is to assign a [[Web search query]] to one or more predefined [[Categorization|categories]], based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query “\'\'apple\'\'” might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the [[document classification]] tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks.\n\n== KDDCUP 2005 ==\n\nKDDCUP 2005 competition<ref>[http://www.kdd.org/kdd-cup/view/kdd-cup-2005 KDDCUP 2005 dataset]</ref> highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query “\'\'apple\'\'”, it should be classified into ranked categories: “\'\'Computers \\ Hardware\'\'; \'\'Living \\ Food & Cooking\'\'”.\n\n{| class="wikitable"\n|-\n! Query\n! Categories\n|-\n| apple\n| Computers \\ Hardware<br />Living \\ Food & Cooking\n|-\n| FIFA 2006\n| Sports \\ Soccer<br />Sports \\ Schedules & Tickets<br />Entertainment \\ Games & Toys\n|-\n| cheesecake recipes\n| Living \\ Food & Cooking<br />Information \\ Arts & Humanities\n|-\n| friendships poem\n| Information \\ Arts & Humanities<br />Living \\ Dating & Relationships\n|}\n\n[[Image:Web query length.gif]]\n[[Image:Web query meaning.gif]]\n\n== Difficulties ==\n\nWeb query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding:\n\n=== How to derive an appropriate feature representation for Web queries? ===\n\nMany queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, "\'\'apple\'\'" can mean a kind of fruit or a computer company. "\'\'Java\'\'" can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a [[vector space model]] for classification is not appropriate.\n\n* Query-enrichment based methods<ref>Shen et al.  [http://www.sigkdd.org/sites/default/files/issues/7-2-2005-12/KDDCUP2005Report_Shen.pdf "Q2C@UST: Our Winning Solution to Query Classification"]. \'\'ACM SIGKDD Exploration, December 2005, Volume 7, Issue 2\'\'.</ref><ref>Shen et al. [http://portal.acm.org/ft_gateway.cfm?id=1165776 "Query Enrichment for Web-query Classification"]. \'\'ACM TOIS, Vol. 24, No. 3, July 2006\'\'.</ref> start by enriching user queries to a collection of text documents through [[search engines]]. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as [[Naive Bayes]] (NB) and [[Support Vector Machines]] (SVMs).\n\nHow about disadvantages and advantages??\ngive the answers:\n\n=== How to adapt the changes of the queries and categories over time? ===\n\nThe meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word "\'\'Barcelona\'\'" has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web.\n\n* Intermediate taxonomy based method<ref>Shen et al.  [http://portal.acm.org/ft_gateway.cfm?id=1148196 "Building bridges for web query classification"]. \'\'ACM SIGIR, 2006\'\'.</ref> first builds a bridging classifier on an intermediate taxonomy, such as [[Open Directory Project]] (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries.\n\n=== How to use the unlabeled query logs to help with query classification? ===\n\nSince the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users\' behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users\' knowledge about the World Wide Web.\n\n* Query clustering method<ref>Wen et al. [http://portal.acm.org/ft_gateway.cfm?id=503108 "Query Clustering Using User Logs"], \'\'ACM TOIS, Volume 20, Issue 1, January 2002\'\'.</ref> tries to associate related queries by clustering “session data”, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering.\n* Selectional preference based method<ref>Beitzel et al. [http://portal.acm.org/ft_gateway.cfm?id=1229183 "Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs"], \'\'ACM TOIS, Volume 25, Issue 2, April 2007\'\'.</ref> tries to exploit some [[association rules]] between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries.\n\n== Applications ==\n\n* \'\'\'[[metasearch|Metasearch engines]]\'\'\' send a user\'s query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users\' navigation.\n* \'\'\'[[Vertical search]]\'\'\', compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly.\n* \'\'\'[[Online advertising]]\'\'\'<ref>[http://www.kdd2007.com/workshops.html#adkdd Data Mining and Audience Intelligence for Advertising (ADKDD\'07)], KDD workshop 2007</ref><ref>[http://research.yahoo.com/workshops/troa-2008/ Targeting and Ranking for Online Advertising (TROA\'08)], WWW workshop 2008</ref> aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs.\nAll these services rely on the understanding Web users\' search intents through their Web queries.\n\n== See also ==\n\n* [[Document classification]]\n* [[Web search query]]\n* [[Information retrieval]]\n* [[Query expansion]]\n* [[Naive Bayes classifier]]\n* [[Support vector machines]]\n* [[Meta search]]\n* [[Vertical search]]\n* [[Online advertising]]\n\n== References ==\n\n{{reflist}}\n\n== Further reading ==\n* Shen.  [http://lbxml.ust.hk/th/th_search.pl?smode=VIEWBYCALLNUM&skeywords=CSED%202007%20Shen "Learning-based Web Query Understanding"]. \'\'Phd Thesis\'\', \'\'HKUST\'\', June 2007.\n{{Internet search}}\n\n{{DEFAULTSORT:Web Query Classification}}\n[[Category:Information retrieval techniques]]\n[[Category:Internet search]]']
['SpyFu', '25580778', '\'\'\'SpyFu\'\'\', originally GoogSpy, is a [[search analytics]] company based out of Scottsdale, AZ. Started in April 2005, SpyFu shows the keywords that websites buy on [[Google Adwords]]<ref>{{Cite web|url=http://searchenginewatch.com/3632613|accessdate=December 28, 2009|title=Advanced Keyword Research Checklist: Using Multiple Datasets}}</ref> as well as the keywords that websites are showing up for within search results. The service also gives cost per click and search volume statistics on keywords and uses that data to approximate what websites are spending on advertising.<ref>{{Cite web|url=http://www.entrepreneur.com/ebusiness/searchoptimization/searchengineoptimizationcolumnistjonrognerud/article175856.html|accessdate=December 28, 2009|title=Using the Competition to Boost Your SEO Performance }}</ref><ref>{{Cite web|url=http://www.pcworld.com/businesscenter/article/141728/top_11_moneywasting_adwords_mistakes.html|accessdate=December 28, 2009|title=Top 11 Money-Wasting AdWords Mistakes}}</ref> Historical advertising budgets offered by SpyFu also help advertisers project what an advertising campaign will cost in the future.<ref>{{Cite web|url=http://searchengineland.com/spying-on-your-paid-search-competitors-13235|accessdate=December 28, 2009|title=Spying On Your Paid Search Competitors}}</ref> The main value proposition is to see or to "spy on" the keywords that competitors use and improve [[Search Engine Marketing|SEM]] and [[Search Engine Optimization|SEO]] strategies based on those.<ref>{{Cite news|url=http://www.wired.com/epicenter/2009/06/coolsearchengines/|accessdate=December 28, 2009|title=Cool Search Engines That Are Not Google | work=Wired|first=Ryan|last=Singel|date=June 30, 2009}}</ref> SpyFu\'s data was also used in the [[Washington Post]] during the [[United States presidential election, 2008|2008 Presidential election]] to disclose various keywords that candidates were advertising on.<ref>{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/10/15/AR2008101503574_2.html?sid=ST2008101503923|accessdate=December 28, 2009|title=In Targeting Online Ads, Campaigns Ask: Who\'s Searching for What? | work=The Washington Post | first=Peter | last=Whoriskey | date=October 16, 2008}}</ref> SpyFu can also uncover emerging or niche markets.<ref>{{Cite book|url=https://books.google.com/books?id=DrQtbOroN0UC&pg=PT188&dq=spyfu&ei=oz44S4ehGqmKkATX1KjNAQ&cd=1#v=onepage&q=spyfu&f=false|accessdate=December 28, 2009|title=The Findability Formula | first=Heather F. | last=Lutze | isbn=978-0-470-42090-4 | year=2009 | publisher=John Wiley and Sons}}</ref> SpyFu has been mentioned in \'\'[[4_hour_work_week|The 4-Hour Work Week]]\'\', Oreilly\'s \'\'[[Complete Web Monitoring]]\'\', and \'\'[[SEO Warrior]]\'\'.\n\nSpyFu\'s data is obtained via [[web scraping]], based on technology developed by [[Velocityscape]], a company that makes web scraping software. The accuracy of its data, especially advertising budgets, was found to be somewhat dependent on the size of the website in question.<ref>{{Cite web|url=http://www.seoptimise.com/blog/2008/09/the-small-but-great-spyfu-experiment.html|accessdate=December 28, 2009|title=The Small (but Great) SpyFu Experiment}}</ref> SpyFu refreshes its data on a monthly basis, and as such is used as a guide to what\'s happening with larger trends in SEM/SEO rather than as a real time tracking engine.<ref>{{Cite book|url=https://books.google.com/books?id=5G4kfJ3DG2kC&pg=PA113&dq=spyfu&ei=gvg4S-WvLZPIlAT3nuzSAQ&cd=2#v=onepage&q=spyfu&f=false|accessdate=December 28, 2009|title=The complete guide to Google advertising | first=Bruce C. | last=Brown | isbn=978-1-60138-045-6 | year=2007 | publisher=Atlantic Publishing Company}}</ref>\n==References==\n{{Reflist}}\n\n==External links==\n* [http://spyfu.com/ SpyFu Corporate Website]\n\n[[Category:Internet search]]\n[[Category:Companies based in Scottsdale, Arizona]]\n[[Category:Companies established in 2005]]']
['Hybrid search engine', '2851233', "{{Notability|date=December 2009}}\nA '''hybrid search engine''' ('''HSE''') is a type of [[computer]] [[search engine]] that uses different types of data with or without ontologies to produce the [[algorithm]]ically generated results based on [[web crawling]]. Previous types of search engines only use text to generate their results.\nHybrid search engines use a combination of both crawler-based results and directory results. More and more search engines these days are moving to a hybrid-based model.\n==References==\n{{No footnotes|date=April 2010}}\n*http://eprints.ecs.soton.ac.uk/17457/\n*http://eprints.whiterose.ac.uk/3771/\n*http://www.picollator.com\n*http://elocalfinder.com/HSearch.aspx\n\n[[Category:Internet search]]\n\n\n{{web-stub}}"]
['User intent', '52689741', '\'\'\'User intent\'\'\' or \'\'\'query intent\'\'\' is the identification and categorization of what a user online intended or wanted when they typed their [[web search query|search terms]] into an online [[web search engine]] for the purpose of [[search engine optimization]] or [[conversion rate optimization]].<ref name="Understanding Sponsored Search: Core Elements of Keyword Advertising">{{cite book|last1=Jansen|first1=Jim|title=Understanding Sponsored Search: Core Elements of Keyword Advertising|date=July 2011|publisher=Cambridge University Press|location=New York, NY, USA|isbn=9781107011977|page=44|url=https://books.google.com/books?id=L4LIGyLOwDoC&pg=PA44&dq=what+is+user+intent&hl=en&gl=us&sa=X&redir_esc=y#v=onepage&q=what%20is%20user%20intent&f=false}}</ref> When a user goes online, there is always a purpose, an intent. The goal can be fact-checking, comparison shopping, filling downtime, or any other activity online.<ref name="The Different Types of User Intent">{{cite web|last1=Shih|first1=Joseph|title=The Different Types of User Intent|url=https://www.twinword.com/blog/understanding-different-types-user-intent/|website=Twinword Blog|publisher=Twinword, Inc.|accessdate=26 December 2016}}</ref>\n\n==Types==\nThough there are various ways of classifying or naming the categories of the different types of user intent, overall they seem to follow the same clusters. In general and up until the rise and explosion<ref name="The Rise of Mobile Search: From 2012 to 2015">{{cite web|title=The Rise of Mobile Search: From 2012 to 2015|url=http://www.texodesign.com.au/the-rise-of-mobile-search/|website=Texo Design|publisher=Texo Design|accessdate=26 December 2016}}</ref> of [[mobile search]], there are and were [[Web search query#Types|three very broad categories]]: informational, transactional, and navigational.<ref>{{cite journal|last1=Broder|first1=Andrei|title=A Taxonomy of Web Search|journal=SIGIR Forum|date=Fall 2002|volume=36|issue=2|pages=5–6|url=http://www.cis.upenn.edu/~nenkova/Courses/cis430/p3-broder.pdf|accessdate=27 December 2016}}</ref> However over time and with the rise<ref name="The Rise of Mobile Search: From 2012 to 2015" /> of [[mobile search]], other categories have appeared or categories have segmented into more specific categorization. The following is a table showing how different organizations have categorize the different types.\n\n{| class="wikitable" style="text-align: center;"\n|+ style="text-align: left;" | The Different Types of User Intents<ref name="The Different Types of User Intent" />\n|-\n! !! Type 1 !! colspan="2" | Type 2 (a/b) !! Type 3 !! Type 4\n|-\n| || "who wrote the Matrix" || "online IQ test" || "office supplies" || "google play store" || "restaurants near me"\n|-\n| [[Microsoft]]<ref>{{cite journal|last1=KhudaBukhsh|first1=Ashiqur|last2=Bennett|first2=Paul|last3=White|first3=Ryen|title=Building Effective Query Classifiers: A Case Study in Self-harm Intent Detection|journal=CIKM \'15 Proceedings of the 24th ACM International on Conference on Information and Knowledge Management|date=2015|pages=1735–1738|url=http://research.microsoft.com/en-us/um/people/pauben/papers/cikm-2015-KhudaBukhsh-et-al.pdf|accessdate=26 December 2016|format=PDF}}</ref> || Informational || colspan="2" | Transactional || Navigational || --\n|-\n| [[Google]]<ref>{{cite book|title=Search Quality Evaluator Guidelines|date=28 March 2016|publisher=Google|pages=61–74|url=http://static.googleusercontent.com/media/www.google.com/en//insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf|accessdate=26 December 2016}}</ref> || Know || colspan="2" | Do || Website || Visit-in-person\n|-\n| [[Hubspot]]<ref>{{cite web|title=Keyword Development: Without a computer!|url=https://cdn2.hubspot.net/hub/137828/file-331703896-pdf/docs/hubspot_keyword_development_worksheet.pdf|website=Hubspot|publisher=Hubspot|accessdate=26 December 2016}}</ref> || Problem based || colspan="2" | Solution based || Brand based || --\n|-\n| [[SEMRush]]<ref>{{cite web|title=Types of keywords: commercial, informational, navigational, transactional|url=https://www.semrush.com/blog/types-of-keywords-commercial-informational-navigational-transactional/|website=SEMRush Blog|publisher=SEMRush|accessdate=26 December 2016}}</ref> || Informational || Commercial || Transactional || Navigational || --\n|-\n| [[Web Analytics World]]<ref>{{cite web|last1=Levitt|first1=Dean|title=Using Intent, Demographics and Micro-Moments to Better Understand your Web Traffic|url=http://www.webanalyticsworld.net/2016/09/intent-demographics-and-micro-moments-in-analytics.html|website=Web Analytics World|publisher=Jump Digital|accessdate=26 December 2016}}</ref> || Know || Do || Buy || -- || Go\n|-\n| Summary || Know || Do || Buy || Web || Local\n|}\n\n* Know - An informational search query looking for facts or other information (e.g. "who wrote the Matrix")\n* Do - A transactional search query wanting to fulfill a task online (e.g. "online IQ test")\n* Buy - A transactional search query wanting to buy something (e.g. "office supplies")\n* Web - A navigational search query wanting to visit to a specific web site or page (e.g. "google play store")\n* Local - A search query wanting to visit-in-person a physical location (e.g. "restaurants near me")\n\nPlease note that many search queries may be ambiguous and thus may be classified into multiple intents. For example, a user who typed a query "matrix" into a search bar may want to purchase the [[The Matrix|1999 American-Australian philosophical sci-fi film]] or may want to learn more about the [[Matrix (mathematics)|matrices in mathematics]].\n\n==Importance==\nWith the prevalence of search engines being the first starting point of many online sessions,<ref>{{cite web|last1=Purcell|first1=Kristen|title=Search and email still top the list of most popular online activities|url=https://searchenginewatch.com/sew/study/2101282/search-engines-92-adult-internet-users-study|website=Pew Research Center Internet, Science & Tech|publisher=Pew Research Center|accessdate=26 December 2016}}</ref> search engines are tasked with surfacing the [[Search engine results page|best results]] or best [[Online advertising|ads]] that will satisfy the various user intents. Because [[Web search engine#Search engine bias|search engines do not actually read and understand]] web pages and ad copy completely, [[Digital marketing|digital marketers]] have to align their [[Keyword research|target keywords]] to the correct user intent that they are trying to satisfy<ref name="The Different Types of User Intent" /> if they want to rank high on [[Search engine result page|SERPs]] and improve their [[Conversion rate optimization|conversion rate]].\n\nTake for example, a company selling colored contact lenses who wants their ad to show up for relevant searches may target the keyword "blue eyes". However, this may not be the most effective strategy as users who search "blue eyes" may want to learn biological facts about blue eyes. Instead, the company can target keywords that clearly indicates that the user is looking to buy colored contact lenses (i.e. "blue contact lenses" most likely implies "buy blue contact lenses"). With the correct keyword intent targeting, studies have shown that conversion rates increase significantly.<ref>{{cite web|last1=daSilva|first1=Tiffany|title=Why Ignoring User Intent is Costing You Money in AdWords|url=http://unbounce.com/ppc/ignoring-user-intent-costs-you-money-in-adwords/|website=unbounce Pay Per Click|publisher=unbounce|accessdate=26 December 2016}}</ref>\n\n==See also==\n* [[Web search query]]\n* [[Keyword research]]\n* [[Intent marketing]]\n* [[Search engine optimization]]\n* [[Conversion rate optimization]]\n* [[Search engine result page]]\n* [[Principle of least astonishment]]\n\n==References==\n{{reflist}}\n\n[[Category:Internet search]]']
['IFACnet', '7344222', "'''IFACnet''', the KnowledgeNet for Professional Accountants, is the global, multilingual search engine developed by the [[International Federation of Accountants]] (IFAC) and its members to provide professional accountants worldwide with one-stop access to [[good practice guidance]], articles, management tools and other resources. This enterprise search engine was launched on October 2, 2006 by INDEZ. Originally marketed to professional accountants in business, IFACnet was expanded in March 2007 to provide resources and information relevant to small and medium accounting practices. It now includes resources and information for accountants in all sectors of the profession.\n\nThe following 31 organizations participate in IFACnet:\n\n*[[American Institute of Certified Public Accountants]] (AICPA)\n*[[Association of Chartered Certified Accountants]] (ACCA)\n*[[Canadian Institute of Chartered Accountants]]\n*[[Certified General Accountants Association of Canada]]\n*[[Chartered Institute of Management Accountants]] (CIMA)\n*[[Chartered Institute of Public Finance and Accountancy]]\n*[[CMA Canada]]\n*[[Compagnie Nationale des Commissaires aux Comptes]]\n*[[Conseil Supérieur de l'Ordre des Experts-Comptables]]\n*[[Consiglio Nazionale Dottori Commercialisti]]\n*[[CPA Australia]]\n*[[Délégation Internationale Pour l'Audit et la Comptabilité]]\n*[[Hong Kong Institute of Certified Public Accountants]] (HKICPA)\n*[[International Federation of Accountants]]  (IFAC)\n*[[Institut der Wirtschaftspruefer in Deutschland]] e.V. (IDW)\n*[[Institute of Certified Public Accountants in Ireland]]\n*[[Institute of Certified Public Accountants of Singapore]]\n*[[Institute of Chartered Accountants of Australia]]\n*[[Institute of Chartered Accountants in England & Wales]] (ICAEW)\n*[[Institute of Chartered Accountants in Ireland]]\n*[[Institute of Chartered Accountants of India]]\n*[[Institute of Chartered Accountants of Pakistan]]\n*[[Institute of Chartered Accountants of Scotland]] (ICAS)\n*[[Institute of Management Accountants]]\n*[[Japanese Institute of Certified Public Accountants]] (JICPA)\n*[[Koninklijk Nederlands Instituut van Registeraccountants]] (Royal NIVRA)\n*[[Malaysian Institute of Accountants]]\n*[[Malta Institute of Accountants]]\n*[[National Association of State Boards of Accountancy]] (NASBA)\n*[[South African Institute of Chartered Accountants]] (SAICA)\n*[[Union of Chambers of Certified Public Accountants of Turkey]] (TÜRMOB)\n\n==External links==\n*[http://www.ifacnet.com/ IFACnet - A KnowledgeNet for Professional Accountants]\n*[http://www.ifac.org/ International Federation of Accountants Homepage]\n\n[[Category:Information retrieval organizations]]\n[[Category:Internet search engines]]\n[[Category:Accounting organizations]]"]
['Coveo', '16001013', "{{Infobox company\n| name = Coveo Solutions Inc.\n| logo = [[Image:Coveo logo.png|120px]]\n| type = Private\n| slogan = \n| foundation =  2005\n| location_city = [[Quebec City]], [[Canada]]\n| key_people = Louis Têtu, Chairman and CEO <br />Laurent Simoneau, President and CTO\n| num_employees =200+\n| industry = [[Enterprise search]]\n| products = Coveo Search & Relevance Platform,<br />Coveo for Sitecore,<br />Coveo for Salesforce\n| homepage = http://www.coveo.com\n}}\n\n'''Coveo''' is a provider of [[enterprise search]] and website search technologies, with integrated plug-ins for [[Salesforce.com]], Sitecore CEP, and [[Microsoft Outlook]] and [[SharePoint]].  APIs also allow for custom integration with other applications.\n\n==History==\nCoveo Solutions Inc. was founded in 2005 as a spin-off of [[Copernic|Copernic Technologies Inc.]] Laurent Simoneau, Coveo's president and chief executive officer was formerly Copernic's chief operating officer. About 30 employees moved into the new company, with offices at that time in [[Quebec City]] and [[Montreal]] in Canada and in [[Palo Alto]], Calif.<ref>http://www.eweek.com/c/a/Enterprise-Applications/Copernic-Ready-to-Take-On-Google-In-Enterprise-Search-Product/</ref>\n\n==Products==\n'''Coveo Search & Relevance Platform'''\n\nCoveo Search & Relevance Platform is a modular enterprise search technology that can index information stored in diverse repositories throughout the company, perform text analytics and metadata enrichment on the indexed content, and make the content findable through search-driven interfaces.\n\n'''Coveo for Sitecore'''\n\nCoveo for Sitecore is an integrated website search product to be used in conjunction with Sitecore’s Customer Experience Platform.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Sitecore console.\n\n'''Coveo for Salesforce'''\n\nCoveo for Salesforce is an integrated CRM search product to be used in conjunction with Salesforce.com Service Cloud and Communities Editions.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Salesforce console.\n\n==Customers==\nCoveo claims its clients include more than 700 implementations including AmerisourceBergen, CA, California Water Service Co., Deloitte, ESPN, Haley & Aldrich, GEICO, Lockheed Martin, P&G, PRTM, PricewaterhouseCoopers, Rabobank, SNC-Lavalin, Spencer Stuart, Theodoor Gilissen, and the U.S. Navy.<ref>{{cite web|url=http://www.coveo.com/en/~/media/Files/about-us/Coveo-Corporate-Fact-Sheet-Q109.ashx |title=Coveo corporate fact sheet |date= |accessdate=2011-02-27}}</ref> These companies were also mentioned while not confirmed by a citation: HP, PwC, Netezza Corporation, NATO, NASA, AC Nielsen, among many others.{{Citation needed|date=February 2010}}\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.coveo.com/ Coveo.com]\n\n[[Category:Companies based in Quebec City]]\n[[Category:Information retrieval organizations]]\n[[Category:BlackBerry development software]]"]
['Category:Yandex', '31871742', '{{commonscat|Yandex}}\n{{Cat main|Yandex}}\n\n[[Category:Wikipedia categories named after information technology companies]]\n[[Category:Wikipedia categories named after companies of Russia]]\n[[Category:Web portals]]\n[[Category:Internet search engines]]\n[[Category:Internet companies of Russia]]\n[[Category:Companies listed on NASDAQ]]\n[[Category:Internet in Russia]]\n[[Category:Internet properties established in 1997]]\n[[Category:Russian websites]]\n[[Category:Information retrieval organizations]]']
['Automatic Content Extraction', '33675011', "{{Multiple issues|\n{{citation style|date=December 2011}}\n{{technical|date=October 2012}}\n{{abbreviations|date=October 2012}}\n}}\n'''Automatic Content Extraction (ACE)''' is a research program for developing advanced [[Information extraction]] [[technologies]] convened by the [[National Institute of Standards and Technology | NIST]] from 1999 to 2008, succeeding [[Message Understanding Conference | MUC]] and preceding [http://www.nist.gov/tac/ Text Analysis Conference]. \n\n==Goals and Efforts==\nIn general objective, the ACE program is motivated by and addresses the same issues as the MUC program that preceded it. The ACE program, however, defines the research objectives in terms of the target objects (i.e., the entities, the relations, and the events) rather than in terms of the words in the text. For example, the so-called “named entity” task, as defined in MUC, is to identify those words (on the page) that are names of entities. In ACE, on the other hand, the corresponding task is to identify the entity so named. This is a different task, one that is more abstract and that involves inference more explicitly in producing an answer. In a real sense, the task is to detect things that “aren’t there”.\n\nWhile the ACE program is directed toward extraction of information from [[Sound|audio]] and [[image]] sources in addition to pure text, the research effort is restricted to information extraction from text. The actual [[transduction (machine learning)|transduction]] of audio and image data into text is not part of the ACE research effort, although the processing of [[Speech recognition | ASR]] and [[Optical character recognition | OCR]] output from such transducers is.\n\nThe effort involves:\n* defining the research tasks in detail,\n* collecting and annotating data needed for training, development, and evaluation,\n* supporting the research with evaluation tools and [[research workshop]]s.\n\n==Topics and exercises==\nGiven a text in [[natural language]], the ACE challenge is to detect:\n# '''entities''' mentioned in the text, such as: persons, organizations, locations, facilities, weapons, vehicles, and geo-political entities.\n# '''relations''' between entities, such as: person A is the manager of company B. Relation types include: role, part, located, near, and social.\n# '''events''' mentioned in the text, such as: interaction, movement, transfer, creation and destruction.\n\nThe program relates to [[English language|English]], [[Arabic language|Arabic]] and [[Chinese language|Chinese]] texts.\n\nThe ACE corpus is one of the standard benchmarks for testing new information extraction [[algorithm]]s.\n\n==References==\n* George Doddington@NIS T, Alexis Mitchell@LD C, Mark Przybocki@NIS T, Lance Ramshaw@BB N, Stephanie Strassel@LD C, Ralph Weischedel@BB N. [http://www.citeulike.org/user/erelsegal-halevi/article/10003935 The automatic content extraction (ACE) program–tasks, data, and evaluation.] 2004\n\n==External links==\n* [http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ MUC] - ACE's predecessor.\n* [http://projects.ldc.upenn.edu/ace/ ACE] (LDC)\n* [http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE] (NIST)\n\n[[Category:Information retrieval organizations]]"]
['Datanet', '13555870', '{{Use mdy dates|date=September 2011}}\n\'\'\'DataNet\'\'\', or \'\'\'Sustainable Digital Data Preservation and Access Network Partner\'\'\' was a research program of the U.S. [[National Science Foundation]] Office of Cyberinfrastructure.  The office announced a request for proposals with this title on September 28, 2007.<ref name="datanetprogram">{{cite web\n|url=http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141\n|publisher=National Science Foundation\n|title=Sustainable Digital Data Preservation and Access Network Partners (DataNet) Program Summary\n|date=September 28, 2007\n|accessdate=October 3, 2007\n}}</ref>  The lead paragraph of its synopsis describes the program as:\n\n<blockquote>Science and engineering research and education are increasingly digital and increasingly data-intensive.  Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams.  This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations (dubbed DataNet Partners) that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.</blockquote>\n\nThe introduction in the solicitation<ref name="datanetsolicitation">{{cite web\n|url=http://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07601\n|publisher=National Science Foundation\n|title=Sustainable Digital Data Preservation and Access Network Partners Program Announcements & Information\n|date=September 28, 2007\n|accessdate=October 3, 2007\n}}</ref> goes on to say:\n\n<blockquote>Chapter 3 (Data, Data Analysis, and Visualization) of [http://www.nsf.gov/pubs/2007/nsf0728/index.jsp NSF’s Cyberinfrastructure Vision for 21st century Discovery] presents a vision in which “science and engineering digital data are routinely deposited in well-documented form, are regularly and easily consulted and analyzed by specialists and non-specialists alike, are openly accessible while suitably protected, and are reliably preserved.” The goal of this solicitation is to catalyze the development of a system of science and engineering data collections that is open, extensible and evolvable.</blockquote>\n\nThe initial plan called for a $100 million initiative: five awards of $20&nbsp;million each over five years with the possibility of continuing funding.  Awards were given in two rounds. In the first round, for which  full proposals were due on March 21, 2008, two DataNet proposals were awarded. [[DataONE]],<ref>{{cite web|author=William Michener |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by William Michener at the [[University of New Mexico]] covers ecology, evolutionary, and earth science. The Data Conservancy,<ref>{{cite web|author=Sayeed Choudhury |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by Sayeed Choudhury of [[Johns Hopkins University]], focuses on astronomy, earth science, life sciences, and social science. \n\nFor the second round, preliminary proposals were due on October 6, 2008 and full proposals on February 16, 2009. Awards from the second round were greatly delayed, and funding was reduced substantially from $20 million per project to $8 million.<ref>{{cite web|author=National Science Foundation |url=http://www.nsf.gov/awardsearch/simpleSearchResult?queryText=%22datanet+full+proposal%3A%22 |title=NSF DataNet Awards |publisher=www.nsf.gov | accessdate=2013-01-19}}</ref> Funding for three second round projects began in Fall 2011. SEAD: Sustainable Environment through Actionable Data,<ref>{{cite web|author=[[Margaret Hedstrom]] |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19|display-authors=etal}}</ref> led by [[Margaret Hedstrom]] of the [[University of Michigan]], seeks to provide data curation software and services for the "long tail" of small- and medium-scale data producers in the domain of sustainability science. The DataNet Federation Consortium,<ref>{{cite web|author=[[Reagan Moore]] |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by Reagan Moore of the [[University of North Carolina]], uses the integrated Rule-Oriented Data System (iRODS) to provide data grid infrastructure for science and engineering. \'\'Terra Populus\'\',<ref>{{cite web|author=[[Steven Ruggles]] |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by [[Steven Ruggles]] of the [[University of Minnesota]] focuses on tools for data integration across the domains of social science and environmental data, allowing interoperability of the three major data formats used in these domains: microdata, areal data, and raster data.\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [http://www.dataone.org DataONE]\n* [http://dataconservancy.org/ Data Conservancy]\n* [http://sead-data.net/ SEAD Sustainable Environment - Actionable Data]\n* [http://datafed.org/ DataNet Federation Consortium]\n* [http://www.terrapop.org/ Terra Populus: Integrated Data on Population and the Environment] \n \n\n[[Category:National Science Foundation]]\n[[Category:Science and technology in the United States]]\n[[Category:Information retrieval organizations]]\n[[Category:Digital library projects]]']
['ChemRefer', '11242818', '{{Orphan|date=February 2009}}\n{{Infobox website\n| name = Chemrefer\n| logo = [[Image:Chemrefer.png]]\n| screenshot = \n| caption = \n| url = http://www.chemrefer.com\n| commercial = Yes\n| type = [[Search engine]]\n| language = English\n| registration = Not Applicable\n| owner = ChemRefer Limited\n| author = William James Griffiths\n| launch date = 2006\n| current status = Offline\n| revenue = \n}}\n\'\'\'ChemRefer\'\'\' is a service that allows searching of freely available and full-text chemical and pharmaceutical literature that is published by authoritative sources.<ref>{{citation|journal=Science Articles |title= Science News Forum|publisher= SciScoop |date=May 19, 2006|url= http://www.sciscoop.com/story/2006/5/19/95844/6293}}</ref>\n\nFeatures include basic and advanced search options, [[mouseover]] detailed view, an integrated chemical structure drawing and search tool, downloadable [[toolbar]], customized [[RSS]] feeds, and newsletter.\n\nChemRefer is primarily of use to readers who do not have subscriptions for accessing restricted chemical literature, and to publishers who offer either [[Open access (publishing)|open access]] or [[hybrid open access journal]]s and seek to attract further subscriptions by publicly releasing part of their archive.\n\n==See also==\n*[[Google Scholar]]\n*[[Windows Live Academic]]\n*[[BASE (search engine)|BASE]]\n*[[PubMed]]\n\n==References==\n{{reflist}}\n\n==External links==\n===Recommendations & reviews===\n*[https://web.archive.org/web/20060902072725/http://www.rowland.harvard.edu/resources/library/lnn_archive/031706.php Cited as an "Internet Site of the Week"] by the library of the [[Rowland Institute for Science]] at [[Harvard University]]\n*[https://web.archive.org/web/20070804051550/http://infoweb.nrl.navy.mil:80/index.cfm?i=156 Recommended in the list of chemical literature databases] by the library of the [[United States Naval Research Laboratory]]\n*[https://web.archive.org/web/20070212122105/http://www.mta.ca:80/library/subject_chemistry.html Recommended in the list of chemical literature databases] by the library of [[Mount Allison University]]\n*[http://depth-first.com/articles/2007/01/15/chemrefer-free-direct-access-to-the-primary-literature Review of ChemRefer] at Depth-First chemoinformatics magazine\n*[https://web.archive.org/web/20080917155607/http://recherche-technologie.wallonie.be:80/fr/particulier/menu/revue-athena/l-annuaire-de-liens/internet/moteurs-de-recherche/www-chemrefer-com.html?PROFIL=PART Recommended in the list of chemical literature databases] by the Technology Research Portal, Belgium\n*[http://www.certh.gr/0E9BF53C.en.aspx Recommended in the list of chemical literature databases] by the Centre for Research and Technology, Thessaloniki\n\n===Background===\n*[http://www.reactivereports.com/56/56_0.html Interview with William James Griffiths] at Reactive Reports chemistry magazine\n*[http://www.earlham.edu/~peters/fos/overview.htm Open access overview] by Professor Peter Suber, Earlham College\n\n[[Category:Scholarly search services]]\n[[Category:Chemistry literature]]\n[[Category:Information retrieval systems]]\n[[Category:Open access projects]]\n\n{{searchengine-website-stub}}']
['IBM Omnifind', '13762814', "'''IBM OmniFind''' was an [[enterprise search]] platform from [[IBM]].\nIt did come in several packages adapted to different business needs, including OmniFind Enterprise Edition, OmniFind Enterprise Starter Edition, and OmniFind Discovery Edition.<ref>[http://www-01.ibm.com/software/ecm/omnifind/library.html IBM - OmniFind - Library]</ref> IBM OmniFind as a standalone product was withdrawn in April 2011<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS911-075 IBM US Announcement Letter]</ref> and is now part of [[IBM Watson Content Analytics with Enterprise Search]].<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?infotype=AN&subtype=CA&htmlfid=897/ENUS211-133 IBM US Announcement Letter]</ref>\n\n'''IBM OmniFind Yahoo! Edition''' was a free-of-charge version that could handle up to 500,000 documents in its index and was intended for small businesses. IBM OmniFind Yahoo! Edition was simple to install, provided a user friendly front end for administration, and incorporated technology from the open source [[Lucene]] project. IBM withdrew this product from marketing effective September 22, 2010 and withdrew support effective June 30, 2011.<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS910-115 IBM US Announcement Letter]</ref>\n\n'''IBM OmniFind Personal E-mail Search''' was a research product launched in 2007 for doing [[semantic search]] over personal emails by extracting and organizing concepts and relationships (such as phone numbers and addresses). The project appears to have been silently abounded sometimes around 2010.\n\n== See also ==\n* [[Languageware]]\n* [[UIMA]]\n* [[Comparison of enterprise search software]]\n* [[List of enterprise search vendors]]\n\n==External links==\n* [http://www.ibm.com/software/data/enterprise-search/ IBM OmniFind]\n* [http://omnifind.ibm.yahoo.com/ IBM OmniFind Yahoo! Edition] {{Dead link|date=May 2012}}\n* [https://web.archive.org/web/20071030125647/http://www.alphaworks.ibm.com/tech/emailsearch IBM OmniFind Personal E-mail Search] \n* [http://www.opentestsearch.com/search-engines/ibm-omnifind-yahoo-edition-review/ Online demo and review of IBM OmniFind Yahoo! Edition]\n\n==Notes==\n{{reflist}}\n\n[[Category:IBM software|OmniFind]]\n[[Category:Information retrieval systems]]"]
['Statistically improbable phrase', '2724706', 'A \'\'\'statistically improbable phrase\'\'\' (\'\'\'SIP\'\'\') is a phrase or set of words that occurs more frequently in a document (or collection of documents) than in some larger [[Text corpus|corpus]].<ref>{{cite web|url=http://courses.cms.caltech.edu/cs145/2011/wikipedia.pdf |title=SIPping Wikipedia |website=Courses.cms.caltech.edu |accessdate=2017-01-01}}</ref><ref>{{cite web|url=https://www.plagiarismtoday.com/2012/07/03/how-long-should-a-statistically-improbably-phrase-be/|title=How Long Should a Statistically Improbably Phrase Be?|author=Jonathan Bailey|date=3 July 2012|work=Plagiarism Today}}</ref><ref>{{cite journal|url=http://bioinformatics.oxfordjournals.org/content/26/11/1453|title=Identifying duplicate content using statistically improbable phrases|first1=Mounir|last1=Errami|first2=Zhaohui|last2=Sun|first3=Angela C.|last3=George|first4=Tara C.|last4=Long|first5=Michael A.|last5=Skinner|first6=Jonathan D.|last6=Wren|first7=Harold R.|last7=Garner|date=1 June 2010|publisher=|journal=Bioinformatics|volume=26|issue=11|pages=1453–1457|accessdate=1 January 2017|via=bioinformatics.oxfordjournals.org|doi=10.1093/bioinformatics/btq146|pmid=20472545|pmc=2872002}}</ref> [[Amazon.com]] uses this concept in determining keywords for a given book or chapter, since keywords of a book or chapter are likely to appear disproportionately within that section.<ref>{{cite web|url=http://www.amazon.com/gp/search-inside/sipshelp.html|title=What are Statistically Improbable Phrases?|accessdate=2007-12-18|publisher=[[Amazon.com]]}}</ref><ref>{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2005/08/29/AR2005082901873.html|title=Amazon\'s Vital Statistics Show How Books Stack Up|last=Weeks|first=Linton|work=[[The Washington Post]]|date=August 30, 2005|accessdate=September 8, 2015}}</ref> [[Christian Rudder]] has also used this concept with data from [[Online dating service|online dating profiles]] and [[Twitter]] posts to determine the phrases most characteristic of a given race or gender in his book \'\'Dataclysm\'\'.<ref>{{cite book |last=Rudder |first=Christian |date=2014 |title=Dataclysm: Who We Are When We Think No One\'s Looking |location=New York |publisher=Crown Publishers |page= |isbn=978-0-385-34737-2}}</ref>\n\n==Example== \nIn a document about [[computer]]s, the most common word is likely to be the word "the", but since "the" is the most commonly used word in the English language, it is likely that any given document will have the word "the" used very frequently.  However, a word like "program" might occur in the document at a much higher rate than its average rate in the English language.  Hence, it is a word unlikely to occur in any given document, but \'\'did\'\' occur in the document given.  "Program" would be a statistically improbable phrase.\n\nThe statistically improbable phrases of Darwin\'s \'\'[[On the Origin of Species]]\'\' are: \'\'temperate productions, genera descended, transitional gradations, unknown progenitor, fossiliferous formations, our domestic breeds, modified offspring, doubtful forms, closely allied forms, profitable variations, enormously remote, transitional grades, very distinct species\'\' and \'\'mongrel offspring\'\'.<ref>[http://crookedtimber.org/2005/04/02/sociologically-improbable-phrases/ Sociologically Improbable Phrases] Crooked Timber April 2005</ref>\n\n==See also==\n* [[Googlewhack]] – A pair of words occurring on a single webpage, as indexed by Google\n* [[tf-idf]] – A statistic used in information retrieval and text mining\n\n==References==\n{{Reflist}}\n\n{{Amazon}}\n\n[[Category:Amazon.com]]\n[[Category:Bookselling]]\n[[Category:Information retrieval systems]]']
['Wolfram Alpha', '21903944', '{{Use mdy dates|date=October 2014}}\n{{Infobox website\n| name = Wolfram Alpha\n| logo =  Wolfram Alpha December 2016.svg\n| caption = Wolfram Alpha is based on the computational platform [[Mathematica]], written by British scientist [[Stephen Wolfram]] in 1988.\n| url = {{URL|http://www.wolframalpha.com/}}.\n| slogan             = Making the world’s knowledge computable.<ref>[http://www.wolframalpha.com/about.html Wolfram Alpha About page]</ref>\n| commercial = Yes\n| type =  [[Answer engine]]\n| registration = Optional\n| owner = Wolfram Alpha LLC\n| author = [[Wolfram Research]]\n| alexa  = {{Decrease}} 1,932 ({{as of|2015|26|31|alt=March 2015}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/wolframalpha.com |title= Wolframalpha.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2015-03-31}}</ref>\n| num_employees ≈ 200 (as of 2012)\n| programming_language = [[Wolfram Language]]\n| launch date = {{start date and age|2009|5|18}}<ref name="launch date">{{cite web |author=The Wolfram&#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |work=Wolfram&#124;Alpha Blog |publisher=Wolfram Alpha |date=May 8, 2009 |accessdate=2013-02-09}}</ref> (official launch)<br>{{start date|2009|5|15}}<ref name="updated launch detail">{{cite web |author=The Wolfram&#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/12/going-live-and-webcasting-it/ |work=Wolfram&#124;Alpha Blog |title=Going Live—and Webcasting It |publisher=Wolfram Alpha |date=May 12, 2009 |accessdate=2013-02-09}}</ref> (public launch)\n| current status = Active\n}}\n\n\'\'\'Wolfram Alpha\'\'\' (also styled \'\'\'WolframAlpha\'\'\' and \'\'\'Wolfram|Alpha\'\'\') is a computational knowledge engine<ref name=Guardiandatasource>{{cite news |title=Where does Wolfram Alpha get its information? |author=Bobbie Johnson |publisher=The Guardian |date=May 21, 2009 |accessdate=2013-03-08 |url=https://www.theguardian.com/technology/2009/may/21/1 }}</ref> or [[answer engine]] developed by [[Wolfram Research]], which was founded by [[Stephen Wolfram]]. It is an online service that answers factual queries directly by computing the answer from externally sourced "curated data",<ref>{{Cite web|title = About Wolfram{{!}}Alpha: Making the World\'s Knowledge Computable|url = http://www.wolframalpha.com/about.html|website=wolframalpha.com|accessdate = 2015-11-25}}</ref> rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] might.<ref>{{cite news |url=https://www.theguardian.com/technology/2009/mar/09/search-engine-google |title=British search engine \'could rival Google\' |last=Johnson |first=Bobbie |date=March 9, 2009 |work=The Guardian |publisher=Guardian News and Media |location=UK |accessdate=2013-02-09}}</ref>\n\nWolfram Alpha, which was released on May 18, 2009, is based on Wolfram\'s earlier flagship product [[Wolfram Mathematica]], a computational platform or toolkit that encompasses computer algebra, symbolic and numerical computation, visualization, and statistics capabilities.<ref name="launch date" /> Additional data is gathered from both academic and commercial websites such as the CIA\'s \'\'[[The World Factbook]]\'\', the United States Geological Survey, a Cornell University Library publication called \'\'All About Birds\'\', \'\'Chambers Biographical Dictionary\'\', [[Dow Jones]], the \'\'Catalogue of Life\'\',<ref name=Guardiandatasource /> [[CrunchBase]],<ref name=techcrunch>{{cite news |last=Dillet |first=Romain |title=Wolfram Alpha Makes CrunchBase Data Computable Just In Time For Disrupt SF |url=http://techcrunch.com/2012/09/07/wolfram-alpha-makes-crunchbase-data-computable-just-in-time-for-disrupt/ |publisher=TechCrunch |date=September 7, 2012 |accessdate=2013-02-09}}</ref> [[Best Buy]],<ref>{{cite news |last=Golson |first=Jordan |title=Wolfram Delivers Siri-Enabled Shopping Results From Best Buy |url=http://www.macrumors.com/2011/12/16/wolfram-delivers-siri-enabled-shopping-results-from-best-buy/ |publisher=MacRumors |date=December 16, 2011 |accessdate=2013-02-09}}</ref> the [[Federal Aviation Administration|FAA]]<ref>{{cite news |last=Barylick |first=Chris |title=Wolfram Alpha search engine now tracks flight paths, trajectory information |url=http://www.engadget.com/2011/11/19/wolfram-alpha-search-engine-now-tracks-flight-paths-trajectory/ |publisher=Engadget |date=November 19, 2011 |accessdate=2013-02-09}}</ref> and optionally a user\'s Facebook account.\n\n== Overview ==\nUsers submit queries and computation requests via a text field.  Wolfram Alpha then computes answers and relevant visualizations from a [[knowledge base]] of [[Data curation|curated]], [[structured data]] that come from other sites and books. The site "use[s] a portfolio of automated and manual methods, including statistics, visualization, source cross-checking, and expert review."<ref>{{cite web |title=Data in Wolfram&#124;Alpha |url=http://www.wolframalpha.com/faqs5.html |website=Wolfram Alpha |accessdate=4 August 2015}}</ref> The curated data makes Alpha different from [[semantic search]] engines, which index a large number of answers and then try to match the question to one.\n\nWolfram Alpha can only provide robust query results based on computational facts, not queries on the social sciences, cultural studies or even many questions about history where responses require more subtlety and complexity. It is able to respond to particularly-phrased [[natural language understanding|natural language]] fact-based questions such as "Where was [[Mary Robinson]] born?" or more complex questions such as "How old was [[Queen Elizabeth II]] in 1974?" It displays its "Input interpretation" of such a question, using standardized phrases such as "age | of Queen Elizabeth II (royalty) | in 1974", the answer of which is "Age at start of 1974: 47 years", and a biography link. Wolfram Alpha does not answer queries which require a narrative response such as "What is the difference between the Julian and the Gregorian calendars?" but will answer factual or computational questions such as "June 1 in Julian calendar".\n\nMathematical symbolism can be parsed by the engine, which typically responds with more than the numerical results. For example, "lim(x->0) (sin x)/x" yields the correct [[limit (functions)|limit]]ing value of 1, as well as a plot, up to 235 terms ({{as of|2013|lc=y}}) of the [[Taylor series]], and (for registered users) a possible derivation using [[L\'Hôpital\'s rule]]. It is also able to perform calculations on data using more than one source. For example, "What is the [[List of countries by GDP (nominal) per capita|fifty-second smallest]] country by [[GDP per capita]]?" yields [[Nicaragua]], $1160 per year.\n\n== Technology ==\nWolfram Alpha is written in 15 million lines of [[Wolfram Language]] code<ref>{{cite web |author=WolframResearch |url=https://www.youtube.com/watch?v=56ISaies6Ws#t=927s |title=Stephen Wolfram: The Background and Vision of Mathematica |publisher=Youtube.com |date=October 10, 2011 |accessdate=2013-02-09}}</ref> and runs on more than 10,000 CPUs.<ref>{{cite news |first=Frederic |last=Lardinois |url=http://readwrite.com/2009/04/25/wolframalpha_our_first_impressions |title=Wolfram&#124;Alpha: Our First Impressions |date=April 25, 2009 |publisher=ReadWriteWeb |accessdate=2013-02-09}}</ref><ref>{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2009/05/15/wolframalpha-is-launching-made-possible-by-mathematica/ |title=Wolfram&#124;Alpha Is Launching: Made Possible by \'\'Mathematica\'\' |work=WolframAlpha Blog |publisher=Wolfram Alpha |date=May 15, 2009 |accessdate=2013-02-09}}</ref> The database currently includes hundreds of datasets, such as "All Current and Historical Weather." The datasets have been accumulated over several years.<ref>{{cite web |title=Taking a first bite out of Wolfram Alpha | first=Jane Fae | last=Ozimek |work=The Register |date=May 18, 2009 |url=http://www.theregister.co.uk/2009/05/18/wolfram_alpha/ |accessdate=2013-02-09}}</ref> The curated (as distinct from auto-generated) datasets are checked for quality either by a scientist or other expert in a relevant field, or someone acting in a clerical capacity who simply verifies that the datasets are "acceptable".<ref name=semanticabyss>{{cite web |title=The Semantic Abyss - Plumbing the Semantic Web: Exploring the depths of the semantic gap between the Semantic Web and real world users and consumers |url=http://semanticabyss.blogspot.ca/2009/03/what-is-curated-data.html |year=2009 |author=Jack Krupansky}}</ref>{{unreliable source?|date=September 2015}}\n\nOne example of a live dataset that Wolfram Alpha can use is the profile of a [[Facebook]] user, through inputting the "facebook report" query. If the user authorizes Facebook to share his or her account details with the Wolfram site, Alpha can generate a "personal analytics" report containing the age distribution of friends, the frequency of words used in status updates and other detailed information.<ref name=techland>{{cite news |first=Thomas E. |last=Weber |url=http://techland.time.com/2012/09/05/wolfram-alphas-facebook-analytics-tool-digs-deep-into-your-social-life/ |title=Wolfram Alpha\'s Facebook Analytics Tool Digs Deep into Your Social Life |work=Tech |publisher=Time Magazine |date=September 5, 2012 |accessdate=2013-02-09}}</ref> Within two weeks of launching the Facebook analytics service, 400,000 users had used it.<ref>{{cite news |last=R. |first=A. |title=Visualising Facebook Who am I? |url=http://www.economist.com/blogs/graphicdetail/2012/09/visualising-facebook |publisher=The Economist |work=Graphic detail |date=September 21, 2012 |accessdate=2013-02-09}}</ref> Downloadable query results are behind a pay wall but summaries are accessible to free accounts.<ref name=publiclibraries>{{cite web |url=http://publiclibrariesonline.org/2013/03/a-wolf-or-a-ram-what-is-wolfram-alpha/ |title=A Wolf or a Ram? What is Wolfram Alpha? |author=Joanna Nelson |date=March 4, 2013 |publisher=Public Libraries Online }}</ref>\n\n== Licensing partners ==\nWolfram Alpha has been used to power some searches in the [[Microsoft]] [[Bing (search engine)|Bing]] and [[DuckDuckGo]] search engines.<ref>{{cite news |first=Tom |last=Krazit |url=http://news.cnet.com/8301-30684_3-10315117-265.html |title=Bing strikes licensing deal with Wolfram Alpha |publisher=CNET |date=August 21, 2009 |accessdate=2013-02-09}}</ref><ref>{{cite web |author=The Wolfram&#124;Alpha Team |date=April 18, 2011 |url=http://blog.wolframalpha.com/2011/04/18/wolframalpha-and-duckduckgo-partner-on-api-binding-and-search-integration/ |title=Wolfram&#124;Alpha and DuckDuckGo Partner on API Binding and Search Integration |work=Wolfram&#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}</ref> For factual [[question answering]], it is also queried by Apple\'s [[Siri (software)|Siri]], Samsung\'s [[S Voice]], as well as Dexetra\'s [[speech recognition]] software for the [[Android (operating system)|Android]] platform, Iris, and the voice control software on [[BlackBerry 10]].<ref>{{cite web|url=http://www.berryreview.com/2013/10/21/blackberry-teams-up-with-wolfram-alpha-for-blackberry-10-voice-control/|title=BlackBerry Teams Up with Wolfram Alpha For BlackBerry 10 Voice Control|work=BerryReview}}</ref>\n\n== History ==\nLaunch preparations began on May 15, 2009 at 7&nbsp;pm [[Central Daylight Time (North America)#Central Daylight Time|CDT]] and were broadcast live on [[Justin.tv]]. The plan was to publicly launch the service a few hours later, with expected issues due to extreme load. The service was officially launched on May 18, 2009.<ref name="BBC">{{cite news |url=http://news.bbc.co.uk/1/hi/technology/8052798.stm |title=Wolfram \'search engine\' goes live |publisher=BBC News |date=May 18, 2009 |accessdate=2013-02-09}}</ref>\n\nWolfram Alpha has received mixed reviews.<ref name="spivack">{{cite web |first=Nova |last=Spivack |title=Wolfram Alpha is Coming – and It Could be as Important as Google |date=March 7, 2009 |url=http://www.novaspivack.com/uncategorized/wolfram-alpha-is-coming-and-it-could-be-as-important-as-google |accessdate=2013-02-09 |publisher=Nova Spivack – Minding the Planet}}</ref><ref>{{cite news |first=Ryan |last=Singel |title=Wolfram&#124;Alpha Fails the Cool Test |date=May 18, 2009 |url=http://www.wired.com/epicenter/2009/05/wolframalpha-fails-the-cool-test/ |publisher=Wired |accessdate=2013-02-09}}</ref> Wolfram Alpha advocates point to its potential, some even stating that how it determines results is more important than current usefulness.<ref name="spivack"/>\n\nOn December 3, 2009, an [[iPhone]] app was introduced. Some users<ref name="ios-price">{{cite web |first=MG |last=Siegler |url=http://techcrunch.com/2009/12/03/wolfram-alpha-iphone-app/ |title=Nice Try, Wolfram Alpha. Still Not Paying $50 For Your App. |publisher=TechCrunch |date=December 3, 2009 |accessdate=2013-02-09}}</ref> considered the initial $50 price of the [[iOS]] app unnecessarily high, since the same features could be freely accessed by using a web browser instead. They also complained about the simultaneous removal of the mobile formatting option for the site.<ref name="mobile-format">{{cite news |url=http://www.tuaw.com/2009/12/03/wolframalpha-iphone-formatted-web-page-no-longer-available/ |first=TJ |last=Luoma |title=WolframAlpha iPhone-formatted web page no longer available |publisher=TUAW |date=December 3, 2009 |accessdate=2013-02-09}}</ref> Wolfram responded by lowering the price to $2, offering a refund to existing customers<ref name="refund">{{cite web|last=Broida |first=Rick |url=http://reviews.cnet.com/8301-19512_7-10471978-233.html |title=Get Wolfram Alpha app for $1.99-and a refund if you paid more |publisher=CNET |date=April 1, 2010 |accessdate=2012-02-28}}</ref> and re-instating the mobile site.\n\nOn October 6, 2010 an Android version of the app was released<ref>{{cite news |url=http://techcrunch.com/2010/10/06/wolframalphas-android-app-now-available/ |title=Wolfram Alpha\'s Android app now available |first=Leena |last=Rao |publisher=TechCrunch |date=October 6, 2010 |accessdate=2013-02-09}}</ref> and it is now available for Kindle Fire and Nook. (The Nook version is not available outside the US). A further 71 apps are available which use the Wolfram Alpha engine for specialized tasks.<ref>{{cite web |url=http://products.wolframalpha.com/mobile/ |title=Wolfram&#124;Alpha: Mobile & Tablet Apps |year=2013 |accessdate=2013-02-09 |publisher=Wolfram Alpha}}</ref>\n\n== Wolfram Alpha Pro ==\nOn February 8, 2012, Wolfram Alpha Pro was released,<ref name="WAProAnnounce">{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2012/02/08/announcing-wolframalpha-pro/ |title=Announcing Wolfram&#124;Alpha Pro |date=February 8, 2012 |work=Wolfram&#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}</ref> offering users additional features for a monthly subscription fee. A key feature is the ability to upload many common file types and data—including raw tabular data, images, audio, XML, and dozens of specialized scientific, medical, and mathematical formats—for automatic analysis. Other features include an extended keyboard, interactivity with [[Computable Document Format|CDF]], data downloads, in-depth step by step solution, the ability to customize and save graphical and tabular results<ref name="Hachman">{{cite news |last=Hachman |first=Mark |title=Data Geeks, Meet Wolfram Alpha Pro |publisher=[[PC Magazine]] |date=February 7, 2012 |url=http://www.pcmag.com/article2/0,2817,2399911,00.asp |accessdate=2012-02-15}}</ref> and extra computation time.<ref name="WAProAnnounce" />\n\nAlong with new premium features, Wolfram Alpha Pro has led to some changes in the free version of the site:\n* An increase in advertisements on the free site.\n* Text and PDF export options now require the user to set up a free account<ref name="WAProAnnounce" /> even though they existed before the introduction of Wolfram Alpha accounts.<ref>{{cite web|url=http://hplusmagazine.com/2009/06/24/users-guide-wolframalpha/|title=A User\'s Guide to Wolfram Alpha|first=Surfdaddy|last=Orca|publisher=H+ Magazine|date=2009-06-24|accessdate=2013-04-24}}</ref>\n* The option to request extra time for a long calculation used to be free<ref name="extra-time-before">{{cite web|url=http://web.mst.edu/~jkmq53/school/Fall_2011/English_160/files/Marlowe_Usability_Test.docx|title=Wolfram Alpha Usability Test Survey|first=James|last=Marlowe|year=2011|accessdate=2013-04-24}}</ref> but is now only available to subscribers.<ref name="WAProAnnounce" />\n* Step-by-Step limited to 3 for free users (previously uncapped)(no longer available).<ref name="StepByStep">{{cite web|url=http://blog.wolframalpha.com/2009/12/01/step-by-step-math/|title=Step-by-Step Math}}</ref>\n\n== Copyright claims ==\n\'\'[[InfoWorld]]\'\' published an article<ref name="copyright">{{cite web |last=McAllister |first=Neil |url=http://www.infoworld.com/d/developer-world/how-wolfram-alpha-could-change-software-248 |title=How Wolfram Alpha could change software |publisher=InfoWorld |date=July 29, 2009 |accessdate=2012-02-28}}</ref> warning readers of the potential implications of giving an automated website proprietary rights to the data it generates. [[Free software movement|Free software]] advocate [[Richard Stallman]] also opposes the idea of recognizing the site as a copyright holder and suspects that Wolfram would not be able to make this case under existing copyright law.<ref name="fsf">{{cite mailing list |url=http://lists.essential.org/pipermail/a2k/2009-August/004865.html |title=How Wolfram Alpha\'s Copyright Claims Could Change Software |date=August 4, 2009 |accessdate=2012-02-17 |mailinglist=[http://lists.essential.org/mailman/listinfo/a2k Access 2 Knowledge] |archiveurl=https://web.archive.org/web/20130428041345/http://lists.essential.org/pipermail/a2k/2009-August/004865.html |archivedate=April 28, 2013 |last=Stallman |first=Richard |authorlink=Richard Stallman}}</ref>\n\n== See also ==\n* [[Commonsense knowledge problem]]\n* [[Artificial general intelligence|Strong AI]]\n* [[Watson (computer)]]\n\n== References ==\n{{Reflist|colwidth=30em}}\n\n== Further reading ==\n* [http://www.businessweek.com/the_thread/techbeat/archives/2009/03/wolfram_alpha_a.html Wolfram Alpha: A New Way To Search?], Stephen Wildstrom, \'\'BusinessWeek\'\', March 9, 2009.\n* [http://www.informationweek.com/news/internet/search/showArticle.jhtml?articleID=215801388&subSection=News Stephen Wolfram\'s Answer To Google: If Wolfram/Alpha works as advertised, it will be able to do something Google can\'t: provide answers that don\'t already exist in indexed documents.] by Thomas Claburn, \'\'InformationWeek\'\', March 10, 2009.\n* [http://bits.blogs.nytimes.com/2009/03/09/better-search-doesnt-mean-beating-google/ Better Search Doesn’t Mean Beating Google] by Saul Hansell, \'\'The New York Times\'\', March 9, 2009.\n* [http://www.pcworld.com/article/160904/wolfram_alpha_will_take_your_questions_any_questions.html Wolfram Alpha will Take Your Questions – Any Questions], Ian Paul, \'\'PC World\'\', March 9, 2009.\n* [http://www.hplusmagazine.com/articles/ai/wolframalpha-searching-truth Wolfram Alpha: Searching for Truth: Stephen Wolfram talks with Rudy Rucker about his Upcoming Release] by [[Rudy Rucker]], \'\'H+ Magazine\'\'.\n*  [http://www.boston.com/business/technology/articles/2009/05/05/a_hungry_little_number_cruncher/ "A hungry little number cruncher: Wolfram Alpha search tool mines databases to yield math-based replies"] by [[Hiawatha Bray]], \'\'[[The Boston Globe]]\'\', May 5, 2009\n* [http://newsbreaks.infotoday.com/NewsBreaks/Wolfram-Alpha-Semantic-Search-Is-Born-53892.asp "Wolfram Alpha: Semantic Search is Born" by [[Woody Evans]], May 21, 2009.]\n\n== External links ==\n* {{official website}}\n\n{{Wolfram Research|state=uncollapsed}}\n{{computable knowledge}}\n{{Intelligent personal assistant software}}\n\n[[Category:Agent-based software]]\n[[Category:Computer algebra systems]]\n[[Category:Educational math software]]\n[[Category:Educational websites]]\n[[Category:Information retrieval systems]]\n[[Category:Intelligent software assistants]]\n[[Category:Internet properties established in 2009]]\n[[Category:Mathematics education]]\n[[Category:Natural language processing software]]\n[[Category:Open educational resources]]\n[[Category:Physics education]]\n[[Category:Semantic Web]]\n[[Category:Software calculators]]\n[[Category:Web analytics]]\n[[Category:Websites which mirror Wikipedia]]\n[[Category:Wolfram Research]]']
['RetrievalWare', '23327147', '{{COI|date=June 2009}}\n{{Infobox software\n| name                   = RetrievalWare\n| logo                   =\n| screenshot             =\n| caption                =\n| developer              = [[Fast Search & Transfer]], [[Convera]], Excalibur Technologies, ConQuest Software, Microsoft\n| latest release version = 8.2\n| latest release date    = {{release date|2006|10|13}}\n| latest preview version =\n| latest preview date    =\n| operating system       = [[Cross-platform]]\n| programming language   = [[C (programming language)|C]], [[C++]], [[Java (programming language)|Java]]\n| genre                  = [[Search algorithm|Search]] and [[Index (search engine)|Index]]\n| website                =\n}}\n\'\'\'RetrievalWare\'\'\' is an [[enterprise search|enterprise search engine]] emphasizing [[natural language processing]] and [[semantic networks]] which was commercially available from 1992 to 2007 and is especially known for its use by government intelligence agencies.<ref>{{cite news\n| url = http://www.washingtonpost.com/ac2/wp-dyn/A30161-2004Dec2\n| title = Agencies Find What They\'re Looking For|publisher = The Washington Post\n| date = 2004-12-03\n| first=David A.\n| last=Vise\n| accessdate=2010-05-22\n}}</ref>\n\n== History ==\n\nRetrievalWare was initially created by [http://www.linkedin.com/pub/paul-nelson/3/316/146 Paul Nelson], [http://kenclark7.home.comcast.net/~kenclark7/ Kenneth Clark], and [http://www.linkedin.com/in/edaddison Edwin Addison] as part of ConQuest Software. Development began in 1989, but the software was not commercially available on a wide scale until 1992. Early funding was provided by [[Rome Laboratory]] via a [[Small Business Innovation Research]] grant.<ref>\n{{citation\n| title = FY 1991 SBIR SOLICITATION - PHASE I AWARD ABSTRACTS - AIR FORCE PROJECTS - VOLUME III\n| pages = 70–71\n| date = 1992-07-06\n| url = http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA252509&Location=U2&doc=GetTRDoc.pdf\n}} - Note that "Synchronetics" was the original name for ConQuest Software Incorporated.\n</ref>\n\nOn July 6, 1995, ConQuest Software was merged with Excalibur Technologies<ref>{{cite press release\n | title = Excalibur Technologies to merge with ConQuest Software; text and multimedia information retrieval leaders join forces to expand products, channels and markets| publisher = Business Wire\n | date = 1995-07-06\n | url = http://findarticles.com/p/articles/mi_m0EIN/is_1995_July_6/ai_17215774/?tag=content;col1\n }}</ref> and the product was rebranded as RetrievalWare. On December 21, 2000, Excalibur Technologies was combined with [[Intel|Intel Corporation]]\'s Interactive Media Services division to form the [[Convera|Convera Corporation]].<ref>{{cite news\n | title = Intel and Excalibur Form Convera Corporation| publisher = Silicon Valley / San Jose Business Journal\n | date = 2000-12-21\n | url = http://sanjose.bizjournals.com/sanjose/stories/2000/12/25/daily5.html\n }}</ref> Finally, on April 9, 2007, the RetrievalWare software and business was purchased by [[Fast Search & Transfer]] at which point the product was officially retired.<ref name="fastpurchase">{{cite news\n | title = FAST Acquires Convera’s RetrievalWare Business| publisher = Information Today, Inc.\n | date = 2007-04-09\n | url = http://newsbreaks.infotoday.com/NewsBreaks/FAST-Acquires-Converas-RetrievalWare-Business-35840.asp\n | quote = While FAST will continue to support the RetrievalWare platform, it will not continue development on it or add new features. RetrievalWare customers will be offered an upgrade path to FAST’s own offering.\n }}</ref> [[Microsoft|Microsoft Corporation]] continues to maintain the product for its existing customer base.\n\nAnnual revenues for RetrievalWare peaked in 2001 at around $40 million US dollars.<ref>{{citation\n| title = Convera Corp · 10-K · For 1/1/01\n| date = 2001-01-01\n| url = http://www.secinfo.com/d12B5f.4f89a.c.htm\n}} - Indicates that Convera products accounted for 85% of the total revenue of $51.5 million.</ref>\n\n== Use of natural language techniques ==\n\nRetrievalWare is a relevancy ranking text search system with processing enhancements drawn from the fields of [[natural language processing|natural language processing (NLP)]] and [[semantic networks]]. NLP algorithms include dictionary-based [[stemming]] (also known as [[lemmatisation]]) and dictionary-based phrase identification. Semantic networks are used by RetrievalWare to expand the query words entered by the user to related terms with terms weights determined by the distance from the user\'s original terms. In addition to automatic expansion, a feedback-mode whereby users could choose the meaning of the word before performing the expansion was available. The first semantic networks were built using [[WordNet]].\n\nIn addition, RetrievalWare implemented a form of [[n-gram]] search (branded as APRP - Adaptive Pattern Recognition Processing<ref>[http://www.thefreelibrary.com/Excalibur+Announces+Excalibur+RetrievalWare+6.5+Featuring+...-a019849416 Excalibur Announces Excalibur RetrievalWare 6.5 Featuring RetrievalWare FileRoom] - Contains a description of APRP</ref>), designed to search over documents with [[Optical character recognition|OCR]] errors. Query terms are divided into sets of 2-grams which are used to locate similarly matching terms from the [[inverted index]]. The resulting matches are weighted based on similarly measures and then used to search for documents.\n\nAll of these features were available no later than 1993<ref name="trec2">[http://trec.nist.gov/pubs/trec2/papers/txt/25.txt Site Report for the Text REtrieval Conference by ConQuest Software Inc. (TREC2)] - Find the complete proceedings [http://trec.nist.gov/pubs/trec2/t2_proceedings.html here]</ref> and ConQuest software has claimed that it was the first commercial text-search system to implement these techniques.<ref>{{cite press release\n | title = Homework Helper debuts on Prodigy using ConQuest search engine| publisher = Business Wire\n | date = 1995-02-09\n | url = http://findarticles.com/p/articles/mi_m0EIN/is_1995_Feb_9/ai_16432681/\n | quote = ConQuest is the only search engine which uses dictionaries, thesauri and other lexical resources to build in a semantic knowledgebase of over 440,000 word meanings, and 1.6 million word relationships.\n }}</ref>\n\n== Other notable features ==\n\nOther notable features of RetrievalWare include distributed search servers,<ref name="trec2"/> synchronizers for indexing external [[content management system]]s and [[relational database]]s,<ref name="kmref">{{cite news\n| url = http://www.kmworld.com/Articles/Editorial/Feature/Excalibur-RetrievalWare-more-than-information-retrieval--9139.aspx\n| title = Excalibur RetrievalWare: more than information retrieval\n| publisher = KMWorld\n| date = 1999-10-01\n}}</ref> a heterogeneous security model,<ref name="kmref"/> [[document classification|document categorization]],<ref name="kmref"/> real-time document-query matching (profiling),<ref name="trec2"/> multi-lingual searches (queries containing terms from multiple languages searching for documents containing terms from multiple languages), and cross-lingual searches (queries in one language searching for documents in a different language).<ref>{{cite news\n| title = Multimedia search, retrieval, categorization\n| url = http://www.kmworld.com/Articles/News/Breaking-News/Multimedia-search,-retrieval,-categorization-12763.aspx\n| date = 2002-03-25\n| publisher = KMWorld\n}}</ref>\n\n== Participation in TREC ==\n\nRetrievalWare participated in the [[Text REtrieval Conference (TREC)|Text REtrieval Conference]] in 1992 (TREC-1), 1993 (TREC-2), and 1995 (TREC-4).\n\nIn TREC-1<ref name="trec1">[http://trec.nist.gov/pubs/trec1/papers/21.txt   Site Report for the Text REtrieval Conference by ConQuest Software Inc. (TREC-1)] - Find the complete proceedings [http://trec.nist.gov/pubs/trec1/t1_proceedings.html here]</ref> and TREC-4,<ref>[http://trec.nist.gov/pubs/trec4/papers/excalibur.ps.gz The Excalibur TREC-4 System, Preparations, and Results] - A PDF version of which can be found [http://www.pnelsoncomposer.com/writings/excalibur-trec4.pdf here] and the complete proceedings can be found [http://trec.nist.gov/pubs/trec4/t4_proceedings.html here]</ref> the RetrievalWare runs for manually entered queries produced the best results based on the 11-point averages over all search engines which participated in the \'\'ad hoc\'\' category where search engines are allowed a single opportunity to process previously unknown queries against an existing database.\n\n== References ==\n{{Reflist}}\n\n== External links ==\n\n*  [http://www.saoug.org.za/archive/1999/9907.pdf Marketing presentation on RetrievalWare semantic networks and adaptive pattern recognition algorithms]\n\n{{DEFAULTSORT:Retrievalware}}\n[[Category:Information retrieval systems]]']
['Poliqarp', '2398780', "'''Poliqarp''' is an [[open source]] [[search engine]] designed to process [[text corpus|text corpora]], among others the [[National Corpus of Polish]] created at the Institute of Computer Science, [[Polish Academy of Sciences]].\n\n==Features==\n* Custom [[query language]]\n* Two-level [[regular expressions]]:\n** operating at the level of characters in words\n** operating at the level of words in statements/paragraphs\n* Good performance\n* Compact corpus representation (compared to similar projects)\n* Portability across operating systems: [[Linux]]/[[BSD]]/[[Win32]]\n* Lack of portability across [[endianness]] (current release works only on little endian devices)\n\n==External links==\n* [http://www.korpus.pl/index.php?lang=en&page=welcome Polish corpus website (in English)]\n* [http://poliqarp.sourceforge.net/ Project website on SourceForge]\n* [http://poliqarp.suxx.pl/ Search plugin for Firefox]\n\n[[Category:Information retrieval systems]]"]
['Relevance (information retrieval)', '442684', '{{other uses|Relevance (disambiguation)}}\n\nIn [[information science]] and [[information retrieval]], \'\'\'relevance\'\'\' denote how well a retrieved document or set of documents meets the [[information need]] of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.\n\n== History ==\n\nThe concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century.{{citation needed|date=June 2015}}\n\nThe formal study of relevance began in the 20th Century with the study of what would later be called [[bibliometrics]]. In the 1930s and 1940s, S. C. Bradford used the term "relevant" to characterize articles relevant to a subject (cf., [[Bradford\'s law]]). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.<ref>Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810‐832.</ref>\n\nSince 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between "relevance to a subject" or "topical relevance" and "user relevance".{{citation needed|date=June 2015}}\n\n== Evaluation ==\n{{main article|Information retrieval#Performance and correctness measures}}\n\nThe information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the [[Cranfield Experiments]] of the early 1960s and culminating in the [[Text Retrieval Conference|TREC]] evaluations that continue to this day as the main evaluation framework for information retrieval research.{{citation needed|date=June 2015}}\n\nIn order to evaluate how well an [[information retrieval]] system retrieved topically relevant results, the relevance of retrieved results must be quantified. In [[Cranfield Experiments|Cranfield]]-style evaluations, this typically involves assigning a \'\'relevance level\'\' to each retrieved result, a process known as \'\'relevance assessment\'\'. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, [[Information retrieval#Performance measures|information retrieval performance measures]] can be used to assess the quality of a retrieval system\'s output.\n\nIn contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance.{{citation needed|date=June 2015}} These studies often focus on aspects of [[human-computer interaction]] (see also [[human-computer information retrieval]]).\n\n== Clustering and relevance ==\n\nThe [[cluster hypothesis]], proposed by [[C. J. van Rijsbergen]] in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.<ref name=diazthesis>F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.</ref>    The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include:\n* cluster-based information retrieval<ref name=croftcbir>W. B. Croft, “A model of cluster searching based on classification,” Information Systems, vol. 5, pp. 189–195, 1980.</ref><ref name=griffithscbir>A. Griffiths, H. C. Luckhurst, and P. Willett, “Using interdocument similarity information in document retrieval systems,” Journal of the American Society for Information Science, vol. 37, no. 1, pp. 3–11, 1986.</ref>\n* cluster-based document expansion such as [[latent semantic analysis]] or its language modeling equivalents.<ref name=lmcbir>X. Liu and W. B. Croft, “Cluster-based retrieval using language models,” in SIGIR ’04: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186–193, ACM Press, 2004.</ref>    It is important to ensure that clusters – either in isolation or combination – successfully model the set of possible relevant documents.\n\nA second interpretation, most notably advanced by Ellen Voorhees,<ref name=voorheescbir>E. M. Voorhees, “The cluster hypothesis revisited,” in SIGIR ’85: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188–196, ACM Press, 1985.</ref>    focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include,\n* multiple cluster retrieval<ref name=griffithscbir/><ref name=voorheescbir/>\n* spreading activation<ref name=preece>S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.</ref> and relevance propagation<ref name=relprop>T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, “A study of relevance propagation for web search,” in SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408–415, ACM Press, 2005.</ref> methods\n* local document expansion<ref name=docexpansion>A. Singhal and F. Pereira, “Document expansion for speech retrieval,” in SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 34–41, ACM Press, 1999.</ref>\n* score regularization<ref name=diazreg>F. Diaz, “Regularizing query-based retrieval scores,” Information Retrieval, vol. 10, pp. 531–562, December 2007.</ref>\nLocal methods require an accurate and appropriate document similarity measure.\n\n==Problems and alternatives==\n\nThe documents which are most relevant are not necessarily those which are most useful to display in the first page of search results.  For example, two duplicate documents might be individually considered quite relevant, but it is only useful to display one of them.  A measure called "maximal marginal relevance" (MMR) has been proposed to overcome this shortcoming. It considers the relevance of each document only in terms of how much new information it brings given the previous results.<ref>{{cite journal|last1=Carbonell|first1=Jaime|last2=Goldstein|first2=Jade|title=The use of MMR, diversity-based reranking for reordering documents and producing summaries|journal=Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval|date=1998|doi=10.1145/290941.291025|url=http://dl.acm.org/citation.cfm?id=291025}}</ref>\n\nIn some cases, a query may have an ambiguous interpretation, or a variety of potential responses.  Providing a diversity of results can be a consideration when evaluating the utility of a result set.<ref>http://www.dcs.gla.ac.uk/workshops/ddr2012/</ref>\n\n==References==\n {{reflist}}\n\n==Additional reading==\n*Hjørland, B. (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.\n*Relevance : communication and cognition. by Dan Sperber; Deirdre Wilson. 2nd ed. Oxford; Cambridge, MA: Blackwell Publishers, 2001. ISBN 978-0-631-19878-9\n*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf pdf])\n*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf pdf])\n*Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. September 19, 2007. ([http://www.sis.utk.edu/lazerow2007 video])\n*Introduction to Information Retrieval: Evaluation. Stanford. ([http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf presentation in PDF])\n\n[[Category:Information retrieval evaluation]]']
['Overlap coefficient', '22049756', "{{unreferenced|date=June 2016}}\nThe '''overlap coefficient''' (or, '''Szymkiewicz-Simpson coefficient''') is a [[String_metric|similarity measure]] related to the [[Jaccard index]] that measures the overlap between two sets, and is defined as the size of the [[intersection (set theory)|intersection]] divided by the smaller of the size of the two sets:\n\n:<math>\\mathrm{overlap}(X,Y) = \\frac{| X \\cap Y | }{\\min(|X|,|Y|)}</math>\n\nIf set ''X'' is a [[subset]] of ''Y'' or the converse then the overlap coefficient is equal to one.\n\n==References==\n<references />\n\n[[Category:Information retrieval techniques]]\n[[Category:Information retrieval evaluation]]\n[[Category:String similarity measures]]\n[[Category:Measure theory]]"]
['Matthews correlation coefficient', '12306500', 'The \'\'\'Matthews correlation coefficient\'\'\' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]], introduced by biochemist [[Brian Matthews (biochemist)|Brian W. Matthews]] in 1975.<ref name="Matthews1975">{{cite journal|last=Matthews|first=B. W.|title=Comparison of the predicted and observed secondary structure of T4 phage lysozyme|journal=Biochimica et Biophysica Acta (BBA) - Protein Structure|date=1975|volume=405|issue=2|pages=442–451|doi=10.1016/0005-2795(75)90109-9}}</ref> It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between &minus;1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and &minus;1 indicates total disagreement between prediction and observation. The statistic is also known as the [[phi coefficient]]. MCC is related to the [[Pearson\'s chi-square test|chi-square statistic]] for a 2×2 [[contingency table]]\n\n: <math>|\\text{MCC}| = \\sqrt{\\frac{\\chi^2}{n}}</math>\n\nwhere \'\'n\'\' is the total number of observations.\n\nWhile there is no perfect way of describing the [[confusion matrix]] of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures.<ref name="Powers2011"/> Other measures, such as the proportion of correct predictions (also termed [[accuracy]]), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.\n\nThe MCC can be calculated directly from the [[confusion matrix]] using the formula:\n\n: <math>\n\\text{MCC} = \\frac{ TP \\times TN - FP \\times FN } {\\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }\n</math>\n\nIn this equation, \'\'TP\'\' is the number of [[true positive]]s, \'\'TN\'\' the number of [[true negative]]s, \'\'FP\'\' the number of [[false positive]]s and \'\'FN\'\' the number of [[false negative]]s. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.\n\nThe original formula as given by Matthews was:<ref name=Matthews1975 />\n: <math>\n\\text{N} = TN + TP + FN + FP\n</math>\n: <math>\n\\text{S} = \\frac{ TP + FN } { N }\n</math>\n: <math>\n\\text{P} = \\frac{ TP + FP } { N }\n</math>\n: <math>\n\\text{MCC} = \\frac{ TP / N - S \\times P } {\\sqrt{ P S  ( 1 - S)  ( 1 - P ) } }\n</math>\n\nThis is equal to the formula given above. As a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[Markedness]] (Δp) and [[Youden\'s J statistic]] ([[Informedness]] or Δp\').<ref name="Powers2011">{{cite journal |first=David M W |last=Powers |date=2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness & Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37–63 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}</ref><ref name="Perruchet2004">{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119 |doi=10.1016/s0911-6044(03)00059-9}}</ref> [[Markedness]] and [[Informedness]] correspond to different directions of information flow and generalize [[Youden\'s J statistic]], the deltap statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.<ref name="Powers2011"/>\n\n== Confusion Matrix ==\n{{main article|Confusion matrix}}\n\n{{Confusion matrix terms|recall=}}\n\nLet us define an experiment from \'\'\'P\'\'\' positive instances and \'\'\'N\'\'\' negative instances for some condition. The four outcomes can be formulated in a 2×2 \'\'[[contingency table]]\'\' or \'\'[[confusion matrix]]\'\', as follows:\n\n{{DiagnosticTesting_Diagram}}\n\n== Multiclass Case ==\nThe Matthews correlation coefficient has been generalized to the multiclass case. This generalization was called the  <math>R_K</math> statistic (for K different classes) by the author, and defined in terms of a <math>K\\times K</math> confusion matrix <math>C</math>\n<ref name="gorodkin2004comparing">{{cite journal|last=Gorodkin|first=Jan|title=Comparing two K-category assignments by a K-category correlation coefficient|journal=Computational biology and chemistry|date=2004|volume=28|number=5|pages=367–374|publisher=Elsevier}}</ref>\n.<ref name="GorodkinRk2006">{{cite web|last1=Gorodkin|first1=Jan|title=The Rk Page|url=http://rk.kvl.dk/introduction/index.html|website=The Rk Page|accessdate=28 December 2016}}</ref>\n\n:<math>\n\\text{MCC} = \\frac{\\sum_{k}\\sum_{l}\\sum_{m} C_{kk}C_{lm} - C_{kl}C_{mk}}{\n\\sqrt{\n\\sum_{k}(\\sum_l C_{kl} )(\\sum_{k\' | k\' \\neq k}\\sum_{l\'} C_{k\'l\'})\n}\n\\sqrt{\n\\sum_{k}(\\sum_l C_{lk} )(\\sum_{k\' | k\' \\neq k}\\sum_{l\'} C_{l\'k\'})\n}\n}\n</math>\n\nWhen there are more than two labels the MCC will no longer range between -1 and +1. Instead the minimum value will be between -1 and 0 depending on the true distribution. The maximum value is always +1. \n\n<!-- \nTODO: potentially un-comment later, for now just stick with referenced version\n\nThis formula can be more easily understood by defining intermediate variables: \n* <math>t_k=\\sum_i C_{ik}</math> the number of times class k truly occurred, \n* <math>p_k=\\sum_i C_{ki}</math> the number of times class k was predicted, \n* <math>c=\\sum_{k} C_{kk}</math> the total number of samples correctly predicted, \n* <math>s=\\sum_i \\sum_j C_{ij}</math> the total number of samples. This allows the formula to be expressed as:\n\n:<math>\n\\text{MCC} = \\frac{cs - \\vec{t} \\cdot \\vec{p}}{\n\\sqrt{s^2 - \\vec{p} \\cdot \\vec{p}}\n\\sqrt{s^2 - \\vec{t} \\cdot \\vec{t}}\n}\n</math>\n-->\n\n== See also ==\n* [[Phi coefficient]]\n* [[F1 score]]\n* [[Cramér\'s V (statistics)|Cramér\'s V]], a similar measure of association between nominal variables.\n* [[Cohen\'s kappa]]\n\n== References ==\n\n{{Reflist}}\n\n<!--should reference in the main text  === General References ===\n* [[Pierre Baldi|Baldi, P.]]; Brunak, S.; Chauvin, Y.; Andersen, C. A. F.; Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview" \'\'Bioinformatics\'\' 2000, 16, 412&ndash;424. [http://bioinformatics.oxfordjournals.org/cgi/content/abstract/16/5/412]\n* Carugo, O., Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots. BMC Bioinformatics 2007. [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148069/]\n-->\n\n{{DEFAULTSORT:Matthews Correlation Coefficient}}\n[[Category:Machine learning]]\n[[Category:Information retrieval evaluation]]\n[[Category:Statistical classification]]\n[[Category:Computational chemistry]]\n[[Category:Cheminformatics]]\n[[Category:Bioinformatics]]\n[[Category:Statistical ratios]]\n[[Category:Summary statistics for contingency tables]]']
['Query likelihood model', '29979321', "The '''query likelihood model''' is a [[language model]] used in [[information retrieval]]. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the [[Likelihood function|likelihood]] of a document being relevant given a query.\n\n==Calculating the likelihood==\nUsing [[Bayes' theorem|Bayes' rule]], the probability <math>P</math> of a document <math>d</math>, given a query <math>q</math> can be written as follows:\n\n:<math>\n P(d|q) = \\frac{P(q|d) P(d)}{P(q)}\n</math>\n\nSince the probability of the query P(q) is the same for all documents, this can be ignored. Further, it is typical to assume that the probability of documents is uniform. Thus, P(d) is also ignored.\n\n:<math>\n P(d|q) = P(q|d)\n</math>\n\nDocuments are then ranked by the probability that a query is observed as a random sample from the document model. The multinomial unigram language model is commonly used to achieve this. We have:\n:<math>\n P(q|M_d) = K_q \\prod_{t \\in V} P(t|M_d)^{tf_{t,q}}\n</math>,where the multinomial coefficient is <math>K_q = L_q!/(tf_{t1,q}!tf_{t2,q}!...tf_{tN,q}!)</math> for query {{math|q}}, \n\nand <math>L_q = \\sum_{1 \\leq i \\leq N}tf_{t_i,q}</math> is the length of query {{math|q}} given the term frequencies {{math|tf}} in the query vocabulary {{math|N}}.\n\nIn practice the multinomial coefficient is usually removed from the calculation. The reason is that it is a constant for a given bag of words (such as all the words from a specific document <math>d</math>). The language model <math>M_d</math> should be the true language model calculated from the distribution of words underlying each retrieved document. In practice this language model is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So <math>P(t|M_d)</math> is the probability of term <math>t</math> being generated by the language model <math>M_d</math> of document <math>d</math>. This probability is multiplied for all terms from query <math>q</math> to get a rank for document <math>d</math> in the interval <math>[0,1]</math>. The calculation is repeated for all documents to create a ranking of all documents in the document collection.\n\n<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: An Introduction to Information Retrieval, page 241. Cambridge University Press, 2009</ref>\n\n==References==\n <references/>\n\n[[Category:Information retrieval techniques]]"]
['Stop words', '1015600', '{{distinguish|Safeword}}\nIn [[computing]], \'\'\'stop words\'\'\' are words which are filtered out before or after [[Natural language processing|processing of natural language]] data (text).<ref>{{Cite book | last1 = Rajaraman | first1 = A. | last2 = Ullman | first2 = J. D. | doi = 10.1017/CBO9781139058452.002 | chapter = Data Mining | title = Mining of Massive Datasets | pages = 1–17| year = 2011 | isbn = 9781139058452 | pmid =  | pmc = | url = http://i.stanford.edu/~ullman/mmds/ch1.pdf}}</ref> Though \'\'\'stop words\'\'\' usually refer to the most common words in a language, there is no single universal list of stop words used by all [[natural language processing]] tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these \'\'\'stop words\'\'\' to support [[phrase search]].\n\nAny group of words can be chosen as the stop words for a given purpose. For some [[search engine]]s, these are some of the most common, short [[function word]]s, such as \'\'the\'\', \'\'is\'\', \'\'at\'\', \'\'which\'\', and \'\'on\'\'. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as "[[The Who]]", "[[The The]]", or "[[Take That]]". Other search engines remove some of the most common words—including [[lexical word]]s, such as "want"—from a query in order to improve performance.<ref>[http://blog.stackoverflow.com/2008/12/podcast-32 Stackoverflow]: "One of our major performance optimizations for the "related questions" query is removing the top 10,000 most common English dictionary words (as determined by Google search) before submitting the query to the SQL Server 2008 full text engine. It’s shocking how little is left of most posts once you remove the top 10k English dictionary words. This helps limit and narrow the returned results, which makes the query dramatically faster".</ref>\n\n[[Hans Peter Luhn]], one of the pioneers in [[information retrieval]], is credited with coining the phrase and using the concept.<ref>{{Cite book|title = Keyword-in-Context Index for Technical Literature (KWIC Index)|last = Luhn|first = H. P.|publisher = International Business Machines Corp.|year = 1959|isbn = |location = Yorktown Heights, NY|pages = |doi = 10.1002/asi.5090110403}}</ref> The phrase "stop word", which is not in Luhn\'s 1959 presentation, and the associated terms "stop list" and "stoplist" appear in the literature shortly afterwards.<ref>{{cite journal|last1=Flood|first1=Barbara J.|title=Historical note: The Start of a Stop List at Biological Abstracts|journal=Journal of the American Society for Information Science|date=1999|volume=50|issue=12|page=1066|doi=10.1002/(SICI)1097-4571(1999)50:12<1066::AID-ASI5>3.0.CO;2-A|url=http://dx.doi.org/10.1002/(SICI)1097-4571(1999)50:12<1066::AID-ASI5>3.0.CO;2-A|accessdate=16 February 2016}}</ref>\n\nA predecessor concept was used in creating some [[Bible concordance|concordance]]s. For example, the first Hebrew concordance, Me’ir nativ, contained a one-page list of unindexed words, with nonsubstantive prepositions and conjunctions which are similar to modern stop words.<ref>{{cite journal|last1=Weinberg|first1=Bella Hass|title=Predecessors of scientific indexing structures in the domain of religion|journal=Second Conference on the History and Heritage of Scientific and Technical Information Systems|date=2004|pages=126–134|url=https://www.asis.org/History/11-weinberg.pdf|accessdate=17 February 2016}}</ref>\n\n== See also ==\n{{Div col|cols=3}}\n* [[Text mining]]\n* [[Concept mining]]\n* [[Information extraction]]\n* [[Natural language processing]]\n* [[Query expansion]]\n* [[Stemming]]\n* [[Index (search engine)|Search engine indexing]]\n* [[Poison words]]\n* [[Function words]]\n* [[Filler_(linguistics) | Filler]]\n{{Div col end}}\n\n==References==\n{{Reflist|2}}\n\n== External links ==\n* [http://xpo6.com/list-of-english-stop-words/  List of English Stop Words (PHP array, CSV) ]\n* [http://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html  Full-Text Stopwords in MySQL ]\n* [http://www.textfixer.com/resources/common-english-words.txt English Stop Words (CSV)]\n* [http://mail.sarai.net/private/prc/Week-of-Mon-20080204/001656.html Hindi Stop Words]\n* [http://solariz.de/de/deutsche_stopwords.htm German Stop Words],[http://aniol-consulting.de/uebersicht-deutscher-stop-words/ German Stop Words and phrases], another list of [http://www.ranks.nl/stopwords/german.html German stop words]\n* [[:pl:Wikipedia:Stopwords|Polish Stop Words]]\n* [https://code.google.com/p/stop-words/ Collection of stop words in 29 languages] [https://web.archive.org/web/*/http://tonyb.sk/_my/ir/stop-words-collection-2014-02-24.zip]\n* [http://www.text-analytics101.com/2014/10/all-about-stop-words-for-text-mining.html A Detailed Explanation of Stop Words by Kavita Ganesan]\n\n{{Natural Language Processing}}\n{{SearchEngineOptimization}}\n\n[[Category:Information retrieval techniques]]']
['Thesaurus (information retrieval)', '39000674', '{{about|thesauri used to support indexing, tagging or searching for information|thesauri used in general/literary applications|Thesaurus|the Clare Fischer album|Thesaurus (album)}}\n\nIn the context of [[information retrieval]], a \'\'\'thesaurus\'\'\' (plural: "thesauri") is a form of controlled vocabulary that seeks to dictate semantic manifestations of [[metadata]] in the indexing of content objects. A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects. ANSI/NISO Z39.19-2005 defines a content object as "any item that is to be described for inclusion in an information retrieval system, website, or other source of information".<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.11</ref> The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object.<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.12</ref>\n\nA thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject. [[ISO 25964]], the international standard for information retrieval thesauri, defines a thesaurus as a “controlled and structured vocabulary in which concepts are represented by terms, organized so that relationships between concepts are made explicit, and preferred terms are accompanied by lead-in entries for synonyms or quasi-synonyms.”\n\nA thesaurus is composed by at least three elements: 1-a list of words (or terms), 2-the relationship amongst the words (or terms), indicated by their hierarchical relative position (e.g. parent/broader term; child/narrower term, synonym, etc.), 3-a set of rules on how to use the thesaurus.\n\n== History ==\nWherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as [[Calvin Mooers]], Charles L. Bernier, [http://pubs.acs.org/cen/priestley/recipients/1951crane.html Evan J. Crane] and [[Hans Peter Luhn]], collected up their index terms in various kinds of list that they called a “thesaurus” (by analogy with the well known thesaurus developed by [[Peter Roget]]).<ref>Roberts, N. The pre-history of the information retrieval thesaurus. \'\'Journal of Documentation\'\', 40(4), 1984, p.271-285.</ref> The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.<ref>Aitchison, J. and Dextre Clarke, S. The thesaurus: a historical viewpoint, with a look to the future. \'\'Cataloging & Classification Quarterly\'\', 37 (3/4), 2004, p.5-21.</ref><ref>Krooks, D.A. and Lancaster, F.W. The evolution of guidelines for thesaurus construction. \'\'Libri\'\', 43(4), 1993, p.326-342.</ref>\n\nThe first two of these lists to be published were the \'\'Thesaurus of ASTIA Descriptors\'\' (1960) and the \'\'Chemical Engineering Thesaurus\'\' of the American Institute of Chemical Engineers (1961), a descendant of the Dupont thesaurus. More followed, culminating in the influential \'\'Thesaurus of Engineering and Scientific Terms\'\' (TEST) published jointly by the Engineers Joint Council and the US Department of Defense in 1967. TEST did more than just serve as an example; its Appendix 1 presented \'\'Thesaurus rules and conventions\'\' that have guided thesaurus construction ever since.\nHundreds of thesauri have been produced since then, perhaps thousands. The most notable innovations since TEST have been:\n(a)\tExtension from monolingual to multilingual capability; and \n(b)\tAddition of a conceptually organized display to the basic alphabetical presentation.\n\nHere we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST:\n\n* [[UNESCO]] \'\'Guidelines for the establishment and development of monolingual thesauri\'\'. 1970 (followed by later editions in 1971 and 1981)\n* DIN 1463 \'\'Guidelines for the establishment and development of monolingual thesauri\'\'. 1972 (followed by later editions)\n* ISO 2788 \'\'Guidelines for the establishment and development of monolingual thesauri\'\'. 1974 (revised 1986)\n* ANSI \'\'American National Standard for Thesaurus Structure, Construction, and Use\'\'. 1974 (revised 1980 and superseded by ANSI/NISO Z39.19-1993)\n* ISO 5964 \'\'Guidelines for the establishment and development of multilingual thesauri\'\'. 1985\n* ANSI/NISO Z39.19 \'\'Guidelines for the construction, format, and management of monolingual thesauri\'\'. 1993 (revised 2005 and renamed \'\'Guidelines for the construction, format, and management of monolingual controlled vocabularies\'\'.)\n* ISO 25964 \'\'Thesauri and interoperability with other vocabularies\'\'. Part 1 (\'\'Thesauri for information retrieval\'\' published 2011; Part 2 (\'\'Interoperability with other vocabularies\'\') published 2013.\n\nThe most clearly visible trend across this history of thesaurus development has been from the context of small-scale isolation to a networked world.<ref>Dextre Clarke, Stella G. and Zeng, Marcia Lei. [http://www.niso.org/publications/isq/2012/v24no1/clarke/ From ISO 2788 to ISO 25964: the evolution of thesaurus standards towards interoperability and data modeling] \'\'Information standards quarterly\'\', 24(1), 2012, p.20-26.</ref> Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications. More recently, as can be seen from the titles of the latest ISO and NISO standards, there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system, such as subject heading schemes, classification schemes, taxonomies and ontologies. The official website for ISO 25964 gives more information, including a reading list.<ref>\'\'[http://www.niso.org/schemas/iso25964/ ISO 25964 – the international standard for thesauri and interoperability with other vocabularies.]\'\' National Information Standards Organization, 2013.</ref>\n\n== Purpose ==\n{{refimprove section|small=z|date=March 2016}}\nIn information retrieval, a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities. A thesaurus helps with expressing the manifestations of a concept in a prescribed way, to aid in improving [[precision and recall]]. This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language. Additionally, a thesaurus is used for maintaining a hierarchical listing of terms; usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity.\n\nThe [[Art and Architecture Thesaurus|Art & Architecture Thesaurus]], for example, is used by countless museums around the world, to catalogue their collections. [[AGROVOC]], the thesaurus of the UN’s [[Food and Agriculture Organization]], is used to index and/or search its AGRIS database of worldwide literature on agricultural research.\n\n== Structure ==\n{{refimprove section|small=z|date=March 2016}}\nInformation retrieval thesauri are formally organized so that existing relationships between concepts are made clear. For example, “citrus fruits” might be linked to the broader concept of “fruits”, and the narrower ones of “oranges”, “lemons”, etc. When the terms are displayed online, the links between them make it very easy to surf around the thesaurus, selecting useful terms for a search. When a single term could have more than one meaning, like tables (furniture) or tables (data), these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results. For any one concept, all the known synonyms are listed, such as “mad cow disease”, “bovine spongiform encephalopathy”, “BSE”, etc. The idea is to guide all the indexers and all the searchers to use the same term for the same concept, so that search results will be as complete as possible. If the thesaurus is multilingual, equivalent terms in other languages are shown too. Following international standards, concepts are generally arranged hierarchically within facets or grouped by themes or topics. Unlike a general thesaurus used for literary purposes, information retrieval thesauri typically focus on one discipline, subject or field of study.\n\n== See also ==\n* [[Controlled vocabulary]]\n* [[ISO 25964]]\n* [[Thesaurus]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.niso.org/schemas/iso25964/ Official site for ISO 25964] \n* [http://www.taxonomywarehouse.com/ Taxonomy Warehouse]\n\n[[Category:Information retrieval techniques]]\n[[Category:Thesauri]]']
['Category:String similarity measures', '9833053', '{{Cat main|String metrics}}\n\n[[Category:Algorithms on strings|Similarity]]\n[[Category:Information retrieval techniques]]\n[[Category:Metric geometry]]\n[[Category:Information theory]]\n[[Category:String (computer science)]]']
['Latent semantic mapping', '11989095', "'''Latent semantic mapping (LSM)''' is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of [[latent semantic analysis]]. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.\n\nLSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.\n\n[[Mac OS X v10.5]] and later includes a [[Software framework|framework]] implementing latent semantic mapping.<ref>[http://developer.apple.com/documentation/TextFonts/Reference/LatentSemanticMapping/index.html API Reference: Latent Semantic Mapping Framework Reference<!-- Bot generated title -->]</ref>\n\n== See also ==\n* [[Latent semantic analysis]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n* {{cite journal\n | url=http://ieeexplore.ieee.org/iel5/79/32367/01511825.pdf\n | title=Latent semantic mapping [information retrieval]\n | author=Bellegarda, J.R.\n | date=2005\n}}\n* {{cite conference\n | url=https://www.securecms.com/ICASSP2006/Tutorial_06.asp\n | title=Latent semantic mapping: Principles and applications\n | author=J. Bellegarda\n | booktitle=ICASSP 2006\n | date=2006\n}}\n\n[[Category:Information retrieval techniques]]\n[[Category:Natural language processing]]\n\n\n{{semantics-stub}}\n{{compu-stub}}"]
['Semantic compression', '34087348', "In [[natural language processing]], '''semantic compression''' is a process of compacting a lexicon used to build \na textual document (or a set of documents) by reducing language heterogeneity, while maintaining text [[semantics]]. \nAs a result, the same ideas can be represented using a smaller set of words.\n\nSemantic compression is a [[lossy compression]], that is, some data is being discarded, and an original document \ncannot be reconstructed in a reverse process.\n\n==Semantic compression by generalization==\nSemantic compression is basically achieved in two steps, using [[frequency list|frequency dictionaries]] and [[semantic network]]:\n#\tdetermining cumulated term frequencies to identify target lexicon,\n#\treplacing less frequent terms with their hypernyms ([[generalization]]) from target lexicon.<ref>[http://dx.doi.org/10.1007/978-3-642-12090-9_10 D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems], Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010</ref>\n\nStep 1 requires assembling word frequencies and \ninformation on semantic relationships, specifically [[hyponymy]]. Moving upwards in word hierarchy, \na cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:\n<math>cum f(k_{i}) = f(k_{i}) + \\sum_{j} cum f(k_{j})</math> where <math>k_{i}</math> is a hypernym of <math>k_{j}</math>.\nThen, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.\n\nIn the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence \nof a less frequent hyponym as its hypernym in output text.\n\n;Example\n\nThe below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.\n\n<blockquote>They are both '''nest''' building '''social insects''', but '''paper wasps''' and honey '''bees''' '''organize''' their '''colonies''' \nin very different '''ways'''. In a new study, researchers report that despite their '''differences''', these insects \n'''rely on''' the same network of genes to guide their '''social behavior'''.The study appears in the Proceedings of the \n'''Royal Society B''': Biological Sciences. Honey '''bees''' and '''paper wasps''' are separated by more than 100 million years of \n'''evolution''', and there are '''striking differences''' in how they divvy up the work of '''maintaining''' a '''colony'''.</blockquote>\n\nThe procedure outputs the following text:\n\n<blockquote>They are both '''facility''' building '''insect''', but '''insect'''s and honey '''insects''' '''arrange''' their '''biological groups''' \nin very different '''structure'''. In a new study, researchers report that despite their '''difference of opinions''', these insects \n'''act''' the same network of genes to '''steer''' their '''party demeanor'''. The study appears in the proceeding of the \n'''institution bacteria''' Biological Sciences. Honey '''insects''' and '''insect''' are separated by more than hundred million years of \n'''organic processes''', and there are '''impinging differences of opinions''' in how they divvy up the work of '''affirming''' a '''biological group'''.</blockquote>\n\n==Implicit semantic compression==\nA natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid [[pleonasm]]s)\n.<ref>[http://dx.doi.org/10.3115/990100.990155 N. N. Percova, On the types of semantic compression of text],\nCOLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982</ref>\n\n==Applications and advantages==\nIn the [[vector space model]], compacting a lexicon leads to a reduction of [[curse of dimensionality|dimensionality]], which results in less \n[[Computational complexity theory|computational complexity]] and a positive influence on efficiency. \n\nSemantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall).<ref>[http://dl.acm.org/citation.cfm?id=1947662.1947683 D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification] Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010</ref> This is due to more precise descriptors (reduced effect of language diversity – limited language redundancy, a step towards a controlled dictionary).\n\nAs in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).\n\n==See also==\n* [[Text simplification]]\n* [[Lexical substitution]]\n* [[Information theory]]\n* [[Quantities of information]]\n\n==References==\n<references/>\n\n==External links==\n* [http://semantic.net.pl/semantic_compression.php Semantic compression on Project SENECA (Semantic Networks and Categorization) website]\n\n[[Category:Information retrieval techniques]]\n[[Category:Natural language processing]]\n[[Category:Quantitative linguistics]]\n[[Category:Computational linguistics]]"]
['Index term', '6118940', 'An \'\'\'index term\'\'\', \'\'\'subject term\'\'\', \'\'\'subject heading\'\'\', or \'\'\'descriptor\'\'\', in [[information retrieval]], is a term that captures the essence of the topic of a document. Index terms make up a [[controlled vocabulary]] for use in [[bibliographic record]]s. They are an integral part of bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a [[search engine]].  A popular form of keywords on the web are [[tag (metadata)|tags]] which are directly visible and can be assigned by non-experts. Index terms can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with [[subject indexing]] or automatically with [[Index (search engine)|automatic indexing]] or more sophisticated methods of keyword extraction. Index terms can either come from a controlled vocabulary or be freely assigned.\n\nKeywords are stored in a [[Index (search engine)|search index]]. Common words like [[article (grammar)|articles]] (a, an, the) and conjunctions (and, or, but) are not treated as keywords because it\'s inefficient. Almost every English-language site on the Internet has the article "\'\'the\'\'", and so it makes no sense to search for it. The most popular search engine, [[Google]] removed [[stop words]] such as "the" and "a" from its indexes for several years, but then re-introduced them, making certain types of precise search possible again.\n\nThe term "descriptor" was coined by [[Calvin Mooers]] in 1948. It is in particular used about a preferred term from a [[Thesaurus (information retrieval)|thesaurus]].\n\nThe [[Simple Knowledge Organization System]] language (SKOS) provides a way to express index terms with [[Resource Description Framework]] for use in the context of [[Semantic Web]].<ref name="auto">{{cite book|last=Svenonius|first=Elaine|author-link=Elaine Svenonius|title=The intellectual foundation of information organization|date=2009|publisher=MIT Press|location=Cambridge, Mass.|isbn=9780262512619|edition=1st MIT Press pbk.}}</ref>\n\n==In web search engines==\nMost [[web search engine]]s are designed to search for words anywhere in a document—the title, the body, and so on. This being the case, a keyword can be any term that exists within the document. However, priority is given to words that occur in the title, words that recur numerous times, and words that are explicitly assigned as keywords within the coding.<ref>Cutts, Matt. (2010, March 4). \'\'How search works.\'\' Retrieved from https://www.youtube.com/watch?v=BNHR6IQJGZs</ref> Index terms can be further refined using [[Boolean algebra|Boolean operators]] such as "AND, OR, NOT." "AND" is normally unnecessary as most search engines infer it. "OR" will search for results with one search term or another, or both. "NOT" eliminates a word or phrase from the search, getting rid of any results that include it. Multiple words can also be enclosed in quotation marks to turn the individual index terms into a specific index \'\'phrase\'\'. These modifiers and methods all help to refine search terms, to better maximize the accuracy of search results.<ref>CLIO. \'\'Keyword search\'\'. Columbia University Libraries. Retrieved from http://www.columbia.edu/cu/lweb/help/clio/keyword.html</ref>\n\n==Author keywords==\nMany journals and databases provides access (also) to index terms made by authors to the articles being published or represented. The relative quality of indexer-provided index terms and author provided index terms is of interest to research in information retrieval. The quality of both kinds of indexing terms depends, of course, on the qualifications of provider. In general authors have difficulties providing indexing terms that characterizes his document \'\'relative\'\' to the other documents in the database. Author keywords are an integral part of literature.<ref name="auto"/>\n\n==Examples==\n*[[Canadian Subject Headings]] (CSH)\n*[[Library of Congress Subject Headings]] (LCSH)\n*[[Medical Subject Headings]] (MeSH)\n*[[Polythematic Structured Subject Heading System]] (PSH)\n*[[Subject Headings Authority File]] (SWD)\n\n==See also==\n*[[Dynamic keyword insertion]]\n*[[Tag cloud]]\n*[[Keyword density]]\n*[[Search engine optimization]]\n*[[Tag (metadata)]]\n*[[Subject (documents)]]\n\n==References==\n{{reflist}}\n\n{{Authority control}}\n[[Category:Information retrieval techniques]]']
['Natural language user interface', '18863997', '\'\'\'Natural language user interfaces\'\'\' (\'\'\'LUI\'\'\' or \'\'\'NLUI\'\'\') are a type of [[User interface|computer human interface]] where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.\n\nIn [[interface design]] natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to [[natural language understanding|understanding]] wide varieties of ambiguous input.<ref>Hill, I. (1983). "Natural language versus computer language." In M. Sime and M. Coombs (Eds.) Designing for Human-Computer Communication. Academic Press.</ref>\nNatural language interfaces are an active area of study in the field of [[natural language processing]] and [[computational linguistics]]. An intuitive general natural language interface is one of the active goals of the [[Semantic Web]].\n\nText interfaces are "natural" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional [[keyword search]] engine could be described as a "shallow" natural language user interface.\n\n==Overview==\nA natural language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form \'which [[United States|U.S.]] state has the highest [[income tax]]?\', conventional search engines ignore the question and instead search on the [[index term|keywords]] \'state\', \'income\' and \'tax\'. Natural language search, on the other hand, attempts to use natural language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.{{Citation needed|date=October 2015}}\n\n==History==\n\nPrototype Nl interfaces had already appeared in the late sixties and early seventies.<ref name="edin">Natural Language Interfaces to Databases – An Introduction,\nI. Androutsopoulos,\nG.D. Ritchie,\nP. Thanisch,\nDepartment of Artificial Intelligence, University of Edinburgh</ref>\n\n*[[SHRDLU]], a natural language interface that manipulates blocks in a virtual "blocks world"\n*\'\'Lunar\'\', a natural language interface to a database containing chemical analyses of Apollo-11 moon rocks by [http://parsecraft.com/ William A. Woods].\n*\'\'Chat-80\'\' transformed English questions into [[Prolog]] expressions, which were evaluated against the Prolog database.  The code of Chat-80 was circulated widely, and formed the basis of several other experimental Nl interfaces. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem5.htm Chat-80 demo]</ref>\n*[[ELIZA]], written at MIT by Joseph Weizenbaum between 1964 and 1966, mimicked a psychotherapist and was operated by processing users\' responses to scripts. Using almost no information about human thought or emotion, the DOCTOR script sometimes provided a startlingly human-like interaction. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem4.htm ELIZA demo]</ref>\n* \'\'Janus\'\' is also one of the few systems to support temporal questions.\n* \'\'Intellect\'\' from [[Trinzic]] (formed by the merger of AICorp and Aion).\n* BBN’s \'\'Parlance\'\' built on experience from the development of the \'\'Rus\'\'  and \'\'Irus\'\' systems.\n* [[IBM]] \'\'Languageaccess\'\'\n* [[Q&A (software)|Q&A]] from [[Symantec]].\n* \'\'Datatalker\'\' from Natural Language Inc.\n* \'\'Loqui\'\'  from [[Bim]].\n* \'\'English Wizard\'\' from [[Linguistic Technology Corporation]].\n* \'\'iAskWeb\'\' from Anserity Inc. fully implemented in [[Prolog]] was providing interactive recommendations in NL to users in tax and investment domains in 1999-2001<ref>{{cite book | last = Galitsky\n | first = Boris\n | title = Natural Language Question Answering: technique of semantic headers\n | publisher = Advance Knowledge International\n | date = 2003\n | location = Adelaide, Australia\n | url = http://www.amazon.com/Natural-Language-Question-Answering-system/dp/0868039799\n | isbn = 0868039799\n  }}</ref>\n\n==Challenges==\nNatural language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the [[AI winter]] of the 1970s and 80s.\n\nA [http://arxiv.org/abs/cmp-lg/9503016 1995 paper] titled \'Natural Language Interfaces to Databases – An Introduction\', describes some challenges:<ref name="edin"/>\n;Modifier attachment\n:The request "List all employees in the company with a driving licence" is ambiguous unless you know that companies can\'t have driving licences.\n;Conjunction and disjunction\n:"List all applicants who live in California and Arizona" is ambiguous unless you know that a person can\'t live in two places at once.\n;[[Anaphora resolution]]\n:resolve what a user means by \'he\', \'she\' or \'it\', in a self-referential query.\n\nOther goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market. In addition, localisation across multiple language sites requires extra consideration - this is based on differing sentence structure and language syntax variations between most languages.\n\nFinally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.\n\n==Uses and applications==\n\nThe natural language interface gives rise to technology used for many different applications.\n\nSome of the main uses are:\n\n* \'\'Dictation\'\', is the most common use for [[automated speech recognition]] (ASR) systems today. This includes medical transcriptions, legal and business dictation, and general word processing. In some cases special vocabularies are used to increase the accuracy of the system.\n* \'\'Command and control\'\', ASR systems that are designed to perform functions and actions on the system are defined as command and control systems. Utterances like "Open Netscape" and "Start a new xterm" will do just that.\n* \'\'Telephony\'\', some PBX/[[Voice Mail]] systems allow callers to speak commands instead of pressing buttons to send specific tones.\n* \'\'Wearables\'\', because inputs are limited for wearable devices, speaking is a natural possibility.\n* \'\'Medical, disabilities\'\', many people have difficulty typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller\'s speech to text.\n* \'\'Embedded applications\'\', some new cellular phones include C&C speech recognition that allow utterances such as "call home". This may be a major factor in the future of automatic speech recognition and [[Linux]].\n\nBelow are named and defined some of the applications that use natural language recognition, and so have integrated utilities listed above.\n\n===Ubiquity===\n{{main|Ubiquity (Firefox)}}\nUbiquity, an [[add-on (Mozilla)|add-on]] for [[Mozilla Firefox]], is a collection of quick and easy natural-language-derived commands that act as [[mashup (web application hybrid)|mashups]] of web services, thus allowing users to get information and relate it to current and other webpages.\n\n===Wolfram Alpha===\n{{main|Wolfram Alpha}}\nWolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] would.<ref>{{cite news |url=https://www.theguardian.com/technology/2009/mar/09/search-engine-google |title=British search engine \'could rival Google\' |last=Johnson |first=Bobbie |date=2009-03-09 |work=[[The Guardian]] |accessdate=2009-03-09}}</ref> It was announced in March 2009 by [[Stephen Wolfram]], and was released to the public on May 15, 2009.<ref name="launch date">{{cite web|url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |publisher=Wolfram Alpha Blog |date=2009-05-08 |accessdate=2009-10-20}}</ref>\n\n===Siri===\n{{main|Siri (software)}}\nSiri is an [[intelligent personal assistant]] application integrated with operating system [[iOS]]. The application uses [[natural language processing]] to answer questions and make recommendations.\n\nSiri\'s marketing claims include that it adapts to a user\'s individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.<ref>[https://www.apple.com/iphone/features/siri.html Siri webpage]</ref>\n\n===Others===\n* [[Ask.com]] - The original idea behind Ask Jeeves (Ask.com) was traditional keyword searching with an ability to get answers to questions posed in everyday, natural language. The current Ask.com still supports this, with added support for math, dictionary, and conversion questions.\n* [[Braina]]<ref>[http://www.brainasoft.com/braina/ Braina]</ref> - Braina is a natural language interface for [[Windows OS]] that allows to type or speak English language sentences to perform a certain action or find information.\n* [http://www.cmantik.com/ CMANTIK] - CMANTIK is a semantic information search engine which is trying to answer user\'s questions by looking up relevant information in Wikipedia and some news sources.\n* [http://minock.github.io/c-phrase/ C-Phrase] - is a web-based natural language front end to relational databases. C-Phrase runs under Linux, connects with PostgreSQL databases via ODBC and supports both select queries and updates. Currently there is only support for English.\n* [http://devtools.korzh.com/easyquery/ EasyQuery] - is a component library (for .NET framework first of all) which allows you to implement natural language query builder in your application. Works both with relational databases or ORM solutions like Entity Framework.\n* [https://friendlydata.io/ FriendlyData] is a natural language interface for relational databases.\n[[File:GNOME Do Classic.png|thumb|Screenshot of GNOME DO classic interface.]]\n* [http://yagadi.com/ Enguage] - this is an open source text understanding interface for web/mobile devices, using publicly available speech-to-text and text-to-speech facilities. This is directed at controlling apps, rather than as a front-end database query or web search. The interpretation of utterances is programmed, and programmable, in natural language utterances; thus, it is (or at least asserts that language is) an [[autopoiesis|autopoietic]] system.<ref>http://www.academia.edu/10177437/An_Autopoietic_Repertoire</ref> It can achieve a deep understanding of text.<ref>http://cit.srce.unizg.hr/index.php/CIT/article/view/2278/1658 if we are holding hands whose hand am i holding</ref> A reference app is available on [https://play.google.com/store/apps/details?id=com.yagadi.iNeed Google Play]\n* [[GNOME Do]] - Allows for quick finding miscellaneous artifacts of GNOME environment (applications, Evolution and Pidgin contacts, Firefox bookmarks, Rhythmbox artists and albums, and so on) and execute the basic actions on them (launch, open, email, chat, play, etc.).<ref>Ubuntu 10.04 Add/Remove Applications description for GNOME Do</ref>\n* [[hakia]] - hakia is an Internet search engine. The company has invented an alternative new infrastructure to indexing that uses SemanticRank algorithm, a solution mix from the disciplines of ontological semantics, fuzzy logic, computational linguistics, and mathematics.\n* [[Lexxe]] - Lexxe is an Internet search engine that uses natural language processing for queries (semantic search). Searches can be made with keywords, phrases, and questions, such as "How old is Wikipedia?" When it comes to facts, Lexxe is quite effective, though needs much improvement in natural language analysis in the area of facts and in other areas.\n* [http://www.mnemoo.com/ Mnemoo] - Mnemoo is an answer engine that aimed to directly answer questions posed in plain text (Natural Language), which is accomplished using a database of facts and an inference engine.\n* [http://www.naturaldateandtime.com/ Natural Date and Time] - Natural language date and time zone engine. It allows you to ask questions about time, daylight saving information and to do time zone conversions via plain English questions such as \'What is the time in São Paulo when it is 6pm on the 2nd of June in Detroit\'.\n* [http://www.linguasys.com/web_production/server-item/NLUI%20Server NLUI Server] - an enterprise-oriented multilingual application server by LinguaSys for natural language user interface scripts, supporting English, Spanish, Portuguese, German, Japanese, Chinese, Pashto, Thai, Russian, Vietnamese, Malay, with Arabic, French, and more languages in development.\n* [[Pikimal]] - Pikimal uses natural language tied to user preference to make search recommendations by template.\n* [[Powerset (company)|Powerset]] — On May 11, 2008, the company unveiled a tool for searching a fixed subset of [[Wikipedia]] using conversational phrases rather than keywords.<ref>{{cite news |url=http://bits.blogs.nytimes.com/2008/05/12/powerset-debuts-with-search-of-wikipedia/ |title=Powerset Debuts With Search of Wikipedia |publisher=The New York Times |first=Miguel |last=Helft |date=May 12, 2008}}</ref> On July 1, 2008, it was purchased by [[Microsoft]].<ref>{{cite web |url=http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archiveurl=https://web.archive.org/web/20090225064356/http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archivedate=February 25, 2009 |title=Microsoft to Acquire Powerset |publisher=Powerset Blog |first=Mark |last=Johnson |date=July 1, 2008}}</ref>\n* [[Q-go]] - The Q-go technology provides relevant answers to users in response to queries on a company’s internet website or corporate intranet, formulated in natural sentences or keyword input alike. Q-go was acquired by [[RightNow Technologies]] in 2011\n* [[START (MIT project)]] - [http://start.csail.mit.edu/ START], Web-based question answering system. Unlike information retrieval systems such as search engines, START aims to supply users with "just the right information," instead of merely providing a list of hits. Currently, the system can answer millions of English questions about places, movies, people and dictionary definitions.\n* [https://www.statmuse.com/ StatMuse] - Natural language analytics platform, currently in private beta with NBA data. Ask natural questions and get rich visualizations and raw data.\n* [http://swingly.com/ Swingly] - Swingly is an answer engine designed to find exact answers to factual questions. Just ask a question in plain English - and Swingly will find you the answer (or answers) you\'re looking for (according to their site).\n* [[Yebol]] - Yebol is a vertical "decision" search engine that had developed a knowledge-based, semantic search platform. Yebol\'s artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and content that it presents in a visually indexed format that is more aligned with initial human intent. Yebol uses association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol integrates natural language processing, metasynthetic-engineered open complex systems, and machine algorithms with human knowledge for each query to establish a web directory that actually \'learns\', using correlation, clustering and classification algorithms to automatically generate the knowledge query, which is retained and regenerated forward.<ref>Humphries, Matthew. [http://www.geek.com/articles/news/yebolcom-steps-into-the-search-market-20090731/ "Yebol.com steps into the search market"] \'\'Geek.com\'\'. 31 July 2009.</ref>\n\n==See also==\n*[[Attempto Controlled English]]\n*[[Natural user interface]]\n*[[Natural language programming]]\n**[[xTalk]], a family of English-like programming languages\n*[[Chatterbot]], a computer program that simulates human conversations\n*[[Noisy text]]\n*[[Question answering]]\n*[[Selection-based search]]\n*[[Semantic search]]\n*[[Semantic query]]\n*[[Semantic Web]]\n\n==References==\n{{reflist}}\n\n{{Internet search}}\n{{Computable knowledge}}\n\n{{DEFAULTSORT:Natural language user interface}}\n[[Category:User interfaces]]\n[[Category:Artificial intelligence applications]]\n[[Category:Natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Information retrieval techniques]]']
['Cosine similarity', '8966592', '\'\'\'Cosine similarity\'\'\' is a measure of similarity between two non zero vectors of an [[inner product space]] that measures the [[cosine]] of the angle between them. The cosine of 0° is 1, and it is less than 1 for any other angle. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. The name derives from the term "direction cosine": in this case, note that unit vectors are maximally "similar" if they\'re parallel and maximally "dissimilar" if they\'re orthogonal (= perpendicular).  It should not escape the alert reader\'s attention that this is analogous to cosine, which is unity (maximum value) when the segments subtend a zero angle and zero (uncorrelated) when the segments are perpendicular.\n\nNote that these bounds apply for any number of dimensions, and cosine similarity is most commonly used in high-dimensional positive spaces. For example, in [[information retrieval]] and [[text mining]], each term is notionally assigned a different dimension and a document is characterised by a vector where the value of each dimension corresponds to the number of times that term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.<ref>Singhal, Amit (2001). "Modern Information Retrieval: A Brief Overview". Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 24 (4): 35–43.</ref>\n\nThe technique is also used to measure cohesion within clusters in the field of [[data mining]].<ref>P.-N. Tan, M. Steinbach & V. Kumar, "Introduction to Data Mining", , Addison-Wesley (2005), ISBN 0-321-32136-7, chapter 8; page 500.</ref>\n\n\'\'Cosine distance\'\' is a term often used for the complement in positive space, that is: <math>D_C(A,B) = 1 - S_C(A,B)</math>. It is important to note, however, that this is not a proper [[distance metric]] as it does not have the [[triangle inequality]] property—or, more formally, the [[Schwarz inequality]]—and it violates the coincidence axiom; to repair the triangle inequality property while maintaining the same ordering, it is necessary to convert to angular distance (see below.)\n\nOne of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors, as only the non-zero dimensions need to be considered.\n\n==Definition==\n\nThe cosine of two non zero vectors can be derived by using the [[Euclidean vector#Dot product|Euclidean dot product]] formula:\n\n:<math>\\mathbf{a}\\cdot\\mathbf{b}\n=\\left\\|\\mathbf{a}\\right\\|\\left\\|\\mathbf{b}\\right\\|\\cos\\theta</math>\n\nGiven two [[Vector (geometric)|vectors]] of attributes, \'\'A\'\' and \'\'B\'\', the cosine similarity, \'\'cos(θ)\'\', is represented using a [[dot product]] and [[Magnitude (mathematics)#Euclidean vector space|magnitude]] as\n\n:<math>\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }</math> , where <math>A_i</math> and <math>B_i</math> are [[Euclidean vector#Decomposition|components]] of vector <math>A</math> and <math>B</math> respectively.\n\nThe resulting similarity ranges from &minus;1 meaning exactly opposite, to 1 meaning exactly the same, with 0 indicating orthogonality (decorrelation), and in-between values indicating intermediate similarity or dissimilarity.\n\nFor text matching, the attribute vectors \'\'A\'\' and \'\'B\'\' are usually the [[tf-idf|term frequency]] vectors of the documents.  The cosine similarity can be seen as a method of normalizing document length during comparison.\n\nIn the case of [[information retrieval]], the cosine similarity of two documents will range from 0 to 1, since the term frequencies ([[tf-idf]] weights) cannot be negative. The angle between two term frequency vectors cannot be greater than&nbsp;90°.\n\nIf the attribute vectors are normalized by subtracting the vector means (e.g., <math>A - \\bar{A}</math>), the measure is called centered cosine similarity and is equivalent to the [[Pearson product-moment correlation coefficient#For a sample|Pearson Correlation Coefficient]].\n\n=== Angular distance and similarity ===\n\nThe term "cosine similarity" is sometimes used to refer to different definition of similarity provided below. However the most common use of "cosine similarity" is as defined above and the similarity and distance metrics defined below are referred to as "angular similarity" and "angular distance" respectively. The normalized angle between the vectors is a formal [[distance metric]] and can be calculated from the similarity score defined above. This angular distance metric can then be used to compute a similarity function bounded between 0 and 1, inclusive.\n\nWhen the vector elements may be positive or negative:\n\n:<math>\\text{distance} = \\frac{ \\cos^{-1}( \\text{similarity} ) }{ \\pi } </math>\n\n:<math>\\text{similarity} = 1 - \\text{distance} </math>\n\nOr, if the vector elements are always positive:\n\n:<math>\\text{distance} = \\frac{ 2 \\cdot \\cos^{-1}( \\text{similarity} ) }{ \\pi }</math>\n\n:<math>\\text{similarity} = 1 - \\text{distance}</math>\n\nAlthough the term "cosine similarity" has been used for this angular distance, the term is oddly used as the cosine of the angle is used only as a convenient mechanism for calculating the angle itself and is no part of the meaning. The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper [[distance metric]], which is not the case for the first meaning. However, for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.\n\n=== Confusion with "Tanimoto" coefficient ===\n\nThe cosine similarity may be easily confused with the Tanimoto metric - a specialised form of a similarity coefficient with a similar algebraic form:\n\n:<math>T(A,B) = {A \\cdot B \\over \\|A\\|^2 +\\|B\\|^2 - A \\cdot B}</math>\n\nIn fact, this algebraic form [[Jaccard index#Tanimoto Similarity and Distance|was first defined by Tanimoto]] as a mechanism for calculating the [[Jaccard coefficient]] in the case where the sets being compared are represented as [[bit vector]]s. While the formula extends to vectors in general, it has quite different properties from cosine similarity and bears little relation other than its superficial appearance.\n\n=== Ochiai coefficient ===\nThis coefficient is also known in biology as Ochiai coefficient, or Ochiai-Barkman coefficient, or Otsuka-Ochiai coefficient:<ref>\'\'Ochiai A.\'\' Zoogeographical studies on the soleoid fishes found Japan and its neighboring regions. II // Bull. Jap. Soc. sci. Fish. 1957. V. 22. № 9. P. 526-530.</ref><ref>\'\'Barkman J.J.\'\' Phytosociology and ecology of cryptogamic epiphytes, including a taxonomic survey and description of their vegetation units in Europe. – Assen. Van Gorcum. 1958. 628 p.</ref>\n:<math>K =\\frac{n(A \\cap B)}{\\sqrt{n(A) \\times n(B)}}</math>\nHere, <math>A</math> and <math>B</math> are sets, and <math>n(A)</math> is the number of elements in <math>A</math>. If sets are represented as [[bit vector]]s, the Ochiai coefficient can be seen to be the same as the cosine similarity.\n\n== Properties ==\nCosine similarity is related to [[Euclidean distance]] as follows. Denote Euclidean distance by the usual <math>\\|A - B\\|</math>, and observe that\n\n:<math>\\|A - B\\|^2 = (A - B)^\\top (A - B) = \\|A\\|^2 + \\|B\\|^2 - 2 A^\\top B</math>\n\nby [[Polynomial expansion|expansion]]. When {{mvar|A}} and {{mvar|B}} are normalized to unit length, <math>\\|A\\|^2 = \\|B\\|^2 = 1</math> so the previous is equal to\n\n:<math>2 (1 - \\cos(A, B))</math>\n\n\'\'\'Null distribution:\'\'\' For data which can be negative as well as positive, the [[null distribution]] for cosine similarity is the distribution of the dot product of two independent random unit vectors. This distribution has a [[mean]] of zero and a [[variance]] of <math>1/n</math> (where <math>n</math> is the number of dimensions), and although the distribution is bounded between -1 and +1, as <math>n</math> grows large the distribution is increasingly well-approximated by the [[normal distribution]].<ref>{{cite journal\n | author = Spruill, Marcus C\n | year = 2007\n | title = Asymptotic distribution of coordinates on high dimensional spheres\n | journal = Electronic communications in probability\n | volume = 12 | pages = 234–247\n | doi = 10.1214/ECP.v12-1294\n}}</ref><ref>[http://stats.stackexchange.com/questions/85916/distribution-of-dot-products-between-two-random-unit-vectors-in-mathbbrd CrossValidated: Distribution of dot products between two random unit vectors in RD]</ref>\nFor other types of data, such as bitstreams (taking values of 0 or 1 only), the null distribution will take a different form, and may have a nonzero mean.<ref>{{cite journal\n | author = Graham L. Giller \n | year = 2012\n | title = The Statistical Properties of Random Bitstreams and the Sampling Distribution of Cosine Similarity\n | journal = Giller Investments Research Notes\n | number = 20121024/1\n | doi = 10.2139/ssrn.2167044\n}}</ref>\n\n== Soft cosine measure ==\nSoft cosine measure\nis a measure of “soft” similarity between two vectors, i.e., the measure that considers similarity of pairs of features.<ref>{{cite journal|last1=Sidorov|first1=Grigori|last2=Gelbukh|first2=Alexander|last3=Gómez-Adorno|first3=Helena|last4=Pinto|first4=David|title=Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model|journal=Computación y Sistemas|volume=18|issue=3|pages=491–504|doi=10.13053/CyS-18-3-2043|url=http://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043|accessdate=7 October 2014}}</ref> The traditional cosine similarity considers the [[vector space model]] (VSM) features as independent or completely different, while the soft cosine measure proposes considering the similarity of features in VSM, which allows generalization of the concepts of cosine measure and also the idea of similarity (soft similarity).\n\nFor example, in the field of [[natural language processing]] (NLP) the similarity among features is quite intuitive. Features such as words, n-grams or syntactic n-grams<ref>{{cite book|last1=Sidorov|first1=Grigori|last2=Velasquez|first2=Francisco|last3=Stamatatos|first3=Efstathios|last4=Gelbukh|first4=Alexander|last5=Chanona-Hernández|first5=Liliana|title=Syntactic Dependency-based N-grams as Classification Features|publisher=LNAI 7630|isbn=978-3-642-37798-3|pages=1–11|url=http://link.springer.com/chapter/10.1007%2F978-3-642-37798-3_1|accessdate=7 October 2014}}</ref> can be quite similar, though formally they are considered as different features in the VSM. For example, words “play” and “game” are different words and thus are mapped to different dimensions in VSM; yet it is obvious that they are related semantically. In case of [[n-grams]] or syntactic n-grams, [[Levenshtein distance]] can be applied (in fact, Levenshtein distance can be applied to words as well).\n\nFor calculation of the soft cosine measure, the matrix {{math|\'\'\'s\'\'\'}} of similarity between features is introduced. It can be calculated using Levenshtein distance or other similarity measures, e.g., various [[WordNet]] similarity measures. Then we just multiply by this matrix.\n\nGiven two {{math|\'\'N\'\'}}-dimension vectors a and b, the soft cosine similarity is calculated as follows:\n\n:<math>\\begin{align}\n    \\operatorname{soft\\_cosine}_1(a,b)=\n    \\frac{\\sum\\nolimits_{i,j}^N s_{ij}a_ib_j}{\\sqrt{\\sum\\nolimits_{i,j}^N s_{ij}a_ia_j}\\sqrt{\\sum\\nolimits_{i,j}^N s_{ij}b_ib_j}},\n\\end{align}\n</math>\n\nwhere {{math|\'\'s<sub>ij</sub>\'\' {{=}} similarity(feature<sub>\'\'i\'\'</sub>, feature<sub>\'\'j\'\'</sub>)}}.\n\nIf there is no similarity between features ({{math|\'\'s<sub>ii</sub>\'\' {{=}} 1}}, {{math|\'\'s<sub>ij</sub>\'\' {{=}} 0}} for {{math|\'\'i\'\' ≠ \'\'j\'\'}}), the given equation is equivalent to the conventional cosine similarity formula.\n\nThe complexity of this measure is quadratic, which makes it perfectly applicable to real world tasks. The complexity can be transformed to subquadratic.{{citation needed|date=December 2015}}\n\n== See also ==\n* [[Sørensen similarity index|Sørensen\'s quotient of similarity]]\n* [[Hamming distance]]\n* [[Correlation]]\n* [[Dice\'s coefficient]]\n* [[Jaccard index]]\n* [[SimRank]]\n* [[Information retrieval]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* [http://mathforum.org/kb/message.jspa?messageID=5658016&tstart=0 Weighted cosine measure]\n* [http://blog.christianperone.com/?p=2497 A tutorial on cosine similarity using Python]\n* [http://www.rxnlp.com/api-reference/text-similarity-api-reference/ Web API to Compute Cosine, Jaccard and Dice for Text in Any Language]\n\n{{DEFAULTSORT:Cosine Similarity}}\n[[Category:Information retrieval techniques]]']
['Policy framework', '21828505', "{{refimprove|date=March 2009}}\n{{globalize|date=June 2015}}\nA '''policy framework''' is a logical structure that is established to organize policy documentation into groupings and categories that make it easier for employees to find and understand the contents of various [[policy]] documents. Policy frameworks can also be used to help in the planning and development of the policies for an organization.\n\n==Principles==\n[[State Services Commission]] of [[New Zealand]] outlines eleven principles of policy framework as below.<ref>http://www.ssc.govt.nz/Documents/policy_framework_for_Government_.htm</ref>\n\n===Availability===\nGovernment departments should make information available easily, widely and equitably to the people of New Zealand (except where reasons preclude such availability as specified in legislation).....\n\n===Coverage===\nGovernment departments should make the following information increasingly available on an electronic basis:\n* all published material or material already in the public domain\n* all policies that could be released publicly\n* all information created or collected on a statutory basis (subject to commercial sensitivity and privacy considerations)\n* all documents that the public may be required to complete\n* corporate documentation in which the public would be interested\n\n===Pricing=== \na) Free dissemination of Government-held information is appropriate where:\n* dissemination to a target audience is desirable for a public policy purpose, or\n* a charge to recover the cost of dissemination is not feasible or cost-effective\n\nb) Pricing to recover the cost of dissemination is appropriate where:\n* there is no particular public policy reason to disseminate the information, and \n* a charge to recover the cost of dissemination is both feasible and cost effective\n\nc) Pricing to recover the cost of transformation is appropriate where:\n* pricing to recover the cost of dissemination is appropriate, and\n* there is an avoidable cost involved in transforming the information from the form in which it is held into a form preferred by the recipient, where it is feasible and cost-effective to recover in addition to the cost of dissemination\n\nd) Pricing to recover the full costs of information production and dissemination is appropriate where:\n* the information is created for the commercial purpose of sale at a profit, and \n* to do so would not breach the other pricing principles\n\n===Ownership===\nGovernment-held information, created or collected by any person employed or engaged by the Crown is a strategic resource 'owned' by the Government as a steward on behalf of the public.\n\n===Stewardship===\nGovernment departments are stewards of Government-held information, and it is their responsibility to implement good information management.\n\n===Collection===\nGovernment departments should only collect information for specified public policy, operational business or legislative purposes.\n\n===Copyright===\nInformation created by departments is subject to Crown copyright but where wide dissemination is desirable, the Crown should permit use of its copyrights subject to acknowledgement of source.\n \n===Preservation===\nGovernment-held information should be preserved only where a public business need, legislative or policy requirement, or a historical or archival reason, exists.\n\n===Quality===\nThe key qualities underpinning Government-held information include accuracy, relevancy, timeliness, consistency and collection without bias so that the information supports the purposes for which it is collected.\n\n===Integrity===\nThe integrity of Government-held information will be achieved when:\n* all guarantees and conditions surrounding the information are met\n* the principles are clear and communicated\n* any situation relating to Government-held information is handled openly and consistently\n* those affected by changes to Government-held information are consulted on those changes\n* those charged as independent guardians of the public interest  (e.g. the Ombudsman) have confidence in the ability of departments to manage the information well\n* there are minimum exceptions to the principles.\n\n===Privacy===\nThe principles of the Privacy Act 1993 apply.\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Policy Framework}}\n[[Category:Information retrieval techniques]]\n[[Category:Government of New Zealand]]"]
['Probabilistic relevance model', '25959000', '\nThe \'\'\'probabilistic relevance model\'\'\'<ref>{{citation | author=S. E. Robertson and  K. S. Jones | title=Relevance weighting of search terms | publisher=Journal of the American Society for Information Science | pages=129–146 | date=May–June 1976 | url=http://portal.acm.org/citation.cfm?id=106783 }}</ref><ref name="robertson2009">{{Cite journal | author=Stephen Robertson and Hugo Zaragoza | title=The Probabilistic Relevance Framework: BM25 and Beyond | date=2009 | url=http://dl.acm.org/citation.cfm?id=1704810 | publisher=Found. Trends Inf. Retr. | volume=3 | issue=4 | pages=333-389 | doi=10.1561/1500000019 }}</ref> was devised by Robertson and Jones as a framework for [[Statistical model | probabilistic models]] to come. It is a formalism of [[information retrieval]] useful to derive [[ranking function]]s used by [[search engine]]s and  [[web search engine]]s in order to rank matching documents according to their [[Relevance (information retrieval)|relevance]] to a given search query.\n \nIt makes an estimation of the probability of finding if a document \'\'d<sub>j</sub>\'\' is relevant to a query \'\'q\'\'. This model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query \'\'q\'\'. Such an ideal answer set is called \'\'R\'\' and should maximize the overall probability of relevance to that user. The prediction is that documents in this set \'\'R\'\' are relevant to the query, while documents not present in the set are non-relevant.\n\n<math>sim(d_{j},q) = \\frac{P(R|\\vec{d}_j)}{P(\\bar{R}|\\vec{d}_j)}</math>\n\n==Related models==\nThere are some limitations to this framework that need to be addressed by further development:\n* There is no accurate estimate for the first run probabilities\n* Index terms are not weighted\n* Terms are assumed mutually independent\n\nTo address these and other concerns there are some developed models from the probabilistic relevance framework. The [[Binary Independence Model]] for one, as it is from the same author. The most known derivative of this framework is the [[Probabilistic relevance model (BM25)|Okapi(BM25)]] weighting scheme and its BM25F brother.\n\n==References==\n{{reflist}}\n\n[[Category:Information retrieval techniques]]\n[[Category:Probabilistic models]]']
['Adversarial information retrieval', '11486091', '\'\'\'Adversarial information retrieval\'\'\' (\'\'\'adversarial IR\'\'\') is a topic in [[information retrieval]] related to strategies for working with a data source where some portion of it has been manipulated maliciously.  Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation.\n\nOn the Web, the predominant form of such manipulation is [[spamdexing|search engine spamming]] (also known as spamdexing), which involves employing various techniques to disrupt the activity of [[web search engines]], usually for financial gain. Examples of spamdexing are [[Google bomb|link-bombing]], [[comment spam (disambiguation)|comment]] or [[referrer spam]], [[spam blog]]s (splogs), malicious tagging.  [[Reverse engineering]] of [[ranking function|ranking algorithms]], [[Ad filtering|advertisement blocking]], [[click fraud]],<ref>Jansen, B. J. (2007) [https://faculty.ist.psu.edu/jjansen/academic/jansen_click_fraud.pdf Click fraud]. IEEE Computer. 40(7), 85-86.</ref> and [[web content filtering]] may also be considered forms of adversarial [[data manipulation]].<ref>B. Davison, M. Najork, and T. Converse (2006), [http://wayback.archive.org/web/20090320173324/http://www.acm.org/sigs/sigir/forum/2006D/2006d_sigirforum_davison.pdf SIGIR Worksheet Report: Adversarial Information Retrieval on the Web (AIRWeb 2006)]</ref>\n\nActivities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic.\n\n== Topics ==\nTopics related to Web spam (spamdexing):\n\n* [[Link spam]]\n* [[Keyword spamming]]\n* [[Cloaking]]\n* Malicious tagging\n* Spam related to blogs, including [[spam in blogs|comment spam]], [[spam blog|splogs]], and [[sping|ping spam]]\n\nOther topics:\n* [[Click fraud]] detection\n* Reverse engineering of  [[search engine]]\'s [[ranking]] algorithm\n* Web [[content filtering]]\n* [[Ad filtering|Advertisement blocking]]\n* Stealth [[web crawling|crawling]]\n*[[Troll (Internet)]]\n* Malicious tagging or voting in [[social networks]]\n* [[Astroturfing]]\n* [[Sockpuppetry]]\n\n== History ==\nThe term "adversarial information retrieval" was first coined in 2000 by [[Andrei Broder]] (then Chief Scientist at [[Alta Vista]]) during the Web plenary session at the [[Text Retrieval Conference|TREC]]-9 conference.<ref>D. Hawking and N. Craswell (2004), [http://es.csiro.au/pubs/trecbook_for_website.pdf Very Large Scale Retrieval and Web Search (Preprint version)]</ref>\n\n== See also ==\n*[[Spamdexing]]\n*[[Information retrieval]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n*[http://airweb.cse.lehigh.edu/ AIRWeb]: series of workshops on Adversarial Information Retrieval on the Web\n*[http://webspam.lip6.fr/ Web Spam Challenge]: competition for researchers on Web Spam Detection\n*[http://wayback.archive.org/web/20100217125910/http://barcelona.research.yahoo.net/webspam/ Web Spam Datasets]: datasets for research on Web Spam Detection\n\n{{DEFAULTSORT:Adversarial Information Retrieval}}\n[[Category:Information retrieval genres]]\n[[Category:Internet fraud]]']
['Multimodal search', '34005384', "'''Multimodal search''' is a type of [[search engines|search]] that uses different methods to get relevant results. They can use any kind of search, [[keyword search|search by keyword]], [[concept search|search by concept]], [[Query by Example|search by example]],etc.\n\n== Introduction ==\n\nA multimodal search engine is designed to imitate the flexibility and agility of how the [[mind|human mind]] works to create, process and refuse irrelevant ideas. So, the more elements you have in the input of the search engine to can compare, the more [[Arithmetic precision|accurate]] the results can be.\nMultimodal search engines use different inputs of different nature and methods of search at the same time with the possibility of combining the results by merging all of the input elements of the search. There are also engines that can use a feedback of the results with the evaluation of the user to perform a more appropriate and relevant search.\n\n[[File:Schema of a simple search.jpg|thumb|Schema of a Simple Search]]\n\nNowadays, mobile devices have been developed to a point that they can perform infinite functions from any place at any time, thanks to the [[internet]] and [[GPS]] connections. Touch screens, motion sensors and voice recognition are now featured on mobile devices called [[smartphone]]s. All the features and functions make possible to can execute multimodal searches from any place of the world at any time.\n\n=== Search elements ===\n\nThe use of text is an option, as well as [[multimedia search]]ing, [[image]]s, [[video]]s, [[Content (media)|audio]], [[voice]], [[document]]s. Even the location of the user can help the search engine to perform a more effective search, adaptable to every situation.\nNowadays, different ways to [[Human–computer interaction|interact]] with a search engine are being discovered, in terms of input elements of the search and in the variety of results obtained.\n\n=== Personal context ===\n\nMany queries from mobiles are [[location-based service|location-based]] (LBS), that use the location of the user to interact with the applications. If available, the browser uses the device GPS, or computes an approximate location based on cell tower triangulation, with the permission of the user, who must be agree to share his/her location with the application in the download.\nTherefore, multimodal searches use not only audiovisual content that the user provides directly, but also the context where the user is, like his/her location, language, time at the moment, web site or document where the user is surfing, or other elements that can help to improve of a search in every situation.\n[[File:Contextual query.jpg|Example of Contextual Query]]\n\n== Classification of the results ==\n\nThe multimodal search engine works in parallel, whilst at the same time, performs a search of more to less relevance of every element introduced directly or indirectly (personal context). Afterwards, it provides a combination of all the results, merging every element with its associated weight for every descriptor.\n\nThe engine analyzes every element and tags them, so a comparison of the tags can be made with existent indexed information in databases. A classification of the results proceeds, to show them from more to less relevance.\n\n[[File:Framework of Multimodal Search.jpg|thumb|Framework of a Multimodal Search]]\n\nIt’s necessary to define the importance of every input element. There are search engines that do this automatically, however there are also engines where the user can do it manually, giving more or less weight to every element of the search.\nIt’s also important that the user provides the appropriate and essential information for the search; too much information can confuse the system and provide unsatisfactory results.\nWith multimodal searches users can get better results than with a simple search, but multimodal searches must process more input information. It can also spend more time to process it and require more memory space.\n\nAn efficient search engine interprets the query of the users, realizes his/her intention and applies a strategy to use an appropriate search, i.e. the engine adapts to every input query and also to the combination of the elements and methods.\n\n== Applications ==\n\nNowadays, existing multimodal search engines are not very complex, and some of them are in an experimental phase. Some of the more simple engines are [[Google Images]] [http://images.google.es/] or [[Bing (search engine)|Bing]] [http://www.bing.com], web interfaces that use text and images as inputs to find images in the output.\n\nMMRetrieval [http://www.aviarampatzis.com/publications/p117-zagoris.pdf] is a multimodal experimental search engine that uses multilingual and multimedia information through a web interface. The engine searches the different inputs in parallel and merges all the results by different chosen methods. The engine also provides different multistage retrieval, as well as a single text index baseline to be able to compare all the different phases of search.\n\nThere are a lot of applications for mobile devices, using the context of the user, like based-location services, and using also text, images, audios or videos that the user provides at the moment or with saved files, or even interacting with the voice.\n\n== References ==\n\n* Query-Adaptive Fusion for Multimodal Search,Lyndon Kennedy, Student Member IEEE, Shih-Fu Chang, Fellow IEEE, and Apostol Natsev [http://www.ee.columbia.edu/~lyndon/pubs/pieee2008-queryadaptive.pdf]\n* Context-aware Querying for Multimodal Search Engines, Jonas Etzold, Arnaud Brousseau, Paul Grimm and Thomas Steiner [http://www.lsi.upc.edu/~tsteiner/papers/2012/context-aware-querying-mmm2012.pdf]\n* Apply Multimodal Search and Relevance Feedback In a Digital Video Library, Thesis of Yu Zhong [http://www.informedia.cs.cmu.edu/documents/zhong_thesis_may00.pdf]\n* Aplicació rica d’internet per a la consulta amb text i imatge al repositori de vídeos de la Corporació Catalana de Mitjans Audiovisuals, Ramon Salla, Universitat Politècnica de Catalunya [http://upcommons.upc.edu/pfc/bitstream/2099.1/8766/1/PFC.pdf]\n\n== External links ==\n* MMRetrieval [http://www.mmretrieval.net]\n* Google Images [http://images.google.es/]\n* Bing [http://www.bing.com]\n\n<!--- Categories --->\n[[Category:Information retrieval genres]]\n[[Category:Internet search engines]]\n[[Category:Multimedia]]"]
['Legal information retrieval', '24997830', '\'\'\'Legal information retrieval\'\'\' is the science of [[information retrieval]] applied to legal text, including [[legislation]], [[case law]], and scholarly works.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 1</ref> Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means.<ref name=Jackson>Jackson et al., p. 60</ref> Legal information retrieval is a part of the growing field of [[legal informatics]].\n\n== Overview ==\n\nIn a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used [[boolean search]] methods (exact matches of specified terms) on full text legal documents have been shown to have an average [[recall rate]] as low as 20 percent,<ref name="Blair, D.C. 1985, p.293">Blair, D.C., and Maron, M.E., 1985, p.293</ref> meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents.<ref name="Blair, D.C. 1985, p.293"/> This may result in failing to retrieve important or [[precedential]] cases. In some jurisdictions this may be especially problematic, as legal professionals are [[legal ethics|ethically]] obligated to be reasonably informed as to relevant legal documents.<ref>American Bar Association, Model Rules of Professional Conduct Rule 1.1, http://www.abanet.org/cpr/mrpc/rule_1_1.html</ref>\n\nLegal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high [[recall rate]]) and reducing the number of irrelevant documents (a high [[precision rate]]). This is a difficult task, as the legal field is prone to [[jargon]],<ref>Peters, W. et al. 2007, p. 118</ref> [[polysemes]]<ref>Peters, W. et al. 2007, p. 130</ref> (words that have different meanings when used in a legal context), and constant change.\n\nTechniques used to achieve these goals generally fall into three categories: [[boolean search|boolean]] retrieval, manual classification of legal text, and [[natural language processing]] of legal text.\n\n== Problems ==\n\nApplication of standard [[information retrieval]] techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent [[Taxonomy (general)|taxonomy]].<ref name=LOIS1>Peters, W. et al. 2007, p. 120</ref> Instead, the law is generally filled with open-ended terms, which may change over time.<ref name=LOIS1 /> This can be especially true in [[common law]] countries, where each decided case can subtly change the meaning of a certain word or phrase.<ref>Saravanan, M. et al.  2009, p. 101</ref>\n\nLegal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term "worker" has four different meanings:<ref name="Peters, W. et al. 2007, p. 131">Peters, W. et al. 2007, p. 131</ref>\n\n#Any worker as defined in Article 3(a) of [[Directive 89/391/EEC]] who habitually uses display screen equipment as a significant part of his normal work.\n#Any person employed by an employer, including trainees and apprentices but excluding domestic servants;\n#Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;\n#Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;\n\nIn addition, it also has the common meaning: \n<ol start="5">\n<li>A person who works at a specific occupation.<ref name="Peters, W. et al. 2007, p. 131"/> </li>\n</ol>\n\nThough the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results.\n\nEven if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case.<ref name=MaxwellA >Maxwell, K.T., and Schafer, B. 2008, p. 8</ref> Case decisions from senior or [[superior court]]s may be more relevant than those from [[lower court]]s, even where the lower court\'s decision contains more discussion of the relevant facts.<ref name=MaxwellA  /> The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case).<ref name=MaxwellA  /> A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.\n\nAdditionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court\'s decision which supports his position more valuable than a senior courts position which does not.<ref name=MaxwellA  /> He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.<ref name=MaxwellA />\n\nOvercoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day<ref name=Jackson />), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.<ref name=Jackson /><ref>Maxwell, K.T., and Schafer, B. 2007, p.1</ref>\n\n== Techniques ==\n\n===Boolean searches===\n\n[[Boolean search]]es, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented by services such as [[Westlaw]], [[LexisNexis]], and [[Findlaw]].  However, they overcome few of the problems discussed above.\n\nThe recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search\'s [[recall rate]] to be roughly 20%, and its precision rate to be roughly 79%.<ref name="Blair, D.C. 1985, p.293"/> Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.<ref>Saravanan M., et al. 2009, p. 116</ref>\n\n===Manual classification===\n\nIn order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an [[ontology]] to classify the texts, based on the way a legal professional might think about them.<ref name="Maxwell, K.T. 2008, p. 2">Maxwell, K.T., and Schafer, B. 2008, p. 2</ref> These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as [[Westlaw]]\'s “Natural Language”<ref name=WL>Westlaw Research, http://www.westlaw.com</ref> or [[LexisNexis]]\' Headnote<ref name=LN>Lexis Research, http://www.lexisnexis.com</ref> searches. Additionally, both of these services allow browsing of their classifications, via Westlaw\'s West Key Numbers<ref name=WL /> or Lexis\' Headnotes.<ref name=LN /> Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).<ref name="Maxwell, K.T. 2008, p. 2"/>\n\nThese systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text.<ref name="Maxwell, K.T. 2008, p. 3">Maxwell, K.T., and Schafer, B. 2008, p. 3</ref> In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals.<ref>Saravanan, M. et al.  2009, p. 116</ref> The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.<ref>Saravanan, M. et al. 2009, p. 103</ref>\n\nThe major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts.<ref name="Maxwell, K.T. 2008, p. 3"/><ref>Schweighofer, E. and Liebwald, D. 2008, p. 108</ref> As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.<ref>Maxwell, K.T., and Schafer, B. 2008, p. 4</ref>\n\n===Natural language processing===\n\nIn order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries.<ref name=Jackson /><ref name=AshleyA>Ashley, K.D. and Bruninghaus, S. 2009, p. 125</ref><ref name=Gelbart>Gelbart, D. and Smith, J.C. 1993, p. 142</ref> Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ [[Natural Language Processing]] (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal [[ontology]]. Though multiple systems have been postulated,<ref name=Jackson /><ref name=AshleyA /><ref name=Gelbart /> few have reported results. One system, “SMILE,” which attempted to automatically extract classifications from case texts, resulted in an [[f-measure]] (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0).<ref name=AshleyB >Ashley, K.D. and Bruninghaus, S. 2009, p. 159</ref> This is probably much lower than an acceptable rate for general usage.<ref name=AshleyB /><ref>Maxwell, K.T., and Schafer, B. 2009, p. 3</ref>\n\nDespite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 9</ref><ref>Ashley, K.D. and Bruninghaus, S. 2009, p. 126</ref>\n\n== List of retrieval systems ==\nFree-to-use law-texts and associated oficial metadata:\n\n* [[LexML Brazil]]\n* [http://www.legislation.gov.uk/ legislation.gov.uk]\n* [[EUR-Lex#N-Lex|N-Lex]]\n* ...\n\n== Notes ==\n{{Reflist|2}}\n\n==References==\n{{Refbegin}}\n*{{cite journal\n|author1=Maxwell, K.T. |author2=Schafer, B.\n|year       = 2008\n|title      = Concept and Context in Legal Information Retrieval\n|url        = http://portal.acm.org/citation.cfm?id=1564016\n|journal    = Frontiers in Artificial Intelligence and Applications\n|volume     = 189\n|pages      = 63–72\n|publisher  = IOS Press\n|accessdate = 2009-11-07\n}}\n*{{cite journal\n|author     = Jackson, P.|year       = 1998\n|title      = Information extraction from case law and retrieval of prior cases by partial parsing and query generation\n|url        = http://portal.acm.org/citation.cfm?id=288627.288642\n|journal    = Conference on Information and Knowledge Management\n|pages      = 60–67\n|publisher  = ACM\n|accessdate = 2009-11-07\n|display-authors=etal}}\n*{{cite journal\n|author1=Blair, D.C. |author2=Maron, M.E.\n|year       = 1985\n|title      = An evaluation of retrieval effectiveness for a full-text document-retrieval\n|url        = http://portal.acm.org/citation.cfm?id=3166.3197&coll=GUIDE&dl=GUIDE&CFID=61732097&CFTOKEN=95519997\n|journal    = Communications of the ACM\n|volume     = 28\n|issue      = 3 \n|pages      = 289–299\n|publisher  = ACM\n|accessdate = 2009-11-07\n|doi=10.1145/3166.3197\n}}\n*{{cite journal\n|author     = Peters, W.|year       = 2007\n|title      = The structuring of legal knowledge in LOIS\n|url        = http://www.springerlink.com/content/d04l7h2507700g45/\n|journal    = Artificial Intelligence and Law\n|volume     = 15\n|issue      = 2\n|pages      = 117–135\n|publisher  = Springer Netherlands\n|accessdate = 2009-11-07\n|doi=10.1007/s10506-007-9034-4\n|display-authors=etal}}\n*{{cite journal\n|author     = Saravanan, M.|year       = 2007\n|title      = Improving legal information retrieval using an ontological framework \n|url        = http://www.springerlink.com/content/h66412k08h855626/\n|journal    = Artificial Intelligence and Law\n|volume     = 17\n|issue      = 2\n|pages      = 101–124\n|publisher  = Springer Netherlands\n|accessdate = 2009-11-07\n|doi=10.1007/s10506-009-9075-y\n|display-authors=etal}}\n*{{cite journal\n|author1=Schweighofer, E.  |author2=Liebwald, D.\n|year       = 2007\n|title      = Advanced lexical ontologies and hybrid knowledge based systems: First steps to a dynamic legal electronic commentary\n|url        = http://www.springerlink.com/content/v62v7131x10413v0/\n|journal    = Artificial Intelligence and Law\n|volume     = 15\n|issue      = 2\n|pages      = 103–115\n|publisher  = Springer Netherlands\n|accessdate = 2009-11-07\n|doi=10.1007/s10506-007-9029-1\n}}\n*{{cite journal\n|author1=Gelbart, D.  |author2=Smith, J.C.\n|year       = 1993\n|title      = FLEXICON: an evaluation of a statistical ranking model adapted to intelligent legal text management\n|url        = http://portal.acm.org/citation.cfm?id=158994\n|journal    = International Conference on Artificial Intelligence and Law\n|pages      = 142–151\n|publisher  = ACM\n|accessdate = 2009-11-07\n}}\n*{{cite journal\n|author1=Ashley, K.D.  |author2=Bruninghaus, S.\n|year       = 2009\n|title      = Automatically classifying case texts and predicting outcomes\n|url        = http://www.springerlink.com/content/lhg8837331hgu024/\n|journal    = Artificial Intelligence and Law\n|volume     = 17\n|issue      = 2\n|pages      = 125–165\n|publisher  = Springer Netherlands\n|accessdate = 2009-11-07\n|doi=10.1007/s10506-009-9077-9\n}}\n{{Refend}}\n\n{{DEFAULTSORT:Legal Information Retrieval}}\n[[Category:Information retrieval genres]]\n[[Category:Natural language processing]]\n[[Category:Legal research]]']
['Expertise finding', '20227676', '{{multiple issues|\n{{original research|date=June 2015}}\n{{refimprove|date=June 2015}}\n{{cleanup|date=November 2010}}{{External links|date=January 2012}}\n}}\n\'\'\'Expertise finding\'\'\' is the use of tools for finding and assessing individual [[expertise]], with particular focus on scientific expertise.\n\n== Importance of expertise ==\n\nIt can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing.  [[Intellectual property]] can be similarly licensed.  Furthermore, expertise finding is also a key aspect of [[institutional memory]], as without its experts an institution is effectively decapitated.  However, finding and “licensing” expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.\n\nUntil very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best.  Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that one’s judgment about those individuals is justified and that their answers are thoughtful.\n\nIn the last fifteen years, a class of [[knowledge management]] software has emerged to facilitate and improve the quality of expertise finding, termed “expertise locating systems”.  These software range from [[Social network service|social networking systems]] to [[knowledge base]]s.  Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as [[Recommender system|“recommender systems”]].\n\nAt the other end of the spectrum are specialized [[knowledge base]]s that rely on experts to populate a specialized type of [[database]] with their self-determined areas of expertise and contributions, and do not rely on user recommendations.  Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.\n\nStill other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed “gated objects”, e.g., [[citation impact]]s for scientific papers or [[data mining]] approaches wherein many of the work products of an expert are collated.  Such systems are more likely to be free of user-introduced biases (e.g., [http://researchscorecard.com/ ResearchScorecard] ), though the use of computational methods can introduce other biases.\n\nMore recently, LinkedIn Expertise Search introduces a hybrid approach based on user-generated data (e.g., member profiles), community-based signals (e.g., recommendations and skill endorsements) and personalized signals (e.g., social connection between searcher and results).<ref name=":0">{{Cite journal|last=Ha-Thuc|first=Viet|last2=Venkataraman|first2=Ganesh|last3=Rodriguez|first3=Mario|last4=Sinha|first4=Shakti|last5=Sundaram|first5=Senthil|last6=Guo|first6=Lin|date=2016-02-15|title=Personalized Expertise Search at LinkedIn|url=http://arxiv.org/abs/1602.04572|journal=arXiv:1602.04572 [cs]}}</ref> Given required [[LinkedIn#Skills|skills]] and other types of information need like location and industries, the system allows recruiters to search for hiring candidates amongst more than 450 million LinkedIn members.\n\nExamples of the systems outlined above are listed in Table 1.\n\n\'\'\'Table 1: A classification of expertise location systems\'\'\'\n\n{| class="wikitable" border="1"\n|-\n! Type\n! Application domain\n! Data source\n! Examples\n|-\n| Social networking\n| Professional networking\n| User-generated and community-generated\n|\n* [[LinkedIn]] <ref name=":0" />\n|-\n| [[Scientific literature]]\n| Identifying publications with strongest research impact\n| Third-party generated\n|\n* [[Science Citation Index]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index]\n|-\n| [[Scientific literature]]\n| Expertise search\n| Software\n|\n* [[Arnetminer]][http://arnetminer.org]\n|-\n| Knowledge base\n| Private expertise database\n| User-Generated\n|\n* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)\n* MIT ExpertFinder (ref. 3)\n* Decisiv Search Matters & Expertise ([[Recommind (software company)|Recommind]], Inc.)\n* [http://www.profinda.com ProFinda] (ProFinda Ltd)\n* [https://skillhive.com Skillhive] (Intunex)\n* [[Tacit Software]] (Oracle Corporation)\n* [http://www.guruscan.nl GuruScan] (GuruScan Social Expert Guide)\n|-\n| Knowledge base\n| Publicly accessible expertise database\n| User-generated\n|\n* [http://expertisefinder.com/ Expertise Finder]<ref>http://expertisefinder.com/</ref>\n* [[Community of Science]] Expertise [http://expertise.cos.com]\n* [[ResearcherID]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/scientific/ResearcherID]\n|-\n| Knowledge base\n| Private expertise database\n| Third party-generated\n|\n* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)\n* MIT ExpertFinder (ref. 3)\n* MindServer Expertise ([[Recommind]], Inc.)\n* Tacit Software\n|-\n| Knowledge base\n| Publicly accessible expertise database\n| Third party-generated\n|\n* [http://researchscorecard.com ResearchScorecard] (ResearchScorecard Inc.)\n* [http://authoratory.com/ authoratory.com]\n* [http://biomedexperts.com BiomedExperts] (Collexis Holdings Inc.)\n* [http://www.hcarknowledgemesh.com/ KnowledgeMesh] (Hershey Center for Applied Research)\n* [http://med.stanford.edu/profiles/ Community Academic Profiles] (Stanford School of Medicine)\n* [https://web.archive.org/web/20081120175851/http://www.researchcrossroads.org/ ResearchCrossroads.org]  (Innolyst, Inc.)\n|-\n| Blog [[search engine]]s\n|\n| Third party-generated\n|\n* [[Technorati]] [http://technorati.com/]\n|}\n\n== Technical problems ==\nA number of interesting problems follow from the use of expertise finding systems:\n\n* The matching of questions from non-expert to the database of existing expertise is inherently difficult, especially when the database does not store the requisite expertise.  This problem grows even more acute with increasing ignorance on the part of the non-expert due to typical search problems involving use of keywords to search unstructured data that are not semantically normalized, as well as variability in how well an expert has set up their descriptive content pages.  Improved question matching is one reason why third-party semantically normalized systems such as [http://researchscorecard.com ResearchScorecard] and [[BiomedExperts]] should be able to provide better answers to queries from non-expert users.\n* Avoiding expert-fatigue due to too many questions/requests from users of the system (ref. 1).\n* Finding ways to avoid “gaming” of the system to reap unjustified expertise [[credibility]].\n* Infer expertise on implicit skills. Since users typically do not declare all of the skills they have, it is important to infer their implicit skills that are highly related their explicit ones. The inference step can significantly improve [[Precision and recall|recall]] in expertise finding.\n\n== Expertise ranking ==\n\nMeans of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful.  This raises the following social problems associated with such systems:\n\n* How can expertise be assessed objectively? Is that even possible?\n* What are the consequences of relying on unstructured social assessments of expertise, such as user recommendations?\n* How does one distinguish [[Authority|\'\'authoritativeness\'\']] as a proxy metric of expertise from simple \'\'popularity\'\', which is often a function of one\'s ability to express oneself coupled with a good social sense?\n* What are the potential consequences of the social or professional stigma associated with the use of an authority ranking, such as used in [http://technorati.com Technorati] and [http://researchscorecard.com ResearchScorecard])?\n* How to make expertise ranking personalized to each individual searcher? This is particularly important for recruiting purpose since given the same skills, recruiters from different companies, industries, locations might have different preferences on candidates <ref name=":0" />\n\n== Sources of data for assessing expertise ==\nMany types of data sources have been used to infer expertise.  They can be broadly categorized based on whether they measure "raw" contributions provided by the expert, or whether some sort of filter is applied to these contributions.\n\nUnfiltered data sources that have been used to assess expertise, in no particular ranking order:\n\n* user recommendations\n* help desk tickets: what the problem was and who fixed it\n* e-mail traffic between users\n* documents, whether private or on the web, particularly publications\n* user-maintained web pages\n* reports (technical, marketing, etc.)\n\nFiltered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:\n\n* [[patent]]s, particularly if issued\n* scientific publications\n* issued grants (failed grant proposals are rarely know beyond the authors)\n* [[clinical trial]]s\n* product launches\n* pharmaceutical drugs\n\n== Approaches for creating expertise content ==\n* Manual, either by experts themselves (e.g., [https://skillhive.com Skillhive]) or by a curator ([http://expertisefinder.com/ Expertise Finder])\n* Automated, e.g., using [[software agent]]s (e.g., MIT\'s [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] and the [http://wiki.foaf-project.org/ExpertFinder ExpertFinder] initiative) or a combination of agents and human curation (e.g., [http://researchscorecard.com/ ResearchScorecard] )\n* In industrial expertise search engines (e.g., LinkedIn), there are many signals coming into the ranking functions, such as, user-generated content (e.g., profiles), community-generated content (e.g., recommendations and skills endorsements) and personalized signals (e.g., social connections). Moreover, user queries might contain many other aspects rather required expertise, such as, locations, industries or companies. Thus, traditional [[information retrieval]] features like text matching are also important. [[Learning to rank]] is typically used to combine all of these signals together into a ranking function <ref name=":0" />\n\n== Interesting expertise systems over the years ==\nIn no particular order...\n\n* [http://www.guruscan.nl/ GuruScan]\n* Autonomy\'s IDOL\n* AskMe\n* [http://expertisefinder.com/ Expertise Finder]\n* Tacit Knowledge Systems\' ActiveNet\n* Triviumsoft\'s SEE-K\n* MIT’s [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] (ref 3)\n* MITRE’s (ref 1) [http://www.mitre.org/news/the_edge/june_98/third.html Expert Finder]\n* MITRE’s XpertNet\n* Arnetminer (ref 2)\n* Dataware II Knowledge Directory\n* Thomson’s tool\n* Hewlett-Packard’s CONNEX\n* Microsoft’s SPUD project\n* [http://www.profinda.com ProFinda]\n* [http://www.xperscore.com Xperscore]\n* [http://intunex.fi/skillhive/ Skillhive]\n* LinkedIn<ref name=":0" />\n\n== Conferences ==\n# [http://expertfinder.info/pickme2008 The ExpertFinder Initiative]\n\n== References ==\n{{Reflist}}\n\n# Ackerman, Mark and McDonald, David (1998) "Just Talk to Me: A Field Study of Expertise Location" \'\'Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work\'\'.\n# Hughes, Gareth and Crowder, Richard (2003) "Experiences in designing highly adaptable expertise finder systems"  \'\'Proceedings of the DETC Conference 2003\'\'.\n# Maybury, M., D’Amore, R., House, D. (2002). "Awareness of organizational expertise." \'\'International Journal of Human-Computer Interaction\'\' \'\'\'14\'\'\'(2): 199-217.\n# Maybury, M., D’Amore, R., House, D. (2000). Automating Expert Finding. \'\'International Journal of Technology Research Management.\'\' 43(6): 12-15.\n# Maybury, M., D’Amore, R, and House, D. December (2001). Expert Finding for Collaborative Virtual Environments.  \'\'Communications of the ACM 14\'\'(12): 55-56. In Ragusa, J. and Bochenek, G. (eds). Special Section on Collaboration Virtual Design Environments.\n# Maybury, M., D’Amore, R. and House, D. (2002). Automated Discovery and Mapping of Expertise.  In Ackerman, M., Cohen, A., Pipek, V. and Wulf, V. (eds.). \'\'Beyond Knowledge Management: Sharing Expertise.\'\' Cambridge: MIT Press.\n# Mattox, D., M. Maybury, \'\'et al.\'\' (1999). "Enterprise expert and knowledge discovery". \'\'Proceedings of the 8th International Conference on Human-Computer Interactions (HCI International 99)\'\', Munich, Germany.\n# Tang, J., Zhang J., Yao L., Li J., Zhang L. and Su Z.(2008) "ArnetMiner: extraction and mining of academic social networks" \'\'Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining\'\'.\n# Viavacqua, A. (1999). "Agents for expertise location". \'\'Proceedings of the 1999 AAAI Spring Symposium on Intelligent Agents in Cyberspace\'\', Stanford, CA.\n\n[[Category:Evaluation methods]]\n[[Category:Metrics]]\n[[Category:Analysis]]\n[[Category:Impact assessment]]\n[[Category:Knowledge sharing]]\n[[Category:Library science]]\n[[Category:Information retrieval genres]]\n[[Category:Science studies]]']
['Multi-document summarization', '6870342', '{{Refimprove|date=January 2016}}\n\'\'\'Multi-document summarization\'\'\' is an automatic procedure aimed at [[information extraction|extraction of information]] from multiple texts written about the same topic. The resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]].\n\n==Key benefits==\nMulti-[[document summarization]] creates information reports that are both concise and comprehensive.\nWith different opinions being put together & outlined, every topic is described from multiple perspectives within a single document.\nWhile the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.\nAutomatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.\n\n==Technological challenges==\nThe multi-document summarization task has turned out to be much more complex than [[automatic summarization|summarizing a single document]], even a very large one. This difficulty arises from inevitable thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and conciseness. Document Understanding Conferences,<ref>{{cite web|url=http://www-nlpir.nist.gov/projects/duc/index.html |title=Document Understanding Conferences |website=Nlpir.nist.gov |date=2014-09-09 |accessdate=2016-01-10}}</ref> conducted annually by [[NIST]], have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge.\n\nAn ideal multi-document summarization system does not simply shorten the source texts but presents information organized around the key aspects to represent a wider diversity of views on the topic. When such quality is achieved, an automatic multi-document summary is perceived more like an overview of a given topic. The latter implies that such text compilations should also meet other basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows:\n*clear structure, including an outline of the main content, from which it is easy to navigate to the full text sections\n*text within sections is divided into meaningful paragraphs\n*gradual transition from more general to more specific thematic aspects\n*good [[readability]]\n\nThe latter point deserves additional note - special care is taken in order to ensure that the automatic overview shows:\n*no paper-unrelated "[[communication noise|information noise]]" from the respective documents (e.g., web pages)\n*no dangling references to what is not mentioned or explained in the overview\n*no text breaks across a sentence\n*no semantic [[Redundancy (information theory)|redundancy]].\n\n==Real-life systems==\nThe multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available.\n* Ultimate Research Assistant<ref>{{cite web|url=http://ultimate-research-assistant.com/ |title=Generate Research Report |publisher=Ultimate Research Assistant |date= |accessdate=2016-01-10}}</ref> - performs text mining on Internet search results to help summarize and organize them and make it easier for the user to perform online research. Specific text mining techniques used by the tool include concept extraction, text summarization, hierarchical concept clustering (e.g., automated taxonomy generation), and various visualization techniques, including tag clouds and mind maps. \n* iResearch Reporter<ref>{{cite web|url=http://www.iresearch-reporter.com/ |title=iResearch Reporter service |website=Iresearch-reporter.com |date= |accessdate=2016-01-10}}</ref> - Commercial Text Extraction and Text Summarization system, free demo site accepts user-entered query, passes it on to Google search engine, retrieves multiple relevant documents, produces categorized, easily  readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, text extraction, extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.\n* Newsblaster<ref>[http://newsblaster.cs.columbia.edu]  {{webarchive |url=https://web.archive.org/web/20130416065538/http://newsblaster.cs.columbia.edu |date=April 16, 2013 }}</ref> is a system that helps users find news that is of the most interest to them. The system automatically collects, clusters, categorizes, and summarizes news from several sites on the web ([[CNN]], [[Reuters]], [[Fox News]], etc.) on a daily basis, and it provides users an interface to browse the results.\n* NewsInEssence<ref>[http://www.newsinessence.com]  {{webarchive |url=https://web.archive.org/web/20110411005726/http://www.newsinessence.com |date=April 11, 2011 }}</ref> may be used to retrieve and summarize a cluster of articles from the web. It can start from a [[Uniform Resource Locator|URL]] and retrieve documents that are similar, or it can retrieve documents that match a given set of keywords. NewsInEssence also downloads news articles daily and produces news clusters from them.\n* NewsFeed Researcher<ref>{{cite web|url=http://newsfeedresearcher.com |title=News Feed Researcher &#124; General Stuff |website=Newsfeedresearcher.com |date= |accessdate=2016-01-10}}</ref> is a news portal performing continuous [[automatic summarization]] of documents initially clustered by the [[news aggregators]] (e.g., [[Google News]]). NewsFeed Researcher is backed by a free online engine covering major events related to business, technology, U.S. and international news. This tool is also available in on-demand mode allowing a user to build a summaries on selected topics.\n* Scrape This<ref>[http://www.scrapethis.com]  {{webarchive |url=https://web.archive.org/web/20090919054723/http://www.scrapethis.com |date=September 19, 2009 }}</ref> is like a search engine, but instead of providing links to the most relevant websites based on a query, it scrapes the pertinent information off of the relevant websites and provides the user with a consolidated multi-document summary, along with dictionary definitions, images, and videos.\n* JistWeb<ref>[http://www.jastatechnologies.com/productList.html]  {{webarchive |url=https://web.archive.org/web/20130529112318/http://www.jastatechnologies.com/productList.html |date=May 29, 2013 }}</ref> is a query specific multiple document summariser.\n* The Simplish Simplifying & Summarizing tool<ref>{{cite web|url=http://simplish.org/ |title=Simplish Basic english Tool |publisher=The Goodwill Consortium |date= |accessdate=2016-02-12}}</ref> - performs automatic multi-lingual multi-document summarization. This tool does not need training of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary). \n\nAs auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face [[copyright]] issues in relation to the [[fair use]] copyright concept.\n\n==Bibliography==\n* Günes Erkan and Dragomir R. Radev. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 2004. [http://clair.si.umich.edu/~radev/papers/lprj.pdf]\n* Dragomir R. Radev, Hongyan Jing, Malgorzata Styś, and Daniel Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919–938, December 2004. [http://clair.si.umich.edu/~radev/papers/centroid.pdf]\n* Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. In Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR\'95, pages 74–82, Seattle, Washington, July 1995. [http://clair.si.umich.edu/~radev/papers/sigir95.pdf]\n* C.-Y. Lin, E. Hovy, "From single to multi-document summarization: A prototype system and its evaluation", In "Proceedings of the ACL", pp.&nbsp;457–464, 2002\n*Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, Julia Hirschberg, "Do Summaries Help? A Task-Based Evaluation of Multi-Document Summarization", SIGIR’05, Salvador, Brazil, August 15–19, 2005 [http://www.cs.columbia.edu/~ani/papers/f98-mckeown.pdf]\n*R. Barzilay, N. Elhadad, K. R. McKeown, "Inferring strategies for sentence ordering in multidocument news summarization", Journal of Artificial Intelligence Research, v. 17, pp.&nbsp;35–55, 2002\n*M. Soubbotin, S. Soubbotin, "Trade-Off Between Factors Influencing Quality of the Summary", Document Understanding Workshop (DUC), Vancouver, B.C., Canada, October 9–10, 2005 [http://duc.nist.gov/pubs/2005papers/freetext.sergei.pdf]\n* C Ravindranath Chowdary, and P. Sreenivasa Kumar. "Esum: an efficient system for query-specific multi-document summarization." In ECIR (Advances in Information Retrieval), pp.&nbsp;724–728. Springer Berlin Heidelberg, 2009.\n\n==See also==\n* [[Automatic summarization]]\n* [[Text mining]]\n* [[News aggregators]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www-nlpir.nist.gov/projects/duc/index.html Document Understanding Conferences]\n*[http://www1.cs.columbia.edu/nlp/projects.html Columbia NLP Projects]\n*[http://lada.si.umich.edu:8080/clair/nie1/nie.cgi NewsInEssence: Web-based News Summarization]\n\n{{Natural Language Processing}}\n\n{{DEFAULTSORT:Multi-Document Summarization}}\n[[Category:Natural language processing]]\n[[Category:Information retrieval genres]]']
['Gerard Salton', '509624', '{{Infobox scientist\n| name              = Gerard Salton\n| birth_date        = {{birth date|1927|03|08}}\n| birth_place       = [[Nuremberg]]\n| death_date        = {{death date and age|1995|08|28 |1927|03|08}}\n| death_place       = \n| fields            = [[information retrieval]]\n| workplaces        = [[Cornell University]]\n| alma_mater        = [[Harvard University]]\n| thesis_title      = An automatic data processing system for public utility revenue accounting\n| thesis_url        = http://hollis.harvard.edu/?itemid=%7Clibrary/m/aleph%7C003918090\n| thesis_year       = 1958\n| doctoral_advisor  = [[Howard Aiken]]\n| doctoral_students = [[Amit Singhal]]\n| known_for         = the father of information retrieval<ref name=father-IR /> <br> [[Gerard Salton Award]]\n}}\n\'\'\'Gerard A. "Gerry" Salton\'\'\' (8 March 1927 in [[Nuremberg]] – 28 August 1995), was a Professor of [[Computer Science]] at [[Cornell University]].  Salton was perhaps the leading computer scientist working in the field of [[information retrieval]] during his time, and "the father of information retrieval".<ref name=father-IR>{{cite web |url=http://www.cs.cornell.edu/gries/40brochure/pg24_25.pdf |title=The father of information retrieval |last1= |first1= |last2= |first2= |date= |website=cs.cornell.edu |publisher= |quote= a founding member of the department and the father of information retrieval. |access-date=10 March 2015}}</ref>  His group at Cornell developed the [[SMART Information Retrieval System]], which he initiated when he was at Harvard.\n\nSalton was born Gerhard Anton Sahlmann on March 8, 1927 in [[Nuremberg, Germany]].  He received a Bachelor\'s (1950) and Master\'s (1952) degree in mathematics from [[Brooklyn College]], and a Ph.D. from [[Harvard University|Harvard]] in [[Applied Mathematics]] in 1958, the last of [[Howard Aiken]]\'s doctoral students, and taught there until 1965, when he joined [[Cornell University]] and co-founded its department of Computer Science.\n\nSalton was perhaps most well known for developing the now widely used [[vector space model]] for Information Retrieval.<ref>{{Cite journal | last1 = Salton | first1 = G. | authorlink1 = Gerard Salton| last2 = Wong | first2 = A. | last3 = Yang | first3 = C. S. | doi = 10.1145/361219.361220 | title = A vector space model for automatic indexing | journal = Communications of the ACM | volume = 18 | issue = 11 | pages = 613 | year = 1975 | pmid =  | pmc = }}</ref>  In this model, both documents and queries are represented as vectors of term counts, and the similarity between a document and a query is given by the cosine between the term vector and the document vector.  In this paper, he also introduced [[TF-IDF]], or term-frequency-inverse-document frequency, a model in which the score of a term in a document is the ratio of the number of terms in that document divided by the frequency of the number of documents in which that term occurs. (The concept of inverse document frequency, a measure of specificity, had been introduced in 1972 by [[Karen Spärck Jones|Karen Sparck-Jones]].<ref>{{Cite journal | last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| doi = 10.1108/eb026526 | title = A Statistical Interpretation of Term Specificity and Its Application in Retrieval | journal = Journal of Documentation | volume = 28 | pages = 11–21 | year = 1972 | url = http://www.emeraldinsight.com/doi/abs/10.1108/eb026526| pmid =  | pmc = }}</ref>) Later in life, he became interested in automatic text summarization and analysis,<ref>{{Cite journal | last1 = Salton | first1 = G. | authorlink1 = Gerard Salton| last2 = Allan | first2 = J. | last3 = Buckley | first3 = C. | last4 = Singhal | first4 = A. | title = Automatic Analysis, Theme Generation, and Summarization of Machine-Readable Texts | doi = 10.1126/science.264.5164.1421 | journal = Science | volume = 264 | issue = 5164 | pages = 1421–1426 | year = 1994 | pmid =  17838425| pmc = }}</ref> as well as automatic hypertext generation.<ref>{{cite web|url=http://www.cs.cornell.edu/Info/Department/Annual95/Faculty/Salton.html |title=Gerard Salton |publisher=Cs.cornell.edu |date= |accessdate=2013-09-14}}</ref>  He published over 150 research articles and 5 books during his life.\n\nSalton was editor-in-chief of the [[Communications of the ACM]] and the [[Journal of the ACM]], and chaired [[Special Interest Group on Information Retrieval]] (SIGIR).  He was an associate editor of the [[ACM Transactions on Information Systems]]. He was an [[List of Fellows of the Association for Computing Machinery|ACM Fellow]] (elected 1995),<ref name=fellow-acm>{{cite web |url=http://awards.acm.org/award_winners/salton_2316166.cfm |title=Gerard Salton ACM Fellows  1995 |last1= |first1= |last2= |first2= |date= |website=acm.org |publisher= |quote=contributions over 30 years to information organization and retrieval |access-date=10 March 2015}}</ref> received an Award of Merit from the [[American Society for Information Science]] (1989), and was the first recipient of the SIGIR Award for outstanding contributions to study of information retrieval (1983) -- now called the [[Gerard Salton Award]].\n\n==Bibliography==\n*Salton, \'\'Automatic Information Organization and Retrieval\'\', 1968.\n*{{cite book\n | author     = Gerard Salton\n | title      = A Theory of Indexing\n | publisher  = Society for Industrial and Applied Mathematics\n | year       = 1975\n | page      = 56\n}}\n*--- and Michael J. McGill, \'\'Introduction to modern information retrieval\'\', 1983.  ISBN 0-07-054484-0\n*{{cite book\n | author     = Gerard Salton\n | title      = Automatic Text Processing\n | publisher  = Addison-Wesley Publishing Company\n | year       = 1989\n | page      = 530\n | isbn       = 0-201-12227-8\n}}\n*[http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Salton:Gerard.html DBLP Bibliography]\n* G. Salton, A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," \'\'Communications of the ACM\'\', vol. 18, nr. 11, pages 613–620. \'\'(Article in which a vector space model was presented)\'\'\n\n==See also==\n* [[List of pioneers in computer science]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.cs.cornell.edu/Info/Department/Annual96/Beginning/salton.html In Memoriam]\n* [http://blog.tomevslin.com/2006/01/search_down_mem.html Fractals of Change: Search Down Memory Lane]\n* [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] - This 2004 Library Trends paper by David Dubin serves as a historical review of the metamorphosis of the term discrimination value model (TDV) into the vector space model as an information retrieval model (VSM as an IR model). This paper calls into question what the Information Retrieval research community believed Salton\'s vector space model was originally intended to model. What much later became an information retrieval model was originally a data-centric mathematical–computational model used as an explanatory device. In addition, Dubin\'s paper points out that a 1975 Salton paper oft cited does not exist but is probably a combination of two other papers, neither of which actually refers to the VSM as an IR model.\n\n{{Authority control}}\n\n{{DEFAULTSORT:Salton, Gerard}}\n[[Category:1927 births]]\n[[Category:1995 deaths]]\n[[Category:American computer scientists]]\n[[Category:Harvard University alumni]]\n[[Category:Harvard University faculty]]\n[[Category:Cornell University faculty]]\n[[Category:Fellows of the Association for Computing Machinery]]\n[[Category:Guggenheim Fellows]]\n[[Category:Information retrieval researchers]]']
['Susan Dumais', '2232087', '{{ Infobox scientist\n| name              = Susan T. Dumais\n| image             = Susan Dumais.jpg\n| image_size        = 200px\n| caption           = Susan Dumais in 2009 in her office at Microsoft Research.\n| birth_date        = \n| birth_place       = [[Maine]], [[United States|US]]  \n| death_date        = \n| death_place       = \n| nationality       = American\n| fields            = [[Computer Science]]\n| workplaces        = [[Microsoft Research]]\n| alma_mater        = [[Indiana University]] <br />[[Bates College]]\n| doctoral_advisor  = \n| doctoral_students = \n| known_for         = [[Human Computer Interaction]]<br /> [[Information Retrieval]]\n| website           = {{URL|http://research.microsoft.com/~sdumais/}} \n| awards            = ACM-W Athena Lecturer Award (2014)\n}}\n\n\'\'\'Susan Dumais\'\'\' is an American computer scientist who is a leader in the field of information retrieval, and has been a significant contributor to Microsoft\'s search technologies.<ref>{{cite news|title=100 Top Women in Seattle Tech|url=http://www.bizjournals.com/seattle/blog/techflash/2009/05/Top_100_Women_in_Seattle_Tech_44225472.html|accessdate=23 February 2016|newspaper=Puget Sound Business Journal|date=8 May 2009}}</ref>\nAccording to Mary Jane Irwin, who heads the Athena Lecture awards committee, “Her sustained contributions have shaped the thinking and direction of human-computer interaction and information retrieval."<ref>{{cite news|last=Burns|first=Jay|title=Microsoft’s Susan Dumais ’75 Is a Big Reason Why, Computer-Wise, You Find What You Seek|url=https://www.bates.edu/news/2014/05/01/microsoft-susan-dumais-75/|accessdate=23 February 2016|newspaper=Bates News|date=28 October 2015}}</ref>\n\n==Biography==\n\nSusan Dumais is a Distinguished Scientist at Microsoft and deputy managing director of the [[Microsoft Research]] lab in Redmond. She is also an Affiliate Professor at the [[University of Washington Information School]].\n\nBefore joining Microsoft in 1997, Dumais was a researcher at Bellcore (now [[Telcordia Technologies]]), where she and her colleagues conducted research into what is now called the [[vocabulary problem]] in [[information retrieval]].<ref>{{cite journal\n| title=The Vocabulary Problem in Human-System Communication\n| journal=Communications of the ACM\n| author=[[George Furnas|G. W. Furnas]], [[Thomas Landauer|T. K. Landauer]], L. M. Gomez, S. T. Dumais\n| volume = 30\n| pages = 964–971\n| year = 1987\n| url = http://citeseer.ist.psu.edu/furnas87vocabulary.html\n| doi=10.1145/32206.32212\n| issue = 11\n}}</ref> Their study demonstrated, through a variety of experiments, that different people use different vocabulary to describe the same thing, and that even choosing the "best" term to describe something is not enough for others to find it.  One implication of this work is that because the author of a document may use different vocabulary than someone searching for the document, traditional [[information retrieval]] methods will have limited success.\n\nDumais and the other Bellcore researchers then began investigating ways to build search systems that avoided the vocabulary problem.  The result was their invention of [[Latent Semantic Indexing]].<ref>\n {{cite journal\n | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf \n | title=Indexing by Latent Semantic Analysis\n | author=[[Scott Deerwester|S. Deerwester]], Susan Dumais, [[George Furnas|G. W. Furnas]], [[Thomas Landauer|T. K. Landauer]], [[Richard Harshman|R. Harshman]]\n | journal=Journal of the American Society for Information Science\n | volume=41\n | issue=6\n | pages=391–407\n |year=1990 \n | doi=10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9\n}}</ref>\n\n==Awards==\n\nIn 2006, Dumais was inducted as a [[Fellow]] of the [[Association for Computing Machinery]]. In 2009, she received the [[Gerard Salton Award]], an information retrieval lifetime achievement award. In 2011, she was inducted to the [[National Academy of Engineering]] for innovation and leadership in organizing, accessing, and interacting with information. In 2014, Dumais received the Athena Lecturer Award for "fundamental contributions to computer science.".<ref>{{cite news|last=Knies|first=Rob|title=Dumais Receives Athena Lecturer Award|url=http://blogs.technet.com/b/inside_microsoft_research/archive/2014/04/08/dumais-receives-athena-lecturer-award.aspx|accessdate=28 April 2014|newspaper=Inside Microsoft Research|date=April 2014}}</ref> and the [[Tony Strix]] Award for "sustained contributions that are both innovative and practical" with "significant impact". <ref>{{cite web|title=The winner of the 2014 Tony Kent Strix Award is Dr Susan Dumais|url=http://www.ukeig.org.uk/awards/tony-kent-strix|accessdate=17 September 2014}}</ref>\nIn 2015, she was inducted into the [[American Academy of Arts and Sciences]].<ref>{{cite news|last=Tice|first=Lindsay|title=Lewiston native inducted into American Academy of Arts and Sciences|url=http://www.sunjournal.com/news/lewiston-auburn/0001/11/30/lewiston-native-inducted-american-academy-arts-and-sciences/1808943|accessdate=23 February 2016|newspaper=Lewinston-Auburn Sun-Journal|date=28 October 2015}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://research.microsoft.com/~sdumais/ Home page at Microsoft Research]\n\n{{DEFAULTSORT:Dumais, Susan}}\n[[Category:People in information technology]]\n[[Category:Fellows of the Association for Computing Machinery]]\n[[Category:Microsoft employees]]\n[[Category:Living people]]\n[[Category:Women computer scientists]]\n[[Category:University of Washington faculty]]\n[[Category:Information retrieval researchers]]']
['Meta Content Framework', '1053030', "'''Meta Content Framework''' ('''MCF''') is a specification of a [[content format]] for structuring [[metadata]] about [[web site]]s and other [[data]].\n\n==History==\nMCF was developed by [[Ramanathan V. Guha]] at [[Apple Advanced Technology Group|Apple Computer's Advanced Technology Group]] between 1995 and 1997. Rooted in [[Knowledge representation and reasoning | knowledge-representation]] systems such as [[CycL]], [[KRL (programming language)| KRL]], and [[Knowledge Interchange Format|KIF]], it sought to describe objects, their attributes, and the relationships between them.<ref name=hammersley>{{Cite book| publisher = O'Reilly| isbn = 978-0-596-00383-8| last = Hammersley| first = Ben| title = Content Syndication with RSS| location = Sebastopol| date = 2003| page=2}}</ref>\n\nOne application of MCF was [[HotSauce]], also developed by Guha while at Apple. It generated a [[3D computer graphics|3D]] [[visualization (graphic)|visualization]] of a web site's table of contents, based on MCF descriptions. By late 1996, a few hundred sites were creating MCF files and Apple HotSauce allowed users to browse these MCF representations in 3D.<ref name=hammersley />\n\nWhen the research project was discontinued, Guha left Apple for [[Netscape Communications Corporation|Netscape]], where, in collaboration with [[Tim Bray]], he adapted MCF to use [[XML]]<ref>{{Cite conference\n| publisher = W3C\n| last = Guha\n| first = R V\n|author2=Tim Bray\n | title = Meta Content Framework Using XML\n| accessdate = 2014-09-14\n| date = 1997-06-06\n| url = http://www.w3.org/TR/NOTE-MCF-XML/\n}}</ref><ref>{{Cite web|last1=Guha |first1=R.V. |last2=Bray |first2=Tim |title=Meta Content Framework Using XML |work=Netscape |accessdate=2015-12-12 |date=1997-06-13 |url=http://developer.netscape.com/mcf.html |deadurl=yes |archiveurl=https://web.archive.org/web/19970615144715/http://developer.netscape.com/mcf.html |archivedate=June 15, 1997 }}</ref> and created the first version of the [[Resource Description Framework]] (RDF).<ref>{{Cite web|last=Andreessen |first=Marc |title=Innovators of the Net: R.V. Guha and RDF |work=Netscape |accessdate=2014-09-14 |date=1999-01-08 |url=http://wp.netscape.com/columns/techvision/innovators_rg.html |deadurl=yes |archiveurl=https://web.archive.org/web/20080205163659/http://wp.netscape.com/columns/techvision/innovators_rg.html |archivedate=February 5, 2008 }}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.textuality.com/mcf/MCF-tutorial.html MCF Tutorial] (using XML syntax)\n*[http://www.guha.com/mcf/ Guha MCF site]\n*[http://downlode.org/Etext/MCF/towards_a_theory_of_metacontent.html The metacontent concept]\n\n[[Category:Knowledge representation]]\n[[Category:Apple Inc. software]]\n\n{{compu-AI-stub}}"]
['Medical algorithm', '1551981', "{{Expand Russian|Медицинский алгоритм|date=September 2015}}\n{{Original research|date=October 2007}}\n[[File:Assessment and treatment algorithm for overweight and obesity.png|thumb|450px|A medical algorithm for assessment and treatment of [[overweight]] and [[obesity]].]]\nA '''medical algorithm''' is any [[computation]], [[formula]], [[statistical survey]], [[nomogram]], or [[look-up table]], useful in [[healthcare]].  [[Medical]] [[algorithm]]s include [[decision tree]] approaches to healthcare treatment (e.g., if [[symptom]]s A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty.\n\n==Scope==\nMedical algorithms are part of a broader field which is usually fit under the aims of [[medical informatics]] and medical [[decision-making]]. Medical decisions occur in several areas of medical activity including medical test selection, [[diagnosis]], therapy and [[prognosis]], and [[automatic control]] of [[medical equipment]].\n\nIn relation to [[logic]]-based and [[artificial neural network]]-based [[clinical decision support system]], which are also computer applications to the medical decision-making field, algorithms are less complex in architecture, data structure and user interface. Medical algorithms are not necessarily implemented using digital computers. In fact, many of them can be represented on paper, in the form of diagrams, nomographs, etc.\n\n==Examples==\nA wealth of medical information exists in the form of published medical algorithms.  These algorithms range from simple [[calculation]]s to complex outcome [[prediction]]s.  Most [[clinician]]s use only a small subset routinely.\n\nExamples of medical algorithms are:\n* '''[[Calculators]],'''. e.g., an on-line or stand-alone calculator for [[body mass index]] (BMI) when stature and body weight are given;\n* '''[[Flowcharts]],''' e.g., a [[Wiktionary:binary|binary]] [[decision tree]] for deciding what is the [[etiology]] of [[chest pain]]\n* '''[[Look-up table]]s,''' e.g., for looking up [[food energy]] and nutritional contents of foodstuffs\n* '''[[Nomogram]]s,''' e.g., a moving circular slide to calculate body surface area or drug dosages.\n\nA common class of algorithms are embedded in guidelines on the choice of treatments produced by many national, state, financial and local healthcare organisations and provided as knowledge resources for day to day use and for induction of new physicians. A field which has gained particular attention is the choice of medications for psychiatric conditions. In the United Kingdom, guidelines or algorithms for this have been produced by most of the circa 500 primary care trusts, substantially all of the circa 100 secondary care psychiatric units and many of the circa 10 000 general practices. In the US, there is a national (federal) initiative to provide them for all states, and by 2005 six states were adapting the approach of the [[Texas Medication Algorithm Project]] or otherwise working on their production.\n\nA grammar—the [[Arden syntax]]—exists for describing algorithms in terms of [[medical logic module]]s. An approach such as this should allow exchange of MLMs between doctors and establishments, and enrichment of the common stock of tools.\n\n==Purpose==\nThe intended purpose of medical algorithms is to improve and standardize decisions made in the delivery of [[medical care]]. Medical algorithms assist in standardizing selection and application of treatment regimens, with algorithm [[automation]] intended to reduce potential introduction of errors.  Some attempt to predict the outcome, for example [[ICU scoring systems|critical care scoring systems]].\n\nComputerized health diagnostics algorithms can provide timely clinical decision support, improve adherence to [[evidence-based medicine|evidence-based]] [[guideline (medical)|guidelines]], and be a resource for education and research. \n\nMedical algorithms based on best practice can assist everyone involved in delivery of standardized treatment via a wide range of clinical care providers. Many are presented as [[Clinical trial protocol|protocol]]s and it is a key task in training to ensure people step outside the protocol when necessary.  In our present state of knowledge, generating hints and producing guidelines may be less satisfying to the authors, but more appropriate.\n\n==Cautions==\nIn common with most science and medicine, algorithms whose contents are not wholly available for scrutiny and open to improvement should be regarded with suspicion.  \n\n[[Computation]]s obtained from medical algorithms should be compared with, and tempered by, clinical knowledge and [[physician]] judgment.\n\n==See also==\n* [[Consensus (medical)]]\n* [[Evidence-based medicine]]\n* [[Journal club]]\n* [[Medical guideline]]\n* [[Medical informatics]]\n* [[Odds algorithm]]\n* ''[[Treatment Guidelines from The Medical Letter]]''\n\n==Further reading==\n* {{cite journal| title=Automated Medical Algorithms:  Issues for Medical Errors| first1=Kathy A.| last1=Johnson| first2=John R.| last2=Svirbely| first3=M.G.| last3=Sriram| first4=Jack W.| last4=Smith| first5=Gareth |last5=Kantor| first6=Jorge Raul |last6=Rodriguez| journal=[[Journal of the American Medical Informatics Association]]| pmc=419420| doi=10.1197/jamia.M1228| volume=9| issue=6 Suppl 1| pages=s56-s57| date=November 2002}}\n\n==External links==\n\n* [http://www.alternativementalhealth.com/articles/fieldmanual.htm AlternativeMentalHealth.com] - 'Alternative Health Medical Evaluation Field Manual', Lorrin M. Koran, MD, [[Stanford University]] Medical Center (1991)\n\n[[Category:Health informatics]]\n[[Category:Algorithms]]\n[[Category:Knowledge representation]]"]
['CDS ISIS', '2889648', '{{Use dmy dates|date=May 2014}}\n\'\'\'CDS/ISIS\'\'\' is a [[software]] package for generalised \'\'Information Storage and Retrieval systems\'\' developed, maintained and disseminated by [[UNESCO]]. It was first released in 1985 and since then over 20,000 [[license|licences]] have been issued by UNESCO and a worldwide network of distributors. It is particularly suited to bibliographical applications and is used for the [[library catalog|catalogues]] of many small and medium-sized [[library|libraries]]. Versions have been produced in Arabic, Chinese, English, French, German, Portuguese, Russian and Spanish amongst other languages. UNESCO makes the software available free for non-commercial purposes, though distributors are allowed to charge for their expenses.\n\nCDS/ISIS is an acronym which stands for Computerised Documentation Service / Integrated Set of Information Systems. In 2003 it was stated that "This package is accepted by libraries in the developing countries as a standard software for information system development".<ref>National Science Foundation of Sri Lanka. "CDS ISIS Library Software" [Last Update 10 January 2003.] http://www.nsf.ac.lk/slstic/isis.htm  Accessed 20 June 2007.</ref>\n\nThe original CDS/ISIS ran on an [[IBM]] [[mainframe computer|mainframe]] and was designed in the mid-1970s under Mr Giampaolo Del Bigio for UNESCO\'s Computerized Documentation System (CDS). It was based on the internal ISIS (Integrated Set of Information Systems) at the [[International Labour Organization]] in Geneva.\n\nIn 1985 a version was produced for mini- and microcomputers programmed in Pascal. It ran on an [[IBM PC]] under [[MS-DOS]]<ref>Buxton, Andrew and Hopkinson, Alan. \'\'The CDS/ISIS handbook\'\'. London: Library Association, 1994</ref>\n. \'\'Winisis\'\', the [[Microsoft Windows|Windows]] version,  first demonstrated in 1995, may run on a single [[computer]] or in a [[local area network]]. A \'\'JavaISIS\'\' client/server component was designed in 2000, allowing remote [[database management system|database management]] over the [[Internet]] from [[Microsoft Windows|Windows]], [[Linux]] and [[Apple Macintosh|Macintosh]] computers. Furthermore, \'\'GenISIS\'\' allows the user to produce [[HTML]] Web forms for CDS/ISIS database searching. The \'\'ISIS_DLL\'\' provides an [[API]] for developing CDS/ISIS based applications. The [[OpenIsis]] library, developed independently from 2002 to 2004, provided another [[API]] for developing CDS/ISIS-like applications.\n\nThe most recent effort towards a completely renewed [[Free and open-source software|FOSS]], [[Unicode]] implementation of CDS/ISIS is the J-Isis project, developed by UNESCO since 2005 and currently maintained by Mr Jean Claude Dauphin.\n\n== See also ==\n* [[IDIS (software)|IDIS]] is a tool for direct data exchange between CDS/ISIS and IDAMS.\n\n== External links ==\n* [http://kenai.com/projects/j-isis J-ISIS New UNESCO Java CDS/ISIS Software]\n* [http://portal.unesco.org/ci/en/ev.php-URL_ID=2071&URL_DO=DO_TOPIC&URL_SECTION=201.html CDS/ISIS database software (UNESCO)]\n* [http://lists.iccisis.org International list hosted from 2010 by the ICCIsis (International Coordination Committee on ISIS)]\n* [https://listserv.surfnet.nl/archives/cds-isis.html Archives of CDS-ISIS@NIC.SURFNET.NL (discontinued in 2010)]\n* http://openisis.org/ (discontinued)\n* http://sourceforge.net/projects/isis (discontinued)\n* [http://pecl.php.net/package/isis PHP extension for reading CDS/ISIS databases]\n\n== References ==\n{{Reflist}}\n\n[[Category:Knowledge representation]]\n[[Category:Proprietary database management systems]]']
['Conceptual graph', '346755', "'''Conceptual graphs''' ('''CGs''') are a formalism for [[knowledge representation]]. In the first published paper  on CGs, [[John F. Sowa]] {{harv|Sowa|1976}} used them to represent the [[conceptual schema]]s used in [[database system]]s. The first book on CGs {{harv|Sowa|1984}} applied them to a wide range of topics in [[artificial intelligence]], [[computer science]], and [[cognitive science]].\n\nSince 1984, the model has been developed along three main directions.\n\n== A graphical interface for first-order logic ==\nIn this approach, a formula in [[first-order logic]] (predicate calculus) is represented by a labeled graph.\n\nA linear notation, called the '''Conceptual Graph Interchange Format (CGIF)''', has been standardized in the ISO standard for [[common logic]].\n\n[[Image:Cat-on-mat.svg|thumb|250px|Elsie the cat is sitting on a mat]]\nThe diagram on the right is an example of the '''display form''' for a conceptual graph.  Each box is called a '''concept node''', and each oval is called a '''relation node'''.  In CGIF, this CG would be represented by the following statement:\n\n    [Cat Elsie] [Sitting *x] [Mat *y] (agent ?x Elsie) (location ?x ?y)\nIn CGIF, brackets enclose the information inside the concept nodes, and parentheses enclose the information inside the relation nodes.  The letters x and y, which are called '''coreference labels''', show how the concept and relation nodes are connected.  In the '''Common Logic Interchange Format (CLIF)''', those letters are mapped to variables, as in the following statement:\n\n    (exists ((x Sitting) (y Mat)) (and (Cat Elsie) (agent x Elsie) (location x y)))\n\nAs this example shows, the asterisks on the coreference labels *x and *y in CGIF map to existentially quantified variables in CLIF, and the question marks on ?x and ?y map to bound variables in CLIF.  A universal quantifier, represented '''@every*z''' in CGIF, would be represented '''forall (z)''' in CLIF.\n\nReasoning can be done by translating graphs into logical formulas, then applying a logical inference engine.\n\n== Diagrammatic calculus of logics ==\nAnother research branch continues the work on [[existential graph]]s of [[Charles Sanders Peirce]], which were one of the origins of conceptual graphs as proposed by Sowa.\nIn this approach, developed in particular by Dau {{harv|Dau|2003}}, conceptual graphs are conceptual [[diagram]]s rather than graphs in the sense of [[graph theory]], and reasoning operations are performed by operations on these diagrams.\n\n== Graph-based knowledge representation and reasoning model ==\nKey features of '''GBKR''', the graph-based knowledge representation and reasoning model developed by Chein and Mugnier and the Montpellier group {{harv|Chein|Mugnier|2009}},\ncan be summarized as follows:\n\n* all kinds of knowledge (ontology, rules, constraints and facts) are labeled graphs, which provide an intuitive and easily understandable means to represent knowledge,\n* reasoning mechanisms are based on graph notions, basically the classical notion of graph homomorphism; this allows, in particular, to link basic reasoning problems to other fundamental problems in computer science (problems concerning conjunctive queries in relational databases, constraint satisfaction problem, ...),\n* the formalism is logically founded, i.e., it has a semantics in first-order logic and the inference mechanisms are sound and complete with respect to deduction in first-order logic,\n* from a computational viewpoint, the graph homomorphism notion was recognized in the 1990s as a central notion, and complexity results and efficient algorithms have been obtained in several domains.\n\nCOGITANT and COGUI are tools that implement the '''GBKR''' model. COGITANT is a library of C++ classes that implement most of the GBKR notions and reasoning mechanisms. COGUI  is a graphical user interface dedicated to the construction of a GBKR knowledge base (it integrates COGITANT and, among numerous functionalities, it contains a translator from GBKR to RDF/S and conversely).\n\n== Sentence generalization and generalization diagrams ==\nSentence [[generalization]] and generalization diagrams can be defined as a special sort of conceptual graphs which can be constructed automatically from syntactic [[parse tree]]s and support semantic classification task {{harv|Galitsky et al|2010}}. Similarity measure between syntactic parse trees can be done as a generalization operation on the lists of sub-trees of these trees. The diagrams are representation of mapping between the [[syntax]] generalization level and [[semantics]] generalization level (anti-unification of [[logic forms]]). Generalization diagrams are intended to be more accurate semantic representation than conventional conceptual graphs for individual sentences because only syntactic commonalities are represented at semantic level.\n\n==See also==\n*[[Resource Description Framework]] (RDF)\n*[[SPARQL]] (Graph Query Language)\n*[[Semantic network]]\n*[[Knowledge representation]]\n*[[Chunking (psychology)]]\n*[[Concept map]]\n*[[Conceptual schema]]\n*[[Diagrammatic reasoning]]\n\n==References==\n* {{cite book |last=Chein |first=Michel |last2=Mugnier |first2=Marie-Laure |year=2009 |title=Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs |publisher=Springer |url=http://www.lirmm.fr/gbkrbook/ |isbn=978-1-84800-285-2 |ref=harv | doi=10.1007/978-1-84800-286-9}}\n* {{cite journal |last=Dau |first=F. |year=2003 |title=The Logic System of Concept Graphs with Negation and Its Relationship to Predicate Logic |journal=[[Lecture Notes in Computer Science]] |volume=2892 |publisher=Springer |isbn= |ref=harv }}\n* {{cite journal |last=Sowa |authorlink = John Sowa |first=John F. |date=July 1976 |title=Conceptual Graphs for a Data Base Interface |journal=IBM Journal of Research and Development |volume=20 |issue=4 |pages=336–357 |url=http://www.research.ibm.com/journal/rd/204/ibmrd2004E.pdf |ref=harv |doi=10.1147/rd.204.0336}}\n* {{cite book |last=Sowa |first=John F. |year=1984 |title=Conceptual Structures:  Information Processing in Mind and Machine |location=Reading, MA |publisher=Addison-Wesley |isbn=978-0-201-14472-7 |ref=harv }}\n* {{cite journal | last=Galitsky | first=Boris | last2=Dobrocsi |first2=Gabor | last3=de la Rosa |first3=Josep Lluis | last4=Kuznetsov |first4=Sergei O. |year=2010 |title=From Generalization of Syntactic Parse Trees to Conceptual Graphs |journal=Lecture Notes in Computer Science |volume=6208 |publisher=Springer |isbn= |url=http://dl.acm.org/citation.cfm?id=1881190|ref=harv }}\n* {{cite journal|title=Conceptual graphs for the analysis and generation of sentences\n|first1=Paola |last1=Velardi |first2=Maria Teresa |last2=Pazienza |first3=Mario |last3=De' Giovanetti |journal=IBM Journal of Research and Development |volume=32 |number=2 |date=March 1988 |pages=251–267 |publisher=IBM Corp. Riverton, NJ, USA |doi=10.1147/rd.322.0251}}\n\n==External links==\n* [http://conceptualstructures.org Conceptual Structures Home Page]. (Old site:  [http://conceptualgraphs.org Conceptual Graphs Home Page])\n* [http://www.informatik.uni-trier.de/~ley/db/conf/iccs/index.html Yearly international conferences (ICCS)]\n* [http://www.jfsowa.com/cg/index.htm Conceptual Graphs on John F. Sowa's Website]\n\n[[Category:Knowledge representation]]\n[[Category:Diagrams]]\n[[Category:Application-specific graphs]]"]
['National Library of Medicine classification', '4165078', "The '''National Library of Medicine (NLM) classification system''' is a [[Library classification|library indexing system]] covering the fields of [[medicine]] and preclinical basic sciences. The [[National Library of Medicine|NLM]] classification is patterned after the [[Library of Congress Classification|Library of Congress (LC) Classification system]]: [[Alphabet|alphabetical letters ]]  denote broad subject categories which are subdivided by numbers.<ref name=NLM_Factsheet>{{cite web | title =Fact Sheet: NLM Classification | work = | url = https://www.nlm.nih.gov/pubs/factsheets/nlmclassif.html | date = 2005-07-15 | accessdate = 2007-05-12}}</ref> For example, ''QW 279'' would indicate a book on an aspect of [[microbiology]] or [[immunology]].\n\nThe one- or two-letter alphabetical codes in the NLM classification use a limited range of letters: only QS–QZ and W–WZ. This allows the NLM system to co-exist with the larger LC coding scheme as neither of these ranges are used in the LC system. There are, however, three pre-existing codes in the LC system which overlap with the NLM: ''Human Anatomy'' (QM), ''Microbiology'' (QR), and ''Medicine'' (R). To avoid further confusion, these three codes are not used in the NLM.\n\nThe headings for the individual ''schedules'' (letters or letter pairs) are given in brief form (e.g., QW - ''Microbiology and Immunology''; WG - ''Cardiovascular System'') and together they provide an outline of the subjects covered by the NLM classification. Headings are interpreted broadly and include the [[Physiology|physiological]] system, the specialties connected with them, the regions of the body chiefly concerned and subordinate related fields. The NLM system is [[hierarchical]], and within each schedule, division by [[Organ (anatomy)|organ]] usually has priority. Each main schedule, as well as some sub-sections, begins with a group of form numbers ranging generally from 1–49 which  classify materials by publication type, e.g., [[Dictionary|dictionaries]], [[atlas]]es, laboratory manuals, etc.\n\nThe main schedules QS-QZ, W-WY, and WZ (excluding the range WZ 220–270)  classify works published after 1913; the 19th century schedule is used for works published 1801-1913; and WZ 220-270 is used to provide century groupings for works published before 1801.\n\n==Overview of the NLM Classification categories==\n\n'''Preclinical Sciences'''\n* QS Human Anatomy\n* QT Physiology\n* QU Biochemistry\n* QV Pharmacology\n* QW Microbiology & Immunology\n* QX Parasitology\n* QY Clinical Pathology\n* QZ Pathology\n\n'''Medicine and Related Subjects'''\n\n* W  Health Professions\n* WA Public Health\n* WB Practice of Medicine\n* WC Communicable Diseases\n* WD Disorders of Systemic, Metabolic, or Environmental Origin, etc.\n* WE Musculoskeletal System\n* WF Respiratory System\n* WG Cardiovascular System\n* WH Hemic and Lymphatic Systems\n* WI Digestive System\n* WJ Urogenital System\n* WK Endocrine System\n* WL Nervous System\n* WM Psychiatry\n* WN Radiology. Diagnostic Imaging\n* WO Surgery\n* WP Gynecology\n* WQ Obstetrics\n* WR Dermatology\n* WS Pediatrics\n* WT Geriatrics. Chronic Disease\n* WU Dentistry. Oral Surgery\n* WV Otolaryngology\n* WW Ophthalmology\n* WX Hospitals & Other Health Facilities\n* WY Nursing\n* WZ History of Medicine\n* 19th Century Schedule\n\n==See also==\n*[[Dewey Decimal Classification]]\n*[[Colon Classification]]\n*[[Library of Congress Classification]]\n*[[Universal Decimal Classification]]\n\n==References==\n<!-- ---------------------------------------------------------------\nSee http://en.wikipedia.org/wiki/Wikipedia:Footnotes for a\ndiscussion of different citation methods and how to generate\nfootnotes using the<ref> & </ref>  tags and the {{Reflist}} template\n-------------------------------------------------------------------- -->\n{{Reflist}}\n\n{{refbegin}}\n* {{USGovernment|sourceURL=[http://wwwcf.nlm.nih.gov/class/ The NLM Classification 2005]}}\n{{refend}}\n\n[[Category:Classification systems]]\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]"]
['Category:Multi-agent systems', '8050180', 'This [[Wikipedia:category|category]] is about [[multi-agent system]]s, systems composed of several [[software agent]]s. \n\n\n[[Category:Computing platforms]]\n[[Category:Artificial intelligence]]\n[[Category:Knowledge representation]]\n[[Category:Distributed computing architecture]]\n\n== See also ==\n*[[Cognitive architecture]]\n*[[Intelligent agent]]\n*[[Autonomous agent]]\n*[[Internet bot]]\n*[[Daemon (computer software)|Daemon]]']
['Is-a', '294441', 'In [[knowledge representation]], [[object-oriented programming]] and [[Object-oriented design|design]] (see [[object oriented]] [[program architecture]]), \'\'\'is-a\'\'\' (\'\'\'is_a\'\'\' or \'\'\'is a\'\'\') is a [[wikt:subsume|subsumption]]<ref>See [[Liskov substitution principle]].</ref> relationship between [[abstractions]] (e.g. [[type (disambiguation)|types]], [[class (disambiguation)|classes]]), where one [[Class (computer programming)|class]] \'\'A\'\' is a [[subclass (disambiguation)|subclass]] <!-- This deliberately links to the disambiguation page --> of another class \'\'B\'\' (and so \'\'B\'\' is a [[superclass (disambiguation)|superclass]] <!--This deliberately links to the disambiguation page--> of \'\'A\'\').\nIn other words, type A is a subtype of type B when A’s [[Formal specification|speciﬁcation]] implies B’s speciﬁcation. That is, any object (or class) that satisﬁes A’s speciﬁcation also satisﬁes B’s speciﬁcation, because B’s speciﬁcation is weaker.<ref>{{cite web|title=Subtypes and Subclasses|url=http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-170-laboratory-in-software-engineering-fall-2005/lecture-notes/lec14.pdf|publisher=MIT OCW|accessdate=2 October 2012}}</ref>\n\nThe \'\'is-a\'\' relationship is to be contrasted with the \'\'[[has-a]]\'\' (\'\'has_a\'\' or \'\'has a\'\') relationship between types (classes); confusing the relations \'\'has-a\'\' and \'\'is-a\'\' is a common error when designing a model (e.g., a [[computer program]]) of the real-world relationship between an object and its subordinate. The \'\'is-a\'\' relationship may also be contrasted with the \'\'[[Typeof|instance-of]]\'\' relationship between objects (instances) and types (classes): see "[[type-token distinction]]" and "[[type-token relations]]."<ref>[[Type–token relations]]</ref> \n\nTo summarize the relations, we have\n\n* [[hyperonym]]-[[hyponym]] (supertype-subtype) relations between types (classes) defining a taxonomic hierarchy, where\n** for a [[Inheritance (object-oriented programming)|subsumption]] relation: a hyponym (subtype, subclass) has a \'\'type-of\'\' (\'\'is-a\'\') relationship with its hypernym (supertype, superclass);\n* [[holonym]]-[[meronym]] (whole/entity/container-part/constituent/member) relations between types (classes) defining a possessive hierarchy, where \n** for an [[Aggregation (object-oriented programming)|aggregation]] (i.e. without ownership) relation: \n*** a holonym (whole) has a \'\'has-a\'\' relationship with its meronym (part),\n** for a [[Composition (object-oriented programming)|composition]] (i.e. with ownership) relation: \n*** a meronym (constituent) has a \'\'[[part-of]]\'\' relationship with its holonym (entity),\n** for a [[Object composition#Containment|containment]]<ref>See also [[Containment (computer programming)]].</ref> relation:\n*** a meronym (member) has a \'\'[[member-of]]\'\' relationship with its holonym ([[Container (abstract data type)|container]]);\n* concept-object (type-token) relations between types (classes) and objects (instances), where\n** a token (object) has an \'\'[[Instance (computer science)|instance-of]]\'\' relationship with its type (class).\n\n==Examples of subtyping==\n\n[[Subtype polymorphism|Subtyping]] enables a given type to be substituted for another type or abstraction. Subtyping is said to establish an \'\'\'is-a\'\'\' relationship between the subtype and some existing abstraction, either implicitly or explicitly, depending on language support. The relationship can be expressed explicitly via inheritance in languages that support inheritance as a subtyping mechanism.\n\n===C++===\nThe following C++ code establishes an explicit inheritance relationship between classes \'\'\'B\'\'\' and \'\'\'A\'\'\', where \'\'\'B\'\'\' is both a subclass and a subtype of \'\'\'A\'\'\', and can be used as an \'\'\'A\'\'\' wherever a \'\'\'B\'\'\' is specified (via a reference, a pointer or the object itself).\n\n<source lang=cpp>class A \n{ public:\n   void DoSomethingALike() const {}\n};\n\nclass B : public A \n{ public:\n   void DoSomethingBLike() const {}\n};\n\nvoid UseAnA(A const& some_A)\n{\n   some_A.DoSomethingALike();\n}\n\nvoid SomeFunc()\n{\n   B b;\n   UseAnA(b); // b can be substituted for an A.\n}\n</source><ref name="Mitchell2002">\n{{cite book\n | last=Mitchell\n | first=John\n | authorlink=John C. Mitchell\n | title=Concepts in programming language\n | year=2002\n | publisher=Cambridge University Press\n | location=Cambridge, UK\n | isbn=0-521-78098-5\n | page=287\n | chapter=10 "Concepts in object-oriented languages"}}\n</ref>\n\n===Python===\nThe following python code establishes an explicit inheritance relationship between classes \'\'\'B\'\'\' and \'\'\'A\'\'\', where \'\'\'B\'\'\' is both a subclass and a subtype of \'\'\'A\'\'\', and can be used as an \'\'\'A\'\'\' wherever a \'\'\'B\'\'\' is required.\n\n<source lang=python>class A:\n    def doSomethingALike(self):\n        pass\n\nclass B(A):\n    def doSomethingBLike(self):\n        pass\n\ndef useAnA(some_A):\n    some_A.doSomethingALike()\n\ndef someFunc():\n    b = B();\n    useAnA(b)  # b can be substituted for an A.\n</source>\n\nThe following example, type(a) is a "regular" type, and type(type(a)) is a metatype. While as distributed all types have the same metatype (PyType_Type, which is also its own metatype), this is not a requirement. The type of classic classes, known as types.ClassType, can also be considered a distinct metatype.<ref>{{cite web|last=Guido van Rossum|title=Subtyping Built-in Types|url=https://www.python.org/dev/peps/pep-0253/|accessdate=2 October 2012}}</ref>\n\n<source lang=python>\n>>> a = 0\n>>> type(a)\n<type \'int\'>\n>>> type(type(a))\n<type \'type\'>\n>>> type(type(type(a)))\n<type \'type\'>\n>>> type(type(type(type(a))))\n<type \'type\'>\n</source>\n\n===Java===\n\nIn Java, \'\'\'is-a\'\'\' relation between the type parameters of one class or interface and the type parameters of another are determined by the extends and [[Interface (Java)|implements]] clauses.\n\nUsing the Collections classes, ArrayList<E> implements List<E>, and List<E> extends Collection<E>. So ArrayList<String> is a subtype of List<String>, which is a subtype of Collection<String>. The subtyping relationship is preserved between the types automatically. When we define an interface, PayloadList, that associates an optional value of generic type P with each element. Its declaration might look like:\n\n<source lang=java>\ninterface PayloadList<E, P> extends List<E> {\n    void setPayload(int index, P val);\n    ...\n}\n</source>\n\nThe following parameterizations of PayloadList are subtypes of List<String>:\n\n<source lang=java>\nPayloadList<String, String>\nPayloadList<String, Integer>\nPayloadList<String, Exception>\n</source>\n\n==Liskov substitution principle==\n{{main|Liskov substitution principle}}\nLiskov substitution principle explains a property, \'\'"If for each object o1 of type S there is an object o2 of type T such that for all programs P deﬁned in terms of T, the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T,"\'\'.<ref>{{cite book|last=Liskov|first=Barbara|title=Data Abstraction and Hierarchy|date=May 1988|publisher=SIGPLAN Notices}}</ref> Following example shows a violation of LSP.\n\n<source lang=cpp>void DrawShape(const Shape& s)\n{\n  if (typeid(s) == typeid(Square))\n    DrawSquare(static_cast<Square&>(s));\n  else if (typeid(s) == typeid(Circle))\n    DrawCircle(static_cast<Circle&>(s));\n}</source>\nObviously, the DrawShape function is badly formatted. It has to know about every derivative classes of Shape class. Also, it should be changed whenever new subclass of Shape are created. In [[Object-oriented design|Object Oriented Design]], many view the structure of this as anathema.\n\nHere is a more subtle example of violation of LSP\n\n<source lang=cpp>\nclass Rectangle\n{\n  public:\n    void   SetWidth(double w)  { itsWidth = w; }\n    void   SetHeight(double h) { itsHeight = h; }\n    double GetHeight() const   { return itsHeight; }\n    double GetWidth() const    { return itsWidth; }\n  private:\n    double itsWidth;\n    double itsHeight;\n};\n</source>\nThis works well but when it comes to Square class, which inherits Rectangle class, it violates LSP even though the \'\'\'is-a\'\'\' relationship holds between Rectangle and Square. Because square is rectangular. The following example overrides two functions, Setwidth and SetHeight, to fix the problem. But fixing the code implies that the design is faulty.\n\n<source lang=cpp>\npublic class Square : Rectangle\n{\n  public:\n    virtual void SetWidth(double w);\n    virtual void SetHeight(double h);\n};\nvoid Square::SetWidth(double w)\n{\n    Rectangle::SetWidth(w);\n    Rectangle::SetHeight(w);\n}\nvoid Square::SetHeight(double h)\n{\n    Rectangle::SetHeight(h);\n    Rectangle::SetWidth(h);\n}\n</source>\n\nThe following example, function g just works for Rectangle class but not for Square, and so the open-closed principle has been violated.\n\n<source lang=cpp>\nvoid g(Rectangle& r)\n{\n  r.SetWidth(5);\n  r.SetHeight(4);\n  assert(r.GetWidth() * r.GetHeight()) == 20);\n}\n</source>\n<ref>{{cite web|title=The Liskov Substitution Principle|url=http://www.objectmentor.com/resources/articles/lsp.pdf|publisher=Robert C. Martin, 1996|accessdate=2 October 2012}}</ref>\n\n== See also ==\n\n* [[Inheritance (object-oriented programming)]]\n* [[Liskov substitution principle]] (in [[object-oriented programming]])\n* [[Subsumption (disambiguation)|Subsumption]]<!--This deliberately links to the disambiguation page-->\n* Is-a\n** [[Hypernymy]] (and [[supertype]])\n** [[Hyponymy]] (and [[subtype]])\n* [[Has-a]]\n** [[Holonymy]]\n** [[Meronymy]]\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n* [[Ronald J. Brachman]]; [http://dblp.uni-trier.de/rec/bibtex/journals/computer/Brachman83 What IS-A is and isn\'t. An Analysis of Taxonomic Links in Semantic Networks]. IEEE Computer, 16 (10); October 1983\n* Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: [http://www.informatik.uni-trier.de/~ley/db/conf/er/HainautHEHR96.html Understanding Implementations of IS-A Relations]. ER 1996: 42-57\n\n[[Category:Object-oriented programming]]\n[[Category:Knowledge representation]]\n[[Category:Abstraction]]\n[[Category:Articles with example Java code]]']
['Fuzzy cognitive map', '11270885', '[[File:FCMdrug520.png|thumb|right|Rod Tabers FCM depicting eleven factors of the American drug market]]\nA \'\'\'fuzzy cognitive map\'\'\' is a [[cognitive map]] within which the relations between the elements (e.g. concepts, events, project resources) of a "mental landscape" can be used to compute the "strength of impact" of these elements.  Fuzzy cognitive maps were introduced by [[Bart Kosko]].<ref>{{cite journal|author=[[Bart Kosko]]|title=\'\'Fuzzy Cognitive Maps\'\'|journal=International Journal of Man-Machine Studies|volume=24|date=1986|pages=65-75|url=http://sipi.usc.edu/~kosko/FCM.pdf|format=PDF}}</ref><ref>[http://sipi.usc.edu/~kosko/Virtual_Worlds_FCM.pdf] {{dead link|date=January 2017}}</ref>  Ron Axelord introduced Cognitive Maps as a formal way of representing social scientific knowledge and modeling [[decision making]] in social and political systems. Then brought in the computation [[fuzzy logic]].\n\n==Details==\nFuzzy cognitive maps are signed fuzzy [[directed graph|digraph]]s.  They may look at [[first blush]] like [[Hasse diagrams]] but they are not.\n[[Spreadsheet]]s or tables are used to map FCMs into [[matrix (Mathematics)|matric]]es for further computation.<ref>{{cite web|url=http://www.FCMappers.net/joomla/index.php?option=com_content&view=article&id=52&Itemid=53 |title=FCMapper - our Fuzzy Cognitive Mapping Software Solution |website=Fcmappers.net |date=2016-01-27 |accessdate=2017-01-09}}</ref><ref>{{cite web|url=http://www.ochoadeaspuru.com/fuzcogmap/index.php |title=Fuzzy Cognitive Maps |website=Ochoadeaspuru.com |date= |accessdate=2017-01-09}}</ref><ref>{{cite web|url=http://jfcm.megadix.it/ |title=JFCM - Java Fuzzy Cognitive Maps |website=Jfcm.megadix.it |date= |accessdate=2017-01-09}}</ref>\nFCM is a technique used for causal knowledge acquisition and representation, it supports causal knowledge reasoning process and belong to the neuro-fuzzy system that aim at solving decision making problems, modeling and simulate [[complex system]]s. \nLearning algorithms  have been proposed for training and updating FCMs weights mostly based on ideas coming from the field of [[Artificial Neural Network]]s. Adaptation and learning methodologies used to adapt the FCM model and adjust its weights.  Kosko and Dickerson (Dickerson & Kosko, 1994) suggested the Differential [[Hebbian Learning]] (DHL) to train FCM.<ref>{{cite web|url=http://home.eng.iastate.edu/~julied/publications/FCM96.pdf |title=IEEEBook8.dvi |website=Home.eng.iastate.edu |format=PDF |date= |accessdate=2017-01-09}}</ref> There have been proposed algorithms based on the initial Hebbian algorithm;<ref>{{cite journal |doi=10.1016/j.ijar.2004.01.001 |title=Active Hebbian learning algorithm to train fuzzy cognitive maps |journal=International Journal of Approximate Reasoning |volume=37 |issue=3 |page=219 |year=2004 |last1=Papageorgiou |first1=E.I. |last2=Stylios |first2=C.D. |last3=Groumpos |first3=P.P. }}</ref> others algorithms come from the field of [[genetic algorithms]], [[swarm intelligence]]<ref>{{cite journal |doi=10.1007/s10844-005-0864-9 |title=Fuzzy Cognitive Maps Learning Using Particle Swarm Optimization |journal=Journal of Intelligent Information Systems |volume=25 |page=95 |year=2005 |last1=Papageorgiou |first1=Elpiniki I. |last2=Parsopoulos |first2=Konstantinos E. |last3=Stylios |first3=Chrysostomos S. |last4=Groumpos |first4=Petros P. |last5=Vrahatis |first5=Michael N. }}</ref> and  [[evolutionary computation]].<ref>{{cite book |doi=10.1109/FUZZY.2005.1452465 |chapter=Evolutionary Development of Fuzzy Cognitive Maps |title=The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ \'05 |pages=619– |year=2005 |last1=Stach |first1=W. |last2=Kurgan |first2=L. |last3=Pedrycz |first3=W. |last4=Reformat |first4=M. |isbn=0-7803-9159-4 }}</ref> [[Learning algorithms]] are used to overcome the shortcomings that the traditional FCM present i.e. decreasing the human intervention by suggested automated FCM candidates; or by activating only the most relevant concepts every execution time; or by making models more transparent and dynamic.<ref>{{cite journal |doi=10.1016/j.ijhcs.2006.02.009 |title=Unsupervised learning techniques for fine-tuning fuzzy cognitive map causal links |journal=International Journal of Human-Computer Studies |volume=64 |issue=8 |page=727 |year=2006 |last1=Papageorgiou |first1=Elpiniki I. |last2=Stylios |first2=Chrysostomos |last3=Groumpos |first3=Peter P. }}</ref>\n\nFuzzy cognitive maps (FCMs) have gained considerable research interest due to their ability in representing structured knowledge and model complex systems in various fields. This growing interest led to the need for enhancement and making more reliable models that can better represent real situations.\nA first simple application of FCMs is described in a book<ref name="confusion">William R. Taylor: \'\'[http://www.americanconfusion.com/?p=122 Lethal American Confusion] (How Bush and the Pacifists Each Failed in the War on Terrorism)\'\', 2006, ISBN 0-595-40655-6 (FCM application in chapter 14) {{webarchive |url=https://web.archive.org/web/20070930103802/http://www.americanconfusion.com/?p=122 |date=September 30, 2007 }}</ref> of William R. Taylor, where the war in Afghanistan and Iraq is analyzed. And in [[Bart Kosko]]\'s book \'\'Fuzzy Thinking\'\',<ref name="FuzzyThinking">Bart Kosko: \'\'Fuzzy Thinking\'\', 1993/1995, ISBN 0-7868-8021-X (Chapter 12: Adaptive Fuzzy Systems)\'\'</ref> several Hasse diagrams illustrate the use of FCMs. As an example, one FCM quoted from Rod Taber<ref name="Drugs">Rod Taber: \'\'Knowledge Processing with Fuzzy Cognitive Maps\'\', Expert Systems with Applications, vol. 2, no. 1, 83-87, 1991 ([[:de:Bild:FCMdrug520.png|Hasse diagram]] in German Wikipedia)</ref> describes 11 factors of the American cocaine market and the relations between these factors. For computations, Taylor uses pentavalent logic (scalar values out of {-1,-0.5,0,+0.5,+1}). That particular map of Taber uses [[trivalent logic]] (scalar values out of {-1,0,+1}). Taber et al.  also illustrate the dynamics of map fusion and give a theorem on the convergence of combination in a related article <ref name=\'Medical\'>{{cite journal |doi=10.1002/int.20185 |title=Quantization effects on the equilibrium behavior of combined fuzzy cognitive maps |journal=International Journal of Intelligent Systems |volume=22 |issue=2 |page=181 |year=2007 |last1=Taber |first1=Rod |last2=Yager |first2=Ronald R. |last3=Helgason |first3=Cathy M. }}</ref>\n\nWhile applications in social sciences<ref name="confusion"/><ref name="FuzzyThinking"/><ref name="Drugs"/><ref>Costas Neocleous, Christos Schizas, Costas Yenethlis: \'\'[http://www.congreso-info.cu/UserFiles/File/Info/Info2006/Ponencias/271.pdf Fuzzy Cognitive Models in Studying Political Dynamics  - The case of the Cyprus problem]\'\' {{webarchive |url=https://web.archive.org/web/20070929055849/http://www.congreso-info.cu/UserFiles/File/Info/Info2006/Ponencias/271.pdf |date=September 29, 2007 }}</ref> introduced FCMs to the public, they are used in a much wider range of applications, which all have to deal with creating and using models<ref>Chrysostomos D. Stylios, Voula C. Georgopoulos, Peter P. Groumpos: \'\'[http://med.ee.nd.edu/MED5/PAPERS/067/067.PDF The Use of Fuzzy Cognitive Maps in Modeling Systems]\'\' {{webarchive |url=https://web.archive.org/web/20110720011915/http://med.ee.nd.edu/MED5/PAPERS/067/067.PDF |date=July 20, 2011 }}</ref> of uncertainty and complex processes and systems. Examples:\n*In business FCMs can be used for product planning.<ref>Antonie Jetter: \'\'Produktplanung im Fuzzy Front End\'\', 2005, ISBN 3-8350-0144-2</ref>\n*In economics, FCMs support the use of [[game theory]] in more complex settings.<ref>Vesa A. Niskanen: \'\'[http://www.ijicic.org/fic04-20.pdf Application of Fuzzy Linguistic Cognitive Maps to Prisoner\'s Dilemma]\'\', 2005, ICIC International pp. 139-152, ISSN 1349-4198 {{webarchive |url=https://web.archive.org/web/20070929040145/http://www.ijicic.org/fic04-20.pdf |date=September 29, 2007 }}</ref>\n* In Medical applications to model systems, provide diagnosis,<ref>{{cite journal |doi=10.1016/S0933-3657(02)00076-3 |pmid=14656490 |title=A fuzzy cognitive map approach to differential diagnosis of specific language impairment |journal=Artificial Intelligence in Medicine |volume=29 |issue=3 |pages=261–78 |year=2003 |last1=Georgopoulos |first1=Voula C |last2=Malandraki |first2=Georgia A |last3=Stylios |first3=Chrysostomos D }}</ref> develop [[decision support systems]]<ref>{{cite journal |doi=10.1109/TBME.2003.819845 |pmid=14656062 |title=An integrated two-level hierarchical system for decision making in radiation therapy based on fuzzy cognitive maps |journal=IEEE Transactions on Biomedical Engineering |volume=50 |issue=12 |pages=1326–39 |year=2003 |last1=Papageorgiou |first1=E.I. |last2=Stylios |first2=C.D. |last3=Groumpos |first3=P.P. }}</ref> and [[medical assessment]].<ref>{{cite book |doi=10.1007/978-3-319-11457-6_18 |chapter=Supervisory Fuzzy Cognitive Map Structure for Triage Assessment and Decision Support in the Emergency Department |title=Simulation and Modeling Methodologies, Technologies and Applications |volume=319 |pages=255–69 |series=Advances in Intelligent Systems and Computing |year=2015 |last1=Georgopoulos |first1=Voula C. |last2=Stylios |first2=Chrysostomos D. |isbn=978-3-319-11456-9 }}</ref>\n* In Engineering for [[process modeling|modeling]] and [[Process control|control]]<ref>{{cite web|url=http://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs089?resultNumber=0&totalResults=11&start=0&q=stylios&resultsPageSize=10&rows=10 |title=Fuzzy Cognitive Maps in modeling supervisory control systems - IOS Press |website=Content.iospress.com |date= |accessdate=2017-01-09}}</ref> mainly of complex systems<ref>{{cite journal |doi=10.1109/TSMCA.2003.818878 |title=Modeling Complex Systems Using Fuzzy Cognitive Maps |journal=IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans |volume=34 |page=155 |year=2004 |last1=Stylios |first1=C.D. |last2=Groumpos |first2=P.P. }}</ref>\n*In project planning FCMs help to analyze the mutual dependencies between project resources.\n*In robotics<ref name="FuzzyThinking"/><ref>Marc Böhlen: \'\'[http://www.realtechsupport.org/pdf/SpaceRobotics2000.pdf More Robots in Cages]\'\',</ref> FCMs support machines to develop fuzzy models of their environments and to use these models to make crisp decisions.\n*In computer assisted learning FCMs enable computers to check whether students understand their lessons.<ref>Benjoe A. Juliano, Wylis Bandler: \'\'Tracing Chains-of-Thought (Fuzzy Methods in Cognitive Diagnosis)\'\', Physica-Verlag Heidelberg 1996, ISBN 3-7908-0922-5</ref>\n*In [[expert system]]s<ref name="Drugs"/> a few or many FCMs can be aggregated into one FCM in order to process estimates of knowledgeable persons.<ref>W. B. Vasantha Kandasamy, Florentin Smarandache: \'\'[http://www.gallup.unm.edu/~smarandache/NCMs.pdf Fuzzy Cognitive Maps and Neutrosophic Cognitive Maps]\'\', 2003, ISBN 1-931233-76-4</ref>\n*In IT project management, a FCM-based methodology helps to success modelling.<ref>{{cite journal |doi=10.1016/j.eswa.2006.01.032 |title=Modelling IT projects success with Fuzzy Cognitive Maps |journal=Expert Systems with Applications |volume=32 |issue=2 |page=543 |year=2007 |last1=Rodriguez-Repiso |first1=Luis |last2=Setchi |first2=Rossitza |last3=Salmeron |first3=Jose L. }}</ref>\n\nFCMappers<ref>FCMappers - international community for fuzzy cognitive mapping: http://www.FCMappers.net/</ref> - an international online community for the analysis and the visualization of fuzzy cognitive maps offer support for starting with FCM and also provide an MS-Excel-based tool that is able to check and analyse FCMs. The output is saved as [[Pajek]] file and can be visualized within 3rd party software like Pajek, Visone,... . They also offer to adapt the software to specific research needs. On their webpage you also will find a linklist for interesting scientific articles, related software, institutes, people and projects. The FCMappers have about one thousand registered members worldwide.\n\nAdditional FCM software tools, such as Mental Modeler,<ref>{{cite book |doi=10.1109/HICSS.2013.399 |chapter=Mental Modeler: A Fuzzy-Logic Cognitive Mapping Modeling Tool for Adaptive Environmental Management |title=2013 46th Hawaii International Conference on System Sciences |pages=965– |year=2013 |last1=Gray |first1=Steven A. |last2=Gray |first2=Stefan |last3=Cox |first3=Linda J. |last4=Henly-Shepard |first4=Sarah |isbn=978-1-4673-5933-7 }}</ref><ref>{{cite web|url=http://www.mentalmodeler.com/ |title=Fuzzy Logic Cognitive Mapping |publisher=Mental Modeler |date= |accessdate=2017-01-09}}</ref> have recently been developed as a decision-support tool for use in [[social science]] research, [[collaborative decision-making]], and [[Natural resource management|natural resource planning]].\n\n==Bipolar Fuzzy Cognitive Maps==\nFuzzy cognitive maps have been further extended to bipolar fuzzy cognitive maps based on bipolar fuzzy sets <ref>Wen-Ran Zhang, 1998, (Yin)(Yang) Bipolar Fuzzy Sets. Proceedings of IEEE World Congress on Computational Intelligence – Fuzz-IEEE, Anchorage, AK, 835-840 </ref> and bipolar cognitive mapping.<ref>{{cite journal |doi=10.1109/21.24529 |title=Pool2: A generic system for cognitive map development and decision analysis |journal=IEEE Transactions on Systems, Man, and Cybernetics |volume=19 |page=31 |year=1989 |last1=Zhang |first1=W.R. |last2=Chen |first2=S.S. |last3=Bezdek |first3=J.C. }}</ref><ref>{{cite journal |doi=10.1109/21.141315 |title=A cognitive-map-based approach to the coordination of distributed cooperative agents |journal=IEEE Transactions on Systems, Man, and Cybernetics |volume=22 |page=103 |year=1992 |last1=Zhang |first1=W.-R. |last2=Chen |first2=S.-S. |last3=Wang |first3=W. |last4=King |first4=R.S. }}</ref><ref>{{cite journal |doi=10.1109/TSMCB.2003.810444 |title=Equilibrium relations and bipolar cognitive mapping for online analytical processing with applications in international relations and strategic decision support |journal=IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics) |volume=33 |issue=2 |page=295 |year=2003 |last1=Wen-Ran Zhang }}</ref><ref>Wen-Ran Zhang,  2003b, Equilibrium Energy and Stability Measures for Bipolar Decision and Global Regulation. Int’l J. of Fuzzy Sys. Vol. 5, No. 2, 2003, 114-122</ref> Bipolar fuzzy set theory as an equilibrium-based extension to fuzzy sets is recognized by [[L. A. Zadeh]]. <ref>L. A. Zadeh, 2008, Fuzzy logic. Scholarpedia, 3(3):1766, Created: 10 July 2006, reviewed: 27 March 2007, accepted: 31 March 2008.</ref>\n\n==See also==\n[[Soft Computing]]   \n\n==References==\n{{Reflist|30em}}\n\n{{Commons category|Cognitive maps}}\n\n[[Category:Knowledge representation]]\n[[Category:Fuzzy logic]]']
['Historical Thesaurus of the Oxford English Dictionary', '12612212', '{{italic title}}\n{{Infobox website\n| name            = \'\'Historical Thesaurus of English\'\'\n| logo            = Historical Thesaurus of English logo.png\n| logo_size       = <!-- default is 250px -->\n| logo_alt        =\n| url             = {{URL|www.glasgow.ac.uk/thesaurus}}\n| commercial      = No\n| type            = Academic\n| registration    = None\n| content_licence = Free for personal and non-commercial research<ref name=hte-using>{{cite web|title=Using \'\'Historical Thesaurus\'\' Data|url=http://historicalthesaurus.arts.gla.ac.uk/using-data/|website=The Historical Thesaurus of English|publisher=University of Glasgow|accessdate=25 October 2014}}</ref>\n| programming_language = \n| owner           = [[University of Glasgow]]\n| author          = Marc Alexander and Christian Kay<ref name=hte-cite>{{cite web|last1=Alexander|first1=Marc|last2=Kay|first2=Christian|title=How to Cite|url=http://historicalthesaurus.arts.gla.ac.uk/how-to-cite/|website=The Historical Thesaurus of English, version 4.2|publisher=University of Glasgow|accessdate=25 October 2014}}</ref>\n| editor          = [[Christian Kay]], Jane Roberts, [[Michael Samuels (academic)|Michael Samuels]], Irené Wotherspoon, and Marc Alexander (editors)\n| current_status  = Version 4.2, since September, 2014<ref name=hte-versions />\n| footnotes       = \n}}\n{{Infobox book\n|name           = Historical Thesaurus of the Oxford English Dictionary : with additional material from "A Thesaurus of Old English"\n|image          = Historical Thesaurus.jpg\n|caption        = Print edition of version 1.0 of the \'\'Historical Thesaurus of English\'\'<ref name="hte-versions" />\n|alt            = Printed boxed set\n|author         = Christian Kay, Jane Roberts, [[Michael Samuels (academic)|Michael Samuels]], and Irené Wotherspoon (editors)\n|title_working  = Historical Thesaurus of English\n|country        = Great Britain\n|language       = English\n|subject        = [[History of the English language]]\n|genre          = [[Thesaurus|Thesauri]] \n|published      = 2009 ([[Oxford University Press]])\n|pages          = 4,448\n|awards         = Scottish Research Book of the Year Award, [[Saltire Society Literary Awards]], 2009\n|isbn           = 978-0199208999\n|oclc           = 318409912\n|dewey          = \n|congress       =  PE1591 .H55 2009\n}}\n\nThe \'\'\'\'\'Historical Thesaurus of the Oxford English Dictionary\'\'\'\'\' (\'\'\'\'\'HTOED\'\'\'\'\') is the print edition of the largest [[thesaurus]] in the world, the \'\'\'\'\'Historical Thesaurus of English\'\'\'\'\' (\'\'\'\'\'HTE\'\'\'\'\'), conceived and compiled by the English Language Department of the [[University of Glasgow]]. The \'\'HTE\'\' is a complete database of all the words in the second edition of [[Oxford English Dictionary|\'\'The Oxford English Dictionary\'\']], arranged by [[semantic field]] and date. In this way, the \'\'HTE\'\' arranges the whole vocabulary of [[English language|English]], from the earliest written records in [[Old English language|Old English]] to the present, alongside types and dates of use. It is the first historical thesaurus to be compiled for any of the world\'s languages and contains 800,000 meanings for 600,000 words, within 230,000 categories, covering more than 920,000 words and meanings.<ref name="Woolcock">{{cite news| url=http://entertainment.timesonline.co.uk/tol/arts_and_entertainment/books/article6644646.ece | location=London | work=The Times | first=Nicola | last=Woolcock | title=After a 44-year labour of love worlds biggest thesaurus is born | date=2009-07-06}}{{subscription required}}</ref><ref>{{cite news|last1=Hitchings|first1=Henry|authorlink1=Henry Hitchings|title=Historical Thesaurus is a masterpiece worth waiting 40 years for|url=http://www.telegraph.co.uk/comment/personal-view/6413166/Historical-Thesaurus-is-a-masterpiece-worth-waiting-40-years-for.html|accessdate=25 October 2014|publisher=The Telegraph|date=23 October 2009|ref=Hitchings-2009|location=London}}</ref>  As the \'\'HTE\'\' website states, "in addition to providing hitherto unavailable information for linguistic and textual scholars, the \'\'Historical Thesaurus\'\' online is a rich resource for students of social and cultural history, showing how concepts developed through the words that refer to them."<ref name="hte">{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/ |title=Home page |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-24}}</ref>\n\nThe ambitious project was announced at a 1965 meeting of the [[Philological Society]] by its originator, [[Michael Samuels (academic)|Michael Samuels]].<ref name=Crystal-2014>{{cite book|last1=Crystal|first1=David|authorlink1=David Crystal|title=Words in Time and Place: Exploring Language Through the \'\'Historical Thesaurus of the Oxford English Dictionary\'\'|date=2014|publisher=Oxford University Press|location=Oxford|isbn=0199680477|page=vii}}</ref>  Work on the \'\'HTE\'\' started in 1965.\n\nOn 22 October 2009, after 44 years of work, version 1.0 was published as a two-volume set as \'\'HTOED\'\'.<ref>{{cite news|url=http://news.bbc.co.uk/1/hi/england/oxfordshire/8136122.stm |title=UK &#124; England &#124; Oxfordshire &#124; Forty-year wait for new thesaurus |publisher=BBC News |date=2009-07-06 |accessdate=2010-04-15}}</ref> It consists of two slipcased hardcover volumes, totaling nearly 4,000 pages. The \'\'HTE\'\', released as version 4.2 in September 2014, is freely available online from the University of Glasgow.<ref name="hte-versions">{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/versions-and-changes/ |title=Versions of the Thesaurus |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-24}}</ref>\n\n==Main sections==\nThe work is divided into three main sections: the External World, the Mind, and Society. These are broken down into successively narrower domains. The text eventually discriminates more than 236,000 categories.\nThe second order categories are:<ref>{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/classification/ |title=Classification |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-22}} An oversize, one-page listing of all categories in top three tiers is available for download here.</ref>\n{{col-begin-small}}\n{{col-break}}\n;I. The External World\n \n# The Earth\n# Life\n# Physical sensibility\n# Matter\n# Existence\n# Relative properties\n# The Supernatural\n{{col-break}} \n;II. The Mind\n \n# Soul, spirit, mind\n# Emotion/feeling\n# Judgement, opinion\n# Aesthetics\n# Will/faculty of will\n# Expectation\n# Having/possession\n# Languages\n{{col-break}}\n;III. Society\n \n# Society/life in association with others\n# Inhabiting/dwelling\n# Relations between social groups\n# Authority\n# Law\n# Education\n# Religion\n# Communications\n# Travel/travelling\n# Work / Serious occupation\n# Leisure/The Arts\n{{col-end}}\n\n==References==\n{{reflist}}\n\n==External links==\n* {{cite web|title=Search|url=http://historicalthesaurus.arts.gla.ac.uk/search/|website=The Historical Thesaurus of English|publisher=University of Glasgow}} {{open access}}\n{{Dictionaries of English}}\n\n[[Category:Thesauri]]\n[[Category:Classification systems]]\n[[Category:Knowledge representation]]\n[[Category:Language histories]]\n[[Category:History of the English language]]']
['Defeasible reasoning', '2628057', '{{No footnotes|date=April 2010}}\nIn [[logic]], \'\'\'defeasible reasoning\'\'\' is a kind of [[reasoning]] that is rationally compelling though not [[deductive reasoning|deductively valid]].<ref>{{cite web | url=http://plato.stanford.edu/entries/reasoning-defeasible | title="Defeasbile Reasoning," \'\'Stanford Encyclopedia of Philosophy | accessdate=1 July 2016 }}</ref> The distinction between defeasibility and indefeasibility may be seen in the context of this joke:\n\n:During a train trip through the countryside, an engineer, a physicist, and a mathematician observe a flock of sheep. The engineer remarks, "I see that the sheep in this region are white." The physicist offers a correction, "\'\'Some\'\' sheep in this region are white." And the mathematician responds, "In this region there exist sheep that are white on at least one side."\n\nThe engineer in this story has reasoned defeasibly; since [[engineering]] is a highly practical discipline, it is receptive to generalizations. In particular, engineers cannot and need not defer decisions until they have acquired perfect and complete knowledge. But [[mathematics|mathematical reasoning]], having different goals, inclines one to account for even the rare and special cases, and thus typically leads to a stance that is indefeasible.\n\nDefeasible reasoning is a particular kind of non-demonstrative reasoning, where the reasoning does not produce a full, complete, or final demonstration of a claim, i.e., where fallibility and corrigibility of a conclusion are acknowledged. In other words defeasible reasoning produces a [[Wiktionary:contingent|contingent]] statement or claim.  Other kinds of non-demonstrative reasoning are [[probabilistic reasoning]], [[inductive reasoning]], [[statistical]] reasoning, [[abductive reasoning]], and [[paraconsistent]] reasoning.  Defeasible reasoning is also a kind of [[ampliative]] reasoning because its conclusions reach beyond the pure meanings of the premises.\n\nThe differences between these kinds of reasoning correspond to differences about the conditional that each kind of reasoning uses, and on what premise (or on what authority) the conditional is adopted:\n* \'\'[[deductive reasoning|Deductive]]\'\' (from meaning postulate, axiom, or contingent assertion): if \'\'p\'\' then \'\'q\'\' (i.e., \'\'q\'\' or \'\'not-p\'\')\n* \'\'Defeasible\'\' (from authority): if \'\'p\'\' then (defeasibly) \'\'q\'\'\n* \'\'[[Probabilistic logic|Probabilistic]]\'\' (from combinatorics and indifference): if \'\'p\'\' then (probably) \'\'q\'\'\n* \'\'[[Statistics|Statistical]]\'\' (from data and presumption):  the frequency of \'\'q\'\'s among \'\'p\'\'s is high (or inference from a model fit to data); hence, (in the right context) if \'\'p\'\' then (probably) \'\'q\'\'\n* \'\'[[inductive reasoning|Inductive]]\'\' (theory formation; from data, coherence, simplicity, and confirmation): (inducibly) "if \'\'p\'\' then \'\'q\'\'"; hence, if \'\'p\'\' then (deducibly-but-revisably) \'\'q\'\'\n* \'\'[[abductive reasoning|Abductive]]\'\' (from data and theory):  \'\'p\'\' and \'\'q\'\' are correlated, and \'\'q\'\' is sufficient for \'\'p\'\'; hence, if \'\'p\'\' then (abducibly) \'\'q\'\' as cause\n\nDefeasible reasoning finds its fullest expression in [[jurisprudence]], [[ethics]] and [[moral philosophy]], [[epistemology]], [[pragmatics]] and conversational [[Convention (norm)|conventions]] in [[linguistics]], [[Constructivist epistemology|constructivist]] [[Decision theory|decision theories]], and in [[knowledge representation]] and [[planning]] in [[artificial intelligence]].  It is also closely identified with [[prima facie]] (presumptive) reasoning (i.e., reasoning on the "face" of evidence), and [[ceteris paribus]] (default) reasoning (i.e., reasoning, all things "being equal").\n\n== History ==\n\nThough [[Aristotle]] differentiated the forms of reasoning that are valid for [[logic]] and [[philosophy]] from the more general ones that are used in everyday life (see [[dialectics]] and [[rhetoric]]), 20th century philosophers mainly concentrated on deductive reasoning. At the end of the 19th century, logic texts would typically survey both demonstrative and non-demonstrative reasoning, often giving more space to the latter. However, after the blossoming of [[mathematical logic]] at the hands of [[Bertrand Russell]], [[Alfred North Whitehead]] and [[Willard van Orman Quine]], latter-20th century logic texts paid little attention to the non-deductive modes of inference.\n\nThere are several notable exceptions. [[John Maynard Keynes]] wrote his dissertation on non-demonstrative reasoning, and influenced the thinking of [[Ludwig Wittgenstein]] on this subject. Wittgenstein, in turn, had many admirers, including the [[positivist]] legal scholar [[H.L.A. Hart]] and the [[speech act]] linguist [[John L. Austin]], [[Stephen Toulmin]] in rhetoric ([[Chaim Perelman]] too), the moral theorists [[W.D. Ross]] and [[C.L. Stevenson]], and the [[vagueness]] epistemologist/ontologist [[Friedrich Waismann]].\n\nThe etymology of \'\'defeasible\'\' usually refers to Middle English law of contracts, where a condition of defeasance is a clause that can invalidate or annul a contract or deed. Though \'\'defeat\'\', \'\'dominate\'\', \'\'defer\'\', \'\'defy\'\', \'\'deprecate\'\' and \'\'derogate\'\' are often used in the same contexts as \'\'defeasible,\'\' the verbs \'\'annul\'\' and \'\'invalidate\'\' (and \'\'nullify,\'\' \'\'overturn,\'\' \'\'rescind,\'\' \'\'vacate,\'\' \'\'repeal,\'\' \'\'debar\'\', \'\'void\'\', \'\'cancel\'\', \'\'countermand\'\', \'\'preempt\'\', etc.) are more properly correlated with the concept of defeasibility than those words beginning with the letter \'\'d\'\'. Many dictionaries do contain the verb, \'\'to defease\'\' with past participle, \'\'defeased.\'\'\n\nPhilosophers in moral theory and rhetoric had taken defeasibility largely for granted when American epistemologists rediscovered Wittgenstein\'s thinking on the subject: John Ladd, [[Roderick Chisholm]], [[Roderick Firth]], [[Ernest Sosa]], [[Robert Nozick]], and [[John L. Pollock]] all began writing with new conviction about how \'\'appearance as red\'\' was only a defeasible reason for believing something to be red.  More importantly Wittgenstein\'s orientation toward [[language-games]] (and away from [[semantics]]) emboldened these epistemologists to manage rather than to expurgate \'\'prima facie\'\' logical inconsistency.\n\nAt the same time (in the mid-1960s), two more students of Hart and Austin at Oxford, [[Brian Barry]] and [[David Gauthier]], were applying defeasible reasoning to political argument and practical reasoning (of action), respectively. [[Joel Feinberg]] and [[Joseph Raz]] were beginning to produce equally mature works in ethics and jurisprudence informed by defeasibility.  \n\nBy far the most significant works on defeasibility by the mid-1970s were in epistemology, where [[John L. Pollock|John Pollock]]\'s 1974 \'\'Knowledge and Justification\'\' popularized his terminology of \'\'undercutting\'\' and \'\'rebutting\'\' (which mirrored the analysis of Toulmin). Pollock\'s work was significant precisely because it brought defeasibility so close to philosophical logicians. The failure of logicians to dismiss defeasibility in epistemology (as Cambridge\'s logicians had done to Hart decades earlier) landed defeasible reasoning in the philosophical mainstream.  \n\nDefeasibility had always been closely related to argument, rhetoric, and law, except in epistemology, where the chains of reasons, and the origin of reasons, were not often discussed. [[Nicholas Rescher]]\'s \'\'Dialectics\'\' is an example of how difficult it was for philosophers to contemplate more complex systems of defeasible reasoning. This was in part because proponents of [[informal logic]] became the keepers of argument and rhetoric while insisting that formalism was anathema to argument.\n\nAbout this time, researchers in [[artificial intelligence]] became interested in [[non-monotonic reasoning]] and its [[semantics]]. With philosophers such as Pollock and Donald Nute (e.g., [[defeasible logic]]), dozens of computer scientists and logicians produced complex systems of defeasible reasoning between 1980 and 2000. No single system of defeasible reasoning would emerge in the same way that Quine\'s system of logic became a de facto standard. Nevertheless, the 100-year headstart on non-demonstrative logical calculi, due to [[George Boole]], [[Charles Sanders Peirce]], and [[Gottlob Frege]] was being closed: both demonstrative and non-demonstrative reasoning now have formal calculi.\n\nThere are related (and slightly competing) systems of reasoning that are newer than systems of defeasible reasoning, e.g., [[belief revision]] and [[dynamic logic (modal logic)|dynamic logic]]. The dialogue logics of [[Charles Leonard Hamblin|Charles Hamblin]] and Jim Mackenzie, and their colleagues, can also be tied closely to defeasible reasoning. Belief revision is a non-constructive specification of the desiderata with which, or constraints according to which, epistemic change takes place. Dynamic logic is related mainly because, like paraconsistent logic, the reordering of premises can change the set of justified conclusions. Dialogue logics introduce an adversary, but are like belief revision theories in their adherence to deductively consistent states of belief.\n\n==Political and judicial use==\nMany political philosophers have been fond of the word \'\'indefeasible\'\' when referring to rights, e.g., that were \'\'inalienable,\'\' \'\'divine,\'\' or \'\'indubitable.\'\'  For example, in the 1776 [[Virginia Declaration of Rights]], "community hath an indubitable, inalienable, and indefeasible right to reform, alter or abolish government..." (also attributed to [[James Madison]]); and [[John Adams]], "The people have a right, an indisputable, unalienable, indefeasible, divine right to that most dreaded and envied kind of knowledge – I mean of the character and conduct of their rulers."\nAlso, [[Lord Aberdeen]]:  "indefeasible right inherent in the British Crown" and [[Gouverneur Morris]]:  "the Basis of our own Constitution is the indefeasible Right of the People."  Scholarship about [[Abraham Lincoln]] often cites these passages in the justification of secession.  Philosophers who use the word \'\'defeasible\'\' have historically had different world views from those who use the word \'\'indefeasible\'\' (and this distinction has often been mirrored by Oxford and Cambridge zeitgeist); hence it is rare to find authors who use both words.\n\nIn judicial opinions, the use of \'\'defeasible\'\' is commonplace.  There is however disagreement among legal logicians whether \'\'defeasible reasoning\'\' is central, e.g., in the consideration of \'\'open texture\'\', [[precedent]], [[wikt:exception|exceptions]], and \'\'rationales\'\', or whether it applies only to explicit defeasance clauses.  [[H.L.A. Hart]] in \'\'[[The Concept of Law]]\'\' gives two famous examples of defeasibility:  "No vehicles in the park" (except during parades); and "Offer, acceptance, and memorandum produce a contract" (except when the contract is illegal, the parties are minors, inebriated, or incapacitated, etc.).\n\n== Specificity ==\n\nOne of the main disputes among those who produce systems of defeasible reasoning is the status of a \'\'rule of specificity.\'\'  In its simplest form, it is the same rule as subclass [[inheritance (computer science)|inheritance]] preempting class inheritance:  \n\n  (R1) if \'\'r\'\' then (defeasibly) \'\'q\'\'                  e.g., if bird, then can fly\n  (R2) if \'\'p\'\' then (defeasibly) \'\'not-q\'\'              e.g., if penguin, then cannot fly\n  (O1) if \'\'p\'\' then (deductively) \'\'r\'\'                 e.g., if penguin, then bird\n  (M1) arguably, p                               e.g., arguably, penguin\n  (M2) R2 is a more specific reason than R1      e.g., R2 is better than R1\n  (M3) therefore, arguably, not-q                e.g., therefore, arguably, not-flies\n\nApproximately half of the systems of defeasible reasoning discussed today adopt a rule of specificity, while half expect that such \'\'preference\'\' rules be written explicitly by whoever provides the defeasible reasons.  For example, Rescher\'s dialectical system uses specificity, as do early systems of multiple inheritance (e.g., [[David Touretzky]]) and the early argument systems of Donald Nute and of [[Guillermo Simari]] and [[Ronald Loui]].  Defeasible reasoning accounts of  precedent ([[stare decisis]] and [[case-based reasoning]]) also make use of specificity (e.g., [[Joseph Raz]] and the work of Kevin D. Ashley and Edwina Rissland).  Meanwhile, the argument systems of Henry Prakken and Giovanni Sartor, of Bart Verheij and Jaap Hage, and the system of Phan Minh Dung do not adopt such a rule.\n\n== Nature of defeasibility ==\n\nThere is a distinct difference between those who theorize about defeasible reasoning as if it were a system of confirmational revision (with affinities to [[belief revision]]), and those who theorize about defeasibility as if it were the result of further (non-empirical) investigation.  There are at least three kinds of further non-empirical investigation:  progress in a lexical/syntactic process, progress in a computational process, and progress in an adversary or legal proceeding.  \n\n\'\'\'\'\'Defeasibility as corrigibility:\'\'\'\'\'  Here, a person learns something new that annuls a prior inference.  In this case, defeasible reasoning provides a constructive mechanism for belief revision, like a [[truth maintenance system]] as envisioned by Jon Doyle.\n\n\'\'\'\'\'Defeasibility as shorthand for preconditions:\'\'\'\'\'  Here, the author of a set of rules or legislative code is writing rules with exceptions.  Sometimes a set of defeasible rules can be rewritten, with more cogency, with explicit (local) pre-conditions instead of (non-local) competing rules.  Many non-monotonic systems with [[fixed point (mathematics)|fixed-point]] or [[preferential]] semantics fit this view.  However, sometimes the rules govern a process of argument (the last view on this list), so that they cannot be re-compiled into a set of deductive rules lest they lose their force in situations with incomplete knowledge or incomplete derivation of preconditions.  \n\n\'\'\'\'\'Defeasibility as an [[anytime algorithm]]:\'\'\'\'\'  Here, it is assumed that calculating arguments takes time, and at any given time, based on a subset of the potentially constructible arguments, a conclusion is defeasibly justified.  [[Isaac Levi]] has protested against this kind of defeasibility, but it is well-suited to the heuristic projects of, for example, [[Herbert A. Simon]].  On this view, the \'\'best move so far\'\' in a chess-playing program\'s analysis at a particular depth is a defeasibly justified conclusion.  This interpretation works with either the prior or the next semantical view.\n\n\'\'\'\'\'Defeasibility as a means of controlling an investigative or social process:\'\'\'\'\'  Here, justification is the result of the right kind of procedure (e.g., a fair and efficient hearing), and defeasible reasoning provides impetus for pro and con responses to each other.  Defeasibility has to do with the alternation of verdict as locutions are made and cases presented, not the changing of a mind with respect to new (empirical) discovery.  Under this view, defeasible reasoning and defeasible argumentation refer to the same phenomenon.\n\n==See also==\n* [[Defeasible estate]]\n* [[Indefeasible rights of use]]\n* [[Argument (logic)]]\n* [[Prima facie]]\n* [[Practical reasoning]]\n* [[Pragmatics]]\n* [[Non-monotonic reasoning]]\n\n== References ==\n{{Reflist}}\n* [http://www.springerlink.com/index/UQ708JX7XG823H5F.pdf Defeasible logic], Donald Nute, Lecture Notes in Computer Science, Springer, 2003.\n* [http://portal.acm.org/citation.cfm?id=371581 Logical models of argument], Carlos Chesnevar, et al., ACM Computing Surveys 32:4, 2000.\n* [https://books.google.com/books?hl=en&lr=&id=bQHce6eNhDIC&oi=fnd&pg=PA219&dq=prakken&ots=h7xemV-dM1&sig=E6Ar7mAiBU0raO-rlNmzq8-8HG4 Logics for defeasible argumentation], Henry Prakken and Gerard Vreeswijk, in Handbook of Philosophical Logic, [[Dov M. Gabbay]], [[Franz Guenthner]], eds., Kluwer, 2002.\n* [https://books.google.com/books?hl=en&lr=&id=DN5ERAAxUSYC&oi=fnd&pg=PR9&dq=rescher&ots=vRu4s0Ely-&sig=__Dvw746CkFNdCaAdNLCGImTbFU Dialectics], [[Nicholas Rescher]], SUNY Press, 1977.\n* [http://linkinghub.elsevier.com/retrieve/pii/S0364021387800174 Defeasible reasoning], John Pollock, Cognitive Science, 1987.\n* [https://scholar.google.com/scholar?hl=en&lr=&cites=7198700474843277547 Knowledge and Justification], John Pollock, Princeton University Press, 1974.\n* [http://webdigg.net/Defeasible/Defeasible-reasoning/ Abstract argumentation systems], Gerard Vreeswijk, Artificial Intelligence, 1997.\n* [http://portal.acm.org/citation.cfm?id=222099 Hart\'s critics on defeasible concepts and ascriptivism], [[Ronald Loui]], Proc. 5th Intl. Conf. on AI and Law, 1995.\n* [https://scholar.google.com/scholar?hl=en&lr=&cites=7525164436422571935 Political argument], [[Brian Barry]], Routledge & Kegan Paul, 1970.\n* [https://scholar.google.com/scholar?hl=en&lr=&cites=8944770465668267468 The uses of argument], [[Stephen Toulmin]], Cambridge University Press, 1958.\n* [http://portal.acm.org/citation.cfm?id=981352&dl= Discourse relations and defeasible knowledge], Alex Lascarides and Nicholas Asher, Proc. of the 29th Meeting of the Assn. for Comp. Ling., 1991.\n* [http://journals.cambridge.org/action/displayAbstract;jsessionid=E68F5CAC6B0001D1ABEFD7C8C24F919F.tomcat1?fromPage=online&aid=191503 Defeasible logic programming: an argumentative approach], Alejandro Garcia and [[Guillermo Simari]], Theory and Practice of Logic Programming 4:95–138, 2004. \n* [http://portal.acm.org/citation.cfm?id=180954.180957 Philosophical foundations of deontic logic and the logic of defeasible conditionals], Carlos Alchourron, in Deontic logic in computer science: normative system specification, J. Meyer, R. Wieringa, eds., Wiley, 1994.\n* [http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6TYF-47YRKSD-7C&_user=10&_coverDate=02%2F29%2F1992&_rdoc=1&_fmt=high&_orig=browse&_origin=browse&_zone=rslt_list_item&_srch=doc-info%28%23toc%235617%231992%23999469997%23391734%23FLP%23display%23Volume%29&_cdi=5617&_sort=d&_docanchor=&_ct=16&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=0735cebb41ce81bdfe8e260dbef2c71d&searchtype=a A Mathematical Treatment of Defeasible Reasoning and its Implementation.] [[Guillermo Simari]], [[Ronald Loui]], Artificial Intelligence Journal, 53(2–3): 125–157 (1992).\n\n== External links ==\n* [http://plato.stanford.edu/entries/reasoning-defeasible/ Article on Defeasible Reasoning] in the [[Stanford Encyclopedia of Philosophy]]\n* [http://william-king.www.drexel.edu/top/prin/txt/Intro/Eco112c.html An example of defeasible reasoning in action]\n\n[[Category:Epistemology]]\n[[Category:Logic]]\n[[Category:Logic programming]]\n[[Category:Knowledge representation]]\n[[Category:Reasoning]]']
['Category:Mereology', '14239157', '{{Cat main|Mereology}}\n[[Category:Philosophical logic]]\n[[Category:Ontology]]\n[[Category:Knowledge representation]]\n[[Category:Materialism]]\n[[Category:Quantity]]']
['Open Knowledge Base Connectivity', '4720390', "{{Unreferenced stub|auto=yes|date=December 2009}}\n'''Open Knowledge Base Connectivity''' ('''OKBC''')  is a [[protocol (computer science)|protocol]] and an [[application programming interface|API]] for accessing knowledge in [[knowledge representation]] systems such as [[ontology (computer science)|ontology]] repositories and [[object-relational database]]s. It is somewhat complementary to the [[Knowledge Interchange Format]] that serves as a general representation language for knowledge. It is developed by [[SRI International]]'s [[Artificial Intelligence Center]] for [[DARPA]]'s High Performance Knowledge Base program (HPKB).\n\n==External links==\n* [http://www.ai.sri.com/~okbc/ Open Knowledge Base Connectivity Home Page]\n\n[[Category:SRI International software]]\n[[Category:Knowledge representation]]\n\n{{Comp-sci-stub}}"]
['Ronald J. Brachman', '853832', '{{Infobox scientist\n| name              = Ronald Jay Brachman\n| image             = <!--(filename only)-->\n| image_size        = \n| alt               = \n| caption           = \n| birth_date        = {{Birth year and age|1949}} \n| birth_place       = \n| death_date        = <!-- {{Death date and age|YYYY|MM|DD|YYYY|MM|DD}} (death date then birth date) -->\n| death_place       = \n| resting_place             = \n| resting_place_coordinates = <!-- {{Coord|LAT|LONG|type:landmark|display=inline,title}} -->\n| residence         = \n| citizenship       = \n| nationality       = \n| fields            = \n| workplaces        = [[Harvard University]]<br>[[Yahoo! Research]]<br>[[AT&T Corporation]]<br>[[DARPA]]\n| alma_mater        = [[Harvard University]]<br>[[Princeton University]]\n| thesis_title      = A structural paradigm for representing knowledge\n| thesis_url        = https://books.google.com/books?id=ThS-HAAACAAJ\n| thesis_year       = 1977\n| doctoral_advisor  = William A. Woods\n| academic_advisors = \n| doctoral_students = \n| notable_students  = \n| known_for         = \n| author_abbrev_bot = \n| author_abbrev_zoo = \n| influences        = \n| influenced        = \n| awards            = \n| signature         = <!--(filename only)-->\n| signature_alt     = \n| website           = {{URL|www.brachman.org}}<br>{{URL|research.yahoo.com/Ron_Brachman}}\n| footnotes         = \n| spouse            = \n}}\'\'\'Ronald Jay "Ron" Brachman\'\'\' (born 1949) is the director of the Jacobs Technion-Cornell Institute at [[Cornell Tech]].<ref>{{Cite web|url=http://tech.cornell.edu/news/ron-brachman-joins-the-jacobs-technion-cornell-institute-at-cornell-tech-as|title=Ron Brachman Joins the Jacobs Technion-Cornell Institute at Cornell Tech as the New Director|website=Cornell Tech|access-date=2016-05-25}}</ref> Previously, he was the Chief Scientist of Yahoo! and head of [[Yahoo! Labs]].  Prior to that, he was the Associate Head of Yahoo! Labs and Head of Worldwide Labs and Research Operations.\n\n==Education==\nBrachman earned his [[Bachelor of Engineering|B.S.E.E.]] degree from [[Princeton University]], and his [[Master of Science|S.M.]] and [[Doctor of Philosophy|Ph.D.]] degrees from [[Harvard University]].\n\n==Career==\nPrior to working at Yahoo!, Brachman worked at [[DARPA]] as the Director of the [[Information Processing Techniques Office]] (IPTO), one of DARPA\'s eight offices at the time.  While at IPTO, he helped develop [[DARPA]]\'s Cognitive Systems research efforts. Before that, he worked at [[AT&T Corporation|AT&T]] [[Bell Labs|Bell Laboratories]] ([[Murray Hill, New Jersey]]) as the Head of the [[Artificial Intelligence]] Principles Research Department (2004) and Director of the Software and Systems Research Laboratory.  When AT&T split with Lucent in 1996, he became Communications Services Research Vice President and was one of the founders of [[AT&T Labs]].\n\nHe is considered by some to be the godfather{{citation needed|date=August 2012}} of [[Description Logic]], the logic-based [[knowledge representation]] [[Semantics (computer science)|formalism]] underlying the [[Web Ontology Language]] OWL.]\n\n==Publications==\nHe is the co-author with [[Hector Levesque]] of a popular book on [[knowledge representation and reasoning]]<ref>{{cite book |author1=Reiter, Ray |author2=Brachman, Ronald J. |author3=Levesque, Hector J. |title=Knowledge representation |publisher=MIT Press |location=Cambridge, Mass |year=1992 |pages= |isbn=0-262-52168-7 |oclc= |doi= |accessdate=}}</ref><ref>{{cite book |author1=Levesque, Hector J. |author2=Brachman, Ronald J. |title=Knowledge representation and reasoning |publisher=Elsevier/Morgan Kaufmann |location=Amsterdam |year=2004 |pages= |isbn=1-55860-932-6 |oclc= |doi= |accessdate=}}</ref> and many scientific papers.<ref name="microsoft">{{AcademicSearch|9029466}}</ref><ref name="dblp">{{DBLP|name=Ronald J. Brachman}}</ref><ref>Ronald J. Brachman (1983) "What IS-A is and isn\'t. An Analysis of Taxonomic Links in [[Semantic network|Semantic Networks]]"; \'\'IEEE Computer\'\', 16 (10); October.</ref>\n\n==References==\n{{reflist}}\n\n== External links ==\n* [http://www.cc.gatech.edu/events/dr-ronald-brachman-yahoo-research-distinguished-guest-lecture External biography]\n\n{{DEFAULTSORT:Brachman, Ronald J.}}\n[[Category:Living people]]\n[[Category:Artificial intelligence researchers]]\n[[Category:Knowledge representation]]\n[[Category:Harvard University alumni]]\n[[Category:Princeton University alumni]]\n[[Category:Fellow Members of the IEEE]]\n[[Category:Fellows of the Association for the Advancement of Artificial Intelligence]]\n[[Category:Yahoo! employees]]\n[[Category:1959 births]]\n\n\n{{compu-bio-stub}}']
['Semantic knowledge management', '20298912', "{{Orphan|date=February 2009}}\n'''Semantic knowledge management''' is a set of practices that seeks to classify content so that the knowledge it contains may be immediately accessed and transformed for delivery to the desired audience, in the required format. This classification of content is semantic in its nature &ndash; identifying content by its type or meaning within the content itself and via external, descriptive metadata – and is achieved by employing [[XML]] technologies.\n\nThe specific outcomes of these practices are:\n\n* Maintain content for multiple audiences together in a single document \n* Transform content into various delivery formats without re-authoring  \n* Search for content more effectively \n* Involve more [[subject-matter expert]]s in the creation of content without reducing quality \n* Reduce production costs for delivery formats \n* Reduce the manual administration of getting the right knowledge to the right people \n* Reduce the cost and time to localize content\n\n==References==\n{{refbegin}}\n* {{cite book|title=Semantic Knowledge Management: Integrating Ontology Management, Knowledge Discovery, and Human Language Technologies|author1=John Davies |author2=Marko Grobelnik |author3=Dunja Mladenic |isbn=3-540-89164-1|year=2008}}\n{{refend}}\n\n== Notable semantic knowledge management systems ==\n*Learn eXact\n*Thinking Cap LCMS\n*Thinking Cap LMS\n*Xyleme LCMS\n\n[[Category:Knowledge representation]]"]
['Paradigm classification', '21862082', "{{Multiple issues|\n{{Orphan|date=October 2015}}\n{{no footnotes|date=April 2013}}\n}}\n\n'''Paradigm classification''' in [[ontology]] is a two-dimensional classification scheme, such as a spreadsheet. It is a subset of [[faceted classification]].\n\n== Overview ==\nParadigm classification deals with the large subset of faceted classification where an item may be classified within two dimensions. Examples might include [[genealogy]], where individuals are classified by their gender and relations with other individuals.\n\n== References ==\n{{reflist|2}}\n\n== External links ==\n{{Commons category|Ontology}}\n*[http://www.miskatonic.org/library/facet-web-howto.html How to Make a Faceted Classification and Put It On the Web]\n*[http://annotalia.com/philosophy/ontology Ontology]\n\n{{Philosophy topics}}\n\n[[Category:Knowledge representation]]\n[[Category:Ontology]]\n\n\n{{Ontology-stub}}"]
['Framing (social sciences)', '10438439', '{{globalize|date=July 2010}}\n\nIn the [[social sciences]], \'\'\'framing\'\'\' comprises a set of concepts and theoretical perspectives on how individuals, groups, and societies, organize, perceive, and communicate about [[reality]]. Framing involves [[social construction]] of a [[social phenomenon]] – by [[mass media]] sources, political or social movements, political leaders, or other actors and organizations. Participation in a language community necessarily influences an individual\'s \'\'[[perception]]\'\' of the meanings attributed to words or phrases. Politically, the language communities of [[advertising]], [[religion]], and mass media are highly contested, whereas framing in less-sharply defended [[speech community|language communities]] might evolve imperceptibly and organically over [[cultural]] time frames, with fewer overt modes of disputation.  \n\nFraming itself can be framed in one of two ways, depending on whether one chooses to emphasise processes of [[cognition|thought]] or processes of interpersonal [[communication]]. \'\'Frames in thought\'\' consist of the mental representations, interpretations, and simplifications of reality. \'\'Frames in communication\'\' consist of the communication of frames between different actors.<ref name="Druckman2001">{{cite journal | last1 = Druckman | first1 = J.N. | year = 2001 | title = The Implications of Framing Effects for Citizen Competence | url = | journal = Political Behavior | volume = 23 | issue = 3| pages = 225–256 | doi=10.1023/A:1015006907312}}</ref>\n\nOne can view framing in communication as positive or negative – depending on the audience and what kind of information is being presented. Framing might also be understood as being either \'\'equivalence frames\'\', which represent logically equivalent alternatives portrayed in different ways (see [[framing effect (psychology)|framing effect]]) or as \'\'emphasis frames\'\', which simplify reality by focusing on a subset of relevant aspects of a situation or issue.<ref name="Druckman2001" /> In the case of  "equivalence frames", the information being presented is based on the same facts, but the "frame" in which it is presented changes, thus creating a reference-dependent perception.\n\nThe effects of framing can be seen in many journalism applications. With the same information being used as a base, the "frame" surrounding the issue can change the reader\'s perception without having to alter the actual facts. In the context of politics or mass-media communication, a frame defines the packaging of an element of [[rhetoric]] in such a way as to encourage certain interpretations and to discourage others.  For political purposes, framing often presents facts in such a way that implicates a problem that is in need of a solution. Members of political parties attempt to frame issues in a way that makes a solution favoring their own political leaning appear as the most appropriate course of action for the situation at hand.<ref name="van der Pas">{{cite journal|last=van der Pas|first=D.|title=Making Hay While the Sun Shines: Do Parties Only Respond to Media Attention When The Framing is Right?|journal=Journal of Press/Politics|year=2014|volume=19|issue=1|pages=42–65|doi=10.1177/1940161213508207}}<!--|accessdate=6 March 2014--></ref>\n\nIn [[social theory]], framing is a [[Schema (psychology)|schema]] of [[interpretation (logic)|interpretation]], a collection of [[Anecdotal evidence|anecdotes]] and [[stereotype]]s, that individuals rely on to understand and respond to events.<ref name="Goffman1974">\nGoffman, E. (1974). Frame analysis: An easy on the organization of experience. Cambridge, MA: Harvard University Press. \n</ref> In other words, people build a series of mental "filters" through biological and cultural influences. They then use these filters to make sense of the world. The choices they then make are influenced by their creation of a frame.\n\nFraming is also a key component of [[sociology]], the study of social interaction among humans.  Framing is an integral part of conveying and processing data on a daily basis.  Successful framing techniques can be used to reduce the ambiguity of intangible topics by contextualizing the information in such a way that recipients can connect to what they already know.\n\n== Explanation ==\nWhen one seeks to explain an event, the understanding often depends on the frame referred to. If a friend rapidly closes and opens an eye, we will respond very differently depending on whether we attribute this to a purely "physical" frame (they blinked) or to a social frame (they winked).\n\nThough the former might result from a speck of dust (resulting in an involuntary and not particularly meaningful reaction), the latter would imply a voluntary and meaningful action (to convey humor to an accomplice, for example). Observers will read events seen as purely physical or within a frame of "nature" differently from those seen as occurring with social frames. But we do not look at an event and then "apply" a frame to it. Rather, individuals constantly project into the world around them the interpretive frames that allow them to make sense of it; we only shift frames (or realize that we have habitually applied a frame) when incongruity calls for a frame-shift. In other words, we only become aware of the frames that we always already use when something forces us to replace one frame with another.<ref>\nThis example borrowed from Clifford Geertz: \'\'Local Knowledge: Further Essays in Interpretive Anthropology\'\' (1983), Basic Books 2000 paperback: ISBN 0-465-04162-0\n</ref><ref>\nGoffman offers the example of the woman bidding on a mirror at an auction who first examines the frame and surface for imperfections, and then "checks" herself in the mirror and adjusts her hat. See Goffman, Erving. \'\'Frame Analysis: An essay on the organization of experience\'\'. Boston: Northeastern University Press, 1986. ISBN 0-930350-91-X, page 39. In each case the mirror represents more than simply a physical object.\n</ref>\n\nFraming is so effective because it is a heuristic, or mental shortcut that may not always yield desired results; and is seen as a \'rule of thumb\'. According to Susan T. Fiske and Shelley E. Taylor, human beings are by nature "cognitive misers", meaning they prefer to do as little thinking as possible.<ref>Fiske, S. T., & Taylor, S. E. (1991). Social cognition (2nd ed.). New York: McGraw-Hill</ref> Frames provide people a quick and easy way to process information. Hence, people will use the previously mentioned mental filters (a series of which is called a schema) to make sense of incoming messages. This gives the sender and framer of the information enormous power to use these schemas to influence how the receivers will interpret the message.<ref name="EntmanRobertTree">Entman,Robert "Tree Beard". Framing: Toward Clarification of a Fractured Paradigm. Journal of Communication; Autumn 1993, 43, 4, p.51</ref>\n\nThough some consider framing to be synonymous with [[Agenda-setting theory|agenda setting]], other scholars state that there is a distinction. According to an article written by Donald H. Weaver, framing selects certain aspects of an issue and makes them more prominent in order to elicit certain interpretations and evaluations of the issue, whereas agenda setting introduces the issue topic to increase its salience and accessibility.<ref>{{Cite journal|last=Weaver|first=David H.|title=Thoughts on Agenda Setting, Framing, and Priming|journal=Journal of Communication|volume=57}}</ref>\n\n==Framing effect in communication research==\nIn the field of communication, framing defines how news media coverage shapes [[mass opinion]]. [[Richard Vatz|Richard E. Vatz\'s]] discourse on creation of rhetorical meaning relates directly to framing, although he references it little.  To be specific, framing effects refer to behavioral or attitudinal strategies and/or outcomes that are due to how a given piece of information is being framed in [[public discourse]]. Today, many volumes of the major communication journals contain papers on media frames and framing effects.<ref>Scheufele, D. A. & Iyengar, S. (forthcoming). The state of framing research: A call for new directions. In K. kENSKI, & K. H. Jamieson (Eds.), The Oxford Handbook of political communication theories. New York: Oxford University Press.</ref> Approaches used in such papers can be broadly classified into two groups: studies of framing as the dependent variable and studies of framing as the independent variable.<ref>Tewksbury & Scheufele (2009). News framing theory and research, In J. Bryant, & M. B. Oliver (Eds.) Media effects: Advances in theory and research, New York: Routledge.</ref> The former usually deals with \'\'frame building\'\' (i.e. how frames create societal discourse about an issue and how different frames are adopted by journalists) and latter concerns \'\'frame setting\'\' (i.e. how media framing influences an audience).\n\n===Frame building===\nFrame building is related to at least three areas: journalist norms, political actors, and cultural situations. It assumes that several media frames compete to set one frame regarding an issue, and one frame finally gains influence because it resonates with [[popular culture]], fits with media practices, or is heavily sponsored by [[elite]]s. \nFirst, in terms of practices of news production, there are at least five aspects of news work that may influence how journalists frame a certain issue: larger societal norms and values, organizational pressures and constraints, external pressures from [[interest group]]s and other [[policy maker]]s, professional routines, and ideological or political orientations of journalists. The second potential influence on frame building comes from elites, including interest groups, government bureaucracies, and other political or corporate actors. Empirical studies show that these influences of elites seem to be strongest for issues in which journalists and various players in the policy arena can find shared narratives. Finally, cultural contexts of a society are also able to establish frame. Goffman<ref name="Goffman1974"/> assumes that the meaning of a frame has implicit cultural roots. This context dependency of media frame has been described as \'cultural resonance\'<ref>Gamson, W. A. & Modigliani, A. (1987) The changing culture of affirmative action. Research in Political Sociology, 3, 137-177</ref> or \'narrative fidelity\'.<ref name="SnowBenford1988">Snow, D. A., & Benford, R. D. (1988). Ideology, frame resonance, and participant mobilization. In B. Klandermans, H. Kriesi, & S. Tarrow (Eds.), International social movement research. Vol 1, From structure on action: Comparing social movement research across cultures (pp. 197-217). Greenwich, CT: JAI Press.</ref>\n\n===Frame setting===\nWhen people are exposed to a novel news frame, they will accept the constructs made applicable to an issue, but they are significantly more likely to do so when they have existing schema for those constructs. This is called the applicability effect. That is, when new frames invite people to apply their existing schema to an issue, the implication of that application depends, in part, on what is in that schema. Therefore, generally, the more the audiences know about issues, the more effective are frames.\n\nThere are a number of levels and types of framing effects that have been examined. For example, scholars have focused on attitudinal and behavioral changes, the degrees of perceived importance of the issue, voting decisions, and opinion formations. Others are interested in psychological processes other than applicability. For instance, Iyengar<ref>Iyengar, S. (1991). Is anyone responsible? How television frames political issues. Chicago: University of Chicago Press.</ref> suggested that news about social problems can influence attributions of causal and treatment responsibility, an effect observed in both cognitive responses and evaluations of political leaders, or other scholars looked at the framing effects on receivers\' evaluative processing style and the complexity of audience members\' thoughts about issues.\n\n==In mass communication research==\nNews media frame all news items by emphasizing specific values, facts, and other considerations, and endowing them with greater apparent applicability for making related judgments.  News media promotes particular definitions, interpretations, evaluations and recommendations.<ref name=Entman1993>{{cite journal|last=Entman|first=R.M.|title=Framing: Toward clarification of a fractured paradigm|journal=Journal of Communication|year=1993|volume=43|issue=4|pages=51–58|doi=10.1111/j.1460-2466.1993.tb01304.x}}</ref><ref name=NelsonClawsonOxley1997>{{cite journal|last=Nelson|first=T.E.|author2=Clawson, R.A. |author3=Oxley, Z.M. |title=Media framing of a civil liberties conflict and its effect on tolerance|journal=American Political Science Review|year=1997|volume=91|issue=3|pages=567–583|doi=10.2307/2952075}}</ref>\n\n===Foundations in mass communication research===\n\nAnthropologist [[Gregory Bateson]] first articulated the concept of framing in his 1972 book \'\'[[Steps to an Ecology of Mind]]\'\'.  A frame, Bateson wrote, is "a spatial and temporal bounding of a set of interactive messages."<ref name=Bateson1972>{{cite book|last=Bateson|first=G.|title=Steps to an Ecology of Mind|year=1972|publisher=Ballantine Books|location=New York}}</ref>\n\n====Sociological roots of media framing research====\n\nMedia framing research has both sociological and psychological roots.  Sociological framing focuses on "the words, images, phrases, and presentation styles" that communicators use when relaying information to recipients.<ref name="Druckman2001" /> Research on frames in sociologically driven media research generally examines the influence of "social norms and values, organizational pressures and constraints, pressures of interest groups, journalistic routines, and ideological or political orientations of journalists" on the existence of frames in media content.<ref name=Scheufele2000>{{cite journal|last=Scheufele|first=D.A.|title=Agenda-setting, priming, and framing revisited: Another look at cognitive effects of political communication|journal=Mass Communication & Society|year=2000|volume=3|issue=2&3|pages=297–316|doi=10.1207/S15327825MCS0323_07}}</ref>\n\n[[Todd Gitlin]], in his analysis of how the news media trivialized the student [[New Left]] movement during the 1960s, was among the first to examine media frames from a sociological perspective.  Frames, Gitlin wrote, are "persistent patterns of cognition, interpretations, and presentation, of selection [and] emphasis ... [that are] largely unspoken and unacknowledged ... [and] organize the world for both journalists [and] for those of us who read their reports."<ref name=Gitlin1980>{{cite book|last=Gitlin|first=T.|title=The Whole World is Watching: Mass Media in the Making and Unmaking of the New Left|year=1980|publisher=University of California Press|location=Berkeley, CA}}</ref>\n\n====Psychological roots of media framing research====\n\nResearch on frames in psychologically driven media research generally examines the effects of media frames on those who receive them.  For example, Iyengar explored the impact of episodic and thematic news frames on viewers\' attributions of responsibility for political issues including crime, terrorism, poverty, unemployment, and racial inequality.<ref name=Iyengar1991>{{cite book|last=Iyengar|first=S.|title=Is Anyone Responsible? How Television Frames Political Issues|year=1991|publisher=University of Chicago Press|location=Chicago}}</ref> According to Iyengar, an episodic news frame "takes the form of a case study or event-oriented report and depicts public issues in terms of concrete instances," while a thematic news frame "places public issues in some more general abstract context ... directed at general outcomes or conditions."<ref name=Entman1993 /><ref name=Iyengar1991 /> Iyengar found that the majority of television news coverage of poverty, for example, was episodic.<ref name=Iyengar1991 />  In fact, in a content analysis of six years of television news, Iyengar found that the typical news viewer would have been twice as likely to encounter episodic rather than thematic television news about poverty.<ref name=Iyengar1991 />  Further, experimental results indicate participants who watched episodic news coverage of poverty were more than twice as likely as those who watched thematic news coverage of poverty to attribute responsibility of poverty to the poor themselves rather than society.<ref name=Iyengar1991 />  Given the predominance of episodic framing of poverty, Iyengar argues that television news shifts responsibility of poverty from government and society to the poor themselves.<ref name=Iyengar1991 />  After examining content analysis and experimental data on poverty and other political issues, Iyengar concludes that episodic news frames divert citizens\' attributions of political responsibility away from society and political elites, making them less likely to support government efforts to address those issue and obscuring the connections between those issues and their elected officials\' actions or lack thereof.<ref name=Iyengar1991 />\n\n===Clarifying and distinguishing a "fractured paradigm"===\n\nPerhaps because of their use across the social sciences, frames have been defined and used in many disparate ways.  Entman called framing "a scattered conceptualization" and "a fractured paradigm" that "is often defined casually, with much left to an assumed tacit understanding of the reader."<ref name=Entman1993 /> In an effort to provide more conceptual clarity, Entman suggested that frames "select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described."<ref name=Entman1993 />\n\nEntman\'s<ref name=Entman1993 />  conceptualization of framing, which suggests frames work by elevating particular pieces of information in salience, is in line with much early research on the psychological underpinnings of framing effects (see also Iyengar,<ref name=Iyengar1991 />  who argues that accessibility is the primary psychological explanation for the existence of framing effects).  Wyer and Srull<ref name=WyerSrull1984 />  explain the construct of accessibility thus:\n# People store related pieces of information in "referent bins" in their long-term memory.<ref name=WyerSrull1984>{{cite book|last=Wyer, Jr.|first=R.S.|title=Social Cognition: The Ontario Symposium|year=1984|publisher=Lawrence Erlbaum|location=Hillsdale, NJ|author2=Srull, T.K.|editor=E.T. Higgins |editor2=N.A. Kuiper |editor3=M.P Zanna (Eds.)|chapter=Category Accessibility: Some theoretic and empirical issues concerning the processing of social stimulus information}}</ref> \n# People organize "referent bins" such that more frequently and recently used pieces of information are stored at the top of the bins and are therefore more accessible.<ref name=WyerSrull1984 /> \n# Because people tend to retrieve only a small portion of information from long-term memory when making judgments, they tend to retrieve the most accessible pieces of information to use for making those judgments.<ref name=WyerSrull1984 />\n\nThe argument supporting accessibility as the psychological process underlying framing can therefore be summarized thus: Because people rely heavily on news media for public affairs information, the most accessible information about public affairs often comes from the public affairs news they consume.  The argument supporting accessibility as the psychological process underlying framing has also been cited as support in the debate over whether framing should be subsumed by [[agenda-setting theory]] as part of the second level of agenda setting.  McCombs and other agenda-setting scholars generally agree that framing should be incorporated, along with [[Priming (media)|priming]], under the umbrella of agenda setting as a complex model of media effects linking media production, content, and audience effects.<ref name=Kosicki1993>{{cite journal|last=Kosicki|first=G.M.|title=Problems and opportunities in Agenda-setting research|journal=Journal of Communication|year=1993|volume=43|issue=2|pages=100–127|doi=10.1111/j.1460-2466.1993.tb01265.x}}</ref><ref name=McCombsShaw1993>{{cite journal|last=McCombs|first=M.E.|author2=Shaw, D.L.|title=The evolution of agenda-setting research: Twenty-five years in the marketplace of ideas|journal=Journal of Communication|year=1993|volume=43|issue=2|pages=58–67|doi=10.1111/j.1460-2466.1993.tb01262.x}}</ref><ref name=McCombsLlamasLopez-EscobarRey1997 />  Indeed, McCombs, Llamas, Lopez-Escobar, and Rey justified their attempt to combine framing and agenda-setting research on the assumption of parsimony.<ref name=McCombsLlamasLopez-EscobarRey1997>{{cite journal|last=McCombs|first=M.F.|author2=Llamas, J.P. |author3=Lopez-Escobar, E. |author4=Rey, F. |title=Candidate images in Spanish elections: Second-level agenda-setting effects|journal=Journalism & Mass Communication Quarterly|year=1997|volume=74|pages=703–717|doi=10.1177/107769909707400404|issue=4}}</ref>\n\nScheufele, however, argues that, unlike agenda setting and priming, framing does not rely primarily on accessibility, making it inappropriate to combine framing with agenda setting and priming for the sake of parsimony.<ref name=Scheufele2000 /> Empirical evidence seems to vindicate Scheufele\'s claim.  For example, Nelson, Clawson, and Oxley empirically demonstrated that applicability, rather than their salience, is key.<ref name="NelsonClawsonOxley1997" /> By operationalizing accessibility as the response latency of respondent answers where more accessible information results in faster response times, Nelson, Clawson, and Oxley demonstrated that accessibility accounted for only a minor proportion of the variance in framing effects while applicability accounted for the major proportion of variance.<ref name="NelsonClawsonOxley1997" /> Therefore, according to Nelson and colleagues, "frames influence opinions by stressing specific values, facts, and other considerations, endowing them with greater apparent relevance to the issue than they might appear to have under an alternative frame."<ref name="NelsonClawsonOxley1997" />\n\nIn other words, while early research suggested that by highlighting particular aspects of issues, frames make certain considerations more accessible and therefore more likely to be used in the judgment process,<ref name=Entman1993 /><ref name=Iyengar1991 />  more recent research suggests that frames work by making particular considerations more applicable and therefore more relevant to the judgment process.<ref name="NelsonClawsonOxley1997" /><ref name=Scheufele2000 />\n\n===Equivalency versus emphasis: two types of frames in media research===\n\nChong and Druckman suggest framing research has mainly focused on two types of frames: equivalency and emphasis frames.<ref name=ChongDruckman2007>{{cite journal|last=Chong|first=D.|author2=Druckman, J.N. |title=Framing theory|journal=Annual Review of Political Science|year=2007|volume=10|pages=103–126|doi=10.1146/annurev.polisci.10.072805.103054}}</ref>  Equivalency frames offer "different, but logically equivalent phrases," which cause individuals to alter their preferences.<ref name="Druckman2001" /> Equivalency frames are often worded in terms of "gains" versus "losses."  For example, Kahneman and Tversky asked participants to choose between two "gain-framed" policy responses to a hypothetical disease outbreak expected to kill 600 people.<ref name=KahnemanTversky1984>{{cite journal |last=Kahneman |first=D.|author2=Tversky, A.|title=Choices, values, and frames|journal=American Psychologist |year=1984|volume=39|issue=4|pages=341–350 |doi=10.1037/0003-066X.39.4.341}}</ref>  Response A would save 200 people while Response B had a one-third probability of saving everyone, but a two-thirds probability of saving no one.  Participants overwhelmingly chose Response A, which they perceived as the less risky option. Kahneman and Tversky asked other participants to choose between two equivalent "loss-framed" policy responses to the same disease outbreak.  In this condition, Response A would kill 400 people while Response B had a one-third probability of killing no one but a two-thirds probability of killing everyone.  Although these options are  mathematically identical to those given in the "gain-framed" condition, participants overwhelmingly chose Response B, the risky option.  Kahneman and Tversky, then, demonstrated that when phrased in terms of potential gains, people tend to choose what they perceive as the less risky option (i.e., the sure gain).  Conversely, when faced with a potential loss, people tend to choose the riskier option.<ref name=KahnemanTversky1984 />\n\nUnlike equivalency frames, emphasis frames offer "qualitatively different yet potentially relevant considerations" which individuals use to make judgments.<ref name=ChongDruckman2007 /> For example, Nelson, Clawson, and Oxley exposed participants to a news story that presented the [[Ku Klux Klan]]\'s plan to hold a rally.<ref name="NelsonClawsonOxley1997" />  Participants in one condition read a news story that framed the issue in terms of public safety concerns while participants in the other condition read a news story that framed the issue in terms of free speech considerations.  Participants exposed to the public safety condition considered public safety applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed lower tolerance of the Klan\'s right to hold a rally.<ref name="NelsonClawsonOxley1997" />  Participants exposed to the free speech condition, however, considered free speech applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed greater tolerance of the Klan\'s right to hold a rally.<ref name="NelsonClawsonOxley1997" />\n\n==Framing effect in psychology and economics==\n[[File:Daniel KAHNEMAN.jpg|thumb|180px|[[Daniel Kahneman]]]]\n{{Main|Framing effect (psychology)}}\n[[Amos Tversky]] and [[Daniel Kahneman]] have shown that framing can affect the outcome (i.e. the choices one makes) of choice problems, to the extent that several of the classic axioms of [[rational choice]] do not hold.<ref name="TverskyKahneman1981">{{cite journal | last1 = Tversky | first1 = Amos | last2 = Kahneman | first2 = Daniel | year = 1981 | title = The Framing of Decisions and the Psychology of Choice | url = | journal = Science | volume = 211 | issue = 4481| pages = 453–458 | doi = 10.1126/science.7455683 | pmid = 7455683 }}</ref> This led to the development of [[prospect theory]] as an alternative to rational choice theory.<ref>Econport. "Decision-Making Under Uncertainty - Advanced Topics: An Introduction to Prospect Theory". (EconPort is an economics digital library specializing in content that emphasizes the use of experiments in teaching and research.) [http://www.econport.org/econport/request?page=man_ru_advanced_prospect]</ref>\n\nThe context or framing of problems adopted by decision-makers results in part from extrinsic manipulation of the decision-options offered, as well as from forces intrinsic to decision-makers, e.g., their norms, habits, and unique [[temperament]].\n\n===Experimental demonstration===\nTversky and Kahneman (1981) demonstrated systematic [[preference reversal|reversals of preference]] when the same problem is presented in different ways, for example in the Asian disease problem. Participants were asked to "imagine that the U.S. is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume the exact scientific estimate of the consequences of the programs are as follows."\n\nThe first group of participants was presented with a choice between programs:\nIn a group of 600 people,\n* Program A: "200 people will be saved"\n* Program B: "there is a 1/3 probability that 600 people will be saved, and a 2/3 probability that no people will be saved"\n\n72 percent of participants preferred program A (the remainder, 28%, opting for program B).\n\nThe second group of participants was presented with the choice between the following:\nIn a group of 600 people,\n* Program C: "400 people will die"\n* Program D: "there is a 1/3 probability that nobody will die, and a 2/3 probability that 600 people will die"\n\nIn this decision frame, 78% preferred program D, with the remaining 22% opting for program C.\n\nPrograms A and C are identical, as are programs B and D. The change in the decision frame between the two groups of participants produced a preference reversal: when the programs were presented in terms of lives saved, the participants preferred the secure program, A (= C). When the programs were presented in terms of expected deaths, participants chose the gamble D (= B).<ref>{{Cite journal\n| last = Entman\n| first = R. M.\n| year = 1993\n| contribution = Framing: Toward Clarification of a Fractured Paradigm\n| periodical = [[Journal of Communication]]\n| volume = 43\n| issue = 4\n| pages = 51–58 [pp. 53–54] |doi=10.1111/j.1460-2466.1993.tb01304.x\n| postscript = <!--None-->\n}}</ref>\n\n===Absolute and relative influences===\nFraming effects arise because one can frequently frame a decision using multiple [[scenario]]s, wherein one may express benefits either as a relative risk reduction (RRR), or as absolute risk reduction (ARR). Extrinsic control over the cognitive distinctions (between [[risk tolerance]] and [[Incentive|reward anticipation]]) adopted by decision makers can occur through altering the presentation of [[relative risk]]s and [[Three degrees of comparison|absolute]] benefits.\n\nPeople generally prefer the absolute certainty inherent in a positive framing-effect, which offers an assurance of gains. When decision-options appear framed as a \'\'likely gain\'\', risk-averse choices predominate.\n\nA shift toward risk-seeking behavior occurs when a decision-maker frames decisions in negative terms, or adopts a negative framing effect.\n\nIn [[Decision-making|medical decision making]], [[framing bias]] is best avoided by using absolute measures of efficacy.<ref name="pmid21792695">{{cite journal|vauthors=Perneger TV, Agoritsas T | title=Doctors and Patients\' Susceptibility to Framing Bias: A Randomized Trial | journal=J Gen Intern Med | year= 2011 | volume= 26| issue= 12| pages= 1411–7| pmid=21792695 | doi=10.1007/s11606-011-1810-x | pmc= 3235613| url= }}</ref>\n\n===Frame-manipulation research===\nResearchers have found<ref name="TverskyKahneman1981" /> that framing decision-problems in a positive light generally results in less-risky choices; with negative framing of problems, riskier choices tend to result. According to [[behavioral economics|behavioral economist]]s{{Citation needed|date=November 2007}}:\n\n*positive framing effects (associated with [[risk aversion]]) result from presentation of options as sure (or absolute) gains\n*negative framing effects (associated with a preference shift toward choosing riskier options) result from options presented as the relative likelihood of losses\n\nResearchers have found{{Citation needed|date=October 2007}} that framing-manipulation invariably affects subjects, but to varying degrees. Individuals proved risk averse when presented with value-increasing options; but when faced with value decreasing contingencies, they tended towards increased risk-taking. Researchers {{Who|date=September 2008}} found that variations in decision-framing achieved by manipulating the options to represent either a gain or as a loss altered the risk-aversion preferences of decision-makers.\n\nIn one study, 57% of the subjects chose a medication when presented with benefits in relative terms, whereas only 14.7% chose a medication whose benefit appeared in absolute terms. Further questioning of the patients suggested that, because the subjects ignored the underlying risk of disease, they perceived benefits as greater when expressed in relative terms.<ref>[http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&list_uids=8271086&dopt=Abstract The framing effect of relative and absolute risk. [J Gen Intern Med. 1993&#93; - PubMed Result<!-- Bot generated title -->]</ref>\n\n===Theoretical models===\nResearchers have proposed<ref>Chong, D. and Druckman, J. N. (2007): Framing Theory, Annual Review of Political Science, vol. 10</ref><ref>Price, V., Tewksburg, D. and Powers, E. (1997): Switching Trains of Thought: The Impact of News Frames on Readers\' Cognitive Responses, Communication Research, Vol. 24 No. 5 s. 481 - 506</ref> various models explaining the \'\'\'framing effect\'\'\':\n\n*cognitive theories, such as the [[fuzzy-trace theory]], attempt to explain the framing-effect by determining the amount of cognitive processing effort devoted to determining the value of potential gains and losses.\n*[[prospect theory]] explains the framing-effect in functional terms, determined by preferences for differing perceived values, based on the assumption that people give a greater weighting to losses than to equivalent gains.\n*[[motivation]]al theories explain the framing-effect in terms of [[hedonic]] forces affecting individuals, such as fears and wishes—based on the notion that negative emotions evoked by potential losses usually out-weigh the emotions evoked by hypothetical gains.\n*cognitive [[Cost-benefit analysis|cost-benefit]] trade-off theory defines choice as a compromise between desires, either as a preference for a correct decision or a preference for minimized cognitive effort. This model, which dovetails elements of cognitive and motivational theories, postulates that calculating the value of a sure gain takes much less cognitive effort than that required to select a risky gain.\n\n===Neuroimaging===\nCognitive [[neuroscientist]]s have linked the framing-effect to neural activity in the [[amygdala]], and have identified another brain-region, the orbital and medial [[prefrontal cortex]] (OMPFC), that appears to moderate the role of [[emotion]] on decisions. Using [[functional magnetic resonance imaging]] (fMRI) to monitor brain-activity during a financial decision-making task, they observed greater activity in the OMPFC of those research subjects less susceptible to the framing-effect.<ref>{{cite journal | last1 = De Martino | first1 = B. | last2 = Kumaran | first2 = D. | last3 = Seymour | first3 = B. | last4 = Dolan | first4 = R. J. | year = 2006 | title = Frames, biases, and rational decision-making in the human brain | url = | journal = Science | volume = 313 | issue = 5787| pages = 684–687 | doi = 10.1126/science.1128356 | pmid = 16888142 | pmc = 2631940 }}</ref>\n\n==Framing theory and frame analysis in sociology==\nFraming theory and frame analysis provide a broad theoretical approach that analysts have used in [[communication studies]], [[journalism|news]] (Johnson-Cartee, 1995), politics, and [[social movement]]s (among other applications).\n\nAccording to some sociologists, the "social construction of collective action frames" involves "public discourse, that is, the interface of media discourse and interpersonal interaction; persuasive communication during mobilization campaigns by movement organizations, their opponents and countermovement organizations; and consciousness raising during episodes of collective action."<ref>\nBert Klandermans. 1997. \'\'The Social Psychology of Protest\'\'. Oxford: Blackwell, page 45\n</ref>\n\n===History===\n[[diction|Word-selection]] or diction has been a component of [[rhetoric]] since time immemorial. But most commentators attribute the concept of framing to the work of [[Erving Goffman]] on [[frame analysis]] and point especially to his 1974 book, \'\'Frame analysis: An essay on the organization of experience\'\'. Goffman used the idea of frames to label "schemata of interpretation" that allow individuals or groups "to locate, perceive, identify, and label" events and occurrences, thus rendering meaning, organizing experiences, and guiding actions.<ref>\nErving Goffman (1974). \'\'Frame Analysis: An essay on the organization of experience\'\'. Cambridge: Harvard University Press, 1974, page 21.\n</ref>\nGoffman\'s framing concept evolved out of his 1959 work, \'\'[[The Presentation of Self in Everyday Life]]\'\', a commentary on the [[management]] of [[Impression management|impression]]s. These works arguably depend on [[Kenneth Boulding]]\'s concept of image.<ref>\nKenneth Boulding: \'\'The Image: Knowledge in Life and Society\'\', University of Michigan Press, 1956)\n</ref>\n\n===Social movements===\nSociologists have utilized framing to explain the process of [[social movement]]s.<ref name="SnowBenford1988" />\nMovements act as carriers of beliefs and ideologies (compare [[meme]]s). In addition, they operate as part of the process of constructing meaning for participants and opposers (Snow & Benford, 1988). Sociologists deem the mobilization of mass-movements "successful" when the frames projected align with the frames of participants to produce resonance between the two parties. Researchers of framing speak of this process as \'\'frame re-alignment\'\'.\n\n===Frame-alignment===\nSnow and Benford (1988) regard frame-alignment as an important element in social mobilization or movement. They argue that when individual frames become linked in congruency and complementariness, "frame alignment" occurs,<ref name="Snowetal1986">\nSnow, D. A., Rochford, E. B., Worden, S. K., & Benford, R. D. (1986). Frame alignment processes, micromobilization, and movement participation. American Sociological Review, 51, page 464\n</ref>\nproducing "frame resonance", a catalyst in the process of a group making the transition from one frame to another (although not all framing efforts prove successful). The conditions that affect or constrain framing efforts include the following:\n\n*"The robustness, completeness, and thoroughness of the framing effort". Snow and Benford (1988) identify three core framing-tasks, and state that the degree to which framers attend to these tasks will determine participant mobilization. They characterize the three tasks as the following:\n*#diagnostic framing for the identification of a problem and assignment of blame\n*#prognostic framing to suggest solutions, strategies, and tactics to a problem\n*#motivational framing that serves as a call to arms or rationale for action\n*The relationship between the proposed frame and the larger [[belief system|belief-system]]; centrality: the frame cannot be of low hierarchical significance and salience within the larger belief system. Its range and interrelatedness, if the framer links the frame to only one core belief or value that, in itself, has a limited range within the larger belief system, the frame has a high degree of being discounted.\n*Relevance of the frame to the realities of the participants; a frame must seem relevant to participants and must also inform them. Empirical credibility or testability can constrain relevancy: it relates to participant experience, and has narrative fidelity, meaning that it fits in with existing cultural myths and narrations.\n*[[Protest cycle|Cycles of protest]] (Tarrow 1983a; 1983b); the point at which the frame emerges on the timeline of the current era and existing preoccupations with social change. Previous frames may affect efforts to impose a new frame.\n\nSnow and Benford (1988) propose that once someone has constructed proper frames as described above, large-scale changes in society such as those necessary for social movement can be achieved through frame-alignment.\n\n====Types====\nFrame-alignment comes in four forms: frame bridging, frame amplification, frame extension and frame transformation.\n\n#\'\'Frame bridging\'\' involves the "linkage of two or more ideologically congruent but structurally unconnected frames regarding a particular issue or problem" (Snow et al., 1986, p.&nbsp;467). It involves the linkage of a movement to "unmobilized [\'\'[[sic]]\'\'] sentiment pools or public opinion preference clusters" (p.&nbsp;467) of people who share similar views or grievances but who lack an organizational base.\n#\'\'Frame amplification\'\' refers to "the clarification and invigoration of an interpretive frame that bears on a particular issue, problem, or set of events" (Snow et al., 1986, p.&nbsp;469). This interpretive frame usually involves the invigorating of values or beliefs.\n#\'\'Frame extensions\'\' represent a movement\'s effort to incorporate participants by extending the boundaries of the proposed frame to include or encompass the views, interests, or sentiments of targeted groups (Snow et al., 1986, p.&nbsp;472).\n#\'\'Frame transformation\'\' becomes necessary when the proposed frames "may not resonate with, and on occasion may even appear antithetical to, conventional lifestyles or rituals and extant interpretive frames" (Snow et al., 1986, p.&nbsp;473).\n\nWhen this happens, the securing of participants and support requires new values, new meanings and understandings. Goffman (1974, p.&nbsp;43–44) calls this "keying", where "activities, events, and biographies that are already meaningful from the standpoint of some primary framework, in terms of another framework" (Snow et al., 1986, p.&nbsp;474) such that they are seen differently. Two types of frame transformation exist:\n\n#Domain-specific transformations, such as the attempt to alter the status of groups of people, and\n#Global interpretive frame-transformation, where the scope of change seems quite radical—as in a change of [[world view|world-views]], total conversions of thought, or uprooting of everything familiar (for example: moving from [[communism]] to [[market capitalism]], or vice versa; [[religious conversion]], etc.).\n\n==Frame analysis as rhetorical criticism==\nAlthough the idea of language-framing had been explored earlier by [[Kenneth Burke]] (terministic screens), political communication researcher [[Jim A. Kuypers]] first published work advancing [[frame analysis]] (framing analysis) as a rhetorical perspective in 1997. His approach begins inductively by looking for themes that persist across time in a text (for Kuypers, primarily news narratives on an issue or event) and then determining how those themes are framed. Kuypers\'s work begins with the assumption that frames are powerful rhetorical entities that "induce us to filter our perceptions of the world in particular ways, essentially making some aspects of our multi-dimensional reality more noticeable than other aspects. They operate by making some information more salient than other information...."<ref>\nJim A. Kuypers, "Framing Analysis" in \'\'Rhetorical Criticism: Perspectives in Action\'\', edited by J.A. Kuypers,  Lexington Press, 2009. p. 181.</ref>\n\nIn his 2009 essay "Framing Analysis" in \'\'Rhetorical Criticism: Perspectives in Action\'\'<ref>\n\'\'Rhetorical Criticism: Perspectives in Action\'\'</ref> and his 2010 essay "Framing Analysis as a Rhetorical Process",<ref>Kuypers, Jim A. "Framing Analysis as a Rhetorical Process," Doing News Framing Analysis.  Paul D\'Angelo and  Jim A. Kuypers, eds. (New York: Routeledge, 2010).</ref> Kuypers offers a detailed conception for doing framing analysis from a rhetorical perspective. According to Kuypers, "Framing is a process whereby communicators, consciously or unconsciously, act to construct a point of view that encourages the facts of a given situation to be interpreted by others in a particular manner. Frames operate in four key ways: they define problems, diagnose causes, make moral judgments, and suggest remedies. Frames are often found within a narrative account of an issue or event, and are generally the central organizing idea."<ref>Jim A. Kuypers, \'\'Bush\'s War: Media Bias and Justifications for War in a Terrorist Age\'\', Rowman & Littlefield Publishers, Inc., 2009.</ref> Kuypers\'s work is based on the premise that framing is a rhetorical process and as such it is best examined from a rhetorical point of view. Curing the problem is not rhetorical and best left to the observer.\n\n==Rhetorical framing in politics==\n\n===Semiotic analysis of 2016 Republican primaries===\n\nFraming is used to construct, refine, and deliver messages. Framing in politics is essential to getting your message across to the masses. Frames are mental structures that shape the way we view the world (Lakoff, Don\'t Think of an Elephant! Know Your Values and Frame the Debate 2004).<ref name=Lakoff>In Don\'t Think of an Elephant! Know Your Values and Frame the Debate, by George Lakoff, 144. Chelsea Green Publishing, 2004.</ref> Reframing is used particularly well by both conservatives and liberals in the political arena, so well that they have news anchors and commentators discussing the ideas, supplied phrases and framing (Lakoff, Don\'t Think of an Elephant! Know Your Values and Frame the Debate 2004).<ref name=Lakoff/> \n\nThe neoconservatives in the Bush Administration and the Pentagon viewed the 9/11 attack as an opportunity to go to war in the Middle East and finally take out Saddam Hussain. The Bush administration sold the war by convincing the nation that Iraq had WMDs and collected supportive evidence that they had Secretary of State Colin Powell present at the United Nations. The War on Terror was the label assigned by the Bush administration to its national security policy, launched in response to the attacks of 9/11 (Lewis 2009).<ref name="Lewis 2009">Lewis, Stephen D. Reese and Seth C. "Framing the War on Terror The internalization of policy in the US press." Journalism, 2009: 777–797.</ref> The cultural construction and political rationale supporting this slogan represent a powerful organizing principle that has become a widely accepted framing, laying the groundwork for the invasion of Iraq (Lewis 2009).<ref name="Lewis 2009" /> \n\nThe challenge of political violence has grown with new means of global coordination and access to weapons of mass destruction. The Bush administration\'s response to this threat, following the now iconic policy reference point of 11 September 2001, has had far-ranging implications for national security strategy, relations with the world community, and civil liberties (Lewis 2009).<ref name="Lewis 2009" /> Labeled the \'War on Terror\', the policy was framed within a phrase now part of the popular lexicon, becoming a natural and instinctive shorthand. More than phrases though, frames are \'organizing principles that are socially shared and persistent over time, that work symbolically to meaningfully structure the social world\' (Lewis 2009).<ref name="Lewis 2009" /> As a particularly powerful organizing principle, the War on Terror created a supportive political climate for what has been called the biggest US foreign policy blunder in modern times: the invasion of Iraq. Thus, in the scope and consequences of its policy-shaping impact, the War on Terror may be the most important frame in recent memory. (Lewis 2009)\n\nIn the now well-known evolution of the administration\'s policy, influential neoconservatives within the administration had advocated regime change in Iraq for some time, but the events of 9/11 gave them a compelling way to fast-track their ideas and justify a new policy of preemptive war, fist in Afghanistan and then in Iraq. The National Strategy for Combating Terrorism defined the attacks of 9/11 as \'acts of war against the United States of America and its allies, and against the very idea of civilized society\'. It identified the enemy as terrorism, an \'evil\' threatening our \'freedoms and our way of life. The related National Security Strategy of the United States of America clearly divides \'us\' from \'them\', linking terrorism to rogue states that \'hate the United States and everything for which it stands (Lewis 2009).<ref name="Lewis 2009" /> Presenting himself as God\'s agent, Bush\'s Manichean struggle pitted the USA and its leader against the evildoers (Lewis 2009).<ref name="Lewis 2009" /> \n\nThis argument is being played out in the 2016 Republican primaries, especially by Donald Trump. Trump has portrayed the Syrian refugees as foot soldiers for ISIS, coming to America to kill us in our main streets. Trump\'s rhetoric appears to be working; many middle class Americans are consuming his rhetoric.{{citation needed|date=June 2016}} The Americans that are supporting Trump and the Republicans in general, many of them are working class and the Republican agenda although it appears to be in their favor it is not. Framing their message to say one thing and mean something completely different is what the conservatives have become masters at. The 2016 Republican primary has been a knock down fight since it started in August 2015. Donald Trump has approached this contest as if Vince McMahon were the promoter and the rest of the field are a bunch of jobbers (persons who are paid to lose). Trump was inducted into the World Wrestling Entertainment (WWE) Hall of Fame in 2003. Even his attacks on Megan Kelly from FOX News are straight out of the WWE\'s playbook. Roland Barthes analyzed wrestling and boxing in his book \'\'Mythologies\'\'.\n<blockquote>\'\'This public knows very well the distinction between wrestling and boxing; it knows that boxing is a Jansenist sport, based on a demonstration of excellence. One can bet on the outcome of a boxing-match: with wrestling, it would make no sense. A boxing- match is a story which is constructed before the eyes of the spectator; in wrestling, on the contrary, it is each moment which is intelligible, not the passage of time... The logical conclusion of the contest does not interest the wrestling-fan, while on the contrary a boxing-match always implies a science of the future. In other words, wrestling is a sum of spectacles, of which no single one is a function: each moment imposes the total knowledge of a passion which rises erect and alone, without ever extending to the crowning moment of a result.\'\' (Legum 2015)<ref>{{cite web|author=Legum, Judd|title=This French Philosopher Is The Only One Who Can Explain The Donald Trump Phenomenon|work=thinkprogress.org|date=September 14, 2015|url=http://thinkprogress.org/politics/2015/09/14/3701084/donald-trump/|accessdate=April 23, 2016}}</ref></blockquote>\n\n==Applications==\n\n===Finance===\nPreference reversals and other associated phenomena are of wider relevance within behavioural economics, as they contradict the predictions of [[rational choice]], the basis of traditional economics. Framing biases affecting investing, lending, borrowing decisions make one of the themes of [[behavioral finance]].\n\n===Law===\n[[Edward Zelinsky]] has shown that framing effects can explain some observed behaviors of legislators.<ref>[[Edward Zelinsky|Zelinsky, Edward A.]]. 2005. Do Tax Expenditures Create Framing Effects? Volunteer Firefighters, Property Tax Exemptions, and the Paradox of Tax Expenditure Analysis. \'\'Virginia Tax Review\'\' 24. [http://www.allbusiness.com/accounting/3584666-1.html]</ref>\n\n===Media===\nThe role framing plays in the effects of media presentation has been widely discussed, with the central notion that associated perceptions of factual information can vary based upon the presentation of the information.\n\n====News media examples====\nIn \'\'Bush\'s War: Media Bias and Justifications for War in a Terrorist Age,\'\'<ref>Jim A. Kuypers, \'\'Bush\'s War: Media Bias and Justifications for War in a Terrorist Age\'\' (Lanham, MD: Rowman and Littlefield, 2006),</ref>[[Jim A. Kuypers]] examined the differences in framing of the war on terror between the Bush administration and the U.S. Mainstream News between 2001 and 2005.  Kuypers looked for common themes between presidential speeches and press reporting of those speeches, and then determined how the president and the press had framed those themes.  By using a rhetorical version of framing analysis, Kuypers determined that the U.S. news media advanced frames counter to those used by the Bush administration:\n\n{{quote| the press actively contested the framing of the War on Terror as early as eight weeks following 9/11. This finding stands apart from a collection of communication literature suggesting the press supported the President or was insufficiently critical of the President\'s efforts after 9/11. To the contrary, when taking into consideration how themes are framed, [Kuypers] found that the news media framed its response in such a way that it could be viewed as supporting the idea of some action against terrorism, while concommitantly opposing the initiatives of the President.  The news media may well relay what the president says, but it does not necessarily follow that it is framed in the same manner; thus, an echo of the theme, but not of the frame.  The present study demonstrates, as seen in Table One [below], that shortly after 9/11 the news media was beginning to actively counter the Bush administration and beginning to leave out information important to understanding the Bush Administration\'s conception of the War on Terror.  In sum, eight weeks after 9/11, the news media was moving beyond reporting political opposition to the President—a very necessary and invaluable press function—and was instead actively choosing themes, and framing those themes, in such a way that the President\'s focus was opposed, misrepresented, or ignored.<ref>Jim A. Kuypers, Stephen D. Cooper, Matthew T. Althouse, "George W. Bush, The American Press, and the Initial Framing of the War on Terror after 9/11," \'\'The George W. Bush Presidency: A Rhetorical Perspective,\'\' Robert E. Denton, ed. (Lanham, MD: Lexington Books, 2012), 89-112.</ref>}}\n\nTable One: Comparison of President and News Media Themes and Frames 8 Weeks after 9/11<ref>Jim A. Kuypers, Stephen D. Cooper, Matthew T. Althouse, "George W. Bush, "The American Press, and the Initial Framing of the War on Terror after 9/11," \'\'The George W. Bush Presidency: A Rhetorical Perspective,\'\' Robert E. Denton, ed. (Lanham, MD: Lexington Books, 2012), 105.</ref>\n\n{| class="wikitable"\n|-\n! Themes !! President\'s Frame !! Press Frame\n|-\n| Good v. Evil || Struggle of good and evil || Not mentioned\n|-\n| Civilization v. Barbarism || Struggle of civilization v. barbarism || Not mentioned\n|-\n| Nature of Enemy ||Evil, implacable, murderers || Deadly, indiscriminant\n\nBush Administration\n|-\n| Nature of War || Domestic/global/enduring\n\nWar\n || Domestic/global/longstanding\n\nWar or police action\n|-\n| Similarity to Prior Wars || Different Kind of War || WWII or Vietnam?\n|-\n| Patience || Not mentioned || Some, but running out\n|-\n| International Effort || Stated || Minimally reported\n|-\n\n|}\n\nIn 1991 Robert M. Entman published findings<ref>Entman, R. M. (1991), Symposium Framing U.S. Coverage of International News: Contrasts in Narratives of the KAL and Iran Air Incidents. Journal of Communication, 41: 6–27. {{DOI|10.1111/j.1460-2466.1991.tb02328.x}}</ref> surrounding the differences in media coverage between [[Korean Air Lines Flight 007]] and [[Iran Air Flight 655]]. After evaluating various levels of media coverage, based on both amount of airtime and pages devoted to similar events, Entman concluded that the frames the events were presented in by the media were drastically different:\n\n{{quote| By de-emphasizing the agency and the victims and by the choice of graphics and adjectives, the news stories about the U.S. downing of an Iranian plane called it a technical problem, while the Soviet downing of a Korean jet was portrayed as a moral outrage… [T]he contrasting news frames employed by several important U.S. media outlets in covering these two tragic misapplications of military force. For the first, the frame emphasized the moral bankruptcy and guilt of the perpetrating nation, for the second, the frame de-emphasized the guilt and focused on the complex problems of operating military high technology. }}\n\nDifferences in coverage amongst various media outlets:\n\n{| class="wikitable"\n|-\n! Amounts of Media coverage dedicated to each event !! Korean Air !! Iran Air\n|-\n| Time Magazine and Newsweek || 51 pages || 20 pages\n|-\n| CBS || 303 minutes || 204 minutes\n|-\n| New York Times || 286 stories || 102 stories\n|}\n\nIn 1988 Irwin Levin and Gary Gaeth did a study on the effects of framing attribute information on consumers before and after consuming a product (1988). In this study they found that in a study on beef. People who ate beef labeled as 75% lean rated it more favorably than people whose beef was labelled 25% fat.\n\n===Politics===\n\nLinguist and rhetoric scholar [[George Lakoff]] argues that, in order to persuade a political audience of one side of and argument or another, the facts must be presented through a rhetorical frame.  It is argued that, without the frame, the facts of an argument become lost on an audience, making the argument less effective.  The rhetoric of politics uses framing to present the facts surrounding an issue in a way that creates the appearance of a problem at hand that requires a solution.  Politicians using framing to make their own solution to an exigence appear to be the most appropriate compared to that of the opposition.<ref name="van der Pas" />  Counter-arguments become less effective in persuading an audience once one side has framed an argument, because it is argued that the opposition then has the additional burden of arguing the frame of the issue in addition to the issue itself.\n\nFraming a political issue, a political party or a political opponent is a [[strategy|strategic]] goal in [[politics]], particularly in the [[United States of America]]. Both the [[Democratic Party (United States)|Democratic]] and [[Republican Party (United States)|Republican]] political parties compete to successfully harness its power of persuasion. According to the \'\'[[New York Times]]\'\':\n\n{{quote|Even before the [[United States presidential election, 2004|election]], a new political word had begun to take hold of the party, beginning on the [[West Coast of the United States|West Coast]] and spreading like a virus all the way to the inner offices of the [[United States Capitol|Capitol]]. That word was \'framing.\' Exactly what it means to \'frame\' issues seems to depend on which Democrat you are talking to, but everyone agrees that it has to do with choosing the language to define a debate and, more important, with fitting individual issues into the contexts of broader story lines.|<ref name="framingwars">\n[http://www.nytimes.com/2005/07/17/magazine/17DEMOCRATS.html?pagewanted=1&ei=5070&en=e3e686efd4fa97c5&ex=1183608000 The Framing Wars. \'\'[[New York Times]]\'\' 17 July 2005]</ref>}}\n\nBecause framing has the ability to alter the public\'s perception, politicians engage in battles to determine how issues are framed. Hence, the way the issues are framed in the media reflects who is winning the battle. For instance, according to Robert Entman, professor of Communication at George Washington University, in the build-up to the Gulf War the conservatives were successful in making the debate whether to attack sooner or later, with no mention of the possibility of not attacking. Since the media picked up on this and also framed the debate in this fashion, the conservatives won.<ref name="EntmanRobertTree" />\n\nOne particular example of [[George Lakoff|Lakoff\'s]] work that attained some degree of fame was his advice to rename<ref>[[Walter Olson]], [http://www.overlawyered.com/2005/07/some_framing_advice.html Overlawyered weblog], 2005-07-18</ref> [[trial lawyer]]s (unpopular in the United States) as "public protection attorneys". Though Americans have not generally adopted this suggestion, the [[Association of Trial Lawyers of America]] did rename themselves the "American Association of Justice", in what the [[Chamber of Commerce]] called an effort to hide their identity.<ref>[[Al Kamen]], [http://www.washingtonpost.com/wp-dyn/content/article/2007/01/16/AR2007011601429_pf.html "Forget Cash -- Lobbyists Should Set Support for Lawmakers in Stone"], \'\'[[Washington Post]]\'\', 2007-01-17</ref>\n\nThe \'\'[[New York Times]]\'\' depicted similar intensity among Republicans:\n\n{{quote|In one recent memo, titled \'The 14 Words Never to Use,\' [[Frank Luntz|[Frank] Luntz]] urged conservatives to restrict themselves to phrases from what he calls ... the \'New American Lexicon.\' Thus, a smart Republican, in Luntz\'s view, never advocates \'[[oil drilling|drilling for oil]]\'; he prefers \'exploring for energy.\' He should never criticize the \'government,\' which cleans our streets and pays our firemen; he should attack \'[[Washington, D.C.|Washington]],\' with its ceaseless thirst for taxes and regulations. \'We should never use the word [[outsourcing]],\' Luntz wrote, \'because we will then be asked to defend or end the practice of allowing companies to ship American jobs overseas.\'|<ref name="framingwars"/>}}\n\nFrom a political perspective, framing has widespread consequences. For example, the concept of framing links with that of [[agenda setting theory|agenda-setting]]: by consistently invoking a particular frame, the framing party may effectively control discussion and perception of the issue. [[Sheldon Rampton]] and [[John Stauber]] in \'\'[[Trust Us, We\'re Experts]]\'\' illustrate how [[Public Relations|public-relations]] (PR) firms often use language to help frame a given issue, structuring the questions that then subsequently emerge. For example, one firm advises clients to use "bridging language" that uses a strategy of answering questions with specific terms or ideas in order to shift the discourse from an uncomfortable topic to a more comfortable one.<ref>\nRampton, Sheldon and Stauber, John. \'\'Trust Us, We\'re Experts!\'\' Putnam Publishing, New York, NY, 2002. Page 64.</ref>\nPractitioners of this strategy might attempt to draw attention away from one frame in order to focus on another. As Lakoff notes, "On the day that [[George W. Bush]] took office, the words "tax relief" started coming out of the White House."<ref name="Lakoff2004">{{Cite book|last=Lakoff|first=George|title=Don\'t think of an elephant!: know your values and frame the debate|year=2004|publisher=Chelsea Green Publishing|isbn=978-1-931498-71-5|page=56}}</ref>\nBy refocusing the structure away from one frame ("tax burden" or "tax responsibilities"), individuals can set the agenda of the questions asked in the future.\n\n[[Cognitive linguistics|Cognitive linguists]] point to an example of framing in the phrase "[[tax cut|tax relief]]". In this frame, use of the concept "relief" entails a concept of (without mentioning the benefits resulting from) taxes putting strain on the citizen:\n\n{{quote|The current tax code is full of inequities. Many single moms face higher marginal tax rates than the wealthy. Couples frequently face a higher tax burden after they marry. The majority of Americans cannot deduct their charitable donations. Family farms and businesses are sold to pay the death tax. And the owners of the most successful small businesses share nearly half of their income with the government. President Bush\'s tax cut will greatly reduce these inequities. It is a fair plan that is designed to provide tax relief to everyone who pays income taxes.|<ref>[http://georgewbush-whitehouse.archives.gov/news/reports/taxplan.html The President\'s Agenda for Tax Relief] retrieved 3 July 2007.</ref>}}\n\nAlternative frames may emphasize the concept of taxes as a source of infrastructural support to businesses:\n\n{{quote|The truth is that the wealthy have received more from America than most Americans—not just wealth but the infrastructure that has allowed them to amass their wealth: banks, the Federal Reserve, the stock market, the Securities and Exchange Commission, the legal system, federally sponsored research, patents, tax supports, the military protection of foreign investments, and much much more. American taxpayers support the infrastructure of wealth accumulation. It is only fair that those who benefit most should pay their fair share.|<ref>[http://www.cognitivepolicyworks.com/resource-center/planning-tools/framing-tutorials/simple-framing/ Cognitive Policy Works/Rockridge Institute: Simple Framing]</ref>}}\n\nFrames can limit debate by setting the vocabulary and [[metaphor]]s through which participants can comprehend and discuss an issue. They form a part not just of political discourse, but of [[cognition]]. In addition to generating new frames, politically oriented framing research aims to increase public awareness of the connection between framing and reasoning.\n\n====Examples====\n*The initial response of the [[George W. Bush administration|Bush administration]] to the [[September 11, 2001 attacks|assault of September 11, 2001]] was to frame the acts of [[Counterterrorism|terror]] as [[crime]]. This framing was replaced within hours by a war metaphor, yielding the "[[War on Terrorism|War on Terror]]". The difference between these two framings is in the implied response. Crime connotes bringing criminals to justice, putting them on trial and sentencing them, whereas as [[war]] implies enemy territory, military action and war powers for government.<ref name="Lakoff2004" /><ref>{{Cite journal|last=Zhang|first=Juyan |title=Beyond anti-terrorism: Metaphors as message strategy of post-September-11 U.S. public diplomacy |journal=Public Relations Review|year=2007|volume=33|issue=1|pages=31–39|doi=10.1016/j.pubrev.2006.11.006}}</ref>\n*The term "escalation" to describe an increase in American troop-levels in [[Iraq]] in 2007 implied that the United States deliberately increased the scope of conflict in a provocative manner and possibly implies that U.S. strategy entails a long-term military presence in Iraq, whereas [[Iraq War troop surge of 2007|"surge"]] framing implies a powerful but brief, transitory increase in intensity.<ref>[http://www.alternet.org/waroniraq/48059/ "It\'s Escalation, Stupid." \'\'Alternet\'\'] retrieved 3 July 2007</ref>\n*The "bad apple" frame, as in the proverb "one bad [[apple]] spoils the barrel". This frame implies that removing one underachieving or corrupt official from an [[institution]] will solve a given problem; an opposing frame presents the same problem as systematic or structural to the institution itself—a source of infectious and spreading rot.<ref>[http://www.huffingtonpost.com/bruce-budner/the-rumsfeld-dilemma-dem_b_29550.html "The Rumsfeld Dilemma: Demand an Exit Strategy, Not a Facelift"] by Bruce Budner, in \'\'The Huffington Post\'\' 15 September 2006</ref>\n*The "[[taxpayers]] money" frame, rather than [[government spending|public or government funds]], which implies that individual taxpayers have a claim or right to set [[government policy]] based upon their payment of tax rather than their status as [[citizen]]s or [[voters]] and that taxpayers have a right to control public funds that are the shared property of all citizens and also privileges individual self-interest above group interest.{{Citation needed|date=April 2009}}\n*The "collective property" frame, which implies that property owned by individuals is really owned by a collective in which those individuals are members. This collective can be a territorial one, such as a nation, or an abstract one that does not map to a specific territory.\n*Program-names that may describe only the intended effects of a program but may also imply their effectiveness. These include the following:\n**"[[Foreign aid]]"<ref>[http://hij.sagepub.com/cgi/content/abstract/12/2/120 "Is It All in a Word? The Effect of Issue Framing on Public Support for U.S. Spending on HIV/AIDS in Developing Countries."] by Sara Bleich. Retrieved 2007-07-03\n</ref> (which implies that spending money will aid foreigners, rather than harm them)\n**"[[Social security]]" (which implies that the program can be relied on to provide security for a society)\n**"[[Stabilisation policy]]" (which implies that a policy will have a stabilizing effect).\n* Based on [[opinion polling]] and [[focus group]]s, [[ecoAmerica]], a nonprofit environmental marketing and messaging firm, has advanced the position that [[global warming]] is an ineffective framing due to its identification as a leftist advocacy issue. The organization has suggested to government officials and environmental groups that alternate formulations of the issues would be more effective.<ref>[http://www.nytimes.com/2009/05/02/us/politics/02enviro.html "Seeking to Save the Planet, With a Thesaurus"] article by John M. Broder in \'\'[[The New York Times]]\'\' May 1, 2009</ref>\n*In her 2009 book \'\'Frames of War\'\', [[Judith Butler]] argues that the justification within liberal-democracies for war, and atrocities committed in the course of war, (referring specifically to the current war in Iraq and to [[Abu Ghraib torture and prisoner abuse|Abu Ghraib]] and [[Guantanamo Bay detention camp|Guantanamo Bay]]) entails a framing of the (especially Muslim) \'other\' as pre-modern/primitive and ultimately not human in the same way as citizens within the liberal order.<ref>Butler, J. (2009), \'\'Frames of War\'\', London: Verso.</ref>\n\n==See also==\n{{div col|3}}\n*[[Anecdotal value]]\n*[[Alternative facts]]\n*[[Argumentation theory]]\n*[[Bias]]\n*[[Choice architecture]]\n*[[Code word (figure of speech)]]\n*[[Communication theory]]\n*[[Connotation]]\n*[[Cultural bias]]\n*[[Decision making]]\n*[[Definition of the situation]]\n*[[Demagoguery]]\n*[[Domain of discourse]]\n*[[Echo chamber (media)]]\n*[[Fallacy of many questions]]\n*[[Figure of speech]]\n*[[Filter bubble]]\n*[[Freedom of speech]]\n*[[Freedom of the press|Free press]]\n*[[Idea networking]]\n*[[Language and thought]]\n*[[Meme]]\n*[[Newspeak]]\n<!--* [[Political frame]] - self ref after merge/redirect-->\n*[[Power word]]\n*[[Overton window]]\n*[[Political correctness]]\n*[[Rhetorical device]]\n*[[Semantics]]\n*[[Semantic domain]]\n*[[Social heuristics]]\n*[[Sophism]]\n*[[Spin doctor]]\n*[[Stovepiping]]\n*\'\'[[Thought Reform (book)]]\'\'\n*[[Trope (linguistics)|Trope]]\n*[[Unspeak]] (book)\n*[[Virtue word]]\n{{div col end}}\n\n==References==\n{{reflist|colwidth=30em}}\nLevin, Irwin P., and Gary J. Gaeth. "How Consumers Are Affected By The Framing Of Attribute Information Before And After Consuming The Product." Journal of Consumer Research 15.3 (1988): 374. Print.\n\n==Further reading==\n*[[Bernard Baars|Baars, B]]. \'\'A cognitive theory of consciousness\'\', NY: [[Cambridge University Press]] 1988, ISBN 0-521-30133-5.\n*[[Kenneth E. Boulding|Boulding, Kenneth E.]] (1956). The Image: Knowledge in Life and Society. Michigan University Press.\n* {{cite journal | last1 = Carruthers | first1 = P. | authorlink = Peter Carruthers (philosopher) | year = 2003 | title = On Fodor\'s Problem | url = | journal = Mind and Language | volume = 18 | issue = 5| pages = 502–523 | doi = 10.1111/1468-0017.00240 }}\n*Clark, A. (1997), Being There: Putting Brain, Body, and World Together Again, Cambridge, MA: MIT Press.\n*Cutting, Hunter and Makani Themba Nixon (2006). Talking the Walk: A Communications Guide for Racial Justice: AK Press\n*[[Daniel Dennett|Dennett, D.]] (1978), Brainstorms, Cambridge, MA: MIT Press.\n*Fairhurst, Gail T. and Sarr, Robert A. 1996. \'\'The Art of Framing: Managing the Language of Leadership.\'\' USA: Jossey-Bass, Inc.\n*Feldman, Jeffrey. (2007), \'\'Framing the Debate: Famous Presidential Speeches and How Progressives Can Use Them to Control the Conversation (and Win Elections)\'\'. Brooklyn, NY: Ig Publishing.\n*[[Jerry Fodor|Fodor, J.A.]] (1983), The Modularity of Mind, Cambridge, MA: MIT Press.\n*Fodor, J.A. (1987), "Modules, Frames, Fridgeons, Sleeping Dogs, and the Music of the Spheres", in Pylyshyn (1987).\n*Fodor, J.A. (2000), The Mind Doesn\'t Work That Way, Cambridge, MA: MIT Press.\n*Ford, K.M. & Hayes, P.J. (eds.) (1991), Reasoning Agents in a Dynamic World: The Frame Problem, New York: JAI Press.\n*[[Erving Goffman|Goffman, Erving]]. 1974. \'\'Frame Analysis: An Essay on the Organization of Experience.\'\' London: Harper and Row.\n*Goffman, E. (1974). Frame Analysis. Cambridge: Harvard University Press.\n*Goffman, E. (1959). Presentation of Self in Everyday Life. New York: Doubleday.\n*Goodman, N. (1954), Fact, Fiction, and Forecast, Cambridge, MA: Harvard University Press.\n*{{cite journal | last1 = Hanks | first1 = S. | last2 = McDermott | first2 = D. | year = 1987 | title = Nonmonotonic Logic and Temporal Projection | url = | journal = Artificial Intelligence | volume = 33 | issue = 3| pages = 379–412 | doi = 10.1016/0004-3702(87)90043-9 }}\n*Haselager, W.F.G. (1997). Cognitive science and folk psychology: the right frame of mind. London: Sage\n*{{cite journal | last1 = Haselager | first1 = W.F.G. | last2 = Van Rappard | first2 = J.F.H. | year = 1998 | title = Connectionism, Systematicity, and the Frame Problem | url = | journal = Minds and Machines | volume = 8 | issue = 2| pages = 161–179 | doi = 10.1023/A:1008281603611 }}\n*Hayes, P.J. (1991), "Artificial Intelligence Meets David Hume: A Reply to Fetzer", in Ford & Hayes (1991).\n*Heal, J. (1996), "Simulation, Theory, and Content", in Theories of Theories of Mind, eds. P. Carruthers & P. Smith, Cambridge: Cambridge University Press, pp.&nbsp;75–89.\n*Johnson-Cartee, K. (2005). News narrative and news framing: Constructing political reality. Lanham, MD: Rowman & Littlefield.\n*[[Diana Kendall|Kendall, Diana]], \'\'Sociology In Our Times\'\', Thomson Wadsworth, 2005, ISBN 0-534-64629-8 [https://books.google.com/books?vid=ISBN0534646298&id=kzU-gtx2VfoC&pg=PA531&lpg=PA531&dq=%22Resource+Mobilization%22&sig=NgTePMtdl2stO7V2FofPqeZuP5I&hl=en Google Print, p.531]\n*Klandermans, Bert. 1997. \'\'The Social Psychology of Protest.\'\' Oxford: Blackwell.\n*[[George Lakoff|Lakoff, G.]] & Johnson, M. (1980), Metaphors We Live By, Chicago: University of Chicago Press.\n*Leites, N. & Wolf, C., Jr. (1970). Rebellion and authority. Chicago: Markham Publishing Company.\n*{{cite journal | last1 = Martino | first1 = De | year = 2006 | title = Frames, Biases, and Rational Decision-Making in the Human Brain | url = | journal = Science | volume = 313 | issue = 5787| pages = 684–687 | doi = 10.1126/science.1128356 | pmid = 16888142 | last2 = Kumaran | first2 = D | last3 = Seymour | first3 = B | last4 = Dolan | first4 = RJ | pmc = 2631940 }}\n*McAdam, D., McCarthy, J., & Zald, M. (1996). Introduction: Opportunities, Mobilizing Structures, and Framing Processes—Toward a Synthetic, Comparative Perspective on Social Movements. In D. McAdam, J. McCarthy & M. Zald (Eds.), Comparative Perspectives on Social Movements; Political Opportunities, Mobilizing Structures, and Cultural Framings (pp.&nbsp;1–20). New York: Cambridge University Press.\n*McCarthy, J. (1986), "Applications of Circumscription to Formalizing Common Sense Knowledge", [[Artificial Intelligence (journal)|Artificial Intelligence]], vol. 26(3), pp.&nbsp;89–116.\n*McCarthy, J. & Hayes, P.J. (1969), "Some Philosophical Problems from the Standpoint of Artificial Intelligence", in Machine Intelligence 4, ed. D.Michie and B.Meltzer, Edinburgh: Edinburgh University Press, pp.&nbsp;463–502.\n*McDermott, D. (1987), "We\'ve Been Framed: Or Why AI Is Innocent of the Frame Problem", in Pylyshyn (1987).\n*Mithen, S. (1987), \'\'The Prehistory of the Mind\'\', London: Thames & Hudson.\n*{{cite journal | last1 = Nelson | first1 = T. E. | last2 = Oxley | first2 = Z. M. | last3 = Clawson | first3 = R. A. | year = 1997 | title = Toward a psychology of framing effects | url = | journal = Political Behavior | volume = 19 | issue = 3| pages = 221–246 | doi = 10.1023/A:1024834831093 }}\n*{{cite journal | last1 = Pan | first1 = Z. | last2 = Kosicki | first2 = G. M. | year = 1993 | title = Framing analysis: An approach to news discourse | url = | journal = Political Communication | volume = 10 | issue = 1| pages = 55–75 | doi = 10.1080/10584609.1993.9962963 }}\n*Pan. Z. & Kosicki, G. M. (2001). Framing as a strategic action in public deliberation. In S. D. Reese, O. H. Gandy, Jr., & A. E. Grant (Eds.), Framing public life: Perspectives on media and our understanding of the social world, (pp.&nbsp;35–66). Mahwah, NJ: Lawrence Erlbaum Associates.\n*Pan, Z. & Kosicki, G. M. (2005). Framing and the understanding of citizenship. In S. Dunwoody, L. B. Becker, D. McLeod, & G. M. Kosicki (Eds.), Evolution of key mass communication concepts, (pp.&nbsp;165–204). New York: Hampton Press.\n*[[Zenon Pylyshyn|Pylyshyn, Zenon W.]] (ed.) (1987), The Robot\'s Dilemma: The Frame Problem in Artificial Intelligence, Norwood, NJ: Ablex.\n*Stephen D. Reese, Oscar H. Gandy and August E. Grant. (2001). [https://books.google.com/books?id=I0BlAAAAMAAJ&q=journalist+subject:%22Reporters+and+reporting%22&dq=journalist+subject:%22Reporters+and+reporting%22&lr=&client=firefox-a&pgis=1 \'\'Framing Public Life: Perspectives on Media and Our Understanding of the Social World.\'\'] Maywah, New Jersey: Lawrence Erlbaum. ISBN 978-0-8058-3653-0\n*Russell, S. & Wefald, E. (1991), Do the Right Thing: Studies in Limited Rationality, Cambridge, MA: MIT Press.\n*{{cite journal | last1 = Scheufele | first1 =  DA| authorlink = Scheufele | last2 = Dietram | first2 = A.  | year = 1999 | title = Framing as a theory of media effects | url = | journal = [[Journal of Communication]] | volume = 49 | issue = 1| pages = 103–122 | doi = 10.1111/j.1460-2466.1999.tb02784.x }}\n*Shanahan, Murray P. (1997), \'\'Solving the Frame Problem: A Mathematical Investigation of the Common Sense Law of Inertia\'\', Cambridge, MA: MIT Press. ISBN 0-262-19384-1\n*Shanahan, Murray P. (2003), "The Frame Problem", in \'\'The Macmillan Encyclopedia of Cognitive Science\'\', ed. L.Nadel, Macmillan, pp.&nbsp;144–150.\n*[[Herbert A. Simon|Simon, Herbert]] (1957), \'\'Models of Man, Social and Rational: Mathematical Essays on Rational Human Behavior in a Social Setting\'\', New York: John Wiley. {{OCLC|165735}}\n*{{cite journal | last1 = Snow | first1 = D. A. | last2 = Benford | first2 = R. D. | year = 1988 | title = Ideology, frame resonance, and participant mobilization | url = | journal = International Social Movement Research | volume = 1 | issue = | pages = 197–217 }}\n*{{cite journal | last1 = Snow | first1 = D. A. | last2 = Rochford | first2 = E. B. | last3 = Worden | first3 = S. K. | last4 = Benford | first4 = R. D. | year = 1986 | title = Frame alignment processes, micromobilization, and movement participation | url = | journal = American Sociological Review | volume = 51 | issue = 4| pages = 464–481 | doi = 10.2307/2095581 }}\n*{{cite journal | last1 = Sperber | first1 = D. | last2 = Wilson | first2 = D. | year = 1996 | title = Fodor\'s Frame Problem and Relevance Theory | url = | journal = Behavioral and Brain Sciences | volume = 19 | issue = 3| pages = 530–532 | doi = 10.1017/S0140525X00082030 }}\n*[[Sidney Tarrow|Tarrow, S.]] (1983a). "Struggling to Reform: social Movements and policy change during cycles of protest". Western Societies Paper No. 15. Ithaca, NY: Cornell University.\n*Tarrow, S. (1983b). "Resource mobilization and cycles of protest: Theoretical reflections and comparative illustrations". Paper presented at the Annual Meeting of the [[American Sociological Association]], Detroit, August 31–September 4.\n*Triandafyllidou, A. and Fotiou, A. (1998), [http://www.socresonline.org.uk/3/1/2.html "Sustainability and Modernity in the European Union: A Frame Theory Approach to Policy-Making"], Sociological Research Online, vol. 3, no. 1.\n*[[Charles Tilly|Tilly, C.]], Tilly, L., & Tilly, R. (1975). \'\'The rebellious century, 1830–1930\'\'. Cambridge, MA: Cambridge University Press.\n*Turner, R. H., & Killian, L. M. (1972). \'\'Collective Behavior\'\'. Englewood Cliffs, NJ: Prentice-Hall.\n* "Rational Choice and the Framing of Decisions", A.Tversky, D.Kahneman, \'\'Journal of Business\'\', 1986, vol.59, no.4, pt.2.\n*{{cite journal | last1 = Wilkerson | first1 = W.S. | year = 2001 | title = Simulation, Theory, and the Frame Problem | url = | journal = [[Philosophical Psychology (journal)|Philosophical Psychology]] | volume = 14 | issue = 2| pages = 141–153 | doi = 10.1080/09515080120051535 }}\n*[[Charles Arthur Willard|Willard, Charles Arthur]]. \'\'Liberalism and the Social Grounds of Knowledge\'\' Chicago: University of Chicago Press, 199\n\n==External links==\n*[http://www.nytimes.com/2005/07/17/magazine/17DEMOCRATS.html The Framing Wars. \'\'New York Times\'\' 17 July 2005]\n*Curry, Tom. 2005. [http://www.msnbc.msn.com/id/7640262/page/2/ "Frist chills talk of judges deal (Page 2)."] "The question in the poll was not \'\'\'framed\'\'\' as a matter of whether nominee ought to get an up-or-down vote. And that \'\'\'framing\'\'\' of the issue, Republican strategists believe, is the most advantageous one..."; [[MSNBC]]\n*[http://www.ccbi.cmu.edu/reprints/Gonzalez_JOEP2005-decision-making.pdf CMU.edu (pdf)] - \'The Framing effect and risky decision: Examining cognitive functions with fMRI\', C. Gonzalez, et al., \'\'[[Journal of Economic Psychology]]\'\' (2005)\n*[http://hbswk.hbs.edu/item/5488.html HBS.edu] - \'Fixing Price Tag Confusion\'(interview), Sean Silverthorne (December 11, 2006)\n*[http://www.msnbc.msn.com/id/14170927/ "\'Framing effect\' influences decisions: Emotions play a role in decision-making when information is too complex"], Charles Q. Choi, [[MSNBC]] (August 3, 2006)\n\n{{Media culture}}\n{{Media manipulation}}\n{{Propaganda}}\n{{World view}}\n\n{{DEFAULTSORT:Framing (Social Sciences)}}\n[[Category:Cognitive biases]]\n[[Category:Framing (social sciences)| ]]\n[[Category:Knowledge representation]]\n[[Category:Propaganda techniques]]\n[[Category:Prospect theory]]\n[[Category:Social constructionism]]']
['Ontology (information science)', '49681', '{{redirect|Knowledge graph|the Google knowledge base|Knowledge Graph||Knowledge engine (disambiguation)}}\n{{About|ontology in information science|the study of the nature of being|Ontology}}\n{{Information science}}\n\nIn [[computer science]] and [[information science]], an \'\'\'ontology\'\'\' is a formal naming and definition of the types, properties, and interrelationships of the [[entities]] that really or fundamentally exist for a particular [[domain of discourse]]. It is thus a practical application of philosophical [[ontology]], with a [[taxonomy (general)|taxonomy]].\n\nAn ontology compartmentalizes the variables needed for some set of computations and establishes the relationships between them.<ref name="TRG93">{{cite journal |first=Thomas R. |last=Gruber |authorlink=Tom Gruber |date=June 1993 |url=http://tomgruber.org/writing/ontolingua-kaj-1993.pdf |format=PDF |title=A translation approach to portable ontology specifications |journal=[[Knowledge Acquisition]] |volume=5 |issue=2 |pages=199–220 |doi=10.1006/knac.1993.1008}}</ref><ref>{{cite web |first1=F. |last1=Arvidsson |first2=A. |last2=Flycht-Eriksson |url=http://www.ida.liu.se/~janma/SemWeb/Slides/ontologies1.pdf |title=Ontologies I |format=PDF |accessdate=26 November 2008}}</ref>\n\nThe fields of [[artificial intelligence]], the [[Semantic Web]], [[systems engineering]], [[software engineering]], [[biomedical informatics]], [[library science]], [[enterprise bookmarking]], and [[information architecture]] all create ontologies to limit complexity and to organize information. The ontology can then be applied to [[problem solving]].\n\n==Etymology and definition==\nThe term \'\'[[ontology]]\'\' has its origin in [[philosophy]] and has been applied in many different ways. The word element \'\'[[wiktionary:onto-|onto-]]\'\' comes from the [[Greek language|Greek]] \'\'[[wiktionary:ὤν|ὤν]], ὄντος\'\', ("being", "that which is"), present participle of the verb \'\'[[wiktionary:εἰμί|εἰμί]]\'\' ("be"). The core meaning within [[computer science]] is a model for describing the world that consists of a set of types, properties, and relationship types. There is also generally an expectation that the features of the model in an ontology should closely resemble the real world (related to the object).<ref>{{cite web |first=L. M. |last=Garshol |year=2004 |url=http://www.ontopia.net/topicmaps/materials/tm-vs-thesauri.html#N773 |title=Metadata? Thesauri? Taxonomies? Topic Maps! Making sense of it all\'\' |accessdate=13 October 2008 }}</ref>\n\n==Overview==\nWhat many ontologies have in common in both computer science and in philosophy is the representation of entities, ideas, and events, along with their properties and relations, according to a system of categories. In both fields, there is considerable work on problems of [[Confirmation holism|ontological relativity]] (e.g., [[Willard Van Orman Quine|Quine]] and [[Saul Kripke|Kripke]] in philosophy, [[John F. Sowa|Sowa]] and [[Nicola Guarino|Guarino]] in computer science),<ref>{{cite journal |first=J. F. |last=Sowa |title=Top-level ontological categories\n|journal=[[International Journal of Human-Computer Studies]] |volume=43 |issue=5-6 (November/December)\n|year=1995 |pages=669–85 |doi=10.1006/ijhc.1995.1068 }}</ref> and debates concerning whether a [[normative]] ontology is viable (e.g., debates over [[foundationalism]] in philosophy, and over the [[Cyc]] project in AI). Differences between the two are largely matters of focus. Computer scientists are more concerned with establishing fixed, controlled vocabularies, while philosophers are more concerned with first principles, such as whether there are such things as [[Essence|fixed essences]] or whether enduring objects must be ontologically more primary than processes.\n\nOther fields make ontological assumptions that are sometimes explicitly elaborated and explored.  For instance, the [[philosophy and economics#Definition and ontology of economics|definition and ontology of economics]] (also sometimes called the [[political economy]]) is hotly debated especially in [[Marxist economics]]<ref>{{cite journal|first=Giulio |last=Palermo|url=http://cje.oxfordjournals.org/content/31/4/539.short |title=The ontology of economic power in capitalism: mainstream economics and Marx |journal=Cambridge Journal of Economics |volume=31 |issue=4 |pages=539–561 |date=10 January 2007 |doi=10.1093/cje/bel036 |accessdate=16 June 2013 |via=Oxford Journals }}</ref> where it is a primary concern, but also in other subfields.<ref>{{cite web|author=Zuniga, Gloria L. |url=https://ideas.repec.org/p/pra/mprapa/5566.html |title=An Ontology Of Economic Objects |website=Ideas |publisher=Research Division of the Federal Reserve Bank of St. Louis |date=1999-02-02 |accessdate=2013-06-16}}</ref>  Such concerns intersect with those of information science when a simulation or model is intended to enable decisions in the economic realm; for example, to determine what [[capital asset]]s are at risk and if so by how much (see [[risk management]]).  Some claim all social sciences have explicit ontology issues because they do not have hard [[falsifiability]] criteria like most models in physical sciences and that indeed the lack of such widely accepted hard falsification criteria is what defines a social or soft science.{{Citation needed|date=March 2012}}\n\n==History==\nHistorically, ontologies arise out of the branch of [[philosophy]] known as [[metaphysics]], which deals with the nature of reality&nbsp;– of what exists. This fundamental branch is concerned with analyzing various types or modes of existence, often with special attention to the relations between [[particular]]s and [[Universal (metaphysics)|universals]], between [[Intrinsic and extrinsic properties (philosophy)|intrinsic and extrinsic properties]], and between [[essence]] and [[existence]]. The traditional goal of ontological inquiry in particular is to divide the world "at its joints" to discover those fundamental categories or kinds into which the world’s objects naturally fall.<ref name="PCB94">{{cite web |first1=Perakath C. |last1=Benjamin |first2=Christopher P. |last2=Menzel |first3=Richard J. |last3=Mayer |first4=Florence |last4=Fillion |first5=Michael T. |last5=Futrell |first6=Paula S. |last6=deWitte |first7=Madhavi |last7=Lingineni |date=September 21, 1994 |url=http://www.idef.com/pdf/Idef5.pdf |format=PDF |title=IDEF5 Method Report |publisher=Knowledge Based Systems, Inc.}}</ref>\n\nDuring the second half of the 20th century, philosophers extensively debated the possible methods or approaches to building ontologies without actually \'\'building\'\' any very elaborate ontologies themselves. By contrast, computer scientists were building some large and robust ontologies, such as [[WordNet]] and [[Cyc]], with comparatively little debate over \'\'how\'\' they were built.\n\nSince the mid-1970s, researchers in the field of [[artificial intelligence]] (AI) have recognized that capturing knowledge is the key to building large and powerful AI systems.  AI researchers argued that they could create new ontologies as [[computational model]]s that enable certain kinds of [[automated reasoning]]. In the 1980s, the AI community began to use the term \'\'ontology\'\' to refer to both a theory of a modeled world and a component of knowledge systems. Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.<ref name="TG08">{{cite book |first=T. |last=Gruber |authorlink=Tom Gruber |year=2008 |url=http://tomgruber.org/writing/ontology-definition-2007.htm  |title=Ontology |work=Encyclopedia of Database Systems |editor1-first=Ling |editor1-last=Liu |editor2-first=M. Tamer |editor2-last=Özsu |publisher=Springer-Verlag|isbn=978-0-387-49616-0}}</ref>\n\nIn the early 1990s, the widely cited Web page and paper "Toward Principles for the Design of Ontologies Used for Knowledge Sharing" by [[Tom Gruber]]<ref name="TRG95">{{cite journal |first=T. |last=Gruber |authorlink=Tom Gruber |title=Toward Principles for the Design of Ontologies Used for Knowledge Sharing |journal=[[International Journal of Human-Computer Studies]] |volume=43 |issue=5-6 |pages=907–928 |year=1995 |doi=10.1006/ijhc.1995.1081}}</ref> is credited with a deliberate definition of \'\'ontology\'\' as a technical term in [[computer science]]. Gruber introduced the term to mean a specification of a conceptualization: <blockquote>An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.<ref name="TRG01">{{cite web |first=T. |last=Gruber |authorlink=Tom Gruber |year=2001 |url=http://www-ksl.stanford.edu/kst/what-is-an-ontology.html |title=What is an Ontology? |publisher=[[Stanford University]] |accessdate=2009-11-09}}</ref></blockquote>\n\nAccording to Gruber (1993): <blockquote>Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to [[Conservative extension|conservative definitions]]&nbsp;&mdash; that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world.<ref>{{cite book\n|first=H. B. |last=Enderton |authorlink=Herbert Enderton |date=1972-05-12 |title=A Mathematical Introduction to Logic |location=San Diego, CA |publisher=Academic Press |edition=1 |page=295 |isbn=978-0-12-238450-9 |postscript=&nbsp;2nd edition; January 5, 2001, ISBN 978-0-12-238452-3}}</ref> To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.<ref name="TRG93"/></blockquote>\n\n== Components ==\n{{Main article|Ontology components}}\nContemporary ontologies share many structural similarities, regardless of the language in which they are expressed.  As mentioned above, most ontologies describe individuals (instances), classes (concepts), attributes, and relations.  In this section each of these components is discussed in turn.\n\nCommon components of ontologies include:\n; Individuals\n: Instances or objects (the basic or "ground level" objects)\n; [[Class (set theory)|Class]]es<!-- This deliberately links to the disambiguation page -->\n: Sets, collections, concepts, [[Class (computer science)|classes in programming]], [[Class (philosophy)|types of objects]], or kinds of things\n; [[Attribute (computing)|Attribute]]s\n: Aspects, properties, features, characteristics, or parameters that objects (and classes) can have\n; [[Relation (mathematics)|Relations]]\n: Ways in which classes and individuals can be related to one another\n; Function terms\n: Complex structures formed from certain relations that can be used in place of an individual term in a statement\n; Restrictions\n: Formally stated descriptions of what must be true in order for some assertion to be accepted as input\n; Rules\n: Statements in the form of an if-then (antecedent-consequent) sentence that describe the logical inferences that can be drawn from an assertion in a particular form\n; Axioms\n: Assertions (including rules) in a [[logical form]] that together comprise the overall theory that the ontology describes in its domain of application.  This definition differs from that of "axioms" in [[generative grammar]] and [[formal logic]].  In those disciplines, axioms include only statements asserted as \'\'a priori\'\' knowledge.  As used here, "axioms" also include the theory derived from axiomatic statements\n; [[Event (philosophy)|Events]]<!-- this links  to the philosophy sense of \'Events\' as that is currently the only article describing the issues around defining events in the ontology community-->\n: The changing of attributes or relations\n\nOntologies are commonly encoded using [[ontology language]]s.\n\n== Types ==\n\n=== Domain ontology<!--linked from \'Domain ontology\'--> ===\nA domain ontology (or domain-specific ontology) represents concepts which belong to part of the world. Particular meanings of terms applied to that domain are provided by domain ontology. For example, the word \'\'[[:wikt:card|card]]\'\' has many different meanings. An ontology about the domain of [[poker]] would model the "[[playing card]]" meaning of the word, while an ontology about the domain of [[computer hardware]] would model the "[[punched card]]" and "[[video card]]" meanings.\n\nSince domain ontologies represent concepts in very specific and often eclectic ways, they are often incompatible. As systems that rely on domain ontologies expand, they often need to merge domain ontologies into a more general representation.  This presents a challenge to the ontology designer. Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.).\n\nAt present, merging ontologies that are not developed from a common foundation ontology is a largely manual process and therefore time-consuming and expensive. Domain ontologies that use the same foundation ontology to provide a set of basic elements with which to specify the meanings of the domain ontology elements can be merged automatically. There are studies on generalized techniques for merging ontologies,<ref name="Dynamic Ontology Repair">{{cite web |url=http://dream.inf.ed.ac.uk/projects/dor/ |title=Project: Dynamic Ontology Repair |publisher= University of Edinburgh Department of Informatics|accessdate=2 January 2012}}</ref> but this area of research is still largely theoretical.\n\n=== Upper ontology ===\n{{Main article|Upper ontology}}\n\nAn [[Upper ontology (computer science)|upper ontology]] (or foundation ontology) is a model of the common objects that are generally applicable across a wide range of domain ontologies. It usually employs a [[core glossary]] that contains the terms and associated object descriptions as they are used in various relevant domain sets.\n\nThere are several standardized upper ontologies available for use, including [[Basic Formal Ontology|BFO]], [[BORO method]], [[Dublin Core]], [[General Formal Ontology|GFO]], [[OpenCyc]]/[[ResearchCyc]], [[Suggested Upper Merged Ontology|SUMO]], the Unified Foundational Ontology (UFO),<ref>{{cite web|last=Giancarlo Guizzardi & Gerd Wagner|url=http://ceur-ws.org/Vol-125/paper2.pdf|accessdate=31 March 2014|title=A Unified Foundational Ontology and some Applications of it in Business Modeling}}</ref> and [[Upper ontology (computer science)#DOLCE and DnS|DOLCE]].<ref name="DOLCE">{{cite web |url=http://www.loa-cnr.it/DOLCE.html |title=Laboratory for Applied Ontology - DOLCE |publisher=Laboratory for Applied Ontology (LOA)|accessdate=10 February 2011}}</ref><ref name="DOLCE-OWL">{{cite web |url=http://www.ontologydesignpatterns.org/ont/dul/DUL.owl |title=OWL version of DOLCE+DnS  |publisher=Semantic Technology Lab|accessdate=21 February 2013}}</ref> [[WordNet]], while considered an upper ontology by some, is not strictly an ontology. However, it has been employed as a linguistic tool for learning domain ontologies.<ref>{{cite journal |first1=Roberto |last1=Navigli |authorlink1=Roberto Navigli |first2=Paola |last2=Velardi |authorlink2=Paola Velardi |year=2004 |url=http://www.mitpressjournals.org/doi/pdf/10.1162/089120104323093276 |format=PDF |title=Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites |journal=[[Computational Linguistics (journal)|Computational Linguistics]] |volume=30 |issue=2 |publisher=MIT Press |pages=151–179 |doi=10.1162/089120104323093276}}</ref>\n\n=== Hybrid ontology ===\n\nThe [[Gellish]] ontology is an example of a combination of an upper and a domain ontology.\n\n== Visualization ==\nA survey of ontology visualization techniques is presented by Katifori et al.<ref>{{cite journal |last1=Katifori |first1=A. |last2=Halatsis |first2=C. |last3=Lepouras |first3=G. |last4=Vassilakis |first4=C. |last5=Giannopoulou |first5=E. |title=Ontology Visualization Methods - A Survey |journal=ACM Computing Surveys |volume=39 |issue=4 |page=10 |date=2007 |url=http://entrezneuron.googlecode.com/svn-history/r2/trunk/references/12-onto-vis-survey-final.pdf |archive-url=http://web.archive.org/web/20160304203317/http://entrezneuron.googlecode.com/svn-history/r2/trunk/references/12-onto-vis-survey-final.pdf |archive-date=4 March 2016 |format=PDF }}</ref> An evaluation of two most established ontology visualization techniques: indented tree and graph is discussed in.<ref>{{cite conference |first1=Bo |last1=Fu |first2=Natalya F. |last2=Noy |first3=Margaret-Anne |last3=Storey |title=Indented Tree or Graph? A Usability Study of Ontology Visualization Techniques in the Context of Class Mapping Evaluation |book-title=The Semantic Web – ISWC 2013: 12th International Semantic Web Conference, Sydney, NSW, Australia, October 21–25, 2013, Proceedings, Part I |series=Lecture Notes in Computer Science |volume=8218 |pages=117–134 |url=http://link.springer.com/chapter/10.1007/978-3-642-41335-3_8 |doi=10.1007/978-3-642-41335-3_8 |isbn=978-3-642-41335-3 |publisher=Springer |location=Berlin |date=2013 |via=SpringerLink }}</ref> A visual language for ontologies represented in [[Web Ontology Language|OWL]] is specified by the \'\'Visual Notation for OWL Ontologies (VOWL)\'\'.<ref>{{cite web |last1=Negru |first1=Stefan |last2=Lohmann |first2=Steffen |last3=Haag |first3=Florian |date=7 April 2014 |title=VOWL: Visual Notation for OWL Ontologies: Specification of Version 2.0 |website=Visual Data Web |url=http://vowl.visualdataweb.org/v2/ }}</ref>\n\n== Engineering ==\n{{Main article|Ontology engineering}}\n[[Ontology engineering]] (or ontology building) is a subfield of [[knowledge engineering]]. It studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.<ref name="PFC04">{{cite book |first1=Ascunion |last1=Gómez-Pérez |authorlink1=Ascunion Gómez-Pérez |first2=Mariano |last2=Fernández-López |authorlink2=Mariano Fernández-López |first3=Oscar |last3=Corcho |authorlink3=Oscar Corcho |year=2004 |title=Ontological Engineering: With Examples from the Areas of Knowledge Management, E-commerce and the Semantic Web |publisher=Springer |isbn=978-1-85233-551-9 |page=403 |edition=1 }}</ref><ref name="DMN">{{cite journal |first1=Antonio |last1=De Nicola |authorlink1=Antonio De Nicola |first2=Michele |last2=Missikoff |authorlink2=Michele Missikoff |first3=Roberto |last3=Navigli |authorlink3=Roberto Navigli |year=2009 |url=http://www.dsi.uniroma1.it/~navigli/pubs/De_Nicola_Missikoff_Navigli_2009.pdf |format=PDF |title=A Software Engineering Approach to Ontology Building |journal=[[Information Systems (journal)|Information Systems]] |volume=34 |issue=2 |publisher=Elsevier |pages=258–275 | doi = 10.1016/j.is.2008.07.002 }}</ref>\n\nOntology engineering aims to make explicit the knowledge contained within software applications, and within enterprises and business procedures for a particular domain. Ontology engineering offers a direction towards solving the interoperability problems brought about by semantic obstacles, such as the obstacles related to the definitions of business terms and software classes. Ontology engineering is a set of tasks related to the development of ontologies for a particular domain.<ref name="PIS00">{{cite journal |first1=Line  |last1=Pouchard |authorlink1=Line Pouchard |first2=Nenad |last2=Ivezic |authorlink2=Nenad Ivezic |first3=Craig |last3=Schlenoff |authorlink3=Craig Schlenoff |date=March 2000 |url=http://www.mel.nist.gov/msidlibrary/doc/AISfinal2.pdf |format=PDF |title=Ontology Engineering for Distributed Collaboration in Manufacturing |work=Proceedings of the AIS2000 conference }}</ref>\n\nKnown challenges with ontology engineering include:\n# Ensuring the ontology is \'\'current\'\' with domain knowledge and term use\n# Providing \'\'sufficient specificity and concept coverage\'\' for the domain of interest, thus minimizing the [[content completeness problem]]\n# Ensuring the ontology can support its use cases\n\n=== Editor ===\n\'\'\'Ontology editors\'\'\' are applications designed to assist in the creation or  manipulation of [[ontology (computer science)|ontologies]]. They often express ontologies in one of many [[ontology language (computer science)|ontology languages]]. Some provide export to other ontology languages however.\n\nAmong the most relevant criteria for choosing an ontology editor are the degree to which the editor abstracts from the actual [[Ontology language (computer science)|ontology representation language]] used for [[persistence (computer science)|persistence]] and the visual navigation possibilities within the [[knowledge model]]. Next come built-in [[inference engine]]s and [[information extraction]] facilities, and the support of meta-ontologies such as [[OWL-S]], [[Dublin Core]], etc. Another important feature is the ability to import & export foreign [[knowledge representation]] languages for [[ontology matching]]. Ontologies are developed for a specific purpose and application.\n\n*a.k.a. software (Ontology, taxonomy and thesaurus management software available from The Synercon Group)\n*Anzo for Excel (Includes an RDFS and OWL ontology editor within Excel; generates ontologies from Excel spreadsheets)\n*Chimaera (Other web service by Stanford)\n*CmapTools Ontology Editor (COE) (Java based ontology editor from the Florida Institute for Human and Machine Cognition. Supports numerous formats)\n*[[dot15926]] Editor (Open source ontology editor for data compliant to engineering ontology standard [[ISO 15926]]. Allows [[Python (programming language)|Python]] scripting and pattern-based data analysis. Supports extensions.)\n*EMFText OWL2 Manchester Editor, Eclipse-based, open-source, Pellet integration\n* Enterprise Architect, along with [[Unified Modeling Language|UML]] modeling, supports [[Object Management Group|OMG\'s]] [[Ontology Definition MetaModel]] which includes [[Web Ontology Language|OWL]] and [[Resource Description Framework|RDF]].\n*[[Fluent Editor]], a comprehensive ontology editor for OWL and SWRL with Controlled Natural Language (Controlled English). Supports [[Web Ontology Language|OWL]], [[Resource Description Framework|RDF]], [[Description Logic|DL]] and Functional rendering, unlimited imports and built-in reasoning services.\n*[[HOZO]] (Java-based graphical editor especially created to produce heavy-weight and well thought out ontologies, from [[Osaka University]] and Enegate Co, ltd.)\n*Java Ontology Editor (JOE)  (1998)\n*[[KAON]] (single user and server based solutions possible, open source, from FZI/AIFB Karlsruhe)\n*KMgen (Ontology editor for the KM language.&nbsp;km: The Knowledge Machine)\n*Knoodl (Free web application/service that is an ontology editor, [[wiki]], and [[Digital repository|ontology registry]].  Supports creation of communities where members can collaboratively import, create, discuss, document and publish ontologies.  Supports [[Web Ontology Language|OWL]], [[Resource Description Framework|RDF]], [[RDF Schema|RDFS]], and [[SPARQL]] queries.  Available since early Nov 2006 from Revelytix, Inc..)\n*Model Futures IDEAS AddIn (free) A plug-in for Sparx Systems [[Enterprise Architect (software)|Enterprise Architect]] that allows [[IDEAS Group]] [[4D ontology|4D ontologies]] to be developed using a [[Unified Modeling Language|UML]] profile\n*Model Futures OWL Editor (Free) Able to work with very large OWL files (e.g. [[Cyc]]) and has extensive import and export capabilities (inc. [[Unified Modeling Language|UML]], Thesaurus Descriptor, [[MS Word]], CA ERwin Data Modeler, CSV, etc.)\n*myWeb (Java-based, mySQL connection, bundled with applet that allows online browsing of ontologies (including OBO))\n*Neologism (Web-based, open source, supports RDFS and a subset of OWL, built on [[Drupal]])\n*[[NeOn Toolkit (software)|NeOn Toolkit]]  (Eclipse-based, open source, OWL support, several import mechanisms, support for reuse and management of networked ontologies, visualization, etc.…from NeOn Project)\n*OBO-Edit (Java-based, downloadable, open source, developed by the [[Gene Ontology Consortium]] for editing biological ontologies)\n*OntoStudio (Eclipse-based, downloadable, support for RDF(S), OWL and F-Logic, graphical rule editor, visualizations, from ontoprise)\n*Ontolingua (Web service offered by Stanford University)\n* [[Open Semantic Framework]] (OSF), an integrated software stack using semantic technologies for knowledge management, which includes an ontology editor\n*OWLGrEd (A graphical ontology editor, easy-to-use)\n*PoolParty Thesaurus Server (Commercial ontology, taxonomy and thesaurus management software available from Semantic Web Company, fully based on standards like RDFS, SKOS and SPARQL, integrated with [[Virtuoso Universal Server]])\n*[[Protégé (software)|Protégé]] (Java-based, downloadable, Supports OWL, open source, many sample ontologies, from Stanford University)\n*ScholOnto (net-centric representations of research)\n*Semantic Turkey (Firefox extension - also based on Java - for managing ontologies and acquiring new knowledge from the Web; developed at University of Rome, Tor Vergata )\n*[[Sigma knowledge engineering environment]] is a system primarily for development of the [[Suggested Upper Merged Ontology]]\n*Swoop (Java-based, downloadable, open source, OWL Ontology browser and editor from the University of Maryland)\n*Semaphore Ontology Manager  (Commercial ontology, taxonomy and thesaurus management software available from [[Smartlogic Semaphore Limited]]. Intuitive tool to manage the entire "build - enhance - review - maintain" ontology lifecycle.)\n*Synaptica  (Ontology, taxonomy and thesaurus management software available from Synaptica, LLC. Web based, supports [[Web Ontology Language|OWL]] and [[SKOS]].)\n*TopBraid Composer  (Eclipse-based, downloadable, full support for RDFS and OWL, built-in inference engine, SWRL editor and SPARQL queries, visualization, import of XML and UML, from TopQuadrant)\n*Transinsight (The editor is especially designed for creating text mining ontologies and part of GoPubMed.org)\n*WebODE (Web service offered by the Technical University of Madrid)\n*TwoUse Toolkit (Eclipse-based, open source, model-driven ontology editing environment especially designed for software engineers)\n*Be Informed Suite (Commercial tool for building large ontology based applications. Includes visual editors, inference engines, export to standard formats)\n*Thesaurus Master (Manages creation and use of ontologies for use in data management and semantic enrichment by enterprise, government, and scholarly publishers.)\n*[[Tool for Ontology Development and Editing (TODE)|TODE]] (A Dot Net-based Tool for Ontology Development and Editing)\n*VocBench (Collaborative Web Application for SKOS/SKOS-XL Thesauri Management - developed on a joint effort between University of Rome, Tor Vergata and the Food and Agriculture Organization of the United Nations: FAO )\n*OBIS (Web based user interface that allows to input ontology instances in a user friendly way that can be accessed via SPARQL endpoint)\n*[[Menthor Editor]] (An ontology engineering tool for dealing with [[OntoUML]]. It also includes OntoUML syntax validation, [[Alloy Analyzer|Alloy]] simulation, [[Anti-pattern|Anti-Pattern]] verification, and transformations from [[OntoUML]] to [[Web Ontology Language|OWL]], [[Semantics of Business Vocabulary and Business Rules|SBVR]] and [[Brazilian Portuguese|Natural Language (Brazilian Portuguese)]])\n\n=== Learning ===\n{{Main article|Ontology learning}}\n\nOntology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain\'s terms from natural language text. As building ontologies manually is extremely labor-intensive and time consuming, there is great motivation to automate the process. \nInformation extraction and text mining methods have been explored to automatically link ontologies to documents, e.g. in the context of the BioCreative challenges.<ref>{{Cite journal\n | pmid = 22438567\n| year = 2012\n| author1 = Krallinger\n| first1 = M\n| title = How to link ontologies and protein-protein interactions to literature: Text-mining approaches and the Bio \'\'Creative\'\' experience\n| journal = Database\n| volume = 2012\n| pages = bas017\n| last2 = Leitner\n| first2 = F\n| last3 = Vazquez\n| first3 = M\n| last4 = Salgado\n| first4 = D\n| last5 = Marcelle\n| first5 = C\n| last6 = Tyers\n| first6 = M\n| last7 = Valencia\n| first7 = A\n| last8 = Chatr-Aryamontri\n| first8 = A\n| doi = 10.1093/database/bas017\n| pmc = 3309177\n}}</ref>\n\n== Languages ==\n{{Main article|Ontology language}}\nAn [[ontology language]] is a [[formal language]] used to encode the ontology. There are a number of such languages for ontologies, both proprietary and standards-based:\n* [[Common Algebraic Specification Language]] is a general logic-based specification language developed within the IFIP working group 1.3 "Foundations of System Specifications" and functions as a de facto standard in the area of software specifications. It is now being applied to ontology specifications in order to provide modularity and structuring mechanisms.\n* [[Common logic]] is ISO standard 24707, a specification for a family of ontology languages that can be accurately translated into each other.\n* The [[Cyc]] project has its own ontology language called [[CycL]], based on [[first-order predicate calculus]] with some higher-order extensions.\n* [[DOGMA]] (Developing Ontology-Grounded Methods and Applications) adopts the fact-oriented modeling approach to provide a higher level of semantic stability.\n* The [[Gellish]] language includes rules for its own extension and thus integrates an ontology with an ontology language.\n* [[IDEF5]] is a [[software engineering]] method to develop and maintain usable, accurate, domain ontologies.\n* [[Knowledge Interchange Format|KIF]] is a syntax for [[first-order logic]] that is based on [[S-expression]]s.  SUO-KIF is a derivative version supporting the [[Suggested Upper Merged Ontology]].\n* [[Meta-Object Facility|MOF]] and [[Unified Modeling Language|UML]] are standards of the [[Object Management Group|OMG]]\n* [[Olog]] is a [[Category theory|category theoretic]] approach to ontologies, emphasizing translations between ontologies using [[functor]]s. \n* [[Open Biomedical Ontologies|OBO]], a language used for biological and biomedical ontologies.\n* [[OntoUML]] is an ontologically well-founded profile of UML for conceptual modeling of domain ontologies.\n* [[Web Ontology Language|OWL]] is a language for making ontological statements, developed as a follow-on from [[Resource Description Framework|RDF]] and [[RDFS]], as well as earlier ontology language projects including [[Ontology Inference Layer|OIL]], [[DARPA Agent Markup Language|DAML]], and [[DAMLplusOIL|DAML+OIL]]. OWL is intended to be used over the [[World Wide Web]], and all its elements (classes, properties and individuals) are defined as RDF [[web resource|resource]]s, and identified by [[Uniform Resource Identifier|URI]]s.\n* [[Rule Interchange Format]] (RIF) and [[F-Logic]] combine ontologies and rules.\n* [[Semantic Application Design Language]] (SADL)<ref>{{cite web |url=http://sadl.sourceforge.net/sadl.html |title=SADL |work=[[Sourceforge]] |accessdate=10 February 2011}}</ref> captures a subset of the expressiveness of [[Web Ontology Language|OWL]], using an English-like language entered via an [[Eclipse (software)|Eclipse]] Plug-in.\n* [[SBVR]] (Semantics of Business Vocabularies and Rules) is an OMG standard adopted in industry to build ontologies.\n* [[TOVE Project]], TOronto Virtual Enterprise project\n\n== Published examples ==\n* AURUM - Information Security Ontology,<ref>{{cite web |url=http://www.securityontology.com |title=AURUM - Information Security Ontology |accessdate=29 January 2016}}</ref> An ontology for information security knowledge sharing, enabling users to collaboratively understand and extend the domain knowledge body. It may serve as a basis for automated information security risk and compliance management.\n* [[BabelNet]], a very large multilingual semantic network and ontology, lexicalized in many languages\n* Basic Formal Ontology,<ref>{{cite web |url=http://www.ifomis.org/bfo/ |title=Basic Formal Ontology (BFO)\n|publisher=[[Institute for Formal Ontology and Medical Information Science]] (IFOMIS) |accessdate=}}</ref> a formal upper ontology designed to support scientific research\n* BioPAX,<ref>{{cite web |url=http://biopax.org |title=BioPAX |accessdate=10 February 2011}}</ref> an ontology for the exchange and interoperability of biological pathway (cellular processes) data\n* BMO,<ref>{{cite journal |first1=Alexander |last1=Osterwalder |first2=Yves |last2=Pigneur | author-link2= Yves Pigneur | url=http://129.3.20.41/eps/io/papers/0202/0202004.pdf |title=An e-Business Model Ontology for Modeling e-Business |location=[[Bled eConference|15th Bled eConference]], [[Slovenia]] |date=June 17–19, 2002}}</ref> an e-Business Model Ontology based on a review of enterprise ontologies and business model literature\n* SSBMO,<ref>{{cite journal |first1=Antony |last1=Upward |first2=Peter |last2=Jones |url=https://www.academia.edu/14461116 |title=An Ontology for Strongly Sustainable Business Models: Defining an Enterprise Framework Compatible with Natural and Social Science |journal=Organization & Environment |volume=29 |issue=1 |pages=97-123 |doi=10.1177/1086026615592933}}</ref> a Strongly Sustainable Business Model Ontology based on a review of the systems based natural and social science literature (including business).  Includes critique of and significant extensions to the Business Model Ontology (BMO).\n* CCO and GexKB,<ref>{{cite web|title=About CCO and GexKB|url=http://www.semantic-systems-biology.org/apo/|publisher=Semantic Systems Biology}}</ref> Application Ontologies (APO) that integrate diverse types of knowledge with the Cell Cycle Ontology (CCO) and the Gene Expression Knowledge Base (GexKB)\n* CContology (Customer Complaint Ontology),<ref>{{cite web |url=http://www.jarrar.info/CContology/ |title=CContology |accessdate=10 February 2011}}</ref> an e-business ontology to support online customer complaint management\n* [[CIDOC Conceptual Reference Model]], an ontology for [[cultural heritage]]<ref>{{cite web |url=http://www.cidoc-crm.org/ |title=The CIDOC Conceptual Reference Model (CRM) |accessdate=10 February 2011}}</ref>\n* COSMO,<ref>{{cite web |url=http://micra.com/COSMO/ |title=COSMO |publisher=MICRA Inc.|accessdate=10 February 2011}}</ref> a Foundation Ontology (current version in OWL) that is designed to contain representations of all of the primitive concepts needed to logically specify the meanings of any domain entity.  It is intended to serve as a basic ontology that can be used to translate among the representations in other ontologies or databases.  It started as a merger of the basic elements of the OpenCyc and SUMO ontologies, and has been supplemented with other ontology elements (types, relations) so as to include representations of all of the words in the [[Longman Dictionary of Contemporary English|Longman dictionary]] [[defining vocabulary]].\n* [[Cyc]], a large Foundation Ontology for formal representation of the universe of discourse\n* [[Disease Ontology]],<ref>{{cite journal |pmid=19594883}}</ref> designed to facilitate the mapping of diseases and associated conditions to particular medical codes\n* [[Upper ontology (computer science)#DOLCE and DnS|DOLCE]], a Descriptive Ontology for Linguistic and Cognitive Engineering<ref name="DOLCE"/><ref name="DOLCE-OWL"/>\n* [[Drammar]], ontology of drama\n* [[Dublin Core]], a simple ontology for documents and publishing\n* Foundational, Core and Linguistic Ontologies<ref>{{cite web |url=http://www.loa-cnr.it/Ontologies.html\n|title=Foundational, Core and Linguistic Ontologies |accessdate=10 February 2011}}</ref>\n* [[Foundational Model of Anatomy]],<ref>{{cite web |url=http://sig.biostr.washington.edu/projects/fm/AboutFM.html |title=Foundational Model of Anatomy |accessdate=10 February 2011}}</ref> an ontology for human anatomy\n* [[FOAF (software)|Friend of a Friend]], an ontology for describing persons, their activities and their relations to other people and objects\n* [[Gene Ontology]] for [[genomics]]\n* [[Gellish English dictionary]], an ontology that includes a dictionary and taxonomy that includes an upper ontology and a lower ontology that focusses on industrial and business applications in engineering, technology and procurement.\n* [[Geopolitical ontology]], an ontology describing geopolitical information created by [[Food and Agriculture Organization]](FAO). The geopolitical ontology includes names in multiple languages (English, French, Spanish, Arabic, Chinese, Russian and Italian); maps standard coding systems (UN, ISO, FAOSTAT, AGROVOC, etc.); provides relations among territories (land borders, group membership, etc.); and tracks historical changes. In addition, FAO provides web services of geopolitical ontology and a module maker to download modules of the geopolitical ontology into different formats (RDF, XML, and EXCEL). See more information at [[FAO Country Profiles]].\n* GOLD,<ref>{{cite web |url=http://www.linguistics-ontology.org/gold.html |title=GOLD |accessdate=10 February 2011}}</ref> General Ontology for [[descriptive linguistics|Linguistic Description]]\n* GUM (Generalized Upper Model),<ref>{{cite web |url=http://www.fb10.uni-bremen.de/anglistik/langpro/webspace/jb/gum/index.htm |title=Generalized Upper Model |accessdate=10 February 2011}}</ref> a linguistically motivated ontology for mediating between clients systems and natural language technology\n* [[IDEAS Group]],<ref>{{cite web |url=http://www.ideasgroup.org |title=The IDEAS Group Website |accessdate=10 February 2011}}</ref> a formal ontology for enterprise architecture being developed by the Australian, Canadian, UK and U.S. Defence Depts.\n* Linkbase,<ref>{{cite web |url=http://www.landcglobal.com/pages/linkbase.php |title=Linkbase |accessdate=10 February 2011}}</ref> a formal representation of the biomedical domain, founded upon [http://www.ifomis.org/bfo/ Basic Formal Ontology].\n* LPL, Lawson Pattern Language\n* NCBO Bioportal,<ref>{{cite web|title=Bioportal|url=http://www.bioontology.org/tools/portal/bioportal.html|publisher=National Center for Biological Ontology (NCBO)}}</ref> biological and biomedical ontologies and associated tools to search, browse and visualise\n* [[NIFSTD]] Ontologies from the [[Neuroscience Information Framework]]: a modular set of ontologies for the neuroscience domain.\n* OBO-Edit,<ref>{{cite web|title=Ontology browser for most of the Open Biological and Biomedical Ontologies|url=http://oboedit.org/?page=index|publisher=Berkeley Bioinformatics Open Source Project (BBOP)}}</ref> an ontology browser for most of the Open Biological and Biomedical Ontologies\n* [[OBO Foundry]],<ref>{{cite web|title=The Open Biological and Biomedical Ontologies|url=http://www.obofoundry.org/|publisher=Berkeley Bioinformatics Open Source Project (BBOP)}}</ref> a suite of interoperable reference ontologies in biology and biomedicine\n* OMNIBUS Ontology,<ref>{{cite web |url=http://edont.qee.jp/omnibus/ |title=OMNIBUS Ontology |accessdate=10 February 2011}}</ref> an ontology of learning, instruction, and instructional design\n* [[Ontology for Biomedical Investigations]], an open access, integrated ontology for the description of biological and clinical investigations\n* ONSTR,<ref>{{cite web |url= https://nbsdc.org/onstr.php |title=ONSTR |accessdate=16 April 2014}}</ref> Ontology for Newborn Screening Follow-up and Translational Research, Newborn Screening Follow-up Data Integration Collaborative, Emory University, Atlanta.\n* Plant Ontology<ref name="Plant Ontology">{{cite web |url=http://www.plantontology.org/ |title=Plant Ontology |accessdate=10 February 2011}}</ref> for plant structures and growth/development stages, etc.\n* POPE, Purdue Ontology for Pharmaceutical Engineering\n* PRO,<ref>{{cite web |url=http://pir.georgetown.edu/pro/ |title=PRO |accessdate=10 February 2011}}</ref> the Protein Ontology of the Protein Information Resource, Georgetown University\n* Program abstraction taxonomy\n* Protein Ontology<ref>{{cite web |url=http://pir.georgetown.edu/pro/ |title=Protein Ontology |accessdate=10 February 2011}}</ref> for [[proteomics]]\n* [[RXNO Ontology]], for [[name reaction]]s in chemistry\n* [[Sequence Ontology]],<ref>{{cite journal |vauthors= Eilbeck K, Lewis SE, Mungall CJ, Yandell M, Stein L, Durbin R, Ashburner M |title= The Sequence Ontology: a tool for the unification of genome annotations |journal= Genome Biology |volume= 6 |issue= 5 |pages= R44 |year= 2005 |pmid= 15892872 |pmc= 1175956 |doi= 10.1186/gb-2005-6-5-r44 |authorlink5= Lincoln Stein |authorlink6= Richard M. Durbin |authorlink7= Michael Ashburner |authorlink2= Suzanna Lewis}}</ref> for representing genomic feature types found on [[Sequence (biology)|biological sequences]]\n* [[SNOMED CT]] (Systematized Nomenclature of Medicine—Clinical Terms)\n* [[Suggested Upper Merged Ontology]], a formal upper ontology\n* [[Systems Biology Ontology]] (SBO), for computational models in biology\n* SWEET,<ref>{{cite web |url=http://sweet.jpl.nasa.gov/ |title=SWEET |accessdate=10 February 2011}}</ref> Semantic Web for Earth and Environmental Terminology\n* [[ThoughtTreasure]] ontology\n* [[TIME-ITEM]], Topics for Indexing Medical Education\n* [[Uberon]],<ref>{{cite journal|pmid=22293552}}</ref> representing [[metazoa|animal]] anatomical structures\n* [[UMBEL]], a lightweight reference structure of 20,000 subject concept classes and their relationships derived from [[Opencyc|OpenCyc]]\n* [[WordNet]], a lexical reference system\n* YAMATO,<ref>{{cite web |url=http://www.ei.sanken.osaka-u.ac.jp/hozo/onto_library/upperOnto.htm |title=YAMATO |accessdate=10 February 2011}}</ref> Yet Another More Advanced Top-level Ontology\n\nThe W3C [[Linked data#Linking Open Data community project|Linking Open Data community project]] coordinates attempts to converge different ontologies into worldwide [[Semantic Web]].\n\n== Libraries ==\nThe development of ontologies for the Web has led to the emergence of services providing lists or directories of ontologies with search facility. Such directories have been called ontology libraries.\n\nThe following are libraries of human-selected ontologies.\n* COLORE<ref>{{cite web |url=http://stl.mie.utoronto.ca/colore/ |title=COLORE |accessdate=4 May 2011}}</ref> is an open repository of first-order ontologies in [[Common Logic]] with formal links between ontologies in the repository.\n* DAML Ontology Library<ref>{{cite web |url=http://www.daml.org/ontologies/ |title=DAML Ontology Library |accessdate=10 February 2011}}</ref> maintains a legacy of ontologies in DAML.\n* Ontology Design Patterns portal<ref>{{cite web |url=http://www.ontologydesignpatterns.org |title=ODP Library |accessdate=21 February 2013}}</ref> is a wiki repository of reusable components and practices for ontology design, and also maintains a list of \'\'exemplary ontologies\'\'. \n* Protégé Ontology Library<ref>{{cite web\n|url=http://protegewiki.stanford.edu/index.php/Protege_Ontology_Library |title=Protege Ontology Library |accessdate=10 February 2011}}</ref> contains a set of OWL, Frame-based and other format ontologies.\n* SchemaWeb<ref>{{cite web |url=http://www.schemaweb.info/ |title=SchemaWeb |accessdate=10 February 2011}}</ref> is a directory of RDF schemata expressed in RDFS, OWL and DAML+OIL.\n\nThe following are both directories and search engines. They include crawlers searching the Web for well-formed ontologies.\n* [[OBO Foundry]] is a suite of interoperable reference ontologies in biology and biomedicine.<ref>{{cite web |url=http://www.obofoundry.org/ |title=OBO Foundry|accessdate=10 February 2011}}</ref><ref name="pmid17989687">{{Cite journal \n| last1 = Smith | first1 = B. \n| authorlink1 = Barry Smith (ontologist)\n| last2 = Ashburner | first2 = M. \n| authorlink2 = Michael Ashburner\n| last3 = Rosse | first3 = C. \n| last4 = Bard | first4 = J. \n| last5 = Bug | first5 = W. \n| last6 = Ceusters | first6 = W. \n| last7 = Goldberg | first7 = L. J. \n| last8 = Eilbeck | first8 = K. \n| last9 = Ireland | first9 = A. \n| last10 = Mungall \n| doi = 10.1038/nbt1346 | first10 = C. J. \n| last11 = Leontis | first11 = N. \n| last12 = Rocca-Serra | first12 = P. \n| last13 = Ruttenberg | first13 = A. \n| last14 = Sansone | first14 = S. A. \n| last15 = Scheuermann | first15 = R. H. \n| last16 = Shah | first16 = N. \n| last17 = Whetzel | first17 = P. L. \n| last18 = Lewis | first18 = S. | authorlink18 = Suzanna Lewis\n| title = The OBO Foundry: Coordinated evolution of ontologies to support biomedical data integration \n| journal = [[Nature Biotechnology]] \n| volume = 25 \n| issue = 11 \n| pages = 1251–1255 \n| year = 2007 \n| pmid = 17989687 \n| pmc =2814061 \n}} {{open access}}</ref>\n* Bioportal (ontology repository of NCBO)\n* OntoSelect<ref>{{cite web |url=http://olp.dfki.de/OntoSelect/ |title=OntoSelect |accessdate=10 February 2011}}</ref> Ontology Library offers similar services for RDF/S, DAML and OWL ontologies.\n* Ontaria<ref>{{cite web |url=http://www.w3.org/2004/ontaria/ |title=Ontaria |accessdate=10 February 2011}}</ref> is a "searchable and browsable directory of semantic web data" with a focus on RDF vocabularies with OWL ontologies. (NB Project "on hold" since 2004).\n* [[Swoogle]] is a directory and search engine for all RDF resources available on the Web, including ontologies.\n* Open Ontology Repository initiative\n* ROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO.\n\n== Examples of applications ==\nIn general, ontologies can be used beneficially in \n* enterprise applications.<ref>{{cite journal |first=Daniel |last=Oberle |title=How ontologies benefit enterprise applications |journal=Semantic Web Journal |volume=5 |issue=6 |pages=473–491 |publisher=IOS Press |date=2014 |doi=10.3233/SW-130114 |url=http://www.semantic-web-journal.net/system/files/swj212_2.pdf |format=PDF }}</ref> A more concrete example is [[SAPPHIRE (Health care)]] or \'\'Situational Awareness and Preparedness for Public Health Incidences and Reasoning Engines\'\' which is a [[semantics]]-based [[health information system]] capable of tracking and evaluating situations and occurrences that may affect [[public health]].\n* [[geographic information systems]] bring together data from different sources and benefit therefore from ontological metadata which helps to connect the semantics of the data.<ref>{{cite journal|first=Andrew U. |last=Frank|title=Tiers of ontology and consistency constraints in geographical information systems|journal=International Journal of Geographical Information Science|volume=15|issue=7|year=2001|pages=667–678|doi=10.1080/13658810110061144}}</ref>\n\n== See also ==\n{{div col||25em}}\n* [[Commonsense knowledge bases]]\n* [[Controlled vocabulary]]\n* [[Folksonomy]]\n* [[Formal concept analysis]]\n* [[Formal ontology]]\n* [[Gene Ontology]]\n* [[General formal ontology]]\n* [[Lattice (order)|Lattice]]\n* [[Ontology]]\n* [[Ontology alignment]]\n* [[Ontology chart]]\n* [[Open Biomedical Ontologies]]\n* [[Open Semantic Framework]]\n* [[Soft ontology]]\n* [[Terminology extraction]]\n* [[Weak ontology]]\n* [[Web Ontology Language]]\n{{div col end}}\n\n;Related philosophical concepts\n* [[Alphabet of human thought]]\n* [[Characteristica universalis]]\n* [[Interoperability]]\n* [[Metalanguage]]\n* [[Natural semantic metalanguage]]\n\n==References==\n{{Reflist|2}}\n\n==Further reading==\n* Oberle, D., Guarino, N., & Staab, S. (2009) [http://userpages.uni-koblenz.de/~staab/Research/Publications/2009/handbookEdition2/what-is-an-ontology.pdf What is an ontology?]. In: "Handbook on Ontologies". Springer, 2nd edition, 2009.\n* Fensel, D., van Harmelen, F., Horrocks, I., McGuinness, D. L., & Patel-Schneider, P. F. (2001). [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=920598 "OIL: an ontology infrastructure for the Semantic Web"]. In: \'\'Intelligent Systems\'\'. IEEE, 16(2): 38&ndash;45.\n* Gangemi A., Presutti V. (2009). [http://hem.hj.se/~blev/HandbookChapter_ODPs.pdf Ontology Design Patterns].{{dead link|date=September 2016}} In Staab S. et al. (eds.): Handbook on Ontologies (2nd edition), Springer, 2009.\n* Maria Golemati, Akrivi Katifori, Costas Vassilakis, George Lepouras, Constantin Halatsis (2007). [http://oceanis.mm.di.uoa.gr/pened/papers/11-onto-user-final.pdf "Creating an Ontology for the User Profile: Method and Applications"]. In: \'\'Proceedings of the First IEEE International Conference on Research Challenges in Information Science (RCIS)\'\', Morocco 2007.\n* Mizoguchi, R. (2004). [http://www.ei.sanken.osaka-u.ac.jp/pub/miz/Part3V3.pdf "Tutorial on ontological engineering: part 3: Advanced course of ontological engineering"]. In: \'\'New Generation Computing\'\'. Ohmsha & Springer-Verlag, 22(2):198-220.\n* [[Tom Gruber|Gruber, T. R.]] 1993. [http://tomgruber.org/writing/ontolingua-kaj-1993.pdf "A translation approach to portable ontology specifications"]. In: \'\'Knowledge Acquisition\'\'. 5: 199&ndash;199.\n* Maedche, A. & Staab, S. (2001). [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=920602 "Ontology learning for the Semantic Web"]. In: \'\'Intelligent Systems\'\'. IEEE, 16(2): 72&ndash;79.\n* Natalya F. Noy and [[Deborah McGuinness|Deborah L. McGuinness]]. [http://www-ksl.stanford.edu/people/dlm/papers/ontology-tutorial-noy-mcguinness-abstract.html Ontology Development 101: A Guide to Creating Your First Ontology]. Stanford Knowledge Systems Laboratory Technical Report KSL-01-05 and Stanford Medical Informatics Technical Report SMI-2001-0880, March 2001.\n* Prabath Chaminda Abeysiriwardana, Saluka R Kodituwakku, [http://www.ijorcs.org/manuscript/id/51/prabath-chaminda-abeysiriwardana-saluka-r-kodituwakku/ontology-based-information-extraction-ford-disease-intelligence "Ontology Based Information Extraction for Disease Intelligence"]. International Journal of Research in Computer Science, 2 (6): pp.&nbsp;7–19, November 2012. doi:10.7815/ijorcs.26.2012.051\n* Razmerita, L., Angehrn, A., & Maedche, A. 2003. [http://www.springerlink.com/index/THW9RMVMVKLX9HAC.pdf "Ontology-Based User Modeling for Knowledge Management Systems"]. In: \'\'Lecture Notes in Computer Science\'\': 213&ndash;17.\n* Soylu, A., De Causmaecker, Patrick. 2009.[http://dx.doi.org/10.1109/ISCIS.2009.5291915 Merging model driven and ontology driven system development approaches pervasive computing perspective]. in Proc 24th Intl Symposium on Computer and Information Sciences. pp 730–735.\n* Smith, B. [http://precedings.nature.com/documents/2027/version/2 Ontology (Science)], in C. Eschenbach and [[Michael Gruninger|M. Gruninger]] (eds.), Formal Ontology in Information Systems. Proceedings of FOIS 2008, Amsterdam/New York: ISO Press, 21&ndash;35.\n* [[Uschold, Mike]] & [[Michael Gruninger|Gruninger, M.]] (1996). [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.5903&rep=rep1&type=pdf Ontologies: Principles, Methods and Applications]. Knowledge Engineering Review, 11(2).\n* W. Pidcock, [http://infogrid.org/wiki/Reference/PidcockArticle \'\'What are the differences between a vocabulary, a taxonomy, a thesaurus, an ontology, and a meta-model?\'\']\n* Yudelson, M., Gavrilova, T., & Brusilovsky, P. 2005. [http://www.springerlink.com/index/3n0ekp8dgm4v3pr2.pdf Towards User Modeling Meta-ontology]. Lecture Notes in Computer Science, 3538: 448.\n* Movshovitz-Attias, Dana and Cohen, William W. (2012) [http://www.cs.cmu.edu/~dmovshov/papers/dma_bioNELL_bioNLP2012.pdf Bootstrapping Biomedical Ontologies for Scientific Text using NELL]. BioNLP in NAACL, Association for Computational Linguistics, 2012.\n\n==External links==\n{{Commons category|Ontology}}\n* [http://www.dmoz.org/Reference/Knowledge_Management/Knowledge_Representation/ Knowledge Representation] at Open Directory Project\n* [http://protegewiki.stanford.edu/wiki/Protege_Ontology_Library Library of ontologies]\n* [http://www.GoPubMed.com GoPubMed] using Ontologies for searching\n* [http://ontolog.cim3.net/wiki ONTOLOG] (a.k.a. "[http://ontolog.cim3.net/forum/ontolog-forum/ Ontolog Forum]") - an Open, International, Virtual Community of Practice on Ontology, Ontological Engineering and Semantic Technology\n* [http://trimc-nlp.blogspot.com/2013/08/nlp-driven-ontology-modeling-for.html Use of Ontologies in Natural Language Processing]\n* [http://ontolog.cim3.net/cgi-bin/wiki.pl?OntologySummit Ontology Summit] - an annual series of events (first started in 2006) that involves the ontology community and communities related to each year\'s theme chosen for the summit.\n<!--\n***********************************************************************************************\n\n      This section shouldn\'t contain external links to specific ontologies,\n      or to specific new subjects.\n\n***********************************************************************************************\n-->\n* [http://kore-nordmann.de/talks/09_04_standardization_of_ontologies_paper.pdf Standardization of Ontologies]\n\n{{Semantic Web}}\n{{Software engineering}}\n{{computable knowledge}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Ontology (Information Science)}}\n[[Category:Knowledge engineering]]\n[[Category:Technical communication]]\n[[Category:Information science]]\n[[Category:Semantic Web]]\n[[Category:Ontology (information science)| ]]\n[[Category:Knowledge representation]]\n[[Category:Knowledge bases]]']
['Enterprise interoperability', '24007403', '\'\'\'Enterprise interoperability\'\'\' is the ability of an enterprise—a company or other large organization—to functionally link activities, such as [[product design]], [[supply chains]], manufacturing, in an efficient and competitive way. \n\nThe research in interoperability of enterprise practised in is various domains itself ([[Enterprise Modelling]], [[Ontologies]], [[Information systems]], Architectures and Platforms) which it is a question of positioning.<ref name="Doumei2008"/>\n\n== Enterprise interoperability topics ==\n===Interoperability in enterprise architecture===\nEnterprise architecture (EA) presents a high level design of enterprise capabilities that defines successful IT projects in coherence with enterprise principals and business related requirements. EA covers mainly (i) the business capabilities analysis and validation; (ii) the development of business, application, data and technical architectures and solutions, and finally (iii) the control of programme and project implementation and governance. The application of EA methodology feeds the enterprise repository reference frame with sets of building blocks used to compose the targeted system.\n\nThe interoperability can be considered either as a principal, requirement or constraint that impact the definition of patterns to compose building blocks in the definition of targeted architectural roadmap. In this scope, EA within the TOGAF perspective,<ref name="TOGAF2011"/> aims to reconcile interoperability requirements with potential solutions that make developed systems interoperable.\nSo as to maintain the interoperability challenge quite present in the next steps of system’s lifecycle, several models and Frameworks are developed under the topic enterprise interoperability.\n\n===Enterprise interoperability frameworks===\nTo preserve interoperability, several [[Enterprise Interoperability Framework|enterprise interoperability frameworks]] can be identified in the literature:\n* 2003: IDEAS:<ref name="IDEAS"/> Interoperability Developments for Enterprise Application and Software.\n* 2004: EIF:<ref name="EIF"/> The European Interoperability Framework\n* 2004: e-GIF:<ref name="eGIF"/> e-Government Interoperability Framework\n* 2006: FEI:<ref name="FEI"/> The Framework for Enterprise Interoperability\n* 2006: C4IF:<ref name="Peristeras2006"/> Connection, Communication, Consolidation, Collaboration Interoperability Framework\n* 2007: AIF:<ref name="ATHENA"/> Athena Interoperability Framework\n* 2007:<ref name="EntSysArch2007"/>  Enterprise Architecture Framework for Agile and Interoperable Virtual Enterprises\n\nThe majority of these frameworks considers enterprise at several aspects, viewpoints or abstraction levels: business, process, knowledge, application, technology, data, technic, etc. and proposes guidelines to support modeling and connection capabilities between these levels. The semantic challenge is considered as transversal to all these abstraction levels.\nSetting up and applying guidelines and methodologies developed within these frameworks requires modeling efforts that identify and connect artifacts.\n\n===Interoperability in software engineering===\nThe evolution of IT technologies aims to outsource IT capabilities to vendors to manage for use on demand. The evolution pathway starts form packaged solutions and goes through Infrastructure as a service (Iaas), Platform as a service (Paas), Software as a service (Saas) and recently the Cloud. Interoperability efforts are still mainly expected among these levels:\n* strategy to business \n* business to processes\n* processes to application\n\nDealing with business process definition, alignment, collaboration and interoperability, several international standards propose methodologies and guidelines in these perspectives:\n* ISO 15704—Requirements for enterprise-reference architectures and methodologies\n* CEN-ISO DIS 19439—Framework for Enterprise Modeling\n* CEN-ISO WD 19440—Constructs for Enterprise Modeling\n* ISO 18629—Process specification language\n* ISO/IEC 15414—ODP Reference Model—Enterprise Language \nIn addition, recent standards (BPMN, BPEL, etc.) and their implementation technologies propose relevant integration capabilities. Furthermore, model driven-engineering <ref name="MDAguide"/> provides capabilities that connect, transform and refine models to support interoperability.\n\n===Metrics for interoperability maturity assessment===\nThe following approaches propose some metrics to assess the interoperability maturity,<ref name="Ford2008"/><ref name="GUEDRIA"/> \n* LISI: Levels of Information Systems Interoperability\n* OIM: Organizational Interoperability Model\n* NMI: NC3TA reference Model for Interoperability\n* LCIM: Levels of Conceptual Interoperability Model\n* EIMM: Enterprise Interoperability Maturity Model\n* Smart Grid Interoperability Maturity Model Rating System\n\nFor the several interoperability aspects identified previously, the listed maturity approaches define interoperability categories (or dimensions) and propose qualitative as well as qualitative cross cutting issues to assess them. While interoperability aspects are not covered by a single maturity approach, some propositions go deeply in the definition of metric dimensions at one interoperability aspect such as the business interoperability measurement proposed by Aneesh.<ref name="Zutshi"/>\n\n== See also ==\n* [[INTEROP-VLab]]\n\n== References ==\n{{reflist|\nrefs=\n<ref name=Doumei2008>Chen, D., [[Guy Doumeingts|Doumeingts]], G., and [[François Vernadat|Vernadat, F.]] 2008. Architectures for enterprise integration and interoperability: Past, present and future. \'\'Comput. Ind.\'\' 59, 7 (Sep. 2008), 647–659. {{en icon}} [http://dx.doi.org/10.1016/j.compind.2007.12.016 : DOI]</ref>\n<ref name=TOGAF2011>TOGAF® 9 Certified, 2nd edition. The Open Group, 2011.</ref>\n<ref name=IDEAS>“A Contribution to Enterprise Interoperability Maturity Assessment”</ref>\n<ref name=EIF>EIF 2.0 http://ec.europa.eu/idabc/servlets/Docb0db.pdf</ref>\n<ref name=eGIF>http://edina.ac.uk/projects/interoperability/e-gif-v6-0_.pdf</ref>\n<ref name=FEI>http://chen33.free.fr/M2/Elearning/CIGI2009.Chen.final.pdf</ref>\n<ref name=Peristeras2006>Peristeras, V., and K. Tarabanis (2006): The Connection, Communication, Consolidation, Collaboration Interoperability Framework (C4IF) for Information Systems Interoperability, International Journal of Interoperability in Business Information Systems (IBIS), Vol. 1, No. 1, pp. 61-72.</ref>\n<ref name=ATHENA>http://www.asd-ssg.org/html/ATHENA/Deliverables/Deliverables%20provided%20to%20EC%206th%206%20Months/070306_ATHENA_DA82_V10.pdf</ref>\n<ref name=EntSysArch2007>Handbook of Enterprise Systems Architecture in Practice, 2007</ref>\n<ref name=MDAguide>http://www.omg.org/cgi-bin/doc?omg/03-06-01.pdf</ref>\n<ref name=Ford2008>Ford T., et al. Measuring System Interoperability: An i-Score Improvement. Proceedings of the 6th Annual Conference on Systems Engineering Research. Los Angeles, CA, April 4–5, 2008</ref>\n<ref name=GUEDRIA>GUEDRIAhttp://ori-oai.u-bordeaux1.fr/pdf/2012/GUEDRIA_WIDED_2012.pdf</ref>\n<ref name=Zutshi>http://run.unl.pt/bitstream/10362/2646/1/Zutshi_2010.pdf</ref>\n}}\n\n== External links ==\n* [http://www.interop-vlab.eu INTEROP-VLab]\n\n[[Category:Interoperability]]\n[[Category:Enterprise modelling]]\n[[Category:Knowledge representation]]']
['Completeness (knowledge bases)', '25154746', "A [[knowledge base]] KB is '''complete''' ''if'' there is no formular α such that KB ⊭ α and KB ⊭ ¬α.\n\nExample of knowledge base with incomplete knowledge:\n\nKB := { A ∨ B }\n\nThen we have KB ⊭ A and KB ⊭ ¬A.\n\nIn some cases, you can make a [[Consistency (knowledge bases)|consistent knowledge base]] complete with the [[closed world assumption]] - that is, adding all not-entailed literals as negations to the knowledge base. In the above example though, this would not work because it would make the knowledge base inconsistent:\n\nKB' = { A ∨ B, ¬A, ¬B }\n\nIn the case you have KB := { P(a), Q(a), Q(b) }, you have KB ⊭ P(b) and KB ⊭ ¬P(b), so with the closed world assumption you would get KB' = { P(a), ¬P(b), Q(a), Q(b) } where you have KB' ⊨ ¬P(b).\n\nSee also:\n\n* [[Vivid knowledge]]\n\n{{computable knowledge}}\n\n[[Category:Knowledge representation]]\n{{logic-stub}}\n{{database-stub}}"]
['Semantic data model', '19558680', '[[File:A2 4 Semantic Data Models.svg|thumb|320px|Semantic data models.<ref name ="FIPS184">[http://www.itl.nist.gov/fipspubs/idef1x.doc FIPS Publication 184] released of IDEF1X by the Computer Systems Laboratory of the National Institute of Standards and Technology (NIST). 21 December 1993.</ref>]]\nA \'\'\'semantic data model\'\'\' in [[software engineering]] has various meanings: \n# It is a [[conceptual data model]] in which semantic information is included. This means that the model describes the meaning of its instances. Such a semantic [[data model]] is an abstraction that defines how the stored [[symbol]]s (the instance data) relate to the real world.<ref name ="FIPS184"/>\n# It is a [[conceptual data model]] that includes the capability to express information that enables parties to the information exchange to interpret meaning (semantics) from the instances, without the need to know the meta-model. Such semantic models are fact oriented (as opposed to object oriented). Facts are typically expressed by [[binary relations]] between [[data]] elements, whereas higher order relations are expressed as collections of binary relations. Typically binary relations have the form of triples: Object-RelationType-Object. For example: the Eiffel Tower <is located in> Paris.\nTypically the instance data of semantic data models explicitly include the kinds of relationships between the various data elements, such as <is located in>. To interpret the meaning of the facts from the instances it is required that the meaning of the kinds of relations (relation types) be known. Therefore, semantic data models typically standardise such relation types. This means that the second kind of semantic data models enable that the instances express facts that include their own meaning. \nThe second kind of semantic data models are usually meant to create semantic databases. The ability to include meaning in semantic databases facilitates building [[distributed database]]s that enable applications to interpret the meaning from the content. This implies that semantic databases can be integrated when they use the same (standard) relation types. This also implies that in general they have a wider applicability than relational or object oriented databases.\n\n== Overview ==\nThe logical data structure of a [[database management system]] (DBMS), whether [[Hierarchical model|hierarchical]], [[Network model|network]], or [[Relational model|relational]], cannot totally satisfy the [[Requirements analysis|requirements]] for a conceptual definition of data, because it is limited in scope and biased toward the implementation strategy employed by the DBMS. Therefore, the need to define data from a [[Three schema approach|conceptual view]] has led to the development of semantic data modeling techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure. The real world, in terms of resources, ideas, events, etc., are symbolically defined within physical data stores. A semantic data model is an abstraction which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world.<ref name ="FIPS184"/>\n\nAccording to Klas and Schrefl (1995), the "overall goal of semantic data models is to capture more meaning of data by integrating relational concepts with more powerful abstraction concepts known from the [[Artificial Intelligence]] field. The idea is to provide high level modeling primitives as integral part of a data model in order to facilitate the representation of real world situations".<ref>Wolfgang Klas, Michael Schrefl (1995). "Semantic data modeling" In: \'\'Metaclasses and Their Application\'\'. Book Series Lecture Notes in Computer Science. Publisher Springer Berlin / Heidelberg. Volume Volume 943/1995.</ref>\n\n== History ==\nThe need for semantic data models was first recognized by the U.S. Air Force in the mid-1970s as a result of the [[Integrated Computer-Aided Manufacturing]] (ICAM) Program. The objective of this program was to increase manufacturing productivity through the systematic application of computer technology. The ICAM Program identified a need for better analysis and communication techniques for people involved in improving manufacturing productivity. As a result, the ICAM Program developed a series of techniques known as the IDEF (ICAM Definition) Methods which included the following:<ref name ="FIPS184"/>\n* [[IDEF0]] used to produce a “function model” which is a structured representation of the activities or processes within the environment or system.\n* [[IDEF1]] used to produce an “information model” which represents the structure and semantics of information within the environment or system.\n** [[IDEF1X]] is a semantic data modeling technique. It is used to produce a graphical information model which represents the structure and semantics of information within an environment or system. Use of this standard permits the construction of semantic data models which may serve to support the management of data as a resource, the integration of information systems, and the building of computer databases.\n* [[IDEF2]] used to produce a “dynamics model” which represents the time varying behavioral characteristics of the environment or system.\n\nDuring the 1990s the application of semantic modelling techniques resulted in the semantic data models of the second kind. An example of such is the semantic data model that is standardised as [[ISO 15926]]-2 (2002), which is further developed into the semantic modelling language [[Gellish]] (2005). The definition of the Gellish language is documented in the form of a semantic data model. Gellish itself is a semantic modelling language, that can be used to create other semantic models. Those semantic models can be stored in Gellish Databases, being semantic databases.\n\n== Applications ==\nA semantic data model can be used to serve many purposes. Some key objectives include:<ref name ="FIPS184"/>\n* Planning of Data Resources: A preliminary data model can be used to provide an overall view of the data required to run an enterprise. The model can then be analyzed to identify and scope projects to build shared data resources.\n* Building of Shareable Databases: A fully developed model can be used to define an application independent view of data which can be validated by users and then transformed into a physical database design for any of the various DBMS technologies. In addition to generating databases which are consistent and shareable, development costs can be drastically reduced through data modeling.\n* Evaluation of Vendor Software: Since a data model actually represents the infrastructure of an organization, vendor software can be evaluated against a company’s data model in order to identify possible inconsistencies between the infrastructure implied by the software and the way the company actually does business.\n* Integration of Existing Databases: By defining the contents of existing databases with semantic data models, an integrated data definition can be derived. With the proper technology, the resulting conceptual schema can be used to control transaction processing in a distributed database environment. The U.S. Air Force Integrated Information Support System (I2S2) is an experimental development and demonstration of this type of technology applied to a heterogeneous DBMS environment.\n\n== See also ==\n* [[Conceptual schema]]\n* [[Object-role modeling]]\n* [[Entity-relationship model]]\n* [[Information model]]\n* [[Relational Model/Tasmania]]\n* [[Three schema approach]]\n* [[QuakeSim]]\n\n== References ==\n{{NIST-PD}}\n{{reflist}}\n\n== Further reading ==\n* [http://hpdrc.cs.fiu.edu/library/books/datades-book/ Database Design - The Semantic Modelling Approach]\n* Johan ter Bekke (1992). \'\'Semantic Data Modeling\'\'. Prentice Hall.\n* Alfonso F. Cardenas and Dennis McLeod (1990). \'\'Research Foundations in Object-Oriented and Semantic Database Systems\'\'. Prentice Hall.\n* Peter Gray, Krishnarao G. Kulkarni and, Norman W. Paton (1992). \'\'Object-Oriented Databases: A Semantic Data Model Approach\'\'. Prentice-Hall International Series in Computer Science.\n* Michael Hammer and Dennis McLeod (1978). "The Semantic Data Model: a Modeling Mechanism for Data Base Applications." In: \'\'Proc. ACM SIGMOD Int’l. Conf. on Management of Data\'\'. Austin, Texas, May 31 - June 2, 1978, pp.&nbsp;26–36.\n\n== External links ==\n* [http://www.jhterbekke.net/SemanticDataModeling.html Semantic Data Modeling] Johan ter Bekke tribute site.\n\n{{Data model}}\n\n{{DEFAULTSORT:Semantic Data Model}}\n[[Category:Data modeling]]\n[[Category:Systems analysis]]\n[[Category:Knowledge representation]]']
['Semantic parameterization', '18922270', '\'\'\'Semantic parameterization\'\'\' is a conceptual modeling process for expressing natural language descriptions of a domain in first-order predicate logic.<ref>Travis D. Breaux and Annie I. Antón (2004). [http://theprivacyplace.org/blog/wp-content/uploads/2008/07/tr-2004-36.pdf Deriving Semantic Models from Privacy Policies]. North Carolina State University Computer Science Technical Report TR-2004-36.</ref><ref>Travis D. Breaux and Annie I. Antón (2008). [http://theprivacyplace.org/blog/wp-content/uploads/2008/07/tr-2005-31.pdf "Mining Rule Semantics to Understand Legislative Compliance"]. North Carolina State University Computer Science Technical Report TR-2005-31.</ref><ref name="Breaux">T.D. Breaux, A.I. Anton, J. Doyle, [http://www4.ncsu.edu/~tdbreaux/publications/tdbreaux-tosem09.pdf "Semantic parameterization: a process for modeling domain descriptions"], \'\'ACM Transactions on Software Engineering Methodology\'\', vol. 18, no. 2, Article 5, 2008.</ref> The process yields a formalization of natural language sentences in [[Description Logic]] to answer the \'\'who,\'\' \'\'what\'\' and \'\'where\'\' questions in the Inquiry-Cycle Model (ICM) developed by Colin Potts and his colleagues at the Georgia Institute of Technology.<ref name="Potts">C. Potts, K. Takahashi, and A.I. Anton, "Inquiry-based requirements analysis", \'\'IEEE Software\'\' 11(2): 21–32, 1994.</ref> The parameterization process complements the Knowledge Acquisition and autOmated Specification (KAOS) method,<ref>A. Dardenne, A. van Lamsweerde and S. Fickas, "Goal-Directed Requirements Acquisition", \'\'Science of Computer Programming\'\' v. 20, North Holland, 1993, pp. 3-50.</ref> which formalizes answers to the \'\'when\'\', \'\'why\'\' and \'\'how\'\' ICM questions in [[Temporal Logic]], to complete the ICM formalization. The artifacts used in the parameterization process include a dictionary that aligns the domain lexicon with unique concepts, distinguishing between [[synonyms]] and [[polysemes]], and several natural language patterns that aid in mapping common domain descriptions to formal specifications.\n\n== Relationship to other theories ==\n\nSemantic Parameterization defines a meta-model consisting of eight roles that are domain-independent and reusable. Seven of these roles correspond to Jeffrey Gruber\'s [[thematic relations]]<ref>J. Gruber, \'\'Lexical Structures in Syntax and Semantics\'\', North Holland, New York, 1976.</ref> and [[case role]]s in Charles Fillmore\'s [[case grammar]]:<ref>C. Fillmore, "The Case for Case", \'\'Universals in Linguistic Theory\'\', Holt, Rhinehart and Winston, New York, 1968.</ref>\n\n{| class="wikitable" border="1" cellpadding="3" align="center"\n|+ Meta-model Mapping to Case Frames and Thematic Relations\n! Breaux\'s Meta-model\n! Fillmore\'s Case Roles\n! Thematic Relations\n|-\n| Subject\n| Agentive\n| Agent\n|-\n| Action\n|\n|-\n| Object\n| Objective/ Factitive\n| Theme/ Patient\n|-\n| Target\n| Dative\n| Goal\n|-\n| Source\n| Source\n| Source\n|-\n| Instrument\n| Instrumental\n| Instrument\n|-\n| Purpose\n|\n| Purposive\n|-\n| Location\n| Locative\n| Location\n|-\n|\n| Comitative\n| Accompaniment\n|}\n\nThe Inquiry-Cycle Model (ICM) was introduced to drive elicitation between engineers and stakeholders in requirements engineering.<ref name="Potts" /> The ICM consists of \'\'who\'\', \'\'what\'\', \'\'where\'\', \'\'why\'\', \'\'how\'\' and \'\'when\'\' questions. All but the \'\'when\'\' questions, which require a [[Temporal Logic]] to represent such phenomena, have been aligned with the meta-model in semantic parameterization using [[Description Logic]] (DL).\n\n{| class="wikitable" border="1" cellpadding="3" align="center"\n|+ Mapping from DL roles to questions in the Inquiry-Cycle Model\n! DL Role in Meta-model\n! ICM Question\n|-\n| isSubjectOf.Activity\n| Who performs the action?\n|-\n| isObjectOf.Activity\n| Upon what is the action performed?\n|-\n| isTargetOf.Activity\n| With whom is the transaction performed?\n|-\n| isPurposeOf.Activity\n| Why is the action performed?\n|-\n| isInstrumentOf.Activity\n| How is the action performed?\n|-\n| isLocationOf.Activity\n| Where is the action performed?\n|}\n\n== Introduction with Example ==\n\nThe semantic parameterization process is based on [[Description Logic]], wherein the TBox is composed of words in a \'\'dictionary\'\', including nouns, verbs, and adjectives, and the ABox is partitioned into two sets of assertions: 1) those assertions that come from words in the natural language statement, called the \'\'grounding\'\', and 2) those assertions that are inferred by the (human) modeler, called the \'\'meta-model\'\'. Consider the following unstructured natural language statement (UNLS) (see Breaux et al.<ref name="Breaux" /> for an extended discussion):\n\n;UNLS<sub>1.0</sub>: The customer<sub>1,1</sub> must not share<sub>2,2</sub> the access-code<sub>3,3</sub> of the customer<sub>1,1</sub> with someone<sub>4,4</sub> who is not the provider<sub>5,4</sub>.\n\nThe modeler first identifies intensional and extensional polysemes and synonyms, denoted by the subscripts: the first subscript uniquely refers to the intensional index, i.e., the same first index in two or more words refer to the same concept in the TBox; the second subscript uniquely refers to the extensional index, i.e., two same second index in two or more words refer to the same individual in the ABox. This indexing step aligns words in the statement and concepts in the dictionary. Next, the modeler identifies concepts from the dictionary to compose the meta-model. The following table illustrates the complete DL expression that results from applying semantic paramterization.\n\n{| class="wikitable" border="1" cellpadding="3" align="center"\n|+ The grounding {{mvar|G}} and meta-model {{mvar|M}} derived from UNLS<sub>1.0</sub>\n! Grounding ({{mvar|G}})\n! Meta-model ({{mvar|M}})\n|-valign="top"\n| {{math|Customer(\'\'p\'\'<sub>1</sub>) <br />\n⨅ Share(\'\'p\'\'<sub>2</sub>) <br />\n⨅ isAccessCodeOf(\'\'p\'\'<sub>3</sub>, \'\'p\'\'<sub>1</sub>) <br />\n⨅ Someone(\'\'p\'\'<sub>4</sub>) <br />\n⨅ Provider(\'\'p\'\'<sub>4</sub>)}}\n| {{math|Activity(\'\'p\'\'<sub>5</sub>) <br />\n⨅ hasSubject(\'\'p\'\'<sub>5</sub>, \'\'p\'\'<sub>1</sub>) <br />\n⨅ hasAction(\'\'p\'\'<sub>5</sub>, \'\'p\'\'<sub>2</sub>) <br />\n⨅ hasObject(\'\'p\'\'<sub>5</sub>, \'\'p\'\'<sub>3</sub>) <br />\n⨅ hasTarget(\'\'p\'\'<sub>5</sub>, \'\'p\'\'<sub>4</sub>) <br />\n⨅ isRefrainmentOf(\'\'p\'\'<sub>5</sub>, \'\'p\'\'<sub>1</sub>)}}\n|}\n\n== References ==\n{{Reflist}}\n\n{{DEFAULTSORT:Semantic Parameterization}}\n[[Category:Knowledge representation]]']
['Darwin Core Archive', '29824007', '{{Orphan|date=January 2011}}\n\n\'\'\'Darwin Core Archive\'\'\' (DwC-A) is a [[biodiversity informatics]] data standard that makes use of the [[Darwin Core]] terms to produce a single, self-contained dataset for species occurrence or checklist data. Essentially it is a set of text (CSV) files with a simple descriptor (meta.xml) to inform others how your files are organized. The format is defined in the Darwin Core Text Guidelines.<ref name="dwc-text">[http://rs.tdwg.org/dwc/terms/guides/text/index.htm Darwin Core Text Guidelines]</ref> It is the preferred format for publishing data to the [[GBIF]] network. \n\n__TOC__\n\n==Darwin Core==\nThe Darwin Core standard has been used to mobilise the vast majority of specimen occurrence and observational records within the GBIF network.<ref name="gbif-dwca">[http://www.gbif.org/resources/2552 GBIF Darwin Core Archive, How-to Guide]</ref> The [[Darwin Core]] standard was originally conceived to facilitate the discovery, retrieval, and integration of information about modern biological specimens, their spatio-temporal occurrence, and their supporting evidence housed in collections (physical or digital).\n\nThe Darwin Core today is broader in scope. It aims to provide a stable, standard reference for sharing information on biological diversity. As a glossary of terms, the Darwin Core provides stable semantic definitions with the goal of being maximally reusable in a variety of contexts. This means that Darwin Core may still be used in the same way it has historically been used, but may also serve as the basis for building more complex exchange formats, while still ensuring interoperability through a common set of terms.\n\n==Archive Format==\n{{unreferenced section|date=December 2010}}\nThe central idea of an archive is that its data files are logically arranged in a star-like manner, with one core data file surrounded by any number of ’extensions’. Each extension record (or ‘extension file row’) points to a record in the core file; in this way, many extension records can exist for each single core record.\n\nDetails about recommended extensions can be found in their respective subsections and will be extensively documented in the GBIF registry, which will catalogue all available extensions.\n\nSharing entire datasets instead of using pageable web services like DiGIR and TAPIR allows much simpler and more efficient data transfer. For example, retrieving 260,000 records via TAPIR takes about nine hours, issuing 1,300 http requests to transfer 500 MB of XML-formatted data. The exact same dataset, encoded as DwC-A and zipped, becomes a 3 MB file. Therefore, GBIF highly recommends compressing an archive using ZIP or GZIP when generating a DwC-A. \n\nAn archive requires stable identifiers for core records, but not for extensions. For any kind of shared data it is therefore necessary to have some sort of local record identifiers. It’s good practice to maintain – with the original data – identifiers that are stable over time and are not being reused after the record is deleted. If you can, please provide globally unique identifiers instead of local ones.\n\n===Archive Descriptor===\nTo be completed.\n\n<!--\n===Data Files===\nTo be completed.\n-->\n\n===Dataset Metadata===\nA Darwin Core Archive should contain a file containing metadata describing the whole dataset. The [[Ecological Metadata Language]] (EML) is the most common format for this, but simple Dublin Core files are being used too.\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://rs.tdwg.org/dwc/terms/index.htm Darwin Core Quick Reference Guide]\n* [[Biodiversity Information Standards]] (TDWG)\n* [[Global Biodiversity Information Facility]] (GBIF)\n* [[Biodiversity informatics]]\n\n[[Category:Bioinformatics]]\n[[Category:Knowledge representation]]\n[[Category:Interoperability]]']
['Integrated Operations in the High North', '22713707', '{{Infobox Organization\n|name         = Integrated Operations in the High North\n|image        = IOHN logo small.gif\n|size         = 200\n|alt          = Logo for Integrated Operations in the High North.\n|caption      = Logo for Integrated Operations in the High North.\n|abbreviation = IOHN or IO High North\n|formation    = 2008-05-06\n|status       = Project at [[Det Norske Veritas|Det Norske Veritas (DNV)]]\n|purpose      = Designing, implementing and testing a Digital Platform for the next generation of [[Integrated Operations]]\n|location     = Bærum, Norway\n|region_served = Worldwide\n|membership   = 22\n|language     = English\n|leader_title = Project Manager\n|leader_name  = [http://www.linkedin.com/in/fredericverhelst Frédéric Verhelst]\n|main_organ   = Steering Committee\n|affiliations = <!-- if any -->\n|num_staff    = \n|num_volunteers =\n|budget       = \n|website      = http://www.IOHN.org/\n}}\n\'\'\'Integrated Operations in the High North\'\'\' (\'\'\'IOHN\'\'\', \'\'\'IO High North or IO in the High North\'\'\') is a unique collaboration project that during a four-year period starting May 2008 is working on designing, implementing and testing a Digital Platform for what in the [[Upstream (oil industry)|Upstream Oil and Gas Industry]] is called the next or second generation of [[Integrated Operations]].<ref>\n{{cite web \n|url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf \n|title=Integrated Operations and the Oil and Gas Ontology \n|author=The [[Norwegian Oil Industry Association]] (OLF) and POSC Caesar Association (PCA) \n|accessdate=2009-05-06 \n|date=2007-09-19\n}}</ref> \nThe work on the Digital platform is focussed on capture, transfer and integration of [[Real-time data]] from the remote production installations to the decision makers. A risk evaluation across the whole chain is also included. The platform is based on [[open standards]] and enables a higher degree of [[interoperability]]. Requirements for the digital platform come from use cases defined within the [[Oil_and_gas_well_drilling#Drilling|Drilling]] and [[Oil_and_gas_well_drilling#Completion|Completion]], Reservoir and Production and Operations and Maintenance domains. The platform will subsequently be demonstrated through pilots within these three domains.<ref name="IOHNsite">\n{{cite web \n|url=http://trac.posccaesar.org/wiki/IOHN\n|title=Short introduction to the Integrated Operations in the High North (IOHN) project\n|author=Integrated Operations in the High North (IOHN) \n|accessdate=2009-05-07 \n}}</ref> \n\nThis new platform is considered an important enabler for safe and sustainable operations in remote, vulnerable and hazardous areas such as the [[Arctic|High North]],<ref>\n{{cite web \n|url=http://www.olf.no/news/norway-takes-a-leading-role-in-next-generation-integrated-operations-article18586-291.html \n|title=Norway takes a leading role in next generation Integrated Operations \n|author=The [[Norwegian Oil Industry Association]] (OLF) \n|accessdate=2009-05-07 \n|date=2008-08-26\n}}</ref><ref name="Rigzone">\n{{cite web \n|url=http://www.rigzone.com/news/article.asp?a_id=65883 \n|title=Norway Takes Reign to Provide Next Generation Integrated Operations \n|author=Rigzone E&P News\n|accessdate=2009-05-08 \n|date=2008-08-26\n}}</ref><ref name="DEJ">\n{{cite web \n|url=http://www.digitalenergyjournal.com/displaynews.php?NewsID=758&PHPSESSID=9hhp6qe4qqpi7qnbgffgqhv1h7\n|title=Norway developing next generation Integrated Operations \n|author=Digital Energy Journal\n|accessdate=2009-05-08 \n|date=2008-08-27\n}}</ref><ref name="EPMag">\n{{cite web \n|url=http://www.epmag.com/Magazine/2008/12/item24047.php \n|title=Offshore R&D pushes the limits \n|author=E&P Magazine\n|accessdate=2009-05-08 \n|date=2008-12-02\n}}</ref> but the technology is clearly also applicable in more general applications.\n\nThe IOHN project consortium consists of 23 participants,<ref name="IOHNmembers">\n{{cite web \n|url=http://www.posccaesar.org/wiki/IOHN/participants \n|title=List of participating companies in the IOHN project \n|author=[[IOHN]] \n|accessdate=2010-03-10 \n|date=2010-03-10\n}}</ref> including operators, service providers, software vendors, technology providers, research institutions and universities. In addition, the [[Norwegian Defence Force]] is working with the project to resolve common infrastructural and [[interoperability]] challenges.<ref name="IOHNsite"/>\n\nThe project is managed by [[DNV|Det Norske Veritas (DNV)]].<ref>\n{{cite web \n|url=http://www.dnv.com/news_events/news/2008/dnvleadsintegratedoperationsdevelopment.asp\n|title=DNV leads Integrated Operations development \n|author=[[Det Norske Veritas]] (DNV) \n|accessdate=2009-05-07 \n|date=2008-08-26\n}}</ref> Nils Sandsmark was the project manager during the initiation and start-up phase. Frédéric Verhelst took over as project manager from the beginning of 2009.<ref>\n{{cite web \n|url=http://www.linkedin.com/in/fredericverhelst\n|title=Profile of Frédéric Verhelst \n|author=[[LinkedIn]]\n|accessdate=2009-09-28 \n}}</ref>\n\nFinancing comes from the participants and the [[Research Council of Norway]] (RCN) for parts of the project (GOICT<ref>\n{{cite web \n|url=http://www.forskningsradet.no/servlet/Satellite?c=Prosjekt&cid=1207296035860&pagename=verdikt/Hovedsidemal&p=1226993814962 \n|title=Dependable ICT for the Energy Sector (GOICT, RCN proj.no. 183235, VERDIKT-programme)\n|author=The [[Research Council of Norway]] (RCN) \n|accessdate=2009-05-07 \n}}</ref>\nand AutoConRig<ref>\n{{cite web \n|url=http://www.forskningsradet.no/servlet/Satellite?c=Prosjekt&cid=1198060412649&pagename=ForskningsradetNorsk/Hovedsidemal&p=1181730334233 \n|title=Semi-autonomous control system for unmanned drilling rigs (AutoConRig, RCN proj.no. 187473, PETROMAKS-programme)\n|author=The [[Research Council of Norway]] (RCN) \n|accessdate=2009-05-07 \n}}</ref><ref>\n{{cite web \n|url=http://www.posccaesar.org/wiki/IOHN/AutoConRig \n|title=RCN/NFR project "AutoConRig"\n|author=Jens Ornæs (NOV)\n|accessdate=2009-07-02 \n}}</ref>).\n\n== Participants ==\nThe consortium consists of the following 22 participants<ref name="IOHNmembers"/> (in alphabetical order):<br />\n{| class="wikitable"\n|-\n| [[ABB Group|ABB]]\n| [http://www.abelia.no Abelia]\n| [[Baker Hughes]]\n| [[Cisco]]\n| [http://www.computas.no Computas]\n|-\n| [[Det Norske Veritas]]\n| [[Eni|ENI]]\n| [http://www.epsis.no Epsis]\n| [[FMC Technologies]]\n| [http://www.fsi.no FSI]\n|-\n| [http://www.ntnu.no/iocenter IO Center]\n| [http://www.iris.no IRIS]\n| [[National Oilwell Varco]]\n| [[Norwegian University of Science and Technology|NTNU]]\n| [http://www.olf.no OLF]\n|-\n| [[POSC Caesar Association]]\n| [http://www.ptil.no Petroleum Safety Authority Norway]\n| [[Siemens]]\n| [[Statoil]]\n| [[Norwegian Defence]]\n|-\n| [[University of Oslo]]\n| [[University of Stavanger]]\n|}\n\n== See also ==\n* [[Integrated Operations]]\n* [[Semantic Web]]\n* [[ISO 15926]] aka [[Oil and Gas Ontology]], an enabler for the next or second generation of [[Integrated Operations]] by integrating data across disciplines and business domains.\n* [[Petroleum exploration in the Arctic]]\n* [[POSC Caesar Association]], the custodian of [[ISO 15926]], the [[Oil and Gas Ontology]].\n\n== References ==\n{{reflist|2}}\n\n== External links ==\n* [http://www.IOHN.org/ Integrated Operations in the High North] website\n* [[W3C]] workshop on [http://www.w3.org/2008/12/ogws-report.html Semantic Web in Oil and Gas industry], Houston, December 9–10, 2008. [http://www.w3.org/2008/12/ogws-report#papers Position papers] from several participants in IOHN.\n* [http://www.posccaesar.org/wiki/PCA/SemanticDays2009/AboutSemanticDays Semantic Days 2009] conference, Stavanger, May 18–20, 2009. One [http://www.posccaesar.org/wiki/PCA/SemanticDays2009#Session6:SemantictechnologyforIOGeneration2 session] is devoted to IOHN.\n* [http://www.ioconf.no/2009/ IO 09 Science and Practice] conference, Trondheim, September 29–30, 2009. One [http://ioconf.no/2009/parallel6 session] is devoted to IOHN.\n* [http://www.oilit.com/2journal/2article/1003_16.htm#IOHN Integrated Operations in the High North—mid term report], \'\'Oil IT Journal\'\', March 2010.\n\n{{DEFAULTSORT:Integrated Operations In The High North}}\n[[Category:Petroleum organizations]]\n[[Category:Petroleum engineering]]\n[[Category:Semantic Web]]\n[[Category:Knowledge engineering]]\n[[Category:Information science]]\n[[Category:Ontology (information science)]]\n[[Category:Knowledge representation]]']
['NeOn Toolkit', '30724333', '\'\'\'The NeOn Toolkit\'\'\' is an open source, multi-platform [[ontology editor]], which supports the development of ontologies in [[Web Ontology Language|OWL]]/[[Resource Description Framework|RDF]]. The editor is based on the [[Eclipse (software)|Eclipse platform]] and provides a set of plug-ins (currently 20 plug-ins are available for the latest version, v2.4) covering a number of ontology engineering activities, including Annotation and Documentation, Modularization and Customization, Reuse, Ontology Evolution, translation<ref name="Espinoza2008">M. Espinoza, A. Gomez-Perez, and E. Mena. [http://sid.cps.unizar.es/PUBLICATIONS/POSTSCRIPTS/eswc08-localization.pdf Enriching an ontology with multilingual information]. In Proc. of 5th European Semantic Web Conference (ESWC\'08), Tenerife, (Spain), June 2008.</ref> and others. \n\nThe NeOn Toolkit has been developed in the course of the EU-funded NeOn project and is currently maintained and distributed by the NeOn Technologies Foundation.\n\n==References==\n<references/>\n\n== External links ==\n* [http://www.neon-foundation.org/ NeOn Technologies Foundation]\n* [http://neon-toolkit.org/ NeOn Toolkit Website]\n* [http://www.neon-project.org/ NeOn Project Website]\n\n[[Category:Knowledge representation]]\n[[Category:Free integrated development environments]]\n[[Category:Ontology (information science)]]\n[[Category:Ontology editors]]\n[[Category:Free software programmed in Java (programming language)]]']
['Category:Lexical databases', '31484297', '{{catmain|Lexical database}}\n\n[[Category:Translation databases]]\n[[Category:Computational linguistics]]\n[[Category:Knowledge representation]]']
['IMARK', '31933815', "{{Infobox website\n|name=Information Management Resource Kit (IMARK)\n|logo = \n|url ={{URL|http://www.imarkgroup.org}}\n|type = [[Capacity building]]\n|commercial      = No\n|registration    = Optional\n|language        = English, French, Spanish, Arabic, Chinese\n|launch date = 2001\n|current status = Online\n|screenshot      = }}\n\nThe Information Management Resource Kit ('''IMARK''') is a partnership-based [[e-learning]] initiative developed by the [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]] and partner organizations to support individuals, institutions and networks world-wide in the effective management of information and agricultural development. IMARK consists of a suite of [[distance learning]] resources and tools on [[information management]].<ref>{{cite web|url=http://www.imarkgroup.org/index_en.asp?m=0 |title=IMARK - Information Management Resource Kit |publisher=Imarkgroup.org |date= |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://usain.org/links2.html |title=Links for Agricultural Librarians |publisher=USAIN |date=2010-05-19 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://knowledge.cta.int/index.php/en/Dossiers/S-T-Issues-in-Perspective/ICT-for-transforming-research-for-agricultural-and-rural-development/Links/IMARK-FAO |title=IMARK (FAO) / Links / ICT for transforming research for agricultural and rural development / S&T Issues in Perspective / Dossiers / Home - Knowledge for Development |publisher=Knowledge.cta.int |date= |accessdate=2011-06-07}}</ref>\n\n== About IMARK ==\nThe [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]] initiated a partnership-based [[e-learning]] programme in 2001 to support [[information management]].<ref>http://www.fao.org/rdd/doc/IMARK%20General%20Sheet%20EN%2011-05.pdf</ref> IMARK is targeted at information professionals in developing countries. Each IMARK curriculum is designed through a consultative process with [[Subject-matter expert|subject matter experts]], field practitioners and representatives from the target audience from around the world. The IMARK initiative is a response to demand for enhanced information and [[knowledge management]] in the effort to achieve the [[Millennium Development Goals|Millennium Development Goals (MDGs)]], especially those related to [[hunger]] and the information society, in the context of bridging the digital divide.<ref>{{cite web|url=http://www.fao.org/rdd/doc/FAO%20Approach%20to%20WSIS2005.pdf |title=FAO's strategies towards the WSIS 2005 |format=PDF |date= |accessdate=2011-06-07}}</ref>  The development goal of IMARK is to improve the capabilities of people concerned with [[information management]] and [[knowledge sharing]].<ref>{{cite web|url=http://editlib.org/noaccesspresent/26495 |title=Ed/ITLib Digital Library → No Access |publisher=Editlib.org |date=2007-10-15 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://www.infodev.org/en/Publication.183.html |title=Quick Guide: ICT and Rural Livelihood Resources at FAO |publisher=infoDev.org |date=2006-09-28 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://www.icimod.org/index.php?page=23 |title=Knowledge Management |publisher=Icimod.org |date=2010-12-16 |accessdate=2011-06-07}}</ref>\n\n=== Objectives and Scope ===\nThe development goal of IMARK is to improve the overall effectiveness of programmes in agricultural development and [[food security]] by enhancing access to information by key stakeholders.<ref>http://eprints.rclis.org/bitstream/10760/15682/1/FAO%E2%80%99s%20Capacity-Building%20Initiatives%20in%20Accessing,%20Documenting,%20Communicating%20and%20Managing%20Agricultural%20Information.pdf</ref>\n\n=== Steering Committee ===\nIMARK has over 30 partners and collaborating institutions since its inception in 2001, and its activities are coordinated through a Steering Committee whose members include [[Association for Progressive Communications|The Association for Progressive Communications (APC)]], [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]], the [[Agence universitaire de la Francophonie|Agence Universitaire de la Francophonie (AUF)]], [[Commonwealth of Learning|Commonwealth of Learning (COL)]], [[Groupe de Recherches et d'Echanges Technologiques|Groupe de Recherches et d'Echanges Technologiques (GRET)]], [[Bibliotheca Alexandrina]] and [[UNESCO]].<ref>{{cite web|url=http://portal.unesco.org/ci/en/ev.php-URL_ID=21458&URL_DO=DO_TOPIC&URL_SECTION=201.html |title=IMARK launches new e-learning module |publisher=Portal.unesco.org |date=2008-01-24 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://www.apc.org/en/news/development/world/e-learning-module-developing-electronic-communitie |title=e-Learning module developing electronic communities &#124; Association for Progressive Communications |publisher=Apc.org |date=2006-05-08 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://webapp.ciat.cgiar.org/ccc/imark.htm |title=CCC: E-Learning Initiatives - IMARK-FAO |publisher=Webapp.ciat.cgiar.org |date= |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://www.iaald.org/docs/iisast2_report.pdf |title=2nd Expert Consultation IISAST |format=PDF |date= |accessdate=2011-06-07}}</ref>\n\n== See also ==\n* [[E-Learning|E-learning]]\n* [[FAO|Food and Agriculture Organization of the United Nations (FAO)]]\n* [[Agricultural Information Management Standards|Agricultural Information Management Standards (AIMS)]]\n\n== References ==\n{{Reflist|2}}\n\n== External links ==\n* [http://www.imarkgroup.org/ Official IMARK Website ]\n* [http://www.fao.org Food and Agriculture Organization of the United Nations]\n\n{{DEFAULTSORT:Imark}}\n[[Category:Information technology]]\n[[Category:Distance education]]\n[[Category:Education]]\n[[Category:Virtual learning environments]]\n[[Category:Learning methods]]\n[[Category:Educational technology projects]]\n[[Category:Rural development]]\n[[Category:Non-profit technology]]\n[[Category:Food and Agriculture Organization]]\n[[Category:Information technology management]]\n[[Category:Knowledge representation]]"]
['Social History and Industrial Classification', '34821434', "{{Underlinked|date=April 2014}}\n\n'''Social History and Industrial Classification''' (SHIC) is a classification system used by many British museums for social history and industrial collections.\nIt was first published in 1983.<ref>{{cite web|title=SHIC Home|url=http://www.holm.demon.co.uk/shic/}}</ref>\n\n==Purpose==\nSHIC was classifies materials (books, objects, recordings etc.) by their interaction with the people who used them. For example, a carpenter's hammer is classified with other tools of the carpenter, and not with a blacksmith's hammer.<ref>{{cite web|title=SHIC Section A|url=http://www.holm.demon.co.uk/shic/shicint.htm}}</ref> In contrast other classification systems, for example the [[Dewey Decimal Classification]], might class all hammers together and close to the classification for other percussive tools. The specialist subject network, Social History Curator's Group (SHCG), obtained funding in 2012 to develop an on-line version, now on their website http://www.shcg.org.uk/<ref>{{cite web|title=Social History Curators' Group - SHCG|url=http://www.shcg.org.uk/About-SHIC|accessdate=29 October 2012}}</ref>\n\n==Scheme==\nMaterials are classified under four major category numbers:\n#Community life\n#Domestic and family life\n#Personal life\n#Working life\n \nFurther classification within a category is by the use of further numbers after the decimal point.<ref>{{cite web|title=SHIC Section B|url=http://www.holm.demon.co.uk/shic/shicint.htm}}</ref> \n\nIt is permissible to assign more than one classification in cases where the object had more than one use.<ref>{{cite web|title=SHIC Section F |url=http://www.holm.demon.co.uk/shic/shicint.htm}}</ref>\n\n==References==\n{{reflist}}\n*''Social history and industrial classification (SHIC), a subject classification for museum collections'', University of Sheffield, 1983\n\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]\n[[Category:Social history]]\n[[Category:Industrial history]]"]
['GermaNet', '33768132', '{{primary sources|date=November 2011}}\n\'\'\'GermaNet\'\'\' is a lexical-semantic net for the [[German language]] that relates [[noun]]s, [[verb]]s, and [[adjective]]s semantically by grouping lexical units that express the same concept into \'\'[[synset]]s\'\' and by defining [[semantic]] relations between these synsets.<ref name="Storjohann2010">{{cite book|author=Petra Storjohann|title=Lexical-semantic relations: theoretical and practical perspectives|url=https://books.google.com/books?id=OYBWObJ547AC&pg=PA165|accessdate=16 November 2011|date=23 June 2010|publisher=John Benjamins Publishing Company|isbn=978-90-272-3138-3|pages=165–}}</ref> GermaNet has much in common with the English [[WordNet]] and can be viewed as an on-line [[thesaurus]] or a light-weight [[ontology (information science)|ontology]]. GermaNet has been developed and maintained within various projects at the research group for General and Computational Linguistics, [[University of Tübingen]] since 1997. It has been integrated into the [[EuroWordNet]], a multilingual lexical-semantic database.<ref name="homepage">[http://www.sfs.uni-tuebingen.de/lsd/index.shtml GermaNet homepage]</ref>\n\n==Database==\n\n===Contents===\nGermaNet  partitions the lexical space into a set of concepts that are interlinked by semantic relations. A semantic concept is modeled by a \'\'[[synset]]\'\'. A synset is a set of words (called lexical units) where all the words are taken to have (almost) the same meaning. Thus a synset is a set-representation of the semantic relation of synonymy, which means that it consists of a list of lexical units and a definition (paraphrase). The lexical units in turn have frames (which specify syntactic valence) and examples of their use.<ref name="GernEdiT">V. Henrich, E. Hinrichs. 2010. [http://www.lrec-conf.org/proceedings/lrec2010/pdf/264_Paper.pdf GernEdiT - The GermaNet Editing Tool]. In: \'\'Proceedings of the Seventh Conference on International Language Resources and Evaluation\'\'.</ref>\nJust as in WordNet, for each word category the semantic space is divided into a number of [[semantic field]]s closely related to major nodes in the semantic network: \'\'Ort\'\', or "location", \'\'Körper\'\', or "body", etc.<ref name="homepage"/>\n\nThe following is an up-to-date statistics of GermaNet\'s version 6.0 contents (release April 2011):\n \n*Number of synsets: 69594\n**Of which adjectives: 5991\n**Of which nouns: 53753\n**Of which verbs: 9850\n*Number of lexical units: 93407\n**Of which adjectives: 8582\n**Of which nouns: 71844\n**Of which verbs: 12981 <ref name="homepage"/>\n\n===Format===\nAll GermaNet data is stored in a relational [[PostgreSQL]] 5 database. The database model follows the internal structure of GermaNet: there are tables to store synsets, lexical units, conceptual and lexical relations, etc.<ref name="GernEdiT"/> The distribution format of all GermaNet data is [[XML]]. The two types of files, one for synsets and the other for relations, represent all data that is available in the GermaNet database.\n\n==Interfaces==\nThere are several [[Application Programming Interface]]s (API) available for [[Java (programming language)|Java]]<ref name="api">[http://www.sfs.uni-tuebingen.de/lsd/tools.shtml GermaNet APIs in Java]</ref> and for [[Perl]]. These APIs are distributed freely and provide easy access to all information in various versions of GermaNet.\n\n==Licenses==\nGermaNet 6.0 (released April 2011) can be distributed under one of the following types of [[software license agreement|license agreements]]: \'\'Academic Research Agreement\'\', \'\'Research and Development Agreement\'\', or \'\'Commercial Agreement\'\'. GermaNet is free for academic use.\n\n==Applications==\nGermaNet has been used for a variety of applications, including semantic analysis, shallow recognition of implicit document structure, compound analysis;<ref>Manuela Kunze and Dietmar Rösner. 2004. Issues in Exploiting GermaNet as a Resource in Real Applications.</ref> for analyzing selectional preferences,<ref>Sabine Schulte im Walde, 2004. GermaNet Synsets as Selectional Preferences in Semantic Verb Clustering.</ref> for word sense disambiguation,<ref>Saito et al., 2002. Evaluation of GermanNet: Problems Using GermaNet for Automatic Word Sense Disambiguation.</ref> etc.\n== See also==\n* [[Hyponym]]\n* [[Is-a]]\n* [[Machine-readable dictionary]]\n* [[Ontology (information science)]]\n* [[Semantic network]]\n* [[Semantic Web]]\n* [[Synonym Ring]]\n* [[Taxonomy (general)|Taxonomy]]\n* [[ThoughtTreasure]]\n* [[UBY-LMF]]\n* [[Word sense disambiguation]]\n\n==References==\n{{Reflist}}\n\n{{Authority control}}\n[[Category:German language]]\n[[Category:Thesauri]]\n[[Category:Lexical databases]]\n[[Category:Knowledge representation]]\n[[Category:Computational linguistics]]\n[[Category:Online dictionaries]]']
['Protégé (software)', '5007318', '{{Infobox software\n| name                   = Protégé\n| logo                   = <!-- Image name is enough -->\n| logo alt               = \n| screenshot             = <!-- Image name is enough -->\n| caption                = \n| screenshot alt         = \n| collapsible            = \n| author                 = \n| developer              = [[Stanford University School of Medicine|Stanford]] Center for Biomedical Informatics Research\n| released               = {{Start date and age|1999|11|11|df=yes}}<ref name=versions>{{cite web |title=Protege Desktop Older Versions |website=Protege Wiki |date=24 May 2016 |url=http://protegewiki.stanford.edu/wiki/Protege_Desktop_Old_Versions }}</ref>\n| discontinued           = \n| latest release version = 5.0.0\n| latest release date    = {{Start date and age|2016|05|24|df=yes}}<ref name=versions/>\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| status                 = Active\n| programming language   = [[Java (programming language)|Java]]\n| operating system       = Linux, Mac OS X & Windows<ref name=install5>{{cite web |title=Protege Desktop 5.0 Installation Instructions |website=Protege Wiki |url=http://protegewiki.stanford.edu/wiki/Install_Protege5 }}</ref>\n| platform               = Java {{abbr|VM|Virtual Machine}}<ref>{{cite web |title= Protege Desktop Frequently Asked Questions  |website=Protege Wiki |url=http://protegewiki.stanford.edu/wiki/Protege-OWL_4_FAQ }}</ref>\n| size                   = \n| language               = \n| language count         = <!-- Number only -->\n| language footnote      = \n| genre                  = [[Ontology (information science)#Editor|Ontology editor]]\n| license                = [[BSD licenses#2-clause license ("Simplified BSD License" or "FreeBSD License")|BSD 2-clause]]\n| alexa                  = \n| website                = {{URL|protege.stanford.edu}}\n| repo                   = {{URL|https://github.com/protegeproject/protege}}\n| standard               = \n| AsOf                   = \n}}\n\n\'\'\'Protégé\'\'\' is a free, open source [[Ontology (computer science)|ontology]] editor and a [[knowledge management]] system. Protégé provides a graphic user interface to define ontologies. It also includes [[deductive classifier]]s to validate that models are consistent and to infer new information based on the analysis of an ontology. Like [[Eclipse (software)|Eclipse]], Protégé is a framework for which various other projects suggest plugins. This application is written in [[Java (programming language)|Java]] and heavily uses [[Swing (Java)|Swing]] to create the user interface. Protégé recently has over 300,000 registered users.<ref>[http://protege.stanford.edu/community.php Protégé Community]</ref> According to a 2009 book it is "the leading ontological engineering tool".<ref name="SelicGaševic2009">{{cite book|author1=Dragan Gašević|author2=Dragan Djurić|author3=Vladan Devedžić|title=Model Driven Engineering and Ontology Development|url=https://books.google.com/books?id=s-9yu7ubSykC&pg=PA194|year= 2009|publisher=Springer|isbn=978-3-642-00282-3|pages=194|edition=2nd}}</ref>\n\nProtégé is being developed at [[Stanford University]] and is made available under the [[BSD licenses#2-clause license ("Simplified BSD License" or "FreeBSD License")|BSD 2-clause license]].<ref>{{cite web |title=protege/license.txt |website=GitHub |url=https://github.com/protegeproject/protege/blob/master/license.txt }}</ref> Earlier versions of the tool were developed in collaboration with the [[University of Manchester]].\n\n== References ==\n<references/>\n\n== External links ==\n* {{Official website|http://protege.stanford.edu/}}\n* [http://protegewiki.stanford.edu/wiki/Main_Page Protégé wiki]\n\n{{DEFAULTSORT:Protege (software)}}\n[[Category:Knowledge representation]]\n[[Category:Free integrated development environments]]\n[[Category:Ontology (information science)]]\n[[Category:Ontology editors]]\n[[Category:Free software programmed in Java (programming language)]]\n\n\n{{programming-software-stub}}']
['Harvard–Yenching Classification', '8311927', '[[Alfred Kaiming Chiu]] <ref>{{zh|t=裘開明 | s=裘开明 |p=Qiú Kāimíng|w=Ch\'iu<sup>2</sup> K\'ai<sup>1</sup>-Ming<sup>2</sup>}}.</ref> (1898–1977) was a pioneer of establishing a library classification system for Chinese language materials in the United States of America.  The system devised by him was known as \'\'\'Harvard–Yenching Classification System\'\'\'.  The system was primarily created for the classification of Chinese language materials in the [[Harvard-Yenching Library]] which was founded in 1927 at the [[Harvard-Yenching Institute]].<ref>Eugene W. Wu, "The Founding of the Harvard-Yenching Library," \'\'Journal of East Asian Libraries\'\' 101.1  (1993):  65-69.     [http://scholarsarchive.byu.edu/jeal/vol1993/iss101/16/]</ref>\n\nDuring that early period other systems, such as the early edition of the [[Library of Congress Classification]], did not consist of appropriate subject headings to classify the Chinese language materials, particularly the ancient published materials.  As many American libraries started to collect the ancient and contemporary published materials from China, a number of American libraries subsequently followed Harvard University to adopt Harvard–Yenching classification system, such as the East Asian Library of the University of California in Berkeley, Columbia University, The University of Chicago, Washington University in St. Louis etc.\n\nIn addition to American libraries, the libraries of other universities in the world including England, Australia, New Zealand, Hong Kong, Singapore etc. also followed Harvard University to adopt the system.  During the period from the 1930s to the 1970s, the use of the system became popular for classifying not only Chinese language materials but also other East Asian materials including Korean and Japanese language materials.\n\nDuring the period from the 1970s to the 1980s, a comprehensive subset of subject headings for Chinese language materials was gradually established in the Library of Congress Classification System so that almost a full spectrum of ancient and contemporary Chinese topics can be widely covered. As a result of this, the Library of Congress Classification System eventually replaced the Harvard–Yenching Classification System for all Chinese language materials acquired after the 1970s in many American Libraries.\n\nThough the system has largely been phased out, the system is still being used in some libraries for Chinese language materials acquired prior to the Library of Congress update. Such previously acquired books are normally stored in separate stacks in libraries. However, some of the university libraries in the Commonwealth countries of the United Kingdom such as England, Australia and New Zealand still continue to use the Harvard-Yenching system; for example, the Institute for Chinese Studies Library of the University of Oxford, University of Sydney, University of Melbourne, and University of Auckland.\n\n== The Harvard–Yenching classification system ==\nThe key classes of the system are listed as follows:\n\n===Key classes===\n** 0100–0999 Chinese Classics\n** 1000–1999 Philosophy and Religion\n** 2000–3999 Historical Sciences\n** 4000–4999 Social Sciences\n** 5000–5999 Language and Literature\n** 6000–6999 Fine and Recreative Arts\n** 7000–7999 Natural Sciences\n** 8000–8999 Agriculture and Technology\n** 9000–9999 Generalia and Bibliography\n\n===Subjects of sub-classes===\n{{Empty section|date=January 2011}}\n\n===0100 to 0999 Chinese Classics===\n** 0100–0199 Chinese classics in general\n** 0200–0299 I Ching\n** 0300–0399 Shu Ching\n** 0400–0499 Shih Ching\n** 0500–0669 San Li\n** 0680–0799 Ch’un Ch’iu\n** 0800–0849 Hsiao Ching\n** 0850–0999 Ssu Shu\n\n===1000 to 1999 Philosophy and Religion===\n** 1000–1008 Philosophy & religion in general\n** 1010–1429 Chinese philosophy\n** 1470–1499 Hindu philosophy\n** 1500–1539 Occident philosophy\n** 1540–1569 Philosophical problems and systems\n** 1570–1609 Logic\n** 1610–1649 Metaphysics\n** 1650–1699 Ethics\n** 1700–1729 Religion in general\n** 1730–1738 Mythology\n** 1739–1749 Occultism numerology\n** 1750–1779 History of religions\n** 1780–1799 Chinese state cults\n** 1800–1919 Buddhism\n** 1920–1939 Taoism\n** 1975–1987 Christianity\n** 1988–1999 Other religions\n\n===2000 to 3999 Historical Sciences===\n** 2000–2049 Archaeology, Antiquities in general\n** 2060–2159 China archaeology\n** 2200–2249 Ethnology, ethnography\n** 2250–2299 Genealogy and biography\n** 2300–2349 World history\n** 2350–2399 World geography\n** 2400–2440 Asian history and geography\n** 2450–2459 History of China in general\n** 2461–2469 Chinese historiography\n** 2470–2479 History of Chinese civilisation\n** 2480–2509 Diplomatic history of China\n** 2510–2519 General China history\n** 2520–2533 Ancient history of China in general\n** 2535 Ch’in, Han and 3 Kingdom in general\n** 2536–2543 Ch’in Dynasty\n** 2545–2559 Han Dynasty\n** 2560–2567 The Three Kingdom\n** 2570 Chin Dynasty and the Southern / Northern Dynasties\n** 2571–2578 Chin Dynasty (265–420)\n** 2581–2588 The Southern Dynasties\n** 2590–2599 The Northern Dynasties\n** 2605–2618 Sui, T’ang & the Five Dynasties in general\n** 2605–2619 Sui Dynasty\n** 2620–2639 T’ang Dynasty\n** 2640–2649 Epoch of the Five Dynasties (North)\n** 2650–2660 The Ten Kingdoms (South)\n** 2662 Sung, Liao, Chin and Yuan Dynasties in general\n** 2665–2684 Sung Dynasty (960–1279)\n** 2685–2688 The Liao Kingdom (916–1201)\n** 2690 The Chin Kingdom (1115–1234)\n** 2695 The [[Hsi Hsia Kingdom]] (982–1227)\n** 2700–2713 Yuan Dynasty (1280–1268)\n** 2718 Ming and Ching Dynasties in general\n** 2720–2739 Ming Dynasty\n** 2740–2969 Ch’ing Dynasty\n** 2970 Period of Republic, 1912\n** 3000–3019 China: geography & history in general\n** 3020–3031 General system treatises\n** 3032–3049 Special works of geography: China\n** 3507–3079 China: local description and travel\n** 3080–3109 Maps, Atlas of China\n** 3110–3299 Gazetteers of China\n** 3300–3479 [[Japanese history]]\n** 3400–3479 [[Geography of Japan|Japanese geography]]\n** 3480–3489 [[Korean history]]\n** 3490–3499 [[Hong Kong]], [[Macau]] history and geography\n** 3500–3599 Other counties in Asia: history and geography\n** 3600–3799 Europe: history and geography\n** 3800–3899 America: history and geography\n** 3900–3999 Africa, Oceania: history and geography\n\n===4000 to 4999 Social Sciences===\n** 4000–4019 Social sciences in general\n** 4020–4099 Statistics\n** 4100–4299 Sociology\n** 4300–4599 Economics\n** 4600–4899 Politics and Law\n** 4900–4999 Education\n\n===5000 to 5999 Language and Literature===\n** 5000–5039 Linguistics in general\n** 5040–5059 Literature in general\n** 5060–5069 Chinese language in general\n** 5070–5089 Semantic studies\n** 5090–5119 Graphic studies\n** 5120–5139 Phonological Studies\n** 5140–5149 Grammar\n** 5150–5159 Dialects\n** 5160–5169 Texts: learning the language\n** 5170–5199 Lexicography dictionaries\n** 5200–5209 Chinese literature in general\n** 5210–5217 Chinese literature: literary criticism\n** 5218–5229 Chinese literature: history & biography\n** 5230–5235 Chinese literature: collection of individual complete works\n** 5236–5241 Chinese literature: general anothlogies\n** 5242–5569 Collected Chinese literart works of individual authors\n** 5570–5649 Tz’u\n** 5650–5730 Lyrical works and drama\n** 5731–5769 Chinese Fiction\n** 5770–5779 Letters\n** 5780–5799 Miscellany: proverbs, fables, juv. lit.\n** 5800–5809 Minor languages in China\n** 5810–5859 Japanese language\n** 5860–5959 Japanese literature\n** 5973 Korean language and literature\n** 5975–5993 Indo-European language and literature\n** 5994–5999 Other language and literature\n\n===6000 to 6999 Fine and Recreative Arts===\n** 6000–6019 Fine and recreative arts in general\n** 6020–6029 Aesthetics\n** 6030–6069 History of arts\n** 6070–6289 Chinese & Japanese Calligraphy and painting\n** 6290–6299 Materials & instruments\n** 6300–6349 Western painting\n** 6350–6359 Engraving Prints\n** 6360–6399 Photography\n** 6400–6499 Sculpture\n** 6500–6599 Architecture\n** 6600–6699 Industrial arts\n** 6700–6799 Music\n** 6800–6899 Amusements & games\n** 6900–6999 Physical training & sports\n\n===7000 to 7999 Natural Sciences===\n** 7000–7019 Natural science in general\n** 7020–7099 [[Mathematics]]\n** 7100–7199 [[Astronomy]]\n** 7200–7299 [[Physics]]\n** 7300–7399 [[Chemistry]]\n** 7400–7499 [[Geology|Geological science]]\n** 7500–7599 [[Natural history]]\n** 7600–7699 [[Botany]]\n** 7700–7799 [[Zoology]]\n** 7800–7869 Anthropology (Physical)\n** 7870–7899 Psychology\n** 7900–7999 Medical science\n\n===8000 to 8999 Agriculture and Technology===\n** 8000–8009 Agriculture & technology in general\n** 8020–8239 Agriculture\n** 8240–8289 Home economics (Domestic)\n** 8290–8299 Technology in general\n** 8300–8349 Handicrafts & artisan trades\n** 8400–8499 Manufactures\n** 8500–8599 Chemical technology\n** 8600–8699 Mining & Metallurgy\n** 8700–8899 Engineering\n** 8900–8999 Military & Naval science\n\n===9000 to 9999 Generalia and Bibliography===\n** 9000–9007 Generalia and bibliography in general\n** 9100 Chinese general series of composite nature\n** 9101–9109 Chinese general series of a special type\n** 9110 Chinese series of particular locality\n** 9111–9120 Chinese family & individual author\n** 9130–9163 Sinology\n** 9164–9179 Japanese general series\n** 9180–9199 Japanese individual polygraphic books\n** 9200–9229 General periodicals & society publications\n** 9230–9289 General congresses & museums\n** 9290–9339 General encyclopedias and reference books\n** 9401–9409 Bibliography in general\n** 9410–9510 Bibliography\n** 9511–9519 Subject bibliographies\n** 9520–9539 Chinese collective bibliographies\n** 9540–9549 Other general bibliographies of various countries\n** 9550–9559 Reading lists & best books, periodical index\n** 9562–9569 Special bibliographies\n** 9570–9579 Bibliographies of critical reviews\n** 9600–9629 Library catalogues\n** 9630–9639 Dealers’ & publishers’ catalogues\n** 9640–9684 Japanese bibliographies\n** 9696–9699 Bibliographies of Western countries\n** 9700–9929 Librarianship\n** 9930–9999 Journalism, newspapers\n\n== See also ==\nThe official library classification in China is:\n\n* [[Chinese Library Classification]] (CLC)\n\nThe other library classifications for Chinese materials outside China are:\n* [http://www.lib.cam.ac.uk/mulu/class.html Cambridge University Library Chinese Classification System], Classification Scheme for Chinese Books drawn up by Profs. Haloun and P. van der Loon for Cambridge University, UK.\n* \'\'University of Leeds Classification of Books in Chinese, UK\'\' ([http://library.leeds.ac.uk/downloads/file/126/chinese 36 pages of Catalog in pdf])\n\n== Notes==\n\n<references />\n\n==References==\n* [http://www.lib.unimelb.edu.au/collections/asian/Harvard-Yenching.html Harvard-Yenching Classification in the University of Melbourne] - Subject headings in both Chinese and English.\n* [http://research.dils.tku.edu.tw/joemls/41/41-2/139-162.pdf PDF] - Brief history of the Harvard–Yenching Classification System, and an overview of the collections of Chinese language materials in Columbia University, Cornell University, Harvard University, the Library of Congress, Princeton University and Yale University.\n* [http://www.news.harvard.edu/gazette/2003/10.30/19-yenching.html Yenching, The singular history of a singular library, Ken Gewertz, Harvard News Office] - A brief history of the Harvard-Yenching Library and the Harvard–Yenching Classification System.\n\n== External links ==\n* [https://www.lib.uchicago.edu/e/easia/shelf.html Examples of the Harvard-Yenching classification system]\n* [http://www.library.ucla.edu/libraries/eastasian/collect.htm East Asian Library of the University of California in Los Angeles]\n\n{{Library classification systems}}\n\n{{DEFAULTSORT:Harvard-Yenching Classification}}\n[[Category:Library cataloging and classification]]\n[[Category:Classification systems]]\n[[Category:Knowledge representation]]\n[[Category:Chinese culture]]\n[[Category:Harvard University]]\n[[Category:Yenching University]]']
['Designated Community', '39348172', "In information and archival communities, a '''Designated Community''' is an identified group of potential consumers who should be able to understand a particular set of information. These consumers may consist of multiple communities, are designated by the archive, and may change over time.<ref>{{cite web|title=Reference Model for an Open Archival Information System (ISO 14721:2012)|url=http://public.ccsds.org/publications/archive/650x0m2.pdf}}</ref>\n\n\n==References==\n{{reflist}}\n{{library-stub}}\n\n[[Category:Archival science]]\n[[Category:Knowledge representation]]\n[[Category:Digital libraries]]"]
['East Pole–West Pole divide', '21766677', '{{redirect|West Pole|the album by The Gathering|The West Pole|the location in Texas|The West Pole, Texas}}\n\nThe \'\'\'East Pole–West Pole divide\'\'\' in the fields of [[cognitive psychology]] and [[cognitive neuroscience]] is an intellectual schism between researchers subscribing to the [[psychological nativism|nativist]] and [[empiricism|empiricist]] schools of thought.  The term arose from the fact that much of the theory and research supporting [[psychological nativism|nativism]], [[modularity of mind]], and [[computational theory of mind]] originated at several universities located on the East Coast, including [[Harvard University]], the [[University of Michigan]], [[Massachusetts Institute of Technology]], and [[Tufts University]]. Conversely, much of the research and theory supporting [[empiricism]], [[emergentism]], and [[embodied cognition]] originated at several universities located on the West Coast, including the [[University of California, Berkeley]], the [[Salk Institute]], and, most notably, the [[University of California, San Diego]].  In reality, the divide is not so clear, with many universities and scholars on both coasts (as well as the Midwest and around the world) supporting each position, as well as more moderate positions in between the two extremes.  The phrase was coined by [[Jerry Fodor]] at an [[MIT]] conference on [[cognition]], at which he referred to another researcher as a "West Coast theorist," apparently unaware that the researcher worked at [[Yale University]].<ref>{{cite book |title=The Blank Slate:The Modern Denial of Human Nature |last=Pinker |first=Steven |authorlink=Steven Pinker |year=2003 |publisher=Penguin |location=New York |isbn=978-0-14-200334-3 }}</ref>\n\nVery few researchers adhere strictly to the extreme positions highlighted by the East Pole–West Pole debate.  That is, there are very few empiricists who believe in the [[John Locke|Lockean]] ideal of the \'\'[[tabula rasa]]\'\' (namely, that children are born with no innate knowledge or constraints), and there are very few nativists who agree with [[Jerry Fodor|Fodor\'s]] assertion that all concepts that are learned over the course of life are present in the mind prior to birth.  Nevertheless, most scholars within the fields of [[cognitive science]] and [[developmental psychology]] affiliate themselves with one of the two positions through the means of their research.\n\nThe two books best known for espousing the empiricist and nativist positions within the context of cognitive psychology are \'\'[[Rethinking Innateness]]\'\' by [[Jeffrey Elman]] et al. and \'\'[[The Modularity of Mind]]\'\' by [[Jerry Fodor]], respectively.  Incidentally, the authors are affiliated with the two institutions on which the East Pole–West Pole metaphor is based, [[UCSD]] and [[MIT]], affirming the relevance and pervasiveness of this moniker for the intellectual divide.\n\n==Notable scholars with affiliations==\n\n{{Col-begin}}\n{{Col-break}}\n\'\'\'Nativists\'\'\'\n*[[Jerry Fodor]], [[Massachusetts Institute of Technology]]\n*[[Steven Pinker]], [[Harvard University]]\n*[[Lila R. Gleitman]], [[University of Pennsylvania]]\n*[[Leda Cosmides]], [[University of California, Santa Barbara]]\n*[[Elizabeth Spelke]], [[Harvard University]]\n*[[Thomas Bever]], [[University of Arizona]]\n*[[Daniel Dennett]], [[Tufts University]]\n*[[Nancy Kanwisher]], [[Massachusetts Institute of Technology]]\n\n{{Col-break}}\n\'\'\'Empiricists\'\'\'\n*[[Elizabeth Bates]], [[University of California, San Diego]]\n*[[George Lakoff]], [[University of California, Berkeley]]\n*[[Brian MacWhinney]], [[Carnegie Mellon University]]\n*[[Jeffrey Elman]], [[University of California, San Diego]]\n*[[Ronald Langacker]], [[University of California, San Diego]]\n*[[Dan Slobin]], [[University of California, Berkeley]]\n*[[David Rumelhart]], [[Stanford University]]\n*[[James McClelland (psychologist)|James McClelland]], [[Stanford University]]\n\n{{col-end}}\n\n==See also==\n*[[Nature and nurture]]\n*[[Empiricism]]\n*[[Psychological nativism]]\n*[[Computational theory of mind]]\n*[[Embodied cognition]]\n*[[Reductionism]]\n*[[Emergentism]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://query.nytimes.com/gst/fullpage.html?res=9B05E4DA1230F937A35752C1A961958260&sec=&spon=&pagewanted=all Recipe for a Brain: Cups of genes or a dash of experience? NY Times article]\n*[http://www.edge.org/3rd_culture/lakoff/lakoff_p4.html George Lakoff\'s discussion of the philosophical roots of embodied cognition]\n\n{{DEFAULTSORT:East Pole-West Pole divide}}\n[[Category:Cognition]]\n[[Category:Cognitive science]]\n[[Category:Knowledge representation]]\n[[Category:Arguments in philosophy of mind]]']
['Enactivism', '7082881', '\'\'\'Enactivism\'\'\' argues that [[cognition]] arises through a dynamic interaction between an acting [[organism]] and its environment.<ref name="Evan Thompson"/> It claims that our environment is one which we selectively create through our capacities to interact with the world.<ref name=Rowlands/> "Organisms do not passively receive information from their environments, which they then translate into internal representations. Natural cognitive systems...participate in the generation of meaning ...engaging in transformational and not merely informational interactions: \'\'they enact a world\'\'."<ref name=Jaegher1/> These authors suggest that the increasing emphasis upon enactive terminology presages a new era in thinking about cognitive science.<ref name=Jaegher1/> How the actions involved in enactivism relate to age-old questions about [[free will]] remains a topic of active debate.<ref name=Manetti/>\n\nThe term \'enactivism\' is close in meaning to \'enaction\', defined as "the manner in which a subject of perception creatively matches its actions to the requirements of its situation".<ref name=Tascano0/> The introduction of the term \'\'enaction\'\' in this context is attributed to  [[Francisco Varela]], [[Evan Thompson]], and [[Eleanor Rosch]],<ref name=Tascano0/><ref name=RWilson/> who proposed the name to "emphasize the growing conviction that cognition is not the representation of a pre-given world by a pre-given mind but is rather the enactment of a world and a mind on the basis of a history of the variety of actions that a being in the world performs".<ref name=Varela/> This was further developed by Thompson and others,<ref name="Evan Thompson"/> to place emphasis upon the idea that experience of the world is a result of mutual interaction between the sensorimotor capacities of the organism and its environment.<ref name=RWilson/>\n\nThe initial emphasis of enactivism upon sensorimotor skills has been criticized as "cognitively marginal",<ref name=ClarkA/> but it has been extended to apply to higher level cognitive activities, such as social interactions.<ref name="Jaegher1"/> "In the enactive view,... knowledge is constructed: it is constructed by an agent through its sensorimotor interactions with its environment, co-constructed between and within living species through their meaningful interaction with each other. In its most abstract form, knowledge is co-constructed between human individuals in socio-linguistic interactions...Science is a particular form of social knowledge construction...[that] allows us to perceive and predict events beyond our immediate cognitive grasp...and also to construct further, even more powerful scientific knowledge."<ref name=Rohde/>\n\nEnactivism is closely related to [[situated cognition]] and [[embodied cognition]], and is presented as an alternative to [[cognitivism (psychology)|cognitivism]], [[computationalism]], and [[Cartesian dualism]].\n\n==Philosophical aspects<!--\'Enaction (philosophy)\' redirects here-->==\n\nEnactivism is one of  a cluster of related theories sometimes known as the \'\'4Es\'\', As described by [[Mark Rowlands]], mental processes are:<ref name=Rowlands/>\n* \'\'\'Embodied\'\'\' involving more than the brain, including a more general involvement of bodily structures and processes.\n* \'\'\'Embedded\'\'\' functioning only in a related external environment.\n* \'\'\'Enacted\'\'\' involving not only neural processes, but also things an organism \'\'does\'\'.\n* \'\'\'Extended\'\'\' into the organism\'s environment.\n\nEnactivism proposes an alternative to [[Dualism (philosophy of mind)|dualism]] as a philosophy of mind, in that it emphasises the interactions between mind, body and the environment, seeing them all as inseparably intertwined in mental processes.<ref name=EThompson/> The self arises as part of the process of an embodied entity interacting with the environment in precise ways determined by its physiology.   In this sense, individuals can be seen to "grow into" or arise from their interactive role with the world.<ref name=Burman/>\n:"Enaction is the idea that organisms create their own experience through their actions. Organisms are not passive receivers of input from the environment, but are actors in the environment such that what they experience is shaped by how they act."<ref name=Hutchins/>\n\nIn \'\'The Tree of Knowledge\'\' Maturana & Varela proposed the term \'\'enactive\'\'<ref name=Maturana/> "to evoke the view of knowledge that what is known is brought forth, in contraposition to the more classical views of either cognitivism<ref group=Note name=Cognitivism/> or connectionism.<ref group=Note name=Connectionism/> They see enactivism as providing a middle ground between the two extremes of [[representationalism]] and [[solipsism]]. They seek to "confront the problem of understanding how our existence-the [[Praxis (process)|praxis]] of our living- is coupled to a surrounding world which appears filled with regularities that are at every instant the result of our biological and social histories.... to find a \'\'via media\'\': to understand the regularity of the world we are experiencing at every moment, but without any point of reference independent of ourselves that would give certainty to our descriptions and cognitive assertions.  Indeed the whole mechanism of generating ourselves, as describers and observers tells us that our world, as the world which we bring forth in our coexistence with others, will always have precisely that mixture of regularity and mutability, that combination of solidity and shifting sand, so typical of human experience when we look at it up close."[\'\'Tree of Knowledge\'\', p.&nbsp;241]\n\nEnactivism also addresses the [[hard problem of consciousness]], referred to by Thompson as part of the \'\'[[explanatory gap]]\'\' in explaining how consciousness and subjective experience are related to brain and body.<ref name=EThompson2/>  "The problem with the dualistic concepts of consciousness and life in standard formulations of the hard problem is that they exclude each other by construction".<ref name=EThompson3/> Instead, according to Thompson\'s view of enactivism, the study of consciousness or [[Phenomenology (philosophy)|phenomenology]] as exemplified by [[Husserl]] and [[Merleau-Ponty]] is to complement science and its objectification of the world. "The whole universe of science is built upon the world as directly experienced, and if we want to subject science itself to rigorous scrutiny and arrive at a precise assessment of its meaning and scope, we must begin by reawakening the basic experience of the world of which science is the second-order expression" (Merleau-Ponty, \'\'The phenomenology of perception\'\' as quoted by Thompson, p.&nbsp;165). In this interpretation, enactivism asserts that science is formed or enacted as part of humankind\'s interactivity with its world, and by embracing phenomenology "science itself is properly situated in relation to the rest of human life and is thereby secured on a sounder footing."<ref name=EThompson4/><ref name=Baldwin/>\n\nEnaction has been seen as a move to conjoin [[representationalism]] with [[phenomenalism]], that is, as adopting a [[constructivist epistemology]], an epistemology centered upon the active participation of the subject in constructing reality.<ref name=Mutelesi/><ref name=Chiari/> However, \'constructivism\' focuses upon more than a simple \'interactivity\' that could be described as a minor adjustment to \'assimilate\' reality or \'accommodate\' to it.<ref name=Glaserfeld/> Constructivism looks upon interactivity as a radical, creative, revisionist process in which the knower \'\'constructs\'\' a personal \'knowledge system\' based upon their experience and tested by its viability in practical encounters with their environment. Learning is a result of perceived anomalies that produce dissatisfaction with existing conceptions.<ref name=Glasersfeld2/>\n\nHow does constructivism relate to enactivism? From the above remarks it can be seen that [[Ernst von Glasersfeld|Glasersfeld]] expresses an interactivity between the knower and the known quite acceptable to an enactivist, but does not emphasize  the structured probing of the environment by the knower that leads to the "perturbation relative to some expected result" that then leads to a new understanding.<ref name=Glasersfeld2/> It is this probing activity, especially where it is not accidental but deliberate, that characterizes enaction, and invokes \'\'affect\'\',<ref name=Ward2/> that is, the motivation and planning that lead to doing and to fashioning the probing, both observing and modifying the environment, so that "perceptions and nature condition one another through generating one another."<ref name=Diettrich/> The questioning nature of this probing activity is not an emphasis of [[Jean Piaget|Piaget]] and Glasersfeld.\n\nSharing enactivism\'s stress upon both action and embodiment in the incorporation of knowledge, but giving Glasersfeld\'s mechanism of viability an [[Introduction to evolution|evolutionary]] emphasis,<ref name=Diettrich2/> is [[evolutionary epistemology]]. Inasmuch as an organism must reflect its environment well enough for the organism to be able to survive in it, and to be competitive enough to be able to reproduce at sustainable rate, the structure and reflexes of the organism itself embody knowledge of its environment. This biology-inspired theory of the growth of knowledge is closely tied to [[universal Darwinism]], and is associated with evolutionary epistemologists such as [[Karl Popper]], [[Donald T. Campbell]], [[Peter Munz]], and [[Gary Cziko]].<ref name=Gontier/> According to Munz, "an organism is an \'\'embodied theory\'\' about its environment... Embodied theories are also no longer expressed in language, but in anatomical structures or reflex responses, etc."<ref name=Gontier/><ref name=Munz/>\n\n==Psychological aspects==\nMcGann & others<ref name=McGann>{{cite journal |author1=Marek McGann |author2=Hanne De Jaegher |author3=Ezequiel Di Paolo |year= 2013 |title=Enaction and psychology |journal=Review of General Psychology |volume=17 |issue=2 |pages=203–209 |url= http://www.academia.edu/4993021/Enaction_and_Psychology |doi= 10.1037/a0032935}}\n</ref> argue that enactivism attempts to mediate between the explanatory role of the coupling between cognitive agent and environment and the traditional emphasis on brain mechanisms found in neuroscience and psychology.  In the interactive approach to social cognition developed by De Jaegher &  others,<ref name=Gallagher0>{{cite journal |author=Shaun Gallagher |year=2001 |title=The practice of mind |journal=Journal of Consciousness Studies |volume=8 |issue=5–7 |pages=83–107 |url=http://www.ummoss.org/Gallagher01.pdf}}\n</ref><ref name=Gallager1>\n{{cite book |author=Shaun Gallagher |isbn=978-0199204168 |edition=Paperback |year=2006 |title=How the Body Shapes the Mind |publisher=Oxford University Press |url=https://books.google.com/books/about/How_the_Body_Shapes_the_Mind.html?id=zhv5F-GYm98C}}\n</ref><ref name=Ratcliffe>\n{{cite book |author=Matthew Ratcliffe |year=2008 |title=Rethinking Commonsense Psychology: A Critique of Folk Psychology, Theory of Mind and Simulation |publisher=Palgrave Macmillan |isbn=978-0230221208 |url=https://books.google.com/books/about/Rethinking_Commonsense_Psychology.html?id=-JNyQgAACAAJ}}\n</ref> the dynamics of interactive processes are seen to play significant roles in coordinating interpersonal understanding, processes that in part include what they call [[#Participatory sense-making|\'\'participatory sense-making\'\']].<ref name=DeJaeger0>\n{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |url=http://www.enactionschool.com/resources/papers/DeJaegherDiPaolo2007.pdf |year=2007 |title=Participatory Sense-Making: An enactive approach to social cognition |journal=Phenomenology and the Cognitive Sciences |volume=6 |issue=4 |pages=485–507 |doi=10.1007/s11097-007-9076-9}}\n</ref><ref name=DeJaegher1>\n{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |author3=Shaun Gallagher |year=2010 |title=Can social interaction constitute social cognition? |journal=Trends in Cognitive Sciences |volume=14 |issue=10 |pages=441–447 |url=http://ezequieldipaolo.files.wordpress.com/2011/10/dejaegher_dipaolo_gallagher_tics_2010.pdf |doi=10.1016/j.tics.2010.06.009 |pmid=20674467}}\n</ref> Recent developments of enactivism in the area of social neuroscience involve the proposal of \'\'The Interactive Brain Hypothesis\'\'<ref name=DiPaolo3>\n{{cite journal |url=http://journal.frontiersin.org/Journal/10.3389/fnhum.2012.00163/full |author1=Ezequiel Di Paolo |author2=Hanne De Jaegher |date=June 2012 |title= The Interactive Brain Hypothesis |journal=Frontiers in Human Neuroscience |volume=7 |issue=6 |doi=10.3389/fnhum.2012.00163}}</ref> where social cognition brain mechanisms, even those used in non-interactive situations, are proposed to have interactive origins.\n\n===Enactive views of perception===\nIn the enactive view, perception "is not conceived as the transmission of information but more as an exploration of the world by various means. Cognition is not tied into the workings of an \'inner mind\', some cognitive core, but occurs in directed interaction between the body and the world it inhabits."<ref name=McGann2>\n{{cite book |title=Consciousness & Emotion: Agency, conscious choice, and selective perception |page=184 |author1=Marek McGann |author2=Steve Torrance |chapter=Doing It and Meaning It: And the relation between the two  |isbn=9789027294616 |publisher=John Benjamins Publishing |year=2005 |url=https://books.google.com/books?id=LZk6AAAAQBAJ&pg=PA184 |editor1=Ralph D. Ellis |editor2=Natika Newton }}\n</ref>\n\n[[Alva Noë]] in advocating an enactive view of perception<ref name=Noe>\n{{cite book |author=Alva Noë |title= Action in Perception |url=https://books.google.com/books?id=kFKvU2hPhxEC&pg=PA1 |pages=1 \'\'ff\'\' |chapter=Chapter 1: The enactive approach to perception: An introduction |isbn=9780262140881 |year=2004 |publisher=MIT Press}}\n</ref> sought to resolve how we perceive three-dimensional objects, on the basis of two-dimensional input.  He argues that we perceive this solidity (or \'volumetricity\') by appealing to patterns of sensorimotor expectations. These arise from our agent-active  \'movements and interaction\' with objects, or \'object-active\' changes in the object itself. The solidity is perceived through our expectations and skills in knowing how the object\'s appearance would change with changes in how we relate to it. He saw all perception as an active exploration of the world, rather than being a passive process, something which happens to us.\n\nNoë\'s idea of the role of \'expectations\' in three-dimensional perception has been opposed by several philosophers, notably by [[Andy Clark]].<ref name=ClarkA1/> Clark points to difficulties of the enactive approach.  He points to internal processing of visual signals, for example, in the ventral and dorsal pathways, [[Two-streams hypothesis|the two-streams hypothesis]]. This results in an integrated perception of objects (their recognition and location, respectively) yet this processing cannot be described as an action or actions. In a more general criticism, Clark suggests that perception is not a matter of expectations about sensorimotor mechanisms guiding perception. Rather, although the limitations of sensorimotor mechanisms constrain perception, this sensorimotor activity is drastically filtered to fit current needs and purposes of the organism, and it is these imposed \'expectations\' that govern perception, filtering for the \'relevant\' details of sensorimotor input (called "sensorimotor summarizing").<ref name=ClarkA1/>\n\nAnother application of enaction to perception is analysis of the human hand. The many remarkably demanding uses of the hand are not learned by instruction, but through a history of engagements that lead to the acquisition of skills. According to one interpretation, it is suggested that "the hand [is]...an organ of cognition", not a faithful subordinate working under top-down instruction, but a partner in a "bi-directional interplay between manual and brain activity."<ref name=Hutto>\n{{cite book |title= Radicalizing Enactivism: Minds without content |author=[[Daniel D Hutto]], Erik Myin |pages=46 \'\'ff\'\' |chapter=A helping hand |url=https://books.google.com/books?id=pAj-96LlBuMC&pg=PA46 |isbn= 9780262018548 |year=2013 |publisher=MIT Press}}\n</ref> According to [[Daniel Hutto]]: "Enactivists are concerned to defend the view that our most elementary ways of engaging with the world and others - including our basic forms of perception and perceptual experience - are mindful in the sense of being phenomenally charged and intentionally directed, despite being non-representational and content-free."<ref name=Hutto2>\n{{cite book |title= Radicalizing Enactivism: Minds without content |author1=Daniel D Hutto |author2=Erik Myin |pages=12–13  |chapter=Chapter 1: Enactivism: The radical line |url=https://books.google.com/books?id=pAj-96LlBuMC&pg=PA12 |isbn= 9780262018548 |year=2013 |publisher=MIT Press}}\n</ref> Hutto calls this position \'REC\' (<u>R</u>adical <u>E</u>nactive <u>C</u>ognition): "According to REC, there is no way to distinguish neural activity that is imagined to be genuinely content involving (and thus truly mental, truly cognitive) from other non-neural activity that merely plays a supporting or enabling role in making mind and cognition possible."<ref name=Hutto2/>\n\n===Participatory sense-making===\n\n[[Hanne De Jaegher]] and [[Ezequiel Di Paolo]] (2007)<ref name="DeJaeger0"/> have extended the enactive concept of sense-making<ref name="EThompson3"/> into the social domain. The idea takes as its departure point the process of interaction between individuals in a social encounter.<ref name=DeJaegher_etal>{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |author3=Shaun Gallagher |title= Can social interaction constitute social cognition? |journal=Trends in Cognitive Sciences |year=2010  |volume=14 |issue=10 |pages=441–447 |doi=10.1016/j.tics.2010.06.009 |pmid=20674467}}\n</ref> De Jaegher and Di Paolo argue that the interaction process itself can take on a form of autonomy (operationally defined). This allows them to define social cognition as the generation of meaning and its transformation through interacting individuals.\n\nThe notion of participatory sense-making has led to the proposal that interaction processes can sometimes play constitutive roles in social cognition (De Jaegher, Di Paolo, Gallagher, 2010).<ref name="DeJaegher1"/> It has been applied to research in [[social neuroscience]]\'\'<ref name="DiPaolo3"/><ref name=SchilbachTimmermans>\n{{cite journal |author1=Leonhard Schilbach |author2=Bert Timmermans |author3=Vasudevi Reddy |author4=Alan Costall |author5=Gary Bente |author6=Tobias Schlicht |author7=Kai Vogeley |title= Toward a second-person neuroscience |journal=Behavioral and Brain Sciences |year=2013  |volume=36 |issue=4 |pages=393–414 |doi=10.1017/S0140525X12000660}}</ref>\'\' and [[autism]].\'\'<ref name="DeJaegher_autism">{{cite journal |author= Hanne De Jaegher |title= Embodiment and sense-making in autism|journal=Frontiers in Integrative Neuroscience |year=2012  |volume=7 |pages=15 |doi=10.3389/fnint.2013.00015}}\n</ref>\'\'\n\nIn a similar vein, "an inter-enactive approach to agency holds that the behavior of agents in a social situation unfolds not only according to their individual abilities and goals, but also according to the conditions and constraints imposed by the autonomous dynamics of the interaction process itself".<ref name=STorrance>\n{{cite journal |title=An Inter-Enactive Approach to Agency: Participatory Sense-Making, Dynamics, and Sociality |author1=Steve Torrance |author2=Tom Froese |url=http://sacral.c.u-tokyo.ac.jp/pdf/froese_humana_2011.pdf |journal=Human Mente |volume=15 |pages=21–53 |year=2011 }} \n</ref> According to Torrance, enactivism involves five interlocking themes related to the question "What is it to be a (cognizing, conscious) agent?" It is:<ref name=STorrance/>\n:1. to be a biologically autonomous ([[Autopoiesis|autopoietic]]) organism\n:2. to generate \'\'significance\'\' or \'\'meaning\'\', rather than to act via...updated internal representations of the external world\n:3. to engage in sense-making via dynamic coupling with the environment\n:4. to \'enact\' or \'bring forth\' a world of significances by mutual co-determination of the organism with its enacted world\n:5. to arrive at an experiential awareness via lived embodiment in the world.\n\nTorrance adds that "many kinds of agency, in particular the agency of human beings, cannot be understood separately from understanding the nature of the interaction that occurs between agents." That view introduces the social applications of enactivism. "Social cognition is regarded as the result of a special form of action, namely \'\'social interaction\'\'...the enactive approach looks at the circular dynamic within a dyad of embodied agents."<ref name=FuchsT>\n{{cite book |url=https://books.google.com/books?id=Olm10GVwV74C&pg=PA206 |page=206 |chapter=Non-representational intersubjectivity |author1=Thomas Fuchs |author2=Hanne De Jaegher |isbn=9783794527915 |year=2010 |publisher=Schattauer Verlag |title=The Embodied Self: Dimensions, Coherence and Disorders |editor1=Thomas Fuchs |editor2=Heribert C. Sattel |editor3=Peter Henningsen }}\n</ref>\n \nIn [[cultural psychology]], enactivism is seen as a way to uncover cultural influences upon feeling, thinking and acting.<ref name=Verheggen>{{cite book |chapter=Chapter 8: Enactivism |author1=Cor Baerveldt |author2=Theo Verheggen |title=The Oxford Handbook of Culture and Psychology  |url=https://books.google.com/books?id=WljI1r2e-SUC&pg=PA165 |pages=165\'\'ff\'\' |doi=10.1093/oxfordhb/9780195396430.013.0009 |isbn=9780195396430 |date=May 2012 |quote= Whereas the enactive approach in general has focused on sense-making as an embodied and situated activity, enactive cultural psychology emphasizes the expressive and dynamically enacted nature of cultural meaning.}}</ref>  Baerveldt and Verheggen argue that "It appears that seemingly natural experience is thoroughly intertwined with sociocultural realities." They suggest that the social patterning of experience is to be understood through enactivism, "the idea that the reality we have in common, and in which we find ourselves, is neither a world that exists independently from us, nor a socially shared way of representing such a pregiven world, but a world itself brought forth by our ways of communicating and our joint action....The world we inhabit is manufactured of \'meaning\' rather than \'information\'.<ref name=Baerveldt>\n{{cite journal |title=Enactivism and the experiential reality of culture: Rethinking the epistemological basis of cultural psychology |author1=Cor Baerveldt |author2=Theo Verheggen |url=https://docs.google.com/file/d/0Bz8cVS8LoO7OTk9ZUkVqazFiU1U/edit |journal=Culture & Psychology |volume=5 |issue=2 |pages=183–206 |year=1999 |doi=10.1177/1354067x9952006}}\n</ref>\n\n[[Niklas Luhmann|Luhmann]] attempted to apply Maturana and Varela\'s notion of autopoiesis to social systems.<ref name=Luhmann>\n{{cite book |title=Social systems |url=https://books.google.com/books?id=zVZQW4gxXk4C&pg=PA34&lpg=PA34 |isbn= 9780804726252 |year=1995 |publisher=Stanford University Press |author=Niklas Luhmann}}\n</ref> "A core concept of social systems theory is derived from biological systems theory: the concept of \'\'autopoiesis\'\'. Chilean biologist Humberto Maturana come up with the concept to explain how biological systems such as cells are a product of their own production." "Systems exist by way of operational closure and this means that they each construct themselves and their own realities."<ref name=Moeller>\n{{cite book  |chapter=Part 1: A new way of thinking about society |pages= 12 \'\'ff\'\' |author=Hans-Georg Moeller |year=2011 |isbn= 978-0812695984 |publisher=Open Court |title=Luhmann Explained: From Souls to Systems |url=https://books.google.com/books?id=tuKsEvpcj9MC&pg=PA12}}\n</ref>\n\n==Educational aspects==\nThe first definition of enaction was introduced by psychologist [[Jerome Bruner]],<ref name=Pugliese>\n{{cite book |title=Intelligent Virtual Agents: |chapter=A framework for motion based bodily enaction with virtual characters; §2.1 Enaction |author1=Roberto Pugliese |author2=Klaus Lehtonen |url=https://books.google.com/books?id=QU9b_IjVMF4C&pg=PA163 |page=163 |isbn=9783642239731 |publisher=Springer |year=2011}}\n</ref><ref name=Beck>\n{{cite book |url=https://books.google.com/books?id=8V9BAAAAQBAJ&pg=PA104 |page=104 |title=From Diagnostics to Learning Success: Proceedings in Vocational Education and Training |isbn=978-9462091894 |edition=Paperback |year=2013 |publisher=Springer Science & Business |author=Stephanie A Hillen |chapter=Chapter III: What can research on technology for learning in vocational educational training teach media didactics? |editor1=Klaus Beck |editor2=Olga Zlatkin-Troitschanskaia }}\n</ref> who introduced enaction as \'learning by doing\' in his discussion of how children learn, and how they can best be helped to learn.<ref name=Bruner>{{cite book |author=[[Jerome Bruner]]|year=1966 |title=Toward a theory of instruction |publisher=Belknap Press of Harvard University Press |isbn=978-0674897007}}</ref><ref name=Bruner2>{{cite book |author=Jerome Bruner |year=1968 |title=Processes of cognitive growth: Infancy |publisher= Crown Pub |isbn= 978-0517517482}}{{oclc|84376}}</ref> He associated enaction with two other ways of knowledge organization: [[Cultural icon|Iconic]] and [[Symbol]]ic.<ref name=Bruner3>Quote from\n{{cite book |title=Toward a Theory of Instruction |author=Jerome Seymour Bruner |url=http://h.uib.no/examplewiki/en/images/5/5a/Bruner_1966_Theory_of_Instruction.pdf |isbn=9780674897014 |publisher=Harvard University Press |year=1966 |page=44}} as quoted from {{cite book |title=Fundamental Constructs in Mathematics Education |author=J Bruner |editor1=John Mason |editor2=Sue Johnston-Wilder |url=https://books.google.com/books?id=EA3LtKYTa7YC&pg=PA260 |page=260 |chapter=Chapter 10: Sustaining mathematical activity |year=2004 |publisher=Taylor & Francis |isbn= 0415326982 |edition=Paperback}}</ref>\n\n:"Any domain of knowledge (or any problem within that domain of knowledge) can be represented in three ways: by a set of actions appropriate for achieving a certain result (enactive representation); by a set of summary images or graphics that stand for a concept without defining it fully  (iconic representation); and by a set of symbolic or logical propositions drawn from a symbolic system that is governed by rules or laws for forming and transforming propositions (symbolic representation)"\nThe term \'enactive framework\' was elaborated upon by [[Francisco Varela]] and [[Humberto Maturana]].<ref name=Bopry>\n{{cite book |title=The Praeger Handbook of Education and Psychology, Volume 1 |author=Jeanette Bopry |chapter=Providing a warrant for constructivist practice: the contribution of Francisco Varela |quote=Varela\'s enactive framework beginning with his collaboration on [[autopoiesis]] theory with his mentor Humberto Maturana [and the development of] enaction as a framework within which these theories work as a matter of course. |editor1=Joe L. Kincheloe |editor2=Raymond A. Horn |year=2007 |publisher=Greenwood Publishing Group |isbn=9780313331237 |url=https://books.google.com/books?id=O1ugEIEid6YC&pg=PA474 |pages=474 \'\'ff\'\'}}\n</ref>\n\nSriramen argues that enactivism provides "a rich and powerful explanatory theory for learning and being."<ref name= Sriraman>\n{{cite book |title=Theories of Mathematics Education: Seeking New Frontiers |author1=Bharath Sriraman |author2=Lyn English |isbn=3642007422 |year=2009 |publisher=Springer |url=https://books.google.com/books?id=Kd_LgW2AXIoC&pg=PA42 |pages=42 \'\'ff\'\' |chapter=Enactivism}}</ref> and that it is closely related to both the [[Piaget\'s theory of cognitive development|ideas of cognitive development]] of [[Jean Piaget|Piaget]], and also the [[social constructivism]] of [[Vygotsky]].<ref name=Sriraman/> Piaget focused on the child\'s immediate environment, and suggested cognitive structures like spatial perception emerge as a result of the child\'s interaction with the world.<ref name=Roth>\n{{cite book |title=Geometry as Objective Science in Elementary School Classrooms: Mathematics in the Flesh |author=Wolff-Michael Roth |isbn=1136732209 |year=2012 |publisher=Routledge |pages=41 \'\'ff\'\' |url=https://books.google.com/books?id=cXSsAgAAQBAJ&pg=PT41 |chapter=Epistemology and psychology: Jean Piaget and modern constructivism}}\n</ref> According to Piaget, children \'\'construct\'\' knowledge, using what they know in new ways and testing it, and the environment provides feedback concerning the adequacy of their construction.<ref name= Cziko>\n{{cite book |title=Without Miracles: Universal Selection Theory and the Second Darwinian Revolution |author=Gary Cziko |chapter=Chapter 12: Education; The provision and transmission of truth, or the selectionist growth of fallible knowledge? |page=222 |url=https://books.google.com/books?id=v1JEypylerUC&pg=PA222&lpg=PA222 |year=1997 |isbn=9780262531474 |publisher=MIT Press}}\n</ref> In a cultural context, Vygotsky suggested that the kind of cognition that can take place is not dictated by the engagement of the isolated child, but is also a function of social interaction and dialogue that is contingent upon a sociohistorical context.<ref name=Kincheloe>\n{{cite book |title=The Praeger Handbook of Education and Psychology, Volume 1 |chapter=Interpretivists drawing on the power of enactivism |url=https://books.google.com/books?id=O1ugEIEid6YC&pg=PA24 |pages=24 \'\'ff\'\' |publisher=Greenwood Publishing Group |year=2007 |editor1=Joe L. Kincheloe |editor2=Raymond A. Horn |author=Joe L Kincheloe |isbn=0313331235}}\n</ref>  Enactivism in educational theory "looks at each learning situation as a complex system consisting of teacher, learner, and context, all of which frame and co-create the learning situation."<ref name=Vithal>\n{{cite book |editor1=Renuka Vithal |editor2=Jill Adler |editor3=Christine Keitel |title=Researching Mathematics Education in South Africa: Perspectives, Practices and Possibilities |chapter=Chapter 9: Dilemmas of change: seeing the complex rather than the complicated?  |page=240 |author=Chris Breen |isbn=0796920478 |publisher=HSRC Press |year=2005 |url=https://books.google.com/books?id=byWHt_NVUEgC&pg=RA6-PA240}}\n</ref> Enactivism in education is very closely related to [[situated cognition]],<ref name=VanDeGevel>\n{{cite book |title=The nexus between artificial intelligence and economics |chapter=§3.2.2 Enactive artificial intelligence |quote=\'\'Enactivism\'\' may be considered as the most developed model of embodied situated cognition...Knowing is inseparable from doing. |url=https://books.google.com/books?id=uek_AAAAQBAJ&pg=PA21 |page=21 |author=Ad J. W. van de Gevel, Charles N. Noussair |isbn=3642336477 |publisher=Springer |year=2013}}\n</ref> which holds that "knowledge is situated, being in part a product of the activity, context, and culture in which it is developed and used."<ref name=Collins>\n{{cite journal |title=Situated cognition and the culture of learning |author1=John Seely Brown |author2=Allan Collins |author3=Paul Duguid |url=http://www.exploratorium.edu/ifi/resources/museumeducation/situated.html |journal=Educational Researcher |volume=18 |number=1 |pages=32–42 |date=Jan–Feb 1989 |doi=10.3102/0013189x018001032}}\n</ref> This approach challenges the "separating of what is learned from how it is learned and used."<ref name=Collins/>\n\n==Artificial intelligence aspects==\n{{importance section|date=May 2014}}\n{{main|Enactive interfaces}}\nThe ideas of enactivism regarding how organisms engage with their environment have interested those involved in [[Cognitive robotics|robotics]] and [[Human–computer interaction|man-machine interfaces]]. The analogy is drawn that a robot can be designed to interact and learn from its environment in a manner similar to the way an organism does that,<ref name=Sandini>\n{{cite book |chapter=The \'\'iCub\'\' cognitive humanoid robot: An open-system research platform for enactive cognition |author1=Giulio Sandini |author2=Giorgio Metta |author3=David Vernon |title=50 Years of Artificial Intelligence: Essays Dedicated to the 50th Anniversary of Artificial Intelligence |editor1=Max Lungarella |editor2=Fumiya Iida |editor3=Josh Bongard |editor4=Rolf Pfeifer |publisher=Springer |year=2007 |isbn= 9783540772958}}\n</ref> and a human can interact with a computer-aided design tool or data base using an interface that creates an enactive environment for the user, that is, all the user\'s tactile, auditory, and visual capabilities are enlisted in a mutually explorative engagement, capitalizing upon all the user\'s abilities, and not at all limited to cerebral engagement.<ref name=Bordegoni>\n{{cite book |title=Emotional Engineering: Service Development |chapter=§4.5.2 Design tools based upon enactive interfaces |url=https://books.google.com/books?id=ow-UFDj15rUC&pg=PA78 |pages=78 \'\'ff\'\' |isbn=9781849964234 |year=2010 |publisher=Springer |author=Monica Bordegoni |editor=Shuichi Fukuda}}\n</ref> In these areas it is common to refer to [[affordance]]s as a design concept, the idea that an environment or an interface affords opportunities for enaction, and good design involves optimizing the role of such affordances.<ref name=Norman>\n{{cite book |title=The Design of Everyday Things |edition=Revised and expanded |quote=An affordance is a relationship between the properties of an object and the capabilities of the agent that determine just how the object could possibly be used. |url=https://books.google.com/books?id=nVQPAAAAQBAJ&pg=PT17 |year=2013 |page=11 |isbn=978-0465050659 |publisher=Basic Books |author=Don Norman |chapter=Affordances }}\n</ref><ref name=Kim>\n{{cite book |title=Encyclopedia of human computer interaction |chapter=The use and evolution of affordance in HCI  |url=https://books.google.com/books?id=h9iZh_I1YREC&pg=PA668 |pages=668 \'\'ff\'\' |isbn=9781591407980 |year=2006 |publisher=Idea Group Inc |author=Georgios S Christou |editor=Claude Ghaoui}}\n</ref><ref name=Kaipainen>\n{{cite journal |title=Enactive Systems and Enactive Media: Embodied Human-Machine Coupling beyond Interfaces |url=http://www.mitpressjournals.org/doi/abs/10.1162/LEON_a_00244#.U3_JKygT0cs  |journal=Leonardo |volume=44 |pages=433–438 |date=October 2011 |issue=5 |doi=10.1162/LEON_a_00244 |author1=Mauri Kaipainen |author2=Niklas Ravaja |author3=Pia Tikka |display-authors=etal}} \n</ref><ref name=Boy>\n\n{{cite book |title=Orchestrating Human-Centered Design |author=Guy Boy |url=https://books.google.com/books?id=I5gCTZCIL3AC&pg=PA118&lpg=PA118 |isbn=9781447143383 |year=2012 |publisher=Springer |page=118 |quote=The organization producing the system can itself be defined as an autopoietic system in Maturana and Varela\'s sense. An autopoietic system is producer and product at the same time. HCD [Human Centered Design] is both the process of design and the design itself.}}\n\n</ref><ref name=Thannhuber>\n{{cite journal |title=An autopoietic approach for knowledge management systems in manufacturing enterprises |author1=Markus Thannhuber |author2=Mitchell M Tseng |author3=Hans-Jörg Bullinger |url=http://www.researchgate.net/publication/223035600_An_Autopoietic_Approach_for_Building_Knowledge_Management_Systems_in_Manufacturing_Enterprises/file/50463525a5a320287e.pdf%26sa%3DX%26scisig%3DAAGBfm3GtB0hiqz1jul4MXuCQxnRzPbcHQ%26oi%3Dscholarr&rct=j&q=&esrc=s&sa=X&ei=N-h_U6HtHIiEogSy_oHAAw&ved=0CCcQgAMoADAA&usg=AFQjCNEt_M1NOffumXQSxrJIVuZI48XRGQ&cad=rja |journal=Annals of the CIRP-Manufacturing Technology |volume=50 |issue=1 |year=2001 |pages=313 \'\'ff\'\' |doi=10.1016/s0007-8506(07)62129-5}}</ref>\n\nThe activity in the AI community also has influenced enactivism as whole. Referring extensively to modeling techniques for [[evolutionary robotics]] by Beer,<ref name=Beer>\n{{cite journal |author=Randall D Beer |year=1995 |title=A dynamical systems perspective on agent-environment interaction.\n |journal= Artificial Intelligence |volume=72 |pages=173–215 |url=http://dx.doi.org/10.1016/0004-3702%2894%2900005-L |doi=10.1016/0004-3702(94)00005-l}}\n</ref> the modeling of learning behavior by Kelso,<ref name=Kelso>\n{{cite book |author=James AS Kelso |year=2009 |chapter=Coordination dynamics |editor=R. A. Meyers |title= Encyclopedia of complexity and system science |pages= 1537–1564 |isbn=978-0-387-75888-6 |url=http://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30440-3_101}}\n</ref> and to modeling of sensorimotor activity by Saltzman,<ref name=Saltzman>\n{{cite book |author=Eliot L. Saltzman |year=1995 |chapter=Dynamics and coordinate systems in skilled sensorimotor activity |editor1=T. van Gelder |editor2=R. F. Port |title= Mind as motion: Explorations in the dynamics of cognition  |publisher= MIT Press |isbn=9780262161503 |url=https://books.google.com/books?id=e6HUM6V8QbQC&pg=PA151 |page=151 \'\'ff\'\'}}\n</ref> McGann, De Jaegher, and Di Paolo discuss how this work makes the dynamics of coupling between an agent and its environment, the foundation of enactivism, "an operational, empirically observable phenomenon."<ref name=McGann3>{{cite journal |author1=Marek McGann |author2=Hanne De Jaegher |author3=Ezequiel Di Paolo |year= 2013 |title=Enaction and psychology |journal=Review of General Psychology |volume=17 |issue=2 |pages=203–209 |url= http://www.academia.edu/4993021/Enaction_and_Psychology |doi= 10.1037/a0032935 |quote=Such modeling techniques allow us to explore the parameter space of coupling between agent and environment...to the point that their basic principles (the universals, if such there are, of enactive psychology) can be brought clearly into view.}}\n</ref> That is, the AI environment invents examples of enactivism using concrete examples that, although not as complex as living organisms, isolate and illuminate basic principles.\n\n==See also==\n{{colbegin}}\n*[[Action-specific perception]]\n*[[Autopoesis]]\n*[[Biosemiotics]]\n*[[Cognitive science]]\n*[[Cognitive psychology]]\n*[[Computational theory of mind]]\n*[[Connectivism]]\n*[[Cultural psychology]]\n*[[Distributed cognition]]\n*[[Embodied cognition]]\n*[[Embodied embedded cognition]]\n*[[Enactive interfaces]]\n*[[Extended cognition]]\n*[[Extended mind]]\n*[[Externalism#Enactivism and embodied cognition]]\n*[[Mind–body problem]]\n*[[Phenomenology (philosophy)]]\n*[[Representationalism]]\n*[[Situated cognition]]\n*[[Social cognition]]\n{{colend}}\n\n==References==\n{{reflist|30em|refs=\n\n<ref name=Baldwin>\n{{cite book |author=Thomas Baldwin |title=[[Maurice Merleau-Ponty]]: Basic Writings |chapter-url=https://books.google.com/books?id=OS8FM-AFvvsC&pg=PA65 |chapter=Part One: Merleau-Ponty\'s prospectus of his work |page=65  |quote=Science has not and never will have, by its nature, the same significance \'\'qua\'\' form of being as the world which we perceive, for the simple reason that it is a rationale or explanation of that world. |isbn= 978-0415315869 |year=2003 |publisher=Routledge}}\n</ref>\n\n<ref name=Burman>\n{{cite journal |author=Jeremy Trevelyan Burman  |year=2006 |journal=Journal of Consciousness Studies |title=Book reviews: \'\'Consciousness & Emotion\'\' |url=http://www.imprint.co.uk/pdf/13_12_br.pdf  |volume=13 |issue=12 |pages=115–124}}  From a review of {{cite book |title=Consciousness & Emotion: Agency, conscious choice, and selective perception |editor1=Ralph D. Ellis |editor2=Natika Newton |url=https://books.google.com/books?id=LZk6AAAAQBAJ&printsec=frontcover |isbn=9789027294616 |year=2005 |publisher=John Benjamins Publishing}}\n</ref>\n\n<ref name=Chiari>\n{{cite web |title=Constructivism |author1=Gabriele Chiari |author2=M. Laura Nuzzo |work=The Internet Encyclopaedia of Personal Construct Psychology |url=http://www.pcp-net.org/encyclopaedia/constructivism.html}}\n</ref>\n\n<ref name=ClarkA>\n{{cite journal |author1=Andy Clark |author2=Josefa Toribio |title=Doing without representing |journal =Synthese |volume=101 |pages=401–434 |year=1994 |url=http://www.philosophy.ed.ac.uk/people/clark/pubs/DoingW-O-rep.pdf |doi=10.1007/bf01063896}}\n</ref>\n\n<ref name=ClarkA1>\n{{cite journal  |author=Andy Clark   |title=Vision as Dance? Three Challenges for Sensorimotor Contingency Theory |journal= Psyche |volume=12 |issue=1 |date=March 2006 |url= https://www.era.lib.ed.ac.uk/bitstream/1842/1444/1/Psyche%20Clark.pdf}}\n</ref>\n\n<ref name=Diettrich>\n{{cite book |author=Olaf Diettrich |chapter=The biological boundary conditions for our classical physical world view |title=Evolutionary Epistemology, Language and Culture |page=88 |year=2006 |publisher=Springer |editor1=Nathalie Gontier |editor2=Jean Paul van Bendegem |editor3=Diederik Aerts |isbn=9781402033957 |url=https://books.google.com/books?id=hp2JiTDBbWkC&pg=PA88}}\n</ref>\n\n<ref name=Diettrich2>\n"The notion of \'truth\' is replaced with \'viability\' within the subjects\' experiential world." From {{cite book |title= The handbook of evolution: The evolution of human societies and culture |author=Olaf Diettrich |chapter=Cognitive evolution; footnote 2 |page=61 |url=https://books.google.com/books?id=Ex5c_pyOsTwC&pg=PA61&lpg=PA61#v=onepage&q&f=false |editor1=Franz M. Wuketits |editor2=Christoph Antweiler |year=2008 |publisher=Wiley-Blackwell}} and in \'\'Evolutionary Epistemology, Language and Culture\'\' cited above, p. 90.\n</ref>\n\n<ref name=Glaserfeld>\n{{cite book |author= Ernst von Glasersfeld |title=Epistemology and education |chapter=Report no. 14: Piaget and the Radical Constructivist Epistemology |url=http://www.vonglasersfeld.com/034 |editor1=CD Smock |editor2=E von Glaserfeld |publisher=Follow Through Publications |pages=1–24 |year=1974}} \n</ref>\n\n<ref name=Glasersfeld2>\n{{cite journal |author= Ernst von Glasersfeld |url=http://www.univie.ac.at/constructivism/EvG/papers/118.pdf |title=Cognition, construction of knowledge and teaching |journal=Synthese |volume=80 |issue=1 |pages=121–140 |year=1989 |doi=10.1007/bf00869951}}\n</ref>\n\n<ref name=Gontier>\n{{cite web |author=Nathalie Gontier |title=Evolutionary Epistemology |url=http://www.iep.utm.edu/evo-epis/ |work=Internet Encyclopedia of Philosophy |year=2006}}\n</ref>\n\n<ref name=Hutchins>\t\n{{cite book |title=Cognition in the Wild |author=Edwin Hutchins |url= |isbn=9780262581462 |year=1996 |page=428 |publisher=MIT Press }} Quoted by {{cite journal |title=Cognitive, embodied or enacted? :Contemporary perspectives for HCI and interaction  |url=http://trans-techresearch.net/wp-content/uploads/2010/11/Rocha-01.pdf |author=Marcio Rocha |year=2011 |publisher=Transtechnology Research Reader |isbn=978-0-9538332-2-1}}\t\n</ref>\n\n<ref name=Jaegher1>\n{{cite book |author1=Ezequiel A Di Paolo |author2=Marieke Rhohde |author3=Hanne De Jaegher |chapter=Horizons for the enactive mind: Values, social interaction, and play |title=Enaction: Toward a New Paradigm for Cognitive Science |editor1=John Stewart |editor2=Oliver Gapenne |editor3=Ezequiel A Di Paolo |url=https://books.google.com/books?id=UtFDJx-gysQC&pg=PA39 |pages=33 \'\'ff\'\' |isbn=  978-0262526012 |publisher=MIT Press |year=2014}}\n</ref>\n\n<ref name=Manetti>\nA collection of papers on this topic is introduced by {{cite journal |title=Agency: From embodied cognition to free will |author1=Duccio Manetti |author2=Silvano Zipoli Caiani |journal=Humana Mente |volume=15 |date=January 2011 |pages=\'\'V\'\'-\'\'XIII\'\' |url=http://www.humanamente.eu/PDF/Issue15_CompletePDF.pdf}}\n</ref>\n\n<ref name=Maturana>\n{{cite book |author1=Humberto R Maturana |author2=Francisco J Varela |year=1992 |title= The tree of knowledge: the biological roots of human understanding |edition=Revised |publisher=Shambhala Publications Inc |chapter=Afterword |page=255 |isbn=978-0877736424}}\n</ref>\n\n<ref name=Munz>\n{{cite book |url=https://books.google.com/books?id=tMuIAgAAQBAJ&pg=PA154&lpg=PA154 |page=154 |author=Peter Munz |title=Philosophical Darwinism: On the Origin of Knowledge by Means of Natural Selection |year=2002 |isbn=9781134884841 |publisher=Routledge}}\n</ref>\n\n<ref name=Mutelesi>\n{{cite journal |title=Radical constructivism seen with Edmund Husserl as starting point |author=Edmond Mutelesi |url=http://www.univie.ac.at/constructivism/journal/2/1/006.mutelesi |journal=Constructivist foundations |volume=2 |issue=1 |pages=6–16 |date=November 15, 2006}}\n</ref>\n\n<ref name=Rohde>\n{{cite book |title=Enaction, Embodiment, Evolutionary Robotics: Simulation Models for a Post-Cognitivist Science of Mind  |chapter= §3.1 The scientist as observing subject |pages=30 \'\'ff\'\' |author=Marieke Rohde |isbn=978-9078677239 |publisher=Atlantis Press |year=2010 |url=https://books.google.com/books?id=LlpZjLMPiHYC&pg=PA30}}\n</ref>\n\n<ref name=Rowlands>\n{{cite book |author=Mark Rowlands |chapter=Chapter 3: The mind embedded §5 The mind enacted |pages=70 \'\'ff\'\' |year=2010 |isbn=0262014556 |publisher=MIT Press |url=https://books.google.com/books?id=AiwjpL-0hDgC&pg=PA70 |title=The new science of the mind: From extended mind to embodied phenomenology}} Rowlands attributes this idea to {{cite book |author=D M MacKay |year=1967 |chapter=Ways of looking at perception |title=Models for the perception of speech and visual form (Proceedings of a symposium) |editor=W Watthen-Dunn |publisher=MIT Press |pages=25 \'\'ff\'\' |url=https://books.google.com/books?id=Ts9JAAAAMAAJ&focus=searchwithinvolume&q=MacKay+Ways+of+looking+at+perception}}\n</ref>\n\n<ref name=EThompson>\n{{cite book |title= Mind in life |chapter=The enactive approach |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |pages=13 \'\'ff\'\' |url=https://books.google.com/books?id=OVGna4ZEpWwC&pg=PA13 |publisher=Harvard University Press |year=2007 }}  ToC, first 65 pages, and index [http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf found here]\n</ref>\n\n<ref name=EThompson2>\n{{cite book |title= Mind in life |chapter=Autonomy and emergence |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |pages=37 \'\'ff\'\' |url=https://books.google.com/books?id=OVGna4ZEpWwC&pg=PA13 |publisher=Harvard University Press |year=2007}} See also the Introduction, p. \'\'x\'\'.\n</ref>\n\n<ref name=EThompson3>\n{{cite book |title= Mind in life |chapter=Chapter 8: Life beyond the gap |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |page=225  |url=https://books.google.com/books?id=OVGna4ZEpWwC&pg=PA225 |publisher=Harvard University Press |year=2007}}\n</ref>\n\n<ref name=EThompson4>\n{{cite book |title= Mind in life |chapter=Life can be known only by life |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |page=165  |url=https://books.google.com/books?id=OVGna4ZEpWwC&pg=PA165 |publisher=Harvard University Press |year=2007}}\n</ref>\n\n<ref name="Evan Thompson">\n{{cite book |title=Mind in life:Biology, phenomenology, and the sciences of mind |author=Evan Thompson |url=http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf |publisher=Harvard University Press |isbn= 978-0674057517 |chapter=Chapter 1: The enactive approach |year=2010}} ToC, first 65 pages, and index [http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf found here].\n</ref>\n\n<ref name=Tascano0>\n{{cite book |title=A Dictionary of Continental Philosophy |editor=John Protevi |url=https://books.google.com/books?id=kRUZ61uISUMC&pg=PA169 |pages=169–170 |chapter = Enaction |isbn=9780300116052 |publisher=Yale University Press |year=2006}}\n</ref>\n\n<ref name=Varela>\n{{cite book |title=The embodied mind: Cognitive science and human experience |author1=Francisco J Varela |author2=Evan Thompson |author3=Eleanor Rosch |url=https://books.google.com/books?id=QY4RoH2z5DoC&printsec=frontcover#v=snippet&q=We%20propose%20as%20a%20%20name%20the%20term%20%20enactive&f=false |year=1992 |publisher=MIT Press |page=9 |quote= |isbn=978-0262261234}}</ref>\n\n<ref name=Ward2>\n"The underpinnings of cognition are inextricable from those of affect, that the phenomenon of cognition itself is essentially bound up with affect.." See p. 104: {{cite book |author1=Dave Ward |author2=Mog Stapleton |year=2012 |url=https://books.google.com/books?id=Y1E7FogqvJ0C&pg=PA89 |chapter=Es are good. Cognition as enacted, embodied, embedded, affective and extended |editor= Fabio Paglieri |title=Consciousness in Interaction: The role of the natural and social context in shaping consciousness |publisher=John Benjamins Publishing |pages=89 \'\'ff\'\' |isbn=978-9027213525}} [http://philpapers.org/archive/WAREAG.pdf On-line version here].\n</ref>\n\n<ref name=RWilson>\n{{cite web |author1=Robert A Wilson |author2=Lucia Foglia |title=Embodied Cognition: §2.2 Enactive cognition |work=The Stanford Encyclopedia of Philosophy (Fall 2011 Edition) |editor=Edward N. Zalta |url = http://plato.stanford.edu/archives/fall2011/entries/embodied-cognition/#EnaCog |date=July 25, 2011}}\n</ref>\n\n}}\n\n==Further reading==\n* {{cite journal |author1=De Jaegher H. |author2=Di Paolo E. A. | year = 2007 | title = Participatory sense-making: An enactive approach to social cognition | url = | journal = Phenomenology and the Cognitive Sciences | volume = 6 | issue = 4| pages = 485–507 | doi=10.1007/s11097-007-9076-9}}\n* Di Paolo, E. A., Rohde, M. and De Jaegher, H., (2010). \'\'Horizons for the Enactive Mind: Values, Social Interaction, and Play.\'\' In J. Stewart, O. Gapenne and E. A. Di Paolo (eds), Enaction: Towards a New Paradigm for Cognitive Science, Cambridge, MA: MIT Press, pp.&nbsp;33 – 87. ISBN 9780262014601\n* [[Daniel Hutto|Hutto, D. D.]] (Ed.)  (2006).  \'\'Radical Enactivism: Intentionality, phenomenology, and narrative.\'\'  In R. D. Ellis & N. Newton (Series Eds.), \'\'Consciousness & Emotion, vol. 2.\'\' ISBN 90-272-4151-1\n* McGann, M. & Torrance, S. (2005).  Doing it and meaning it (and the relationship between the two).  In R. D. Ellis & N. Newton, \'\'Consciousness & Emotion, vol. 1: Agency, conscious choice, and selective perception\'\'. Amsterdam: John Benjamins. ISBN 1-58811-596-8\n* {{cite journal |title=The enactive approach: Theoretical sketches from cell to society |author1=Tom Froese |author2=Ezequiel A DiPaolo |citeseerx = 10.1.1.224.5504 |journal=Pragmatics and Cognition |volume=19 |issue=1 |year=2011 |pages=1–36 |doi=10.1075/pc.19.1.01fro}}\n* {{cite journal |author1=Steve Torrance |author2=Tom Froese |title=An inter-enactive approach to agency: participatory sense-making, dynamics, and sociality. |journal=Humana. Mente |volume=15 |year=2011 |pages=21–53 |citeseerx = 10.1.1.187.1151 }}\n\n==Notes==\n{{reflist |group=Note |refs=\n<ref group=Note name=Cognitivism>\nCognition as information processing like that of a digital computer. From {{cite book |author=Evan Thompson |title=Mind in Life |isbn=978-0674057517}} \'\'Cognitivism\'\', p. 4; See also {{cite web |title=The computational theory of mind |author=Steven Horst |work= The Stanford Encyclopedia of Philosophy (Spring 2011 Edition) |editor=Edward N. Zalta |url=http://plato.stanford.edu/archives/spr2011/entries/computational-mind/ |date=December 10, 2009}}\n</ref>\n\n<ref group=Note name=Connectionism>\nCognition as emergent patterns of activity in a neural network. From {{cite book |author=Evan Thompson |title=Mind in Life |isbn=978-0674057517}} \'\'Connectionism\'\', p. 8; See also {{cite web |title=Connectionism |author=James Garson |work= The Stanford Encyclopedia of Philosophy (Spring 2011 Edition) |editor=Edward N. Zalta |url=http://plato.stanford.edu/archives/win2012/entries/connectionism/ |date=July 27, 2010}}\n</ref>\n\n}}\n\n==External links==\n*{{cite web |title=Consciousness as the emergent property of the interaction between brain, body, & environment: the crucial role of haptic perception  |author=Pietro Morasso |url=http://www.consciousness.it/iwac2005/Material/Morasso.pdf |year=2005 }} Slides related to a chapter on [[haptic perception]] (recognition through touch): {{cite book |editor1=Antonio Chella |editor2=Riccardo Manzotti |author=Pietro Morasso |chapter=Chapter 14: The crucial role of haptic perception |page=234 \'\'ff\'\' |title= Artificial Consciousness |publisher= Academic  |year=2007 |isbn=978-1845400705 |url=https://www.google.com/search?tbo=p&tbm=bks&q=isbn:1845400704&num=10}}\n*{{cite web |title=Questioning Life and Cognition: Some Foundational Issues in the Paradigm of Enaction |url=http://www.enactionseries.com/library/bookjs/co/Original_book_JS.html#Pk1qsEYBVxgUwAM6tVeiff |author=John Stewart |work=Enaction Series: Online Collaborative Publishing |editor1=Olivier Gapenne |editor2=Bruno Bachimont |publisher=Enaction Series |accessdate=April 27, 2014}} \n*{{cite web |title=Educational Multimedia Task Force – MM 1045, REPRESENTATION |url=http://halshs.archives-ouvertes.fr/docs/00/00/18/64/PDF/REPRDel1.pdf |publisher= |author1=George-Louis Baron |author2=Eric Bruillard |author3=Christophe Dansac |date=January 1999}} An overview of the rationale and means and methods for the study of representations that the learner constructs in his/her attempt to understand knowledge in a given field. See in particular §1.2.1.4 \'\'Toward social representations\'\' (p.&nbsp;24)\n*{{cite web |author=Randall Whittaker |year=2001 |title=Autopoiesis and enaction |url=http://www.enolagaia.com/AT.html |publisher=Observer Web}} An extensive but uncritical introduction to the work of [[Francisco Varela]] and [[Humberto Maturana]]\n*{{cite journal|title=Enactivism: Arguments & Applications.|journal=Avant|date=Autumn 2014|volume= V| issue =  2/2014|doi=10.12849/50202014.0109.0002|url=http://avant.edu.pl/en/22014-2|accessdate=27 November 2014}} Entire journal issue on enactivism\'s status and current debates.\n\n[[Category:Behavioral neuroscience]]\n[[Category:Cognitive science]]\n[[Category:Consciousness]]\n[[Category:Educational psychology]]\n[[Category:Enactive cognition]]\n[[Category:Epistemology of science]]\n[[Category:Knowledge representation]]\n[[Category:Metaphysics of mind]]\n[[Category:Motor cognition]]\n[[Category:Neuropsychology]]\n[[Category:Perception]]\n[[Category:Philosophical theories]]\n[[Category:Philosophy of psychology]]\n[[Category:Psychological concepts]]\n[[Category:Psychological theories]]\n[[Category:Sociology of knowledge]]\n[[Category:Action (philosophy)]]']
['Open-world assumption', '2692616', 'In a [[Mathematical logic|formal system of logic]] used for [[knowledge representation]], the \'\'\'open-world assumption\'\'\' is the assumption that the [[truth value]] of a [[statement (logic)|statement]] may be true irrespective of whether or not it is \'\'known\'\' to be true. It is the opposite of the [[closed-world assumption]], which holds that any statement that is true is also known to be true.\n\nThe open-world assumption (OWA) codifies the informal notion that in general no single agent or observer has complete knowledge, and therefore cannot make the closed-world assumption. The OWA limits the kinds of inference and deductions an agent can make to those that follow from statements that are known to the agent to be true. In contrast, the closed world assumption allows an agent to infer, from its lack of knowledge of a statement being true, anything that [[Logical consequence|follows from]] that statement being false.\n\nHeuristically, the open-world assumption applies when we represent knowledge within a system as we discover it, and where we cannot guarantee that we have discovered or will discover complete information. In the OWA, statements about knowledge that are not included in or inferred from the knowledge explicitly recorded in the system may be considered unknown, rather than wrong or false. \n\n[[Semantic Web]] languages such as [[Web Ontology Language|OWL]] make the open-world assumption. The absence of a particular statement within the web means, in principle, that the statement has not been made explicitly yet, irrespective of whether it would be true or not, and irrespective of whether we believe that it would be true or not. In essence, from the absence of a statement alone, a deductive reasoner cannot (and must not) infer that the statement is false.\n\nMany [[procedural programming language]]s and [[database]]s make the closed-world assumption. For example, if a typical airline database does not contain a seat assignment for a traveler, it is assumed that the traveler has not checked in. The closed-world assumption typically applies when a system has complete control over information; this is the case with many database applications where the [[database transaction]] system acts as a central broker and arbiter of concurrent requests by multiple independent clients (e.g., airline booking agents). There are, however, many databases with incomplete information: for example, one cannot assume that because there is no mention on a patient\'s history of a particular allergy, that the patient does not suffer from that allergy.\n\n\'\'\'Example\'\'\'\n  Statement: "Mary" "is a citizen of" "France"\n\n  Question: Is Paul a citizen of France?\n\n  "Closed world" (for example SQL) answer: No.\n  "Open world" answer: Unknown.\n\nUnder OWA, failure to derive a fact does not imply the opposite. For example, assume we only know that Mary is a citizen of France. From this information we can neither conclude that Paul is not a citizen of France, nor that he is. Therefore, we admit the fact that our knowledge of the world is incomplete. The open-world assumption is closely related to the [[Monotonicity of entailment|monotonic]] nature of [[first-order logic]]: adding new information never falsifies a previous conclusion. Namely, if we subsequently learn that Paul is also a citizen of France, this does not change any earlier positive or negative conclusions.\n\nThe language of logic programs with [[Stable_model_semantics#Strong_negation|strong negation]] allows us to postulate the closed-world assumption for some predicates and leave the other predicates in the realm of the open-world assumption.\n\n==See also==\n*[[Closed-world assumption]]\n\n==References==\n*{{cite book |last1=Russell |first1=Stuart J. |authorlink1=Stuart J. Russell |last2=Norvig |first2=Peter |authorlink2=Peter Norvig |title=Artificial Intelligence: A Modern Approach |year=2010 |publisher=Prentice Hall |location=Upper Saddle River |isbn=9780136042594 |url=http://www.pearsonhighered.com/educator/product/Artificial-Intelligence-A-Modern-Approach/9780136042594.page |edition=3rd}}\n\n{{DEFAULTSORT:Open-world assumption}}\n[[Category:Logic programming]]\n[[Category:Knowledge representation]]']
['User profile', '35773358', "{{pp-move-indef|small=yes}}\n{{pp-semi-indef|small=yes}}\n{{for|Wikipedia's guideline on its own user pages|WP:USERPAGE}}\n\n{{unreferenced|date=September 2014}}\n[[File:User Profile Info Model.png|thumb|User Profile Info Model]]\n\nA '''user profile''' is a visual display of [[personal data]] associated with a specific [[User (computing)|user]], or a [[customized]] [[desktop environment]]. A profile refers therefore to the explicit digital representation of a person's [[Online identity|identity]]. A user profile can also be considered as the computer representation of a [[user modeling|user model]].\n\nA profile can be used to store the description of the characteristics of person. This information can be exploited by systems taking into account the persons' characteristics and preferences.\n\n[[Profiling (information science)|Profiling]] is the process that refers to construction of a profile via the extraction from a set of data.\n\nUser profiles can be found on [[operating system]]s, [[computer program]]s, [[recommender system]]s, or [[Website|dynamic websites]] (such as [[Social network service|online social networking]] sites or [[bulletin board]]s).\n\n==See also==\n*[[Online identity]]\n*[[Online identity management]]\n*[[Personally identifiable information]]\n*[[Web mining]]\n*[[Internet privacy]]\n\n{{Online social networking}}\n{{Social networking}}\n\n[[Category:Identity management]]\n[[Category:Knowledge representation]]\n[[Category:Software features]]\n\n\n{{compu-stub}}"]
['WYSIWYM (interaction technique)', '15998635', "{{for|the editing paradigm|WYSIWYM}}\n\n'''What you see is what you meant''' ('''WYSIWYM''') is a text editing [[interaction technique]] that emerged from two projects at [[University of Brighton]]. It allows users to create abstract [[knowledge representation]]s such as those required by the [[Semantic Web]] using a natural language interface. [[Natural language understanding]] (NLU) technology is not employed. Instead,  [[natural language generation]] (NLG) is used in a highly interactive manner.\n\nThe text editor accepts repeated refinement of a selected span of text as it becomes increasingly less vacuous of authored semantics. Using a mouse, a text property held in the evolving text can be further refined by a set of options derived by NLG from a built-in [[ontology]]. An invisible representation of the semantic knowledge is created which can be used for multilingual document generation, formal knowledge formation, or any other task that requires formally specified information.<ref>{{Cite web|url = http://oro.open.ac.uk/39116/1/Thesis_Final.pdf|title = Generating Natural Language Explanations For Entailments In Ontologies|date = 2013|accessdate = 10 November 2014|website = Open Research Online|publisher = The Open University|last = Nguyen|first = Tu}}</ref>\n\nThe two projects at Brighton worked in the field of Conceptual Authoring to lay a foundation for further research and development of a Semantic Web Authoring Tool (SWAT). This tool has been further explored as a means for developing a knowledge base by those without prior experience with Controlled Natural Language tools.<ref>{{Cite journal|url = http://oro.open.ac.uk/40385/|title = How easy is it to learn a controlled natural language for building a knowledge base?|last = Williams|first = Sandra|date = 13 June 2014|journal = Fourth Workshop on Controlled Natural Language, 20–22 August 2014, Galway, Ireland (forthcoming), Springer International Publishing AG.|accessdate = 10 November 2014|doi = |pmid = }}</ref>\n\n==See also==\n*[[Semantic markup]]\n* [[Web Ontology Language|OWL [Web Ontology Language]]]\n* [[WYSIWYM]]\n* [[Protégé (software)]]\n\n== References ==\n<references />\n\n==External links==\n* Nguyen, Tu (2013).  ''[http://oro.open.ac.uk/39116/ Generating Natural Language Explanations For Entailments In Ontologies.]  ''PhD thesis The Open University.\n* [http://mcs.open.ac.uk/nlg/research/Conceptual_Authoring.html Conceptual Authoring] at Natural Language Generation group of the Open University\n* [http://mcs.open.ac.uk/nlg/SWAT/ SWAT: Semantic Web Authoring Tool] research project\n* [http://mcs.open.ac.uk/nlg/old_projects/wysiwym/ WYSIWYM home page]\n\n{{DEFAULTSORT:WYSIWYM (interaction technique)}}\n[[Category:Knowledge representation]]\n[[Category:Natural language generation]]\n[[Category:Ontology (information science)]]\n[[Category:Ontology editors]]\n[[Category:Semantic Web]]\n\n\n{{software-stub}}"]
['Class (knowledge representation)', '46926920', '{{Main|Ontology components#Classes|l1 = Classes on the ontology component page}}\n\nIn  [[Knowledge representation and reasoning|knowledge representation]], a \'\'\'class\'\'\' is a collection of individuals or objects.<ref name="DLs">{{cite proceedings|url=http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=110B072B7265573684AB8D4F0D6B2306?doi=10.1.1.177.2787&rep=rep1&type=pdf|title=Description Logics: Foundations for Class-based Knowledge Representation|author1=Diego Calvanese|author2=Giuseppe De Giacomo|author3=Maurizio Lenzerini|conference=[[Logic in Computer Science]]|year=2002}}</ref> A class can be defined either by [[Extensional definition|extension]], or by [[Intensional definition|intension]], using what is called in some ontology languages like [[Web Ontology Language|OWL]]. If we follow the [[Type–token distinction]], the ontology is divided into individuals, who are real worlds objects, or events, and types, or classes, who are sets of real world objects. Class expressions or definitions gives the properties that the individuals must fulfill to be members of the class. Individuals that fulfill the property are called [[Instance (computer science)|Instances]].\n\n== Relationships ==\n\n=== Instantiation ===\n\nThe instantiation [[relation (mathematics)|relationship]] is a relation between objects and classes. We say that an object O, say \'\'Harry the eagle\'\' is an instance of a class, say \'\'Eagle\'\'. \'\'Harry the eagle\'\' has all the properties that we can attribute to an eagle, for example his parents were eagles, he\'s a bird, he\'s a meat eater and so on. It\'s a special kind of [[is a]] relationship. It\'s noted [[Concept assertion]] (<math> : </math>) in [[Description logic]]s, a family of logic based on classes, class assertion <ref name="owlclass">{{cite web|url=http://www.w3.org/TR/owl2-syntax/#Class_Assertions|title=owl2 syntax}}</ref>\n\n=== Subsumption ===\nClasses can [[is a|subsume]] each other. We say usually that if <code>A</code> and <code>B</code> are classes, and all <code>A</code> instances are also <code>B</code> instances, then B subsumes A, or A is a subclass of B, for example in the OWL Language it\'s called subclassof.<ref name="owlclass"/>\n\n==References==\n<references/>\n\n== See also ==\n* [[Metaclass (Semantic Web)]]\n* [[Ontology (information science)|Ontology]]\n* [[Ontology components]]\n* [[Description logic]]\n\n[[Category:Knowledge representation]]\n[[Category:Semantic Web Ontology]]\n\n\n{{Information-science-stub}}']
['Flail space model', '47369663', 'The \'\'\'flail space model (FSM)\'\'\' is a [[Physical model|model]] of how a [[passenger|car passenger]] moves in a [[vehicle]] that collides with a roadside feature such as a [[Traffic barrier|guardrail]] or a [[crash cushion]]. Its principal purpose is to assess the potential risk of harm to the hypothetical occupant as he or she impacts the interior of the passenger compartment and, ultimately, the efficacy of an experimental roadside feature undergoing full-scale vehicle crash testing.\n\nThe FSM eliminates the complexity and expense of using instrumented [[Crash test dummy|anthropometric dummies]] during the crash test experiments. Furthermore, while crash test dummies were developed to model collisions between vehicles, they are not accurate when used for the sorts of collision angles that occur when a vehicle collides with a roadside feature; by contrast, the FSM was designed for such collisions.<ref name="gabauer">Gabauer, Douglas, "A methodology to evaluate the flail space model using event data recorder technology", Department of Mechanical Engineering, Rowan University, Glassboro, NJ, 2004.</ref>\n\n== History ==\nThe FSM is based on research performed at [[Southwest Research Institute]] in 1980<ref>Michie, J. D., "Development of improved criteria for evaluating safety performance of highway appurtenances", Final Report of  Internal Research Project No. 03-9254, Southwest Research Institute, San Antonio, Texas, June 1980.</ref> and published in 1981 in the paper entitled "Collision Risk Assessment Based on Occupant Flail-Space Model" by Jarvis D. Michie.<ref name=":0">Michie, J. D., "Collision risk assessment based on occupant flail space model," in Transportation Research Record 796, 1981, pp. 1–9.</ref> The FSM (coined by Michie) was accepted by the highway community and published as a key part of the "Recommended Procedures for the Safety Evaluation of Highway Appurtenances" published in 1981 in [[National Cooperative Highway Research Program]] (NCHRP) Report 230.<ref>Michie, J. D.  National Cooperative Highway Research Program Report 230: Recommended Procedures for the Safety Performance Evaluation of Highway Appurtenances.  NCHRP Transportation Research Board, Washington, DC, March 1981.</ref> In 1993, the NCHRP Report was updated and presented as NCHRP Report 350;<ref>Ross, H. E., Jr. et al.  National Cooperative Highway Research Program Report 350:  Recommended Procedures for the Safety Evaluation of Highway Features.  NCHRP Transportation Research Board, Washington, DC, 1993.</ref> in this research effort performed by the [[Texas A&M Transportation Institute|Texas Transportation Research Institute]], the FSM was reexamined and was unmodified in the new publication. In 2004, Douglas Gabauer further examined the efficacy of the FSM in his [[PhD thesis]].<ref name="gabauer" /> The [[American Association of State Highway and Transportation Officials]] (AASHTO) retained the FSM as the method of assessing the risk of harm to vehicle occupants in the 2009 "Manual for Assessing Safety Hardware" that replaced NCHRP Report 350, stating that the FSM had "served its intended purpose well".<ref>Manual for Assessing Safety Hardware.  American Association of State Highway and Transportation Officials, Washington, DC,  2009.</ref>\n\n== Details ==\nThe FSM hypothesis divides the collision into two stages.  In stage one, the unrestrained occupant is propelled forward and sideways in the compartment space due to vehicle collision [[Acceleration|accelerations]] and then impacts one or more surfaces (including the steering wheel) with velocity "V". According to the model, the vehicle (instead of the occupant) is the object that is accelerating. The occupant experiences no injury-producing force prior to contact with the compartment surfaces.<ref name=":0" />\n\nIn stage two, the occupant is assumed to remain in contact with the compartment surface and experiences the same accelerations as the vehicle for the rest of the collision.  The occupant may sustain [[Blunt trauma|injury]] at the end of stage one based on the velocity of impact with the compartment surfaces and due to vehicle accelerations during stage two.  The occupant impact velocity and acceleration are computed from the vehicle collision acceleration history and the compartment geometry.  Finally, the hypothetical occupant impact velocity and acceleration are then compared to threshold values of [[Engineering tolerance|human tolerance]] to these forces.<ref name=":0" />\n\n==References==\n{{reflist|colwidth=30em}}\n\n[[Category:Articles created via the Article Wizard]]\n[[Category:Scientific modeling]]\n[[Category:Applied mathematics]]\n[[Category:Knowledge representation]]\n[[Category:Mathematical modeling]]\n[[Category:Transport safety]]']
['Categorization', '72717', '{{Refimprove|date=January 2017}}\n{{selfref|For information about Wikipedia\'s article categories, see [[Help:Category]].}}\n{{for|particular uses|Category (disambiguation)}}\n{{Information science}}\n\'\'\'Categorization\'\'\' is the process in which ideas and objects are recognized, differentiated, and understood.<ref>Cohen, H., & Lefebvre, C. (Eds.). (2005).[https://books.google.com/books?id=5WDfl14RgKMC \'\'Handbook of Categorization in Cognitive Science\'\']. Elsevier.</ref> Categorization implies that objects are grouped into categories, usually for some specific purpose. Ideally, a category illuminates a [[Binary relation|relationship]] between the [[subject (philosophy)|subject]]s and [[object (philosophy)|object]]s of knowledge. Categorization is fundamental in language, prediction, [[inference]], decision making and in all kinds of environmental interaction. It is indicated that categorization plays a major role in [[computer programming]].<ref>Frey, T., Gelhausen, M., & Saake (2011).[http://ecs.victoria.ac.nz/twiki/pub/Events/PLATEAU/Program/plateau2011-frey.pdf \'\' Categorization of Concerns – A Categorical Program Comprehension Model. In Proceedings of the Workshop on Evaluation and Usability of Programming Languages and Tools (PLATEAU) at the ACM Onward! and SPLASH Conferences. October, 2011. Portland, Oregon, USA\'\'].</ref>\n\nThere are many categorization theories and techniques. In a broader historical view, however, three general approaches to categorization may be identified:\n* Classical categorization\n* Conceptual clustering\n* Prototype theory\n\n==The classical view==\n{{main|Categories (Aristotle)}}\n\'\'\'Classical categorization\'\'\' first appears in the context of [[Western Philosophy]] in the work of [[Plato]], who, in his [[Statesman (dialogue)|Statesman]] dialogue, introduces the approach of grouping objects based on their similar [[Property (philosophy)|properties]]. This approach was further explored and systematized by [[Aristotle]] in his [[Categories (Aristotle)|Categories]] treatise, where he analyzes the differences between [[Class (philosophy)|class]]es and [[Object (philosophy)|object]]s. Aristotle also applied intensively the classical categorization scheme in his approach to the classification of living beings (which uses the technique of applying successive narrowing questions such as "Is it an animal or vegetable?", "How many feet does it have?", "Does it have fur or feathers?", "Can it fly?"...), establishing this way the basis for natural [[Taxonomy (biology)|taxonomy]].\n\nThe classical [[Aristotelianism|Aristotelian]] view claims that categories are discrete entities characterized by a set of properties which are shared by their members. In [[analytic philosophy]], these properties are assumed to establish the conditions which are both [[necessary and sufficient condition]]s to capture meaning. \n\nAccording to the classical view, categories should be clearly defined, mutually exclusive and collectively exhaustive. This way, any entity of the given classification universe belongs unequivocally to one, and only one, of the proposed categories.\n\n==Conceptual clustering==\n{{main|Conceptual clustering}}\n\'\'\'Conceptual clustering\'\'\' is a modern variation of the classical approach, and derives from attempts to explain how knowledge is represented. In this approach, [[Class (philosophy)|class]]es (clusters or entities) are generated by first formulating their conceptual descriptions and then classifying the entities according to the descriptions. \n\nConceptual clustering developed mainly during the 1980s, as a machine paradigm for [[unsupervised learning]]. It is distinguished from ordinary [[Cluster analysis|data clustering]] by generating a concept description for each generated category. \n\nCategorization tasks in which category labels are provided to the learner for certain objects are referred to as supervised classification, [[supervised learning]], or [[concept learning]]. Categorization tasks in which no labels are supplied are referred to as unsupervised classification, [[unsupervised learning]], or [[Cluster analysis|data clustering]]. The task of supervised classification involves extracting information from the labeled examples that allows accurate prediction of class labels of future examples. This may involve the [[abstraction]] of a rule or concept relating observed object features to category labels, or it may not involve abstraction (e.g., [[Exemplar theory|exemplar model]]s). The task of clustering involves recognizing inherent structure in a data set and grouping objects together by similarity into classes. It is thus a process of \'\'generating\'\' a classification structure. \n\nConceptual clustering is closely related to [[fuzzy set]] theory, in which objects may belong to one or more groups, in varying degrees of fitness.\n\n==Prototype theory==\n{{main|Prototype theory}}\nSince the research by [[Eleanor Rosch]] and [[George Lakoff]] in the 1970s, categorization can also be viewed as the process of grouping things based on [[prototype]]s—the idea of necessary and sufficient conditions is almost never met in categories of naturally occurring things. It has also been suggested that categorization based on prototypes is the basis for human development, and that this learning relies on learning about the world via [[embodied cognition|embodiment]].\n\nA [[cognition|cognitive]] approach accepts that natural categories are graded (they tend to be fuzzy at their boundaries) and inconsistent in the status of their constituent members.\n\nSystems of categories are not objectively "out there" in the world but are rooted in people\'s experience. Conceptual categories are not identical for different cultures, or indeed, for every individual in the same culture. \n\nCategories form part of a hierarchical structure when applied to such subjects as [[Taxonomy (biology)|taxonomy]] in [[biological classification]]: higher level: life-form level, middle level: generic or [[genus]] level, and lower level: the [[species]] level. These can be distinguished by certain traits that put an item in its distinctive category. But even these can be arbitrary and are subject to revision.\n\nCategories at the middle level are perceptually and conceptually the more salient. The generic level of a category tends to elicit the most responses and richest images and seems to be the psychologically basic level. Typical taxonomies in zoology for example exhibit categorization at the [[embodied cognition|embodied]] level, with similarities leading to formulation of "higher" categories, and differences leading to differentiation within categories.\n\n== Miscategorization ==\nMiscategorization can be a [[Fallacy|logical fallacy]] in which diverse and dissimilar objects, concepts, entities, etc. are grouped together based upon illogical common denominators, or common denominators that virtually any concept, object or entity have in common. A common way miscategorization occurs is through an over-categorization of concepts, objects or entities, and then miscategorization based upon characters that virtually all things have in common.\n\n== See also ==\n{{too many see alsos|date=October 2013}}\n{{columns-list|3| \n* [[Lumpers and splitters]]\n* [[Artificial neural network]]\n* [[Category learning]]\n* [[Categorical perception]]\n* [[Classification in machine learning]]\n* [[Family resemblance]]\n* [[Fuzzy concept]]\n* [[Language acquisition]]\n* [[Library classification]]\n* [[Machine learning]]\n* [[Multi-label classification]]\n* [[Natural kind]]\n* [[Ontology]]\n* [[Pattern recognition]]\n* [[Perceptual learning]]\n* [[Semantics]]\n* [[Socrates]]\n* [[Sortal]]\n* [[Structuralism]]\n* [[Symbol grounding]]\n* [[Taxonomy (general)]]\n}}\n\n== References ==\n{{Reflist}}\n\n==External links==\n{{Wiktionary}}\n* [http://eprints.ecs.soton.ac.uk/11725/ To Cognize is to Categorize: Cognition is Categorization]\n*[http://toolserver.org/~dapete/catgraph/ Wikipedia Categories Visualizer]\n*[http://www.revue-emulations.net/archives/n8/categentretien Interdisciplinary Introduction to Categorization: Interview with Dvora Yanov (political sciences), Amie Thomasson (philosophy) and Thomas Serre (artificial intelligence)]\n\n{{philosophy of language}}\n\n[[Category:Knowledge representation]]\n[[Category:Concepts in epistemology]]\n[[Category:Semantics]]\n[[Category:Cognition]]']
['Pattern language', '182837', '{{About|the structured design approach by architect Christopher Alexander}}\nA \'\'\'pattern language\'\'\' is a method of describing good design practices or patterns of useful organization within a field of expertise. The term was coined by architect [[Christopher Alexander]] and popularized by his 1977 book \'\'[[A Pattern Language]]\'\'.\n\nA pattern language can also be an attempt to express the deeper wisdom of what brings aliveness within a particular field of human endeavor, through a set of interconnected patterns. Aliveness is one placeholder term for "the quality that has no name": a sense of wholeness, spirit, or grace, that while of varying form, is precise and empirically verifiable.{{cn|date=March 2016}} Some advocates{{who|date=March 2016}} of this design approach claim that ordinary people can use it to successfully solve very large, complex design problems.\n\n==What is a pattern?==\n{{See also|Design pattern}}\nWhen a designer designs something – whether a house, computer program, or lamp – they must make many decisions about how to solve problems. A single problem is documented with its typical place (the [[syntax]]), and use (the [[grammar]]) with the most common and recognized good solution seen in the wild, like the examples seen in [[dictionary|dictionaries]]. Each such entry is a single [[design pattern]]. Each pattern has a name, a descriptive entry, and some cross-references, much like a dictionary entry. A documented pattern should explain why that solution is good in the pattern\'s contexts.\n\nElemental or universal \'\'patterns\'\' such as "door" or "partnership" are versatile ideals of design, either as found in experience or for use as components in practice, explicitly described as holistic resolutions of the forces in recurrent contexts and circumstances, whether in architecture, medicine, software development or governance, etc. Patterns might be invented or found and studied, such as the naturally occurring patterns of design that characterize human environments.<ref>Henshaw, J. [http://www.synapse9.com/pub/2015_PURPLSOC-JLHfinalpub.pdf Guiding Patterns of Naturally Occurring Design: Elements. PURPLSOC 2015 proceedings, July 3-5 2015 Krems, Austria] PURPLSOC meeting on the many open scientific questions, e.g. regarding the theoretical background of patterns and the practical implementation of pattern methods in research and teaching.</ref>\n\nLike all languages, a pattern language has [[vocabulary]], [[syntax]], and [[grammar]] – but a pattern language applies to some complex activity other than communication. In pattern languages for design, the parts break down in this way:\n* The language description – the \'\'vocabulary\'\' – is a collection of named, described solutions to problems in a field of interest. These are called \'\'design patterns\'\'. So, for example, the language for architecture describes items like: settlements, buildings, rooms, windows, latches, etc.\n* Each solution includes \'\'syntax\'\', a description that shows where the solution fits in a larger, more comprehensive or more abstract design. This automatically links the solution into a web of other needed solutions. For example, rooms have ways to get light, and ways to get people in and out.\n* The solution includes \'\'grammar\'\' that describes how the solution solves a problem or produces a benefit. So, if the benefit is unneeded, the solution is not used. Perhaps that part of the design can be left empty to save money or other resources; if people do not need to wait to enter a room, a simple doorway can replace a waiting room.\n* In the language description, grammar and syntax cross index (often with a literal alphabetic index of pattern names) to other named solutions, so the designer can quickly think from one solution to related, needed solutions, and document them in a logical way. In Christopher Alexander\'s book \'\'A Pattern Language\'\', the patterns are in decreasing order by size, with a separate alphabetic index.\n* The web of relationships in the index of the language provides many paths through the design process.\n\nThis simplifies the design work because designers can start the process from any part of the problem they understand and work toward the unknown parts. At the same time, if the pattern language has worked well for many projects, there is reason to believe that even a designer who does not completely understand the design problem at first will complete the design process, and the result will be usable. For example, skiers coming inside must shed snow and store equipment. The messy snow and boot cleaners should stay outside. The equipment needs care, so the racks should be inside.\n\n==Many patterns form a language==\nJust as [[words]] must have [[Grammar|grammatical]] and [[Semantics|semantic]] relationships to each other in order to make a spoken [[language]] useful, design patterns must be related to each other in position and utility order to form a pattern language. Christopher Alexander\'s work describes a process of decomposition, in which the designer has a problem (perhaps a commercial assignment), selects a solution, then discovers new, smaller problems resulting from the larger solution. Occasionally, the smaller problems have no solution, and a different larger solution must be selected. Eventually all of the remaining design problems are small enough or routine enough to be solved by improvisation by the builders, and the "design" is done.\n\nThe actual organizational structure ([[Hierarchy|hierarchical]], [[Iterative method|iterative]], etc.) is left to the discretion of the designer, depending on the problem. This explicitly lets a designer explore a design, starting from some small part. When this happens, it\'s common for a designer to realize that the problem is actually part of a larger solution. At this point, the design almost always becomes a better design.\n\nIn the language, therefore, each pattern has to indicate its relationships to other patterns and to the language as a whole. This gives the designer using the language a great deal of guidance about the related problems that must be solved.\n\nThe most difficult part of having an outside expert apply a pattern language is in fact to get a reliable, complete list of the problems to be solved. Of course, the people most familiar with the problems are the people that need a design. So, Alexander famously advocated on-site improvisation by concerned, empowered users,<ref>A Pattern Language, ibid</ref><ref>Alexander, Christopher, The Oregon Project</ref> as a powerful way to form very workable large-scale initial solutions, maximizing the utility of a design, and minimizing the design rework. The desire to empower users of architecture was, in fact, what led Alexander to undertake a pattern language project for architecture in the first place.\n\n==Design problems in a context==\nAn important aspect of design patterns is to identify and document the key ideas that make a good system different from a poor system (that may be a house, a computer program or an object of daily use), and to assist in the design of future systems. The idea expressed in a pattern should be general enough to be applied in very different systems within its context, but still specific enough to give constructive guidance.\n\nThe range of situations in which the problems and solutions addressed in a pattern apply is called its context. An important part in each pattern is to describe this context. Examples can further illustrate how the pattern applies to very different situation.\n\nFor instance, Alexander\'s pattern "A PLACE TO WAIT" addresses bus stops in the same way as waiting rooms in a surgery, while still proposing helpful and constructive solutions. The [[Design Patterns|"Gang-of-Four" book \'\'Design Patterns\'\']] by Gamma et al. proposes solutions that are independent of the programming language, and the program\'s application domain.\n\nStill, the problems and solutions described in a pattern can vary in their level of abstraction and generality on the one side, and specificity on the other side. In the end this depends on the author\'s preferences. However, even a very abstract pattern will usually contain examples that are, by nature, absolutely concrete and specific.\n\nPatterns can also vary in how far they are proven in the real world. Alexander gives each pattern a rating by zero, one or two stars, indicating how well they are proven in real-world examples. It is generally claimed that all patterns need at least some existing real-world examples. It is, however, conceivable to document yet unimplemented ideas in a pattern-like format.\n\nThe patterns in Alexander\'s book also vary in their level of scale – some describing how to build a town or neighbourhood, others dealing with individual buildings and the interior of rooms. Alexander sees the low-scale artifacts as constructive elements of the large-scale world, so they can be connected to a [[#Aggregation in an associative network (pattern language)|hierarchic network]].\n\n===Balancing of forces===\nA pattern must characterize the problems that it is meant to solve, the context or situation where these problems arise, and the conditions under which the proposed solutions can be recommended.\n\nOften these problems arise from a conflict of different interests or "forces". A pattern emerges as a dialogue that will then help to balance the forces and finally make a decision.\n\nFor instance, there could be a pattern suggesting a wireless telephone. The forces would be the need to communicate, and the need to get other things done at the same time (cooking, inspecting the bookshelf). A very specific pattern would be just "WIRELESS TELEPHONE". More general patterns would be "WIRELESS DEVICE" or "SECONDARY ACTIVITY", suggesting that a secondary activity (such as talking on the phone, or inspecting the pockets of your jeans) should not interfere with other activities.\n\nThough quite unspecific in its context, the forces in the "SECONDARY ACTIVITY" pattern are very similar to those in "WIRELESS TELEPHONE". Thus, the competing forces can be seen as part of the essence of a design concept expressed in a pattern.\n\n===Patterns contain their own rationale===\nUsually a pattern contains a rationale referring to some given values. For Christopher Alexander, it is most important to think about the people who will come in contact with a piece of architecture. One of his key values is making these people feel more alive. He talks about the "quality without a name" (QWAN).\n\nMore generally, we could say that a good system should be accepted, welcomed and happily embraced as an enrichment of daily life by those who are meant to use it, or – even better – by all people it affects. For instance, when discussing a street café, Alexander discusses the possible desires of a guest, but also mentions people who just walk by.\n\nThe same thinking can be applied to technical devices such as telephones and cars, to social structures like a team working on a project, or to the user interface of a computer program. The qualities of a software system, for instance, could be rated by observing whether users spend their time enjoying or struggling with the system.\n\nBy focusing on the impacts on human life, we can identify patterns that are independent from changing technology, and thus find "timeless quality" (Alexander).\n\n==Generic structure and layout==\nUsually the author of a pattern language or collection chooses a generic structure for all the patterns it contains, breaking each into generic sections like context, problem statement, solution etc.\n\nChristopher Alexander\'s patterns, for instance, each consist of a short name, a rating (up to two \'*\' symbols), a sensitizing picture, the context description, the problem statement, a longer part of text with examples and explanations, a solution statement, a sketch and further references. This structure and layout is sometimes referred to as the "Alexandrian form".\n\nAlexander uses a special text layout to mark the different sections of his patterns. For instance, the problem statement and the solution statement are printed in bold font, the latter is always preceded by the "Therefore:" keyword. Some authors instead use explicit labels, which creates some degree of redundancy.\n\n===Meaningful names===\nWhen design is done by a team, pattern names will form a vocabulary they can share. This makes it necessary for pattern names to be easy to remember and highly descriptive. Some examples from Alexander\'s works are WINDOW PLACE (helps define where windows should go in a room) and A PLACE TO WAIT (helps define the characteristics of bus stops and hospital waiting rooms, for example).\n\n==Aggregation in an associative network (pattern language)==\nA pattern language, as conceived by Alexander, contains links from one pattern to another, so when trying to apply one pattern in a project, a designer is pushed to other patterns that are considered helpful in its context.\n\nIn Alexander\'s book, such links are collected in the "references" part, and echoed in the linked pattern\'s "context" part – thus the overall structure is a directed graph. A pattern that is linked to in the "references" usually addresses a problem of lower scale, that is suggested as a part of the higher-scale problem. For instance, the "PUBLIC OUTDOOR ROOM" pattern has a reference to "STAIR SEATS".\n\nEven without the pattern description, these links, along with meaningful names, carry a message: When building a place outside where people can spend time ("PUBLIC OUTDOOR ROOM"), consider to surround it by stairs where people can sit ("STAIR SEATS"). If you are planning an office ("WORKSHOPS AND OFFICES"), consider to arrange workspaces in small groups ("SMALL WORKING GROUPS"). Alexander argues that the connections in the network can be considered even more meaningful than the text of the patterns themselves.\n\nThe links in Alexander\'s book clearly result in a hierarchic network. Alexander draws a parallel to the hierarchy of a grammar – that is one argument for him to speak of a pattern \'\'language\'\'.\n\nThe idea of linking is generally accepted among pattern authors, though the semantic rationale behind the links may vary. Some authors, however, like Gamma et al. in \'\'[[Design Patterns]]\'\', make only little use of pattern linking – possibly because it did not make that much sense for their collection of patterns. In such a case we would speak of a \'\'pattern catalogue\'\' rather than a \'\'pattern language\'\'.<ref name="dearden">{{cite journal | author = Andy Dearden, Janet Finlay | title = Pattern Languages in HCI: A critical review | date = January 2006 | journal = Human Computer Interaction | volume = 21 | issue = 1 }}</ref>\n\n===Usage===\nAlexander encouraged people who used his system to expand his language with patterns of their own. In order to enable this, his books do not focus strictly on architecture or civil engineering; he also explains the general method of pattern languages. The original concept for the book \'\'A Pattern Language\'\' was that it would be published in the form of a 3-ring binder, so that pages could easily be added later; this proved impractical in publishing.<ref>Portland Urban Architecture Research Laboratory\nSymposium 2009, presentation by 4 of 6 original authors of \'\'A Pattern Language\'\'.</ref>  The pattern language approach has been used to document expertise in diverse fields. Some examples are [[Design pattern (architecture)|architectural patterns]], [[Design pattern (computer science)|computer science patterns]], [[interaction design pattern]]s, [[pedagogical patterns]],  social action patterns, and group facilitation patterns. The pattern language approach has also been recommended as a way to promote [[civic intelligence]] by helping to coordinate actions for diverse people and communities who are working together on significant shared problems (see <ref>Schuler, D. [http://publicsphereproject.org/sites/default/files/Critical%20Enablers%20of%20Civic%20Intelligence.reduced.pdf Choosing Success: Pattern Languages as Critical Enablers of Civic Intelligence]; PUARL Conference, Portland, OR. 2009</ref> for additional discussion of motivation and rationale as well as examples and experiments).  Alexander\'s specifications for using pattern languages as well as creating new ones remain influential, and his books are referenced for style by experts in unrelated fields.\n\nIt is important to note that notations such as [[Unified Modeling Language|UML]] or the [[flowchart]] symbol collection are not pattern languages. They could more closely be compared to an alphabet: their symbols could be used to document a pattern language, but they are not a language by themselves. A [[recipe]] or other sequential set of steps to be followed, with only one correct path from start to finish, is also not a pattern language. However, the process of designing a new recipe might benefit from the use of a pattern language.\n\n===Simple example of a pattern===\n*\'\'Name\'\': ChocolateChipRatio\n*\'\'Context\'\': You are baking chocolate chip cookies in small batches for family and friends\n*\'\'Consider these patterns first\'\': SugarRatio, FlourRatio, EggRatio\n*\'\'Problem\'\': Determine the optimum ratio of chocolate chips to cookie dough\n*\'\'Solution\'\': Observe that most people consider chocolate to be the best part of the chocolate chip cookie. Also observe that too much chocolate may prevent the cookie from holding together, decreasing its appeal. Since you are cooking in small batches, cost is not a consideration. Therefore, use the maximum amount of chocolate chips that results in a really sturdy cookie.\n*\'\'Consider next\'\': NutRatio or CookingTime or FreezingMethod\n\n==Origin==\n[[Christopher Alexander]], an architect and author, coined the term pattern language.<ref>{{Cite book | publisher = [[Oxford University Press]], USA | isbn = 0-19-501919-9 | last = Alexander | first = Christopher | title = A Pattern Language: Towns, Buildings, Construction | year = 1977 | page = 1216}}</ref> He used it to refer to common problems of the [[design]] and [[construction]] of buildings and towns and how they should be solved. The solutions proposed in the book include suggestions ranging from how cities and towns should be structured to where windows should be placed in a room.\n\nThe framework and philosophy of the "pattern language" approach was initially popularized in the book \'\'[[A Pattern Language]]\'\' that was written by Christopher Alexander and five colleagues at the Center for Environmental Structure in Berkeley, California in the late 1970s. While \'\'A Pattern Language\'\' contains 253 "patterns" from the first pattern, "Independent Regions" (the most general) to the last, "Things from Your Life", Alexander\'s book \'\'[[The Timeless Way of Building]]\'\' goes into more depth about the motivation and purpose of the work. The following definitions of "pattern" and "pattern language" are paraphrased from \'\'A Pattern Language\'\':\n\n"A \'\'pattern\'\' is a careful description of a perennial solution to a recurring problem within a building context, describing one of the configurations that brings life to a building.\n\nEach pattern describes a problem that occurs over and over again in our environment, and then describes the core solution to that problem, in such a way that you can use the solution a million times over, without ever doing it the same way twice."\n\nA \'\'pattern language\'\' is a network of patterns that call upon one another. Patterns help us remember insights and knowledge about design and can be used in combination to create solutions.\n\n== Application domains ==\nChristopher Alexander\'s idea has been adopted in other disciplines, often much more heavily than the original [[Pattern (architecture)|application of patterns to architecture]] as depicted the book \'\'[[A Pattern Language]]\'\'. Recent examples include [[software design pattern]]s in software engineering and, more generally, [[Architectural pattern (computer science)|architectural patterns in computer science]], as well as [[interaction design pattern]]s. [[Pedagogical patterns]] are used to document good practices in teaching. The book \'\'Liberating Voices: A Pattern Language for Communication Revolution\'\', containing [http://www.publicsphereproject.org/patterns/lv 136 patterns] for using information and communication to promote sustainability, democracy and positive social change, was published in 2008. The deck "Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings" was published in 2011. Recently, patterns were also introduced into [[systems architecture]] design.<ref>{{cite web|last=Hein|first=Andreas|title=Adopting Patterns for Space Mission and Space Systems Architecting|url=http://www.academia.edu/2110976/A.M._Hein_Adopting_Patterns_for_Space_Mission_and._Space_Systems_Architecting_|work=5 th International Workshop on System & Concurrent Engineering for Space ApplicationsSECESA 2012|accessdate=2 March 2013}}</ref><ref>{{cite web|last=Hein|first=Andreas|title=Project Icarus: Stakeholder Scenarios for an Interstellar Exploration Program|url=http://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM|work=Journal of the British Interplanetary Society, 64, 224-233, 2011|accessdate=2 March 2013}}</ref><ref>{{cite web|last=Cloutier|first=Robert|title=The Concept of Reference Architectures|url=http://www.calimar.com/TheConceptOfReferenceArchitectures.pdf|work=Systems Engineering Vol. 13, No. 1, 2010|accessdate=2 March 2013}}</ref>  [[Chess]] [[Chess strategy|strategy]] and [[Chess tactics|tactics]] involve many patterns from [[Chess opening|opening]] to [[checkmate]].\n\n==See also==\n* [[Feng shui]]\n* [[Method engineering]]\n* [[Rule of thumb]]\n* [[Typology (urban planning and architecture)]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* Christopher Alexander, Sara Ishikawa, Murray Silverstein (1974). \'A Collection of Patterns which Generate Multi-Service Centres\' in Declan and Margrit Kennedy (eds.): \'\'The Inner City.\'\' Architects Year Book 14, Elek, London. ISBN 0 236 15431 1.\n* Alexander, C. (1977). \'\'[[A Pattern Language: Towns, Buildings, Construction]]\'\'. USA: [[Oxford University Press]]. ISBN 978-0-19-501919-3.\n* Alexander, C. (1979). \'\'The Timeless Way of Building\'\'. USA: Oxford University Press. ISBN 978-0-19-502402-9.\n* Schuler, D. (2008). \'\'Liberating Voices: A Pattern Language for Communication Revolution\'\'. USA: [[MIT Press]]. ISBN 978-0-262-69366-0.\n* Leitner, Helmut (2015): \'\'Pattern Theory: Introduction and Perspectives on the Tracks of Christopher Alexander\'\'. ISBN 1505637430.\n\n==External links==\n\n===About patterns in general===\n* [http://www.c2.com/cgi/wiki?TipsForWritingPatternLanguages Tips For Writing Pattern Languages], by [[Ward Cunningham]]\n* [http://www.gardenvisit.com/landscape/architecture/3.1-patternlanguage.htm Essay on the pattern language as it relates to urban design]\n* [http://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM Use of patterns for scenario development for large scale aerospace projects]\n* [http://torgronsund.wordpress.com/2010/01/06/lean-startup-business-model-pattern/ Lean Startup Business Model Pattern]\n* [http://www.informit.com/articles/printerfriendly.aspx?p=30084 What Is a Quality Use Case?] from the book \'\'Patterns for Effective Use Cases\'\'\n* [http://groupworksdeck.org/what-we-mean-by-pattern Characteristics of group facilitation patterns]\n\n===Online pattern collections===\n* [http://www.patternlanguage.com/ patternlanguage.com], by the Center for Environmental Structure\n* [http://www.fusedgrid.ca/ Fused Grid] – A Contemporary Urban Pattern "a collection and synthesis of neighbourhood patterns"\n* [http://www.reliableprosperity.net ReliableProsperity.net] – Patterns for building a "restorative, socially just, and reliably prosperous society"\n* [http://www.hcipatterns.org/ hcipatterns.org] – Patterns for HCI\n* [http://www.c2.com/cgi/wiki?PatternIndex The Portland Pattern Repository]\n* [http://developer.yahoo.com/ypatterns Yahoo! Design Pattern Library]\n* [http://groupworksdeck.org Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings] – A pattern language of group process\n* [http://liveingreatness.com/core-protocols/ The Core Protocols] – A set of team communication patterns\n* [http://www.publicsphereproject.org/patterns/lv Liberating Voices! Pattern Language Project] — Short versions of patterns available in [http://www.publicsphereproject.org/patterns_arabic Arabic], [http://www.publicsphereproject.org/patterns_chinese Chinese], and [http://www.publicsphereproject.org/patterns_spanish Spanish]\n\n{{DEFAULTSORT:Pattern Language}}\n\n[[Category:Architectural theory]]\n[[Category:Cybernetics]]\n[[Category:Design]]\n[[Category:Knowledge representation]]\n[[Category:Linguistics]]\n\n[[fi:Suunnittelumalli]]']
['Semantic triple', '51445651', 'A \'\'\'semantic triple\'\'\', or simply \'\'\'triple\'\'\', is the atomic data entity in the [[Resource Description Framework]] (RDF) data model.<ref>http://www.w3.org/TR/PR-rdf-syntax/ "Resource Description Framework (RDF) Model and Syntax Specification"</ref> As its name indicates, a triple is a [[tuple|set of three entities]] that codifies a [[statement (programming)|statement]] about [[Data model|semantic data]] in the form of subject–predicate–object expressions (e.g. "Bob is 35", or "Bob knows John").\n\nThis format enables [[Knowledge representation and reasoning|knowledge to be represented]] in a machine-readable way. Particularly, every part of an RDF triple is individually addressable via unique [[Uniform Resource Identifier|URIs]] — for example, the second statement above might be represented in RDF as <code><nowiki>http://example.name#BobSmith12 http://xmlns.com/foaf/0.1/knows http://example.name#JohnDoe34</nowiki></code>.\nGiven this precise representation, semantic data can be unambiguously [[Semantic query|queried]] and [[Semantic reasoner|reasoned]] about.\n\nThe components of a triple, such as the statement "The sky has the color blue", consist of a [[Subject (grammar)|subject]] ("the sky"), a [[Predicate (grammar)|predicate]] ("has the color"), and an [[Object (grammar)|object]] ("blue"). This is similar to the classical notation of an [[entity–attribute–value model]] within [[object-oriented design]], where this example would be expressed as an entity (sky), an attribute (color) and a value (blue). From this basic structure, triples can be composed into [[Semantic network|more complex models]], by using triples as objects or subjects of other triples — for example, <code>Mike → said → (triples → can be → objects)</code>.\n\nGiven their particular, consistent structure, a collection of triples is often stored in purpose-built databases called [[Triplestore]]s.\n\n== See also ==\n* [[Named graph#Named graphs and quads]], an extension to semantic triples to also include a context node as a fourth element.\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{cite web |url = https://www.w3.org/TR/rdf11-primer/#section-triple |title = RDF 1.1 Primer § Triples |publisher = [[World Wide Web Consortium|W3C]] }}\n\n{{Semantic Web}}\n\n[[Category:Semantic Web]]\n[[Category:Data modeling]]\n[[Category:Resource Description Framework]]\n[[Category:Knowledge representation]]']
['Plinian Core', '49236889', '{{Multiple issues|\n{{Orphan|date=June 2016}}\n{{primary sources|date=January 2016}}\n{{Underlinked|date=October 2016}}\n}}\n\n\'\'\'Plinian Core\'\'\' is a set of vocabulary terms that can be used to describe different aspects of [[biodiversity informatics|biological species information]]. Under "biological species Information" all kinds of properties or traits related to taxa—biological and non-biological—are included. Thus, for instance, terms pertaining descriptions,  legal aspects, conservation, management, demographics, nomenclature, or related resources are incorporated.\n\n== Description ==\n\nThe \'\'\'Plinian Core\'\'\' is aimed to facilitate the exchange of information about the species and upper taxa.\n\t\nWhat is in scope?\n* \tSpecies level catalogs of any kind of biological objects or data. \n* \tTerminology associated with biological collection data. \n* \tStriving for compatibility with other biodiversity-related standards. \n* \tFacilitating the addition of components and attributes of biological data. \n\tWhat is not in scope? \n* \tData interchange protocols. \n* \tNon-biodiversity-related data. \n* \tOccurrence level data. \n\tThis standard is named after Pliny the Elder, a very influential figure in the study of the biological species.\n\t\n\'\'\'Plinian Core\'\'\' design requirements includes: ease of use, to be self-contained, able to support data integration from multiple databases, and ability to handle different levels of granularity. Core terms can be grouped in its current version as follows:\n* \t[[Metadata]]\n* \tBase Elements\n* \tRecord Metadata\n* \t[[Nomenclature#Nomenclature.2C classification and identification|Nomenclature and Classification]]\n* \t[[Taxonomic description]]\n* \t[[Natural history]]\n* \t[[Invasive species]]\n* \t[[Habitat|Habitat and Distribution]]\n* \t[[Demography]] and Threats\n* \tUses, Management and Conservation\n* \tassociatedParty, MeasurementOrFact, References, AncillaryData\n\n== Background ==\nPlinian Core started as a collaborative project between [[Instituto Nacional de Biodiversidad]] and [[GBIF]] Spain in 2005. A series of iterations in which elements were defined and implanted in different projects resulted in a "Plinian Core Flat" [deprecated].\n\nAs a result, a new development was impulse to overcome them in 2012. New formal requirements, additional input and a will to better support the standard and its documentation, as well as to align it with the processes of [[TDWG]], the world reference body for biodiversity information standards.\n\nA new version, \'\'\'Plinian Core v3.1\'\'\' was defined. This provides more flexibility to fully represent the information of a species in a variety of scenarios. New elements to deal with aspects such as IPR, related resources, referenced, etc. were introduced, and elements already included were better-defined and documented.\n\nPartner for the development of Plinian Core in this new phase incorporated the [[University of Granada]] (UG, Spain), the [[Alexander von Humboldt Institute]] (IAvH, Colombia), the [[National Commission for the Knowledge and Use of Biodiversity]] (Conabio, Mexico) and the [[University of São Paulo]] (USP, Brazil).\n\nA "Plinian Core Task Group" within TDWG "Interest Group on species Information"<ref>{{Cite web|title = TDWG: (TDWG) Species Information Interest Group - Charter|url = http://www.tdwg.org/activities/species-information/charter/|website = www.tdwg.org|access-date = 2016-01-29}}</ref> in being proposed.\n\n== Levels of the standard ==\nPlinian Core is presented in to levels: the \'\'\'abstract model\'\'\' and the \'\'\'application profiles\'\'\'.\n\nThe abstract model (AM), comprising the abstract model schema(xsd)  and the terms\' URIs, is the normative part. It is all comprehensive, and allows for different levels of granularity in describing species properties. The AM should be taken as a "menu" from which to choose terms and level of detail needed in any specific project.\n\nThe subsets of the abstract model intended to be implemented in specific projects are the "application profiles" (APs). Besides containing part of the elements of the AM, APs can impose additional specifications on the included elements, such as controlled vocabularies.  Some exampes of APs in use follow:\n# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_CONABIO.xsd CONABIO]\n# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_INBIO.xsd INBIO]\n# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_GBIF_ES.xsd GBIF.ES]\n# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_MAGRAMA.xsd MAGRAMA]\n# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_SIB-COLOMBIA.xsd SIB-COLOMBIA]\n\n== Relation to other standards ==\nPlinian incorporates a number of elements already defined by other standards. The following table summarizes these standards and the elements used in Plinian Core:\n{| class="wikitable"\n!Standard\n!Elements\n|-\n|[[Darwin Core]]\n|taxonConceptID, Hierarchy, MeasurementOrFact, ResourceRelationShip.\n|-\n|[[Ecological Metadata Language]]\n|associatedParty, keywordSet, coverage, dataset\n|-\n|[[Encyclopedia of Life]] Schema\n|AncillaryData: DataObjectBase\n|-\n|[[Global Invasive Species Network]]\n|origin, presence, persistence, distribution, harmful, modified, startValidDate, endValidDate, countryCode, stateProvince, county, localityName, county, language, citation, abundance...\n|-\n|[http://www.tdwg.org/schemas/tcs/1.01 Taxon Concept Schema. TCS]\n|scientificName\n|}\n\n== External Links ==\n* [http://www.github.com/PlinianCore Main page]\n* [http://tools.gbif.org/dwca-validator/extensions.do An Implementation of Plinian Core as GBIF\'s IPT Extensions]\n* [https://github.com/PlinianCore/Documentation/wiki/PlinianCore_Terms Plinian Core Terms Quick Reference Guide]\n* [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/abstract%20models/stable%20version/PlinianCore_AbstractModel_v3.2.1.xsd Plinian Core Abstract Model (xsd). Current version (3.2.1)]\n* [http://www.tdwg.org/ Biodiversity Information Standards (TDWG)]\n* [http://www.ingurumena.ejgv.euskadi.eus/r49-u95/es/contenidos/informacion/naturaeuskadi/es_def/adjuntos/plinian.pdf Sistema de información de la naturaleza de euskadi. Aplicación del estandar Plinian Core]\n* [http://www.magrama.gob.es/es/biodiversidad/servicios/banco-datos-naturaleza/informacion-disponible/BDN_Modelos_Datos.aspx Estándar Plinian Core para la gestión integrada de la información sobre especies. Ministerio de Agricultura, Alimentación y Medio Ambiente de España]\n* [http://patrimonio.ambiente.gob.ec/bndv/modelo.php Modelo conceptual de la Base Nacional de Datos de Vegetación. Ministerio del Ambiente, Ecuador]\n\n== References ==\n<references />\n\n[[Category:Biodiversity informatics]]\n[[Category:Knowledge representation]]\n[[Category:Interoperability]]\n[[Category:Metadata standards]]']
