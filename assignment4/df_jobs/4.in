['Information retrieval', '15271', '{{Information science}}\n\n\'\'\'Information retrieval\'\'\' (\'\'\'IR\'\'\') is the activity of obtaining [[information]] resources relevant to an information need from a collection of information resources.  Searches can be based on [[Full text search|full-text]] or other content-based indexing.\n\nAutomated information retrieval systems are used to reduce what has been called "[[information overload]]". Many [[University|universities]] and [[public library|public libraries]] use IR systems to provide access to books, journals and other documents. [[Web search engine]]s are the most visible [[Information retrieval applications|IR applications]].\n\n== Overview ==\n\nAn information retrieval process begins when a user enters a [[query string|query]] into the system. Queries are formal statements of [[information need]]s, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of [[Relevance (information retrieval)|relevancy]].\n\nAn object is an entity that is represented by information in a content collection or [[database]]. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.<ref>Jansen, B. J. and Rieh, S. (2010) [https://faculty.ist.psu.edu/jjansen/academic/jansen_theoretical_constructs.pdf The Seventeen Theoretical Constructs of Information Searching and Information Retrieval]. Journal of the American Society for Information Sciences and Technology. 61(8), 1517-1534.</ref>\n\nDepending on the [[Information retrieval applications|application]] the data objects may be, for example, text documents, images,<ref name=goodron2000>{{cite journal |first=Abby A. |last=Goodrum |title=Image Information Retrieval: An Overview of Current Research |journal=Informing Science |volume=3 |number=2 |year=2000 }}</ref> audio,<ref name=Foote99>{{cite journal |first=Jonathan |last=Foote |title=An overview of audio information retrieval |journal=Multimedia Systems |year=1999 |publisher=Springer }}</ref> [[mind maps]]<ref name=Beel2009>{{cite conference|first=Jöran |last=Beel |first2=Bela |last2=Gipp |first3=Jan-Olaf |last3=Stiller |title=Information Retrieval On Mind Maps - What Could It Be Good For? |url=http://www.sciplore.org/publications_en.php |conference=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom\'09) |year=2009 |publisher=IEEE |place=Washington, DC }}</ref> or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or [[metadata]].\n\nMost IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.<ref name="Frakes1992">{{cite book |last=Frakes |first=William B. |title=Information Retrieval Data Structures & Algorithms |publisher=Prentice-Hall, Inc. |year=1992 |isbn=0-13-463837-9 |url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes }}</ref>\n\n== History ==\n{{Rquote|right|there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, ca be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute| J. E. Holmstrom, 1948}}\nThe idea of using computers to search for relevant pieces of information was popularized in the article \'\'[[As We May Think]]\'\' by [[Vannevar Bush]] in 1945.<ref name="Singhal2001">{{cite journal |last=Singhal |first=Amit |title=Modern Information Retrieval: A Brief Overview |journal=Bulletin of the IEEE Computer Society Technical Committee on Data Engineering|volume=24 |issue=4 |pages=35–43 |year =2001 |url=http://singhal.info/ieee2001.pdf }}</ref> It would appear that Bush was inspired by patents for a \'statistical machine\' - filed by [[Emanuel Goldberg]] in the 1920s and \'30s - that searched for documents stored on film.<ref name="Sanderson2012">{{cite journal |author=Mark Sanderson & W. Bruce Croft |title=The History of Information Retrieval Research |journal=Proceedings of the IEEE |volume=100 |pages=1444–1451 |year =2012 |url=http://dx.doi.org/10.1109/JPROC.2012.2189916 |doi=10.1109/jproc.2012.2189916}}</ref> The first description of a computer searching for information was described by Holmstrom in 1948,<ref name="Holmstrom1948">{{cite journal |author=JE Holmstrom |title=‘Section III. Opening Plenary Session |journal=The Royal Society Scientific Information Conference, 21 June-2 July 1948: report and papers submitted |pages=85|year =1948|url=https://books.google.com.au/books?ei=44VxVZrkGYqU8QX4wYPoBA&id=M34lAAAAMAAJ&dq=%E2%80%98Section+III.+Opening+Plenary+Session%22.+The+Royal+Society+Scientific+Information+Conference&focus=searchwithinvolume&q=univac}}</ref> detailing an early mention of the [[Univac]] computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, [[Desk Set]]. In the 1960s, the first large information retrieval research group was formed by [[Gerard Salton]] at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small [[text corpora]] such as the Cranfield collection (several thousand documents).<ref name="Singhal2001" /> Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\n\nIn 1992, the US Department of Defense along with the [[National Institute of Standards and Technology]] (NIST), cosponsored the [[Text Retrieval Conference]] (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that [[scalability|scale]] to huge corpora. The introduction of [[web search engine]]s has boosted the need for very large scale retrieval systems even further.\n\n== Model types ==\n[[File:Information-Retrieval-Models.png|thumb|500px|Categorization of IR-models (translated from [[:de:Informationsrückgewinnung#Klassifikation von Modellen zur Repräsentation natürlichsprachlicher Dokumente|German entry]], original source [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id= Dominik Kuropka]).]]\nFor effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\n\n=== First dimension: mathematical basis ===\n* \'\'Set-theoretic\'\' models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:\n** [[Standard Boolean model]]\n** [[Extended Boolean model]]\n** [[Fuzzy retrieval]]\n* \'\'Algebraic models\'\' represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.\n** [[Vector space model]]\n** [[Generalized vector space model]]\n** [[Topic-based vector space model|(Enhanced) Topic-based Vector Space Model]]\n** [[Extended Boolean model]]\n** [[Latent semantic indexing]] a.k.a. [[latent semantic analysis]]\n* \'\'Probabilistic models\'\' treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the [[Bayes\' theorem]] are often used in these models.\n** [[Binary Independence Model]]\n** [[Probabilistic relevance model]] on which is based the [[Probabilistic relevance model (BM25)|okapi (BM25)]] relevance function\n** [[Uncertain inference]]\n** [[Language model]]s\n** [[Divergence-from-randomness model]]\n** [[Latent Dirichlet allocation]]\n* \'\'Feature-based retrieval models\'\' view documents as vectors of values of \'\'feature functions\'\' (or just \'\'features\'\') and seek the best way to combine these features into a single relevance score, typically by [[learning to rank]] methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.\n\n=== Second dimension: properties of the model ===\n* \'\'Models without term-interdependencies\'\' treat different terms/words as independent. This fact is usually represented in vector space models by the [[orthogonality]] assumption of term vectors or in probabilistic models by an [[Independence (mathematical logic)|independency]] assumption for term variables.\n* \'\'Models with immanent term interdependencies\'\' allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by [[dimension reduction|dimensional reduction]]) from the [[co-occurrence]] of those terms in the whole set of documents.\n* \'\'Models with transcendent term interdependencies\'\' allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)\n\n== Performance and correctness measures ==\n{{further|Evaluation measures (information retrieval)}}\n\nThe \'\'\'evaluation of an information retrieval system\'\'\' is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for [[Standard Boolean model|Boolean retrieval]] or top-k retrieval, include [[precision and recall]].  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a [[ground truth]] notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be [[ill-posed]] and there may be different shades of relevancy.\n\nVirtually all modern evaluation metrics (e.g., [[Information retrieval#Mean average precision|mean average precision]], [[Information retrieval#Discounted cumulative gain|discounted cumulative gain]]) are designed for \'\'\'ranked retrieval\'\'\' without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.{{citation needed|date=June 2015}}\n\nThe mathematical symbols used in the formulas below mean:\n* <math>X \\cap Y</math> - [[Intersection (set theory)|Intersection]] - in this case, specifying the documents in \'\'both\'\' sets X and Y\n* <math>| X |</math> - [[Cardinality]] - in this case, the number of documents in set X\n* <math>\\int</math> - [[Integral]]\n* <math>\\sum</math> - [[Summation]]\n* <math>\\Delta</math> - [[Symmetric difference]]\n\n=== Precision ===\n{{main|Precision and recall}}\n\nPrecision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user\'s information need.\n\n:<math> \\mbox{precision}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{retrieved documents}\\}|} </math>\n\nIn [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called \'\'precision at n\'\' or \'\'P@n\'\'.\n\nNote that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].\n\n=== Recall ===\n{{main|Precision and recall}}\n\nRecall is the fraction of the documents that are relevant to the query that are successfully retrieved.\n\n:<math>\\mbox{recall}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{relevant documents}\\}|} </math>\n\nIn binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as \'\'the probability that a relevant document is retrieved by the query\'\'.\n\nIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\n\n=== Fall-out ===\nThe proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:\n\n:<math> \\mbox{fall-out}=\\frac{|\\{\\mbox{non-relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{non-relevant documents}\\}|} </math>\n\nIn binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to <math>(1-\\mbox{specificity})</math>. It can be looked at as \'\'the probability that a non-relevant document is retrieved by the query\'\'.\n\nIt is trivial to achieve fall-out of 0% by returning zero documents in response to any query.\n\n=== F-score / F-measure ===\n{{main|F-score}}\nThe weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:\n\n:<math>F = \\frac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{(\\mathrm{precision} + \\mathrm{recall})}</math>\n\nThis is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.\n\nThe general formula for non-negative real <math>\\beta</math> is:\n:<math>F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\mathrm{precision} \\cdot \\mathrm{recall})}{(\\beta^2 \\cdot \\mathrm{precision} + \\mathrm{recall})}\\,</math>\n\nTwo other commonly used F measures are the <math>F_{2}</math> measure, which weights recall twice as much as precision, and the <math>F_{0.5}</math> measure, which weights precision twice as much as recall.\n\nThe F-measure was derived by van Rijsbergen (1979) so that <math>F_\\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen\'s effectiveness measure <math>E = 1 - \\frac{1}{\\frac{\\alpha}{P} + \\frac{1-\\alpha}{R}}</math>.  Their relationship is:\n:<math>F_\\beta = 1 - E</math> where <math>\\alpha=\\frac{1}{1 + \\beta^2}</math>\n\nF-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.{{citation needed|date=June 2015}}\n\n=== Average precision ===\n<!-- [[Average precision]] redirects here -->\nPrecision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision <math>p(r)</math> as a function of recall <math>r</math>. Average precision computes the average value of <math>p(r)</math> over the interval from <math>r=0</math> to <math>r=1</math>:<ref name="zhu2004">{{cite journal |first=Mu |last=Zhu |title=Recall, Precision and Average Precision |url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}</ref>\n:<math>\\operatorname{AveP} = \\int_0^1 p(r)dr</math>\nThat is the area under the precision-recall curve.\nThis integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:\n:<math>\\operatorname{AveP} = \\sum_{k=1}^n P(k) \\Delta r(k)</math>\nwhere <math>k</math> is the rank in the sequence of retrieved documents, <math>n</math> is the number of retrieved documents, <math>P(k)</math> is the precision at cut-off <math>k</math> in the list, and <math>\\Delta r(k)</math> is the change in recall from items <math>k-1</math> to <math>k</math>.<ref name="zhu2004" />\n\nThis finite sum is equivalent to:\n:<math> \\operatorname{AveP} = \\frac{\\sum_{k=1}^n (P(k) \\times \\operatorname{rel}(k))}{\\mbox{number of relevant documents}} \\!</math>\nwhere <math>\\operatorname{rel}(k)</math> is an indicator function equaling 1 if the item at rank <math>k</math> is a relevant document, zero otherwise.<ref name="Turpin2006">{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06–11, 2006) |publisher=ACM |location=New York, NY |pages=11–18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}</ref> Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.\n\nSome authors choose to interpolate the <math>p(r)</math> function to reduce the impact of "wiggles" in the curve.<ref name=voc2010>{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303–338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}</ref><ref name="nlpbook">{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}</ref> For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:<ref name="voc2010" /><ref name="nlpbook" />\n:<math>\\operatorname{AveP} = \\frac{1}{11} \\sum_{r \\in \\{0, 0.1, \\ldots, 1.0\\}} p_{\\operatorname{interp}}(r)</math>\nwhere <math>p_{\\operatorname{interp}}(r)</math> is an interpolated precision that takes the maximum precision over all recalls greater than <math>r</math>:\n:<math>p_{\\operatorname{interp}}(r) = \\operatorname{max}_{\\tilde{r}:\\tilde{r} \\geq r} p(\\tilde{r})</math>.\n\nAn alternative is to derive an analytical <math>p(r)</math> function by assuming a particular parametric distribution for the underlying decision values. For example, a \'\'binormal precision-recall curve\'\' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.<ref>K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves]. \'\'Proceedings of the 20th International Conference on Pattern Recognition\'\', 4263-4266.</ref>\n\n=== Precision at K ===\n\nFor modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 or "Precision at 10" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.{{citation needed|date=June 2015}}  Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.<ref name="stanford" />  It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.\n\n=== R-Precision ===\n\nR-precision requires knowing all documents that are relevant to a query.  The number of relevant documents, <math>R</math>, is used as the cutoff for calculation, and this varies from query to query.  For example, if there are 15 documents relevant to "red" in a corpus (R=15), R-precision for "red" looks at the top 15 documents returned, counts the number that are relevant <math>r</math> turns that into a relevancy fraction: <math>r/R = r/15</math>.<ref name="trec15"/>\n\nPrecision is equal to recall at the \'\'\'R\'\'\'-th position.<ref name="stanford">{{cite web|url=http://nlp.stanford.edu/IR-book/pdf/08eval.pdf|title=Chapter 8: Evaluation in information retrieval|accessdate=2015-06-14|date=2009|authors=Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze}}  Part of \'\'Introduction to Information Retrieval\'\' [http://nlp.stanford.edu/IR-book/]</ref>\n\nEmpirically, this measure is often highly correlated to mean average precision.<ref name="stanford" />\n\n=== Mean average precision ===\n<!-- [[Mean average precision]] redirects here -->\nMean average precision for a set of queries is the mean of the average precision scores for each query.\n:<math> \\operatorname{MAP} = \\frac{\\sum_{q=1}^Q \\operatorname{AveP(q)}}{Q} \\!</math>\nwhere \'\'Q\'\' is the number of queries.\n\n=== Discounted cumulative gain ===\n{{main|Discounted cumulative gain}}\nDCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\n\nThe DCG accumulated at a particular rank position <math>p</math> is defined as:\n\n:<math> \\mathrm{DCG_{p}} = rel_{1} + \\sum_{i=2}^{p} \\frac{rel_{i}}{\\log_{2}i}. </math>\n\nSince result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (<math>IDCG_p</math>), which normalizes the score:\n\n:<math> \\mathrm{nDCG_{p}} = \\frac{DCG_{p}}{IDCG{p}}. </math>\n\nThe nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.\n\n=== Other measures ===\n{{Confusion matrix terms}}\n* [[Mean reciprocal rank]]\n* [[Spearman\'s rank correlation coefficient]]\n* bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents<ref name="trec15">http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf</ref>\n* GMAP - geometric mean of (per-topic) average precision<ref name="trec15" />\n* Measures based on marginal relevance and document diversity - see {{section link|Relevance (information retrieval)|Problems and alternatives}}\n\n===Visualization===\n\nVisualizations of information retrieval performance include:\n* Graphs which chart precision on one axis and recall on the other<ref name="trec15" />\n* Histograms of average precision over various topics<ref name="trec15" />\n* [[Receiver operating characteristic]] (ROC curve)\n* [[Confusion matrix]]\n\n== Timeline ==\n\n* Before the \'\'\'1900s\'\'\'\n*: \'\'\'1801\'\'\': [[Joseph Marie Jacquard]] invents the [[Jacquard loom]], the first machine to use punched cards to control a sequence of operations.\n*: \'\'\'1880s\'\'\': [[Herman Hollerith]] invents an electro-mechanical data tabulator using punch cards as a machine readable medium.\n*: \'\'\'1890\'\'\' Hollerith [[Punched cards|cards]], [[keypunch]]es and [[Tabulating machine|tabulators]] used to process the [[1890 US Census]] data.\n* \'\'\'1920s-1930s\'\'\'\n*: [[Emanuel Goldberg]] submits patents for his "Statistical Machine” a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.\n* \'\'\'1940s–1950s\'\'\'\n*: \'\'\'late 1940s\'\'\': The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.\n*:: \'\'\'1945\'\'\': [[Vannevar Bush]]\'s \'\'[[As We May Think]]\'\' appeared in \'\'[[Atlantic Monthly]]\'\'.\n*:: \'\'\'1947\'\'\': [[Hans Peter Luhn]] (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.\n*: \'\'\'1950s\'\'\': Growing concern in the US for a "science gap" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems ([[Allen Kent]] \'\'et al.\'\') and the invention of citation indexing ([[Eugene Garfield]]).\n*: \'\'\'1950\'\'\': The term "information retrieval" was coined by [[Calvin Mooers]].<ref>Mooers, Calvin N.; \'\'[https://babel.hathitrust.org/cgi/pt?id=mdp.39015034570591;view=1up;seq=3 The Theory of Digital Handling of Non-numerical Information and its Implications to Machine Economics]\'\' (Zator Technical Bulletin No. 48), cited in {{cite journal|last1=Fairthorne|first1=R. A.|title=Automatic Retrieval of Recorded Information|journal=The Computer Journal|date=1958|volume=1|issue=1|page=37|doi=10.1093/comjnl/1.1.36|url=http://comjnl.oxfordjournals.org/content/1/1/36.short}}</ref>\n*: \'\'\'1951\'\'\': Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at [[MIT]].<ref name="Doyle1975">{{cite book |last=Doyle |first=Lauren |last2=Becker |first2=Joseph |title=Information Retrieval and Processing |publisher=Melville |year=1975 |pages=410 pp. |isbn=0-471-22151-1 }}</ref>\n*: \'\'\'1955\'\'\': Allen Kent joined [[Case Western Reserve University]], and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed "framework" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.<ref>{{cite journal |title=Machine literature searching X. Machine language; factors underlying its design and development |doi=10.1002/asi.5090060411}}</ref>\n*: \'\'\'1958\'\'\': International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: \'\'Proceedings of the International Conference on Scientific Information, 1958\'\' (National Academy of Sciences, Washington, DC, 1959)\n*: \'\'\'1959\'\'\': [[Hans Peter Luhn]] published "Auto-encoding of documents for information retrieval."\n* \'\'\'1960s\'\'\':\n*: \'\'\'early 1960s\'\'\': [[Gerard Salton]] began work on IR at Harvard, later moved to Cornell.\n*: \'\'\'1960\'\'\': [[Melvin Earl Maron]] and John Lary<!-- sic --> Kuhns<ref name="Maron2008">{{cite journal |title=An Historical Note on the Origins of Probabilistic Indexing |last=Maron | first=Melvin E. |journal=Information Processing and Management |volume=44 |year=2008 |pages=971–972 |url=http://yunus.hacettepe.edu.tr/~tonta/courses/spring2008/bby703/maron-on-probabilistic%20indexing-2008.pdf |doi=10.1016/j.ipm.2007.02.012 |issue=2 }}</ref> published "On relevance, probabilistic indexing, and information retrieval" in the Journal of the ACM 7(3):216–244, July 1960.\n*: \'\'\'1962\'\'\':\n*:* [[Cyril W. Cleverdon]] published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, "Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems". Cranfield Collection of Aeronautics, Cranfield, England, 1962.\n*:* Kent published \'\'Information Analysis and Retrieval\'\'.\n*: \'\'\'1963\'\'\':\n*:* Weinberg report "Science, Government and Information" gave a full articulation of the idea of a "crisis of scientific information."  The report was named after Dr. [[Alvin Weinberg]].\n*:* Joseph Becker and [[Robert M. Hayes]] published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. \'\'Information storage and retrieval: tools, elements, theories\'\'. New York, Wiley (1963).\n*: \'\'\'1964\'\'\':\n*:* [[Karen Spärck Jones]] finished her thesis at Cambridge, \'\'Synonymy and Semantic Classification\'\', and continued work on [[computational linguistics]] as it applies to IR.\n*:* The [[National Bureau of Standards]] sponsored a symposium titled "Statistical Association Methods for Mechanized Documentation." Several highly significant papers, including G. Salton\'s first published reference (we believe) to the [[SMART Information Retrieval System|SMART]] system.\n*:\'\'\'mid-1960s\'\'\':\n*::* National Library of Medicine developed [[MEDLARS]] Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.\n*::* Project Intrex at MIT.\n*:: \'\'\'1965\'\'\': [[J. C. R. Licklider]] published \'\'Libraries of the Future\'\'.\n*:: \'\'\'1966\'\'\': [[Don Swanson]] was involved in studies at University of Chicago on Requirements for Future Catalogs.\n*: \'\'\'late 1960s\'\'\': [[F. Wilfrid Lancaster]] completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.\n*:: \'\'\'1968\'\'\':\n*:* Gerard Salton published \'\'Automatic Information Organization and Retrieval\'\'.\n*:* John W. Sammon, Jr.\'s RADC Tech report "Some Mathematics of Information Storage and Retrieval..." outlined the vector model.\n*:: \'\'\'1969\'\'\': Sammon\'s "A nonlinear mapping for data structure analysis" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.\n* \'\'\'1970s\'\'\'\n*: \'\'\'early 1970s\'\'\':\n*::* First online systems—NLM\'s AIM-TWX, MEDLINE; Lockheed\'s Dialog; SDC\'s ORBIT.\n*::* [[Theodor Nelson]] promoting concept of [[hypertext]], published \'\'Computer Lib/Dream Machines\'\'.\n*: \'\'\'1971\'\'\': [[Nicholas Jardine]] and [[Cornelis J. van Rijsbergen]] published "The use of hierarchic clustering in information retrieval", which articulated the "cluster hypothesis."<ref>{{cite journal|author=N. Jardine, C.J. van Rijsbergen|title=The use of hierarchic clustering in information retrieval|journal=Information Storage and Retrieval|volume=7|issue=5|pages=217–240|date=December 1971|doi=10.1016/0020-0271(71)90051-9}}</ref>\n*: \'\'\'1975\'\'\': Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:\n*::* \'\'A Theory of Indexing\'\' (Society for Industrial and Applied Mathematics)\n*::* \'\'A Theory of Term Importance in Automatic Text Analysis\'\' ([[JASIS]] v. 26)\n*::* \'\'A Vector Space Model for Automatic Indexing\'\' ([[Communications of the ACM|CACM]] 18:11)\n*: \'\'\'1978\'\'\': The First [[Association for Computing Machinery|ACM]] [[Special Interest Group on Information Retrieval|SIGIR]] conference.\n*: \'\'\'1979\'\'\': C. J. van Rijsbergen published \'\'Information Retrieval\'\' (Butterworths). Heavy emphasis on probabilistic models.\n*: \'\'\'1979\'\'\': Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.<ref>Doszkocs, T.E. & Rapp, B.A. (1979). "Searching MEDLINE in English: a Prototype User Inter-face with Natural Language Query, Ranked Output, and relevance feedback," In: Proceedings of the ASIS Annual Meeting, 16: 131-139.</ref>\n* \'\'\'1980s\'\'\'\n*: \'\'\'1980\'\'\': First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.\n*: \'\'\'1982\'\'\': [[Nicholas J. Belkin]], Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.\n*: \'\'\'1983\'\'\': Salton (and Michael J. McGill) published \'\'Introduction to Modern Information Retrieval\'\' (McGraw-Hill), with heavy emphasis on vector models.\n*: \'\'\'1985\'\'\': David Blair and [[Bill Maron]] publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System\n*: \'\'\'mid-1980s\'\'\': Efforts to develop end-user versions of commercial IR systems.\n*:: \'\'\'1985–1993\'\'\': Key papers on and experimental systems for visualization interfaces.\n*:: Work by [[Donald B. Crouch]], [[Robert R. Korfhage]], Matthew Chalmers, Anselm Spoerri and others.\n*: \'\'\'1989\'\'\': First [[World Wide Web]] proposals by [[Tim Berners-Lee]] at [[CERN]].\n* \'\'\'1990s\'\'\'\n*: \'\'\'1992\'\'\': First [[Text Retrieval Conference|TREC]] conference.\n*: \'\'\'1997\'\'\': Publication of [[Robert R. Korfhage|Korfhage]]\'s \'\'Information Storage and Retrieval\'\'<ref name="Korfhage1997">{{cite book |last=Korfhage |first=Robert R. |title=Information Storage and Retrieval |publisher=Wiley |year=1997 |pages=368 pp. |isbn=978-0-471-14338-3 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471143383,descCd-authorInfo.html }}</ref> with emphasis on visualization and multi-reference point systems.\n*: \'\'\'late 1990s\'\'\': [[Web search engine]]s implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.\n\n== Awards in the field ==\n\n* [[Tony Kent Strix award]]\n* [[Gerard Salton Award]]\n\n== Leading IR Research Groups ==\n* [[Center for Intelligent Information Retrieval]] (CIIR) at the University of Massachusetts Amherst <ref>{{Cite web|url=http://ciir.cs.umass.edu|title=Center for Intelligent Information Retrieval {{!}} UMass Amherst|website=ciir.cs.umass.edu|access-date=2016-07-29}}</ref>\n* Information Retrieval Group at the University of Glasgow <ref>{{Cite web|url=http://www.gla.ac.uk/schools/computing/research/researchoverview/informationretrieval/|title=University of Glasgow - Schools - School of Computing Science - Research - Research overview - Information Retrieval|website=www.gla.ac.uk|access-date=2016-07-29}}</ref>\n* Information and Language Processing Systems (ILPS) at the University of Amsterdam <ref>{{Cite web|url=http://ilps.science.uva.nl/|title=ILPS - information and language processing systems|website=ILPS|language=en-US|access-date=2016-07-29}}</ref>\n* Language Technologies Institutes (LTI) at the Carnegie Mellon University\n* Text Information Management and Analysis Group (TIMAN) at  the University of Illinois at Urbana-Champaign\n\n==See also==\n\n{{div col}}\n\n* [[Adversarial information retrieval]]\n* [[Collaborative information seeking]]\n* [[Controlled vocabulary]]\n* [[Cross-language information retrieval]]\n* [[Data mining]]\n* [[European Summer School in Information Retrieval]]\n* [[Human–computer information retrieval]] (HCIR)\n* [[Information extraction]]\n* [[Information Retrieval Facility]]\n* [[Knowledge visualization]]\n* [[Multimedia information retrieval]]\n* [[Personal information management]]\n* [[Relevance (Information Retrieval)]]\n* [[Relevance feedback]]\n* [[Rocchio Classification]]\n* [[Index (search engine)|Search index]]\n* [[Social information seeking]]\n* [[Special Interest Group on Information Retrieval]]\n* [[Subject indexing]]\n* [[Temporal information retrieval]]\n* [[tf-idf]]\n* [[XML-Retrieval]]\n\n{{div col end}}\n\n== References ==\n{{reflist}}\n\n==Further reading==\n* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&uuml;tze. [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval]. Cambridge University Press, 2008.\n*Stefan B&uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\n\n==External links==\n{{wikiquote}}\n* [http://www.acm.org/sigir/ ACM SIGIR: Information Retrieval Special Interest Group]\n* [http://irsg.bcs.org/ BCS IRSG: British Computer Society - Information Retrieval Specialist Group]\n* [http://trec.nist.gov Text Retrieval Conference (TREC)]\n* [http://www.isical.ac.in/~fire Forum for Information Retrieval Evaluation (FIRE)]\n* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval] (online book) by [[C. J. van Rijsbergen]]\n* [http://ir.dcs.gla.ac.uk/wiki/ Information Retrieval Wiki]\n* [http://ir-facility.org/ Information Retrieval Facility]\n* [http://www.nonrelevant.net Information Retrieval @ DUTH]\n* [http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf TREC report on information retrieval evaluation techniques]\n* [http://www.ebaytechblog.com/2010/11/10/measuring-search-relevance/ How eBay measures search relevance]\n* [http://retrieval.ceti.gr Information retrieval performance evaluation tool @ Athena Research Centre]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Information Retrieval}}\n[[Category:Articles with inconsistent citation formats]]\n[[Category:Information retrieval| ]]\n[[Category:Natural language processing]]']
['Category:Information retrieval systems', '46964839', '[[Category:Information retrieval]]']
['Dwell time (information retrieval)', '48317971', '{{one source|date=October 2015}}\n\nIn [[information retrieval]], \'\'\'dwell time\'\'\' denotes the time which a user spends viewing a document after clicking a link on a [[Search engine results page|search engine results page (SERP)]]. Dwell time is the duration between when a user clicks on a [[search engine]] result, and when the user returns from that result, or the user is otherwise seen to have left the result. Dwell time is a [[Relevance (information retrieval)|relevance]] indicator of the search result correctly satisfying the [[Information needs|intent]] of the user. Short dwell times indicate the user\'s query intent was not satisfied by viewing the result. Long dwell times indicate the user\'s query intent was satisfied.<ref>{{Cite web|url=https://blogs.bing.com/webmaster/2011/08/02/how-to-build-quality-content/|title=How To Build Quality Content|publisher=Bing blogs}}</ref> \n\n==References==\n<references />2. [https://www.impression.co.uk/blog/4004/dwell-time/ "Understanding dwell time and its impact on search rankings"] Impression Digital\n[[Category:Information retrieval]]\n[[Category:Information retrieval evaluation]]\n[[Category:Internet search engines]]\n\n{{web-software-stub}}']
['Thomas write rule', '217343', "In [[computer science]], particularly the field of [[database]]s, the '''Thomas write rule''' is a rule in [[timestamp-based concurrency control]].  It can be summarized as ''ignore outdated writes''.\n\nIt states that, if a more recent transaction has already written the value of an object, then a less recent transaction does not need perform its own write since it will eventually be overwritten by the more recent one.  \n\nThe Thomas write rule is applied in situations where a predefined '''logical''' order is assigned to transactions when they start.  For example a transaction might be assigned a monotonically increasing timestamp when it is created.  The rule prevents changes in the order in which the transactions are executed from creating different outputs: The outputs will always be consistent with the predefined logical order.\n\nFor example consider a database with 3 variables (A, B, C), and two atomic operations C := A (T1), and C := B (T2).  Each transaction involves a read (A or B), and a write (C).  The only conflict between these transactions is the write on C.  The following is one possible schedule for the operations of these transactions:\n\n:<math>\\begin{bmatrix}\nT_1 & T_2 \\\\\n& Read(A) \\\\\nRead(B) &   \\\\\n &Write(C)   \\\\\nWrite(C) &  \\\\\nCommit & \\\\\n& Commit \\end{bmatrix} \\Longleftrightarrow\n\\begin{bmatrix}\nT_1 & T_2 \\\\\n& Read(A) \\\\\nRead(B) & \\\\\n& Write(C) \\\\\n & \\\\\nCommit & \\\\\n& Commit\\\\\n\\end{bmatrix}\n</math>\n\nIf (when the transactions are created) T1 is assigned a timestamp that precedes T2 (i.e., according to the logical order, T1 comes first), then only T2's write should be visible.  If, however, T1's write is executed after T2's write, then we need a way to detect this and discard the write.\n\nOne practical approach to this is to label each value with a write timestamp (WTS) that indicates the timestamp of the last transaction to modify the value.  Enforcing the Thomas write rule only requires checking to see if the write timestamp of the object is greater than the time stamp of the transaction performing a write.  If so, the write is discarded   \n\nIn the example above, if we call TS(T) the timestamp of transaction T, and WTS(O) the write timestamp of object O, then T2's write sets WTS(C) to TS(T2).  When T1 tries to write C, it sees that TS(T1) < WTS(C), and discards the write.  If a third transaction T3 (with TS(T3) > TS(T2)) were to then write to C, it would get TS(T3) > WTS(C), and the write would be allowed.\n\n==References==\n*{{Cite journal | author=Robert H. Thomas | title=A majority consensus approach to concurrency control for multiple copy databases | journal=ACM Transactions on Database Systems | year=1979 | volume=4 | issue=2 | pages= 180–209 | doi=10.1145/320071.320076 }}\n\n[[Category:Data management]]\n[[Category:Transaction processing]]\n\n\n{{compu-sci-stub}}"]
['Category:Computer file systems', '754856', '{{catdiffuse}}\n{{Cat main|File system}}\n{{Commonscat|File systems}}\nA \'\'\'[[file system]]\'\'\' in computing, is a method for storing and organizing [[computer file]]s and the data they contain to make it easy to find and access them. File systems may use a [[data storage device]] such as a [[hard disk]] or [[CD-ROM]] and involve maintaining the physical location of the files, or they may be virtual and exist only as an access method for virtual data or for data over a network (e.g. [[Network File System (protocol)|NFS]]).\n\nMore formally, a file system is a set of [[abstract data type]]s that are implemented for the storage, hierarchical organization, manipulation, navigation, access, and retrieval of [[data]].\n\n== See also ==\n* [[:Category:Computer storage]]\n\n[[Category:Data management]]\n[[Category:Operating system technology|File systems]]\n<!--[[Category:Computer systems|File systems]] deleted  "Computer systems" not an index of systems -->\n[[Category:Storage software]]']
['QuickPar', '1756767', '{{Merge to |Parchive |date=March 2014}}\n\n{{Infobox software\n| logo                   = \n| screenshot             = [[File:QuickPar Screenshot.png|250px]]\n| caption                = QuickPar 0.9 checking a series of [[RAR (file format)|RAR]] files for integrity.\n| collapsible            = \n| author                 = \n| developer              = Peter Clements\n| released               = 0.1, (February 5, 2003)<ref>{{cite web|url=http://www.quickpar.org.uk/ReleaseNotes2.htm|title=QuickPar - Old Release Notes|accessdate=2010-11-19}}</ref>\n| latest release version = 0.9.1\n| latest release date    = {{Start date and age|2004|07|04}}<ref>{{cite web |url=http://www.quickpar.org.uk/ |title=QuickPar for Windows |accessdate=2009-09-27}}</ref>\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD}} -->\n| frequently updated     = \n| programming language   = \n| operating system       = [[Microsoft Windows]]\n| platform               = [[x86]]\n| size                   = \n| language               = \n| status                 = \n| genre                  = [[Data recovery]]\n| license                = [[Proprietary software|Proprietary]], [[Freeware]]\n| website                = {{URL|www.quickpar.org.uk}}\n}}\n\n\'\'\'QuickPar\'\'\' is a computer program that creates [[parchive]]s used as verification and recovery information for a file or group of files, and uses the recovery information, if available, to attempt to reconstruct the originals from the damaged files and the PAR volumes.\n\nDesigned for the [[Microsoft Windows]] [[operating system]], it is often used to recover damaged or missing files that have been downloaded through [[Usenet]].<ref>{{cite book\n| last        = Wang\n| first       = Wallace\n| authorlink  = \n| title       = Steal this File Sharing Book\n| url         = https://books.google.com/books?id=FGfMS5kymmcC&pg=PT183\n| accessdate  = 2009-09-24\n| edition     = 1st\n| date        = 2004-10-25\n| publisher   = [[No Starch Press]]\n| location    = [[San Francisco, California]]\n| isbn        = 1-59327-050-X\n| pages       = 164 – 167\n| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files\n}}</ref> QuickPar may also be used under [[Linux]] via [[Wine (software)|Wine]].<ref>{{cite book\n| last        = Petersen\n| first       = Richard\n| title       = Ubuntu 9.04 Desktop Handbook\n| url         = https://books.google.com/books?id=-XLrpiHDYoQC&pg=PT224\n| accessdate  = 2009-09-27\n| date        = 2009-05-01\n| publisher   = Surfing Turtle Press\n| location    = [[Los Angeles, California]]\n| isbn        = 0-9820998-4-3\n| page        = 224\n| chapter     = Internet Applications\n}}</ref>\n[[Image:QuickPar Protect Screenshot.png|thumb|right|Par2 file creation screen]]\n\nThere are two main versions of [[Parchive|PAR files]]: PAR and PAR2. The PAR2 file format lifts many of its previous restrictions.<ref>{{cite web\n| url         = http://www.quickpar.org.uk/AboutPAR2.htm\n| title       = QuickPar - About PAR2\n| accessdate  = 2009-09-27\n}}</ref> QuickPar is [[freeware]] but not [[open source]]. It uses the [[Reed-Solomon error correction]] algorithm internally to create the error correcting information.<ref name="newsgroups">{{Cite journal | last1 = Fellows | first1 = G. | title = Newsgroups reborn – the binary posting renaissance | doi = 10.1016/j.diin.2006.04.006 | journal = Digital Investigation | volume = 3 | issue = 2 | pages = 73–78 | year = 2006 | pmid =  | pmc = }}</ref>\n\n==Abandonware==\nThough QuickPar works well, it is currently considered [[abandonware]], since there have been no updates for it in {{age|2004|07|04}} years.  The software, [[Parchive#Windows| MultiPar]], is actively being developed by another author named Yutaka Sawada, who is adding support for the new PAR3 file format.\n\n==See also==\n* [[:nl:Data Archiving and Networked Services|DANS; has some similar software]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Official website|http://www.quickpar.org.uk/}}\n* [http://www.quickpar.org.uk/Tutorials.htm QuickPar tutorial referencing Usenet downloads]\n* [https://www.livebusinesschat.com/smf/index.php?board=396.0 MultiPar], successor to QuickPar, supports PAR2, PAR3 and multicore cpu\'s\n\n[[Category:Data management]]\n\n\n{{storage-software-stub}}']
['VMDS', '3047078', '{{COI|date=July 2016}}\n{{unreferenced|date=July 2016}}\n\'\'\'VMDS\'\'\' abbreviates the relational database technology called \'\'\'Version Managed Data Store\'\'\' provided by [[GE Energy]] as part of its [[Smallworld]] technology platform and was designed from the outset to store and analyse the highly complex spatial and topological networks typically used by enterprise utilities such as power distribution and telecommunications.\n\nVMDS was originally introduced in 1990 as has been improved and updated over the years. Its current version is 6.0.\n\nVMDS has been designed as a [[spatial database]]. This gives VMDS a number of distinctive characteristics when compared to conventional attribute only relational databases.\n\n==Distributed server processing==\nVMDS is composed of two parts: a simple, highly scalable data block server called \'\'\'SWMFS\'\'\' (Smallworld Master File Server) and an intelligent client [[Application programming interface|API]] written in [[C (programming language)|C]] and [[Magik programming language|Magik]]. Spatial and attribute data are stored in data blocks that reside in special files called data store files on the server. When the client application requests data it has sufficient intelligence to work out the optimum set of data blocks that are required. This request is then made to SWMFS which returns the data to the client via the network for processing.\n\nThis approach is particularly efficient and scalable when dealing with spatial and topological data which tends to flow in larger volumes and require more processing then plain attribute data (for example during a map redraw operation). This approach makes VMDS well suited to enterprise deployment that might involve hundreds or even thousands of concurrent clients.\n\n==Support for long transactions==\nRelational databases support [[Database transaction|short transactions]] in which changes to data are relatively small and are brief in terms in duration (the maximum period between the start and the end of a transaction is typically a few seconds or less).\n\nVMDS supports long transactions in which the volume of data involved in the transaction can be substantial and the duration of the transaction can be significant (days, weeks or even months). These types of transaction are common in advanced network applications used by, for example, power distribution utilities.\n\nDue to the time span of a long transaction in this context the amount of change can be significant (not only within the scope of the transaction, but also within the context of the database as a whole). Accordingly, it is likely that the same record might be changed more than once. To cope with this scenario VMDS has inbuilt support for automatically managing such conflicts and allows applications to review changes and accept only those edits that are correct.\n\n==Spatial and topological capabilities==\nAs well as conventional relational database features such as attribute querying, join fields, triggers and calculated fields, VMDS has numerous spatial and topological capabilities. This allows spatial data such as points, texts, polylines, polygons and raster data to be stored and analysed.\n\nSpatial functions include: find all features within a polygon, calculate the [[Voronoi polygon]]s of a set of sites and perform a [[cluster analysis]] on a set of points.\n\nVector spatial data such as points, polylines and polygons can be given topological attributes that allow complex networks to be modelled. Network analysis engines are provided to answer questions such as find the shortest path between two nodes or how to optimize a delivery route (the [[travelling salesman problem]]). A topology engine can be configured with a set of rules that define how topological entities interact with each other when new data is added or existing data edited.\n\n==Data abstraction==\nIn VMDS all data is presented to the application as objects. This is different from many relational databases that present the data as rows from a table or query result using say [[JDBC]]. VMDS provides a data modelling tool and underlying infrastructure as part of the [[Smallworld]] technology platform that allows administrators to associate a table in the database with a Magik exemplar (or class). Magik get and set methods for the Magik exemplar can be automatically generated that expose a table\'s field (or column). Each VMDS \'\'row\'\' manifests itself to the application as an instance of a [[Magik programming language|Magik]] object and is known as an \'\'\'RWO\'\'\' (or real world object). Tables are known as collections in Smallworld parlance.\n\n  # all_rwos hold all the rwos in the database and is heterogeneous\n  all_rwos << my_application.rwo_set()\n \n  # valve_collection holds the valve collection\n  valves << all_rwos.select(:collection, {:valve})\n  number_of_valves << valves.size\n\nQueries are built up using predicate objects:\n\n  # find \'open\' valves.\n  open_valves << valves.select(predicate.eq(:operating_status, "open"))\n  number_of_open_valves << open_valves.size\n\n  _for valve _over open_valves.elements()\n  _loop\n    write(valve.id)\n  _endloop\n\nJoins are implemented as methods on the parent RWO. For example a manager might have several employees who report to him:\n\n  # get the employee collection.\n  employees << my_application.database.collection(:gis, :employees)\n\n  # find a manager called \'Steve\' and get the first matching element\n  steve << employees.select(predicate.eq(:name, "Steve").and(predicate.eq(:role, "manager")).an_element()\n\n  # display the names of his direct reports. name is a field (or column)\n  # on the employee collection (or table)\n  _for employee _over steve.direct_reports.elements()\n  _loop\n     write(employee.name)\n  _endloop\n\nPerforming a transaction:\n\n  # each key in the hash table corresponds to the name of the field (or column) in\n  # the collection (or table)\n  valve_data << hash_table.new_with(\n    :asset_id, 57648576,\n    :material, "Iron")\n\n  # get the valve collection directly\n  valve_collection << my_application.database.collection(:gis, :valve)\n\n  # create an insert transaction to insert a new valve record into the collection a\n  # comment can be provide that describes the transaction\n  transaction << record_transaction.new_insert(valve_collection, valve_data, "Inserted a new valve")\n  transaction.run()\n\n==See also==\n* [[List of relational database management systems]]\n* [[List of object-relational database management systems]]\n* [[Spatial database]]\n* [[Multiversion concurrency control]]\n\n[[Category:Data management]]\n[[Category:GIS software]]']
['Data architecture', '4071997', '{{Refimprove|date=November 2008}}\nIn [[information technology]], \'\'\'data architecture\'\'\' is composed of models, policies, rules or standards that govern which data is collected, and how it is stored, arranged, integrated, and put to use in data systems and in organizations.<ref>[http://www.businessdictionary.com/definition/data-architecture.html Business Dictionary - Data Architecture]</ref>  Data is usually one of several [[architecture domain]]s that form the pillars of an [[enterprise architecture]] or [[solution architecture]].<ref>[http://www.learn.geekinterview.com/data-warehouse/data-architecture/what-is-data-architecture.html What is data architecture] GeekInterview, 2008-01-28, accessed 2011-04-28</ref>\n\n== Overview ==\nA data architecture should{{POV statement|date=March 2013}} set data standards for all its data systems as a vision or a model of the eventual interactions between those data systems. [[Data integration]], for example, should be dependent upon data architecture standards since data integration requires data interactions between two or more data systems. A data architecture, in part, describes the [[data structure]]s used by a business and its computer [[applications software]]. Data architectures address data in storage and data in motion; descriptions of data stores, data groups and data items; and [[data mapping|mappings]] of those data artifacts to data qualities, applications, locations etc.\n\nEssential to realizing the target state, Data Architecture describes how data is processed, stored, and utilized in an [[information system]]. It provides criteria for [[data processing]] operations so as to make it possible to design [[data flow]]s and also control the flow of data in the system.\n\nThe [[data architect]] is typically responsible for defining the target state, aligning during development and then following up to ensure enhancements are done in the spirit of the original blueprint.\n\nDuring the definition of the target state, the Data Architecture breaks a subject down to the atomic level and then builds it back up to the desired form. The data architect breaks the subject down by going through 3 traditional architectural processes:\n* Conceptual - represents all business entities.\n* Logical - represents the logic of how entities are related.\n* Physical - the realization of the data mechanisms for a specific type of functionality.\n\nThe "data" column of the [[Zachman Framework]] for enterprise architecture &ndash;\n\n{| border=1\n|\'\'\'Layer\'\'\' || \'\'\'View\'\'\' || \'\'\'Data (What)\'\'\' || \'\'\'Stakeholder\'\'\'\n|-\n|1||\'\'\'Scope/Contextual\'\'\' || List of things and architectural standards<ref>[http://www.strins.com/data-architecture-standards.html Data Architecture Standards]</ref> important to the business || Planner\n|-\n|2||\'\'\'Business Model/Conceptual\'\'\'  || Semantic model or [[Entity-relationship model|Conceptual]]/[http://tdan.com/the-enterprise-data-model/5205 Enterprise Data Model] || Owner\n|-\n|3||\'\'\'System Model/Logical\'\'\' || Enterprise/[[Logical data model|Logical Data Model]] || Designer\n|-\n|4||\'\'\'Technology Model/Physical\'\'\' || [[Physical data model|Physical Data Model]] || Builder\n|-\n|5||\'\'\'Detailed Representations\'\'\'  ||  Actual [[database]]s || Subcontractor\n|}\n\nIn this second, broader sense, data architecture includes a complete analysis of the relationships among an organization\'s functions, available [[technologies]], and [[data type]]s.\n\nData architecture should be defined in the \'\'\'planning phase\'\'\' of the design of a new data processing and storage system. The major types and sources of data necessary to support an enterprise should be identified in a manner that is complete, consistent, and understandable. The primary requirement at this stage is to define all of the relevant \'\'\'data entities\'\'\', not to specify [[computer hardware]] items. A data entity is any real or abstracted thing about which an organization or individual wishes to store data.\n\n== Physical data architecture ==\nPhysical data architecture of an information system is part of a [[Technology roadmapping|technology plan]]. As its name implies, the technology plan is focused on the actual tangible [[element (mathematics)|elements]] to be used in the implementation of the data architecture [[design]]. Physical data architecture encompasses database architecture. Database architecture is a [[Model (abstract)|schema]] of the actual database technology that will support the designed data architecture.\n\n== Elements of data architecture ==\nCertain elements must be defined during the design phase of the data architecture schema. For example, administrative structure that will be established in order to manage the data resources must be described. Also, the methodologies that will be employed to store the data must be defined. In addition, a description of the database technology to be employed must be generated, as well as a description of the processes that will manipulate the data. It is also important to design [[interface (computing)|interfaces]] to the data by other systems, as well as a design for the [[infrastructure]] that will support common data operations (i.e. emergency procedures, [[data import]]s, [[data backup]]s, external [[data transfer|transfers of data]]).\n\nWithout the guidance of a properly implemented data architecture design, common data operations might be implemented in different ways, rendering it difficult to understand and control the flow of data within such systems. This sort of fragmentation is highly undesirable due to the potential increased cost, and the data disconnects involved. These sorts of difficulties may be encountered with rapidly growing enterprises and also enterprises that service different lines of [[business]] (e.g. [[insurance]] [[Product (business)|products]]).\n\nProperly executed, the data architecture phase of information system planning forces an organization to precisely specify and describe both internal and external information flows. These are patterns that the organization may not have previously taken the time to conceptualize. It is therefore possible at this stage to identify costly information shortfalls, disconnects between departments, and disconnects between organizational systems that may not have been evident before the data architecture analysis.<ref>{{cite book|last=Mittal|first=Prashant|title=Author|year=2009|publisher=Global India Publications|location=pg 256|isbn=978-93-8022-820-4|pages=314|url=https://books.google.com/books?id=BpkhYDj4tm0C&dq=inauthor:%22PRASHANT+MITTAL%22&source=gbs_navlinks_s}}</ref>\n\n== Constraints and influences ==\nVarious constraints and influences will have an effect on data architecture design. These include enterprise requirements, technology drivers, economics, business policies and data processing needs.\n\n; Enterprise requirements: These will generally include such elements as economical and effective system expansion, acceptable performance levels (especially system access speed), [[Financial transaction|transaction]] reliability, and transparent [[data management]]. In addition, the [[Data conversion|conversion]] of raw data such as transaction [[Record (computer science)|records]] and [[image]] [[Computer file|files]] into more useful [[information]] forms through such features as [[data warehouse]]s is also a common organizational [[requirement]], since this enables managerial decision making and other organizational processes. One of the architecture techniques is the split between managing [[transaction data]] and (master) [[reference data]]. Another one is splitting [[Automatic identification and data capture|data capture systems]] from data retrieval systems (as done in a data warehouse).\n\n; Technology drivers: These are usually suggested by the completed data architecture and database architecture designs. In addition, some technology drivers will derive from existing organizational integration frameworks and standards, organizational economics, and existing site resources (e.g. previously purchased [[software licensing]]).\n\n; Economics: These are also important factors that must be considered during the data architecture phase. It is possible that some solutions, while optimal in principle, may not be potential candidates due to their cost. External factors such as the [[business cycle]], interest rates, market conditions, and legal considerations could all have an effect on decisions relevant to data architecture.\n\n; Business policies: [[Business policies]] that also drive data architecture design include internal organizational policies, rules of [[regulatory agency|regulatory bodies]], professional standards, and applicable governmental [[laws]] that can vary by applicable [[government agency|agency]]. These policies and rules will help describe the manner in which enterprise wishes to process their data.\n\n; Data processing needs: These include accurate and reproducible [[data transaction|transactions]] performed in high volumes, data warehousing for the support of management information systems (and potential [[data mining]]), repetitive periodic [[Data reporting|reporting]], ad hoc reporting, and support of various organizational initiatives as required (i.e. annual budgets, new [[Product (business)|product]] development).\n\n== See also ==\n* [[Enterprise Information Security Architecture]] - (EISA) positions data security in the enterprise information framework.\n* [[FDIC Enterprise Architecture Framework]]\n* [[Controlled vocabulary]]\n* [[Information silo]]\n* [[Disparate system]]\n* [[Data Warehouse]]\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n* Bass, L.; John, B.; & Kates, J. (2001). \'\'Achieving Usability Through Software Architecture\'\', Carnegie Mellon University.\n* Lewis, G.; Comella-Dorda, S.; Place, P.; Plakosh, D.; & Seacord, R., (2001). \'\'Enterprise Information System Data Architecture Guide\'\' Carnegie Mellon University.\n* Adleman, S.; Moss, L.; Abai, M. (2005). \'\'Data Strategy\'\' Addison-Wesley Professional.\n\n== External links ==\n{{commons category|Data architecture}}\n* [http://www.sei.cmu.edu/library/abstracts/reports/01tr005.cfm Achieving Usability Through Software Architecture], sei.cmu.edu 2001\n* [http://sunsite.uakom.sk/sunworldonline/swol-07-1998/swol-07-itarchitect.html The Logical Data Architecture], by Nirmal Baid\n\n{{Data model}}\n\n[[Category:Computer data]]\n[[Category:Data management]]\n[[Category:Enterprise architecture]]']
['Three-phase commit protocol', '2044655', "In [[computer networking]] and [[database]]s, the '''three-phase commit protocol''' ('''3PC''')<ref name=3PC>{{cite journal\n | last = Skeen\n | first = Dale\n | title = A Formal Model of Crash Recovery in a Distributed System\n | journal = IEEE Transactions on Software Engineering\n | volume = 9\n | issue = 3\n |date=May 1983\n | pages = 219–228\n | doi = 10.1109/TSE.1983.236608\n | last2 = Stonebraker\n | first2 = M.\n}}</ref> is a [[distributed algorithm]] which lets all nodes in a [[distributed system]] agree to [[Commit (data management)|commit]] a [[database transaction|transaction]].  Unlike the [[two-phase commit protocol]] (2PC) however, 3PC is non-blocking.  Specifically, 3PC places an upper bound on the amount of time required before a transaction either commits or [[Abort (computing)|aborts]].  This property ensures that if a given transaction is attempting to commit via 3PC and holds some [[lock (computer science)|resource locks]], it will release the locks after the timeout.\n\n==Protocol Description==\nIn describing the protocol, we use terminology similar to that used in the [[two-phase commit protocol]].  Thus we have a single coordinator site leading the transaction and a set of one or more cohorts being directed by the coordinator.\n\n<center>[[Image:Three-phase commit diagram.png]]</center>\n\n===Coordinator===\n#The coordinator receives a transaction request.  If there is a failure at this point, the coordinator aborts the transaction (i.e. upon recovery, it will consider the transaction aborted).  Otherwise, the coordinator sends a canCommit? message to the cohorts and moves to the waiting state.\n#If there is a failure, timeout, or if the coordinator receives a No message in the waiting state, the coordinator aborts the transaction and sends an abort message to all cohorts.  Otherwise the coordinator will receive Yes messages from all cohorts within the time window, so it sends preCommit messages to all cohorts and moves to the prepared state.\n#If the coordinator succeeds in the prepared state, it will move to the commit state.  However if the coordinator times out while waiting for an acknowledgement from a cohort, it will abort the transaction.  In the case where an acknowledgement is received from the majority of cohorts, the coordinator moves to the commit state as well.\n\n===Cohort===\n#The cohort receives a canCommit? message from the coordinator.  If the cohort agrees it sends a Yes message to the coordinator and moves to the prepared state.  Otherwise it sends a No message and aborts.  If there is a failure, it moves to the abort state.\n#In the prepared state, if the cohort receives an abort message from the coordinator, fails, or times out waiting for a commit, it aborts.  If the cohort receives a  preCommit message, it sends an '''[[acknowledgement (data networks)|ACK]]''' message back and awaits a final commit or abort.\n#If, after a cohort member receives a  preCommit  message, the coordinator fails or times out, the cohort member goes forward with the commit.\n\n==Motivation==\nA [[Two-phase commit protocol]] cannot dependably recover from a failure of both the coordinator and a cohort member during the '''Commit phase'''.  If only the coordinator had failed, and no cohort members had received a commit message, it could safely be inferred that\nno commit had happened.  If, however, both the coordinator and a cohort member\nfailed, it is possible that the failed cohort member was the first to be notified, and had\nactually done the commit.  Even if a new coordinator is selected, it cannot \nconfidently proceed with the operation until it has received an agreement from\nall cohort members ... and hence must block until all cohort members respond.\n\nThe Three-phase commit protocol eliminates this problem by introducing the Prepared to commit\nstate.  If the coordinator fails before sending preCommit messages, the cohort will\nunanimously agree that the operation was aborted.  The coordinator will not send out a doCommit\nmessage until all cohort members have '''ACK'''ed that they are '''Prepared to commit'''. \nThis eliminates the possibility that any cohort member actually completed the \ntransaction before all cohort members were aware of the decision to do so \n(an ambiguity that necessitated indefinite blocking in the [[Two-phase commit protocol]]).\n\n==Disadvantages==\nThe main disadvantage to this algorithm is that it cannot recover in the event the network is segmented in any manner. The original 3PC algorithm assumes a fail-stop model, where processes fail by crashing and crashes can be\naccurately detected, and does not work with network partitions or asynchronous communication.\n\nKeidar and Dolev's E3PC<ref name=E3PC>{{cite journal|last=Keidar|first=Idit|author2=Danny Dolev |title=Increasing the Resilience of Distributed and Replicated Database Systems|journal=Journal of Computer and System Sciences (JCSS)|volume=57|issue=3|date=December 1998|pages=309–324|\nurl=http://webee.technion.ac.il/~idish/Abstracts/jcss.html|doi=10.1006/jcss.1998.1566}}</ref> algorithm eliminates this disadvantage.\n\nThe protocol requires at least 3 round trips to complete, needing a minimum of 3 round trip times (RTTs). This is potentially a long latency to complete each transaction.\n\n==References==\n{{Reflist}}\n\n==See also==\n*[[Two-phase commit protocol]]\n\n[[Category:Data management]]\n[[Category:Transaction processing]]"]
['Content management', '223240', '{{distinguish|Information management|Knowledge management}}\n{{Refimprove|date=July 2007}}\n\n{{Business administration}}\n\'\'\'Content management\'\'\' (\'\'\'CM\'\'\'), is a set of processes and technologies that supports the collection, managing, and publishing of information in any form or medium.  When stored and accessed via computers, this information may be more specifically referred to as [[digital content]], or simply as [[Content (media and publishing)|content]].  Digital content may take the form of text (such as [[electronic document]]s), multimedia files (such as audio or video files), or any other file type that follows a content lifecycle requiring [[Product lifecycle management|management]]. The process is complex enough to manage that several large and small commercial software vendors such as [[Interwoven]] and [[Microsoft]] offer [[Content management system|content management software]] to control and automate significant aspects of the content lifecycle.\n\n==The process of content management==\nContent management practices and goals vary by mission and by organizational governance structure.\nNews organizations, [[e-commerce]] websites, and educational institutions all use content management, but in different ways. This leads to differences in terminology and in the names and number of steps in the process.\n\nFor example, some digital content is created by one or more authors. Over time that content may be edited. One or more individuals may provide some editorial oversight, approving the content for publication. Publishing may take many forms: it may be the act of "pushing" content out to others, or simply granting digital access rights to certain content to one or more individuals.  Later that content may be superseded by another version of the content and thus retired or removed from use (as when this wiki page is modified).\n\nContent management is an inherently collaborative process. It often consists of the following basic roles and responsibilities:\n\n* \'\'\'Creator\'\'\' – responsible for creating and editing content.\n* \'\'\'Editor\'\'\' – responsible for tuning the content message and the style of delivery, including translation and localization.\n* \'\'\'Publisher\'\'\' – responsible for releasing the content for use.\n* \'\'\'Administrator\'\'\' – responsible for managing access permissions to folders and files, usually accomplished by assigning access rights to user groups or roles. Admins may also assist and support users in various ways.\n* \'\'\'Consumer, viewer or guest\'\'\' – the person who reads or otherwise takes in content after it is published or shared.\n\nA critical aspect of content management is the ability to manage versions of content as it evolves (\'\'see also\'\' [[version control]]). Authors and editors often need to restore older versions of edited products due to a process failure or an undesirable series of edits.\n\nAnother equally important aspect of content management involves the creation, maintenance, and application of review standards. Each member of the content creation and review process has a unique role and set of responsibilities in the development or publication of the content. Each review team member requires clear and concise review standards. These must be maintained on an ongoing basis to ensure the long-term consistency and health of the [[knowledge base]].\n\nA content management system is a set of automated processes that may support the following features:\n\n* Import and creation of documents and multimedia material\n* Identification of all key users and their roles\n* The ability to assign roles and responsibilities to different instances of content categories or types\n* Definition of workflow tasks often coupled with messaging so that content managers are alerted to changes in content\n* The ability to track and manage multiple versions of a single instance of content\n* The ability to publish the content to a repository to support access\n* The ability to personalize content based on a set of rules\n* \nIncreasingly, the repository is an inherent part of the system, and incorporates [[enterprise search]] and retrieval. Content management systems take the following forms:\n\n* [[Web content management system]]—[[software]] for [[web site]] management (often what \'\'content management\'\' implicitly means)\n* Output of a [[newspaper]] editorial staff organization\n* [[Workflow]] for [[essay|article]] publication\n* [[Document management system]]\n* [[Single source publishing|Single source]] content management system—content stored in chunks within a relational database\n* Variant management system—where personnel tag source content (usually text and graphics) to represent variants stored as single source "master" content modules, resolved to the desired variant at publication (for example: automobile owners manual content for 12 model years stored as single master content files and "called" by model year as needed)—often used in concert with database chunk storage (see above) for large content objects\n\n==Governance structures==\nContent management expert Marc Feldman defines three primary content management governance structures: localized, centralized, and federated—each having its unique strengths and weaknesses.<ref>http://www.clickz.com/clickz/column/1715089/governance-issues-content-management</ref>\n\n===Localized governance===\nBy putting control in the hands of those closest to the content, the context experts, localized governance models empower and unleash creativity.  These benefits come, however, at the cost of a partial-to-total loss of managerial control and oversight.\n\n===Centralized governance===\nWhen the levers of control are strongly centralized, content management systems are capable of delivering an exceptionally clear and unified brand message.  Moreover, centralized content management governance structures allow for a large number of cost-savings opportunities in large enterprises, realized, for example, (1) the avoidance of duplicated efforts in creating, editing, formatting, repurposing and archiving content, (2) through process management and the streamlining of all content related labor, and/or (3) through an orderly deployment or updating of the content management system.\n\n===Federated governance===\nFederated governance models potentially realize the benefits of both localized and centralized control while avoiding the weaknesses of both. While content management software systems are inherently structured to enable federated governance models, realizing these benefits can be difficult because it requires, for example, negotiating the boundaries of control with local managers and content creators.  In the case of larger enterprises, in particular, the failure to fully implement or realize a federated governance structure equates to a failure to realize the full return on investment and cost savings that content management systems enable.\n\n==Implementation==\nContent management implementations must be able to manage content distributions and digital rights in content life cycle.<ref>{{cite journal |last=White |first=Blake |date=April 2004 |title=A New Era for Content: Protection, Potential, and Profit in the Digital World |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7262562 |journal=SMPTE Motion Imaging Journal |publisher=Society of Motion Picture & Television Engineers |volume=113 |issue=4 |pages=110–120 |doi= |access-date=July 1, 2016}}</ref><ref>{{cite web | url=http://www.giantstepsmts.com/CM-DRMwhitepaper.pdf  | title=Integrating Content Management with Digital Rights Management   | publisher=GiantSteps  | accessdate=2016-07-01}}</ref><ref>{{cite web | url=http://www.locklizard.com/content-distribution-drm/  | title=Content Distribution & DRM - Managing Content Distribution with DRM | publisher=Locklizard.com  | accessdate=2016-07-01}}</ref><ref>{{cite book  | title= The World Beyond Digital Rights Management  |url=https://www.amazon.com/World-Beyond-Digital-Rights-Management-ebook/dp/B004GEAEZM  | location = |last=Umeh   |first =Jude | date =October 2007  | publisher = British Computer Society  |page=320 |isbn= 978-1902505879 }}</ref> Content management systems are usually involved with [[digital rights management]] in order to control user access and digital rights. In this step, the read-only structures of [[digital rights management]] systems force some limitations on content management, as they do not allow authors to change protected content in their life cycle. Creating new content using managed (protected) content is also an issue that gets protected contents out of management controlling systems. A few content management implementations cover all these issues.<ref>{{cite journal |last=White |first=Blake |date=April 2004 |title=A New Era for Content: Protection, Potential, and Profit in the Digital World |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7262562 |journal=SMPTE Motion Imaging Journal |publisher=Society of Motion Picture & Television Engineers |volume=113 |issue=4 |pages=110–120 |doi= |access-date=July 1, 2016}}</ref><ref>{{cite web | url=http://www.giantstepsmts.com/CM-DRMwhitepaper.pdf  | title=Integrating Content Management with Digital Rights Management   | publisher=GiantSteps  | accessdate=2016-07-01}}</ref><ref>{{cite web | url=http://www.locklizard.com/content-distribution-drm/  | title=Content Distribution & DRM - Managing Content Distribution with DRM | publisher=Locklizard.com  | accessdate=2016-07-01}}</ref><ref>{{cite book  | title= The World Beyond Digital Rights Management  |url=https://www.amazon.com/World-Beyond-Digital-Rights-Management-ebook/dp/B004GEAEZM  | location = |last=Umeh   |first =Jude | date =October 2007  | publisher = British Computer Society  |page=320 |isbn= 978-1902505879 }}</ref>\n\n==See also==\n{{Div col|colwidth=22em}}\n* [[Content delivery]]\n* [[Content engineering]]\n* [[Content Management Interoperability Services]]\n* [[Content management system]]\n* [[Digital asset management]]\n* [[Enterprise content management]]\n* [[Enterprise information management]]\n* [[Information architecture]]\n* [[List of content management systems]]\n* [[Single source publishing]]\n* [[Snippet management]]\n* [[Web content lifecycle]]\n* [[Web design]]\n* [[Website architecture]]\n* [[Website governance]]\n{{Div col end}}\n\n==References==\n{{reflist|colwidth=30em}}\n\n==External links==\n* {{cite book | last = Boiko | first = Bob | authorlink = | title = Content Management Bible | publisher = Wiley | date = 2004-11-26 | location = | pages = 1176 | url = | doi = | id = | isbn = 0-7645-7371-3 }}\n* {{cite book | last = Rockley | first = Ann | authorlink = | title = Managing Enterprise Content: A Unified Content Strategy | publisher = New Riders Press | date = 2002-10-27 | location = | pages = 592 | url = | doi = | id = | isbn = 0-7357-1306-5 }}\n* {{cite book | last = Hackos | first = JoAnn T. | authorlink = | title = Content Management for Dynamic Web Delivery | publisher = Wiley | date = 2002-02-14 | location = | pages = 432 | url = | doi = | id = | isbn = 0-471-08586-3 }}\n* {{cite book | last = Glushko| first = Robert J. | authorlink = |author2=Tim McGrath| title = Document Engineering: Analyzing and Designing Documents for Business Informatics and Web Services | publisher = MIT Press| year = 2005 | location = | pages = 728| url = | doi = | id = | isbn = 0-262-57245-1 }}\n* {{cite book | last = Ferran | first = Núria | authorlink = | author2=Julià Minguillón | title = Content Management for E-Learning | publisher = Springer | date = 2011 | location = | pages = 215 | url = http://www.springer.com/us/book/9781441969583 | doi = | id = | isbn = 978-1-4419-6958-3 }}\n\n{{Content management systems}}\n{{WebManTools}}\n\n[[Category:Technical communication]]\n[[Category:Data management]]\n[[Category:Content management systems]]\n\n[[fr:Système de gestion de contenu]]']
['Long-running transaction', '4984219', "{{More sources|date=October 2015}}\n'''Long-running transactions''' (also known as Saga transactions) are computer [[database transaction]]s that avoid [[lock (computer science)|locks]] on non-local resources, use compensation to handle failures, potentially aggregate smaller [[ACID]] transactions (also referred to as [[atomic transaction]]s), and typically use a coordinator to complete or abort the transaction. In contrast to [[rollback (data management)|rollback]] in ACID transactions, compensation restores the original state, or an equivalent, and is business-specific. For example, the compensating action for making a hotel reservation is canceling that reservation, possibly with a penalty.\n\nA number of protocols have been specified for long-running transactions using Web services within business processes. OASIS Business Transaction Processing<ref>http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=business-transaction</ref> and WS-CAF<ref>http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=ws-caf</ref> are examples. These protocols use a coordinator to mediate the successful completion or use of compensation in a long-running transaction.\n\n==See also==\n*[[Optimistic concurrency control]]\n*[[Long-lived transaction]]\n\n==References==\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:Transaction processing]]"]
['Scriptella', '8303455', '{{Multiple issues|\n{{unreferenced|date=March 2012}}\n{{Notability|Products|date=March 2012}}\n}}\n\n{{Infobox Software\n| name = Scriptella\n| logo = [[File:Scriptella logo.png|160px|Scriptella logo]]\n| latest_release_version = 1.1\n| latest_release_date = 28 December 2012\n| operating_system = [[Cross-platform]]\n| genre = [[Extract transform load|ETL]], [[Data migration]] and [[SQL]].\n| license = [[Apache Software License]]\n| website = [http://scriptella.org http://scriptella.org]\n}}\n\n\'\'\'Scriptella\'\'\' is an open source [[Extract transform load|ETL (Extract-Transform-Load)]] and script execution tool written in Java. Its primary focus is simplicity. It doesn\'t require the user to learn another complex XML-based language to use it, but allows the use of SQL or another scripting language suitable for the data source to perform required transformations. Scriptella does not offer any [[graphical user interface]].\n\n==Typical use==\n* Database migration.\n* Database creation/update scripts.\n* Cross-database ETL operations, import/export.\n* Alternative for Ant <sql> task.\n* Automated database schema upgrade.\n\n==Features==\n* \'\'\'Simple XML syntax\'\'\' for scripts. Add dynamics to your existing SQL scripts by creating a thin wrapper XML file:<source lang="xml">\n      <!DOCTYPE etl SYSTEM "http://scriptella.javaforge.com/dtd/etl.dtd">\n      <etl>\n          <connection driver="$driver" url="$url" user="$user" password="$password"/>\n          <script>\n              <include href="PATH_TO_YOUR_SCRIPT.sql"/>\n              -- And/or directly insert SQL statements here\n          </script>\n      </etl>\n</source>\n* Support for \'\'\'multiple datasources\'\'\' (or multiple connections to a single database) in an ETL file.\n* Support for many useful \'\'\'[[Java Database Connectivity|JDBC]] features\'\'\', e.g. parameters in SQL including file blobs and JDBC escaping.\n* \'\'\'Performance.\'\'\' Performance and low memory usage are one of the primary goals.\n* Support for \'\'\'evaluated expressions and properties\'\'\' (JEXL syntax)\n* Support for \'\'\'cross-database ETL scripts\'\'\' by using <dialect> elements\n* \'\'\'Transactional execution\'\'\'\n* \'\'\'Error handling\'\'\' via <onerror> elements\n* \'\'\'Conditional scripts/queries execution\'\'\' (similar to Ant if/unless attributes but more powerful)\n* \'\'\'Easy-to-Use\'\'\' as a standalone tool or Ant task. No deployment/installation required.\n* \'\'\'Easy-To-Run\'\'\' ETL files directly from Java code.\n* \'\'\'Built-in adapters for popular databases\'\'\' for a tight integration. Support for any database with JDBC/[[Open Database Connectivity|ODBC]] compliant driver.\n* Service Provider Interface (SPI) for interoperability with non-JDBC DataSources and integration with scripting languages. Out of the box support for [[JSR 223|JSR 223 (Scripting for the Java Platform)]] compatible languages.\n* Built-In [[Comma-separated values|CSV]], TEXT, [[XML]], [[Lightweight Directory Access Protocol|LDAP]], [[Apache Lucene|Lucene]], [[Apache Velocity|Velocity]], JEXL and Janino providers. Integration with [[Java EE]], [[Spring framework|Spring Framework]], [[Java Management Extensions|JMX]] and [[JNDI]] for enterprise ready scripts.\n\n==External links==\n* [http://scriptella.org Scriptella ETL Site]\n* [https://github.com/scriptella/scriptella-etl GitHub Page]\n* [http://groups.google.com/group/scriptella/ Discussion forum]\n* [http://www.javaforge.com/proj/forum/browseForum.do?forum_id=3126 Discussion forum(deprecated)]\n* [http://jroller.com/page/ejboy Scriptella ETL Author\'s Blog]\n* {{Ohloh project|id=4526|name=Scriptella ETL}}\n\n[[Category:Extract, transform, load tools]]\n[[Category:Data warehousing products]]\n[[Category:Data management]]\n\n\n{{compu-stub}}']
['Data classification (data management)', '3743270', 'In the field of [[data management]], \'\'\'data classification\'\'\' as a part of [[Information Lifecycle Management]] (ILM) process can be defined as a tool for categorization of data to enable/help organization to effectively answer following questions:\n*What [[data type]]s are available?\n*Where are certain data located?\n*What [[access level]]s are implemented?\n*What protection level is implemented and does it adhere to [[compliance (regulation)|compliance]] regulations?\nWhen implemented it provides a bridge between IT professionals and process or application owners. IT staff is informed about the data value and on the other hand management (usually application owners) understands better to what segment of data centre has to be invested to keep operations running effectively.  This can be of particular importance in risk management, legal discovery, and compliance with government regulations.\nData classification is typically a manual process; however, there are many tools from different vendors that can help gather information about the data.\n\n==How to start process of data classification==\n\'\'Note that this classification structure is written from a Data Management perspective and therefore has a focus for text and text convertible binary data sources. Images, videos, and audio files are highly structured formats built for industry standard API\'s and do not readily fit within the classification scheme outlined below.\'\'\n\nFirst step is to evaluate and divide the various applications and data into their respective category as follows:\n*Relational or Tabular data (around 15% of non audio/video data) \n**Generally describes  proprietary data which can be accessible only through application or [[application programming interfaces]] (API)\n**Applications that produce structured data are usually database applications.\n**This type of data usually brings complex procedures of data evaluation and migration between the storage tiers.\n**To ensure adequate quality standards, the classification process has to be monitored by subject matter experts.\n*Semi-structured or Poly-structured data (all other non audio/video data that does not conform to a system or platform defined Relational or Tabular form).\n**Generally describes data files that have a dynamic or non-relational semantic structure (e.g. documents,XML,JSON,Device or System Log output,Sensor Output).\n**Relatively simple process of data classification is criteria assignment.\n**Simple process of [[data migration]] between assigned segments of predefined storage tiers.\n\nTypes of data classification - \'\'note that this designation is entirely orthogonal to the application centric designation outlined above. Regardless of structure inherited from application, data may be of the types below\'\'\n\n1. Geographical : i.e. according to area (supposing the rice production of a state or country etc.)\n2. Chronological: i.e. according to time (sale of last 3 months)\n3. Qualitative  : i.e. according to distinct categories. (E.g.: population on the basis of poor and rich)\n4. Quantitative : i.e. according to magnitude(a) discrete and b)continuous\n\n==Basic criteria for semi-structured or poly-structured data classification==\n*Time criteria is the simplest and most commonly used where different type of data is evaluated by time of creation, time of access, time of update, etc.\n*Metadata criteria as type, name, owner, location and so on can be used to create more advanced classification policy\n*Content criteria which involve usage of advanced content classification algorithms are most advanced forms of unstructured data classification\n\n\'\'Note that any of these criteria may also apply to Tabular or Relational data as "Basic Criteria". These criteria are application specific, rather than inherent aspects of the form in which the data is presented.\'\'.\n\n==Basic criteria for relational or Tabular data classification==\nThese criteria are usually initiated by application requirements such as:\n*Disaster recovery and Business Continuity rules\n*Data centre resources optimization and consolidation\n*Hardware performance limitations and possible improvements by reorganization\n\n\'\'Note that any of these criteria may also apply to semi/poly structured data as "Basic Criteria". These criteria are application specific, rather than inherent aspects of the form in which the data is presented.\'\'\n\n==Benefits of data classification==\nBenefits of effective implementation of appropriate data classification can significantly improve ILM process and save data centre storage resources. If implemented systemically it can generate improvements in data centre performance and utilization. Data classification can also reduce costs and administration overhead. "Good enough" data classification can produce these results:\n*Data compliance and easier [[risk management]]. Data are located where expected on predefined storage tier and "point in time"\n*Simplification of data encryption because all data need not be encrypted. This saves valuable processor cycles and all related consecutiveness. \n*Data indexing to improve user access times\n*Data protection is redefined where RTO ([[Recovery Time Objective]]) is improved.\n\n==See also==\n*[[Data classification (business intelligence)]]\n\n==References==\n\n* Josh Judd and Dan Kruger (2005), Principles of SAN Design. Infinity Publishing\n* Stephen J. Bigelown (November 2005), SearchStorage.com, http://searchstorage.techtarget.com/news/article/0,289142,sid5_gci1139240,00.html\n\n[[Category:Data management]]\n[[Category:Information technology]]\n[[Category:Regulations]]']
['Data Reference Model', '3576033', "[[Image:DRM Collaboration Process.jpg|thumb|320px|The DRM Collaboration Process.]]\nThe '''Data Reference Model''' ('''DRM''') is one of the five reference models of the [[Federal Enterprise Architecture]] (FEA). \n\n== Overview ==\nThe DRM is a framework whose primary purpose is to enable information sharing and reuse across the [[United States federal government]] via the standard description and discovery of common data and the promotion of uniform data management practices. The DRM describes artifacts which can be generated from the data architectures of federal government agencies. The DRM provides a flexible and standards-based approach to accomplish its purpose. The scope of the DRM is broad, as it may be applied within a single agency, within a [[Community of interest (computer security)|Community of Interest]] (COI)1, or cross-COI.\n\n== Data Reference Model topics ==\n=== DRM structure ===\nThe DRM provides a standard means by which [[data]] may be described, categorized, and shared. These are reflected within each of the DRM’s three standardization areas:\n\n* ''Data Description'': Provides a means to uniformly describe data, thereby supporting its discovery and sharing.\n* ''Data Context'': Facilitates discovery of data through an approach to the categorization of data according to taxonomies. Additionally, enables the definition of authoritative data assets within a COI.\n* ''Data Sharing'': Supports the access and exchange of data where access consists of ''ad hoc'' requests (such as a query of a data asset), and exchange consists of fixed, re-occurring transactions between parties. Enabled by capabilities provided by both the Data Context and Data Description standardization areas.\n\n===DRM Version 2 ===\nThe Data Reference Model version 2 released in November 2005 is a 114 page document with detailed architectural diagrams and an extensive glossary of terms.\n\nThe DRM also make many references to ISO standards specifically the [[ISO/IEC 11179]] metadata registry standard.\n\n=== DRM usage ===\nThe DRM is not technically a published technical interoperability standard such as web services, it is an excellent starting point for data architects within federal and state agencies.  Any federal or state agencies that are involved with exchanging information with other agencies or that are involved in [[Data warehouse]]ing efforts should use this document as a guide.\n\n==See also==\n* [[Enterprise architecture framework]]\n* [[Enterprise application integration]]\n* [[Enterprise service bus]]\n* [[Federal Enterprise Architecture]]\n* [[ISO/IEC 11179]]\n* [[Metadata publishing]]\n* [[Semantic spectrum]]\n* [[Semantic web]]\n* [[Synonym ring]]\n<!--\n== References ==\n{{reflist}}-->\n\n==External links==\n* [https://web.archive.org/web/20070617034325/http://www.defenselink.mil/cio-nii/docs/DoD_DRM_V04_5aug.pdf US Department of Defense Data Reference Model]\n* [http://www.whitehouse.gov/sites/default/files/omb/assets/egov_docs/DRM_2_0_Final.pdf US Federal Enterprise Architecture Program Data Reference Model Version 2.0]\n[[Category:Computer data]]\n[[Category:Data management]]\n[[Category:Reference models]]"]
['Client-side persistent data', '13150801', "'''Client-side persistent data''' or CSPD is a term used in [[computing]] for storing data required by [[web application |web applications]] to complete internet tasks on the [[client-side]] as needed rather than exclusively on the [[Server (computing) |server]]. As a framework it is one solution to the needs of [[Occasionally connected computing]] or OCC.\n\nA major challenge for [[HTTP]] as a [[Stateless server |stateless]] [[Protocol (computing)|protocol]] has been asynchronous tasks. The [[Ajax (programming)|AJAX]] pattern using [[XMLHttpRequest]] was first introduced by [[Microsoft]] in the context of the [[Outlook Web App|Outlook]] e-mail product.\n\nThe first CSPD were the [[HTTP cookie |'cookies']] introduced by the [[Netscape]] [[Netscape (web browser)|Navigator]].  [[ActiveX]] components which have entries in the [[Windows registry]] can also be viewed as a form of [[client-side]] [[Persistence (computer science)|persistence]].\n\n==See also==\n* [[Occasionally connected computing]]\n* [[Curl (programming_language)]]\n* [[Ajax (programming)|AJAX]]\n* [[HTTP]]\n* [[Web storage]]\n\n==External links==\n* [http://www.curl.com/developer/faq/cspd/ CSPD]\n* [http://safari.ciscopress.com/0596101996/jscript5-CHP-19-SECT-6 Safari] preview\n* [http://wp.netscape.com/newsref/std/cookie_spec.html Netscape] on persistent client state\n\n[[Category:Clients (computing)]]\n[[Category:Data management]]\n[[Category:Web applications]]"]
['XLDB', '14426202', '\'\'\'XLDB\'\'\' refers to \'\'\'eXtremely Large [[Database|Data Bases]]\'\'\'.  The definition of \'\'extremely large\'\' refers to data sets that are too big in terms of volume (too much), and/or velocity (too fast), and/of variety (too many places, too many formats) to be handled using conventional solutions.\n\n== History ==\n\nIn October 2007 the XLDB experts gathered at [[SLAC]] for the [https://web.archive.org/web/20080417002612/http://www-conf.slac.stanford.edu/xldb07/ First Workshop on Extremely Large Databases]. As a result, the XLDB research community was formed. to meet rapidly growing demands, in addition to the original invitational workshop, an open conference, tutorials, and annual satellite events on different continents were added. The main event, held annually at Stanford gathers over 300 technically savvy attendees. XLDB is one of the premier database events catered towards both academic and industrial communities.\n\n== Goals ==\n\nThe main goals of this community include:<ref>{{ cite web | url=http://www-conf.slac.stanford.edu/xldb09/docs/xldb09_welcomeTalk.ppt | year=2009 | last=Becla| first=Jacek | title=XLDB 3 Welcome | accessdate=2009-08-29 }}</ref>\n\n* Identify trends, commonalities and major roadblocks related to building extremely large databases\n* Bridge the gap between users trying to build extremely large databases and database solution providers worldwide\n* Facilitate development and growth of practical technologies for extremely large data stores\n\n== XLDB Community ==\nAs of 2013, the community consisted of about a thousand members including:\n# Scientists who develop, use, or plan to develop or use XLDB for their research, from laboratories.\n# Commercial users of XLDB.\n# Providers of database products, including commercial vendors and representatives from open source database communities.\n# Academic database researchers.\n\n== XLDB Conferences, Workshops and Tutorials ==\nThe community meets annually at [[Stanford]] where the main event is held each fall, usually in September. These who live too far from California to attend have the opportunity to attend satellite events, organized annually around May/June either in [[Asia]] or in [[Europe]].\n\nA detailed report is produced after each workshop.\n\n{| class="wikitable"\n|-\n! Year\n! Place\n! Link\n! Report\n! Comments\n|-\n| 2015\n| [[Stanford]]\n| [https://web.archive.org/web/20150521105100/http://www-conf.slac.stanford.edu/xldb2015/]\n|\n| 8th XLDB Conference\n|-\n| 2014\n| [http://www.on.br/ Observatório Nacional], [[Rio_de_Janeiro]]\n| [https://web.archive.org/web/20150219081443/http://xldb-rio2014.linea.gov.br/]\n|\n| Satellite XLDB Workshop in South America\n|-\n| 2014\n| [[Stony_Brook_University]]\n| [https://web.archive.org/web/20150521052839/http://www3.cs.stonybrook.edu/~xldb/]\n|\n| XLDB-Healthcare Workshop\n|-\n| 2013\n| [[Stanford]]\n| [https://conf-slac.stanford.edu/xldb-2013/]\n|\n| 7th XLDB Conference\n|-\n| 2013\n| [[CERN]], [[Geneva]]/[[Switzerland]]\n| [http://xldb-europe-workshop-2013.web.cern.ch/]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n|\n| Satellite XLDB Workshop in Europe\n|-\n| 2012\n| [[Stanford]]\n| [http://www-conf.slac.stanford.edu/xldb2012/]\n| [http://www.jstage.jst.go.jp/article/dsj/12/0/12_12_023/_pdf]\n| 6th XLDB Conference, Workshop & Tutorials\n|-\n| 2012\n| [[Beijing]], [[China]]\n| [https://web.archive.org/web/20120708164351/http://idke.ruc.edu.cn/xldb/www.xldb-asia.org/home.html]\n| [http://www.xldb.org/wp-content/uploads/2012/09/XLDBAsia2012Report.pdf]\n| Satellite XLDB Conference in Asia\n|-\n| 2011\n| [[SLAC]]\n| [https://web.archive.org/web/20110426125951/http://www-conf.slac.stanford.edu/xldb2011/]\n| [http://www.jstage.jst.go.jp/article/dsj/11/0/37/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n| 5th XLDB Conference and Workshop\n|-\n| 2011\n| [[Edinburgh]], [[UK]]\n| [https://web.archive.org/web/20160303221547/http://xldb.eu/xldb_europe_2011/]\n| not available\n| Satellite XLDB Workshop in Europe\n|-\n| 2010\n| [[SLAC]]\n| [https://web.archive.org/web/20110727234052/http://www-conf.slac.stanford.edu/xldb2010/]\n| [http://www.jstage.jst.go.jp/article/dsj/9/0/9_MR1/_article]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n| 4th XLDB Conference and Workshop\n|-\n| 2009\n| [[Lyon]], [[France]]\n| [https://web.archive.org/web/20110727234623/http://www-conf.slac.stanford.edu/xldb2009/]\n| [http://www.jstage.jst.go.jp/article/dsj/8/0/MR1/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n| 3rd XLDB Workshop\n|-\n| 2008\n| [[SLAC]]\n| [https://web.archive.org/web/20110727234818/http://www-conf.slac.stanford.edu/xldb2008/]\n| [http://www.jstage.jst.go.jp/article/dsj/7/0/196/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n| 2nd XLDB Workshop\n|-\n| 2007\n| [[SLAC]]\n| [https://web.archive.org/web/20110727235121/http://www-conf.slac.stanford.edu/xldb2007/]\n| [http://www.jstage.jst.go.jp/article/dsj/7/0/1/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n| 1st XLDB Workshop\n|}\n\n== Tangible results ==\n\nThe XLDB events led to initiating the effort of building a new open source, science database, [https://web.archive.org/web/20090220121225/http://scidb.org/ SciDB].<ref>{{ cite web | url=http://www.jstage.jst.go.jp/article/dsj/7/0/88/_pdf  | year=2008 | last=Becla| first=Jacek | title=Report from the SciDB Workshop |accessdate=2008-09-29}}{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\nThe XLDB organizers started defining a [http://www.xldb.org/science-benchmark/ science benchmark] for scientific data management systems called SS-DB.\n\nAt [http://xldb.org/2012|XLDB 2012]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} the XLDB organizers announced that two major databases that support arrays as first-class objects ([[MonetDB]] SciQL and [[SciDB]]) have formed a working group in conjunction with XLDB. This working group is proposing a common syntax (provisionally named “ArrayQL”) for manipulating arrays, including array creation and query.\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n* Pavlo A., Paulson E., Rasin A., Abadi D. J., Dewitt D. J., Madden S., and Stonebraker M., \'\'A Comparison of Approaches to Large-Scale Data Analysis," Proceedings of the 2009 ACM SIGMOD, http://web.archive.org/web/20090611174944/http://database.cs.brown.edu:80/sigmod09/benchmarks-sigmod09.pdf\n* Becla, J., et al. 2006, \'\'Designing a multi-petabyte database for LSST,\'\' http://arxiv.org/abs/cs/0604112\n* Becla, J., & Wang, D. L. 2005, \'\'Lessons Learned from Managing a Petabyte\'\', downloaded from http://web.archive.org/web/20110604223735/http://www.slac.stanford.edu/pubs/slacpubs/10750/slac-pub-10963.pdf on 2007-11-25.\n* Bell, G., Gray, J., & Szalay, A. 2005, \'\'Petascale computations systems: Balanced cyberinfrastructure in a data-centric world,\'\' http://arxiv.org/abs/cs/0701165\n* Duellmann, D. 1999, \'\'Petabyte Databases\'\', ACM SIGMOD Record, vol. 28, p. 506, http://web.archive.org/web/20071012015357/http://www.sigmod.org/sigmod/record/issues/9906/index.html#TutorialSessions.\n* Hanushevsky, A., & Nowak, M. 1999, \'\'Pursuit of a Scalable High Performance Multi-Petabyte Database\'\', 16th IEEE Symposium on Mass Storage Systems, pp. 169–175, http://citeseer.ist.psu.edu/217883.html.\n* Shiers, J., \'\'Building Very Large, Distributed Object Databases\'\', downloaded from http://web.archive.org/web/20070915101842/http://wwwasd.web.cern.ch:80/wwwasd/cernlib/rd45/papers/dbprog.html on 2007-11-25.\n\n[[Category:Types of databases]]\n[[Category:Data management]]']
['Category:Databases', '2276471', "{{Commonscat|Databases}}\n{{distinguish|Category:Database software}}\n\n:*'''[[Database]]s'''\n\n{{clr}}\n::{{Cat main|Database}}\n:::::::{{cat see also|Digital libraries}}\n\n{{catdiffuse}}\n\n{{Database}}\n{{Databases}}\n\n[[Category:Computer data]]\n[[Category:Data management]]\n[[Category:Information retrieval systems]]"]
['Integration competency center', '15014956', '{{ad|date=March 2014}}\n{{peacock|date=March 2014}}\nThe \'\'\'integration competency center\'\'\' (ICC), sometimes referred to as an \'\'\'integration center of excellence\'\'\' (COE), is a [[shared services|shared service]] function within an organization, particularly large corporate enterprises as well as public sector institutions, for performing methodical [[data integration]], [[system integration]] or [[enterprise application integration]]. \n\n[[Data integration]] allows companies to access their enterprise data and functions, fragmented across disparate systems, in order to create a combined, accurate, and consistent view of their core information as well as process assets and leverage them across the enterprise to drive business decisions and operations.  System integration is the bringing together of component subsystems into one system and ensuring that they function together effectively. Enterprise application integration enables efficient information exchanges and business process automation across separate computer applications in a [[Cohesion (computer science)|cohesive]] fashion.\n\n== Overview ==\nThe term may be better understood by examining each of the three words that comprise the acronym. \'\'\'Integration\'\'\' refers to the objective of the ICC to take a holistic perspective and optimize certain qualities such as cost efficiency, organizational agility and effectiveness, operational risk, customer (internal or external) experience, etc. across multiple functional groups. \'\'\'Competency\'\'\' refers to the expertise, knowledge or capability that the ICC offers as services.  \'\'\'Center\'\'\' means that the service is managed or coordinated from a common (central) point independent from the functional areas that it supports. \n\nLarge organizations are usually sub-divided into functional areas such as marketing, sales, distribution, finance, human resources to name just a few. These functional groups have separate operations and are \'\'vertically integrated\'\' and are therefore sometimes referred to as "silos" or "stovepipes".  From an organizational perspective, an ICC is a group of people with special skills, who are centrally coordinated, and offer services to accomplish a mission that requires separate functional areas to work together. \n\nKey objectives of an ICC are:\n\n* Lead and support enterprise integration (data, system and process) projects with the cooperation/coordination of subject matter experts\n* Promote Enterprise integration as a formal discipline. For example, data integration will include [[data warehousing]], [[data migration]], [[data quality]] management, data integration for [[service oriented architecture]] deployments, and [[data synchronization]]. Similar system integration will include common messaging services, business service virtualization etc.\n* Develop staff specialists in integration processes and operations and leverage their expertise company-wide\n* Assess and select integration technology and tools from the marketplace\n* Manage integration pilots and projects across the organization\n* Optimize integration investments across the enterprise level\n* Leverage economies of scale for the integration tools portfolio at enterprise level\n\nICCs allow companies to:\n* Optimize scarce resources by combining integration skills, resources, and processes into one group\n* Reduce project delivery times and development and maintenance costs through effectiveness and efficiency\n* Improve ROI through creation and reuse of enterprise assets like source definitions, application interfaces, and codified business rules\n* Decrease duplication of integration related effort across the enterprise\n* Build on past successes instead of reinventing the wheel with each project\n* Lower total technology cost of ownership by leveraging technology investments across multiple projects\n\nAn ICC may be a temporary group in support of a [[program management|program]] or a permanent part of the organization.  Furthermore, ICC’s can be established at various scales or levels; within a division of a company, at the enterprise level, or across multiple companies in a supply chain.\n\n== History ==\nThe term "integration competency center" and its acronym ICC was popularized by Roy Schulte of [[Gartner]] in a series of articles and conference presentations beginning in 2001 with \'\'The Integration Competency Center\'\' {{citation missing|date=February 2013}}<!--[Ref SPA-14-0456] mostly useless, cannot be found anywhere-->. He picked up the term from one of his colleagues, Gary Long, who found some of his clients using it (they took the established term “competency center” and applied it to integration). Prior to that (from 1997 to 2001) Gartner had been referring to it as the \'\'central integration team\'\'. The concept itself (even before it was given a label) goes back to 1996 in one of Gartner’s first reports on integration. {{citation missing|date=February 2013}}\n\nA major milestone was the publication in 2005 of the first book on the topic: \'\'Integration Competency Center: An Implementation Methodology\'\'<ref name="ICC">John G. Schmidt and David Lyle (2005), \'\'Integration Competency Center, An Implementation Methodology,\'\' ISBN 0-9769163-0-4</ref> by \'\'John G. Schmidt\'\' and \'\'David Lyle\'\'. The book introduced five ICC organizational models and explored the people, process and technology dimensions of ICC’s.  Several reviews of the book can be found at [http://blogs.ittoolbox.com/eai/business/archives/soa-competency-center-5731  IT Toolbox] and at [http://www.amazon.com/Integration-Competency-Center-Implementation-Methodology/dp/0976916304 Amazon]. The concept of integration as a competency in the IT domain has now survived for over 10 years and appears to be picking up momentum and broad-based acceptance. \n\nThese days, ICC’s are often called, integration center of excellence, SOA center of excellence, the data management center of excellence and other variants. The most advanced ICC\'s are using [[Lean Integration]] practices to optimize end-to-end processes and to drive continuous improvements. Universities are also beginning to include integration topics in their MBA programs and computer science curricula. For example, The College of Information Sciences and Technology at Penn State University has established a [http://ist.psu.edu/facultyresearch/facilities/eii/  Enterprise Informatics and Integration Center] with the following mission:\n\n"\'\'The Enterprise Informatics and Integration Center (EI²) will actively engage industry, non-profit, and government agency leaders to address critical issues in enterprise processes, knowledge management, and decision making.\'\'"\n\n== Operating models ==\nThere are a number of ways an ICC can be organized and a wide range of responsibilities with which it can be chartered. The ICC book<ref name="ICC"/> introduced five ICC organizational models and explored the people, process and technology dimensions of ICCs. They include:\n\n=== Best practices ICC ===\nThe primary function of this ICC model is to document best practices. It does not include a central support or development team to implement those standards across projects, and probably not metadata either. To implement a best practices ICC, companies need a flexible development environment that supports diverse teams and that enables the team to enhance and extend existing systems and processes. Such a team might be a subset of an existing enterprise architecture capability and generally consists of a small number of staff (1-5).\n\n=== Standard services ICC ===\nA standard services ICC provides the same knowledge leverage as a best practices ICC, but enforces technical consistency in software development and hardware choices. A standard services ICC focuses on processes, including standardizing and enforcing naming conventions, establishing metadata standards, instituting change management procedures, and providing standards training. This type of ICC also reviews emerging technologies, selects vendors, and manages hardware and software systems.  This style of ICC is often tightly linked with the enterprise architecture team and may be slightly larger than a typical best practices ICC.\n\n=== Shared services ICC ===\nA shared services ICC provides a supported technical environment and services ranging from development support all the way through to a help desk for projects in production. This type of ICC is significantly more complex than a best practices or Standard Services model. It establishes processes for knowledge management, including product training, standards enforcement, technology benchmarking, and metadata management, and it facilitates impact analysis, software quality, and effective use of developer resources across projects. The organizational structure of a Shared Services ICC is sometimes referred to as a hybrid or federated model which often includes a small central coordinating team plus dotted-line reporting relationships with multiple distributed teams.\n\n=== Central services ICC ===\nA central services ICC controls integration across the enterprise. It carries out the same processes as the other models, but in addition usually has its own budget and a charge-back methodology. It also offers more support for development projects, providing management, development resources, [[data profiling]], data quality, and unit testing. Because a central services ICC is more involved in development activities than the other models, it requires a production operator and a data integration developer.  The staff in a central services ICC does not necessarily need to be a central location and may be distributed geographically; the important distinction is that the staffs have a solid-line reporting relationship to the ICC Director.  The size of these teams can vary and may be as large as 10%-15% of the IT staff in an organization.\n\n=== Self service ICC ===\nThe self-service ICC represents the highest level of maturity in an organization.  The ICC itself may be almost invisible in that its functions are so ingrained in the day-to-day systems development life-cycle and its operations are so tightly integrated with the infrastructure that it may require only small central team to sustain itself.  This ICC model achieves both a highly efficient operation and provides an environment where independent development and innovation can flourish. This goal is achieved by strict enforcement of a set of application integration standards through automated processes enabled by tools and systems.\n\n== Key challenges ==\nICC as a concept is fairly simple. It is embodiment of the IT management best practices to deliver shared services. However, being an organizational concept, it is far more challenging to implement in practice than the conceptual view because every organization has different DNA and it takes specific personalization/customization effort for ICC that makes the ICC initiative successful. Here are some of the common challenges in ICC establishment journey:\n* Change management in terms of technology, processes, organization structure\n* Ability of the organization to deal with the pace and quantum of change\n* Alignment of stakeholders and process owners for ICC strategy\n* Inappropriate ownership level for ICC program and lack of senior management sponsorship\n* Highly tactical focus and business program level constraints\n* Ignoring foundation elements and jumping to implementation directly\n* Inappropriate funding\n\nThese issues are important to consider when embarking on the ICC investment since the last leg of the implementation of ICC that\'s what matters most. Intellectual definition of ICC that is not implemented in the organisation has no real value for the enterprise.\n\n== See also ==\n* [[Lean Integration]]\n\n== References ==\n<references/>\n\n== Further reading ==\n* Maurizio Lenzerini (2002). "Data Integration: A Theoretical Perspective". PODS 2002: 243-246.\n\n== External links ==\n* Integration Competency Center book: (http://www.amazon.com/Integration-Competency-Center-Implementation-Methodology/dp/0976916304)\n* [[Informatica]] ICC Blog: (http://blogs.informatica.com/perspectives/category/data-integration/integration-competency-centers/)\n* Gartner paper  (http://www.ebizq.net/topics/tech_in_biz/features/5360.html)\n* Integration Consortium: (http://www.integrationconsortium.org)\n* Infosys ICC Blogs (http://www.infosysblogs.com/bpm-eai/integration_competency_center_icc)\n* ICC Handbook (http://www.unthink.fi/Global/PDF/ICC-Handbook.pdf)\n* Integration Warstories - article about avoiding ICC pitfalls (http://integrationwarstories.com/2013/10/25/avoiding-pitfalls-of-integration-competency-centers/)\n\n[[Category:Data management]]\n[[Category:Software development philosophies]]\n[[Category:Information technology]]']
['Cognos ReportNet', '16942788', '{{Infobox Software\n| name = Cognos ReportNet\n| logo = \n| screenshot = \n| caption = \n| author = [[Cognos]], an [[IBM]] Company\n| developer = \n| released = September, 2003\n| latest release version = Cognos ReportNet 1.3\n| latest release date = \n| latest preview version = \n| latest preview date = \n| operating system = Multiple\n| platform = Multiple\n| language = Multi-lingual\n| status = \'\'\'Inactive\'\'\'<ref>[http://www-01.ibm.com/software/analytics/cognos/products/reportnet/ IBM.com, \'\'Cognos ReportNet : now part of Cognos Enterprise\'\', consulté le 12 février 2014]</ref>\n| genre = [[Business Intelligence]]\n| license = \n| website = [http://www-01.ibm.com/software/data/cognos/products/reportnet/ IBM.com]\n}}\n\n\'\'\'Cognos ReportNet (CRN)\'\'\' is a web-based [[software]] product for creating and managing [[ad hoc]] and custom-made reports. ReportNet is developed by the [[Ottawa]]-based  company [[Cognos]] (formerly Cognos Incorporated), an [[IBM]] company. The web-based reporting tool was launched in September 2003. Since IBM\'s acquisition of Cognos, ReportNet has been renamed \'\'IBM Cognos ReportNet\'\' like all other Cognos products.\n\nReportNet uses web services standards such as [[XML]] and [[Simple Object Access Protocol]] and also supports dynamic [[HTML]] and [[Java (programming language)|Java]].<ref>[https://web.archive.org/web/20080312025955/http://www.vnunet.com/vnunet/news/2123232/bear-sterns-chooses-cognos-reportnet Cognos ReportNet in news]</ref> ReportNet is compatible with multiple databases including [[Oracle Database|Oracle]], [[SAP AG|SAP]], [[Teradata]], [[Microsoft SQL server]], [[IBM DB2|DB2]] and [[Sybase]].<ref>[http://www.cognos.com/solutions/data/ibm/advantages.html Data sources]</ref><ref>[http://support.cognos.com/en/support/products/crn101_software_environments.html CRN Environment details]</ref> The product provides interface in over 10 languages,<ref>[http://www.cognos.com/products/business_intelligence/reporting/features.html CRN Features]</ref> has Web Services architecture to meet the needs of multi-national, diversified enterprises and helps reduce total cost of ownership. Multiple versions of Cognos ReportNet have since been released by the company. Cognos ReportNet was awarded the [[Software and Information Industry Association]] (SIIA) 2005 [[Codie Awards]] for the "Best Business Intelligence or Knowledge Management Solution" category.<ref>[http://www.mywire.com/pubs/PRNewswire/2005/06/08/885642?extID=10051 Cognos ReportNet wins award]</ref> CRN\'s capabilities have been further used in [[IBM Cognos 8 Business Intelligence|IBM Cognos 8 BI (2005)]], the latest reporting tool.<ref>[http://www.cognos.com/products/cognos8businessintelligence Cognos 8 BI]</ref> CRN comes with its own [[software development kit]] (SDK).\n\n==Launch==\nEarly adopters of Cognos ReportNet for their corporate reporting needs included [[Bear Stearns]], [[BMW]] and [[Alfred Publishing]]. Around this same time of launch, Cognos competitor [[Business Objects]] released version 6.1 of its enterprise reporting tool. Cognos ReportNet has been successful since its launch, raising revenues in 2004 from licensing fees.<ref>[http://www.highbeam.com/doc/1G1-131525446.html Cognos ReportNet delivers $30 Million in License Revenue in one Quarter]</ref> Subsequently, other major corporations like [[McDonald\'s]] adopted Cognos ReportNet.<ref>[http://www.ebizq.net/news/5538.html ReportNet and fries]</ref>\n\n==Controversy==\nCognos rival [[Business Objects (company)|Business Objects]] announced in 2005 that BusinessObjects XI significantly outperformed Cognos ReportNet in benchmark tests conducted by VeriTest, an independent software testing firm. The tests performed showed Cognos ReportNet performed poorly when processing styled reports, complex business reports and combination of both.<ref>[http://www.crm2day.com/news/crm/114773.php BO XI Vs Cognos ReportNet]</ref> The tests reported a massive 21 times higher report throughput for BusinessObjects XI than Cognos ReportNet at capacity loads.<ref>[http://goliath.ecnext.com/coms2/summary_0199-4404821_ITM BO XI outperforms Cognos ReportNet]</ref> Cognos soon dismissed the claims by stating Business Objects dictated the environment and testing criteria and Cognos did not provide the software to participate in benchmark test.<ref>[http://www.cognos.com/news/releases/2005/0624_3.html Cognos dismisses the Test results]</ref> Cognos later performed their own test to demonstrate Cognos ReportNet capabilities.<ref>[http://www.cognos.com/pdfs/whitepapers/wp_cognos_reportnet_scalability_benchmakrs_ms_windows.pdf Cognos scalability results]</ref>\n\n==Components==\n* Cognos Report Studio – A Web-based product for creating complex professional looking reports.<ref>[http://web.princeton.edu/sites/datamall/documents/ug_cr_rptstd.pdf Refer definition in introduction page]</ref>\n* Cognos Query Studio - A Web-based product for creating ad-hoc reports.<ref>[http://web.princeton.edu/sites/datamall/documents/ug_cr_qstd.pdf Refer Introduction page]</ref>\n* Cognos Framework Manager – A [[metadata modeling]] tool to create BI metadata for reporting and dashboard applications.<ref>[http://www.cognos.com/products/framework_services Framework Manager Services] {{webarchive |url=https://web.archive.org/web/20080417030129/http://www.cognos.com/products/framework_services |date=April 17, 2008 }}</ref>\n* Cognos Connection – Main [[Enterprise portal|portal]] used to access reports, schedule reports and perform administrator activities.<ref>[http://web.princeton.edu/sites/datamall/documents/ug_cr_qstd.pdf Refer page9]</ref>\n\n==Versions==\n* Cognos ReportNet 1.1 – [[Java EE]]-style professional web-based authoring tool. (base version)\n* Cognos ReportNet IBM Special Edition – comes with an embedded version of [[IBM WebSphere]] as its application server and [[IBM DB2]] as its data store.\n* Cognos Linux – for Intel-based [[Linux]] platforms.<ref>[http://www.ebizq.net/news/5688.html ReportNet on Linux]</ref>\n\n==See also==\n*[[IBM Cognos Business Intelligence]]\n\n==References==\n{{reflist|30em}}\n\n[[Category:Business intelligence]]\n[[Category:Data management]]\n[[Category:IBM software]]']
['Category:Semantic Web', '18014783', '{{Cat main|Semantic Web}}\n{{Commons cat|Semantic Web}}\n\n[[Category:Internet ages|Web 3]]\n[[Category:World Wide Web Consortium]] <!-- the Semantic Web is a major W3C activity -->\n[[Category:Knowledge representation]]\n[[Category:Data management]]']
['British Oceanographic Data Centre', '19075690', "{{Infobox organization\n|name         = British Oceanographic Data Centre\n|image        = Bodc logo.jpg\n|image_border = \n|size         = 150px\n|alt          = British Oceanographic Data Centre\n|caption      = \n|map          =\n|msize        = \n|malt         = \n|mcaption     = \n|abbreviation = \n|motto        = \n|formation    = 1969\n|extinction   = \n|type         = \n|status       = \n|purpose      = \n|headquarters = \n|location     = [[Liverpool]], [[UK]]<br>\n[[UK postcodes|L3 5DA]]\n|region_served =\n|membership   = \n|language     =\n|leader_title = Head of BODC\n|leader_name  = Dr Graham Allen\n|main_organ   = \n|parent_organization = [[Natural Environment Research Council]] (NERC)\n|affiliations = \n|num_staff    = approx. 500\n|num_volunteers =\n|budget       = \n|website      = {{URL|http://www.bodc.ac.uk/}}\n|remarks      =\n}}\nThe '''British Oceanographic Data Centre''' ('''BODC''') is a national facility for looking after and distributing [[data]] about the [[marine (ocean)|marine]] environment. BODC is the designated marine science data centre for the [[UK]] and part of the [[Natural Environment Research Council]] (NERC). The centre provides a resource for science, education and industry, as well as the general public. BODC is hosted by the [[National Oceanography Centre]] (NOC) — primarily at its facility in [[Liverpool]], with small number of its staff in [[Southampton]].\n\n[[File:Bidston Observatory.jpg|250px|right|thumb|Bidston Observatory, home of BODC from 1975 to 2004.]]\n[[File:Joseph Proudman Building.jpg|250px|right|thumb|Joseph Proudman Building, Liverpool.]]\n\n== History ==\nThe origins of BODC go back to 1969 when NERC created the '''British Oceanographic Data Service''' ('''BODS'''). Located at the National Institute of [[Oceanography]], [[Wormley, Surrey|Wormley]] in [[Surrey]], its purpose was to: \n* Act as the UK's National Oceanographic Data Centre\n* Participate in the international exchange of data as part of the [[Intergovernmental Oceanographic Commission]] (IOC) network of national data centres\n\nIn 1975 BODS was transferred to [[Bidston]] Observatory on the [[Wirral Peninsula|Wirral]], near Liverpool, as part of the newly formed Institute of Oceanographic Sciences. The following year BODS became the Marine Information and Advisory Service (MIAS)[http://www.soton.ac.uk/library/about/nol/mias.html]. Its primary activity was to manage the data collected from weather ships, [[oil rigs]] and [[Data Buoys|data buoys]].\nThe data banking component of MIAS was restructured to form BODC in April 1989. Its mission was to 'operate as a world-class data centre in support of UK marine science'. BODC pioneered a start to finish approach to marine data management. This involved:\n* Assisting in the collection of data at sea\n* Quality control of data\n* Assembling the data for use by the scientists\n* The publication of data sets on [[CD-ROM]]\nIn December 2004, BODC moved to the purpose-built [[Joseph Proudman]] Building on the campus of the [[University of Liverpool]]. A small number of its staff are based in the [[National Oceanography Centre]] (NOC), Southampton.\n\n== Aims ==\n* Work alongside scientists during marine research projects\n* Provide quality control and archiving of oceanographic data\n* Maintain an online source of information and improve public access to marine data\n* Provide innovative marine data products\n\n== National role ==\n[[File:Current meter inventory.jpg|250px|right|thumb|BODC [[current meter]] data holdings from around the UK.]]\nBODC is one of six designated data centres that manage NERC's environmental data and has a number of national roles and responsibilities:\n* Performing data management for NERC-funded marine projects\n* Maintaining and developing its archive of marine data, the '''National Oceanographic Database''' ('''NODB''')\n* Managing, checking and archiving data from [[tide gauge]]s around the UK coast for the [[UK National Tide Gauge Network|National Tide Gauge Network]], which aims to obtain high quality [[tidal]] information and to provide warning of possible flooding of coastal areas around the British Isles. This  is part of the [[National Tidal and Sea Level Facility|National Tidal & Sea Level Facility]] (NTSLF)\n* Hosting the Marine Environmental Data and Information Network ([http://www.oceannet.org/ MEDIN])\n* Working in partnership with other NERC marine research centres:\n** [[British Antarctic Survey]] (BAS)\n** [[National Oceanography Centre]] (NOC), Liverpool, formerly [[Proudman Oceanographic Laboratory]] (POL)\n** [[National Oceanography Centre]] (NOC), Southampton\n** [[Plymouth Marine Laboratory]] (PML)\n** [[Scottish Association for Marine Science]] (SAMS)\n** [[Sea Mammal Research Unit]] (SMRU)\n\n== International role ==\nBODC's international roles and responsibilities include:\n* Contributing to the [[International Council for the Exploration of the Sea]] (ICES) Marine Data Management\n* Creating, maintaining and publishing the [[General Bathymetric Chart of the Oceans]] (GEBCO) Digital Atlas\n* BODC is one of over 60 national oceanographic data centres that form part of the IOC [[International Oceanographic Data and Information Exchange]] (IODE)\n\n==Projects and initiatives==\nThe following are a selection of the projects that BODC is or has been involved with:\n:[[Image:RAPID mooring.JPG|250px|right|thumb|Servicing of a RAPID mooring.]]\n* '''Atlantic Meridional Transect (AMT)'''\n:The AMT programme [http://www.bodc.ac.uk/projects/uk/amt/] undertook a twice yearly [[transect]] between the UK and the [[Falkland Islands]] to study the factors determining the [[ecological]] and [[biogeochemical]] variability in the [[planktonic]] [[ecosystems]].\n* '''Autosub Under Ice (AUI)'''\n:The AUI programme [http://www.bodc.ac.uk/projects/uk/aui/] investigated the role of sub-ice shelf processes in the [[climate]] system. The marine environment beneath floating [[ice shelves]] was explored using Autosub, an [[Autonomous_underwater_vehicle|AUV]].\n* '''Marine Productivity (MarProd)'''\n:MarProd [http://www.bodc.ac.uk/projects/uk/marprod/] helped to develop coupled [[Computer simulation|models]] and observation systems for the [[pelagic]] ecosystem, with emphasis on the physical factors affecting [[zooplankton]] dynamics.\n* '''Rapid Climate Change (RAPID)'''\n:The RAPID programme [http://www.bodc.ac.uk/projects/uk/rapid/] aimed to improve understanding of the causes of sudden changes in the Earth's climate.\n* '''Ocean Margin Exchange (OMEX)'''\n:The OMEX project [http://www.bodc.ac.uk/projects/european/omex/] studied, measured and modelled the physical, chemical and biological processes and fluxes at the ocean margin - the interface between the open [[Atlantic ocean]] and the European [[continental shelf]].\n* '''SeaDataNet'''\n:[[SeaDataNet]] [http://www.bodc.ac.uk/projects/european/seadatanet/] aims to develop a [[standardised]], distributed system providing transparent access to marine data sets and data products from countries in and around [[Europe]].\n*'''System of Industry Metocean data for the Offshore and Research Communities (SIMORC)'''\n:SIMORC [http://www.bodc.ac.uk/projects/european/simorc/] aimed to create a central index and database of [[metocean]] data sets collected globally by the oil and gas industry.\n*'''Vocabulary Server'''\n:BODC operates the NERC Vocabulary Server Web Service [http://www.bodc.ac.uk/products/web_services/vocab/], which provides access to [[Controlled_vocabulary|controlled vocabularies]] of relevance to the scientific community.\n\n== External links ==\n* [http://www.bodc.ac.uk BODC homepage]\n* [http://www.bodc.ac.uk/about/news_and_events/ BODC News and events]\n* [http://www.nerc.ac.uk Natural Environment Research Council (NERC) homepage]\n* [http://www.nerc.ac.uk/research/sites/data/ NERC Data centres]\n* [http://ndg.nerc.ac.uk/discovery NERC Data Discovery Service]\n* [http://www.ntslf.org/ National Tidal and Sea Level Facility (NTSLF)]\n\n{{coord|53|24|27.5|N|2|58|8.2|W|type:landmark|display=title}}\n\n[[Category:Oceanographic organizations]]\n[[Category:Scientific organisations based in the United Kingdom]]\n[[Category:Data management]]\n[[Category:Oceanography]]\n[[Category:Marine biology]]\n[[Category:Marine geology]]\n[[Category:Environmental science]]\n[[Category:Environment of the United Kingdom]]\n[[Category:Public bodies and task forces of the United Kingdom government]]\n[[Category:1969 establishments in the United Kingdom]]\n[[Category:Scientific organizations established in 1969]]\n[[Category:Organisations based in Liverpool]]"]
['Electronically stored information (Federal Rules of Civil Procedure)', '19675044', '\'\'\'Electronically stored information\'\'\' (\'\'\'ESI\'\'\'), for the purpose of the [[Federal Rules of Civil Procedure]] (FRCP) is information created, manipulated, communicated, stored, and best utilized in digital form, requiring the use of computer hardware and software.<ref name="nwjtip">[http://www.law.northwestern.edu/journals/njtip/v4/n2/3 \'\'Electronically Stored Information: The December 2006 Amendments to the Federal Rules of Civil Procedure\'\'], Kenneth J. Withers, Northwestern Journal of Technology and Intellectual Property, Vol.4 (2), 171</ref>\n\nESI has become a legally defined phrase as the [[Federal government of the United States|U.S. government]] determined for the purposes of the FRCP rules of 2006 that promulgating procedures for maintenance and discovery for electronically stored information was necessary.  References to “electronically stored information” in the Federal Rules of Civil Procedure (FRCP) invoke an expansive approach to what may be discovered during the fact-finding stage of civil litigation.<ref name="Federal Rules of Civil Procedure">{{cite web|title=Federal Rules of Civil Procedure (FRCP)|url=https://www.law.cornell.edu/rules/frcp/rule_34|website=Legal Information Institute [LII]|publisher=Cornell University Law School|accessdate=October 31, 2015|ref=Rule 34}}</ref>\n\nRule 34(a) enables a party in a civil lawsuit to request another party to produce and permit the requesting party or its representative to inspect, copy, test, or sample the following items in the responding party\'s possession, custody, or control:\n\n<blockquote>any designated documents or electronically stored information—including writings, drawings, graphs, charts, photographs, sound recordings, images, and other data or data compilations—stored in any medium from which information can be obtained either directly or, if necessary, after translation by the responding party into a reasonably usable form...Rule 34(a)(1) is intended to be broad enough to cover all current types of computer-based information, and flexible enough to encompass future changes and developments.</blockquote>\n\n==Types==\n\n===Native files===\nThe term \'\'native files\'\' refers to user-created documents, which could be in [[Microsoft Office]] or [[Apache OpenOffice|Open Office]] document formats as well as other files stored on computer, but could include video surveillance footage saved on a computer hard drive, [[Computer-aided design]] files such as [[blueprint]]s or maps, [[digital photography|digital photographs]], scanned images, [[archive file]]s, e-mail, and [[digital audio]] files, among other data,\n\n===Logical data===\nA judge ruled that [[Random Access Memory|RAM]] is reasonably accessible and retainable for anticipation of litigation.{{Citation needed|date=November 2011}}\n\nIn Australia RAM, can be used in litigation post 1996.\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite book|chapter=Meet the New Rules|title=The Discovery Revolution|author1=George L. Paul |author2=Bruce H. Nearon |publisher=American Bar Association|year=2006|isbn=9781590316054}}\n* {{cite book|title=Discovery of Electronically Stored Information|author=Ronald J. Hedges|publisher=BNA Books|year=2007|isbn=9781570186721}}\n* {{cite book|title=The Sedona Principles 2007: Best Practices Recommendations &amp; Principles for Addressing Electronic Document Production|author=Jonathan M. Redgrave|publisher=BNA Books|year=2007|isbn=9781570186776}}\n* {{cite book|title=Cyber Forensics|author1=Albert J. Marcella |author2=Albert J. Marcella Jr. |author3=Doug Menendez |chapter=Electronically stored information and cyber forensics|publisher=CRC Press|year=2007|isbn=9780849383281}}\n* {{cite book|title=Litigating With Electronically Stored Information|author1= Marian K. Riedy |author2=Suman Beros |author3= Kim Sperduto |publisher=Artech House Telecommunications Library|year=2007|isbn=9781596932203}}\n\n[[Category:Computer data]]\n[[Category:Data management]]\n[[Category:United States discovery law]]\n[[Category:Records management]]\n\n\n{{US-law-stub}}']
['Parchive', '526495', '{{Merge from|QuickPAR|date=March 2014}}\n\n{{Infobox file format\n| name = Parchive\n| extension = .par, .par2, .p??, (.par3 future)\n| mime =\n| owner =\n| creatorcode =\n| genre = [[Erasure code]]\n| containerfor =\n| containedby =\n| extendedfrom =\n| extendedto =\n}}\n\n\'\'\'Parchive\'\'\' (a [[portmanteau]] of \'\'\'parity archive\'\'\', and formally known as \'\'\'Parity Volume Set Specification\'\'\'<ref>[https://www.livebusinesschat.com/smf/index.php?topic=5736.msg38234#msg38234 Re: Correction to Parchive on Wikipedia], reply #3, by Yutaka Sawada: "Their formal title are "Parity Volume Set Specification 1.0" and "Parity Volume Set Specification 2.0."</ref>) is an [[erasure code]] system that produces \'\'\'par\'\'\' files for [[checksum]] verification of [[data integrity]], with the capability to perform [[data recovery]] operations that can repair or regenerate corrupted or missing data. \n\nParchive was originally written to solve the problem of reliable file sharing on [[Usenet]],<ref>{{cite web\n| url         = http://parchive.sourceforge.net/#desc\n| title       = Parchive: Parity Archive Volume Set\n| accessdate  = 2009-10-29\n| quote       = The original idea behind this project was to provide a tool to apply the data-recovery capability concepts of RAID-like systems to the posting and recovery of multi-part archives on Usenet.\n}}</ref> but it is now commonly used for protecting any kind of data from [[data corruption]], [[disc rot]], [[data degradation|bit rot]], and accidental or malicious damage. Despite the name, Parchive uses more advanced techniques that do not utilize simplistic [[Parity bit|parity]] methods of [[error detection and correction]].\n\nAs of 2014, \'\'\'PAR1\'\'\' is obsolete, \'\'\'PAR2\'\'\' is mature for widespread use, and \'\'\'PAR3\'\'\' is an experimental version being developed by MultiPar author Yutaka Sawada.<ref>[http://www.livebusinesschat.com/smf/index.php?topic=5098.0 possibility of new PAR3 file]</ref><ref>[http://www.livebusinesschat.com/smf/index.php?topic=3339.0 Question about your usage of PAR3]</ref><ref>[http://www.livebusinesschat.com/smf/index.php?topic=5025.msg29912;topicseen#msg29912 Risk of undetectable intended modification]</ref><ref>[http://www.livebusinesschat.com/smf/index.php?topic=3527.msg8850;topicseen#msg8850 PAR3 specification proposal not finished as of April 2011]</ref>  The original SourceForge Parchive project has been inactive since November 9, 2010.<ref>{{cite web |url = http://sourceforge.net/projects/parchive/ |title = Parchive: Parity Archive Tool |accessdate = 2012-09-02}}</ref> \n\n== History ==\nParchive was intended to increase the reliability of transferring files via Usenet [[newsgroup]]s. Usenet was originally designed for informal conversations, and the underlying protocol, [[NNTP]] was not designed to transmit arbitrary binary data. Another limitation, which was acceptable for conversations but not for files, was that messages were normally fairly short in length and limited to 7-bit [[ASCII]] text.<ref>{{cite IETF\n| title       = Network News Transfer Protocol\n| rfc         = 977\n| sectionname = Character Codes\n| section     = 2.2\n| page        = 5\n| last1       = Kantor\n| first1      = Brian\n| authorlink1 =\n| last2       = Lapsley\n| first2      = Phil\n| authorlink2 = Phil Lapsley\n| year        = 1986\n| month       = February\n| publisher   = [[Internet Engineering Task Force|IETF]]\n| accessdate  = 2009-10-29\n}}</ref>\n\nVarious techniques were devised to send files over Usenet, such as [[uuencode|uuencoding]] and [[Base64]]. Later Usenet software allowed  8 bit [[Extended ASCII]], which permitted new techniques like [[yEnc]]. Large files were broken up to reduce the effect of a corrupted download, but the unreliable nature of Usenet remained.\n\nWith the introduction of Parchive, parity files could be created that were then uploaded along with the original data files. If any of the data files were damaged or lost while being propagated between Usenet servers, users could download parity files and use them to reconstruct the damaged or missing files. Parchive included the construction of small index files (*.par in version 1 and *.par2 in version 2) that do not contain any recovery data. These indexes contain [[hash function|file hash]]es that can be used to quickly identify the target files and verify their integrity.\n\nBecause the index files were so small, they minimized the amount of extra data that had to be downloaded from Usenet to verify that the data files were all present and undamaged, or to determine how many parity volumes were required to repair any damage or reconstruct any missing files. They were most useful in version 1 where the parity volumes were much larger than the short index files. These larger parity volumes contain the actual recovery data along with a duplicate copy of the information in the index files (which allows them to be used on their own to verify the integrity of the data files if there is no small index file available).\n\nIn July 2001, Tobias Rieper and Stefan Wehlus proposed the Parity Volume Set specification, and with the assistance of other project members, version 1.0 of the specification was published in October 2001.<ref>{{cite web|url=http://sourceforge.net/docman/display_doc.php?docid=7273&group_id=30568 |title=Parchive: Parity Volume Set specification 1.0 |accessdate=2009-04-07 |last=Nahas |first=Michael |date=2001-10-14 |deadurl=yes |archiveurl=https://web.archive.org/web/20081220184024/http://sourceforge.net/docman/display_doc.php?docid=7273&group_id=30568 |archivedate=December 20, 2008 }}</ref> Par1 used [[Reed–Solomon error correction]] to create new recovery files. Any of the recovery files can be used to rebuild a missing file from an incomplete [[download]].\n\nVersion 1 became widely used on Usenet, but it did suffer some limitations:\n* It was restricted to handle at most 255 files.\n* The recovery files had to be the size of the largest input file, so it did not work well when the input files were of various sizes. (This limited its usefulness when not paired with the proprietary RAR compression tool.)\n* The recovery algorithm had a bug, due to a flaw<ref>{{cite web\n| url         = http://web.eecs.utk.edu/~plank/plank/papers/CS-03-504.html\n| title       = Note: Correction to the 1997 Tutorial on Reed-Solomon Coding\n| accessdate  = 2009-10-29\n| last        = Plank\n| first       = James S.\n|author2=Ding, Ying\n|date=April 2003\n}}</ref> in the academic paper<ref>{{cite web\n| url         = http://web.eecs.utk.edu/~plank/plank/papers/SPE-9-97.html\n| title       = A Tutorial on Reed-Solomon Coding for Fault-Tolerance in RAID-like Systems\n| accessdate  = 2009-10-29\n| last        = Plank\n| first       = James S.\n|date=September 1997\n}}</ref> on which it was based.\n* It was strongly tied to Usenet and it was felt that a more general tool might have a wider audience.\n\nIn January 2002, Howard Fukada proposed that a new Par2 specification should be devised with the significant changes that data verification and repair should work on blocks of data rather than whole files, and that the algorithm should switch to using 16 bit numbers rather than the 8 bit numbers that PAR 1 used. Michael Nahas and Peter Clements took up these ideas in July 2002, with additional input from Paul Nettle and Ryan Gallagher (who both wrote Par1 clients). Version 2.0 of the Parchive specification was published by Michael Nahas in September 2002.<ref>{{cite web\n| url         = http://parchive.sourceforge.net/docs/specifications/parity-volume-spec/article-spec.html\n| title       = Parity Volume Set Specification 2.0\n| accessdate  = 2009-10-29\n| last        = Nahas\n| first       = Michael |author2=Clements, Peter |author3=Nettle, Paul |author4=Gallagher, Ryan\n| date        = 2003-05-11\n}}</ref>\n\nPeter Clements then went on to write the first two Par2 implementations, [[QuickPar]] and par2cmdline. Abandoned since 2004, Paul Houle created phpar2 to supersede par2cmdline. Yutaka Sawada created MultiPar to supersede QuickPar. Sawada maintains par2cmdline to use as MultiPar\'s PAR engine backend.\n\nOn May 10, 2014, Sawada reported a hash collision security problem in par2cmdline (the backend for MultiPar):<ref name="livebusinesschat.com">[https://www.livebusinesschat.com/smf/index.php?topic=5579.0 v1.2.5.3 is public]</ref>\n\n<blockquote>I\'m not sure this problem can be used for DoS attack against automated Par2 usage. If someone has a skill to forge CRC-32, it is possible to make a set of source file and Par2 file, which freeze a Par2 client for several hours.</blockquote>\n\n== Versions ==\nVersions 1 and 2 of the [[file format]] are incompatible. (However, many clients support both.)\n\n=== Parity Volume Set Specification 1.0 ===\nFor Par1, the files \'\'f1\'\', \'\'f2\'\', ..., \'\'fn\'\', the Parchive consists of an index file (\'\'f.par\'\'), which is CRC type file with no recovery blocks, and a number of "parity volumes" (\'\'f.p01\'\', \'\'f.p02\'\', etc.). Given all of the original files except for one (for example, \'\'f2\'\'), it is possible to create the missing \'\'f2\'\' given all of the other original files and any one of the parity volumes. Alternatively, it is possible to recreate two missing files from any two of the parity volumes and so forth.<ref>{{cite book\n| last        = Wang\n| first       = Wallace\n| authorlink  =\n| title       = Steal this File Sharing Book\n| url         = https://books.google.com/books?id=FGfMS5kymmcC&pg=PT183\n| accessdate  = 2009-09-24\n| edition     = 1st\n| date        = 2004-10-25\n| publisher   = [[No Starch Press]]\n| location    = [[San Francisco, California]]\n| isbn        = 1-59327-050-X\n| pages       = 164 – 167\n| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files\n}}</ref>\n\nPar1 supports up to 256 recovery files. Each recovery file must be the size of the largest input file.\n\n=== Parity Volume Set Specification 2.0 ===\nPar2 files generally use this naming/extension system: \'\'filename.vol000+01.PAR2\'\', \'\'filename.vol001+02.PAR2\'\', \'\'filename.vol003+04.PAR2\'\', \'\'filename.vol007+06.PAR2\'\', etc. The +01, +02, etc. in the filename indicates how many blocks it contains, and the vol000, vol001, vol003 etc. indicates the number of the first recovery block within the PAR2 file. If an index file of a download states that 4 blocks are missing, the easiest way to repair the files would be by downloading \'\'filename.vol003+04.PAR2\'\'. However, due to the redundancy, \'\'filename.vol007+06.PAR2\'\' is also acceptable. There is also an index file \'\'filename.PAR2\'\', it is identical in function to the small index file used in PAR1.\n\nPar2 supports up to 65536 (2<sup>16</sup>) recovery blocks (however, par2cmdline, the official PAR2 implementation, it limited to 32767 blocks at once). Input files are split into multiple equal-sized blocks so that recovery files do not need to be the size of the largest input file.\n\nAlthough [[Unicode]] is mentioned in the PAR2 specification as an option, most PAR2 implementations do not support unicode.<ref>[http://www.quickpar.co.uk/forum/viewtopic.php?id=1065 QuickPar forum posting] {{webarchive |url=https://web.archive.org/web/20120302104523/http://www.quickpar.co.uk/forum/viewtopic.php?id=1065 |date=March 2, 2012 }}</ref>\n\nDirectory support is included in the PAR2 specification, but most or all implementations do not support it.\n\n=== Parity Volume Set Specification 3.0 ===\nPar3 is a planned improvement over Par2.<ref>{{cite web|url=http://hp.vector.co.jp/authors/VA021385/|title=MultiPar announcement|publisher=}}</ref><ref>[http://www.quickpar.org.uk/forum/viewtopic.php?id=1264 QuickPar forum posting&nbsp;– status PAR3] {{webarchive |url=https://web.archive.org/web/20101127125317/http://www.quickpar.org.uk/forum/viewtopic.php?id=1264 |date=November 27, 2010 }}</ref><ref>[http://www.quickpar.co.uk/forum/viewtopic.php?id=1047 QuickPar forum posting&nbsp;– PAR3 specifications] {{webarchive |url=https://web.archive.org/web/20120316104813/http://www.quickpar.co.uk/forum/viewtopic.php?id=1047 |date=March 16, 2012 }}</ref><ref>[http://hp.vector.co.jp/authors/VA021385/par3_spec_prop.htm PAR3 proposal] {{webarchive |url=https://web.archive.org/web/20100911002706/http://hp.vector.co.jp/authors/VA021385/par3_spec_prop.htm |date=September 11, 2010 }}</ref> The authors intend to fix problems related to creating or repairing when the block count or block size is very high. Par3 also adds support for including directories (file folders) in a parchive and Unicode characters in file names. In addition, the authors plan to enable the Par3 algorithm to identify files that have been moved or renamed.<ref>http://www.livebusinesschat.com/smf/index.php?topic=4751.0 PAR3 move/rename brainstorming</ref>\n\n== Software ==\n\n=== Windows ===\n* MultiPar (freeware) &nbsp;— Builds upon QuickPar\'s features and [[GUI]], and Yutaka Sawada\'s fork of par2cmdline as the PAR2 backend.<ref name="livebusinesschat.com"/> It has support for Par3, [[multithreading (software)|multithreading]], [[Symmetric multiprocessor system|multiple processors]], and the ability to recurse subfolders. MultiPar is able to add recovery data to [[Zip (file format)|ZIP]] and [[7-Zip]]<ref>{{cite web|url=https://sourceforge.net/p/sevenzip/feature-requests/1006/|title=7-Zip|publisher=}}</ref> files, with a few minor caveats.<ref>[http://www.livebusinesschat.com/smf/index.php?topic=4922.0 How to add recovery record to ZIP or 7-Zip archive]</ref> MultiPar is also verified to work with [[Wine (software)|Wine]] under [[TrueOS]], and may work with other operating systems too.<ref>[http://www.livebusinesschat.com/smf/index.php?topic=4902.0 MultiPar works with PCBSD 9.0]</ref> Although the Par2 and Par3 components are (or will be) open source, the MultiPar GUI on top of them is currently not open source.<ref>[https://www.livebusinesschat.com/smf/index.php?topic=5402.0 contacted you, asking about sourcecode]</ref>  Download from [https://www.livebusinesschat.com/smf/index.php?board=396.0 MultiPar forum]. \n* [[QuickPar]] (freeware)&nbsp;— unmaintained since 2004, superseded by MultiPar.\n* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] ([[GNU General Public License|GPLv2]])&nbsp;— a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]].\n* Par-N-Rar ([[GNU General Public License|GPL]])\n* [http://paulhoule.com/phpar2/index.php phpar2] &nbsp;— advanced par2cmdline with multithreading and highly optimized assemblercode (about 66% faster than QuickPar 0.9.1)\n* Rarslave ([[GNU General Public License|GPLv2]])\n* [[SmartPAR]] (freeware) &nbsp;— Unmaintained since 2002 and obsolete as this application written for Microsoft Windows only works with the original Par1 (PAR) Parchive format parity files. Superseded by QuickPar. It uses Reed–Solomon error correction to create new recovery files. SmartPAR is able to correct errors and recover missing parts of distributed files from PAR files.<ref>{{cite book\n| last        = Wang\n| first       = Wallace\n| authorlink  = \n| title       = Steal this File Sharing Book\n| url         = https://books.google.com/books?id=FGfMS5kymmcC&pg=PT183\n| accessdate  = 2009-09-24\n| edition     = 1st\n| date        = 2004-10-25\n| publisher   = [[No Starch Press]]\n| location    = [[San Francisco, California]]\n| isbn        = 1-59327-050-X\n| pages       = 164 – 167 \n| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files\n}}</ref> Last stable release 0.13d1 dated {{Start date and age|2002|01|22}}<ref>{{cite web |url=http://parchive.sourceforge.net/ |title=Parchive: Parity archive tool |accessdate=2009-09-26}}</ref>\n* [http://www.wehlus.de/mirror/index.html Mirror]&nbsp;— First PAR implementation, unmaintained since 2001.\n* [http://parchive.sourceforge.net/ Original par2cmdline]&nbsp;— (obsolete).\n* [https://github.com/Parchive/par2cmdline par2cmdline] by BlackIkeEagle.\n\n=== Mac OS X ===\n* [https://gp.home.xs4all.nl/Site/MacPAR_deLuxe.html MacPAR deLuxe 4.2]\n* [http://www.unrarx.com/ UnRarX]\n* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later.\n\n=== [[Linux]] ===\n* The [https://github.com/Parchive/par2cmdline par2] utility, which is a maintained fork of par2cmdline. \n* [http://pypar2.silent-blade.org/index.php?n=Main.HomePage PyPar2 1.4], a frontend for par2.\n* [http://sourceforge.net/projects/parchive/ GPar2 2.03]\n* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later.\n* [https://github.com/jkansanen/par2cmdline-mt par2cmdline-mt] is another multithreaded version of par2cmdline using [[OpenMP]], [[GNU General Public License|GPLv2]], or later.\n\n=== [[FreeBSD]] ===\n* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later. It is available in the [[FreeBSD Ports]] system as [https://www.freshports.org/archivers/par2cmdline-tbb/ par2cmdline-tbb].\n* [http://parchive.sourceforge.net/ par2cmdline] is available in the [[FreeBSD Ports]] system as [https://www.freshports.org/archivers/par2cmdline/ par2cmdline].\n\n=== [[POSIX]] ===\nSoftware for POSIX conforming operating systems:\n* [http://sourceforge.net/projects/ekpar2/ Par2 for KDE 4]\n\n== See also ==\n* [[Data degradation|Bit rot]]\n* [[Disc rot]]\n* [[Data corruption]]\n* [[Checksum]]\n* [[Comparison of file archivers]] – Some [[file archivers]] are capable of integrating parity data into their formats for error detection and correction:\n* [[RAID]]&nbsp;– RAID levels at and above RAID 5 make use of parity data to detect and repair errors.\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* [http://parchive.sourceforge.net/ Parchive project - full specifications and math behind it]\n* [http://www.ydecode.com/page_articles003.htm Introduction to PAR and PAR2]\n* [http://www.slyck.com/Newsgroups_Guide_PAR_PAR2_Files Slyck\'s Guide To The Usenet Newsgroups: PAR & PAR2 Files]\n* [http://www.warezfaq.com/allaboutpar.htm Another introduction to PAR and PAR2] and [http://www.warezfaq.com/more_info.htm more information from the same site]\n* [http://www.binaries4all.com/quickpar/repair.php Guide to repair files using PAR2]\n* [https://web.archive.org/web/20100912073937/http://chuchusoft.com/par2_tbb/ par2+tbb]\n* [http://www.milow.net/public/projects/parnrar/parnrar.html Par-N-Rar]\n* [http://www.irasnyder.com/devel/#rarslave Rarslave]\n\n[[Category:Archive formats]]\n[[Category:Data management]]\n[[Category:Usenet]]']
['Mobile content management system', '22464607', "A '''Mobile Content Management system''' ('''MCMs''') is a type of [[content management system]] (CMS) capable of storing and delivering content and services to mobile devices, such as mobile phones, smart phones, and PDAs. Mobile content management systems may be discrete systems, or may exist as features, modules or add-ons of larger content management systems capable of multi-channel content delivery. Mobile content delivery has unique, specific constraints including widely variable device capacities, small screen size, limited wireless bandwidth, small storage capacity, and comparatively weak device processors.<ref>[http://www.insight-corp.com/%5CExecSummaries%5Ccontent08ExecSum.pdf Content Management for Wireless Networks, 2008-2013 - Insight Research Report]</ref>\n\nDemand for mobile content management increased as mobile devices became increasingly ubiquitous and sophisticated. MCMS technology initially focused on the business to consumer (B2C) mobile market place with ringtones, games, text-messaging, news, and other related content. Since, mobile content management systems have also taken root in business-to-business (B2B) and business-to-employee (B2E) situations, allowing companies to provide more timely information and functionality to business partners and mobile workforces in an increasingly efficient manner. A 2008 estimate put global revenue for mobile content management at US$8 billion.<ref>[http://www.wirelessweek.com/Content-Management-Systems-Mobile-Embrace.aspx Content Management Systems’ Mobile Embrace, By Evan Koblentz, WirelessWeek, August 28, 2008]</ref>\n\n==Key features==\n\n===Multi-channel content delivery===\nMulti-channel content delivery capabilities allow users to manage a central content repository while simultaneously delivering that content to mobile devices such as mobile phones, smartphones, tablets and other mobile devices. Content can be stored in a raw format (such as Microsoft Word, Excel, PowerPoint, PDF, Text, HTML etc.) to which device-specific presentation styles can be applied.<ref>[http://www.apoorv.info/2007/05/26/content-management-for-mobile-delivery/ Content Management for Mobile Delivery, Posted by Apoorv, PCM.Blog, May 26, 2007]</ref>\n\n===Content access control===\nAccess control includes authorization, authentication, access approval to each content.  In many cases the access control also includes download control, wipe-out for specific user, time specific access.  For the authentication, MCM shall have basic authentication which has user ID and password.  For higher security many MCM supports IP authentication and mobile device authentication.\n\n===Specialized templating system===\nWhile traditional web content management systems handle templates for only a handful of web browsers, mobile CMS templates must be adapted to the very wide range of target devices with different capacities and limitations. There are two approaches to adapting templates: multi-client and multi-site. The multi-client approach makes it possible to see all versions of a site at the same domain (e.g. sitename.com), and templates are presented based on the device client used for viewing. The multi-site approach displays the mobile site on a targeted sub-domain (e.g. mobile.sitename.com).\n\n===Location-based content delivery===\nLocation-based content delivery provides targeted content, such as information, advertisements, maps, directions, and news, to mobile devices based on current physical location. Currently, GPS (global positioning system)  navigation systems offer the most popular [[location-based service]]s. Navigation systems are specialized systems, but incorporating mobile phone functionality makes greater exploitation of location-aware content delivery possible.\n\n==See also==\n*[[Mobile Web]]\n*[[Content management]]\n*[[Web content management system]]\n*[[Enterprise content management]]\n*[[Apache Mobile Filter]]\n\n==References==\n<references/>\n\n[[Category:Content management systems]]\n[[Category:Mobile Web]]\n[[Category:Data management]]"]
['Semantic translation', '2994894', '{{Unreferenced|date=December 2009}}\n\'\'\'Semantic translation\'\'\' is the process of using [[semantic]] information to aid in the translation of data in one representation or [[data model]] to another representation or data model.  Semantic translation takes advantage of semantics that associate meaning with individual [[data element]]s in one [[data dictionary|dictionary]] to create an equivalent meaning in a second system.\n\nAn example of semantic translation is the conversion of [[XML]] data from one data model to a second data model using formal [[ontologies]] for each system such as the [[Web Ontology Language]] (OWL).  This is frequently required by [[intelligent agents]] that wish to perform searches on remote computer systems that use different data models to store their data elements.  The process of allowing a single user to search multiple systems with a single search request is also known as [[federated search]].\n\nSemantic translation should be differentiated from [[data mapping]] tools that do simple one-to-one translation of data from one system to another without actually associating meaning with each data element.\n\nSemantic translation requires that data elements in the source and destination systems have "semantic mappings" to a central registry or registries of data elements. The simplest mapping is of course where there is equivalence.\nThere are three types of [[Semantic equivalence]]:\n\n* \'\'\'[[Class (computer science)|Class]] Equivalence\'\'\'{{Anchor|Class equivalence}} - indicating that class or "concepts" are equivalent.  For example: "Person" is the same as "Individual"\n* \'\'\'[[Relation (mathematics)|Property]] Equivalence\'\'\'{{Anchor|Property equivalence}} - indicating that two properties are equivalent.  For example: "PersonGivenName" is the same as "FirstName"\n* \'\'\'[[Instance (computer science)|Instance]] Equivalence\'\'\'{{Anchor|Instance equivalence}} - indicating that two individual instances of objects are equivalent.  For example: "Dan Smith" is the same person as "Daniel Smith"\n\nSemantic translation is very difficult if the terms in a particular data model do not have direct one-to-one mappings to data elements in a foreign data model. In that situation an alternative approach must be used to find mappings from the original data to the foreign data elements.  This problem can be alleviated by centralized metadata registries that use the ISO-11179 standards such as the [[National Information Exchange Model]] (NIEM).\n\n==See also==\n* [[Data mapping]]\n* [[Semantic heterogeneity]]\n* [[Semantic mapper]]\n* [[Federated search]]\n* [[Intelligent agents]]\n* [[ISO/IEC 11179]]\n* [[National Information Exchange Model]]\n* [[Semantic Web]]\n* [[Vocabulary-based transformation]]\n* [[Web Ontology Language]]\n\n{{DEFAULTSORT:Semantic Translation}}\n[[Category:Data management]]\n[[Category:Enterprise application integration]]']
['Hybrid array', '24278544', 'A \'\'\'hybrid array\'\'\' is a form of [[hierarchical storage management]] that combines [[hard disk drive]]s (HDDs) with [[solid-state drive]]s (SSDs) for [[I/O]] speed improvements.\n\nHybrid storage arrays aim to mitigate the ever increasing price-performance gap between HDDs and [[DRAM]] by adding a non-volatile flash level to the [[memory hierarchy]].<ref name="MicheloniMarelli2012">{{cite book|author1=Rino Micheloni|author2=Alessia Marelli|author3=Kam Eshghi|title=Inside Solid State Drives (SSDs)|url=https://books.google.com/books?id=S8xRtkF7hUkC&pg=PA62|year=2012|publisher=Springer|isbn=978-94-007-5145-3|page=62}}</ref> Hybrid arrays thus aim to lower the cost per I/O, compared to using only SSDs for storage.  Hybrid architectures can be as simple as involving a single SSD [[Cache (computing)|cache]] for desktop or laptop computers, or can be more complex as configurations for [[data center]]s and [[cloud computing]].\n\n== Implementations ==\n<!-- please only add products covered in [[WP:SECONDARY]] sources at adequate depth -->\nSome commercial products for building hybrid arrays include:\n* [[Adaptec]] demonstrated the MaxIQ series in 2009.<ref>{{cite web |author=Charlie Demerjian |url= http://semiaccurate.com/2009/09/09/adaptecs-maxiq-caches-raids-ssds/ |title= Adaptec\'s MaxIQ caches RAIDs with SSDs |publisher= SemiAccurate |date= September 9, 2009 |accessdate= October 10, 2016 }}</ref>\n* Apple\'s [[Fusion Drive]]\n*  [[Linux]] software includes [[bcache]], [[dm-cache]], and [[Flashcache]] (and its fork EnhanceIO).\n* Condusive\'s [[ExpressCache]] is marketed for laptops.\n* [[EMC Corporation]] VFcache was announced in 2012.<ref>{{cite web |last= Larry Dignan |url= http://www.zdnet.com/blog/btl/emc-unveils-vfcache-targets-fusion-io/68657 |title=EMC unveils VFCache, targets Fusion-io |publisher= ZDNet |work= Between the Lines |date= February 5, 2012 |accessdate= October 10, 2016 }}</ref><ref>{{Cite news |title= One day later: EMC declares war on all-flash array, server flash card rivals: Rolls out XtremIO array, renamed VFCache |date= March 5, 2013 |work= The Register |author= Chris Mellor |url= http://www.theregister.co.uk/2013/03/05/emc_xtremsf/ |accessdate= October 10, 2016 }}</ref>\n* [[Fusion-io]] acquired ioTurbine in 2011,<ref name="io">{{cite web |url= http://www.theregister.co.uk/2013/06/25/fusionio_spins_ioturbine_faster/ |title=Fusion-io spins up ioTurbine, enhances server flash caching |work= The Register |accessdate= October 10, 2016 }}</ref> and the product line it acquired by buying NexGen in 2013.<ref>{{cite web|url=http://www.theregister.co.uk/2013/04/24/fusion_io_nexgen/|title=Fusion-io buys NexGen|work=theregister.co.uk |accessdate=2015-03-26}}</ref>\n* [[Hitachi]] Accelerated Flash Storage (HAFS) used together with the Hitachi Dynamic Tiering software<ref>{{citation |url=http://www.computerweekly.com/feature/Big-storage-turns-the-tide-in-the-hybrid-flash-array-market |title=Big storage turns the tide in the hybrid flash array market |work=[[Computer Weekly]] |date=September 2013 |accessdate=2015-03-26}}</ref>\n* [[IBM]] Flash Cache Storage Accelerator (FCSA) server software<ref>{{cite web|author=The SSD Guy |url=http://thessdguy.com/ibm-adds-server-side-caching/ |title=IBM Adds Server-Side Caching |publisher=The SSD Guy |date=2013-08-20 |accessdate=2013-12-23}}</ref>\n* Intel\'s [[Smart Response Technology]] for desktop\n* Intel\'s [[Cache Acceleration Software]] for servers and workstations \n* [[LSI Corporation|LSI]] CacheCade software for their controllers<ref>{{cite web|url=http://www.storagereview.com/lsi_megaraid_cachecade_pro_20_review|title=LSI MegaRAID CacheCade Pro 2.0 Review |accessdate=2015-03-26 |work=storagereview.com}}</ref>\n* [[Marvell Technology Group|Marvell]]\'s HyperDuo controllers<ref>{{cite web|url=http://www.cnet.com/8301-32254_1-20027657-283.html|title=Hands-on with the Marvell HyperDuo hybrid storage controller |accessdate=2015-03-26 |publisher=CBS Interactive|work=CNET}}</ref>\n* Microsoft\'s [[Automated Tiering]] (since Windows 2012 R2)\n* [[NetApp]]\'s Flash Cache, Flash Pool, Flash Accel<ref>{{cite web|url=http://www.theregister.co.uk/2012/08/21/netapp_server_flash/|title=NetApp: Flash as a STORAGE tier? You must be joking |accessdate=2015-03-26 |work=theregister.co.uk}}</ref>\n* [[Oracle Corporation]] markets products such as [[Exadata]] Smart Cache Flash, and the FS1 flash storage system.<ref>{{Cite news |title= Oracle crashes all-flash bash: Behold, our hybrid FS1 arrays: Mutant flash/disk box a pillar of storage: It\'s axiomatic |date= September 30, 2014 |work= The Register |author= Chris Mellor |url= http://www.theregister.co.uk/2014/09/30/the_fs1_pillar_of_oracle_arrays_storage/ |accessdate= October 10, 2016 }}</ref>\n* Microsoft [[ReadyBoost]] allows personal computers to [[USB flash drive]]s as cache.\n* Nvelo DataPlex SSD caching software was announced in 2011,<ref>{{cite web |url= http://www.thessdreview.com/our-reviews/nvelo-dataplex-ssd-caching-software-review-seven-msata-ssds-prove-an-amazing-concept/  |title= NVELO Dataplex SSD Caching Software Review - Seven mSATA SSDs Prove An Amazing Concept |work= The SSD Review |date= December 4, 2011 |author= Les Tokar |accessdate= October 10, 2016 }}</ref> and was acquired by [[Samsung]] in 2012.<ref>{{cite web |url= http://www.anandtech.com/show/6518/samsung-acquires-ssd-caching-company-nvelo |title=Samsung Acquires SSD Caching Company NVELO |publisher=AnandTech |author= Kristian Vättö |date= December 16, 2012 |accessdate= October 10, 2016 }}</ref>\n* [[SanDisk]] FlashSoft for Windows, Linux, and [[vSphere]]<ref name="io"/>\n* Products are offered by vendors like AMI [[StorTrends]],<ref>{{cite web|author=Ian Barker |url=http://betanews.com/2014/01/27/ami-stortrends-3500i-offers-high-performance-storage-for-smaller-enterprises/ |title=AMI StorTrends 3500i offers high performance storage for smaller enterprises |publisher=BetaNews |date=2014-01-27 |accessdate=2014-10-17}}</ref> [[Tegile Systems]], [[Reduxio]], and [[Tintri]].<ref>{{cite web|url=http://www.theregister.co.uk/2013/04/09/blind_spot/ |accessdate=2015-03-26 |title=Mutant array upstarts feast on EMC, NetApp\'s leavings|work= The Register }}</ref>\n* [[ZFS]] using hybrid storage pools, are used for example in some Oracle Corporation products.<ref>{{cite web|url=http://www.enterprisestorageforum.com/san-nas-storage/oracles-flash-friendly-sun-zfs-storage-is-ready-for-new-sparcs.html|title=Oracle\'s Flash-Friendly Sun ZFS Storage Is Ready for New SPARCs|date=3 April 2013 |accessdate=2015-03-26 |work=enterprisestorageforum.com}}</ref>\n\n== See also ==\n* [[Hybrid drive]]{{spaced ndash}} built-in flash cache, handled by firmware\n* [[Automated tiered storage]]{{spaced ndash}} another name for hierarchical storage management\n* The "[[five-minute rule]]" for caching\n\n== References ==\n{{Reflist|30em}}\n\n[[Category:Data management]]\n[[Category:Solid-state caching]]\n[[Category:Memory management software]]']
['Operational data store', '1740486', 'An \'\'\'operational data store\'\'\' (or "\'\'\'ODS\'\'\'") is a [[database]] designed to [[data integration|integrate data]] from multiple sources for additional operations on the data. Unlike a master data store, the data is not passed back to [[operational system]]s. It may be passed for further operations and to the [[data warehouse]] for reporting. \n\nBecause the [[data]] originate from multiple sources, the integration often involves [[data cleaning|cleaning]], resolving redundancy and checking against [[business rule]]s for [[data integrity|integrity]].  An ODS is usually designed to contain low-level or atomic (indivisible) data (such as transactions and prices) with limited history that is captured "real time" or "near real time" as opposed to the much greater volumes of data stored in the data warehouse generally on a less-frequent basis.\n\n==General Use==\nThe general purpose of an ODS is to integrate data from disparate source systems in a single structure, using [[data integration]] technologies like [[Data Virtualization|data virtualization]], [[Federated database system|data federation]], or [[Extract, transform, load|extract, transform, and load]]. This will allow operational access to the data for operational reporting, [[Master Data|master data or reference data management]].\n\nAn ODS is not a replacement or substitute for a [[data warehouse]] but in turn could become a source.\n\n==See also==\n* Some examples of ODS Architecture Patterns can be found in the article [[Architectural pattern (computer science)#Examples|Architecture Patterns]].\n\n==Publications==\n* {{cite book |last1=Inmon |first1=William |author1-link=Bill Inmon |title=Building the Operational Data Store |edition=2nd |location=New York |publisher=[[John Wiley & Sons]] |year=1999 |isbn=0-471-32888-X}}\n\n==External links==\n*[[Architectural pattern (computer science)#Examples|ODS Architecture Patterns (EA Reference Architecture)]]\n* [http://www.dmreview.com/issues/19980701/469-1.html Bill Inmon Information Management article on ODS]\n* [http://www.information-management.com/issues/20000101/1749-1.html Bill Inmon Information Management article on the five classes of ODS]\n* [http://www.intelsols.com/documents/Imhoff_10-02.pdf Claudia Imhoff Information Management article on ODS] PDF\n\n{{Data warehouse}}\n\n== See also ==\n{{Wikipedia books|Enterprise Architecture}}\n* [[Enterprise architecture]]\n\n{{DEFAULTSORT:Operational Data Store}}\n[[Category:Data management]]\n[[Category:Data warehousing]]\n\n{{database-stub}}']
['Commit (data management)', '1626958', "{{For|the revision control concept|Commit (revision control)}}\n{{Unreferenced|date=May 2014}}\n\nIn [[computer science]] and [[data management]], a '''commit''' is the making of a set of tentative changes permanent. A popular usage is at the end of a [[database transaction|transaction]]. A ''commit'' is an act of committing.\n\n==Data management==\nA <code>[[COMMIT (SQL)|COMMIT]]</code> statement in [[SQL]] ends a [[database transaction|transaction]] within a [[relational database management system]] (RDBMS) and makes all changes visible to other users. The general format is to issue a <code>[[Begin work (SQL)|BEGIN WORK]]</code> statement, one or more SQL statements, and then the <code>COMMIT</code> statement. Alternatively, a <code>[[Rollback (data management)|ROLLBACK]]</code> statement can be issued, which undoes all the work performed since <code>BEGIN WORK</code> was issued. A <code>COMMIT</code> statement will also release any existing [[savepoint]]s that may be in use.\n\nIn terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a [[rollback (data management)|rollback]].\n\n==See also==\n* [[Commit (version control)]]\n* [[Atomic commit]]\n* [[Two-phase commit protocol]]\n* [[Three-phase commit protocol]]\n\n{{databases}}\n\n{{DEFAULTSORT:Commit (Data Management)}}\n[[Category:Data management]]\n[[Category:SQL]]\n[[Category:Transaction processing]]\n\n{{comp-sci-stub}}"]
['Durability (database systems)', '245944', "{{Unreferenced|date=December 2009}}\nIn [[database system]]s, '''durability''' is the [[ACID]] property which guarantees that [[database transaction|transaction]]s that have committed will survive permanently. \nFor example, if a flight booking reports that a seat has successfully been booked, then the seat will remain booked even if the system crashes.\n\nDurability can be achieved by flushing the transaction's log records to [[non-volatile storage]] before acknowledging commitment.\n\nIn [[distributed transaction]]s, all participating servers must coordinate before commit can be acknowledged. This is usually done by a [[two-phase commit protocol]].\n\nMany DBMSs implement durability by writing transactions into a [[transaction log]] that can be reprocessed to recreate the system state right before any later failure. A transaction is deemed committed only after it is entered in the log.\n\n==See also==\n* [[Atomicity (database systems)|Atomicity]]\n* [[Consistency (database systems)|Consistency]]\n* [[Isolation (database systems)|Isolation]]\n* [[Relational database management system]]\n\n{{DEFAULTSORT:Durability (Database Systems)}}\n[[Category:Data management]]\n[[Category:Transaction processing]]\n\n\n{{Compu-sci-stub}}\n{{Database-stub}}"]
['Archive site', '2643012', "{{refimprove|date=January 2016}}\n\nIn [[web archiving]], an '''archive site''' is a [[website]] that stores information on webpages from the past for anyone to view.\n\n==Common techniques==\nTwo common techniques for archiving web sites are using a [[web crawler]] or soliciting user submissions:\n\n# '''Using a [[web crawler]]''': By using a web crawler (e.g., the [[Internet Archive]]) the service will not depend on an active community for its content, and thereby can build a larger database faster. However, web crawlers are only able to index and archive information the public has chosen to post to the Internet, or that is available to be crawled, as web site developers and system administrators have the ability to block web crawlers from accessing [certain] web pages (using a [[Robots Exclusion Standard|robots.txt]]).\n# '''User submissions''': While it can be difficult to start user submissions services due to potentially low rates of user submission, this system can yield some of the best results. By crawling web pages one is only able to obtain the information the public has chosen to post online; however, potential content providers may not bother to post certain information, assuming no one would be interested in it, because they lack a proper venue in which to post it, or because of copyright concerns.<ref>{{cite news|url=http://dlib.org/dlib/march12/niu/03niu1.html|journal=D-Lib Magazine| date=March–April 2012 |\n volume =18| number =3/4| title=An Overview of Web Archiving |author=Jinfang Niu,  University of South Florida|doi=10.1045/march2012-niu1}}</ref> However, users who see someone wants their information may be more apt to submit it.\n\n==Examples==\n\n===Google Groups===\nOn February 12, 2001, [[Google]] acquired the [[usenet]] discussion group archives from [[Deja.com]] and turned it into their [[Google Groups]] service.<ref>{{cite web |title=Google Acquires Usenet Discussion Service and Significant Assets from Deja.com |work= |date=February 12, 2001 |url=https://googlepress.blogspot.com/2001/02/google-acquires-usenet-discussion.html }}</ref> They allow users to search old discussions with Google's search technology, while still allowing users to post to the [[mailing list]]s.\n\n===Internet Archive===\nThe [[Internet Archive]] is building a compendium of websites and [[digital media]]. Starting in 1996, the Archive has been employing a web crawler to build up their database. It is one of the best known archive sites.\n\n===NBCUniversal Archives===\n[[NBCUniversal Archives]] offer access to exclusive content from [[NBCUniversal]] and its subsidiaries. Their NBCUniversal Archives website provides easy viewing of past and recent news clips, and it is a prime example of a news archive.<ref>[http://www.nbcuniversalarchives.com/nbcuni/home.do NBCUniversal Archives]</ref>\n\n===Nextpoint===\n[[Nextpoint]] offers an automated [[Cloud computing|cloud]]-based, [[Software as a service|SaaS]] for marketing, compliance, and litigation related needs including electronic discovery.\n\n===PANDORA Archive===\nPANDORA ([[Pandora Archive]]), founded in 1996 by the National Library of [[Australia]], stands for Preserving and Accessing Networked Documentary Resources of Australia, which encapsulates their mission. They provide a long-term catalog of select online publications and web sites authored by Australians or that are of an Australian topic. They employ their PANDAS (PANDORA Digital Archiving System) when building their catalog.\n\n===textfiles.com===\n[[textfiles.com]] is a large library of old text files maintained by [[Jason Scott Sadofsky]]. Its mission is to archive the old documents that had floated around the [[bulletin board systems]] (BBS) of his youth and to document other people's experiences on the bulletin board systems.\n\n==See also==\n* [[Internet Archive]]\n* [[Pandora Archive]]\n* [[WebCite]]\n* [[Web archiving]]\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Archive Site}}\n[[Category:Data management]]\n[[Category:Online archives]]\n[[Category:Web archiving initiatives]]"]
['White pages schema', '1462050', "{{Unreferenced|date=December 2009}}\nA '''white pages schema''' is a [[data model]], specifically a [[logical schema]], for organizing the data contained in entries in a [[directory service]], database, or application, such as an address book.  In a white pages directory, each entry typically represents an individual [[user (computing)|person that makes use of]] network resources, such as by receiving email or having an account to log in to a system.\nIn some environments, the schema may also include the representation of organizational divisions, roles, groups, and devices.  The term is derived from the [[white pages]], the listing of individuals in a [[telephone directory]], typically sorted by the individual's home location (e.g. city) and then by\ntheir name.\n\nWhile many [[Postal Telephone and Telegraph|telephone service providers]] have for decades published a list of their [[subscriber]]s in a [[telephone directory]], and similarly corporations published a list of their employees in an internal directory, it was not until the rise of [[electronic mail]] systems that a requirement for standards for the electronic exchange of [[subscriber]] information between different systems appeared.\n\nA white pages schema typically defines, for each real-world object being represented:\n\n* what attributes of that object are to be represented in the entry for that object\n* what relationships of that object to other objects are to be represented\n* how is the entry to be named in a [[Directory Information Tree|DIT]]\n* how an entry is to be located by a client searching for it\n* how similar entries are to be distinguished\n* how are entries to be ordered when displayed in a list\n\nOne of the earliest attempts to standardize a white pages schema for electronic mail use was in [[X.520]] and [[X.521]], part of the [[X.500]] specifications,\nthat was derived from the addressing requirements of [[X.400]] and defined a [[Directory Information Tree]] that mirrored the international telephone system, with entries representing residential and organizational subscribers.  This evolved into the [[Lightweight Directory Access Protocol]] standard schema in RFC 2256.  One of the most widely deployed white pages schemas used in LDAP\nfor representing individuals in an organizational context is '''inetOrgPerson''', defined in RFC 2798, although versions of [[Active Directory]] require a different object class, '''User'''.  Many large organizations have\nalso defined their own white pages schemas for their employees or customers, as part of their [[Identity management]] architecture.  Converting between data bases and directories using different schemas is often the\nfunction of a [[Metadirectory]], and data interchange standards such as [[Common Indexing Protocol]].\n\nSome early directory deployments suffered due to poor design choices in their white pages schema, such as:\n\n* attributes used for naming purposes were non-unique in large environments (such as a person's common name)\n* attributes used for naming purposes were likely to change (such as surnames)\n* attributes were included which could lead to [[Identity theft]], such as a [[Social security number]]\n* users were required during [[provisioning]] to choose attributes which are unique but still memorable to them\n\nNumerous other proposed schemas exist, both as standalone definitions suitable for use with general purpose\ndirectories, or as embedded into network protocols.\n\nExamples of other generic white pages schemas include [[vCard]], defined in RFC 2426, and [[FOAF (software)|FOAF]].\n\n==See also==\n* [[Recognition of human individuals]]\n\n{{DEFAULTSORT:White Pages Schema}}\n[[Category:Data modeling]]\n[[Category:Data management]]\n[[Category:Identity management]]"]
['Recording format', '793548', "{{Unreferenced|date=December 2009}}\n[[File:Cylinder Head Sector.svg|thumb|300px|right|A cylinder, head, and sector of a hard drive. The sectors are a recording container format. The digital data on the disks may be both secondary [[Container format (digital)|container file formats]] and raw digital data content formats such as digital audio or ASCII encoded text.]]\n[[File:WorldMapLongLat-eq-circles-tropics-non.png|thumb|440px|A map of Earth showing lines of latitude (horizontally) and longitude (vertically). The lines are a grid, a method for dividing and containing recorded [[cartographical]] data. The land masses and oceans are cartographical data in a raw content ([[pictorial]] graphical) format. The text is in an [[alphanumeric]]al symbolic raw content format.]]\nA '''recording format ''' is a [[content format|format]] for [[encoder|encoding]] data for storage on a [[storage medium]]. The format can be container information such as [[Cylinder-head-sector|sectors]] on a disk, or user/audience information ([[Content (media and publishing)|content]]) such as [[analog signal|analog]] [[stereo]] [[Sound recording and reproduction|audio]].  Multiple levels of encoding may be achieved in one format. For example, a text encoded page may contain [[HTML]] and [[XML]] encoding, combined in a [[plain text]] file format, using either [[EBCDIC]] or [[ASCII]] character encoding, on a [[Universal Disk Format|UDF]] [[Digital data|digital]]ly formatted disk.  \n\nIn [[electronic media]], the primary format is the encoding that requires hardware to interpret (decode) data; while secondary encoding is interpreted by secondary [[signal processing]] methods, usually [[computer software]]. \n\n==Recording container formats==\nA container format is a system for dividing physical storage space or virtual space for data. Data space can be divided evenly by a [[systems of measurement|system of measurement]], or divided unevenly with [[meta data]]. A grid may divide physical or virtual space with physical or virtual (dividers) borders, evenly or unevenly.  Just as a physical container (such as a [[file cabinet]]) is divided by physical borders (such as [[drawer (furniture)|drawer]]s and [[file folder]]s), data space is divided by virtual borders. Meta data such as a [[unit of measurement]], [[Address (geography)|address]], or [[meta tags]] act as virtual borders in a container format. A template may be considered an abstract format for containing a solution as well as the content itself. \n\n* Systems of measurement\n**[[Metric system]]\n** [[Geographic coordinate system]]\n**[[Grid (page layout)|Page grid]]\n* [[Film formats]]\n* [[Audio format|Audio data format]]\n* [[Video tape|Video tape format]]\n* [[Disk format]]\n* [[File format]]\n* [[Meta data]]\n** [[Formatted text|Text formatting]]\n** [[Template (file format)|Template]]\n** [[Data structure]]\n\n==Raw content formats==\n{{Main|content format}}\n\nA raw content format is a system of converting data to displayable [[information]].  Raw content formats may either be recorded in secondary signal processing methods such as a software container format (e.g. [[digital audio]], [[digital video]]) or recorded in the primary format. A primary raw content format may be directly [[information processing|observable]] (e.g. [[image]], [[sound]], [[Motion (physics)|motion]], [[Odor|smell]], [[Haptic perception|sensation]]) or [[physics|physical]] data which only requires hardware to display it, such as a [[phonograph]]ic [[Gramophone needle|needle]] and [[diaphragm (acoustics)|diaphragm]] or a [[Image projector|projector]] [[List of light sources|lamp]] and [[magnifying glass]].\n\n{{Audio format}}{{Homevid}}\n\n{{DEFAULTSORT:Recording Format}}\n[[Category:Communication]]\n[[Category:Information science]]\n[[Category:Data management]]\n[[Category:Film and video technology]]\n[[Category:Computer storage media]]\n[[Category:Recording]]"]
['Automated tiered storage', '5732700', '\'\'\'Automated tiered storage\'\'\' (also \'\'\'automated storage tiering\'\'\') is the automated progression or demotion of data across different tiers (types) of storage devices and media. The movement of data takes place in an automated way with the help of a software or embedded firmware and is assigned to the related media according to performance and capacity requirements. More advanced implementations include the ability to define rules and policies that dictate if and when data can be moved between the tiers, and in many cases provides the ability to pin data to tiers permanently or for specific periods of time. Implementations vary, but are classed into two broad categories: pure software based implementations that run on general purpose processors supporting most forms of general purpose storage media and embedded automated tiered storage controlled by firmware as part of a closed embedded storage system such as a SAN disk array. Software Defined Storage architectures commonly include a component of tiered storage as part of their primary functions.\n\nIn the most general definition, Automated Tiered Storage is a form of Hierarchical Storage Management. However, the term automated tiered storage has emerged to accommodate newer forms of real-time performance optimized [[data migration]] driven by the proliferation of solid state disks and storage class memory. Furthermore, where traditional HSM systems act on files and move data between storage tiers in a batch, scheduled like fashion, automated storage tiered systems are capable of operating at sub-file level both in batch and real-time modes. In the case of the latter, data is moved almost as soon as it enters the storage system or relocated based on its activity levels within seconds of data being accessed, whereas more traditional tiering tends to operate on an hourly, daily or even weekly schedule. Some more background on the relative differences between HSM, ILM and automated tiered storage is available at SNIA web site.<ref>http://www.snia.org/sites/default/education/tutorials/2012/spring/storman/LarryFreeman_What_Old_Is_New_Again.pdf</ref> A general comparison of different approaches can also be found in this \'comparison article on auto tiered storage\'[http://searchstorage.techtarget.co.uk/feature/Automated-storage-tiering-product-comparison].\n\n==OS and Software Based Automated Tiered Storage==\nMost server oriented software automated tiered storage vendors offer tiering as a component of a general storage virtualization stack offering, an example being [[Microsoft]] with their Tiered Storage Spaces.<ref>https://redmondmag.com/articles/2013/08/30/windows-storage-tiering.aspx?m=1</ref> However, automated tiering is now becoming a common part of industry standard operating systems such as Linux and Microsoft Windows, and in the case of consumer PCs, Apple OSX with its Fusion Drive.<ref>[http://www.apple.com/imac/performance/ "Apple iMac Performance Website"] October 24, 2012.</ref> This solution allowed a single SSD and hard disk drive to be combined into a single automated tiered storage drive that ensured that the most frequently accessed data was stored on the SSD portion of the virtual disk. A more OS agnostic version was introduced by Enmotus which supports real-time tiering with its FuzeDrive product for Linux and Windows operating systems, extending support to storage class memory offerings such as NVDIMM and NVRAM devices.<ref>http://cdn2.hubspot.net/hub/486631/file-2586107985-pdf/PDFs/20111129_S2-102_Mills.pdf?t=1447892865729</ref>\n\n==SAN Based Tiered Storage==\nAn example of automated tiered storage in a hardware storage array is a feature called Data Progression from Compellent Technologies. Data Progression has the capability to transparently move blocks of data between different drive types and RAID groups such as RAID 10 and RAID 5. The blocks are part of the "same virtual volume even as they span different RAID groups and drive types.  Compellent can do this because they keep metadata about every block -- which allows them to keep track of each block and its associations.".<ref>[http://blogs.computerworld.com/compellent_ilm  Tony Asaro, Computerworld. "Compellent-Intelligent Tiered Storage."] January 19, 2009.</ref> Another strong example of SAN based tiering is DotHill\'s Autonomous Tiered Storage which moves data between tiers of storage within the SAN disk array with decisions made every few seconds".<ref>[https://www.dothill.com/solutions/tiered-data-storage/ "Hybrid Data Storage Solution with SSD and HDD Tiers"]</ref>\n\n== Automated Tiered Storage vs. SSD Caching ==\nWhile tiering solutions and caching may look the same on the surface, the fundamental differences lie in the way the faster storage is utilized and the algorithms used to detect and accelerate frequently accessed data. SSD caching operates much like SRAM-DRAM caches do i.e. they make a copy of frequently accessed blocks of data, for example in 4K cache page sizes, and store the copy in the SSD and use this copy instead of the original data source on the slower backend storage. Every time a storage IO occurs, the caching software look to see if a copy of this data already exists using a variety of algorithms and service the host request from the SSD if it is found. The SSD is used in this case as a lookaside device as it is not part of the primary storage. While some good caching algorithms can demonstrate native SSD performance on reads and short bursts of writes, caching typically operates well below the maximum sustainable rate of the underlying SSD devices as overhead CPU cycles are introduced during the host IO commands that increasingly impact performance as the amount of data cached grows. Tiering on the other hand operates very differently. Using the specific case of SSDs, once data is identified as frequently used, the identified blocks of data are moved in the background to the SSD and not copied as the SSD is being utilized as a primary storage tier, not a look aside copy area. When the data is subsequently accessed, the IOs occur at or near the native performance of the SSDs as there area are few if any CPU cycles needed to do the simpler virtual to physical addressing translations.<ref>[http://searchsolidstatestorage.techtarget.com/tip/Tiering-vs-caching-in-flash-based-storage-systems] "Tiering vs. caching in flash-based storage systems"</ref>\n\n== See also ==\n* [[Hierarchical storage management]]\n* [[Tiered storage]]\n\n== References ==\n* Russ Taddiken – Senior Storage Architect (2006). Automating Data Movement Between Storage Tiers. Retrieved from the UW Records Management Web site: http://www.compellent.com/\n<references/>\n\n== External links ==\n* http://www.snia.org/sites/default/education/tutorials/2012/spring/storman/LarryFreeman_What_Old_Is_New_Again.pdf\n\n[[Category:Data management]]']
['Data validation and reconciliation', '26683958', '\'\'\'Industrial process data validation and reconciliation\'\'\', or more briefly, \'\'\'data validation and reconciliation (DVR)\'\'\', is a technology that uses process information and mathematical methods in order to automatically correct measurements in industrial processes. The use of DVR allows for extracting accurate and reliable information about the state of industry processes from raw measurement data and produces a single consistent set of data representing the most likely process operation.\n\n==Models, data and measurement errors==\nIndustrial processes, for example chemical or thermodynamic processes in chemical plants, refineries, oil or gas production sites, or power plants, are often represented by two fundamental means:\n# Models that express the general structure of the processes,\n# Data that reflects the state of the processes at a given point in time.\nModels can have different levels of detail, for example one can incorporate simple mass or compound conservation balances, or more advanced thermodynamic models including energy conservation laws. Mathematically the model can be expressed by a [[nonlinear system|nonlinear system of equations]] <math>F(y)=0\\,</math> in the variables <math>y=(y_1,\\ldots,y_n)</math>, which incorporates all the above-mentioned system constraints (for example the mass or heat balances around a unit). A variable could be the temperature or the pressure at a certain place in the plant.\n\n===Error types===\n<gallery caption="Random and systematic errors" widths="300%" perrow="2" align="right">\nFile:Normal_no_bias.jpg|Normally distributed measurements without bias.\nFile:Normal_with_bias.jpg|Normally distributed measurements with bias.\n</gallery>\nData originates typically from [[measurements]] taken at different places throughout the industrial site, for example temperature, pressure, volumetric flow rate measurements etc. To understand the basic principles of DVR, it is important to first recognize that plant measurements are never 100% correct, i.e. raw measurement <math>y\\,</math> is not a solution of the nonlinear system <math>F(y)=0\\,\\!</math>. When using measurements without correction to generate plant balances, it is common to have incoherencies. [[Observational error|Measurement errors]] can be categorized into two basic types:\n# [[random error]]s due to intrinsic [[sensor]] [[accuracy]] and\n# [[systematic errors]] (or gross errors) due to sensor [[calibration]] or faulty data transmission.\n\n[[Random error]]s means that the measurement <math>y\\,\\!</math> is a [[random variable]] with [[mean]] <math>y^*\\,\\!</math>, where <math>y^*\\,\\!</math> is the true value that is typically not known. A [[systematic error]] on the other hand is characterized by a measurement <math>y\\,\\!</math> which is a random variable with [[mean]] <math>\\bar{y}\\,\\!</math>, which is not equal to the true value <math>y^*\\,</math>.  For ease in deriving and implementing an optimal estimation solution, and based on arguments that errors are the sum of many factors (so that the [[Central limit theorem]] has some effect), data reconciliation assumes these errors are [[normal distribution|normally distributed]].   \n\nOther sources of errors when calculating plant balances include process faults such as leaks, unmodeled heat losses, incorrect physical properties or other physical parameters used in equations, and incorrect structure such as unmodeled bypass lines.  Other errors include unmodeled plant dynamics such as holdup changes, and other instabilities in plant operations that violate steady state (algebraic) models.  Additional dynamic errors arise when measurements and samples are not taken at the same time, especially lab analyses.  \n\nThe normal practice of using time averages for the data input partly reduces the dynamic problems.  However, that does not completely resolve timing inconsistencies for infrequently-sampled data like lab analyses.  \n\nThis use of average values, like a [[moving average]], acts as a [[low-pass filter]], so high frequency noise is mostly eliminated.   The result is that, in practice, data reconciliation is mainly making adjustments to correct systematic errors like biases.\n\n===Necessity of removing measurement errors===\nISA-95 is the international standard for the integration of enterprise and control systems<ref>[http://www.isa-95.com/ "ISA-95: the international standard for the integration of enterprise and control systems"]. isa-95.com.</ref> It asserts that:\n<blockquote>Data reconciliation is a serious issue for enterprise-control integration. The data have to be valid to be useful for the enterprise system. The data must often be determined from physical measurements that have associated error factors. This must usually be converted into exact values for the enterprise system. This conversion may require manual, or intelligent reconciliation of the converted values [...].\n\nSystems must be set up to ensure that accurate data are sent to production and from production. Inadvertent operator or clerical errors may result in too much production, too little production, the wrong production, incorrect inventory, or missing inventory.</blockquote>\n\n==History==\nDVR has become more and more important due to industrial processes that are becoming more and more complex. DVR started in the early 1960s with applications aiming at closing [[mass balance|material balances]] in production processes where raw measurements were available for all [[variable (mathematics)|variables]].<ref>D.R. Kuehn, H. Davidson, \'\'Computer Control II. Mathematics of Control\'\', Chem. Eng. Process 57: 44–47, 1961.</ref> At the same time the problem of [[systematic error|gross error]] identification and elimination has been presented.<ref>V. Vaclavek, \'\'Studies on System Engineering I. On the Application of the Calculus of the Observations of Calculations of Chemical Engineering Balances\'\', Coll. Czech Chem. Commun 34: 3653, 1968.</ref> In the late 1960s and 1970s unmeasured variables were taken into account in the data reconciliation process.,<ref>V. Vaclavek, M. Loucka, \'\'Selection of Measurements Necessary to Achieve Multicomponent Mass Balances in Chemical Plant\'\', Chem. Eng. Sci. 31: 1199–1205, 1976.</ref><ref name="Mah-Stanley-Downing-1976">[http://gregstanleyandassociates.com/ReconciliationRectificationProcessData-1976.pdf R.S.H. Mah, G.M. Stanley, D.W. Downing, \'\'Reconciliation and Rectification of Process Flow and Inventory Data\'\', Ind. & Eng. Chem. Proc. Des. Dev. 15: 175–183, 1976.]</ref> DVR also became more mature by considering general nonlinear equation systems coming from thermodynamic models.,<ref>J.C. Knepper, J.W. Gorman, \'\'Statistical Analysis of Constrained Data Sets\'\', AiChE Journal 26: 260–164, 1961.</ref>\n,<ref name="Stanley-Mah-1977">[http://gregstanleyandassociates.com/AIChEJ-1977-EstimationInProcessNetworks.pdf G.M. Stanley and R.S.H. Mah, \'\'Estimation of Flows and Temperatures in Process Networks\'\', AIChE Journal 23: 642–650, 1977.]</ref>\n<ref>P. Joris, B. Kalitventzeff, \'\'Process measurements analysis and validation\'\', Proc. CEF’87: Use Comput. Chem. Eng., Italy, 41–46, 1987.</ref> Quasi steady state dynamics for filtering and simultaneous parameter estimation over time were introduced in 1977 by Stanley and Mah.<ref name="Stanley-Mah-1977"/>  Dynamic DVR was formulated as a nonlinear optimization problem by Liebman et al. in 1992.<ref>M.J. Liebman, T.F. Edgar, L.S. Lasdon, \'\'Efficient Data Reconciliation and Estimation for Dynamic Processes Using Nonlinear Programming Techniques\'\', Computers Chem. Eng. 16: 963–986, 1992.</ref>\n\n==Data reconciliation==\nData reconciliation is a technique that targets at correcting measurement errors that are due to measurement noise, i.e. [[random error]]s. From a statistical point of view the main assumption is that no [[systematic errors]] exist in the set of measurements, since they may bias the reconciliation results and reduce the robustness of the reconciliation.\n\nGiven <math>n</math> measurements <math>y_i</math>, data reconciliation can mathematically be expressed as an [[optimization problem]] of the following form:\n\n<math> \\begin{align}\n \\min_{x,y^*} & \\sum_{i=1}^n\\left(\\frac{y_i^*-y_i}{\\sigma_i}\\right)^2 \\\\\n\\text{subject to  }      & F(x,y^*)=0 \\\\\n& y_\\min \\le y^*\\le y_\\max\\\\\n& x_\\min \\le x\\le x_\\max,\n\\end{align}\\,\\!\n</math>\n\nwhere\n<math>y_i^*\\,\\!</math> is the reconciled value of the <math>i</math>-th measurement (<math>i=1,\\ldots,n\\,\\!</math>), <math>y_i\\,\\!</math> is the measured value of the <math>i</math>-th measurement (<math>i=1,\\ldots,n\\,\\!</math>), <math>x_j\\,\\!</math> is the <math>j</math>-th unmeasured variable (<math>j=1,\\ldots,m\\,\\!</math>),  and <math>\\sigma_i\\,\\!</math> is the standard deviation of the <math>i</math>-th measurement (<math>i=1,\\ldots,n\\,\\!</math>),\n<math>F(x,y^*)=0\\,\\!</math> are the <math>p\\,\\!</math> process equality constraints and\n<math>x_{\\min}, x_{\\max}, y_{\\min}, y_{\\max}\\,\\!</math> are the bounds on the measured and unmeasured variables.\n\nThe term <math>\\left(\\frac{y_i^*-y_i}{\\sigma_i}\\right)^2\\,\\!</math> is called the \'\'penalty\'\' of measurement \'\'i\'\'. The objective function is the sum of the penalties, which will be denoted in the following by <math>f(y^*)=\\sum_{i=1}^n\\left(\\frac{y_i^*-y_i}{\\sigma_i}\\right)^2</math>.\n\nIn other words, one wants to minimize the overall correction (measured in the least squares term) that is needed in order to satisfy the [[constraint (mathematics)|system constraints]]. Additionally, each least squares term is weighted by the [[standard deviation]]  of the corresponding measurement.\n\n===Redundancy===\n<gallery caption="Sensor and topological redundancy" heights="150px" widths="225px" perrow="2" align="right">\nFile:sensor_red.jpg|Sensor redundancy arising from multiple sensors of the same quantity at the same time at the same place.\nFile:topological_red.jpg|Topological redundancy arising from model information, using the mass conservation constraint <math>a=b+c\\,\\!</math>, for example one can calculate <math>c\\,\\!</math>, when <math>a\\,\\!</math> and <math>b\\,\\!</math> are known.\n</gallery>\nData reconciliation relies strongly on the concept of redundancy to correct the measurements as little as possible in order to satisfy the process constraints.  Here, redundancy is defined differently from [[Redundancy (information theory)|redundancy in information theory]].  Instead, redundancy arises from combining sensor data with the model (algebraic constraints), sometimes more specifically called "spatial redundancy",<ref name="Stanley-Mah-1977"/> "analytical redundancy", or "topological redundancy". \n\nRedundancy can be due to [[redundancy (engineering)|sensor redundancy]], where sensors are duplicated in order to have more than one measurement of the same quantity. Redundancy also arises when a single variable can be estimated in several independent ways from separate sets of measurements at a given time or time averaging period, using the algebraic constraints.  \n\nRedundancy is linked to  the concept of [[observability]].  A variable (or system) is observable if the models and sensor measurements can be used to uniquely determine its value (system state).  A sensor is redundant if its removal causes no loss of observability.   Rigorous definitions of observability, calculability, and redundancy, along with criteria for determining it, were established by Stanley and Mah,<ref name="Stanley-Mah-1981a">\n[http://gregstanleyandassociates.com/whitepapers/DataRec/CES-1981a-ObservabilityRedundancy.pdf Stanley G.M. and Mah, R.S.H., "Observability and Redundancy in Process Data Estimation, Chem. Engng. Sci. 36, 259 (1981)]</ref> for these cases with set constraints such as algebraic equations and inequalities.    Next, we illustrate some special cases:\n\nTopological redundancy  is intimately linked with the [[degrees of freedom (physics and chemistry)|degrees of freedom]] (<math>dof\\,\\!</math>) of a mathematical system,<ref name="vdi">VDI-Gesellschaft Energie und Umwelt, "Guidelines - VDI 2048 Blatt 1 - Uncertainties of measurements at acceptance tests for energy conversion and power plants - Fundamentals", \'\'[http://www.vdi.de/401.0.html Association of German Engineers]\'\', 2000.</ref> i.e. the minimum number of pieces of information (i.e. measurements) that are required in order to calculate all of the system variables. For instance, in the example above the flow conservation requires that <math>a=b+c\\,</math>.  One needs to know the value of two of the 3 variables in order to calculate the third one. The degrees of freedom for the model in that case is equal to 2.  At least 2 measurements are needed to estimate all the variables, and 3 would be needed for redundancy.\n\nWhen speaking about topological redundancy we have to distinguish between measured and unmeasured variables. In the following let us denote by <math>x\\,\\!</math> the unmeasured variables and <math>y\\,\\!</math> the measured variables. Then the system of the process constraints becomes <math>F(x,y)=0\\,\\!</math>, which is a nonlinear system in <math>y\\,\\!</math> and <math>x\\,\\!</math>.\nIf the system <math>F(x,y)=0\\,\\!</math> is calculable with the <math>n\\,</math> measurements given, then the level of topological redundancy is defined as <math>red= n - dof\\,\\!</math>, i.e. the number of additional measurements that are at hand on top of those measurements which are required in order to just calculate the system. Another way of viewing the level of redundancy is to use the definition of <math>dof\\,</math>, which is the difference between the number of variables (measured and unmeasured) and the number of equations. Then one gets\n\n:<math>\\begin{align}\nred= n - dof = n-(n+m-p) = p-m,\n\\end{align}</math>\n\ni.e. the redundancy is the difference between the number of equations <math>p\\,</math> and the number of unmeasured variables <math>m\\,</math>. The level of total redundancy is the sum of sensor redundancy and topological redundancy. We speak of positive redundancy if the system is calculable and the total redundancy is positive. One can see that the level of topological redundancy merely depends on the number of equations (the more equations the higher the redundancy) and the number of unmeasured variables (the more unmeasured variables, the lower the redundancy) and not on the number of measured variables. \n\nSimple counts of variables, equations, and measurements are inadequate for many systems, breaking down for several reasons: (a) Portions of a system might have redundancy, while others do not, and some portions might not even be possible to calculate, and  (b) Nonlinearities can lead to different conclusions at different operating points.  As an example, consider the following system with 4 streams and 2 units.\n\n====Example of calculable and non-calculable systems====\n<gallery caption="Calculable and non-calculable systems" heights="150px" widths="225px" perrow="2" align="right">\nFile:calculable_system.jpg|Calculable system, from <math>d\\,\\!</math> one can compute <math>c\\,\\!</math>, and knowing <math>a\\,\\!</math> yields <math>b\\,\\!</math>.\nFile:uncalculable_system.jpg|non-calculable system, knowing <math>c\\,\\!</math> does not give information about <math>a\\,\\!</math> and <math>b\\,\\!</math>.\n</gallery>\n\nWe incorporate only flow conservation constraints and obtain <math>a+b=c\\,\\!</math> and <math>c=d\\,\\!</math>.  It is possible that the system <math>F(x,y)=0\\,\\!</math> is not calculable, even though <math>p-m\\ge 0\\,\\!</math>.\n\nIf we have measurements for <math>c\\,\\!</math> and <math>d\\,\\!</math>, but not for <math>a\\,\\!</math> and <math>b\\,\\!</math>, then the system cannot be calculated (knowing <math>c\\,\\!</math> does not give information about <math>a\\,\\!</math> and <math>b\\,\\!</math>). On the other hand, if <math>a\\,\\!</math> and <math>c\\,\\!</math> are known, but not <math>b\\,\\!</math> and <math>d\\,\\!</math>, then the system can be calculated.\n\nIn 1981, observability and redundancy criteria were proven for these sorts of flow networks involving only mass and energy balance constraints.<ref name="Stanley-Mah-1981b">[http://gregstanleyandassociates.com/whitepapers/DataRec/CES-1981b-ObservabilityRedundancyProcessNetworks.pdf Stanley G.M., and Mah R.S.H., "Observability and Redundancy Classification in Process Networks", Chem. Engng. Sci. 36, 1941 (1981) ]</ref>  After combining all the plant inputs and outputs into an "environment node",  loss of observability corresponds to cycles of unmeasured streams.  That is seen in the second case above, where streams a and b are in a cycle of unmeasured streams.  Redundancy classification follows, by testing for a path of unmeasured streams, since that would lead to an unmeasured cycle if the measurement was removed.  Measurements c and d are redundant in the second case above, even though part of the system is unobservable.\n\n===Benefits===\nRedundancy can be used as a source of information to cross-check and correct the measurements <math>y\\,\\!</math> and increase their accuracy and precision: on the one hand they reconciled Further, the data reconciliation problem presented above also includes unmeasured variables <math>x\\,\\!</math>. Based on information redundancy, estimates for these unmeasured variables can be calculated along with their accuracies. In industrial processes these unmeasured variables that data reconciliation provides are referred to as [[soft sensor]]s or virtual sensors, where hardware sensors are not installed.\n\n==Data validation==\nData validation denotes all validation and verification actions before and after the reconciliation step.\n\n===Data filtering===\nData filtering denotes the process of treating measured data such that the values become meaningful and lie within the range of expected values. Data filtering is necessary before the reconciliation process in order to increase robustness of the reconciliation step. There are several ways of data filtering, for example taking the [[average]] of several measured values over a well-defined time period.\n\n===Result validation===\nResult validation is the set of validation or verification actions taken after the reconciliation process and it takes into account measured and unmeasured variables as well as reconciled values. Result validation covers, but is not limited to, penalty analysis for determining the reliability of the reconciliation, or bound checks to ensure that the reconciled values lie in a certain range, e.g. the temperature has to be within some reasonable bounds.\n\n===Gross error detection===\nResult validation may include statistical tests to validate the reliability of the reconciled values, by checking whether [[systematic error|gross errors]] exist in the set of measured values. These tests can be for example\n* the chi square test (global test)\n* the individual test.\n\nIf no gross errors exist in the set of measured values, then each penalty term in the objective function is a [[normal distribution|random variable]] that is normally distributed with mean equal to 0 and variance equal to 1. By consequence, the objective function is a random variable which follows a [[chi-square distribution]], since it is the sum of the square of normally distributed random variables. Comparing the value of the objective function <math>f(y^*)\\,\\!</math> with a given [[percentile]] <math>P_{\\alpha}\\,</math> of the probability density function of a chi-square distribution (e.g. the 95th percentile for a 95% confidence) gives an indication of whether a gross error exists: If <math>f(y^*)\\le P_{95}</math>, then no gross errors exist with 95% probability. The chi square test gives only a rough indication about the existence of gross errors, and it is easy to conduct: one only has to compare the value of the objective function with the critical value of the chi square distribution.\n\nThe individual test compares each penalty term in the objective function with the critical values of the normal distribution. If the <math>i</math>-th penalty term is outside the 95% confidence interval of the normal distribution, then there is reason to believe that this measurement has a gross error.\n\n==Advanced data validation and reconciliation==\nAdvanced data validation and reconciliation (DVR) is an integrated approach of combining data reconciliation and data validation techniques, which is characterized by\n* complex models incorporating besides mass balances also thermodynamics, momentum balances, equilibria constraints, hydrodynamics etc.\n* gross error remediation techniques to ensure meaningfulness of the reconciled values,\n* robust algorithms for solving the reconciliation problem.\n\n===Thermodynamic models===\nSimple models include mass balances only. When adding thermodynamic constraints such as [[First law of thermodynamics|energy balances]] to the model, its scope and the level of [[Data redundancy|redundancy]] increases. Indeed, as we have seen above, the level of redundancy is defined as <math>p-m</math>, where <math>p</math> is the number of equations. Including energy balances means adding equations to the system, which results in a higher level of redundancy (provided that enough measurements are available, or equivalently, not too many variables are unmeasured).\n\n===Gross error remediation===\n[[image:scheme reconciliation.jpg|thumb|350px|The workflow of an advanced data validation and reconciliation process.]]\nGross errors are measurement systematic errors that may [[bias]] the reconciliation results. Therefore it is important to identify and eliminate these gross errors from the reconciliation process. After the reconciliation [[statistical tests]] can be applied that indicate whether or not a gross error does exist somewhere in the set of measurements. These techniques of gross error remediation are based on two concepts:\n* gross error elimination\n* gross error relaxation.\nGross error elimination determines one measurement that is biased by a systematic error and discards this measurement from the data set. The determination of the measurement to be discarded is based on different kinds of penalty terms that express how much the measured values deviate from the reconciled values. Once the gross errors are detected they are discarded from the measurements and the reconciliation can be done without these faulty measurements that spoil the reconciliation process. If needed, the elimination is repeated until no gross error exists in the set of measurements.\n\nGross error relaxation targets at relaxing the estimate for the uncertainty of suspicious measurements so that the reconciled value is in the 95% confidence interval. Relaxation typically finds application when it is not possible to determine which measurement around one unit is responsible for the gross error (equivalence of gross errors). Then measurement uncertainties of the measurements involved are increased.\n\nIt is important to note that the remediation of gross errors reduces the quality of the reconciliation, either the redundancy decreases (elimination) or the uncertainty of the measured data increases (relaxation). Therefore it can only be applied when the initial level of redundancy is high enough to ensure that the data reconciliation can still be done (see Section 2,<ref name="vdi" />).\n\n===Workflow===\nAdvanced DVR solutions offer an integration of the techniques mentioned above:\n# data acquisition from data historian, data base or manual inputs\n# data validation and filtering of raw measurements\n# data reconciliation of filtered measurements\n# result verification\n#* range check\n#* gross error remediation (and go back to step 3)\n# result storage (raw measurements together with reconciled values)\nThe result of an advanced DVR procedure is a coherent set of validated and reconciled process data.\n\n==Applications==\nDVR finds application mainly in industry sectors where either measurements are not accurate or even non-existing, like for example in the [[upstream (fossil-fuel industry)|upstream sector]] where [[flow measurement|flow meters]] are difficult or expensive to position (see <ref>P. Delava, E. Maréchal, B. Vrielynck, B. Kalitventzeff (1999), \'\'Modelling of a Crude Oil Distillation Unit in Term of Data Reconciliation with ASTM or TBP Curves as Direct Input – Application : Crude Oil Preheating Train\'\', Proceedings of ESCAPE-9 conference, Budapest, May 31-June 2, 1999, supplementary volume, p. 17-20.</ref>); or where accurate data is of high importance, for example for security reasons in [[nuclear power plants]] (see <ref>M. Langenstein, J. Jansky, B. Laipple (2004), \'\'Finding Megawatts in nuclear power plants with process data validation\'\', Proceedings of ICONE12, Arlington, USA, April 25–29, 2004.</ref>). Another field of application is [[Performance test (assessment)|performance and process monitoring]] (see <ref>Th. Amand, G. Heyen, B. Kalitventzeff, \'\'Plant Monitoring and Fault Detection: Synergy between Data Reconciliation and Principal Component Analysis\'\', Comp. and Chem, Eng. 25, p. 501-507, 2001.</ref>) in oil refining or in the chemical industry.\n\nAs DVR enables to calculate estimates even for unmeasured variables in a reliable way, the German Engineering Society (VDI Gesellschaft Energie und Umwelt) has accepted the technology of DVR as a means to replace expensive sensors in the nuclear power industry (see VDI norm 2048,<ref name="vdi" />).\n\n==See also==\n* [[Process simulation]]\n* [[Pinch analysis]]\n* [[Industrial processes]]\n* [[Chemical engineering]]\n\n==References==\n<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes -->\n{{Reflist}}\n\n* Alexander, Dave, Tannar, Dave & Wasik, Larry "Mill Information System uses Dynamic Data Reconciliation for Accurate Energy Accounting" TAPPI Fall Conference 2007.[http://www.tappi.org/Downloads/Conference-Papers/2007/07EPE/07epe87.aspx]\n* Rankin, J. & Wasik, L. "Dynamic Data Reconciliation of Batch Pulping Processes (for On-Line Prediction)" PAPTAC Spring Conference 2009.\n* S. Narasimhan, C. Jordache, \'\'Data reconciliation and gross error detection: an intelligent use of process data\'\', Golf Publishing Company, Houston, 2000.\n* V. Veverka, F. Madron, \'Material and Energy Balancing in the Process Industries\'\', Elsevier Science BV, Amsterdam, 1997.\n* J. Romagnoli, M.C. Sanchez, \'\'Data processing and reconciliation for chemical process operations\'\', Academic Press, 2000.\n\n\n{{DEFAULTSORT:Data Validation And Reconciliation}}\n[[Category:Data management]]']
['SQL programming tool', '12821559', "{{unreferenced|date=April 2010}}\nIn the field of [[software]], '''[[SQL]] programming tools''' provide platforms for [[database administrator]]s (DBAs) and [[application software|application]] developers to perform daily tasks efficiently and accurately.\n\nDatabase administrators and application developers often face constantly changing environments  which they rarely completely control. Many changes result from new development projects or from modifications to existing code, which, when deployed to production, do not always produce the expected result.\n\nFor organizations to better manage development projects and the [[team]]s that develop code, suppliers of SQL programming tools normally provide more than facility to the database administrator or application developer to aid in database management and in quality [[Software deployment|code-deployment]] practices.\n\n==Features==\n\nSQL programming tools may include the following features:\n\n===SQL editing===\n\nSQL editors allow users to edit and execute SQL statements. They may support the following features:\n\n* cut, copy, paste, undo, redo, find (and replace), bookmarks\n* block indent, print, save file, uppercase/lowercase\n* keyword highlighting\n* auto-completion\n* access to frequently used files\n* output of query result\n* editing query-results\n* committing and rolling-back transactions\n* inside cut paper\n\n===Object browsing===\n\nTools may display information about [[database object]]s relevant to developers or to database administrators. Users may:\n\n* view object descriptions\n* view object [[Data Definition Language|definition]]s (DDL)\n* create database objects\n* enable and disable [[database trigger|trigger]]s and [[database constraints|constraint]]s\n* recompile valid or invalid objects\n* query or edit [[Table (database)|table]]s and [[View (database)|view]]s\n\nSome tools also provide features to display dependencies among objects, and allow users to expand these dependent objects recursively (for example: packages may reference views, views generally reference tables, super/subtypes, and so on).\n\n===Session browsing===\n\nDatabase administrators and application developers can use session browsing tools to view the current activities of each user in the database. They can check the resource-usage of individual users, statistics information, locked objects and the current running SQL of each individual session.\n\n===User-security management===\n\nDBAs can create, edit, delete, disable or enable user-accounts in the database using security-management tools. DBAs can also assign [[database role|role]]s, system [[Privilege (computing)|privilege]]s, object privileges, and [[database storage|storage]]-quotas to users.\n\n===Debugging===\n\nSome tools offer features for the debugging of [[stored procedure]]s: [[program animation|Step In]], Step Over, Step Out, Run Until Exception, [[Breakpoint]]s, View & Set Variables, View Call Stack, and so on. Users can debug any program-unit without making any modification to it, including triggers and [[object type]]s.\n\n===Performance monitoring===\n\nMonitoring tools may show the database resources — usage summary, service time summary, recent activities, top sessions, session history or top SQL — in easy-to-read graphs. Database administrators can easily monitor the health of various components in the monitoring instance. Application developers may also make use of such tools to diagnose and correct application-performance problems as well as improve SQL server performance.\n\n===Test Data===\n\nTest data generation tools can populate the database by realistic test data for server or client side testing purposes. Also, this kind of software can upload sample BLOB files to database.\n\n==See also==\n* [[Comparison of database tools]]\n\n{{DEFAULTSORT:Sql Programming Tool}}\n[[Category:Data management]]\n[[Category:Relational database management systems]]"]
['Novell Storage Manager', '28205544', '{{Infobox software\n|name                       = Novell Storage Manager\n|logo                       =\n|screenshot                 =\n|caption                    =\n|collapsible                =\n|author                     =\n|developer                  = [[Novell]]\n|released                   = 2004 <!-- {{Start date|YYYY|MM|DD}} -->\n|discontinued               =\n|latest release version     = 4.1\n|latest release date        = {{Start date|2014|10|07}}\n|latest preview version     =\n|latest preview date        = <!-- {{Start date and age|YYYY|MM|DD}} -->\n|frequently updated         =\n|programming language       =\n|operating system           =\n|platform                   =\n|size                       =\n|language                   =\n|status                     =\n|genre                      = [[System Software]]\n|license                    =\n|website                    = [http://www.novell.com/products/storage-manager/ Novell Storage Manager]\n}}\n\n\'\'\'Novell Storage Manager\'\'\' is a [[system software]] package released by [[Novell]] in 2004 <ref>{{Citation | last=Greyzdorf| first=Noemi| title=Novell Delivers a New Way of Intelligently Managing Organizations\' File-Based Information| journal=IDC #216013 | volume=1| issue=Storage Software: Technology Assessment| year=2009| pages=1–3| url=http://www.novell.com/docrep/2009/01/Novell%20Delivers%20a%20New%20Way%20of%20Intelligently%20Managing%20Organizations_%20File-Based%20Information_en.pdf}}</ref> that uses identity, policy and [[Novell eDirectory|directory]] events to automate full lifecycle management of file storage for individual users and organizational groups. By tying storage management to an organization\'s existing identity infrastructure, it has been pointed out,<ref>{{Citation | last=Greyzdorf| first=Noemi| title=Efficiently Delivering Enterprise-Class File-Based Storage| journal=IDC Spotlight | year=2010| pages=1–5}}</ref> Novell Storage Manager enables the administration of users across all file servers "as a single pool rather than [in] separate independently managed domains." Novell Storage Manager is a component of the [[Novell File Management Suite]].\n\n==How It Works==\n\nNovell Storage Manager dynamically manages and provisions storage based on user and group events that occur in the directory, including user creations, group assignments, moves, renames, and deletions. When a change happens in the directory that affects a user’s file storage needs or user storage policy, Storage Manager applies the appropriate policy and makes the necessary changes at the file system level to address those storage needs.<ref>{{citation| title=Novell Storage Manager for Novell eDirectory | year=2009 | page=4 | url=http://www.novell.com/docrep/2009/04/Novell_Storage_Manager_for_Novell_eDirectory_White_Paper_en.pdf}}</ref>\n\nThe following key components comprise Novell Storage Manager\'s identity and policy-driven [[state machine]] architecture: Directory services; Storage policies; Novell Storage Manager event monitors; Novell Storage Manager policy engine; Novell Storage Manager agents; and Action objects. This state machine architecture enables the engine to properly deal with transient waits with directory synchronization issues. It also allows recovery from failures involving network communications, a target server or a server running a component of Storage Manager—including the policy engine itself. If a failure or interruption occurs at any point during operation, Storage Manager will be able to successfully continue the operation from where it was when the interruption occurred.\n\n==Reviews==\n\nJon Toigo called Novell Storage Manager "a robust and smart approach to corralling user files... into an organized and efficient management scheme".<ref>{{Citation| last = Toigo | first = Jon William | title = Novell Storage Manager Strikes Data Management Gold | url= http://esj.com/articles/2009/04/28/novell-storage-mgr.aspx | accessdate = 26 July 2010}}</ref> He also said it was "best in class" of the products he\'d reviewed.<ref>{{Citation| last = Toigo | first = Jon William | title = Everything We Need to Know About How to Screw Up IT… | url=http://www.drunkendata.com/?p=2916 | accessdate = 30 July 2010}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.novell.com/products/storage-manager/ Novell Storage Manager: Product homepage] - Overview, features, and technical information\n*[http://www.storagemanagersupport.com/nsm/ Novell Storage Manager: Support]\n\n{{Novell}}\n\n[[Category:Novell]]\n[[Category:Novell software]]\n[[Category:Storage software]]\n[[Category:Data management]]\n[[Category:Identity management]]']
['Microsoft Query', '684176', "{{Refimprove|date=December 2009}}\n\n{{wikibooks|Structured Query Language}}\n{{wikibooks|SQL dialects reference}}\n\n'''Microsoft Query''' is a visual method of creating [[database query|database queries]] using examples based on a text string, the name of a [[document]] or a list of documents. The QBE system converts the user input into a formal database query using [[SQL|Structured Query Language]] (SQL) on the backend, allowing the user to perform powerful searches without having to explicitly compose them in SQL, and without even needing to know SQL. It is derived from Moshé M. Zloof's original [[Query by Example]] (QBE) implemented in the mid-1970s at [[IBM]]'s Research Centre in [[Yorktown, New York]].<ref>Zloof, M. M., [http://dx.doi.org/10.1147/sj.164.0324 Query-by-Example: A data base language]</ref>\n\nIn the context of [[Microsoft Access]], QBE is used for introducing students to database querying, and as a user-friendly [[database management system]] for small businesses.\n\n[[Microsoft Excel]] allows results of QBE queries to be embedded in spreadsheets.<ref>[https://support.office.com/en-us/article/Use-Microsoft-Query-to-retrieve-external-data-42a2ea18-44d9-40b3-9c38-4c62f252da2e Use Microsoft Query to retrieve external data]</ref>\n\n==See also==\n*[[Query by Example]]\n*[[Microsoft Access]]\n*[[Microsoft SQL Server]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Microsoft Query By Example}}\n[[Category:Data management]]\n[[Category:Microsoft database software]]\n\n\n{{Microsoft-software-stub}}\n{{database-software-stub}}"]
['Enterprise manufacturing intelligence', '10612614', "{{unreferenced|date=February 2013}}\n'''Enterprise manufacturing intelligence (EMI),''' or simply manufacturing intelligence (MI), is a term which applies to software used to bring a corporation's manufacturing-related data together from many sources for the purposes of reporting, analysis, visual summaries, and passing data between enterprise-level and plant-floor systems. As data is combined from multiple sources, it can be given a new structure or context that will help users find what they need regardless of where it came from.  The primary goal is to turn large amounts of manufacturing data into real knowledge and drive business results based on that knowledge.{{reflist}}\n\n== Core functions ==\n\n[[AMR Research]] has identified five core functions every Enterprise Manufacturing Intelligence application should possess:\n\n* '''Aggregation:''' Making available data from many sources, most often databases.\n* '''Contextualization:''' Providing a structure, or model, for the data that will help users find what they need.  Usually a folder tree utilizing a hierarchy such as the [[ISA-95]] standard.\n* '''Analysis:''' Enabling users to analyze data across sources and especially across production sites.  This often includes the ability for true ''ad hoc'' reporting.\n* '''Visualization:''' Providing tools to create visual summaries of the data to alert decision makers and call attention to the most important information of the moment.  The most common visualization tool is the [[Dashboard (business)|dashboard.]]\n* '''Propagation:''' Automating the transfer of data from the plant-floor up to enterprise-level systems or vice versa.\n\n{{DEFAULTSORT:Enterprise Manufacturing Intelligence}}\n[[Category:Data management]]"]
['Dashboard (business)', '4166591', '[[File:3 Dashboards.JPG|thumb|200px|Business Dashboards.]]\n\n\'\'\'Dashboards\'\'\' often provide at-a-glance views of KPIs ([[key performance indicators]])  relevant to a particular objective or business process (e.g. [[sales]], [[marketing]], [[human resources]], or [[Production (economics)|production]]).<ref>Michael Alexander and John Walkenbach, \'\'Excel Dashboards and Reports\'\' (Wiley, 2010)</ref> In real-world terms, "dashboard" is another name for "progress report" or "report."\n\nOften, the "dashboard" is displayed on a web page that is linked to a database which allows the report to be constantly updated. For example, a manufacturing dashboard may show numbers related to productivity such as number of parts manufactured, or number of failed quality inspections per hour. Similarly, a human resources dashboard may show numbers related to staff recruitment, retention and composition, for example number of open positions, or average days or cost per recruitment.<ref name="Briggs">{{cite web|url=http://www.targetdashboard.com/site/Dashboard-Best-Practice/Management-Report-and-Dashboard-best-practice-index.aspx|title=Management Reports & Dashboard Best Practice|last=Briggs|first=Jonathan|publisher=Target Dashboard|accessdate=18 February 2013}}</ref>\n\nThe term dashboard originates from the [[automobile]] [[dashboard]] where drivers   monitor the major functions at a glance via the instrument cluster. \n\n==Benefits==\nDigital dashboards allow managers to monitor the contribution of the various departments in their organization. To gauge exactly how well an organization is performing overall, digital dashboards allow you to capture and report specific data points from each department within the organization, thus providing a "snapshot" of performance.\n\nBenefits of using digital dashboards include:<ref name="Briggs" />\n*Visual presentation of performance measures\n*Ability to identify and correct negative trends\n*Measure efficiencies/inefficiencies\n*Ability to generate detailed reports showing new trends\n*Ability to make more informed decisions based on collected [[business intelligence]]\n*Align strategies and organizational goals\n*Saves time compared to running multiple reports\n*Gain total visibility of all systems instantly\n*Quick identification of data outliers and correlations\n\n==Classification==\nDashboards can be broken down according to role and are either [[strategic]], analytical, operational, or informational.<ref>Steven Few, \'\'Information Dashboard Design: The Effective Visual Communication of Data\'\' (O\'Reilly, 2006)</ref> Strategic dashboards support managers at any level in an organization, and provide the quick overview that decision makers need to monitor the health and opportunities of the business. Dashboards of this type focus on high level measures of performance, and forecasts. Strategic dashboards benefit from static snapshots of data (daily, weekly, monthly, and quarterly) that are not constantly changing from one moment to the next. Dashboards for analytical purposes often include more context, comparisons, and history, along with subtler performance evaluators. Analytical dashboards typically support interactions with the data, such as drilling down into the underlying details. Dashboards for monitoring operations are often designed differently from those that support strategic decision making or data analysis and often require monitoring of activities and events that are constantly changing and might require attention and response at a moment\'s notice.\n\n==Types of dashboards==\n\nDigital dashboards may be laid out to track the flows inherent in the business processes that they monitor. Graphically, users may see the high-level processes and then [[data drilling|drill down]] into low level data. This level of detail is often buried deep within the corporate enterprise and otherwise unavailable to the senior executives.\n\nThree main types of digital dashboard dominate the market today: stand alone software applications, web-browser based applications, and desktop applications also known as [[desktop widgets]]. The last are driven by a [[Software widget|widget engine]].\n\nSpecialized dashboards may track all corporate functions. Examples include [[human resources]], [[Recruitment|recruiting]], [[sales]], [[Business operations|operations]], [[security]], [[information technology]], [[project management]], [[customer relationship management]] and many more departmental dashboards. For a smaller organization like a startup a compact startup scorecard dashboard tracks important activities across lot of domains ranging from social media to sales.{{cn|date=September 2016}}\n\nDigital dashboard projects involve business units as the driver and the information technology department as the enabler. The success of digital dashboard projects often depends on the [[measurement|metrics]] that were chosen for monitoring. [[Key performance indicator]]s, [[balanced scorecard]]s, and sales performance figures are some of the content appropriate on business dashboards.\n\n==Dashboards and scoreboards==\nBalanced Scoreboards and Dashboards have been linked together as if they were interchangeable. However, although both visually display critical information, the difference is in the format: Scoreboards can open the quality of an operation while dashboards provide calculated direction. \nA balanced scoreboard has what they called a "prescriptive" format. It should always contain these components (Active Strategy)...\n*Perspectives – groupings of high level strategic areas\n*Objectives – verb-noun phrases pulled from a strategy plan\n*Measures – also called Metric or Key Performance Indicators (KPIs)\n*Spotlight Indicators – red, yellow, or green symbols that provide an at-a-glance view of a measure’s performance.\nEach of these sections ensures that a Balanced Scorecard is essentially connected to the businesses critical strategic needs.\n\nThe design of a dashboard is more loosely defined.  Dashboards are usually a series of graphics, charts, gauges and other visual indicators that can be monitored and interpreted.  Even when there is a strategic link, on a dashboard, it may not be noticed as such since objectives are not normally present on dashboards.  However, dashboards can be customized to link their graphs and charts to strategic objectives.<ref>ZSL Inc., \'\'Dashboards Vs Scorecards – An Insight\'\' ZSL Inc. (2006)</ref>\n\n==Design==\nDigital dashboard technology is available "out-of-the-box" from many software providers. Some companies however continue to do in-house development and maintenance of dashboard applications. For example, [[GE Aviation]] has developed a proprietary software/portal called "Digital Cockpit" to monitor the trends in aircraft spare parts business.\n\nA good information design will clearly communicate key information to users and makes supporting information easily accessible.<ref>Stacey Barr, \'\'7 Small Business Dashboard Design Dos and Don\'ts\'\' (Barr, 2010)</ref>\n\n==Assessing the quality of dashboards==\nThere are four key elements to a good dashboard:.<ref>Victoria Hetherington, \'\'Dashboard Demystified: What is a Dashboard?\'\' (Hetherington, 2009)</ref>\n# Simple, communicates easily\n# Minimum distractions...it could cause confusion\n# Supports organized business with meaning and useful data\n# Applies human visual perception to visual presentation of information\n\n==History==\n\nThe idea of digital dashboards followed the study of [[decision support system]]s in the 1970s. Early predecessors of the modern business dashboard were first developed in the 1980s in the form of [[Executive Information Systems]] (EISs). Due to problems primarily with data refreshing and handling, it was soon realized that the approach wasn’t practical as information was often incomplete, unreliable, and spread across too many disparate sources.<ref>Steven Few, \'\'Information Dashboard Design: The Effective Visual Communication of Data\'\' (O\'Reilly, 2006)</ref> Thus, EISs hibernated until the 1990s when the information age quickened pace and data warehousing, and [[online analytical processing]] (OLAP) allowed dashboards to function adequately.{{fact|date=August 2014}} Despite the availability of enabling technologies, the dashboard use didn\'t become popular until later in that decade, with the rise of [[key performance indicators]] (KPIs), and the introduction of Robert S. Kaplan and David P. Norton\'s [[Balanced Scorecard]].<ref>[[Wayne W. Eckerson]], \'\'Performance Dashboards: Measuring, Monitoring, and Managing Your Business\'\' (Wiley , 2010)</ref> In the late 1990s, [[Microsoft]] promoted a concept known as the [[Digital Nervous System]] and "digital dashboards" were described as being one leg of that concept.<ref>{{cite web | url=http://www.kmworld.com/Articles/News/Breaking-News/Microsoft-refines-Digital-Dashboard-concept--12189.aspx | title=Microsoft refines Digital Dashboard concept | accessdate=2009-06-09}}</ref> Today, the use of dashboards forms an important part of Business Performance Management (BPM).\n\n==See also==\n* [[Business activity monitoring]]\n* [[Complex event processing]]\n* [[Corporate performance management]]\n* [[Data presentation architecture]]\n* [[Enterprise manufacturing intelligence]]\n* [[Event stream processing]]\n* [[Infographic|Information graphics]]\n* [[Information design]]\n* [[Scientific visualization]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite book\n  |title=Information Dashboard Design\n  |last=Few   |first=Stephen\n  |publisher=O\'Reilly\n  |isbn=978-0-596-10016-2\n  |date=2006\n}}\n\n* {{cite book\n  |title=Performance Dashboards: Measuring, Monitoring, and Managing Your Business\n  |last=Eckerson|first=Wayne W |author-link=\n  |publisher=John Wiley & Sons\n  |isbn=978-0-471-77863-9\n  |date=2006\n}}\n\n{{Data warehouse}}\n\n{{DEFAULTSORT:Dashboard (Business)}}\n{{Use dmy dates|date=April 2011}}\n[[Category:Business terms]]\n[[Category:Computing terminology]]\n[[Category:Data warehousing]]\n[[Category:Data management]]\n[[Category:Business software]]\n[[Category:Information systems]]']
['Rainbow storage', '8098559', '{{notability|date=November 2014}}\n\'\'\'Rainbow storage\'\'\' is a developing [[paper]]-based [[data storage device|data storage]] technique first demonstrated by Indian student Sainul Abideen in November 2006.<ref name="ArabNews">[http://www.arabnews.com/?page=4&section=0&article=88962&d=18&m=11&y=2006 "Data Can Now Be Stored on Paper"] by M. A. Siraj, \'\'[[ArabNews]]\'\' (published November 18, 2006; accessed November 29, 2006)</ref> Abideen received his [[Master of Computer Applications|MCA]] from [[MES College of Engineering|MES Engineering College]] in [[Kuttipuram]] in [[Kerala]]\'s [[Malappuram]] district.\n\nInitial newspaper reports of the technology were disputed by multiple technical sources, although Abideen says those reports were based on a misunderstanding of the technology. The paper meant to demonstrate the capability of storing relatively large amounts of data (and not necessarily in the gigabyte range) using textures and diagrams.<ref name=theinq>[http://www.theinquirer.net/default.aspx?article=36294 Paper storage man misunderstood] &mdash; \'\'[[The Inquirer]]\'\' article, 12 December 2006 (retrieved 15 December 2006.</ref>\n\nThe Rainbow data storage technology claims to use [[Geometry|geometric]] shapes such as triangles, circles and squares of various colors to store a large amount of data on ordinary paper or plastic surfaces. This would provide several advantages over current forms of [[Optical disc|optical-]] or [[Magnetic storage|magnetic]] [[data storage device|data storage]] like less environmental pollution due to the biodegradability of paper, low cost and high capacity. Data could be stored on "Rainbow Versatile Disk" (RVD) or plastic/paper cards of any form factor (like SIM cards).<ref name="Techworld.com">[http://www.techworld.com/storage/news/index.cfm?newsID=7424 "Store 256GB on an A4 sheet"] by Chris Mellor, Techworld (published November 24, 2006; accessed November 29, 2006)</ref>\n\n==Criticism==\nFollowing the wide media attention this news received, some of the claims have been disputed by various experts.<ref name="ITSoup"> [http://itsoup.blogspot.com/2006/11/scam-of-indian-student-developing.html IT Soup: Scam of Indian student developing technology to store 450 GB of data on a sheet of paper] By ITSoup (published November 25, 2006; accessed November 25, 2006)</ref>\t <ref name="ArsTechnica"> [http://arstechnica.com/news.ars/post/20061126-8288.html "Can you get 256GB on an A4 sheet? No way!"]  By Chris Mellor, Techworld (published November 24, 2006; accessed November 29, 2006)</ref>\t\n\nPrinting at 1,200 dots per inch (DPI) leads to a theoretical maximum of 1,440,000 colored dots per square inch. If a scanner can reliably distinguish between 256 unique colors (thus encoding one byte per dot), the maximum possible storage is approximately 140 megabytes for a sheet of A4 paper&ndash;much lower when the necessary error correction is employed. If the scanner were able to accurately distinguish between 16,777,216 colors (24 bits, or 3 bytes per dot), the capacity would triple, but it still falls well below the media stories\' claims of several hundred gigabytes.\n\nPrinting this quantity of unique colors would require specialized equipment to generate many [[spot color]]s.  The [[process color]] model used by most printers provides only four colors, with additional colors simulated by a [[halftone|halftone pattern]].\n\nAt least one of three things must be true for the claim to be valid:\n* The paper must be printed and scanned at a much higher resolution than 1,200 DPI, \t \n* the printer and scanner must be able to accurately produce and distinguish between an extraordinary number of distinct color values, or \t \n* the compression scheme must be a revolutionary [[lossless compression]] algorithm. \t \n\nThe theory is: If Rainbow\'s "geometric" algorithm is to be encoded and decoded by a computer, it would equally viable to store the compressed data on a conventional disk rather than printing it to paper or other non-digital medium. Printing something as dots on a page rather than bits on a disk will not change the underlying compression ratio, so a lossless compression algorithm that could store 250 gigabytes within a few hundred megabytes of data would be revolutionary indeed. Likewise, data can be compressed with \'\'any\'\' algorithm and subsequently printed to paper as colored dots. The amount of data that can be reliably stored in this way is limited by the printer and scanner, as described above.\n\nHowever Sainul Abideen says that the articles are based on misunderstandings. He claims, it as a method to store data in the form of colour, in any medium where colour can be represented, not only paper. Density of storage in paper will be very small and the density will be depends on the storage medium, capacity of colour representation and retrieval methods etc.\n\n==Demonstrations==\nSainul Abdeen demonstrated his technology to the college and members of the Indian press in the MES College of Engineering computer lab, Kerala, and was able to compress 450 sheets plain text from [[Foolscap folio|foolscap paper]] into a 1 inch square. He also demonstrated a 45-second audio clip compressed using this technology on to an [[ISO 216|A4 sheet]]. Depending on the sampling frequency, bit depth, and audio compression (if any), a 45-second audio clip can consist of anywhere from a few kilobytes to a few megabytes of data.  Abideen claimed that the technology could be extended to 250 gigabytes by using specific materials and devices. {{Fact|date=June 2009}}\n\nThis technology is based on two principles:\n\n;Principle I\n:“Every color or color combinations can be converted into some values and from the values the colors or color combinations can be regenerated”.\n;Principle II\n:“Every different color or color combinations will produce different values”.\n\n==References==\n{{reflist}}\n\n\n==Absolute Rainbow Dots==\nAbsolute rainbow dots are used to detect errors caused by scratches, and whether any fading has occurred. Absolute rainbow dots are predefined dots carrying a unique value. These dots can be inserted in the rainbow picture in pre-specified areas. If fading occurs these dot values will change accordingly, and at the reproduction stage this can be checked and corrected.\nAbsolute rainbow dots will be microscopically small so that they occupy very little space in the rainbow picture. These will be colored differently so that each dot will have its own fixed unique value.\n\n==External links==\n* [http://www.kerlontech.com/RandD.html Sainul Abideen\'s home page] (dead)\n* [http://www.deccanherald.com/deccanherald/Sep62006/cyberspace163748200695.asp Deccan Herald\'s article on Rainbow Storage] (dead)\n* [http://www.dailytech.com/article.aspx?newsid=5052 Article in DailyTech,]\n* [http://itsoup.blogspot.com/2006/11/scam-of-indian-student-developing.html IT Soup: Scam of Indian student developing technology to store 450 GB of data on a sheet of paper]\n* [http://www.theregister.co.uk/2006/11/23/rvd_system/ Article in The Register]\n*[http://www.idm.net.au/storypages/storydata.asp?id=7749 IDM: Paper Storage Claims A Hoax?]  (dead)\n\n[[Category:Data management]]\n[[Category:Vaporware]]']
['Bitmap index', '2017214', 'A \'\'\'bitmap index\'\'\' is a special kind of [[Index (database)|database index]] that uses [[Bit array|bitmap]]s.\n\nBitmap indexes have traditionally been considered to work well for \'\'low-cardinality columns\'\', which have a modest number of distinct values, either absolutely, or relative to the number of records that contain the data. The extreme case of low cardinality is Boolean data (e.g., does a resident in a city have internet access?), which has two values, True and False. Bitmap indexes use [[bit array]]s (commonly called bitmaps) and answer queries by performing [[bitwise operation|bitwise logical operation]]s on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional [[B-tree]] indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for [[online transaction processing]] applications.\n\nSome researchers argue that bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively.<ref name="sharma">[http://www.oracle.com/technetwork/articles/sharma-indexes-093638.html Bitmap Index vs. B-tree Index: Which and When?], Vivek Sharma, Oracle Technical Network.</ref>\n\nBitmap indexes are also useful in [[data warehousing]] applications for joining a large [[fact table]] to smaller [[dimension table]]s such as those arranged in a [[star schema]].\n\nBitmap based representation can also be used for representing a data structure which is labeled and directed attributed multigraph, used for queries in [[graph databases]].<code>[http://www.researchgate.net/publication/236593640_Efficient_graph_management_based_on_bitmap_indices Efficient graph management based on bitmap indices]</code> article shows how bitmap index representation can be used to manage large dataset(billions of data points) and answer queries related to graph efficiently.\n\n==Example==\nContinuing the internet access example, a bitmap index may be logically viewed as follows:\n{| class="wikitable" style="text-align:center; float:left"\n|-\n!rowspan=2| Identifier\n!rowspan=2| HasInternet\n!colspan=2| Bitmaps\n|-\n!Y !! N\n|-\n|1 || Yes || 1|| 0\n|-\n|2 || No || 0 || 1\n|-\n|3 || No || 0 || 1\n|-\n|4 || Unspecified || 0 || 0\n|-\n|5 || Yes || 1 || 0\n|}\n\nOn the left, [[Identifier]] refers to the unique number assigned to each resident, HasInternet is the data to be indexed, the content of the bitmap index is shown as two columns under the heading \'\'bitmaps\'\'. Each column in the left illustration is a \'\'bitmap\'\' in the bitmap index. In this case, there are two such bitmaps, one for "has internet" \'\'Yes\'\' and one for "has internet" \'\'No\'\'. It is easy to see that each bit in bitmap \'\'Y\'\' shows whether a particular row refers to a person who has internet access. This is the simplest form of bitmap index. Most columns will have more distinct values. For example, the sales amount is likely to have a much larger number of distinct values. Variations on the bitmap index can effectively index this data as well. We briefly review three such variations.\n\nNote: Many of the references cited here are reviewed at ([[#JohnWu2007|John Wu (2007)]]).<ref>{{cite web|ref=JohnWu2007|author=John Wu |year=2007 |title=Annotated References on Bitmap Index |url=http://www.cs.umn.edu/~kewu/annotated.html}}</ref> For those who might be interested in experimenting with some of the ideas mentioned here, many of them are implemented in open source software such as FastBit,<ref>[http://codeforge.lbl.gov/projects/fastbit/ FastBit]</ref> the Lemur Bitmap Index C++ Library,<ref>[https://code.google.com/p/lemurbitmapindex/ Lemur Bitmap Index C++ Library]</ref> the Roaring Bitmap Java library,<ref>[http://roaringbitmap.org/ Roaring bitmaps]</ref>  the [[Apache Hive]] Data Warehouse system and [[LucidDB]].\n\n{{Clear}}\n\n==Compression==\nSoftware can [[data compression|compress]] each bitmap in a bitmap index to save spaces.  There has been considerable amount of work on this subject.<ref>{{cite book |author=T. Johnson |editor1=Malcolm P. Atkinson |editor2=[[Maria Orłowska|Maria E. Orlowska]] |editor3=Patrick Valduriez |editor4=Stanley B. Zdonik |editor5=Michael L. Brodie | title = VLDB\'99, Proceedings of 25th International Conference on Very Large Data Bases, September 7–10, 1999, Edinburgh, Scotland, UK | publisher = Morgan Kaufmann | year = 1999 | isbn = 1-55860-615-7 | chapter =Performance Measurements of Compressed Bitmap Indices | pages=278–89 | url=http://www.vldb.org/conf/1999/P29.pdf }}</ref><ref>{{cite web |vauthors=Wu K, Otoo E, Shoshani A | title=On the performance of bitmap indices for high cardinality attributes | date=March 5, 2004 | url=http://www.osti.gov/energycitations/servlets/purl/822860-LOzkmz/native/822860.pdf }}</ref>\nThough there are exceptions such as Roaring bitmaps,<ref name=roaring>{{Cite journal | last1 = Chambi | first1 = S. | last2 = Lemire | first2 = D. | last3 = Kaser | first3 = O. | last4 = Godin | first4 = R.  | title = Better bitmap performance with Roaring bitmaps | doi = 10.1002/spe.2325 | journal = Software: Practice & Experience | volume = 46 | pages = 5 | year = 2016 | pmid =  | pmc = }}</ref> Bitmap compression algorithms typically employ [[run-length encoding]], such as the Byte-aligned Bitmap Code,<ref>{{US Patent|5363098|Byte aligned data compression}}</ref> the Word-Aligned Hybrid code,<ref>{{US Patent|6831575|Word aligned bitmap compression method, data structure, and apparatus}}</ref> the Partitioned Word-Aligned Hybrid (PWAH) compression,<ref>{{cite conference |url=http://dl.acm.org/citation.cfm?doid=1989323.1989419 |title=A memory efficient reachability data structure through bit vector compression | last1=van Schaik | first1=Sebastiaan |last2=de Moor |first2=Oege |year=2011 |publisher=ACM |booktitle=Proceedings of the 2011 international conference on Management of data |pages=913–924 |location=Athens, Greece |doi=10.1145/1989323.1989419 |conference=SIGMOD \'11 |isbn=978-1-4503-0661-4 }}</ref> the Position List Word Aligned Hybrid,<ref name="doi_10.1145/1739041.1739071">{{cite book | chapter = Position list word aligned hybrid: optimizing space and performance for compressed bitmaps |vauthors=Deliège F, Pedersen TB |editor1=Ioana Manolescu |editor2=Stefano Spaccapietra |editor3=Jens Teubner |editor4=Masaru Kitsuregawa |editor5=Alain Leger |editor6=Felix Naumann |editor7=Anastasia Ailamaki |editor8=Fatma Ozcan | title = EDBT \'10, Proceedings of the 13th International Conference on Extending Database Technology | publisher = ACM | location = New York, NY, USA | year = 2010 | pages = 228–39 | isbn = 978-1-60558-945-9 | doi = 10.1145/1739041.1739071 | url = http://alpha.uhasselt.be/icdt/edbticdt2010proc/edbt/papers/p0228-Deliege.pdf }}</ref> the Compressed Adaptive Index (COMPAX),<ref name="autogenerated1382">{{cite journal|author1=F. Fusco |author2=M. Stoecklin |author3=M. Vlachos |title=NET-FLi: on-the-fly compression, archiving and indexing of streaming network traffic |date=September 2010 | volume = 3 | issue = 1–2 | pages = 1382–93 | journal=Proc. VLDB Endow | url=http://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/I01.pdf }}</ref> Enhanced Word-Aligned Hybrid (EWAH) <ref name=ewah>{{Cite journal | last1 = Lemire | first1 = D. | last2 = Kaser | first2 = O. | last3 = Aouiche | first3 = K. | title = Sorting improves word-aligned bitmap indexes | doi = 10.1016/j.datak.2009.08.006 | journal = Data & Knowledge Engineering | volume = 69 | pages = 3 | year = 2010 | pmid =  | pmc = }}</ref> and the COmpressed \'N\' Composable Integer SEt.<ref>[http://ricerca.mat.uniroma3.it/users/colanton/concise.html Concise: Compressed \'n\' Composable Integer Set] {{webarchive |url=https://web.archive.org/web/20110528033714/http://ricerca.mat.uniroma3.it/users/colanton/concise.html |date=May 28, 2011 }}</ref><ref name="doi_10.1016/j.ipl.2010.05.018" /> These compression methods require very little effort to compress and decompress. More importantly, bitmaps compressed with BBC, WAH, COMPAX, PLWAH, EWAH and CONCISE can directly participate in [[bitwise operation]]s without decompression. This gives them considerable advantages over generic compression techniques such as [[LZ77]]. BBC compression and its derivatives are used in a commercial [[database management system]]. BBC is effective in both reducing index sizes and maintaining [[database query|query]] performance. BBC encodes the bitmaps in [[bytes]], while WAH encodes in words, better matching current [[CPU]]s. "On both synthetic data and real application data, the new word aligned schemes use only 50% more space, but perform logical operations on compressed data 12 times faster than BBC."<ref>{{cite book |vauthors=Wu K, Otoo EJ, Shoshani A |editor1=Henrique Paques |editor2=Ling Liu |editor3=David Grossman | chapter =A Performance comparison of bitmap indexes | year=2001 | title = CIKM \'01 Proceedings of the tenth international conference on Information and knowledge management | publisher = ACM | location = New York, NY, USA | pages = 559–61 | isbn = 1-58113-436-3 | doi = 10.1145/502585.502689 | url = http://crd.lbl.gov/~kewu/ps/LBNL-48975.pdf }}</ref> PLWAH bitmaps were reported to take 50% of the storage space consumed by WAH bitmaps and offer up to 20% faster performance on [[logical operation]]s.<ref name="doi_10.1145/1739041.1739071" /> Similar considerations can be done for CONCISE <ref name="doi_10.1016/j.ipl.2010.05.018">{{cite journal |vauthors=Colantonio A, Di Pietro R | title=Concise: Compressed \'n\' Composable Integer Set | journal = Information Processing Letters | volume = 110 | issue = 16 | date = 31 July 2010 | doi = 10.1016/j.ipl.2010.05.018 | url = http://ricerca.mat.uniroma3.it/users/colanton/docs/concise.pdf |pages=644–50 }}</ref> and Enhanced Word-Aligned Hybrid.<ref name="ewah"/>\n\nThe performance of schemes such as BBC, WAH, PLWAH, EWAH, COMPAX and CONCISE is dependent on the order of the rows. A simple lexicographical sort can divide the index size by 9 and make indexes several times faster.<ref>{{cite journal|author1=D. Lemire |author2=O. Kaser |author3=K. Aouiche |title=Sorting improves word-aligned bitmap indexes |journal=Data & Knowledge Engineering | volume=69 | issue=1 |date=January 2010 |arxiv=0901.3751 | doi = 10.1016/j.datak.2009.08.006|pages=3–28 }}</ref> The larger the table, the more important it is to sort the rows. Reshuffling techniques have also been proposed to achieve the same results of sorting when indexing streaming data.<ref name="autogenerated1382"/>\n\n==Encoding==\nBasic bitmap indexes use one bitmap for each distinct value. It is possible to reduce the number of bitmaps used by using a different encoding method.<ref name="autogenerated355">{{cite book |chapter=Bitmap index design and evaluation |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  |lastauthoramp=yes | year=1998 | title = Proceedings of the 1998 ACM SIGMOD international conference on Management of data (SIGMOD \'98) |editor1=Ashutosh Tiwary |editor2=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 355–6 | doi=10.1145/276304.276336 | url = http://www.comp.nus.edu.sg/~chancy/sigmod98.pdf }}</ref><ref>{{cite book |chapter=An efficient bitmap encoding scheme for selection queries |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  |lastauthoramp=yes | year=1999 | title = Proceedings of the 1999 ACM SIGMOD international conference on Management of data (SIGMOD \'99) | publisher = ACM | location = New York, NY, USA | pages = 215–26 | doi = 10.1145/304182.304201 | url = http://www.ist.temple.edu/~vucetic/cis616spring2005/papers/P4%20p215-chan.pdf }}</ref> For example, it is possible to encode C distinct values using log(C) bitmaps with binary encoding.<ref>{{cite journal |author1=P. E. O\'Neil  |author2=D. Quass |lastauthoramp=yes | chapter = Improved Query Performance with Variant Indexes | title = Proceedings of the 1997 ACM SIGMOD international conference on Management of data (SIGMOD \'97) | year = 1997 |editor1=Joan M. Peckman |editor2=Sudha Ram |editor3=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 38–49| doi=10.1145/253260.253268 }}</ref>\n\nThis reduces the number of bitmaps, further saving space, but to answer any query, most of the bitmaps have to be accessed. This makes it potentially not as effective as scanning a vertical projection of the base data, also known as a [[materialized view]] or projection index. Finding the optimal encoding method that balances (arbitrary) query performance, index size and index maintenance remains a challenge.\n\nWithout considering compression, Chan and Ioannidis analyzed a class of multi-component encoding methods and came to the conclusion that two-component encoding sits at the kink of the performance vs. index size curve and therefore represents the best trade-off between index size and query performance.<ref name="autogenerated355"/>\n\n==Binning==\nFor high-cardinality columns, it is useful to bin the values, where each bin covers multiple values and build the bitmaps to represent the values in each bin. This approach reduces the number of bitmaps used regardless of encoding method.<ref>{{cite book |chapter = Space efficient bitmap indexing| title= Proceedings of the ninth international conference on Information and knowledge management (CIKM \'00) | year=2000 | author =N. Koudas | publisher = ACM | location = New York, NY, USA | pages = 194–201 | doi=10.1145/354756.354819 }}</ref> However, binned indexes can only answer some queries without examining the base data. For example, if a bin covers the range from 0.1 to 0.2, then when the user asks for all values less than 0.15, all rows that fall in the bin are possible hits and have to be checked to verify whether they are actually less than 0.15. The process of checking the base data is known as the candidate check. In most cases, the time used by the candidate check is significantly longer than the time needed to work with the bitmap index. Therefore, binned indexes exhibit irregular performance. They can be very fast for some queries, but much slower if the query does not exactly match a bin.\n\n==History==\nThe concept of bitmap index was first introduced by Professor Israel Spiegler and Rafi Maayan in their research "Storage and Retrieval Considerations of Binary Data Bases", published in 1985.<ref>{{cite journal | title = Storage and retrieval considerations of binary data bases | author1 = Spiegler I | author2 = Maayan R | journal = Information Processing and Management: an International Journal | volume = 21 | issue = 3 | year = 1985 | doi = 10.1016/0306-4573(85)90108-6 | pages = 233–54   }}</ref> The first commercial database product to implement a bitmap index was Computer Corporation of America\'s [[Model 204]]. [[Patrick O\'Neil]] published a paper about this implementation in 1987.<ref name="model204">{{cite conference | last = O\'Neil | first = Patrick | title = Model 204 Architecture and Performance | booktitle = Proceedings of the 2nd International Workshop on High Performance Transaction Systems | pages = 40–59 | publisher = Springer-Verlag | year = 1987 | location = London, UK | editor = Dieter Gawlick |editor2=Mark N. Haynie |editor3=Andreas Reuter (Eds.) }}</ref> This implementation is a hybrid between the basic bitmap index (without compression) and the list of Row Identifiers (RID-list). Overall, the index is organized as a [[B+tree]]. When the column cardinality is low, each leaf node of the B-tree would contain long list of RIDs. In this case, it requires less space to represent the RID-lists as bitmaps. Since each bitmap represents one distinct value, this is the basic bitmap index. As the column cardinality increases, each bitmap becomes sparse and it may take more disk space to store the bitmaps than to store the same content as RID-lists. In this case, it switches to use the RID-lists, which makes it a [[B+tree]] index.<ref>{{cite conference | title=Bit-sliced index arithmetic | booktitle= Proceedings of the 2001 ACM SIGMOD international conference on Management of data (SIGMOD \'01) | year = 2001 |author1=D. Rinfret, P. O\'Neil  |author2=E. O\'Neil  |lastauthoramp=yes | editor = Timos Sellis | publisher = ACM | location = New York, NY, USA | pages = 47–57 | doi = 10.1145/375663.375669 }}</ref><ref>{{cite conference |author1=E. O\'Neil |author2=P. O\'Neil |author3=K. Wu | title = Bitmap Index Design Choices and Their Performance Implications | booktitle = 11th International Database Engineering and Applications Symposium (IDEAS 2007) | year = 2007 | pages = 72–84 | url=http://crd.lbl.gov/~kewu/ps/LBNL-62756.pdf | isbn = 0-7695-2947-X | doi = 10.1109/IDEAS.2007.19 }}</ref>\n\n==In-memory bitmaps==\nOne of the strongest reasons for using bitmap indexes is that the intermediate results produced from them are also bitmaps and can be efficiently reused in further operations to answer more complex queries. Many programming languages support this as a bit array data structure. For example, Java has the <code>[http://download.oracle.com/javase/6/docs/api/java/util/BitSet.html BitSet]</code> class.\n\nSome database systems that do not offer persistent bitmap indexes use bitmaps internally to speed up query processing. For example, [[PostgreSQL]] versions 8.1 and later implement a "bitmap index scan" optimization to speed up arbitrarily complex [[logical operation]]s between available indexes on a single table.\n\nFor tables with many columns, the total number of distinct indexes to satisfy all possible queries (with equality filtering conditions on either of the fields) grows very fast, being defined by this formula:\n\n:<math> \\mathbf{C}_n^\\left [ \\frac{n}{2} \\right ] \\equiv \\frac{n!}{\\left(n-\\left [ \\frac{n}{2} \\right ]\\right)! \\left [ \\frac{n}{2} \\right ]!}</math>.<ref>{{cite web|author=Alex Bolenok|date=2009-05-09|title=Creating indexes|url=http://explainextended.com/2009/05/09/creating-indexes/}}</ref><ref>{{cite web|author=Egor Timoshenko|title=On minimal collections of indexes|url=http://explainextended.com/files/index-en.pdf }}</ref>\n\nA bitmap index scan combines expressions on different indexes, thus requiring only one index per column to support all possible queries on a table.\n\nApplying this access strategy to B-tree indexes can also combine range queries on multiple columns. In this approach, a temporary in-memory bitmap is created with one [[bit]] for each row in the table (1 [[MiB]] can thus store over 8 million entries). Next, the results from each index are combined into the bitmap using [[bitwise operation]]s. After all conditions are evaluated, the bitmap contains a "1" for rows that matched the expression. Finally, the bitmap is traversed and matching rows are retrieved. In addition to efficiently combining indexes, this also improves [[locality of reference]] of table accesses, because all rows are fetched sequentially from the main table.<ref>{{cite web |author=Tom Lane |date=2005-12-26 |title=Re: Bitmap indexes etc. |publisher=PostgreSQL mailing lists |url=http://archives.postgresql.org/pgsql-performance/2005-12/msg00623.php |accessdate=2007-04-06 }}</ref> The internal bitmap is discarded after the query. If there are too many rows in the table to use 1 bit per row, a "lossy" bitmap is created instead, with a single bit per disk page. In this case, the bitmap is just used to determine which pages to fetch; the filter criteria are then applied to all rows in matching pages.\n\n==References==\n;Notes\n{{Reflist|30em}}\n\n;Bibliography\n*{{Cite journal|last=O\'Connell|first=S.|year=2005|title=Advanced Databases Course Notes|location=[[Southampton]]|publisher=[[University of Southampton]]|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}\n*{{Cite journal|last1=O\'Neil|first1=P.|last2=O\'Neil|first2=E.|year=2001|title=Database Principles, Programming, and Performance|location=[[San Francisco]]|publisher=[[Morgan Kaufmann Publishers]]|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}\n*{{Cite journal|last1=Zaker|first1=M.|last2=Phon-Amnuaisuk|first2=S.|last3=Haw|first3=S.C.|year=2008|issue=2|volume=2|title=An Adequate Design for Large Data Warehouse Systems: Bitmap index versus B-tree index|journal=[[International Journal of Computers and Communications]]|url=http://www.universitypress.org.uk/journals/cc/cc-21.pdf|accessdate=2010-01-07|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}\n\n{{DEFAULTSORT:Bitmap Index}}\n[[Category:Bit data structures|Index]]\n[[Category:Data management]]\n[[Category:Database index techniques]]']
['Category:Data analysis', '14482748', '{{Commons category|Data analysis}}\n{{cat main|Data analysis}}\n\n[[Category:Analysis]]\n[[Category:Data management|Analysis]]']
['Record linkage', '978951', '\'\'\'Record linkage\'\'\' (RL) refers to the task of finding [[Record (database)|records]] in a data set that refer to the same [[entity]] across different data sources (e.g., data files, books, websites, databases).  Record linkage is necessary when [[Join (SQL)|joining]] data sets based on entities that may or may not share a common identifier (e.g., [[Relational model|database key]], [[Uniform Resource Identifier|URI]], [[National identification number]]), as may be the case due to differences in record shape, storage location, and/or curator style or preference.  A data set that has undergone RL-oriented reconciliation may be referred to as being \'\'cross-linked\'\'. \nRecord Linkage is called Data Linkage in many jurisdictions, but is the same process.\n\n== History ==\nThe initial idea of record linkage goes back to [[Halbert L. Dunn]] in his 1946 article titled "Record Linkage" published in the \'\'[[American Journal of Public Health]]\'\'.<ref>{{cite journal\n | first = Halbert L. | last = Dunn | authorlink = Halbert L. Dunn\n | title = Record Linkage\n | journal = [[American Journal of Public Health]]\n |date=December 1946 | volume = 36 | issue = 12 | pages = \'\'pp.\'\' 1412&ndash;1416\n | url = http://www.ajph.org/cgi/reprint/36/12/1412\n | format = PDF\n | accessdate = 2008-05-31\n | doi = 10.2105/AJPH.36.12.1412\n}}</ref>  Howard Borden Newcombe laid the probabilistic foundations of modern record linkage theory in a 1959 article in \'\'[[Science (journal)|Science]]\'\',<ref>{{cite journal|last=Newcombe|first=H. B. |author2=J.M. Kennedy |author3=S.J. Axford |author4=A. P. James|title=Automatic Linkage of Vital Records|journal=Science|date=October 1959|volume=130|issue=3381|pages=954–959|doi=10.1126/science.130.3381.954|pmid=14426783}}</ref> which were then formalized in 1969 by [[Ivan Fellegi]] and Alan Sunter who proved that the probabilistic decision rule they described was optimal when the comparison attributes were conditionally independent.  Their pioneering work "A Theory For Record Linkage"<ref name=FellegiSunter>{{cite journal \n | first = Ivan  | last = Fellegi | authorlink = Ivan Fellegi \n |author2=Sunter, Alan \n  | title = A Theory for Record Linkage \n | journal = [[Journal of the American Statistical Association]] \n |date=December 1969 | volume = 64 | issue = 328 | pages = \'\'pp.\'\' 1183&ndash;1210  \n | jstor = 2286061\n | doi = 10.2307/2286061 \n| url = http://courses.cs.washington.edu/courses/cse590q/04au/papers/Felligi69.pdf | format = PDF\n }}</ref> remains the mathematical foundation for many record linkage applications even today.\n\nSince the late 1990s, various machine learning techniques have been developed that can, under favorable conditions, be used to estimate the conditional probabilities required by the Fellegi-Sunter (FS) theory.  Several researchers have reported that the conditional independence assumption of the FS algorithm is often violated in practice; however, published efforts to explicitly model the conditional dependencies among the comparison attributes have not resulted in an improvement in record linkage quality.\n{{Citation needed|date=May 2007}} On the other hand, machine learning or neural network algorithms that do not rely on these assumptions often provide far higher accuracy, when sufficient labeled training data is available.<ref name="ReferenceA">{{cite conference | first = D. Randall | last = Wilson, D. Randall | title = Beyond Probabilistic Record Linkage: Using Neural Networks and Complex Features to Improve Genealogical Record Linkage | conference = Proceedings of International Joint Conference on Neural Networks | location = San Jose, California, USA | date = July 31 – August 5, 2011 | url = http://axon.cs.byu.edu/~randy/pubs/wilson.ijcnn2011.beyondprl.pdf}}</ref>\n\nRecord linkage can be done entirely without the aid of a computer, but the primary reasons computers are often used for record linkage are to reduce or eliminate manual review and to make results more easily reproducible.  Computer matching has the advantages of allowing central supervision of processing, better quality control, speed, consistency, and better reproducibility of results.<ref>{{cite web|last=Winkler|first=William E.|title=Matching and Record Linkage|url=http://www.census.gov/srd/papers/pdf/rr93-8.pdf|publisher=U.S. Bureau of the Census|accessdate=12 November 2011}}</ref>\n\n== Naming conventions ==\n"Record linkage" is the term used by statisticians, epidemiologists, and historians, among others, to describe the process of joining records from one data source with another that describe the same entity.  Commercial mail and database applications refer to it as "merge/purge processing" or "list washing".  [[computer science|Computer scientists]] often refer to it as "data matching" or as the "object identity problem".  Other names used to describe the same concept include: "coreference/entity/identity/name/record resolution", "entity disambiguation/linking", "duplicate detection", "deduplication", "record matching", "(reference) reconciliation", "object identification", "data/information integration" and "conflation".<ref>http://homes.cs.washington.edu/~pedrod/papers/icdm06.pdf</ref>  This profusion of terminology has led to few cross-references between these research communities.<ref>[http://datamining.anu.edu.au/linkage.html Cristen, P & T: Febrl - Freely extensible biomedical record linkage (Manual, release 0.3) p.9]</ref><ref>\n{{cite journal\n | first = Ahmed  | last = Elmagarmid \n |author2=Panagiotis G. Ipeirotis |author3=Vassilios Verykios \n  | title = Duplicate Record Detection: A Survey \n | journal = IEEE Transactions on Knowledge and Data Engineering \n |date=January 2007  | volume = 19  | issue = 1  | pages = \'\'pp.\'\' 1&ndash;16 \n | url = http://www.cs.purdue.edu/homes/ake/pub/TKDE-0240-0605-1.pdf | format = PDF  | accessdate = 2009-03-30\n | doi = 10.1109/TKDE.2007.9\n}}\n</ref>\n\nWhile they share similar names, record linkage and [[Linked Data]] are two separate concepts.  Whereas record linkage focuses on the more narrow task of identifying matching entities across different data sets, Linked Data focuses on the broader methods of structuring and publishing data to facilitate the discovery of related information.\n\n== Methods ==\n\n=== Data preprocessing ===\nRecord linkage is highly sensitive to the quality of the data being linked, so all data sets under consideration (particularly their key identifier fields) should ideally undergo a [[data quality assessment]] prior to record linkage.  Many key identifiers for the same entity can be presented quite differently between (and even within) data sets, which can greatly complicate record linkage unless understood ahead of time.  For example, key identifiers for a man named William J. Smith might appear in three different data sets as so:\n\n{| class="wikitable"\n|-\n! Data set !! Name !! Date of birth !! City of residence\n|-\n| Data set 1 || William J. Smith || 1/2/73 || Berkeley, California\n|-\n| Data set 2 || Smith, W. J. || 1973.1.2 || Berkeley, CA\n|-\n| Data set 3 || Bill Smith || Jan 2, 1973 || Berkeley, Calif.\n|}\n\nIn this example, the different formatting styles lead to records that look different but in fact all refer to the same entity with the same logical identifier values.  Most, if not all, record linkage strategies would result in more accurate linkage if these values were first \'\'normalized\'\' or \'\'standardized\'\' into a consistent format (e.g., all names are "Surname, Given name", and all dates are "YYYY/MM/DD").  Standardization can be accomplished through simple rule-based [[data transformation]]s or more complex procedures such as lexicon-based [[Tokenization (lexical analysis)|tokenization]] and probabilistic hidden Markov models.<ref>{{cite journal|last=Churches|first=Tim|author2=Peter Christen |author3=Kim Lim |author4=Justin Xi Zhu |title=Preparation of name and address data for record linkage using hidden Markov models|journal=BMC Medical Informatics and Decision Making|date=13 December 2002|volume=2|doi=10.1186/1472-6947-2-9|url=http://www.biomedcentral.com/1472-6947/2/9 |pages=9}}</ref>  Several of the packages listed in the \'\'Software Implementations\'\' section provide some of these features to simplify the process of data standardization.\n\n===Entity resolution===\n\'\'\'Entity resolution\'\'\' is an operational [[intelligence]] process, typically powered by an entity resolution engine or [[middleware]], whereby organizations can connect disparate data sources with a [[Opinion|view]] to understanding possible entity matches and non-obvious relationships across multiple [[data silos]]. It analyzes all of the [[information]] relating to individuals and/or entities from multiple sources of data, and then applies likelihood and probability scoring to determine which identities are a match and what, if any, non-obvious relationships exist between those identities.\n\nEntity resolution engines are typically used to uncover [[risk]], [[fraud]], and conflicts of interest, but are also useful tools for use within [[Customer Data Integration]] (CDI) and [[Master Data Management]] (MDM) requirements. Typical uses for entity resolution engines include terrorist screening, insurance fraud detection, [[USA Patriot Act]] compliance, [[Organized retail crime]] ring detection and applicant screening.\n\nFor example: Across different data silos - employee records, vendor data, watch lists, etc. - an organization may have several variations of an entity named ABC, which may or may not be the same individual. These entries may, in fact, appear as ABC1, ABC2, or ABC3 within those data sources. By comparing similarities between underlying attributes such as [[Address (geography)|address]], [[date of birth]], or [[social security number]], the user can eliminate some possible matches and confirm others as very likely matches.\n\nEntity resolution engines then apply rules, based on common sense logic, to identify hidden relationships across the data. In the example above, perhaps ABC1 and ABC2 are not the same individual, but rather two distinct people who share common attributes such as address or phone number.\n\n====Data Matching====\nWhile entity resolution solutions include data matching technology, many data matching offerings do not fit the definition of entity resolution. Here are four factors that distinguish entity resolution from data matching, according to John Talburt, director of the [[Ualr|UALR]] Center for Advanced Research in Entity Resolution and Information Quality:\n\n* Works with both structured and unstructured records, and it entails the process of extracting references when the sources are unstructured or semi-structured\n* Uses elaborate business rules and concept models to deal with missing, conflicting, and corrupted information\n* Utilizes non-matching, asserted linking (associate) information in addition to direct matching\n* Uncovers non-obvious relationships and association networks (i.e. who\'s associated with whom)\n\nIn contrast to data quality products, more powerful identity resolution engines also include a rules engine and workflow process, which apply business intelligence to the resolved identities and their relationships. These advanced technologies make automated decisions and impact business processes in real time, limiting the need for human intervention.\n\n=== Deterministic record linkage ===\nThe simplest kind of record linkage, called \'\'deterministic\'\' or \'\'rules-based record linkage\'\', generates links based on the number of individual identifiers that match among the available data sets.<ref>{{cite journal|last=Roos|first=LL|author2=Wajda A |title=Record linkage strategies. Part I: Estimating information and evaluating approaches.|journal=Methods of Information in Medicine|date=April 1991|volume=30|issue=2|pages=117–123|pmid=1857246}}</ref>  Two records are said to match via a deterministic record linkage procedure if all or some identifiers (above a certain threshold) are identical.  Deterministic record linkage is a good option when the entities in the data sets are identified by a common identifier, or when there are several representative identifiers (e.g., name, date of birth, and sex when identifying a person) whose quality of data is relatively high.\n\nAs an example, consider two standardized data sets, Set A and Set B, that contain different bits of information about patients in a hospital system.  The two data sets identify patients using a variety of identifiers: [[Social Security Number]] (SSN), name, date of birth (DOB), sex, and [[ZIP code]] (ZIP).  The records in two data sets (identified by the "#" column) are shown below:\n\n{| class="wikitable"\n|-\n! Data Set !! # !! SSN !! Name !! DOB !! Sex !! ZIP\n|-\n| rowspan="4" | Set A || 1 || 000956723 || Smith, William || 1973/01/02 || Male || 94701\n|- style="background:#f0f0f0;"\n| 2 || 000956723 || Smith, William || 1973/01/02 || Male || 94703\n|-\n| 3 || 000005555 || Jones, Robert || 1942/08/14 || Male || 94701\n|- style="background:#f0f0f0;"\n| 4 || 123001234 || Sue, Mary || 1972/11/19 || Female || 94109\n|-\n| rowspan="2" | Set B || 1 || 000005555 ||Jones, Bob || 1942/08/14 || ||\n|- style="background:#f0f0f0;"\n| 2 || || Smith, Bill || 1973/01/02 || Male || 94701\n|}\n\nThe most simple deterministic record linkage strategy would be to pick a single identifier that is assumed to be uniquely identifying, say SSN, and declare that records sharing the same value identify the same person while records not sharing the same value identify different people.  In this example, deterministic linkage based on SSN would create entities based on A1 and A2; A3 and B1; and A4.  While A1, A2, and B2 appear to represent the same entity, B2 would not be included into the match because it is missing a value for SSN.\n\nHandling exceptions such as missing identifiers involves the creation of additional record linkage rules.  One such rule in the case of missing SSN might be to compare name, date of birth, sex, and ZIP code with other records in hopes of finding a match.  In the above example, this rule would still not match A1/A2 with B2 because the names are still slightly different: standardization put the names into the proper (Surname, Given name) format but could not discern "Bill" as a nickname for "William".  Running names through a [[phonetic algorithm]] such as [[Soundex]], [[NYSIIS]], or [[metaphone]], can help to resolve these types of problems (though it may still stumble over surname changes as the result of marriage or divorce), but then B2 would be matched only with A1 since the ZIP code in A2 is different.  Thus, another rule would need to be created to determine whether differences in particular identifiers are acceptable (such as ZIP code) and which are not (such as date of birth).\n\nAs this example demonstrates, even a small decrease in data quality or small increase in the complexity of the data can result in a very large increase in the number of rules necessary to link records properly.  Eventually, these linkage rules will become too numerous and interrelated to build without the aid of specialized software tools.  In addition, linkage rules are often specific to the nature of the data sets they are designed to link together.  One study was able to link the Social Security [[Death Master File]] with two hospital registries from the [[Midwestern United States]] using SSN, NYSIIS-encoded first name, birth month, and sex, but these rules may not work as well with data sets from other geographic regions or with data collected on younger populations.<ref>{{cite journal|last=Grannis|first=SJ|author2=Overhage JM |author3=McDonald CJ |title=Analysis of identifier performance using a deterministic linkage algorithm|journal=Proc AMIA Symp.|year=2002|pages=305–9|pmid=12463836|pmc=2244404}}</ref>  Thus, continuous maintenance testing of these rules is necessary to ensure they continue to function as expected as new data enter the system and need to be linked.  New data that exhibit different characteristics than was initially expected could require a complete rebuilding of the record linkage rule set, which could be a very time-consuming and expensive endeavor.\n\n=== Probabilistic record linkage ===\n\'\'Probabilistic record linkage\'\', sometimes called \'\'fuzzy matching\'\' (also \'\'probabilistic merging\'\' or \'\'fuzzy merging\'\' in the context of merging of databases), takes a different approach to the record linkage problem by taking into account a wider range of potential identifiers, computing weights for each identifier based on its estimated ability to correctly identify a match or a non-match, and using these weights to calculate the probability that two given records refer to the same entity.  Record pairs with probabilities above a certain threshold are considered to be matches, while pairs with probabilities below another threshold are considered to be non-matches; pairs that fall between these two thresholds are considered to be "possible matches" and can be dealt with accordingly (e.g., human reviewed, linked, or not linked, depending on the requirements).  Whereas deterministic record linkage requires a series of potentially complex rules to be programmed ahead of time, probabilistic record linkage methods can be "trained" to perform well with much less human intervention.\n\nMany probabilistic record linkage algorithms assign match/non-match weights to identifiers by means of two probabilities called \'\'u\'\' and \'\'m\'\'. The \'\'u\'\' probability is the probability that an identifier in two \'\'non-matching\'\' records will agree purely by chance.  For example, the \'\'u\'\' probability for birth month (where there are twelve values that are approximately uniformly distributed) is 1/12 ≈ 0.083; identifiers with values that are not uniformly distributed will have different \'\'u\'\' probabilities for different values (possibly including missing values).  The \'\'m\'\' probability is the probability that an identifier in \'\'matching\'\' pairs will agree (or be sufficiently similar, such as strings with high [[Jaro-Winkler distance]] or low [[Levenshtein distance]]).  This value would be 1.0 in the case of perfect data, but given that this is rarely (if ever) true, it can instead be estimated.  This estimation may be done based on prior knowledge of the data sets, by manually identifying a large number of matching and non-matching pairs to "train" the probabilistic record linkage algorithm, or by iteratively running the algorithm to obtain closer estimations of the \'\'m\'\' probability.  If a value of 0.95 were to be estimated for the \'\'m\'\' probability, then the match/non-match weights for the birth month identifier would be:\n\n{| class="wikitable"\n|-\n! Outcome !! Proportion of links !! Proportion of non-links !! Frequency ratio !! Weight\n|-\n| Match || \'\'m\'\' = 0.95 || \'\'u\'\' ≈ 0.083 || \'\'m\'\'/\'\'u\'\' ≈ 11.4 || ln(\'\'m\'\'/\'\'u\'\')/ln(2) ≈ 3.51\n|-\n| Non-match || 1−\'\'m\'\' = 0.05 || 1-\'\'u\'\' ≈ 0.917 || (1-\'\'m\'\')/(1-\'\'u\'\') ≈ 0.0545 || ln((1-\'\'m\'\')/(1-\'\'u\'\'))/ln(2) ≈ -4.20\n|}\n\nThe same calculations would be done for all other identifiers under consideration to find their match/non-match weights.  Then, every identifier of one record would be compared with the corresponding identifier of another record to compute the total weight of the pair: the \'\'match\'\' weight is added to the running total whenever a pair of identifiers agree, while the \'\'non-match\'\' weight is added (i.e. the running total decreases) whenever the pair of identifiers disagrees.  The resulting total weight is then compared to the aforementioned thresholds to determine whether the pair should be linked, non-linked, or set aside for special consideration (e.g. manual validation).<ref name="prl">{{cite journal|last=Blakely|first=Tony|author2=Salmond, Clare |title=Probabilistic record linkage and a method to calculate the positive predictive value|journal=International Journal of Epidemiology|date=December 2002|volume=31|issue=6|pages=1246–1252|doi=10.1093/ije/31.6.1246|pmid=12540730|url=http://ije.oxfordjournals.org/content/31/6/1246.full}}</ref>\n\nDetermining where to set the match/non-match thresholds is a balancing act between obtaining an acceptable [[Sensitivity and specificity#Sensitivity|sensitivity]] (or \'\'recall\'\', the proportion of truly matching records that are linked by the algorithm) and [[positive predictive value]] (or \'\'precision\'\', the proportion of records linked by the algorithm that truly do match).  Various manual and automated methods are available to predict the best thresholds, and some record linkage software packages have built-in tools to help the user find the most acceptable values.  Because this can be a very computationally demanding task, particularly for large data sets, a technique known as \'\'blocking\'\' is often used to improve efficiency.  Blocking attempts to restrict comparisons to just those records for which one or more particularly discriminating identifiers agree, which has the effect of increasing the positive predictive value (precision) at the expense of sensitivity (recall).<ref name=prl />  For example, blocking based on a phonetically coded surname and ZIP code would reduce the total number of comparisons required and would improve the chances that linked records would be correct (since two identifiers already agree), but would potentially miss records referring to the same person whose surname or ZIP code was different (due to marriage or relocation, for instance).  Blocking based on birth month, a more stable identifier that would be expected to change only in the case of data error, would provide a more modest gain in positive predictive value and loss in sensitivity, but would create only twelve distinct groups which, for extremely large data sets, may not provide much net improvement in computation speed.  Thus, robust record linkage systems often use multiple blocking passes to group data in various ways in order to come up with groups of records that should be compared to each other.\n\n===Machine learning===\nIn recent years, a variety of machine learning techniques have been used in record linkage.  It has been recognized<ref name="ReferenceA"/> that a classic algorithm for probabilistic record linkage is equivalent to the [[Naive Bayes]] algorithm in the field of machine learning,<ref>Quass, Dallan, and Starkey, Paul. “Record Linkage for Genealogical Databases,” ACM SIGKDD ’03 Workshop on Data Cleaning, Record Linkage, and Object Consolidation, August 24–27, 2003, Washington, D.C.</ref> and suffers from the same assumption of the independence of its features (an assumption that is typically not true).<ref>Langley, Pat, Wayne Iba, and Kevin Thompson. “An Analysis of Bayesian Classifiers,” In Proceedings of the 10th National Conference on Artificial Intelligence, (AAAI-92), AAAI Press/MIT Press, Cambridge, MA, pp. 223-228, 1992.</ref><ref>Michie, D., D. Spiegelhalter, and C. Taylor. Machine Learning, Neural and Statistical Classification, Ellis Horwood, Hertfordshire, England. Book 19, 1994.</ref>  Higher accuracy can often be achieved by using various other machine learning techniques, including a single-layer [[perceptron]].<ref name="ReferenceA"/>\n\n== Mathematical model ==\nIn an application with two files, A and B, denote the rows (\'\'records\'\') by <math>\\alpha (a)</math> in file A and <math>\\beta (b)</math> in file B. Assign <math>K</math> \'\'characteristics\'\' to each record. The set of records that represent identical entities is defined by\n\n<math> M = \\left\\{ (a,b); a=b; a \\in A; b \\in B \\right\\} </math>\n\nand the complement of set <math>M</math>, namely set <math>U</math> representing different entities is defined as\n\n<math> U = \\{ (a,b); a \\neq b; a \\in A, b \\in B \\} </math>.\n\nA vector, <math>\\gamma</math> is defined, that contains the coded agreements and disagreements on each characteristic:\n\n<math> \\gamma \\left[ \\alpha ( a ), \\beta ( b ) \\right] = \\{ \\gamma^{1} \\left[ \\alpha ( a ) , \\beta ( b ) \\right] ,...,\t\\gamma^{K} \\left[ \\alpha ( a ), \\beta ( b ) \\right] \\} </math>\n\nwhere <math>K</math> is a subscript for the characteristics (sex, age, marital status, etc.) in the files. The conditional probabilities of observing a specific vector <math>\\gamma</math> given <math>(a, b) \\in M</math>, <math>(a, b) \\in U</math> are defined as\n\n<math>\n m(\\gamma) = P \\left\\{ \\gamma \\left[ \\alpha (a), \\beta (b) \\right] | (a,b) \\in M \\right\\} =\n \\sum_{(a, b) \\in M} P \\left\\{\\gamma\\left[ \\alpha(a), \\beta(b) \\right] \\right\\} \\cdot\n                 P \\left[ (a, b) | M\\right]\n</math>\n\nand\n\n<math>\n u(\\gamma) = P \\left\\{ \\gamma \\left[ \\alpha (a), \\beta (b) \\right] | (a,b) \\in U \\right\\} =\n \\sum_{(a, b) \\in U} P \\left\\{\\gamma\\left[ \\alpha(a), \\beta(b) \\right] \\right\\} \\cdot\n                 P \\left[ (a, b) | U\\right],\n</math>\nrespectively.<ref name=FellegiSunter />\n\n== Applications ==\n\n===Master data management===\nMost [[Master data management]] (MDM) products use a record linkage process to identify records from different sources representing the same real-world entity. This linkage is used to create a "golden master record" containing the cleaned, reconciled data about the entity. The techniques used in MDM are the same as for record linkage generally. MDM expands this matching not only to create a "golden master record" but to infer relationships also. (i.e. a person has a same/similar surname and same/similar address, this might imply they share a household relationship).\n\n=== Data warehousing and business intelligence ===\nRecord linkage plays a key role in [[data warehousing]] and [[business intelligence]].  Data warehouses serve to combine data from many different operational source systems into one [[logical data model]], which can then be subsequently fed into a business intelligence system for reporting and analytics.  Each operational source system may have its own method of identifying the same entities used in the logical data model, so record linkage between the different sources becomes necessary to ensure that the information about a particular entity in one source system can be seamlessly compared with information about the same entity from another source system.  Data standardization and subsequent record linkage often occur in the "transform" portion of the [[extract, transform, load]] (ETL) process.\n\n=== Historical research ===\nRecord linkage is important to social history research since most data sets, such as [[census|census records]] and parish registers were recorded long before the invention of [[National identification number]]s.  When old sources are digitized, linking of data sets is a prerequisite for [[longitudinal study]].  This process is often further complicated by lack of standard spelling of names, family names that change according to place of dwelling, changing of administrative boundaries, and problems of checking the data against other sources.  Record linkage was among the most prominent themes in the [[History and computing]] field in the 1980s, but has since been subject to less attention in research.{{Citation needed|date=November 2011}}\n\n=== Medical practice and research ===\n<!-- any experts out there? -->\nRecord linkage is an important tool in creating data required for examining the health of the public and of the health care system itself. It can be used to improve data holdings, data collection, quality assessment, and the dissemination of information. Data sources can be examined to eliminate duplicate records, to identify under-reporting and missing cases (e.g., census population counts), to create person-oriented health statistics, and to generate disease registries and health surveillance systems. Some cancer registries link various data sources (e.g., hospital admissions, pathology and clinical reports, and death registrations) to generate their registries. Record linkage is also used to create health indicators. For example, fetal and infant mortality is a general indicator of a country\'s socioeconomic development, public health, and maternal and child services. If infant death records are matched to birth records, it is possible to use birth variables, such as birth weight and gestational age, along with mortality data, such as cause of death, in analyzing the data.  Linkages can help in follow-up studies of cohorts or other groups to determine factors such as vital status, residential status, or health outcomes. Tracing is often needed for follow-up of industrial cohorts, clinical trials, and longitudinal surveys to obtain the cause of death and/or cancer.  An example of a successful and long-standing record linkage system allowing for population-based medical research is the [[Rochester Epidemiology Project]] based in [[Rochester, Minnesota]].<ref name="data resource profile">{{cite journal | author1=St. Sauver JL  | author2=Grossardt BR | author3=Yawn BP | author4=Melton LJ 3rd | author5=Pankratz JJ |author6=Brue SM | author7=Rocca WA. | title = Data Resource Profile: The Rochester Epidemiology Project (REP) medical records-linkage system | journal = Int J Epidemiol | volume=41 | issue=6 | pages=1614–24 | year = 2012 | pmid = 23159830 | doi=10.1093/ije/dys195 | url=http://ije.oxfordjournals.org/content/41/6/1614.long}}</ref>\n\n== Criticism of existing software implementations==\nThe main reasons cited are:\n* \'\'\'Project costs\'\'\': costs typically in the hundreds of thousands of dollars\n* \'\'\'Time\'\'\': lack of enough time to deal with large-scale [[data cleansing]] software\n* \'\'\'Security\'\'\': concerns over sharing information, giving an application access across systems, and effects on legacy systems\n\n== See also ==\n* [[Capacity optimization]]\n* [[Content-addressable storage]]\n* [[Data deduplication]]\n* [[Delta encoding]]\n* [[Entity linking]]\n* [[Entity-attribute-value model]]\n* [[Identity resolution]]\n* [[Linked data]]\n* [[Named-entity recognition]]\n* [[Open data]]\n* [[Schema matching]]\n* [[Single-instance storage]]\n\n== Notes and references ==\n{{Reflist|2}}\n\n== External links ==\n* [http://pike.psu.edu/linkage/ Data Linkage Project at Penn State, USA]\n* [http://www.datadecision.com Datadecision - Data matching online tool]\n* [http://www.nameapi.org/en/demos/name-matcher/ NameAPI - Name Matcher]\n* [http://sourceforge.net/projects/oysterer/ OYSTER Entity Resolution]\n* [http://datamining.anu.edu.au/ Febrl - Freely Extensible Biomedical Record Linkage]\n* [http://infolab.stanford.edu/serf/ Stanford Entity Resolution Framework]\n* [http://dbs.uni-leipzig.de/de/research/projects/large_scale_object_matching/ Dedoop - Deduplication with Hadoop]\n* [https://sourceforge.net/projects/erframework/ BlockingFramework A framework for blocking-based Entity Resolution]\n* [http://www.ipdln.org/ International Population Data Linkage Network]\n* [https://github.com/yahoo/FEL Yahoo Fast Entity Linker Core]\n\n{{DEFAULTSORT:Record Linkage}}\n[[Category:Data management]]']
['Customer data management', '31501606', '\'\'\'Customer data management (CDM)\'\'\' is the ways in which businesses keep track of their customer information and survey their [[customer base]] in order to obtain feedback. CDM embraces a range of software or [[cloud computing]] applications designed to give large organizations rapid and efficient access to customer data. Surveys and data can be centrally located and widely accessible within a company, as opposed to being warehoused in separate departments. CDM encompasses the collection, analysis, organizing, reporting and sharing of customer information throughout an organization. Businesses need a thorough understanding of their customers’ needs if they are to retain and increase their customer base. Efficient CDM solutions provide companies with the ability to deal instantly with customer issues and obtain immediate feedback. As a result, [[customer retention]] and [[customer satisfaction]] can show dramatic improvement. According to a recent study by [[Aberdeen Group]] inc.: "Above-average and best-in-class companies... attain greater than 20% annual improvement in retention rates, revenues, data accuracy and partner/customer satisfaction rates."<ref>Smalltree, Hannah (2006) [http://searchcrm.techtarget.com/news/1212337/Best-practices-in-managing-customer-data]</ref>\n\n== Customer data management and cloud computing ==\n\nCloud computing offers an attractive choice for CDM in many companies due to its accessibility and [[Cost efficiency|cost-effectiveness]]. Businesses can decide who, within their company, should have the ability to create, adjust, analyze or share customer information. In December 2010, 52% of [[Information technology|Information Technology]] (IT) professionals worldwide were deploying, or planning to deploy, cloud computing;<ref>Cisco.com (December 2010) [http://newsroom.cisco.com/dlls/2010/prod_120810.html]</ref> this percentage is far higher in many countries.\n\n== Uses for management ==\n\n\'\'\'Customer data management\'\'\'\n* should provide a cost-effective, user-friendly solution for [[marketing]], research, sales, [[human resources]] and IT departments\n* enables companies to create and email online surveys, reports and newsletters\n* encompasses and simplifies [[customer relationship management]] (CRM) and [[Customer feedback management services|customer feedback management]] (CFM)\n\n== Background ==\n\nCustomer data management, as a term, was coined in the 1990s, pre-dating the alternative term [[enterprise feedback management]] (EFM). Customer data management (CDM) was introduced as a software solution that would replace earlier disc-based or paper-based surveys and [[spreadsheet]] data. Initially, CDM solutions were marketed to businesses as software, specific to one company, and often to one department within that company. This was superseded by [[application service  provider]]s (ASPs) where software was hosted for [[end user]] organizations, thus avoiding the necessity for IT professionals to deploy and support software. However, ASPs with their single-tenancy architecture were, in turn, superseded by [[software as a service]] (SaaS), engineered for multi-tenancy. By 2007 SaaS applications, giving businesses on-demand access to their customer information, were rapidly gaining popularity compared with ASPs. Cloud computing now includes SaaS and many prominent CDM providers offer cloud-based applications to their clients.\n\nIn recent years, there has been a push away from the term EFM, with many of those working in this area advocating the slightly updated use of CDM. The return to the term CDM is largely based on the greater need for clarity around the solutions offered by companies, and on the desire to retire terminology veering on techno-jargon that customers may have a hard time understanding.<ref>InSiteSystems.com (December, 2010) [http://www.insitesystems.com/systems/blogs/the-problem-with-efm.html]</ref>\n\n== References ==\n<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\n{{Reflist}}\n\n<!--- Categories --->\n[[Category:Articles created via the Article Wizard]]\n[[Category:Data management]]']
['Cloud Data Management Interface', '32115812', '{{Infobox standardref\n| title             = Cloud Data Management Interface\n| status            = Published\n| year_started      = 2009\n| version           = 1.1.1\n| organization      = [[Storage Networking Industry Association]]\n| base_standards    = [[Hypertext Transfer Protocol]]\n| related_standards = [[Network File System]]\n| abbreviation      = CDMI\n| domain            = [[Cloud computing]]\n| license           = \n| website           = [http://www.snia.org/cloud CDMI Technical Working Group]\n}}\n\nThe \'\'\'Cloud Data Management Interface\'\'\' (\'\'\'CDMI\'\'\') is a [http://www.snia.org SNIA] standard that specifies a protocol for self-provisioning, administering and accessing [[cloud storage]].<ref>{{cite web|title=Cloud Data Management Interface|url=http://www.snia.org/cdmi|publisher=SNIA|accessdate=26 June 2011}}</ref>\n\nCDMI defines [[REST]]ful [[HTTP]] operations for assessing the capabilities of the cloud storage system, allocating and accessing containers and objects, managing users and groups, implementing access control, attaching metadata, making arbitrary queries, using persistent queues, specifying retention intervals and holds for compliance purposes, using a logging facility, billing, moving data between cloud systems, and exporting data via other protocols such as [[iSCSI]] and [[Network File System (protocol)|NFS]]. Transport security is obtained via [[Transport Layer Security|TLS]].\n\n==Capabilities==\nCompliant implementations must provide access to a set of configuration parameters known as \'\'capabilities\'\'.\nThese are either boolean values that represent whether or not a system supports things such as queues, export via other protocols, path-based storage and so on, or numeric values expressing system limits, such as how much metadata may be placed on an object.  As a minimal compliant implementation can be quite small, with few features, clients need to check the cloud storage system for a capability before attempting to use the functionality it represents.\n\n==Containers==\nA CDMI client may access objects, including containers, by either name or object id (OID), assuming the CDMI server supports both methods.  When storing objects by name, it is natural to use nested named containers; the resulting structure corresponds exactly to a traditional filesystem directory structure.\n\n==Objects==\nObjects are similar to files in a traditional file system, but are enhanced with an increased amount of and capacity for [[metadata]].  As with containers, they may be accessed by either name or OID.  When accessed by name, clients use [[Uniform Resource Locator|URLs]] that contain the full pathname of objects to [[create, read, update and delete]] them. When accessed by OID, the URL specifies an OID string in the \'\'\'cdmi-objectid\'\'\' container; this container presents a flat name space conformant with standard object storage system semantics.\n\nSubject to system limits, objects may be of any size or type and have arbitrary user-supplied metadata attached to them. Systems that support query allow arbitrary queries to be run against the metadata.\n\n==Domains, Users and Groups==\nCDMI supports the concept of a \'\'domain\'\', similar in concept to a domain in the [[Windows]] [[Active Directory]] model. Users and groups created in a domain share a common administrative database and are known to each other on a "first name" basis, i.e. without reference to any other domain or system.\n\nDomains also function as containers for usage and billing summary data.\n\n==Access Control==\nCDMI exactly follows the [[Access Control List|ACL]] and [[Access Control Entry|ACE]] model used for file authorization operations by [[NFSv4#NFSv4|NFSv4]]. This makes it also compatible with [[Microsoft Windows]] systems.\n\n==Metadata==\nCDMI draws much of its metadata model from the [[XAM]] specification. Objects and containers have "storage system metadata", "data system metadata" and arbitrary user specified metadata, in addition to the metadata maintained by an ordinary filesystem (atime etc.).\n\n==Queries==\nCDMI specifies a way for systems to support arbitrary queries against CDMI containers, with a rich set of comparison operators, including support for [[regular expression]]s.\n\n==Queues==\nCDMI supports the concept of persistent [[FIFO (computing and electronics)|FIFO]] (first-in, first-out) queues. These are useful for job scheduling,  order processing and other tasks in which lists of things must be processed in order.\n\n==Compliance==\nBoth retention intervals and retention holds are supported by CDMI.  A retention interval consists of a start time and a retention period.  During this time interval, objects are preserved as immutable and may not be deleted. A retention hold is usually placed on an object because of judicial action and has the same effect: objects may not be changed nor deleted until all holds placed on them are removed.\n\n==Logging==\nCDMI clients can sign up for logging of system, security and object access events on servers that support it.  This feature allows clients to see events locally as the server logs them.\n\n==Billing==\nSummary information suitable for billing clients for on-demand services can be obtained by authorized users from systems that support it.\n\n==Serialization==\nSerialization of objects and containers allows export of all data and metadata on a system and importation of that data into another cloud system.\n\n==Foreign Protocols==\nCDMI supports export of containers as NFS or CIFS shares.  Clients that mount these shares see the container hierarchy as an ordinary filesystem directory hierarchy, and the objects in the containers as normal files. Metadata outside of ordinary filesystem metadata may or may not be exposed.\n\nProvisioning of iSCSI LUNs is also supported.\n\n== Client SDKs ==\n* [http://www.snia.org/forums/csi/programs/CDMIportal CDMI Reference Implementation]\n* [https://github.com/scality/Droplet Droplet]\n* [https://github.com/livenson/libcdmi-java libcdmi-java]\n* [https://github.com/livenson/libcdmi-python libcdmi-python]\n* [https://github.com/projectpvg1/.net-SDK .NET SDK]\n\n== See also ==\n[[Comparison of CDMI server implementations]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=40874 ISO-8601]  International Organization for Standardization, "Data elements and interchange formats -- Information interchange -- Representation of dates and times”, ISO 8601:20044\n* [http://www.itu.int/ITU-T/publications/recs.html ITU-T509]  International Telecommunications Union Telecommunication Standardization Sector (ITU-T), Recommendation X.509: Information technology - Open Systems Interconnection - The Directory: Public-key and attribute certificate frameworks, May 2000. Specification and technical corrigenda -\n* [http://www.unix.org/version3/ieee_std.html POSIX ERE] The Open Group, Base Specifications Issue 6, IEEE Std 1003.1, 2004 Edition\n* [http://www.cloudplugfest.org/ Cloud Interoperability Plugfest project]\n\n[[Category:Cloud storage]]\n[[Category:Data management]]']
['Data monetization', '32714470', '\'\'\'Data monetization\'\'\', a form of [[monetization]], is generating [[revenue]] from available data sources or real time streamed data by instituting the discovery, capture, storage, analysis, dissemination, and use of that data.  Said differently, it is the process by which data producers, data aggregators and data consumers, large and small, exchange sell or trade data. Data monetization leverages data generated through business operations as well as data associated with individual actors and with electronic devices and sensors participating in the [[internet of things]].  The ubiquity of the [[internet of things]] is generating [[location data]] and other data from sensors and [[mobile devices]] at an ever increasing rate. When this data is collated against traditional databases, the value and utility of both sources of data increases, leading to tremendous potential to mine data for social good, research and discovery, and achievement of business objectives.  Closely associated with data monetization are the emerging [[data as a service]] models for transactions involving data by the data item.\n\nThere are three [[ethical]] and regulatory vectors involved in data monetization due to the sometimes conflicting interests of actors involved in the [[data supply chain]].  The individual data creator who generates files and records through his own efforts or owns a device such as a sensor or a mobile phone that generates data has a claim to ownership of data.  The business entity that generates data in the course of its operations, such as its transactions with financial institutions or [[risk factors]] discovered through feedback from customers also has a claim on data captured through their systems and platforms. However, the person that contributed the data may also have a legitimate claim on the data.  Internet platforms and service providers, such as [[Google]] or [[Facebook]] that require a user to forgo some ownership interest in their data in exchange for use of the platform also have a legitimate claim on the data.  Thus the practice of data monetization, although common since 2000, is now getting increasing attention from regulators.  The [[European Union]] and the [[United States Congress]] have begun to address these issues.  For instance, in the financial services industry, regulations involving data are included in the [[Gramm–Leach–Bliley Act]] and [[Dodd-Frank]].  Some individual creators of data are shifting to using [[personal data vaults]]<ref>http://www.freepatentsonline.com/y2014/0032267.html</ref> and implementing [[vendor relationship management]]<ref>[[Vendor Relationship Management]]</ref> concepts as a reflection of an increasing resistance to their data being federated or aggregated and resold without compensation.  Groups such as the [[Personal Data Ecosystem Consortium]],<ref>http://personaldataecosystem.org</ref> [[Patient Privacy Rights]],<ref>http://patientprivacyrights.org/</ref> and others are also challenging corporate cooptation of data without compensation.\n\n[[Financial services]] companies are a relatively good example of an industry focused on generating revenue by leveraging data.  [[Credit card]] issuers and [[retail banks]] use customer transaction data to improve targeting of [[cross-sell]] offers.  Partners are increasingly promoting merchant based [[reward programs]] which leverage a bank’s data and provide discounts to customers at the same time.\n\n==Steps==\n# Identification of available data sources – this includes data currently available for monetization as well as other external data sources that may enhance the value of what’s currently available.\n# Connect, aggregate, attribute, validate, authenticate, and exchange data - this allows data to be converted directly into actionable or revenue generating insight or services.\n# Set terms and prices and facilitate data trading - methods for data vetting, storage, and access. For example, many global corporations have locked and siloed data storage infrastructures, which stymies efficient access to data and cooperative and real time exchange. \n# Perform [[Research]] and [[analytics]] – draw predictive insights from existing data as a basis for using data for to reduce [[risk]], enhance product development or performance, or improve [[customer experience]] or business outcomes.\n# Action and leveraging – the last phase of monetizing data includes determining alternative or improved datacentric products, ideas, or services.  Examples may include real time actionable triggered notifications or enhanced channels such as web or mobile response mechanisms.\n\n==Pricing Variables and Factors==\n*  A fee for use of a platform to connect buyers and sellers\n*  A fee for use of a platform to configure, organize, and otherwise process data included in a data trade \n*  A fee for connecting or including a device or sensor into a data supply chain\n*  A fee for connecting and credentialing a creator of a data source and a data buyer - often through a [[federated identity]]\n*  A fee for connecting a data source to other data sources to be included into a data supply chain\n*  A fee for use of an internet service or other transmission service for uploading and downloading data - sometimes, for an individual, through a [[personal cloud]]\n*  A price or exchange or other trade value assigned by a data creator or generator to a data item or a data source \n*  A price or exchange or other trade value offered by a data buyer to a data creator \n*  A price or exchange or other trade value assigned by a data buyer for a data item or a data source formatted according to criteria set by a data buyer\n*  An incremental fee assigned by a data buyer for a data item or a data set scaled to the reputation of the data creator\n*  A fee for use of encrypted keys to achieve secure data transfer\n*  A fee for use of a search algorithm specifically designed to tag data sources that contain data points of value to the data buyer\n*  A fee for linking a data creator or generator to a data collection protocol or form\n*  A fee for server actions - such as a notification - triggered by an update to a data item or data source included into a data supply chain\n\n==Benefits==\n*  Improved decision-making that leads to [[real time (media)|real time]] [[crowd sourced]] research, improved profits, decreased costs, reduced risk and improved compliance\n*  More impactful decisions (e.g., make real time decisions)\n*  More timely (lower latency) decisions (e.g., a vendor making purchase recommendations while the customer is still on the phone or in the store, a customer connecting with multiple vendors to discover a best price, triggered notifications when thresholds are reached for data values )\n*  More granular decisions (e.g., localized pricing decisions at an individual or device or sensor level versus larger aggregates).\n\n==Frameworks==\nThere are a wide variety of industries, firms and business models related to data monetization.  The following frameworks have been offered to help understand the types of business models that are used:\n\nDoug Laney of [[Gartner]], a leading IT research and advisory firm, has posited a model for a range of data monetization methods:\n\n* Indirect Data Monetization\n**Using data to improve efficiencies\n**Using data to measurably reduce risks\n**Using data to develop new products, markets\n**Using data to build and solidify partner relationships\n**Publishing Branded indices\n* Direct Data Monetization\n**Bartering or trading with information\n**Information-enhanced products or services\n**Selling raw data through brokers\n**Offering data/report subscriptions\n\nHe also suggests a set of feasibility tests and questions for any data monetization ideas being considered:\n\n{| class="wikitable"\n|-\n! Type of Feasibility !! Feasibility Question \n|-\n| Practical || Is the idea utilitarian, or merely interesting/cool? Is it usable?\n|-\n| Marketable || Would the idea have sufficiently broad appeal, internally or externally?\n|-\n| Scalable || Can the idea be developed and implemented to the extent required or intended?\n|-\n| Manageable || Do you have the skills to oversee the development & implementation of the idea?\n|-\n| Technological || Do you have the tools, information and skills to develop and rollout the idea?\n|-\n| Economical || Will the idea require too much investment or generate sufficient return on investment?\n|-\n| Legal || Does the idea conform to local laws where it will be used or implemented?\n|-\n| Ethical || Will the idea be something that has the potential for customer/user/public backlash?\n|-\n| Example || Will the idea cause significant positive vs. negative impact on the environment?\n|}\n\nRoger Ehrenberg of IA Ventures, a VC firm that invests in this space has defined three basic types of data product firms:\n:"\'\'\'Contributory databases\'\'\'. The magic of these businesses is that a customer provides their own data in exchange for receiving a more robust set of aggregated data back that provides insight into the broader marketplace, or provides a vehicle for expressing a view. Give a little, get a lot back in return –  a pretty compelling value proposition, and one that frequently results in a payment from the data contributor in exchange for receiving enriched, aggregated data. Once these contributory databases are developed and customers become reliant on their insights, they become extremely valuable and persistent data assets.\n:\n:\'\'\'Data processing platforms\'\'\'. These businesses create barriers through a combination of complex data architectures, proprietary algorithms and rich analytics to help customers consume data in whatever form they please. Often these businesses have special relationships with key data providers, that when combined with other data and processed as a whole create valuable differentiation and competitive barriers. Bloomberg is an example of a powerful data processing platform. They pull in data from a wide array of sources (including their own home grown data), integrate it into a unified stream, make it consumable via a dashboard or through an API, and offer a robust analytics suite for a staggering number of use cases. Needless to say, their scale and profitability is the envy of the industry.\n:\n:\'\'\'Data creation platforms\'\'\'. These businesses solve vexing problems for large numbers of users, and by their nature capture a broad swath of data from their customers. As these data sets grow, they become increasingly valuable in enabling companies to better tailor their products and features, and to target customers with highly contextual and relevant offers. Customers don’t sign up to directly benefit from the data asset; the product is so valuable that they simply want the features offered out-of-the-box. As the product gets better over time, it just cements the lock-in of what is already a successful platform. Mint was an example of this kind of business. People saw value in the core product. But the product continued to get better as more customer data was collected and analyzed. There weren’t network effects, per se, but the sheer scale of the data asset that was created was an essential element of improving the product over time."<ref>{{cite web|last=Ehrenberg |first=Roger |title=Creating competitive advantage through data |url=http://www.iaventures.com/creating-competitive-advantage-through-data |publisher=IA Ventures\' blog |accessdate=23 November 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20131203023719/http://www.iaventures.com/creating-competitive-advantage-through-data |archivedate=3 December 2013 |df= }}</ref>\n\nSelvanathan and Zuk <ref>Big Data Realized: Developing New Data-Driven Products and Services to Drive Growth Perspective</ref> offer a framework that includes "monetization methods that are outside the bounds of the\ntraditional value capture systems employed by an enterprise... tuned to match the context and consumption models for the target customer."  They offer examples of "four distinct approaches: platforms, applications, data-as-a-service, and professional services."\n\nEthan McCallum and Ken Gleason published an O\'Rielly eBook titled \'\'Business Models for the Data Economy\'\'\n:Collect/Supply\n:Store/Host\n:Filter/Refine\n:Enhance/Enrich\n:Simplify Access\n:Analyze\n:Obscure\n:Consult/Advise<ref>{{cite book|last=Gleason|first=Ken|title=Business Models for the Data Economy|year=2013|publisher=O\'Reilly|isbn=978-1-449-37223-1|url=http://www.oreilly.com/data/free/business-models-for-the-data-economy.csp}}</ref>\n\n==Examples==\n*  Packaging of data (with analytics) to be resold to customers for things such as wallet share, [[market share]] and [[benchmarking]]\n*  Integration of data (with analytics) into new products as a value-added differentiator such as [[On-Star]] for [[General Motors]] cars\n* [[GPS]] enabled [[smartphones]]\n* [[Geolocation]]-based offers and location discounts, such as those offered by [[Facebook]]<ref>https://www.theguardian.com/technology/2011/jan/31/facebook-places-deals-uk-europe</ref> and [[Groupon]]<ref>http://mashable.com/2011/05/10/groupon-now-launches/</ref> are other prime examples of data monetization leveraging new emerging channels\n* CRM based ad targeting and media attribution, such as those offered by Circulate\n\n==Intellectual property landscape==\nSome of the patents issued since 2010 by the [[USPTO]] for monetizing data generated by individuals include; 8,271,346, 8,612,307, 8,560,464, 8,510,176, and 7,860,760.  These are usually in the class 705 related to electronic commerce, data processing, and cost and price determination. Some of these patents use the term, the [[data supply chain]] to reflect emerging technology to federate and aggregate data in real time from many individuals and devices linked together through the [[internet of things]].  Another emerging term is [[information banking]].\n\nAn unexplored but potentially disruptive arena for data monetization is the use of [[Bitcoin micropayments]] for data transactions.  Because Bitcoins are emerging as competitors with payment services like Visa or PayPal that can readily enable and reduce or eliminate transaction costs, transactions for as little as a single data item can be facilitated. Consumers as well as enterprises who desire to monetize their participation in a data supply chain may soon be able to access social network enabled Bitcoin exchanges and platforms.<ref>Lomas, Natasha, Techcrunch, August 18, 2014</ref>  [[Clickbait]] and data hijacking may wither as micropayments for data are ubiquitous and enabled. Potentially, even the current need to build out data broker managed data trading exchanges may be bypassed.  Stanley Smith,<ref>http://www.linkedin.com/pub/stan-smith/9/3ab/b37/</ref> who introduced the notion of the data supply chain, has said that simple micropayments for data monetization are the key to evolution of ubiquitous implementation of user configurable data supply schemata, enabling data monetization on a universal scale for all data creators, including the burgeoning internet of things.\n\n==Presentations and Publications ==\n\n2016\n* [https://www.gartner.com/doc/3267517 How CIOs and CDOs Can Use Infonomics to Identify, Justify and Fund Initiatives], Douglas Laney and Michael Smith, [[Gartner]] 29 March 2016\n* [http://www.wsj.com/articles/accountings-21st-century-challenge-how-to-value-intangible-assets-1458605126 Accounting\'s 21st Century Challenge: How to Value Intangible Assets], [[WSJ]] CFO Journal, 22 March 2016 \n* [https://s3.amazonaws.com/files.technologyreview.com/whitepapers/MIT_Oracle+Report-The_Rise_of_Data_Capital.pdf The Rise of Data Capital], [[Oracle Corporation|Oracle]] and [[MIT]] Technology Review Custom, 2016\n* [http://www.gartner.com/smarterwithgartner/treating-information-as-an-asset/ Treating Information as an Asset], Christy Pettey, Douglas Laney and Michael M. Moran, Smarter With [[Gartner]], 17 February 2016\n* [http://www.reuters.com/article/us-europe-data-competition-idUSKCN0VF0KV German competition watchdog wants \'big data\' hoards considered in merger probes], [[Reuters]], 6 Feb 2016\n* [https://www.gartner.com/doc/3188917 Shift From a Project to an Asset Perspective to Properly Value and Fund IT Investments], Michael Smith and Douglas Laney, [[Gartner]], 16 January 2016\n* [http://www.iri.com/blog/iri/business/infonomics-and-you/ Infonomics and You], Eric Leohner, [[IRI CoSort]], January 2016\n* [http://www.nowozin.net/sebastian/blog/the-fair-price-to-pay-a-spy-an-introduction-to-the-value-of-information.html The Fair Price to Pay a Spy: An Introduction to the Value of Information], Sebastian Nowozin, Nowozin.net blog, 9 January 2016 \n2015\n* [https://www.gartner.com/doc/3173343 Measure Your Information Yield to Maximize Return on Information and Analytics Investments], Frank Buytendijk, Andrew White, Douglas Laney and Thomas W. Oestreich, [[Gartner]], 1 December 2015\n* [https://www.gartner.com/doc/3162520 IBM Storms Information, IoT Markets by Buying The Weather Company], Douglas Laney, [[Gartner]], 4 November 2015\n* [https://www.gartner.com/doc/3158117 How to Adopt Open Data for Business Data and Analytics — And Why You Should], Alan D. Duncan & Douglas Laney, [[Gartner]], 28 October 2015\n* [https://www.gartner.com/doc/3151321 Seven Steps to Monetizing Your Information Assets], Douglas Laney, [[Gartner]], 15 October 2015\n* [http://www.gartner.com/smarterwithgartner/why-and-how-to-value-your-information-as-an-asset/ Why and How to Value Your Information as an Asset] Heather Levy & Douglas Laney, Smarter With [[Gartner]] blog, 3 September 2015\n* [https://www.gartner.com/doc/3106721 Hackers Know the Value of Health Information, So Why Don\'t HDOs Appreciate Healthcare Infonomics?], Laura Craft & Douglas Laney, [[Gartner]], 5 August 2015\n* In August 20, 2015 [[Gartner]] Analyst Doug Laney gave a publicly-available webinar (with replay available) on [http://www.gartner.com/webinar/3098518 Methods for Monetizing Your Data]. This is a reprise of the presentation he has given at various [[Gartner]] summits and symposia around the world. \n* [https://www.gartner.com/doc/3106721 Hackers Know the Value of Health Information, So Why Don\'t HDOs Appreciate Healthcare Infonomics?], Laura Craft & Douglas Laney, [[Gartner]], 5 August 2015\n* [https://www.gartner.com/doc/3106719 Why and How to Measure the Value of Your Information Assets], Douglas Laney, [[Gartner]], 5 August 2015\n* [http://prezi.com/xirqf54fix68/?utm_campaign=share&utm_medium=copy&rc=ex0share Applied Infonomics: Measuring the Economic Value of Information Assets], [http://www.mitcdoiq.org/ MIT Chief Data Officer Symposium], Doug Laney, [[Gartner]], 22 July 2015\n* [http://www.kpmg.com/US/en/topics/data-analytics/Documents/kpmg-d-a-main-report-for-web-28-june-2015.pdf Data and Analytics: A New Driver of Performance and Valuation], [[Institutional Investor Research]] and [[KPMG]], 28 June 2015\n* [http://www.thesummits.org/watch.htm?id=132356411 The Convergence of Information Economics and Economic Information] [[Corp Development Summit]] presentation replay, Doug Laney, [[Gartner]], 1 July 2015\n* [http://smartdatacollective.com/rk-paleru/319941/data-opportunity-are-you-monetizing-information Data = Opportunity: But Are You Monetizing Information?] [[Smart Data Collective]], RK Paleru, 28 May 2015\n* [http://blogs.gartner.com/doug-laney/keeping-busy-with-data-strategy/ Keeping Busy with Data Strategy], [[Gartner]] Blog Network, Doug Laney, 26 May 2015\n* [http://blogs.wsj.com/cio/2015/05/20/dollar-value-of-data-radioshack-other-bankrupt-firms-auction-customer-data-to-pay-debt/ Dollar Value of Data: RadioShack, Other Bankrupt Firms Auction Customer Data to Pay Debt], [[Wall Street Journal]], Kim Nash, 20 May 2015\n* [https://www.gartner.com/doc/3024417 The Benefits and Risks of Using Open Data], Doug Laney, [[Gartner]], 8 April 2015\n* [http://goodstrat.com/2015/01/30/consider-this-does-all-data-have-value/ Consider this: Does all data have value?] Good Strategy blog, Martyn Jones, 30 January 2015\n* [http://www.rsd.com/en/resources/white-papers/theory-infonomics-valuating-corporate-information-assets The Theory of Infonomics: Valuating Corporate Information Assets - white paper], [[RSD (company)|RSD]], January 2015\n* [http://www.firstpost.com/business/customer-data-valuable-asset-treat-way-2046119.html Customer data is a valuable asset. Why not treat it that way?], [[F.Business]], Ajay Kelkar, 14 January 2015\n* [https://www.youtube.com/watch?v=du4YVpu4VHE The Rise of Data Capital] (video), [[Oracle Corporation|Oracle]], 8 January 2015\n\n2014\n* [http://www.cmswire.com/cms/information-management/quantifying-the-value-of-your-data-026674.php Quantifying the Value of Your Data], [[CMS Wire]], Bassam Zarkout, 30 September 2014\n* [http://www.rsd.com/en/blog/201409/what-infonomics What is Infonomics?], Ed Hallock, [[RSD (company)|RSD]] blog, 9 September 2014\n* [http://cisr.mit.edu/blog/documents/2014/08/21/2014_0801_datamonetization_wixom.pdf/ Cashing In on Your Data], [[MIT Sloan]] Center for Information Systems Research, Barbara H. Wixom, Volume XIV, Number 8, August 2014\n* [https://www.gartner.com/doc/2813227 Increase the Return on Your Information Investments With the Information Yield Curve], [[Gartner]], Andrew White and Douglas Laney, 31 July 2014\n* [http://www.forbes.com/sites/gartnergroup/2014/07/21/the-hidden-shareholder-boost-from-information-assets/ The Hidden Shareholder Boost from Information Assets], [[Forbes]], Doug Laney, 21 July 2014\n* [http://searchcio.bitpipe.com/data/demandEngage.action?resId=1401817861_376 CIO Decisions: The new infonomics reality: Determining the value of data], [[TechTarget]] SearchCIO, June 2014\n* [http://searchcio.techtarget.com/opinion/Putting-a-price-on-information-The-nascent-field-of-infonomics Putting a price on information: The nascent field of infonomics], [[TechTarget]] SearchCIO, Linda Tucci, 13 May 2014\n* [http://searchcio.techtarget.com/feature/Six-ways-to-measure-the-value-of-your-information-assets Six ways to measure the value of your information assets], [[TechTarget]] SearchCIO, Nicole Laskowski, 13 May 2014\n* [http://searchcio.techtarget.com/feature/Infonomics-treats-data-as-a-business-asset Infonomics treats data as a business asset], [[TechTarget]] SearchCIO, Nicole Laskowski, 13 May 2014\n* [http://pv.tl/blog/2014/04/13/the-economics-of-information-management/?utm_content=buffer29c67&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer, The economics of information management], PVTL Blog, Felix Barbalet, 13 April 2014\n* [http://www.forbes.com/sites/gartnergroup/2014/03/27/the-hidden-tax-advantage-of-monetizing-your-data/ The Hidden Tax Advantage of Monetizing Your Data], [[Forbes]], Doug Laney, 27 March 2014\n* [http://blogs.teradata.com/anz/the-chief-data-officer-managing-the-value-of-data/#comment-2147 The Chief Data Officer – Managing the Value of Data], [[Teradata]] ANZ Blog, Renato Manongdo, March 2014\n* [https://www.gartner.com/doc/2677518 How Organizations Can Monetize Customer Data], [[Gartner]], Olive Huang, Doug Laney, 6 March 2014\n* [https://www.gartner.com/doc/2677515?ref=QuickSearch&sthkw=infonomics Improving the Value of Customer Data Through Applied Infonomics], [[Gartner]] Research Publication, Douglas Laney, Olive Huang, 6 March 2014\n* [http://blogs.gartner.com/andrew_white/2014/02/14/information-value-accrual-and-its-asymmetry/ Information Value Accrual and Its Asymmetry], [[Gartner]] Blog Network, Andrew White, 14 February 2014\n* [http://blogs.gartner.com/andrew_white/2014/01/29/does-information-utility-suffer-a-half-life/ Does Information Utility Suffer a Half Life?], [[Gartner]] Blog Network, Andrew White, 29 January 2014\n\n2013\n* [http://www.rsd.com/en/blog/201312/what-information-information-governance What is the "Information" in "Information Governance"?], [[RSD (company)|RSD]] Blog, James Amsler, 30 December 2013\n* [http://blogs.gartner.com/doug-laney/to-twitter-youre-worth-101-70/ To Twitter You\'re Worth $101.70] [[Gartner]] Blog Network, by Douglas Laney, 12 November 2013\n* [http://www.economistgroup.com/leanback/big-data-2/treat-data-like-money Treat data like money. CMO\'s Advice: Marketers must develop an investment strategy for data], [[The Economist Group]], Jim Davis, SVP & CMO, SAS, October 2013\n* [http://www.ft.com/intl/cms/s/0/205ddf5c-1bf0-11e3-b678-00144feab7de.html#axzz2g8PCOrV3 Infonomics: The New Economics of Information], [[The Financial Times]], Doug Laney, VP Research, Gartner, September 2013\n* [https://www.youtube.com/watch?v=mQk_5Q3VJv4 Value of Information], GigaOM presentation by Dave McCrory, SVP at [[Warner Music Group]], July 2013   \n* [http://www.bankingtech.com/147432/accounting-for-the-value-of-big-data/ Accounting for the value of (big) data], [[Banking Technology Magazine]], David Bannister, 11 June 2013\n* [http://searchcio.techtarget.com/opinion/Putting-a-price-on-information-The-nascent-field-of-infonomics Putting a price on information: The nascent field of infonomics], [[SearchCIO]] Journal, Linda Tucci, May 2013\n* On March 19, 2013 the Chicago Chapter of the [[Product Development and Management Association]] (PDMA) held an event titled "Monetizing Data: An Evening with Eight of Chicago\'s Data Product Management Leaders"<ref>http://www.builtinchicago.org/blog/check-out-ppt-deck-monetizing-data-evening-eight-chicagos-data-product-management-leaders</ref>\n\n2012\n* [http://www.informationweek.com/big-data/news/big-data-analytics/whats-your-big-data-worth/240144449 What\'s Your Big Data Worth], [[InformationWeek]], Ellis Booker, 17 December 2012\n* [https://www.gartner.com/doc/2278915 Future of Money: Infonomics Monetizing Value in Big Data Information Assets], Mary Knox, [[Gartner]], 14 December 2012\n* [http://www.information-age.com/technology/information-management/2134803/an-introduction-to-infonomics An Introduction to Infonomics], [[InformationAge]], Pete Swabey, 26 November 2012\n* [https://www.gartner.com/doc/2186116 The Birth of Infonomics: the New Economics of Information], [[Gartner]] research publication, Douglas Laney, 3 October 2012 (public summary, full text available to Gartner clients)\n* [http://blogs.gartner.com/doug-laney/tobins-q-a-evidence-of-informations-real-market-value-2/ Tobin’s Q & A: Evidence of Information’s Real Market Value], [[Gartner]] Blog Network, Douglas Laney, 14 Aug 2012\n* [http://www.ft.com/intl/cms/s/0/27476ad4-a6a5-11e1-968b-00144feabdc0.html#axzz1vzOCxVYw Extracting Value from Information], [[Financial Times]], interview with Douglas Laney by Paul Taylor, 25 May 2012 (free registration required)\n* [http://www.forbes.com/sites/gartnergroup/2012/05/22/infonomics-the-practice-of-information-economics/ Infonomics: The Practice of Information Economics], [[Forbes]], by Douglas Laney, 22 May 2012\n* [http://blogs.wsj.com/cio/2012/05/03/to-facebook-youre-worth-80-95/?mod=wsjcio_hps_cioreport# To Facebook You\'re Worth $80.95], [[Wall Street Journal]], by Douglas Laney, 3 May 2012\n* [https://www.gartner.com/doc/1958016 Introducing Infonomics: Valuing Information as a Corporate Asset], [[Gartner]] research publication, Douglas Laney, 21 March 2012 (public summary, full text available to Gartner clients)\n* [http://www.ijikm.org/Volume7/IJIKMv7p177-199Evans0650.pdf Barriers to the Effective Deployment of Information Assets: An Executive Management Perspective], [[Interdisciplinary Journal of Information, Knowledge, and Management]], Nina Evans and James Price, Volume 7, 2012\n\nOlder\n* [http://imcue.com/wp-content/uploads/2011/06/What-is-EIM.pdf/ What is Enterprise Information Management (EIM)] by John Ladley, Morgan Kaufmann, 2010\n* [http://blogs.informatica.com/perspectives/2010/01/26/data-as-an-asset/#comment-1072 Data as an Asset] blog series by John Schmidt, 2010\n* [http://www.amazon.com%2FInformation-Driven-Business-Information-Maximum-Advantage%2Fdp%2F0470625775%2Fref%3Dsr_1_1%3Fie%3DUTF8%26s%3Dbooks%26qid%3D1267263302%26sr%3D8-1/ Information Driven Business: How to Manage Data and Information for Maximum Advantage] by Rob Hillard, Wiley 2010\n* [http://www.amazon.com%2FHow-Measure-Anything-Intangibles-Business%2Fdp%2F0470539399%2Fref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316207185%26sr%3D1-1/ How to Measure Anything: Finding the Value of Intangibles in Business] by Douglas W. Hubbard, Wiley 2010\n* [http://www.amazon.com%2FIntangible-Assets-Valuation-Economic-Benefit%2Fdp%2F0471671312%2Fref%3Dsr_1_3%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316206576%26sr%3D1-3/ Intangible Assets: Valuation and Economic Benefit] by Jeffrey A. Cohen, Wiley 2005\n* [http://www.amazon.com%2FValue-Driven-Intellectual-Capital-Intangible%2Fdp%2F0471351040%2Fref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316206774%26sr%3D1-1/ Value Driven Intellectual Capital: How to Convert Intangible Corporate Assets Into Market Value] by Patrick H. Sullivan, Wiley, 2000\n* [http://www.vldb.org/conf/1998/p641.pdf Bank of America Case Study: The Information Currency Advantage], [[Teradata]], Felipe Carino and Mark Jahnke, Proceedings of the 24th VLDB Conference, New York, NY, 1998\n* [http://www.amazon.com/Information-Payoff-Transformation-Work-Electronic/dp/0029317207/ Information Payoff: The Transformation of Work in the Electronic Age] by Paul A. Strassmann, The Free Press, 1985\n\n== See also ==\n*[[Infonomics]]\n*[[Monetization]]\n*[[Business intelligence]]\n*[[Analytics]]\n*[[Bitcoin]]\n*[[Data as a service]]\n\n== References ==\n{{Reflist|33em}}\n\n[[Category:Articles created via the Article Wizard]]\n[[Category:Data management]]']
['Machine-readable data', '34578263', '\'\'\'Machine-readable data\'\'\' is [[data]] (or [[metadata]]) which is in a format that can be understood by a [[computer]].\n\nThere are two types; human-readable data that is [[markup language|marked up]] so that it can also be read by machines (examples; [[microformat]]s, [[RDFa]], [[HTML]]) or data file formats intended principally for processing by machines ([[Resource Description Framework|RDF]], [[XML]], [[JSON]]).\n\n\'\'Machine readable\'\' is not synonymous with \'\'digitally accessible\'\'.  A digitally accessible document may be online, making it easier for a human to access it via a computer, but unless the relevant data is available in a machine readable format, it will be much harder to use the computer to extract, transform and process that data.<ref>{{cite web\n | url=https://www.data.gov/developers/blog/primer-machine-readability-online-documents-and-data\n | title=A Primer on Machine Readability for Online Documents and Data\n | work=Data.gov\n | date=2012-09-24\n | accessdate=2015-02-27 }}\n</ref>\n\nFor purposes of implementation of the [[Government Performance and Results Act]] (GPRA) Modernization Act, the [[Office of Management and Budget]] (OMB) defines "machine readable" as follows:  "Format in a standard computer language (not English text) that can be read automatically by a web browser or computer system. (e.g.; xml). Traditional word processing documents and portable document format (PDF) files are easily read by humans but typically are difficult for machines to interpret. Other formats such as extensible markup language (XML), (JSON), or spreadsheets with header columns that can be exported as comma separated values (CSV) are machine readable formats. As HTML is a structural markup language, discreetly labeling parts of the document, computers are able to gather document components to assemble Tables of Content, outlines, literature search bibliographies, etc. It is possible to make traditional word processing documents and other formats machine readable but the documents must include enhanced structural elements."<ref>[http://www.whitehouse.gov/sites/default/files/omb/assets/a11_current_year/s200.pdf OMB Circular A-11, Part 6], Preparation and Submission of Strategic Plans, Annual Performance Plans, and Annual Program Performance Reports</ref>\n\n==References==\n{{reflist}}\n\n==See also==\n\n* [[Open data]]\n* [[Linked data]]\n* [[Machine-Readable Documents]]\n* [[Human-readable medium]]\n* [http://xml.fido.gov/stratml/references/PL111-532StratML.htm#SEC10 Section 10] of the [[Government Performance and Results Act|GPRA]] Modernization Act (GPRAMA), which requires U.S. federal agencies to publish their strategic and performance plans and reports in machine-readable format, like [[Strategy Markup Language]] (StratML)\n* President Obama\'s [http://xml.fido.gov/stratml/carmel/EOOMRDwStyle.xml Executive Order] Making Open and Machine Readable the New Default for Government Information\n* [http://xml.fido.gov/stratml/carmel/M-13-13wStyle.xml#_78e85ef4-b91c-11e2-bf2b-79d279ad226c OMB M-13-13], Open Data Policy: Managing Information as an Asset, which requires agencies to use open, machine-readable, data format standards\n[[Category:Data management]]\n\n\n{{Comp-stub}}']
['Linked data', '11174052', 'In [[computing]], \'\'\'linked data\'\'\' (often capitalized as \'\'\'Linked Data\'\'\') is a method of publishing structured data so that it can be interlinked and become more useful through [[semantic query|semantic queries]]. It builds upon standard [[World Wide Web|Web]] technologies such as [[Hypertext Transfer Protocol|HTTP]], [[Resource Description Framework|RDF]] and [[uniform resource identifier|URIs]], but rather than using them to serve web pages for human readers, it extends them to share information in a way that can be read automatically by computers. This enables data from different sources to be connected and queried.<ref name=linkeddatastorysofar>{{Cite journal |url=http://tomheath.com/papers/bizer-heath-berners-lee-ijswis-linked-data.pdf\n |title=Linked Data&mdash;The Story So Far |last=Bizer |first=Christian |last2=Heath |first2=Tom |last3=Berners-Lee\n |first3=Tim |author3-link=Tim Berners-Lee |year=2009 |accessdate=2010-12-18 |doi=10.4018/jswis.2009081901 |issn=1552-6283\n |journal=International Journal on Semantic Web and Information Systems |volume=5 |issue=3 |pages=1–22}} Solving Semantic Interoperability Conflicts in Cross–Border E–Government Services.</ref>\n\n[[Tim Berners-Lee]], director of the [[World Wide Web Consortium]] (W3C), coined the term in a 2006 design note about the [[Semantic Web]] project.<ref name=DesignIssues>{{cite web |url=http://www.w3.org/DesignIssues/LinkedData.html\n |title=Linked Data |work=Design Issues |author=Tim Berners-Lee |authorlink=Tim Berners-Lee |date=2006-07-27\n |publisher=[[W3C]] |accessdate=2010-12-18}}</ref>\n\n== Principles ==\nTim Berners-Lee outlined four principles of linked data in his "Linked Data" note of 2006,<ref name=DesignIssues/> paraphrased along the following lines:\n\n<blockquote>\n# Use [[uniform resource identifier|URIs]] to name (identify) things.\n# Use [[Hypertext Transfer Protocol|HTTP]] URIs so that these things can be looked up (interpreted, "dereferenced").\n# Provide useful information about what a name identifies when it\'s looked up, using open standards such as [[Resource Description Framework|RDF]], [[SPARQL]], etc.\n# Refer to other things using their HTTP URI-based names when publishing data on the Web.\n</blockquote>\n\nTim Berners-Lee gave a presentation on linked data at the [[TED (conference)|TED]] 2009 conference.<ref>{{cite web |url=http://www.ted.com/talks/tim_berners_lee_on_the_next_web.html |title=Tim Berners-Lee on the next Web}}</ref>  In it, he restated the linked data principles as three "extremely simple" rules:\n\n<blockquote>\n# All kinds of conceptual things, they have names now that start with HTTP.\n# If I take one of these HTTP names and I look it up...I will get back some data in a standard format which is kind of useful data that somebody might like to know about that thing, about that event.\n# When I get back that information it\'s not just got somebody\'s height and weight and when they were born, its got relationships. And when it has relationships, whenever it expresses a relationship then the other thing that it\'s related to is given one of those names that starts with HTTP.\n</blockquote>\n\n== Components ==\n* [[Uniform resource identifier|URI]]s\n* [[HTTP]]\n* [[Structured data]] using [[controlled vocabulary]] terms and dataset definitions expressed in [[Resource Description Framework]] [[serialization]] formats such as [[RDFa]], [[RDF/XML]], [[Notation 3|N3]], [[Turtle (syntax)|Turtle]], or [[JSON-LD]]\n* [[Linked Data Platform]]\n\n==Linked open data==\n\'\'\'Linked open data\'\'\' is linked data that is [[open content]].<ref>{{cite web|url=http://linkeddata.org/faq|title=Frequently Asked Questions (FAQs) - Linked Data - Connect Distributed Data across the Web|publisher=}}</ref><ref>{{cite web|url=https://www.coar-repositories.org/activities/repository-observatory/second-edition-linked-open-data/7-things-you-should-know-about-open-data/|title=COAR »   7 things you should know about…Linked Data|publisher=}}</ref><ref>{{cite web|url=http://openorg.ecs.soton.ac.uk/wiki/Linked_Data_Basics_for_Techies#Open_Linked_Data|title=Linked Data Basics for Techies|publisher=}}</ref> Tim Berners-Lee gives the clearest definition of linked open data in differentiation with linked data. {{Quote |text=Linked Open Data (LOD) is Linked Data which is released under an open licence, which does not impede its reuse for free. |author=Tim Berners-Lee |title=Linked Data<ref name=DesignIssues /><ref>{{cite web|url=http://5stardata.info/en|title=5 Star Open Data}}</ref>}} Large linked open data sets include [[DBpedia]] and [[Freebase]].\n\n=== History ===\n\nThe term "linked open data" has been in use since at least February 2007, when the "Linking Open Data" mailing list<ref>{{cite web|url=http://lists.w3.org/Archives/Public/public-lod/|title=public-lod@w3.org Mail Archives|publisher=}}</ref> was created.<ref>{{cite web|url=http://www.w3.org/wiki/SweoIG/TaskForces/CommunityProjects/LinkingOpenData/NewsArchive|title=SweoIG/TaskForces/CommunityProjects/LinkingOpenData/NewsArchive|publisher=}}</ref> The mailing list was initially hosted by the [[SIMILE]] project<ref>{{cite web|url=http://simile.mit.edu/mail.html|title=SIMILE Project - Mailing Lists|publisher=}}</ref> at the [[Massachusetts Institute of Technology]].\n\n=== Linking Open Data community project ===\n[[File:LOD Cloud 2014.svg|thumb|400px|The above diagram shows which Linking Open Data datasets are connected, as of August 2014.  This was produced by the \nLinked Open Data Cloud project, which was started in 2007.  Some sets may include copyrighted data which is freely available.<ref>Linking open data cloud diagram 2014, by Max Schmachtenberg, Christian Bizer, Anja Jentzsch and Richard Cyganiak. http://lod-cloud.net/</ref>]]\n\nThe goal of the W3C Semantic Web Education and Outreach group\'s Linking Open Data community project is to extend the Web with a [[Knowledge commons|data commons]] by publishing various [[open knowledge|open]] [[dataset]]s as RDF on the Web and by setting [[Resource Description Framework|RDF]] links between data items from different data sources. In October 2007, datasets consisted of over two billion [[RDF triples]], which were interlinked by over two million RDF links.<ref>[http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData Linking Open Data]</ref><ref>{{cite book |last1=Fensel |first1=Dieter |last2=Facca |first2= Federico Michele |last3=Simperl |first3=Elena |last4=Ioan |first4=Toma |title=Semantic Web Services |year=2011 |publisher=Springer|isbn=3642191924 |pages=99}}</ref>  By September 2011 this had grown to 31 billion RDF triples, interlinked by around 504 million RDF links.  A detailed statistical breakdown was published in 2014.<ref>http://linkeddatacatalog.dws.informatik.uni-mannheim.de/state/</ref>\n\n=== European Union projects ===\nThere are a number of European Union projects{{Definition|date=June 2013}} involving linked data. These include the linked open data around the clock (LATC) project,<ref>[http://latc-project.eu/ Linked open data around the clock (LATC)]</ref> the PlanetData project,<ref>[http://planet-data.eu/ PlanetData]</ref> the DaPaaS (Data-and-Platform-as-a-Service) project,<ref>[http://project.dapaas.eu/ DaPaaS]</ref> and  the Linked Open Data 2 (LOD2) project.<ref>[http://lod2.eu/ Linking Open Data 2 (LOD2)]</ref><ref>{{cite web |url=http://cordis.europa.eu/fetch?CALLER=PROJ_ICT&ACTION=D&CAT=PROJ&RCN=95562 |publisher=European Commission |title=CORDIS FP7 ICT Projects – LOD2 |date=2010-04-20}}</ref><ref>{{cite web |url=http://static.lod2.eu/Deliverables/LOD2_D12.5.1_Project_Fact_Sheet_Version.pdf |title=LOD2 Project Fact Sheet – Project Summary |date=2010-09-01 |accessdate=2010-12-18}}</ref> Data linking is one of the main goals of the [[EU Open Data Portal]], which makes available thousands of datasets for anyone to reuse and link.\n\n=== Datasets ===\n\n* [[DBpedia]] – a dataset containing extracted data from Wikipedia; it contains about 3.4 million concepts described by 1 billion [[Semantic triple|triples]], including abstracts in 11 different languages\n* [[FOAF (software)|FOAF]] – a dataset describing persons, their properties and relationships\n* [[GeoNames]] provides RDF descriptions of more than {{formatnum:7500000}} geographical features worldwide.\n* [[UMBEL]] – a lightweight reference structure of {{formatnum:20000}} subject concept classes and their relationships derived from [[OpenCyc]], which can act as binding classes to external data; also has links to 1.5 million named entities from DBpedia and [[YAGO (ontology)|YAGO]]\n* [[Wikidata]] – a collaboratively-created linked dataset that acts as central storage for the structured data of its [[Wikimedia]] sister projects\n\n=== Dataset instance and class relationships ===\nClickable diagrams that show the individual datasets and their relationships within the DBpedia-spawned LOD cloud (as shown by the figures to the right) are available.<ref>[http://www4.wiwiss.fu-berlin.de/bizer/pub/lod-datasets_2009-07-14.html Instance relationships amongst datasets]</ref><ref>[http://web.archive.org/web/20110828103804/http://umbel.org/sites/umbel.org/lod/lod_constellation.html Class relationships amongst datasets]</ref>\n\n==See also==\n* [[Authority control]] – about \'\'controlled headings\'\' in library catalogs\n* [[Citation analysis]] – for citations between scholarly articles\n* [[Hyperdata]]\n* [[Linked data page]]\n* [[Network model]] – an older type of database management system\n* [[Schema.org]]\n* [[Web Ontology Language]]\n\n== References ==\n{{reflist|30em}}\n\n== Further reading ==\n{{ref begin|2}}\n* Ahmet Soylu, Felix Mödritscher, and Patrick De Causmaecker. 2012. [http://www.ahmetsoylu.com/wp-content/uploads/2013/10/soylu_ICAE2012.pdf “Ubiquitous Web Navigation through Harvesting Embedded Semantic Data: A Mobile Scenario.”] Integrated Computer-Aided Engineering 19 (1): 93–109.\n* \'\'[http://linkeddatabook.com/book Linked Data: Evolving the Web into a Global Data Space]\'\' (2011) by Tom Heath and Christian Bizer, Synthesis Lectures on the Semantic Web: Theory and Technology, Morgan & Claypool <!-- note this resources supersedes the tutorial [http://www4.wiwiss.fu-berlin.de/bizer/pub/LinkedDataTutorial/ How to publish Linked Data on the Web] by Bizer, Cyganiak, and Heath -->\n* [http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/LinkedDataTutorial/ How to Publish Linked Data on the Web], by Chris Bizer, Richard Cyganiak and Tom Heath, Linked Data Tutorial at Freie Universität Berlin, Germany, 27 July 2007.\n* [http://www.scientificamerican.com/article.cfm?id=berners-lee-linked-data The Web Turns 20: Linked Data Gives People Power], part 1 of 4, by Mark Fischetti, \'\'[[Scientific American]]\'\' 2010 October 23\n* [http://knoesis.wright.edu/library/publications/linkedai2010_submission_13.pdf Linked Data Is Merely More Data] – Prateek Jain, Pascal Hitzler, Peter Z. Yeh, Kunal Verma, and Amit P. Sheth. In: Dan Brickley, Vinay K. Chaudhri, Harry Halpin, and Deborah McGuinness: \'\'Linked Data Meets Artificial Intelligence\'\'. Technical Report SS-10-07, AAAI Press, Menlo Park, California, 2010, pp.&nbsp;82–86.\n* [http://knoesis.org/library/resource.php?id=1718 Moving beyond sameAs with PLATO: Partonomy detection for Linked Data] – Prateek Jain, Pascal Hitzler, Kunal Verma, Peter Z. Yeh, Amit Sheth. In:  Proceedings of the 23rd ACM Hypertext and Social Media conference (HT 2012), Milwaukee, WI, USA, June 25–28, 2012.\n* Freitas, André, Edward Curry, João Gabriel Oliveira, and Sean O’Riain. 2012. [http://www.edwardcurry.org/publications/freitas_IC_12.pdf “Querying Heterogeneous Datasets on the Linked Data Web: Challenges, Approaches, and Trends.”] IEEE Internet Computing 16 (1): 24–33.\n* [http://www2008.org/papers/pdf/p1265-bizer.pdf Linked Data on the Web] – Chris Bizer, Tom Heath, [[Kingsley Uyi Idehen]], [[Tim Berners-Lee]]. In Proceedings WWW2008, Beijing, China\n* [http://sites.wiwiss.fu-berlin.de/suhl/bizer/pub/LinkingOpenData.pdf Interlinking Open Data on the Web] – Chris Bizer, Tom Heath, Danny Ayers, Yves Raimond. In Proceedings Poster Track, ESWC2007, Innsbruck, Austria\n* [http://knoesis.wright.edu/library/publications/iswc10_paper218.pdf Ontology Alignment for Linked Open Data] – Prateek Jain, Pascal Hitzler, Amit Sheth, Kunal Verma, Peter Z. Yeh. In proceedings of the 9th International Semantic Web Conference, ISWC 2010, Shanghai, China\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3121711/ Linked open drug data for pharmaceutical research and development] - J Cheminform. 2011; 3: 19. Samwald, Jentzsch, Bouton, Kallesøe, Willighagen, Hajagos, Marshall, Prud\'hommeaux, Hassenzadeh, Pichler, and Stephens (May 2011)\n* [http://www.community-of-knowledge.de/beitrag/the-hype-the-hope-and-the-lod2-soeren-auer-engaged-in-the-next-generation-lod/ Interview with Sören Auer, head of the LOD2 project about the continuation of LOD2 in 2011], June 2011\n* [http://www.semantic-web.at/LOD-TheEssentials.pdf Linked Open Data: The Essentials] - Florian Bauer and Martin Kaltenböck (January 2012)\n* [http://semanticweb.com/the-flap-of-a-butterfly-wing_b26808 The Flap of a Butterfly Wing] - semanticweb.com Richard Wallis (February 2012)\n{{ref end}}\n\n== External links ==\n* [http://www.w3.org/wiki/LinkedData LinkedData] at the W3C Wiki\n* [http://linkeddata.org LinkedData.org]\n* [http://virtuoso.openlinksw.com/white-papers/ OpenLink Software white papers]\n* [http://demo.openlinksw.com/Demo/customers/CustomerID/ALFKI%23this Data from Northwind SQL schema as linked data], use case demo\n* [http://nomisma.org/ Linked data for the discipline of numismatics], use case demo\n* [http://en.lodlive.it Interactive LOD demo]\n* [http://americanartcollaborative.org/ American Art Collaborative], consortium of US art museums committed to establishing a critical mass of linked open data on American art\n\n{{Semantic Web}}\n{{Open data navbox}}\n\n{{Authority control}}\n\n[[Category:Cloud standards]]\n[[Category:Data management]]\n[[Category:Distributed computing architecture]]\n[[Category:Hypermedia]]\n[[Category:Internet terminology]]\n[[Category:Open data]]\n[[Category:World Wide Web]]\n[[Category:Semantic Web]]']
['Clone (database)', '8586147', "{{Multiple issues|\n{{unreferenced|date=December 2006}}\n{{orphan|date=February 2009}}\n}}\n\nA '''database clone ''' is a complete and separate copy of a database system that includes the business data , the [[DBMS]] software and any other application  tiers that make up the environment. Cloning is a different kind of operation to [[Data replication|replication]] and [[backup]]s in that the cloned environment is both fully functional and separate in its own right. Additionally the cloned environment may be modified at its inception due to configuration changes or data subsetting.\n\nThe cloning refers to the replication of the server in order to have a backup, to upgrade the environment.\n\n{{DEFAULTSORT:Clone (Database)}}\n[[Category:Data management]]\n[[Category:Databases]]"]
['Single customer view', '37040021', "A '''Single Customer View''' is an aggregated, consistent and holistic representation of the [[data]] known by an organisation about its customers<ref>[http://www.experian.co.uk/assets/about-us/white-papers/single-customer-view-whitepaper.pdf Exploiting the Single Customer  View to Maximise the Value of Customer Relationships]</ref><ref>[http://www.marketingweek.co.uk/driving-value-from-the-single-customer-view/3015497.article Driving value from the single customer view]</ref> that can be viewed in one place, such as a single page.<ref>[https://spotlessdata.com/blog/data-driven-marketing Data-driven marketing]</ref> The advantage to an organisation of attaining this unified view comes from the ability it gives to analyse past behaviour in order to better target and personalise future customer interactions.<ref>[http://www.atominsight.com/about-us/blog/single-customer-view-essential Why a single customer view is essential]</ref> A single customer view is also considered especially relevant where organisations engage with customers through [[multichannel marketing]], since customers expect those interactions to reflect a consistent understanding of their history and preferences.<ref>[http://econsultancy.com/uk/blog/9612-the-impact-of-a-single-customer-view-on-consumer-behaviour-infographic The impact of a single customer view on consumer behaviour: infographic]</ref> However, some commentators have challenged the idea that a single view of customers across an entire organisation is either natural or meaningful, proposing that the priority should instead be consistency between the multiple views that arise in different contexts.\n\nWhere representations of a customer are held in more than one [[data set]], achieving a single customer view can be difficult: firstly because customer identity must be traceable between the records held in those systems, and secondly because anomalies or discrepancies in the customer data must be [[data cleansing|data cleansed]].<ref>[http://www.atominsight.com/about-us/blog/single-customer-view-hard Why building a single customer view isn’t as easy as you might think]</ref> As such, the acquisition by an organisation of a single customer view is one potential outcome of successful [[master data management]]. Since 31 December, 2010, maintaining a single customer view has become mandatory for [[United Kingdom]] banks and other deposit takers due to new rules introduced by the [[Financial Services Compensation Scheme]].<ref>[https://www.fscs.org.uk/industry/single-customer-view/ Single Customer View]</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Identity management]]\n[[Category:Business intelligence]]\n[[Category:Data management]]\n[[Category:Data warehousing]]\n[[Category:Information technology management]]"]
['SQL/PSM', '11665200', '{{infobox programming language\n| name                   = SQL/PSM\n| logo                   =\n| paradigm               = [[multi-paradigm programming language|Multi-paradigm]]\n| year                   = 1996\n| designer               =\n| developer              =\n| latest_release_version = [[SQL:2011]]\n| latest_release_date    = \n| latest_preview_version = \n| latest_preview_date    = \n| turing-complete        = Yes\n| typing                 = \n| implementations        = [[MySQL]] <br />IBM\'s [[SQL PL]]\n| influenced_by          = [[Ada (programming language)|Ada]]<ref>{{Citation | url = http://ocelot.ca/blog/blog/2015/01/15/stored-procedures-critiques-and-defences/ | title = Stored Procedures: critiques and defences | year = 2015 | first1 = Peter | last1 = Gulutzan }}</ref>\n| influenced             = \n| operating_system       = [[Cross-platform|Cross-platform (multi-platform)]]\n| license                =\n| website                = \n| file_ext               =\n| dialects               =\n| wikibooks              = \n}}\n\n\'\'\'SQL/PSM\'\'\' ([[SQL]]/Persistent Stored Modules) is an [[ISO standard]] mainly defining an extension of SQL with a [[procedural language]] for use in [[stored procedure]]s. Initially published in 1996 as an extension of [[SQL-92]] (ISO/IEC 9075-4:1996, a version sometimes called PSM-96 or even SQL-92/PSM<ref>{{Cite journal | last1 = Eisenberg | first1 = A. | title = New standard for stored procedures in SQL | doi = 10.1145/245882.245907 | journal = ACM SIGMOD Record | volume = 25 | issue = 4 | pages = 81-88| year = 1996 | pmid =  | pmc = }}</ref>), SQL/PSM was later incorporated into the multi-part [[SQL:1999]] standard, and has been part 4 of that standard since then, most recently in [[SQL:2011]].  The SQL:1999 part 4 covered less than the original PSM-96 because the SQL statements for defining, managing, and invoking routines were actually incorporated into part 2 SQL/Foundation, leaving only the procedural language itself as SQL/PSM.<ref>{{cite book| first1 =Jim | last1 = Melton | first2 =Alan R | last2 = Simon | title = SQL: 1999|year=2002| publisher = Morgan Kaufmann|isbn= 978-1-55860-456-8 | pages = 541–42}}</ref> The SQL/PSM facilities are still optional as far as the SQL standard is concerned; most of them are grouped in Features P001-P008.\n\nSQL/PSM standardizes syntax and semantics for [[control flow]], [[exception handling]] (called "condition handling" in SQL/PSM), local variables, assignment of expressions to variables and parameters, and (procedural) use of [[Cursor (databases)|cursors]]. It also defines an information schema ([[metadata]]) for stored procedures.  SQL/PSM is one language in which [[Method (computer programming) |methods]] for the SQL:1999 [[structured type]]s can be defined.  The other is Java, via [[SQL/JRT]].\n\nIn practice [[MySQL]]\'s procedural language and IBM\'s [[SQL PL]] (used in DB2) are closest to the SQL/PSM standard.<ref name = "HarrisonFeuerstein2008">{{cite book | first1 = Guy | last1 = Harrison| first2 = Steven | last2 = Feuerstein|title=MySQL Stored Procedure Programming|url= https://books.google.com/books?id=YpeP0ok0cO4C&pg=PT75 | year=2008|publisher=O\'Reilly |isbn = 978-0-596-10089-6 |page= 49}}</ref> \n\nSQL/PSM resembles and inspired by [[PL/SQL]], as well as [[PL/pgSQL]], so they are similar languages.  With [[PostgreSQL]] v9 some SQL/PSM features, like overloading of SQL-invoked functions and procedures<ref>{{Citation | publisher = PostgreSQL | title = SQL standard features | edition = 9 | contribution-url = http://www.postgresql.org/docs/9.0/static/features-sql-standard.html | contribution = feature T322}}.</ref> are now supported.  A [[PostgreSQL]] addon implements SQL/PSM<ref>{{Citation | url = https://github.com/okbob/plpsm0 | format = git | type = repository | title = plpsm0}}.</ref><ref>{{Citation | publisher = PostgreSQL | url = http://www.postgresql.org/message-id/1305291347.14548.13.camel@jara.office.nic.cz | date = May 2011 | title = Announce}}.</ref><ref>[http://www.postgresql.org/message-id/CAFj8pRDWFdcjNSnwQB_3j1-rMO6b8=TmLTNBvDCSpRrOW2Dfeg@mail.gmail.com 2012-2\'s Proposal PL/pgPSM announce]</ref><ref>{{Citation | title = SQL/PSM | format = wiki | url = http://postgres.cz/wiki/SQL/PSM_Manual | publisher = PostgreSQL | type = manual | year = 2008}}.</ref> (alongside its own procedural language), although it is not part of the core product.<ref>{{Citation | contribution-url = http://www.postgresql.org/docs/9.2/static/features.html | publisher = PostgreSQL | title = Documentation | edition = 9.2 | contribution = SQL Conformance}}.</ref>\n\n==See also==\nThe following implementations adopt the standard, but they are not 100% compatible to SQL/PSM:\n\n[[Open source]]:\n* [[HSQLDB]] stored procedures and functions<ref name="SQL/PSM routines">http://hsqldb.org/doc/2.0/guide/sqlroutines-chapt.html#src_psm_routines</ref>\n* [[MySQL]] stored procedures <ref name="HarrisonFeuerstein2008"/>\n* [[PostgreSQL]] [[PL/pgSQL]]\n\nProprietary:\n* Oracle [[PL/SQL]]\n* Microsoft and Sybase [[Transact-SQL]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* Jim Melton, \'\'Understanding SQL\'s Stored Procedures: A Complete Guide to SQL/PSM\'\', Morgan Kaufmann Publishers, 1998, ISBN 1-55860-461-8\n\n{{SQL}}\n\n__NOTOC__\n\n{{DEFAULTSORT:SQL PSM}}\n[[Category:Data management]]\n[[Category:SQL]]\n[[Category:Data-centric programming languages]]\n[[Category:Programming languages created in 1996]]\n\n\n{{compu-lang-stub}}\n{{database-stub}}']
['Clustered file system', '10459749', '{{distinguish|Data cluster}}\n{{redirect2|Network filesystem|Parallel file system|the Sun NFS protocol|Network File System|the IBM GPFS protocol|IBM General Parallel File System}}\n{{multiple|\n{{refimprove|date=December 2015}}\n{{cleanup|date=December 2013|reason=Merges need to be smoothed over}}\n}}\n\nA \'\'\'clustered file system\'\'\' is a [[file system]] which is shared by being simultaneously [[Mount (computing)|mounted]] on multiple [[Server (computing)|servers]].  There are several approaches to [[computer cluster|clustering]], most of which do not employ a clustered file system (only [[direct attached storage]] for each node).  Clustered file systems can provide features like location-independent addressing and redundancy which improve reliability or reduce the complexity of the other parts of the cluster.  \'\'\'Parallel file systems\'\'\' are a type of clustered file system that spread data across multiple storage nodes, usually for redundancy or performance.<ref>http://www.dell.com/downloads/global/power/ps2q05-20040179-Saify-OE.pdf</ref>\n\n== {{Anchor|SHARED-DISK}}Shared-disk file system ==\nA \'\'\'shared-disk file system\'\'\' uses a [[storage area network|storage-area network]] (SAN) to allow multiple computers to gain  direct disk access at the [[Block (data storage)|block level]].  Access control and translation from file-level operations that applications use to block-level operations used by the SAN must take place on the client node.  The most common type of clustered file system, the shared-disk file system &mdash;by adding mechanisms for [[concurrency control]]&mdash;provides a consistent and [[serialization|serializable]] view of the file system, avoiding corruption and unintended [[data loss]] even when multiple clients try to access the same files at the same time. Shared-disk file-systems commonly employ some sort of [[Fencing (computing)|fencing]] mechanism to prevent data corruption in case of node failures, because an unfenced device can cause data corruption if it loses communication with its sister nodes and tries to access the same information other nodes are accessing.\n\nThe underlying storage area network may use any of a number of block-level protocols, including [[SCSI]], [[iSCSI]], [[HyperSCSI]], [[ATA over Ethernet]] (AoE), [[Fibre Channel]], [[network block device]], and [[InfiniBand]].\n\nThere are different architectural approaches to a shared-disk filesystem. Some distribute file information across all the servers in a cluster (fully distributed). Others utilize a centralized [[metadata]] server. Both achieve the same result of enabling all servers to access all the data on a shared storage device.{{Citation needed|date=December 2009}}\n\n=== Examples ===\n{{Div col||25em}}\n* [[Blue Whale Clustered file system]] (BWFS)\n* [[Silicon Graphics]] (SGI) clustered file system ([[CXFS]])\n* [[Veritas Cluster File System]]\n* DataPlow [[Nasan]] File System\n* [[IBM General Parallel File System]] (GPFS)\n* [[LizardFS]]\n* [[Lustre (file system)|Lustre]]\n* Microsoft [[Cluster Shared Volumes]] (CSV)\n* [[OCFS|Oracle Cluster File System]] (OCFS)\n* PolyServe storage solutions\n* [[Quantum Corporation|Quantum]] [[StorNext File System|StorNext]] File System (SNFS), ex ADIC, ex CentraVision File System (CVFS)\n* Red Hat [[Global File System]] (GFS)\n* Sun [[QFS]]\n* TerraScale Technologies TerraFS\n* Versity VSM\n* [[VMware VMFS]]\n* [[Xsan]]\n{{Div col end}}\n\n=={{Anchor|DISTRIBUTED-FS}}Distributed file systems==\n\'\'Distributed file systems\'\' do not share block level access to the same storage but use a network [[protocol (computing)|protocol]].<ref>Silberschatz, Galvin (1994). \'\'Operating System concepts\'\', chapter 17 \'\'Distributed file systems\'\'. Addison-Wesley Publishing Company. ISBN 0-201-59292-4.</ref><ref name="ostep-1">{{citation|title=Sun\'s Network File System|url=http://pages.cs.wisc.edu/~remzi/OSTEP/dist-nfs.pdf|publisher= Arpaci-Dusseau Books|date = 2014|first1 = Remzi H.|last1 =Arpaci-Dusseau|first2=Andrea C.|last2 = Arpaci-Dusseau}}</ref>  These are commonly known as \'\'network file systems\'\', even though they are not the only file systems that use the network to send data.{{citation needed|date=March 2013}}  Distributed file systems can restrict access to the file system depending on [[access list]]s or [[Capability-based security|capabilities]] on both the servers and the clients, depending on how the protocol is designed.\n\nThe difference between a distributed file system and a [[distributed data store]] is that a distributed file system allows files to be accessed using the same interfaces and semantics as local files{{snd}} for example, mounting/unmounting, listing directories, read/write at byte boundaries, system\'s native permission model.  Distributed data stores, by contrast, require using a different API or library and have different semantics (most often those of a database).{{cn|date=June 2016}}\n\nA distributed file system may also be created by software implementing IBM\'s [[Distributed Data Management Architecture]] (DDM), in which programs running on one computer use local interfaces and semantics to create, manage and access files located on other networked computers.  All such client requests are trapped and converted to equivalent messages defined by the DDM. Using protocols also defined by the DDM, these messages are transmitted to the specified remote computer on which a DDM server program interprets the messages and uses the file system interfaces of that computer to locate and interact with the specified file.\n\n===Design goals===\nDistributed file systems may aim for "transparency" in a number of aspects.  That is, they aim to be "invisible" to client programs, which "see" a system which is similar to a local file system.  Behind the scenes, the distributed file system handles locating files, transporting data, and potentially providing other features listed below.\n\n* \'\'Access transparency\'\' is that clients are unaware that files are distributed and can access them in the same way as local files are accessed.\n* \'\'Location transparency\'\'; a consistent name space exists encompassing local as well as remote files. The name of a file does not give its location.\n* \'\'Concurrency transparency\'\'; all clients have the same view of the state of the file system. This means that if one process is modifying a file, any other processes on the same system or remote systems that are accessing the files will see the modifications in a coherent manner.\n* \'\'Failure transparency\'\'; the client and client programs should operate correctly after a server failure.\n* \'\'Heterogeneity\'\'; file service should be provided across different hardware and operating system platforms.\n* \'\'Scalability\'\'; the file system should work well in small environments (1 machine, a dozen machines) and also scale gracefully to huge ones (hundreds through tens of thousands of systems).\n* \'\'Replication transparency\'\'; to support scalability, we may wish to replicate files across multiple servers. Clients should be unaware of this.\n* \'\'Migration transparency\'\'; files should be able to move around without the client\'s knowledge.\n\n===History===\nThe [[Incompatible Timesharing System]] used virtual devices for transparent inter-machine file system access in the 1960s.  More file servers were developed in the 1970s. In 1976 [[Digital Equipment Corporation]] created the [[File Access Listener]] (FAL), an implementation of the [[Data Access Protocol]] as part of [[DECnet]] Phase II which became the first widely used network file system. In 1985 [[Sun Microsystems]] created the file system called "[[Network File System (protocol)|Network File System]]" (NFS) which became the first widely used [[Internet Protocol]] based network file system.<ref name="ostep-1" />  Other notable network file systems are [[Andrew File System]] (AFS), [[Apple Filing Protocol]] (AFP), [[NetWare Core Protocol]] (NCP), and [[Server Message Block]] (SMB) which is also known as Common Internet File System (CIFS).\n\nIn 1986, [[IBM]] announced client and server support for Distributed Data Management Architecture (DDM) for the [[System/36]], [[System/38]], and IBM mainframe computers running [[CICS]].  This was followed by the support for [[IBM Personal Computer]], [[AS/400]], IBM mainframe computers under the [[MVS]] and [[VSE (operating system)|VSE]] operating systems, and [[FlexOS]].   DDM also became the foundation for [[DRDA|Distributed Relational Database Architecture]], also known as DRDA.\n\n=== Examples ===\n{{Main|List of file systems#Distributed file systems|l1 = List of distributed file systems}}\n{{Div col||25em}}\n* [[BeeGFS]] (Fraunhofer)\n* [[Ceph (software)|Ceph]] (Inktank, Red Hat, SUSE)\n* [[Distributed File System (Microsoft)|Windows Distributed File System (DFS)]] (Microsoft)\n* [[Infinit (file system)|Infinit]]\n* [[Gfarm file system|GfarmFS]]\n* [[GlusterFS]] (Red Hat)\n* [[Google file system|GFS]] (Google Inc.)\n* [[Hadoop distributed file system|HDFS]] (Apache Software Foundation)\n* [[InterPlanetary File System|IPFS]]\n* iRODS\n* [[LizardFS]] (Skytechnology)\n* [[Moose File System|MooseFS]] (Core Technology / Gemius)\n* [[ObjectiveFS]]\n* [[OneFS]] (EMC Isilon)\n* OpenIO\n* [[OrangeFS]] (Clemson University, Omnibond Systems), formerly [[Parallel Virtual File System]]\n* [[Panasas|Panfs]] (Panasas)\n* [[Parallel Virtual File System]] (Clemson University, Argonne National Laboratory, Ohio Supercomputer Center)\n* [[RozoFS]] (Rozo Systems)\n* Torus (CoreOS)\n* [[XtreemFS]]\n{{Div col end}}\n\n==Network-attached storage==\n{{Main|Network-attached storage}}\n\nNetwork-attached storage (NAS) provides both storage and a file system, like a shared disk file system on top of a storage area network (SAN).  NAS typically uses file-based protocols (as opposed to block-based protocols a SAN would use) such as [[Network File System (protocol)|NFS]] (popular on [[UNIX]] systems), SMB/CIFS ([[Server Message Block|Server Message Block/Common Internet File System]]) (used with MS Windows systems), [[Apple Filing Protocol|AFP]] (used with [[Macintosh|Apple Macintosh]] computers), or NCP (used with [[Novell Open Enterprise Server|OES]] and [[NetWare|Novell NetWare]]).\n\n==Design considerations==\n\n===Avoiding single point of failure===\n\nThe failure of disk hardware or a given storage node in a cluster can create a [[single point of failure]] that can result in [[data loss]] or unavailability.  [[Fault tolerance]] and high availability can be provided through [[Replication (computing)|data replication]] of one sort or another, so that data remains intact and available despite the failure of any single piece of equipment.  For examples, see the lists of [[List of file systems#Distributed fault-tolerant file systems|distributed fault-tolerant file systems]] and [[List of file systems#Distributed parallel fault-tolerant file systems|distributed parallel fault-tolerant file systems]].\n\n===Performance===\n\nA common [[performance]] [[measurement]] of a clustered file system is the amount of time needed to satisfy service requests.  In conventional systems, this time consists of a disk-access time and a small amount of [[Central processing unit|CPU]]-processing time.  But in a clustered file system, a remote access has additional overhead due to the distributed structure.  This includes the time to deliver the request to a server, the time to deliver the response to the client, and for each direction, a CPU overhead of running the [[communication protocol]] [[software]].\n\n===Concurrency===\n\nConcurrency control becomes an issue when more than one person or client is accessing the same file or block and want to update it.  Hence updates to the file from one client should not interfere with access and updates from other clients. This problem is more complex with file systems due to concurrent overlapping writes, where different writers write to overlapping regions of the file concurrently.<ref>Pessach, Yaniv (2013). \'\'Distributed Storage: Concepts, Algorithms, and Implementations\'\'. ISBN 978-1482561043.</ref> This problem is usually handled by [[concurrency control]] or [[lock (computer science)|locking]] which may either be built into the file system or provided by an add-on protocol.\n\n==History==\nIBM mainframes in the 1970s could share physical disks and file systems if each machine had its own channel connection to the drives\' control units. In the 1980s, [[Digital Equipment Corporation]]\'s [[TOPS-20]] and [[OpenVMS]] clusters (VAX/ALPHA/IA64) included shared disk file systems.{{Citation needed|date=May 2016}}\n\n==See also==\n{{Div col||25em}}\n* [[Network-attached storage]]\n* [[Storage area network]]\n* [[Shared resource]]\n* [[Direct-attached storage]]\n* [[Peer-to-peer file sharing]]\n* [[Disk sharing]]\n* [[Distributed data store]]\n* [[Distributed file system for cloud]]\n* [[Global file system]]\n* [[Gopher (protocol)]]\n* [[List of file systems#Distributed file systems|List of distributed file systems]]\n* [[CacheFS]]\n* [[RAID]]\n{{Div col end}}\n\n==References==\n{{reflist}}\n\n==Further reading==\n* [http://www.cloudbus.org/reports/DistributedStorageTaxonomy.pdf A Taxonomy of Distributed Storage Systems]\n* [http://trac.nchc.org.tw/grid/raw-attachment/wiki/jazz/09-05-22/A_Taxonomy_and_Survey_on_Distributed_File_Systems.pdf A Taxonomy and Survey on Distributed File Systems]\n* [http://www.cis.upenn.edu/~bcpierce/courses/dd/papers/satya89survey.ps A survey of distributed file systems]\n* [http://www.snia-europe.org/objects_store/Christian_Bandulet_SNIATutorial%20Basics_EvolutionFileSystems.pdf The Evolution of File Systems]\n\n{{File systems|state=collapsed}}\n\n[[Category:Computer file systems]]\n[[Category:Shared disk file systems| ]]\n[[Category:Storage area networks]]\n[[Category:Distributed file systems| ]]\n[[Category:Data management]]\n[[Category:Distributed data storage]]\n[[Category:Network file systems]]']
['System of record', '2100046', 'A \'\'\'system of record\'\'\' (SOR) or \'\'\'Source System of Record\'\'\' (SSoR) is a [[Data Management|data management]] term for an [[information]] storage system (commonly implemented on a [[computer system]]) that is the authoritative data source for a given [[data element]] or piece of information. The need to identify systems of record can become acute in organizations where [[management information system]]s have been built by taking output data from multiple source systems, re-processing this data, and then re-presenting the result for a new business use.\n\nIn these cases, multiple information systems may disagree about the same piece of information. These disagreements may stem from semantic differences, differences in opinion, use of different sources, differences in the timing of the [[Extract, transform, load|ETL]] extracts that create the data they report against, or may simply be the result of bugs. \n\nThe [[integrity]] and [[validity]] of any data set is open to question when there is no [[tracing (software)|traceable]] connection to a good source, such as a known System of Record. Where the integrity of the data is vital, if there is an agreed system of record, the data element must either be linked to, or extracted directly from it. In other cases, the provenance and estimated data quality should be documented. \n\nThe "system of record" approach is a good fit for environments where both:\n* there is a single authority over all data consumers, and \n* all consumers have similar needs \nIn diverse environments, one instead needs to support the presence of multiple opinions. Consumers may accept different authorities or may differ on what constitutes an authoritative source -- researchers may prefer carefully vetted data, while tactical military systems may require the most recent credible report.\n\n==See also==\n* [[Single Source of Truth]] practice of using one source for a particular data element\n* [[Privacy Act of 1974]] United States law including requirement for agencies to publish System Of Records Notices (SORN) in the [[Federal Register]] to identify the system and describe the use of individuals data.\n* [[Master Data Management]] defining the handling of master data\n* [[Systems of Engagement]] — more decentralized systems that incorporate technologies which encourage peer interactions\n\n==References==\n* {{cite web |title=The System of Record in the Global Data Warehouse | url=http://www.information-management.com/issues/20030501/6645-1.html  |publisher = Information Management |accessdate=2007-12-18 |author=[[Bill Inmon]]  |date=May 2003 |work= }}\n* {{cite web \n| title = The Golden Copy \n| url = http://adam.goucher.ca/?p=72\n| last = Goucher\n| first = Adam\n| date = 2006-04-26\n| accessdate = 2013-04-30}}\n* {{cite web\n| title = The Move from Systems of Record to Systems Of Engagement\n| url = http://www.forbes.com/sites/joshbersin/2012/08/16/the-move-from-systems-of-record-to-systems-of-engagement/\n| last = Bersin\n| first = Josh\n| date = 2012-08-16\n| accessdate = 2013-04-30}}\n\n\n[[Category:Information systems]]\n[[Category:Data management]]\n\n\n{{compu-stub}}']
['Online analytical processing', '189239', '\'\'\'Online analytical processing\'\'\', or \'\'\'OLAP\'\'\' ({{IPAc-en|ˈ|oʊ|l|æ|p}}), is an approach to answering [[multi-dimensional analytical]] (MDA) queries swiftly in [[computing]], .<ref name=Codd1993>{{cite web\n  |url=http://www.sgpnyc.com/us/products/dataquest/whitepapers/OLAP_wp_efcodd.pdf\n  |title=Providing OLAP (On-line Analytical Processing) to User-Analysts: An IT Mandate\n  |publisher=Codd & Date, Inc\n  |author1=Codd E.F. |author2=Codd S.B. |author3=Salley C.T.  |last-author-amp=yes |year=1993\n  |accessdate=2008-03-05 }}</ref> OLAP is part of the broader category of [[business intelligence]], which also encompasses [[relational database]], report writing and [[data mining]].<ref>{{cite book\n  |url=https://books.google.com/books?id=M-UOE1Cp9OEC\n  |title=Business Intelligence for Telecommunications\n  |publisher=CRC Press\n  |author=Deepak Pareek\n  |year=2007\n  |pages=294 pp\n  |isbn=0-8493-8792-2\n  |accessdate=2008-03-18\n}}</ref>  Typical applications of OLAP include [[business reporting]] for sales, [[marketing]], management reporting, [[business process management]] (BPM),<ref>{{cite book\n  |url=http://www.google.com/products?q=9783639222166\n  |title=Business Process Management:A Data Cube To Analyze Business Process Simulation Data For Decision Making\n  |publisher=[[VDM Verlag|VDM Verlag Dr. Müller e.K.]]\n  |author=Apostolos Benisis\n  |year=2010\n  |pages=204 pp\n  |isbn=978-3-639-22216-6\n}}</ref> [[budget]]ing and [[forecasting|forecast]]ing, [[financial reporting]] and similar areas, with new applications coming up, such as [[agriculture]].<ref name=ahsan/> The term \'\'OLAP\'\' was created as a slight modification of the traditional database term [[online transaction processing]] (OLTP).<ref>{{cite web\n  |url=http://www.symcorp.com/downloads/OLAP_CouncilWhitePaper.pdf\n  |format=PDF|title=OLAP Council White Paper\n  |publisher=OLAP Council\n  |year=1997\n\n  |accessdate=2008-03-18\n}}</ref>\n\nOLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing.<ref>O\'Brien & Marakas, 2011, p. 402-403</ref> Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region\'s sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the [[OLAP cube]] and view (dicing) the slices from different viewpoints.  These viewpoints are sometimes called dimensions (such as looking at the same sales by salesperson or by date or by customer or by product or by region, etc.)\n\n[[Database]]s configured for OLAP use a multidimensional data model, allowing for complex analytical and [[ad hoc]] queries with a rapid execution time.<ref>{{cite web\n  |url=http://www.dwreview.com/OLAP/Introduction_OLAP.html\n  |title=Introduction to OLAP – Slice, Dice and Drill!\n  |publisher=Data Warehousing Review\n  |author=Hari Mailvaganam\n  |year=2007  |accessdate=2008-03-18\n}}</ref>  They borrow aspects of [[navigational database]]s, [[hierarchical database]]s and relational databases.\n\nOLAP is typically contrasted to [[Online transaction processing|OLTP]] (online transaction processing), which is generally characterized by much less complex queries, in a larger volume, to process transactions rather than for the purpose of business intelligence or reporting. Whereas OLAP systems are mostly optimized for read, OLTP has to processes all kinds of queries (read, insert, update and delete).\n\n== Overview of OLAP systems ==\nAt the core of any OLAP system is an [[OLAP cube]] (also called a \'multidimensional cube\' or a [[hypercube]]). It consists of numeric facts called \'\'measures\'\' that are categorized by \'\'[[Dimension (data warehouse)|dimensions]]\'\'. The measures are placed at the intersections of the hypercube, which is spanned by the dimensions as a [[vector space]]. The usual interface to manipulate an OLAP cube is a matrix interface, like [[Pivot table]]s in a spreadsheet program, which performs projection operations along the dimensions, such as aggregation or averaging.\n\nThe cube metadata is typically created from a [[star schema]] or [[snowflake schema]] or [[fact constellation]] of tables in a [[relational database]]. Measures are derived from the records in the [[fact table]] and dimensions are derived from the [[dimension table]]s.\n\nEach \'\'measure\'\' can be thought of as having a set of \'\'labels\'\', or meta-data associated with it. A \'\'dimension\'\' is what describes these \'\'labels\'\'; it provides information about the \'\'measure\'\'.\n\nA simple example would be a cube that contains a store\'s sales as a \'\'measure\'\', and Date/Time as a \'\'dimension\'\'. Each Sale has a Date/Time \'\'label\'\' that describes more about that sale.\n\nFor example:\n  Sale Fact Table\n +-------------+----------+\n | sale_amount | time_id  |\n +-------------+----------+            Time Dimension\n |      2008.10|     1234 |----+     +---------+-------------------+\n +-------------+----------+    |     | time_id | timestamp         |\n                               |     +---------+-------------------+\n                               +---->|   1234  | 20080902 12:35:43 |\n                                     +---------+-------------------+\n\n=== Multidimensional databases ===\nMultidimensional structure is defined as "a variation of the relational model that uses multidimensional structures to organize data and express the relationships between data".<ref>O\'Brien & Marakas, 2009, pg 177</ref>  The structure is broken into cubes and the cubes are able to store and access data within the confines of each cube. "Each cell within a multidimensional structure contains aggregated data related to elements along each of its dimensions".<ref>O\'Brien & Marakas, 2009, pg 178</ref>  Even when data is manipulated it remains easy to access and continues to constitute a compact database format.  The data still remains interrelated.\nMultidimensional structure is quite popular for analytical databases that use online analytical processing (OLAP) applications.<ref>(O\'Brien & Marakas, 2009)</ref>  Analytical databases use these databases because of their ability to deliver answers to complex business queries swiftly.  Data can be viewed from different angles, which gives a broader perspective of a problem unlike other models.<ref>Williams, C., Garza, V.R., Tucker, S, Marcus, A.M. (1994, January 24). Multidimensional models boost viewing options. InfoWorld, 16(4)</ref>\n\n=== Aggregations ===\nIt has been claimed that for complex queries OLAP cubes can produce an answer in around 0.1% of the time required for the same query on [[OLTP]] relational data.<ref>{{cite web\n  | author=MicroStrategy, Incorporated\n  | year=1995\n  | title=The Case for Relational OLAP\n  | url=http://www.cs.bgu.ac.il/~onap052/uploads/Seminar/Relational%20OLAP%20Microstrategy.pdf\n\n  |format=PDF| accessdate=2008-03-20\n}}</ref><ref>{{cite journal\n  |author1=Surajit Chaudhuri  |author2=Umeshwar Dayal\n   |lastauthoramp=yes | title = An overview of data warehousing and OLAP technology\n  | journal = SIGMOD Rec.\n  | publisher = [[Association for Computing Machinery|ACM]]\n  | volume = 26\n  | issue = 1\n  | year = 1997\n\n  | pages = 65\n  | url = http://doi.acm.org/10.1145/248603.248616\n  | doi = 10.1145/248603.248616\n\n  | accessdate=2008-03-20\n}}</ref>  The most important mechanism in OLAP which allows it to achieve such performance is the use of \'\'aggregations\'\'. Aggregations are built from the fact table by changing the granularity on specific dimensions and aggregating up data along these dimensions. The number of possible aggregations is determined by every possible combination of dimension granularities.\n\nThe combination of all possible aggregations and the base data contains the answers to every query which can be answered from the data.<ref>{{cite journal\n  | last1 = Gray | first1 = Jim\n  | author1-link = Jim Gray (computer scientist)\n  | last2 = Chaudhuri | first2 = Surajit\n  | last3 = Layman | first3 = Andrew\n  | last4 = Reichart | first4 = Don\n  | last5 = Venkatrao | first5 = Murali\n  | last6 = Pellow | first6 = Frank\n  | last7 = Pirahesh | first7 = Hamid\n  | title = Data Cube: {A} Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals\n  | journal = J. Data Mining and Knowledge Discovery\n  | volume = 1\n  | issue = 1\n  | pages = 29–53\n  | year = 1997\n  | url = http://citeseer.ist.psu.edu/gray97data.html\n\n  | accessdate=2008-03-20\n}}</ref>\n\nBecause usually there are many aggregations that can be calculated, often only a predetermined number are fully calculated; the remainder are solved on demand.  The problem of deciding which aggregations (views) to calculate is known as the view selection problem.  View selection can be constrained by the total size of the selected set of aggregations, the time to update them from changes in the base data, or both.  The objective of view selection is typically to minimize the average time to answer OLAP queries, although some studies also minimize the update time.  View selection is [[NP-Complete]]. Many approaches to the problem have been explored, including [[greedy algorithm]]s, randomized search, [[genetic algorithm]]s and [[A* search algorithm]].\n\n==Types==\nOLAP systems have been traditionally categorized using the following taxonomy.<ref name=Pendse2006>{{cite web|url=http://www.olapreport.com/Architectures.htm |title=OLAP architectures |publisher=OLAP Report |author=Nigel Pendse |date=2006-06-27 |accessdate=2008-03-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20080124155954/http://www.olapreport.com/Architectures.htm |archivedate=January 24, 2008 }}</ref>\n\n===Multidimensional OLAP (MOLAP)===\nMOLAP (multi-dimensional online analytical processing) is the classic form of OLAP and is sometimes referred to as just OLAP. MOLAP stores this data in an optimized multi-dimensional array storage, rather than in a relational database.\n\nSome MOLAP tools require the [[pre-computation]] and storage of derived data, such as consolidations – the operation known as processing. Such MOLAP tools generally utilize a pre-calculated data set referred to as a data cube. The data cube contains all the possible answers to a given range of questions. As a result, they  have a very fast response to queries. On the other hand, updating can take a long time depending on the degree of pre-computation. Pre-computation can also lead to what is known as data explosion.\n\nOther MOLAP tools, particularly those that implement the [[Functional Database Model|functional database model]] do not pre-compute derived data but make all calculations on demand other than those that were previously requested and stored in a cache.\n\n\'\'\'Advantages of MOLAP\'\'\'\n* Fast query performance due to optimized storage, multidimensional indexing and caching.\n* Smaller on-disk size of data compared to data stored in [[relational database]] due to compression techniques.\n* Automated computation of higher level aggregates of the data.\n* It is very compact for low dimension data sets.\n* Array models provide natural indexing.\n* Effective data extraction achieved through the pre-structuring of aggregated data.\n\n\'\'\'Disadvantages of MOLAP\'\'\'\n* Within some MOLAP Solutions the processing step (data load) can be quite lengthy, especially on large data volumes. This is usually remedied by doing only incremental processing, i.e., processing only the data which have changed (usually new data) instead of reprocessing the entire data set.\n* Some MOLAP methodologies introduce data redundancy.\n\n====Products====\nExamples of commercial products that use MOLAP are [[Cognos]] Powerplay, [[Oracle OLAP|Oracle Database OLAP Option]], [[MicroStrategy]], [[Microsoft Analysis Services]], [[Essbase]], [[Applix|TM1]], [[Jedox]], and [[icCube]].\n\n===Relational OLAP (ROLAP)===\n\'\'\'ROLAP\'\'\' works directly with relational databases and does not require pre-computation. The base data and the dimension tables are stored as relational tables and new tables are created to hold the aggregated information. It depends on a specialized schema design. This methodology relies on manipulating the data stored in the relational database to give the appearance of traditional OLAP\'s slicing and dicing functionality. In essence, each action of slicing and dicing is equivalent to adding a "WHERE" clause in the SQL statement. ROLAP tools do not use pre-calculated data cubes but instead pose the query to the standard relational database and its tables in order to bring back the data required to answer the question. ROLAP tools feature the ability to ask any question because the methodology does not limit to the contents of a cube.  ROLAP also has the ability to drill down to the lowest level of detail in the database.\n\nWhile ROLAP uses a relational database source, generally the database must be carefully designed for ROLAP use.  A database which was designed for [[OLTP]] will not function well as a ROLAP database.  Therefore, ROLAP still involves creating an additional copy of the data.  However, since it is a database, a variety of technologies can be used to populate the database.\n\n==== Advantages of ROLAP ====\n<!--Note to editors:\nPlease review the discussion page before making changes to the advantages or disadvantages. Thank you.\n-->\n\n* ROLAP is considered to be more scalable in handling large data volumes, especially models with [[Dimension (data warehouse)|dimensions]] with very high [[cardinality]] (i.e., millions of members).\n* With a variety of data loading tools available, and the ability to fine-tune the [[Extract, transform, load|ETL]] code to the particular data model, load times are generally much shorter than with the automated [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] loads.\n* The data are stored in a standard [[relational database]] and can be accessed by any [[SQL]] reporting tool (the tool does not have to be an OLAP tool).\n* ROLAP tools are better at handling \'\'non-aggregatable facts\'\' (e.g., textual descriptions).  [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools tend to suffer from slow performance when querying these elements.\n* By [[Decoupling (electronics)|decoupling]] the data storage from the multi-dimensional model, it is possible to successfully model data that would not otherwise fit into a strict dimensional model.\n* The ROLAP approach can leverage [[database]] authorization controls such as row-level security, whereby the query results are filtered depending on preset criteria applied, for example, to a given user or group of users ([[SQL]] WHERE clause).\n\n==== Disadvantages of ROLAP ====\n<!--Note to editors:\nPlease review the discussion page before making changes to the advantages or disadvantages. Thank you.\n-->\n\n* There is a consensus in the industry that ROLAP tools have slower performance than MOLAP tools. However, see the discussion below about ROLAP performance.\n* The loading of \'\'aggregate tables\'\' must be managed by custom [[Extract, transform, load|ETL]] code.  The ROLAP tools do not help with this task.  This means additional development time and more code to support.\n* When the step of creating aggregate tables is skipped, the query performance then suffers because the larger detailed tables must be queried. This can be partially remedied by adding additional aggregate tables, however it is still not practical to create aggregate tables for all combinations of dimensions/attributes.\n* ROLAP relies on the general purpose database for querying and caching, and therefore several special techniques employed by [[MOLAP]] tools are not available (such as special hierarchical indexing).  However, modern ROLAP tools take advantage of latest improvements in [[SQL]] language such as CUBE and ROLLUP operators, DB2 Cube Views, as well as other SQL OLAP extensions.  These SQL improvements can mitigate the benefits of the [[MOLAP]] tools.\n* Since ROLAP tools rely on [[SQL]] for all of the computations, they are not suitable when the model is heavy on calculations which don\'t translate well into [[SQL]]. Examples of such models include budgeting, allocations, financial reporting and other scenarios.\n\n==== Performance of ROLAP ====\n\nIn the OLAP industry ROLAP is usually perceived as being able to scale for large data volumes, but suffering from slower query performance as opposed to [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]]. The [http://www.olapreport.com/survey.htm OLAP Survey], the largest independent survey across all major OLAP products, being conducted for 6 years (2001 to 2006) have consistently found that companies using ROLAP report slower performance than those using MOLAP even when data volumes were taken into consideration.\n\nHowever, as with any survey there are a number of subtle issues that must be taken into account when interpreting the results.\n* The survey shows that ROLAP tools have 7 times more users than [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools within each company.  Systems with more users will tend to suffer more performance problems at peak usage times.\n* There is also a question about complexity of the model, measured both in number of dimensions and richness of calculations. The survey does not offer a good way to control for these variations in the data being analyzed.\n\n==== Downside of flexibility ====\n\nSome companies select ROLAP because they intend to re-use existing relational database tables—these tables will frequently not be optimally designed for OLAP use.  The superior flexibility of ROLAP tools allows this less than optimal design to work, but performance suffers.  [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools in contrast would force the data to be re-loaded into an optimal OLAP design.\n\n===Hybrid OLAP (HOLAP)===\nThe undesirable trade-off between additional [[Extract, transform, load|ETL]] cost and slow query performance has ensured that most commercial OLAP tools now use a "Hybrid OLAP" (HOLAP) approach, which allows the model designer to decide which portion of the data will be stored in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and which portion in ROLAP.\n\nThere is no clear agreement across the industry as to what constitutes "Hybrid OLAP", except that a database will divide data between relational and specialized storage.<ref name="ieee_cite">{{cite journal\n  | last1 = Bach Pedersen | first1 = Torben\n  | last2 = S. Jensen \n  | title = Multidimensional Database Technology\n  | journal = Distributed Systems Online\n  | volume = \n  | issue = \n  | issn = 0018-9162\n  | pages = 40–46\n  | publisher = [[IEEE]]\n  | location = \n  | date = December 2001\n  | url = http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00970558\n  | doi = \n  | id = \n  | accessdate = | first2 = Christian }}\n</ref> For example, for some vendors, a HOLAP database will use relational tables to hold the larger quantities of detailed data, and use specialized storage for at least some aspects of the smaller quantities of more-aggregate or less-detailed data. HOLAP addresses the shortcomings of [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and [[#Relational_OLAP_.28ROLAP.29|ROLAP]] by combining the capabilities of both approaches. HOLAP tools can utilize both pre-calculated cubes and relational data sources.\n\n==== Vertical partitioning ====\n\nIn this mode HOLAP stores \'\'aggregations\'\' in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] for fast query performance, and detailed data in [[#Relational_OLAP_.28ROLAP.29|ROLAP]] to optimize time of cube \'\'processing\'\'.\n\n==== Horizontal partitioning ====\n\nIn this mode HOLAP stores some slice of data, usually the more recent one (i.e. sliced by Time dimension) in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] for fast query performance, and older data in [[#Relational_OLAP_.28ROLAP.29|ROLAP]]. Moreover, we can store some dices in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and others in [[#Relational_OLAP_.28ROLAP.29|ROLAP]], leveraging the fact that in a large cuboid, there will be dense and sparse subregions.<ref>Owen Kaser and Daniel Lemire, [http://arxiv.org/abs/cs.DB/0702143 Attribute Value Reordering for Efficient Hybrid OLAP], Information Sciences, Volume 176, Issue 16, pages 2279-2438, 2006.</ref>\n\n==== Products ====\nThe first product to provide HOLAP storage was [[Holos]], but the technology also became available in other commercial products such as [[Microsoft Analysis Services]], [[Oracle OLAP|Oracle Database OLAP Option]], [[MicroStrategy]] and [[SAP AG]] BI Accelerator. The hybrid OLAP approach combines ROLAP and MOLAP technology, benefiting from the greater scalability of ROLAP and the faster computation of MOLAP. For example, a HOLAP server may allow large volumes of detail data to be stored in a relational database, while aggregations are kept in a separate MOLAP store. The Microsoft SQL Server 7.0 OLAP Services supports a hybrid OLAP server\n\n===Comparison===\nEach type has certain benefits, although there is disagreement about the specifics of the benefits between providers.\n\n* Some MOLAP implementations are prone to database explosion, a phenomenon causing vast amounts of storage space to be used by MOLAP databases when certain common conditions are met: high number of dimensions, pre-calculated results and sparse multidimensional data.\n* MOLAP generally delivers better performance due to specialized indexing and storage optimizations. MOLAP also needs less storage space compared to ROLAP because the specialized storage typically includes [[Data compression|compression]] techniques.<ref name="ieee_cite"/>\n* ROLAP is generally more scalable.<ref name="ieee_cite"/> However, large volume pre-processing is difficult to implement efficiently so it is frequently skipped.  ROLAP query performance can therefore suffer tremendously.\n* Since ROLAP relies more on the database to perform calculations, it has more limitations in the specialized functions it can use.\n* HOLAP encompasses a range of solutions that attempt to mix the best of ROLAP and MOLAP.  It can generally pre-process swiftly, scale well, and offer good function support.\n\n===Other types===\nThe following acronyms are also sometimes used, although they are not as widespread as the ones above:\n\n* \'\'\'WOLAP\'\'\' – Web-based OLAP\n* \'\'\'DOLAP\'\'\' – [[Desktop computer|Desktop]] OLAP\n* \'\'\'[[Rtolap|RTOLAP]]\'\'\' – Real-Time OLAP\n\n==APIs and query languages==\nUnlike [[relational databases]], which had SQL as the standard query language, and widespread [[Application programming interface|API]]s such as [[ODBC]], [[JDBC]] and [[OLEDB]], there was no such unification in the OLAP world for a long time. The first real standard API was [[OLE DB for OLAP]] specification from [[Microsoft]] which appeared in 1997 and introduced the [[Multidimensional Expressions|MDX]] query language. Several OLAP vendors – both server and client – adopted it. In 2001 Microsoft and [[Hyperion Solutions Corporation|Hyperion]] announced the [[XML for Analysis]] specification, which was endorsed by most of the OLAP vendors. Since this also used MDX as a query language, MDX became the de facto standard.<ref>{{cite web|url=http://www.olapreport.com/Comment_APIs.htm |title=Commentary: OLAP API wars |publisher=OLAP Report |author=Nigel Pendse |date=2007-08-23 |accessdate=2008-03-18 |deadurl=yes |archiveurl=https://web.archive.org/web/20080528220113/http://www.olapreport.com/Comment_APIs.htm |archivedate=May 28, 2008 }}</ref>\nSince September-2011 [[LINQ]] can be used to query [[Microsoft Analysis Services|SSAS]] OLAP cubes from Microsoft .NET.<ref>{{cite web|url=http://www.agiledesignllc.com/Products|title=SSAS Entity Framework Provider for LINQ to SSAS OLAP}}</ref>\n\n==Products==\n\n===History===\nThe first product that performed OLAP queries was \'\'Express,\'\' which was released in 1970 (and acquired by [[Oracle Corporation|Oracle]] in 1995 from Information Resources).<ref>{{cite web|title=The origins of today\'s OLAP products |url=http://olapreport.com/origins.htm |publisher=OLAP Report |date=2007-08-23 |author=Nigel Pendse |accessdate=November 27, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20071221044811/http://www.olapreport.com/origins.htm |archivedate=December 21, 2007 }}</ref> However, the term did not appear until 1993 when it was coined by [[Edgar F. Codd]], who has been described as "the father of the relational database". Codd\'s paper<ref name=Codd1993/> resulted from a short consulting assignment which Codd undertook for former Arbor Software (later [[Hyperion Solutions]], and in 2007 acquired by Oracle), as a sort of marketing coup.  The company had released its own OLAP product, \'\'[[Essbase]]\'\', a year earlier. As a result, Codd\'s "twelve laws of online analytical processing" were explicit in their reference to Essbase. There was some ensuing controversy and when Computerworld learned that Codd was paid by Arbor, it retracted the article.\nOLAP market experienced strong growth in late 90s with dozens of commercial products going into market. In 1998, Microsoft released its first OLAP Server – [[Microsoft Analysis Services]], which drove wide adoption of OLAP technology and moved it into mainstream.\n\n===Product comparison===\n{{Main|Comparison of OLAP Servers}}\n\n===OLAP Clients===\nOLAP clients include many spreadsheet programs like Excel, web application, sql,dashboard tools, etc.\n\n===Market structure===\nBelow is a list of top OLAP vendors in 2006, with figures in millions of [[US Dollar]]s.<ref>{{cite web\n  |url=http://www.olapreport.com/market.htm\n  |title=OLAP Market\n  |publisher=OLAP Report\n  |author=Nigel Pendse\n  |year=2006\n\n  |accessdate=2008-03-17\n}}</ref>\n{| class="wikitable sortable"\n|- bgcolor="#CCCCCC" align="center"\n! Vendor !! Global Revenue  !! Consolidated company\n|-\n| [[Microsoft Corporation]] || 1,806   || Microsoft\n|-\n| [[Hyperion Solutions Corporation]] || 1,077  || Oracle\n|-\n| [[Cognos]] || 735  || IBM\n|-\n| [[Business Objects (company)|Business Objects]] || 416 || SAP\n|-\n| [[MicroStrategy]] || 416 || MicroStrategy\n|-\n| [[SAP AG]] || 330 || SAP\n|-\n| Cartesis ([[SAP AG|SAP]]) || 210  || SAP\n|-\n| [[Applix]] || 205  || IBM\n|-\n| [[Infor]] || 199  || Infor\n|-\n| [[Oracle Corporation]] || 159 || Oracle\n|-\n| Others || 152  || Others\n|-\n| \'\'\'Total\'\'\' || \'\'\'5,700\'\'\'\n|}\n\n=== Open-source ===\n* [[Druid (open-source data store)]] is a popular [[open-source]] distributed data store for OLAP queries that is used at scale in production by various organizations.\n* [[Apache Kylin]] is a distributed data store for OLAP queries originally developed by eBay.\n* [[Cubes (OLAP server)]] is another light-weight [[open-source]] toolkit implementation of OLAP functionality in the [[Python (programming language)|Python programming language]] with built-in ROLAP.\n* [[Linkedin Pinot]] is used at LinkedIn to deliver scalable real time analytics with low latency.<ref>{{cite news |last= Yegulalp |first=Serdar |date=2015-06-11 |title= LinkedIn fills another SQL-on-Hadoop niche |url=http://www.infoworld.com/article/2934506/olap/linkedins-pinot-fills-another-sql-on-hadoop-niche.html |magazine=InfoWorld |access-date=2016-11-19}}</ref> It can ingest data from offline data sources (such as Hadoop and flat files) as well as online sources (such as Kafka). Pinot is designed to scale horizontally.\n\n== See also ==\n{{portal|Computer science}}\n* [[Comparison of OLAP Servers]]\n* [[Data warehouse]]\n* [[Online transaction processing]] (OLTP)\n* [[Business analytics]]\n* [[Predictive analytics]]\n* [[Data Mining]]\n* [[Thomsen Diagrams]]\n* [[Functional Database Model]]\n\n==Bibliography==\n* {{cite web\n  |url= http://www.daniel-lemire.com/OLAP/\n  |title= Data Warehousing and OLAP-A Research-Oriented Bibliography\n  |author= Daniel Lemire\n  |date= December 2007\n  }}\n\n* {{cite book\n  | title = OLAP Solutions: Building Multidimensional Information Systems, 2nd Edition\n  | publisher = John Wiley & Sons\n  | series =\n  | year = 1997\n  | isbn = 978-0-471-14931-6\n  | author = Erik Thomsen. }}\n\n* Ling Liu and Tamer M. Özsu (Eds.) (2009).  "[http://www.springer.com/computer/database+management+&+information+retrieval/book/978-0-387-49616-0 Encyclopedia of Database Systems], 4100 p.&nbsp;60 illus. ISBN 978-0-387-49616-0.\n* O\'Brien, J. A., & Marakas, G. M. (2009). Management information systems (9th ed.). Boston, MA: McGraw-Hill/Irwin.\n\n==References==\n{{Reflist|30em|refs=\n<ref name=ahsan>\n{{cite journal\n|last1=Abdullah\n|first1=Ahsan\n|title=Analysis of mealybug incidence on the cotton crop using ADSS-OLAP (Online Analytical Processing) tool\n|journal=Computers and Electronics in Agriculture |date=November 2009 |volume=69 |issue=1 |pages=59–72 |doi=10.1016/j.compag.2009.07.003\n}}\n</ref>\n}}\n\n{{Data warehouse}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Online Analytical Processing}}\n[[Category:Online analytical processing| ]]\n[[Category:Data management]]\n[[Category:Information technology management]]']
['Abstraction (software engineering)', '60491', '{{Refimprove|date=June 2011}}\n{{Quote box|quote=The essence of abstractions is preserving information that is relevant in a given context, and forgetting information that is irrelevant in that context.|source=– John V. Guttag<ref>{{Cite book | edition = Spring 2013 | publisher = The MIT Press | isbn = 9780262519632 | last = Guttag | first = John V. | title = Introduction to Computation and Programming Using Python | location = Cambridge, Massachusetts | date = 2013-01-18}}</ref>|width=25%}}\n\nIn [[software engineering]] and [[computer science]], \'\'\'abstraction\'\'\' is a technique for arranging complexity of computer systems. It works by establishing a level of complexity on which a person interacts with the system, suppressing the more complex details below the current level. The programmer works with an idealized interface (usually well defined) and can add additional levels of functionality that would otherwise be too complex to handle. For example, a programmer writing code that involves numerical operations may not be interested in the way numbers are represented in the underlying hardware (e.g. whether they\'re \'\'16 bit\'\' or \'\'32 bit [[Integer (computer science)|integers]]\'\'), and where those details have been suppressed it can be said that they were \'\'abstracted away\'\', leaving simply \'\'numbers\'\' with which the programmer can work. In addition, a task of sending an email message across continents would be extremely complex if the programmer had to start with a piece of fiber optic cable and basic hardware components. By using layers of complexity that have been created to abstract away the physical cables and network layout, and presenting the programmer with a virtual data channel, this task is manageable.\n\nAbstraction can apply to control or to data: \'\'\'Control abstraction\'\'\' is the abstraction of actions while \'\'\'data abstraction\'\'\' is that of [[data structures]].\n\n* Control abstraction involves the use of [[subroutine]]s and [[control flow]] abstractions\n* Data abstraction allows handling pieces of data in meaningful ways. For example, it is the basic motivation behind the [[datatype]].\n\nThe notion of an [[object (computer science)|object]] in object-oriented programming can be viewed as a way to combine abstractions of data and code.\n\nThe same abstract definition can be used as a common [[Interface (computer science)|interface]] for a family of objects with different implementations and behaviors but which share the same meaning. The [[Inheritance (computer science)|inheritance]] mechanism in object-oriented programming can be used to define an [[Class (computer science)#Abstract|abstract class]] as the common interface.\n\nThe recommendation that programmers use abstractions whenever suitable in order to avoid duplication (usually [[code duplication|of code]]) is known as the [[Abstraction principle (computer programming)|abstraction principle]]. The requirement that a programming language provide suitable abstractions is also called the abstraction principle.\n\n==Rationale==\nComputing mostly operates independently of the concrete world: The hardware implements a [[model of computation]] that is interchangeable with others. The software is structured in [[software architecture|architecture]]s to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. [[Greenspun\'s Tenth Rule]] is an [[aphorism]] on how such an architecture is both inevitable and complex.\n\nA central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. \'\'[[Modeling languages]]\'\' help in planning. \'\'[[Computer language]]s\'\' can be processed with a computer. An example of this abstraction process is the generational development of [[programming language]]s from the [[First-generation programming language|machine language]] to the [[Second-generation programming language|assembly language]] and the [[Third-generation programming language|high-level language]]. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in [[scripting language]]s and [[domain-specific programming language]]s.\n\nWithin a programming language, some features let the programmer create new abstractions. These include [[subroutine]]s, [[module (programming)|modules]], [[polymorphism (computer science)|polymorphism]], and [[software component]]s. Some other abstractions such as [[software design pattern]]s and [[software architecture#Architecture examples|architectural styles]] remain invisible to a [[translator (computing)|translator]] and operate only in the design of a system.\n\nSome abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer [[Joel Spolsky]] has criticised these efforts by claiming that all abstractions are \'\'[[leaky abstraction|leaky]]\'\' — that they can never completely hide the details below;<ref>{{cite web|last1=Spolsky|first1=Joel|title=The Law of Leaky Abstractions|url=http://www.joelonsoftware.com/articles/LeakyAbstractions.html}}</ref> however, this does not negate the usefulness of abstraction.\n\nSome abstractions are designed to interoperate with other abstractions - for example, a programming language may contain a [[foreign function interface]] for making calls to the lower-level language.\n\n==Language features==\n\n===Programming languages===\n{{Main|Programming language}}\n\nDifferent programming languages provide different types of abstraction, depending on the intended applications for the language. For example:\n\n* In [[object-oriented programming language]]s such as [[C++]], [[Object Pascal]], or [[Java (programming language)|Java]], the concept of \'\'\'abstraction\'\'\' has itself become a declarative statement – using the [[keyword (computer programming)|keyword]]s \'\'<code>virtual</code>\'\' (in [[C++]]) or \'\'<code>abstract</code>\'\'<ref name="Oracle Java abstract">{{cite web|title=Abstract Methods and Classes|url=http://docs.oracle.com/javase/tutorial/java/IandI/abstract.html|website=The Java™ Tutorials|publisher=Oracle|accessdate=4 September 2014}}</ref> and \'\'<code>interface</code>\'\'<ref name="Oracle Java interface">{{cite web|title=Using an Interface as a Type|url=http://docs.oracle.com/javase/tutorial/java/IandI/interfaceAsType.html|website=The Java™ Tutorials|publisher=Oracle|accessdate=4 September 2014}}</ref> (in [[Java (programming language)|Java]]). After such a declaration, it is the responsibility of the programmer to implement a [[Class (computer science)|class]] to instantiate the [[Object (computer science)|object]] of the declaration.\n* [[Functional programming language]]s commonly exhibit abstractions related to functions, such as [[lambda abstraction]]s (making a term into a function of some variable) and [[higher-order function]]s (parameters are functions). <!-- This has to be merged in the following sections. -->\n* Modern members of the Lisp programming language family such as [[Clojure]], [[Scheme (programming language)|Scheme]] and [[Common Lisp]] support [[Macro (computer science)#Syntactic macros|macro systems]] to allow syntactic abstraction. Other programming languages such as [[Scala (programming language)|Scala]] also have macros, or very similar [[metaprogramming]] features (for example, [[Haskell (programming language)|Haskell]] has [[Template Haskell]], and [[OCaml]] has [[MetaOCaml]]). These can allow a programmer to eliminate [[boilerplate code]], abstract away tedious function call sequences, implement new [[Control flow|control flow structures]], and implement [[Domain-specific language|Domain Specific Languages (DSLs)]], which allow domain-specific concepts to be expressed in concise and elegant ways. All of these, when used correctly, improve both the programmer\'s efficiency and the clarity of the code by making the intended purpose more explicit. A consequence of syntactic abstraction is also that any Lisp dialect and in fact almost any programming language can, in principle, be implemented in any modern Lisp with significantly reduced (but still non-trivial in some cases) effort when compared to "more traditional" programming languages such as [[Python (programming language)|Python]], [[C (programming language)|C]] or [[Java (programming language)|Java]].\n\n===Specification methods===\n{{Main|Formal specification}}\n\nAnalysts have developed various methods to formally specify software systems.  Some known methods include:\n\n* Abstract-model based method (VDM, Z);\n* Algebraic techniques (Larch, CLEAR, OBJ, ACT ONE, CASL);\n* Process-based techniques (LOTOS, SDL, Estelle);\n* Trace-based techniques (SPECIAL, TAM);\n* Knowledge-based techniques (Refine, Gist).\n\n===Specification languages===\n{{Main|Specification language}}\n\nSpecification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation. The [[Unified Modeling Language|UML]] specification language, for example, allows the definition of \'\'abstract\'\' classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.\n\n==Control abstraction==\n{{Main|Control flow}}\n\nProgramming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a [[Pascal (programming language)|Pascal]]-like fashion:\n\n:<code>a := (1 + 2) * 5</code>\n\nTo a human, this seems a fairly simple and obvious calculation (\'\'"one plus two is three, times five is fifteen"\'\'). However, the low-level steps necessary to carry out this evaluation, and return the value "15", and then assign that value to the variable "a", are actually quite subtle and complex. The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication). Finally, assigning the resulting value of "15" to the variable labeled "a", so that "a" can be used later, involves additional \'behind-the-scenes\' steps of looking up a variable\'s label and the resultant location in physical or virtual memory, storing the binary representation of "15" to that memory location, etc.\n\nWithout control abstraction, a programmer would need to specify \'\'all\'\' the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable. Such duplication of effort has two serious negative consequences:\n\n# it forces the programmer to constantly repeat fairly common tasks every time a similar operation is needed\n# it forces the programmer to program for the particular hardware and instruction set\n\n===Structured programming===\n{{Main|Structured programming}}\n\nStructured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with reduction of the complexity potential for side-effects.\n\nIn a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.\n\nIn a larger system, it may involve breaking down complex tasks into many different modules. Consider a system which handles payroll on ships and at shore offices:\n\n* The uppermost level may feature a menu of typical end-user operations.\n* Within that could be standalone executables or libraries for tasks such as signing on and off employees or printing checks.\n* Within each of those standalone components there could be many different source files, each containing the program code to handle a part of the problem, with only selected interfaces available to other parts of the program. A sign on program could have source files for each data entry screen and the database interface (which may itself be a standalone third party library or a statically linked set of library routines).\n*Either the database or the payroll application also has to initiate the process of exchanging data with between ship and shore, and that data transfer task will often contain many other components.\n\nThese layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.\n\n==Data abstraction==\n{{Main|Abstract data type}}\n\nData abstraction enforces a clear separation between the \'\'abstract\'\' properties of a [[data type]] and the \'\'concrete\'\' details of its implementation. The abstract properties are those that are visible to client code that makes use of the data type—the \'\'interface\'\' to the data type—while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.\n\nFor example, one could define an [[abstract data type]] called \'\'lookup table\'\' which uniquely associates \'\'keys\'\' with \'\'values\'\', and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a [[hash table]], a [[binary search tree]], or even a simple linear [[List (computing)|list]] of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.\n\nOf course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code. As one way to look at this: the interface forms a \'\'contract\'\' on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.\n\n<!-- This makes no sense to me. [[User:TakuyaMurata|Taku]] 07:13, 19 June 2005 (UTC) -->\nLanguages that implement data abstraction include [[Ada programming language|Ada]] and [[Modula-2]]. [[Object-oriented]] languages are commonly claimed{{By whom|date=March 2009}} to offer data abstraction; however, their [[Inheritance (computer science)|inheritance]] concept tends to put information in the interface that more properly belongs in the implementation; thus, changes to such information ends up impacting client code, leading directly to the [[Fragile binary interface problem]].\n\n==Abstraction in object oriented programming==\n{{Main|Object (computer science)}}\n\nIn [[object-oriented programming]] theory, \'\'\'abstraction\'\'\' involves the facility to define objects that represent abstract "actors" that can perform work, report on and change their state, and "communicate" with other objects in the system. The term [[Encapsulation (object-oriented programming)|encapsulation]] refers to the hiding of [[state (computer science)|state]] details, but extending the concept of \'\'data type\'\' from earlier programming languages to associate \'\'behavior\'\' most strongly with the data, and standardizing the way that different data types interact, is the beginning of \'\'\'abstraction\'\'\'.  When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called [[polymorphism (computer science)|polymorphism]]. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called [[Delegation (object-oriented programming)|delegation]] or [[Inheritance (computer science)|inheritance]].\n\nVarious object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of [[polymorphism (computer science)|polymorphism]] in object-oriented programming, which includes the substitution of one [[type in object-oriented programming|type]] for another in the same or similar role. Although not as generally supported, a [[configuration in object-oriented programming|configuration]] or image or package may predetermine a great many of these [[name binding|bindings]] at [[compile-time]], [[link-time]], or [[loadtime]]. This would leave only a minimum of such bindings to change at [[Run time (program lifecycle phase)|run-time]].\n\n[[Common Lisp Object System]] or [[Self (programming language)|Self]], for example, feature less of a class-instance distinction and more use of delegation for [[polymorphism in object-oriented programming|polymorphism]]. Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from [[Lisp programming language|Lisp]].\n\nC++ exemplifies another extreme: it relies heavily on [[generic programming|templates]] and [[method overloading|overloading]] and other static bindings at compile-time, which in turn has certain flexibility problems.\n\nAlthough these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code - all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.\n\nConsider for example a sample [[Java (programming language)|Java]] fragment to represent some common farm "animals" to a level of abstraction suitable to model simple aspects of their hunger and feeding. It defines an <code>Animal</code> class to represent both the state of the animal and its functions:\n\n<source lang=java>\npublic class Animal extends LivingThing\n{\n     private Location loc;\n     private double energyReserves;\n\n     public boolean isHungry() {\n         return energyReserves < 2.5;\n     }\n     public void eat(Food food) {\n         // Consume food\n         energyReserves += food.getCalories();\n     }\n     public void moveTo(Location location) {\n         // Move to new location\n         this.loc = location;\n     }\n}\n</source>\nWith the above definition, one could create objects of type <tt>Animal</tt> and call their methods like this:\n\n<source lang=java>\nthePig = new Animal();\ntheCow = new Animal();\nif (thePig.isHungry()) {\n    thePig.eat(tableScraps);\n}\nif (theCow.isHungry()) {\n    theCow.eat(grass);\n}\ntheCow.moveTo(theBarn);\n</source>\nIn the above example, the class \'\'<code>Animal</code>\'\' is an abstraction used in place of an actual animal, \'\'<code>LivingThing</code>\'\' is a further abstraction (in this case a generalisation) of \'\'<code>Animal</code>\'\'.\n\nIf one requires a more differentiated hierarchy of animals — to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives — that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.\n\nSuch an abstraction could remove the need for the application coder to specify the type of food, so s/he could concentrate instead on the feeding schedule. The two classes could be related using [[Inheritance (computer science)|inheritance]] or stand alone, and the programmer could define varying degrees of [[polymorphism (computer science)|polymorphism]] between the two types. These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others. A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or other means to achieve polymorphism. The class notation is simply a coder\'s convenience.\n\n===Object-oriented design===\n{{Main|Object-oriented design}}\n\nDecisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and [[domain analysis]]&mdash;actually determining the relevant relationships in the real world is the concern of [[object-oriented analysis]] or [[legacy analysis]].\n\nIn general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged&mdash;thus it is entirely under the control of the programmer, and we refer to abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.\n\n==Considerations==\nWhen discussing [[formal semantics of programming languages]], [[formal methods]] or [[abstract interpretation]], \'\'\'abstraction\'\'\' refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a \'\'\'concrete\'\'\' (more precise) model of execution.\n\nAbstraction may be \'\'\'exact\'\'\' or \'\'\'faithful\'\'\' with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if we wish to know what the result of the evaluation of a mathematical expression involving only integers +, -, ×, is worth [[modular arithmetic|modulo]] \'\'n\'\', we need only perform all operations modulo \'\'n\'\' (a familiar form of this abstraction is [[casting out nines]]).\n\nAbstractions, however, though not necessarily \'\'\'exact\'\'\', should be \'\'\'sound\'\'\'. That is, it should be possible to get sound answers from them&mdash;even though the abstraction may simply yield a result of [[undecidable problem|undecidability]]. For instance, we may abstract the students in a class by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person\'s age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer "I don\'t know".\n\nThe level of abstraction included in a programming language can influence its overall [[usability]]. The [[Cognitive dimensions]] framework includes the concept of \'\'abstraction gradient\'\' in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.\n\nAbstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially [[undecidable problem|undecidable]] (see [[Rice\'s theorem]]). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer "I don\'t know" to some questions).\n\nAbstraction is the core concept of [[abstract interpretation]]. [[Model checking]] generally takes place on abstract versions of the studied systems.\n\n==Levels of abstraction==\n{{Main|Abstraction layer}}\n\nComputer science commonly presents \'\'levels\'\' (or, less commonly, \'\'layers\'\') of abstraction, wherein each level represents a different model of the same information and processes, but uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.\n<ref>[[Luciano Floridi]], [http://www.cs.ox.ac.uk/activities/ieg/research_reports/ieg_rr221104.pdf \'\'Levellism and the Method of Abstraction\'\']\nIEG – Research Report 22.11.04</ref>\nEach relatively abstract, "higher" level builds on a relatively concrete, "lower" level, which tends to provide an increasingly "granular" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.\n\n===Database systems===\n{{Main|Database management system}}\n\nSince many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:\n\n[[Image:Data abstraction levels.png|thumb|Data abstraction levels of a database system]]\n\n\'\'\'Physical level:\'\'\' The lowest level of abstraction describes \'\'how\'\' a system actually stores data. The physical level describes complex low-level data structures in detail.\n\n\'\'\'Logical level:\'\'\' The next higher level of abstraction describes \'\'what\'\' data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This referred to as [[physical data independence]]. [[Database administrator]]s, who must decide what information to keep in a database, use the logical level of abstraction.\n\n\'\'\'View level:\'\'\' The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many [[view (database)|view]]s for the same database.\n\n===Layered architecture===\n{{Main|Abstraction layer}}\nThe ability to provide a [[design]] of different levels of abstraction can\n\n* simplify the design considerably\n* enable different role players to effectively work at various levels of abstraction\n* support the portability of [[software artifact]]s (model-based ideally)\n\n[[Systems design]] and [[Business process modeling|business process design]] can both use this. Some [[Software modeling|design processes]] specifically generate designs that contain various levels of abstraction.\n\nLayered architecture partitions the concerns of the application into stacked groups (layers).\nIt is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.\n\n==See also==\n* [[Abstraction principle (computer programming)]]\n* [[Abstraction inversion]] for an anti-pattern of one danger in abstraction\n* [[Abstract data type]] for an abstract description of a set of data\n* [[Algorithm]] for an abstract description of a computational procedure\n* [[Bracket abstraction]] for making a term into a function of a variable\n* [[Data modeling]] for structuring data independent of the processes that use it\n* [[Encapsulation (object-oriented programming)|Encapsulation]] for abstractions that hide implementation details\n* [[Greenspun\'s Tenth Rule]] for an aphorism about an (the?) optimum point in the space of abstractions\n* [[Higher-order function]] for abstraction where functions produce or consume other functions\n* [[Lambda abstraction]] for making a term into a function of some variable\n* [[List of abstractions (computer science)]]\n* [[Program refinement|Refinement]] for the opposite of abstraction in computing\n\n==References==\n{{Reflist}}\n{{FOLDOC}}\n\n==Further reading==\n{{refbegin}}\n* {{cite book|author1=Harold Abelson|author2=Gerald Jay Sussman|author3=Julie Sussman|title=Structure and Interpretation of Computer Programs|url=http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-10.html|accessdate=22 June 2012|edition=2|date=25 July 1996|publisher=MIT Press|isbn=978-0-262-01153-2}}\n* {{cite web|last=Spolsky|first=Joel|title=The Law of Leaky Abstractions|url=http://www.joelonsoftware.com/articles/LeakyAbstractions.html|work=Joel on Software|date=11 November 2002}}\n* [http://www.cs.cornell.edu/courses/cs211/2006sp/Lectures/L08-abstraction/08_abstraction.html Abstraction/information hiding] - CS211 course, Cornell University.\n* {{cite book|author=Eric S. Roberts|title=Programming Abstractions in C A Second Course in Computer Science|date=1997}}\n* {{cite web|last=Palermo|first=Jeffrey|title=The Onion Architecture|url=http://jeffreypalermo.com/blog/the-onion-architecture-part-1/|work=Jeffrey Palermo|date=29 July 2008}} \n{{refend}}\n\n==External links==\n* [https://sites.google.com/site/simulationarchitecture/ SimArch] example of layered architecture for distributed simulation systems.\n\n{{Use dmy dates|date=June 2011}}\n\n{{DEFAULTSORT:Abstraction (computer science)}}\n[[Category:Data management]]\n[[Category:Programming paradigms]]\n[[Category:Articles with example Java code]]\n[[Category:Abstraction]]']
['Vinelink.com', '42814178', '\'\'\'\'\'Vinelink.com\'\'\'\'\' (VINE) is a national website in the [[United States]] that allows victims of crime, and the general public, to track the movements of prisoners held by the various [[US states|states]] and [[Territories of the United States|territories]]. The first four letters in the websites name, "vine", are an acronym for "Victim Information and Notification Everyday". Vinelink.com displays information, based on the information provided by the various states\' departments of correction and other law enforcement agencies, on whether an inmate is in custody, has been released, has been granted parole or probation, or has escaped from custody. In some cases, the website will reveal whether a defendant has been granted parole or probation, but then subsequently violated conditions of their release and become a fugitive.<ref>{{cite web | url=http://www.correct.state.ak.us/probation-parole/vine|title=Automated Victim Notification System (VINE)|publisher=[[Alaska Department of Corrections]]|accessdate=2014-05-20}}</ref> Information provided on Vinelink.com represents [[metadata]], in that the website lists a defendants custody status; but does not list what the individual is charged with, their criminal history, or the amount of their bail, if applicable.\n\n[[Internet]] users accessing the Vinelink.com website choose from a map of states and provinces within the United States where they wish to perform a search for an inmate. The user may then search for an individual using the inmate or parolees name, or by entering the inmates specific department of corrections inmate number, if known. When the inmates custody status changes, users who have registered to be notified of such changes will be notified via email, phone or both.<ref>{{cite web | url=http://www.doj.state.or.us/victims/pages/vine.aspx|title=Victim Information and Notification Everyday (VINE)|publisher=[[Oregon Department of Justice]]|date=2013-02-03| accessdate=2014-05-20}}</ref> This information is currently released upon request, without the website requesting reasons for the users search or requiring payment, as [[public records]] available to the [[general public]].\n\nInmate information is available for most states, and for [[Puerto Rico]], on the website. The states of [[Arizona]], [[Georgia (U.S. state)|Georgia]], [[Massachusetts]], [[Montana]], [[New Hampshire]] and [[West Virginia]] provide very limited information on the site. The states of [[Kansas]], [[Maine]] and [[South Dakota]] do not participate in the VINE system.<ref>{{cite web | url=http://www.theledger.com/article/20130203/news/130209793| title=Who to Call: VINElink|publisher=[[The Ledger]]|date=2010| accessdate=2014-05-20}}</ref> The website does not provide data on prisoners detained by the [[United States federal government]].\n\n==References==\n==External links==\n* [http://www.vinelink.com official website] \n\n==See also==\nVisit [http://www.bop.gov/inmateloc/ the official Federal Bureau of Prisons website] to search for inmates being held by the [[United States Federal Bureau of Prisons]]\n\n[[Category:Data management]]\n[[Category:Government services web portals in the United States]]\n[[Category:Law enforcement websites]]\n[[Category:Metadata]]\n[[Category:Public records]]\n\n\n{{website-stub}}\n{{Crime-stub}}']
['Computer-aided software engineering', '627071', '[[File:Umbrello 1.png|320px|thumb|Example of a CASE tool.]]\n\'\'\'Computer-aided software engineering\'\'\' (\'\'\'CASE\'\'\') is the domain of software tools used to design and implement applications. CASE tools are similar to and were partly inspired by [[computer-aided design]] (CAD) tools used for designing hardware products.  CASE tools are used for developing high-quality, defect-free, and maintainable software.<ref>Kuhn, D.L (1989). "Selecting and effectively using a computer aided software engineering tool". Annual Westinghouse computer symposium; 6–7 Nov 1989; Pittsburgh, PA (U.S.); DOE Project.</ref> CASE software is often associated with methods for the development of [[information system]]s together with automated tools that can be used in the [[software development process]].<ref>P. Loucopoulos and V. Karakostas (1995). \'\'System Requirements Engineerinuality software which will perform effectively.</ref>\n\n== History ==\nThe Information System Design and Optimization System (ISDOS) project, started in 1968 at the University of Michigan, initiated a great deal of interest in the whole concept of using computer systems to help analysts in the very difficult process of analysing requirements and developing systems. Several papers by Daniel Teichroew fired a whole generation of enthusiasts with the potential of automated systems development. His Problem Statement Language / Problem Statement Analyzer (PSL/PSA) tool was a CASE tool although it predated the term.<ref>{{cite journal|last1=Teichroew|first1=Daniel|last2=Hershey|first2=Ernest Allen|title=PSL/PSA a computer-aided technique for structured documentation and analysis of information processing systems|journal=Proceeding ICSE \'76 Proceedings of the 2nd international conference on Software engineering|date=1976|url=http://dl.acm.org/citation.cfm?id=807641|publisher=IEEE Computer Society Press}}</ref>\n\nAnother major thread emerged as a logical extension to the [[data dictionary]] of a [[database]]. By extending the range of [[metadata]] held, the attributes of an application could be held within a dictionary and used at runtime. This "active dictionary" became the precursor to the more modern [[model-driven engineering]] capability. However, the active dictionary did not provide a graphical representation of any of the metadata. It was the linking of the concept of a dictionary holding analysts\' metadata, as derived from the use of an integrated set of techniques, together with the graphical representation of such data that gave rise to the earlier versions of CASE.<ref>{{cite book|last1=Coronel|first1=Carlos|last2=Morris|first2=Steven|title=Database Systems: Design, Implementation, & Management|date=February 4, 2014|publisher=Cengage Learning|isbn=1285196147|pages=695–700|url=https://books.google.com/books?id=QWLpAgAAQBAJ&pg=PA698&lpg=PA698&dq=case+tools+data+dictionary&source=bl&ots=eJt_GWYHGx&sig=MZEMesWkJrGdczKSEZ_6bnqdNAY&hl=en&sa=X&ei=HNF0VOvWDu3xigK5FQ&ved=0CFIQ6AEwCQ#v=onepage&q=case%20tools%20data%20dictionary&f=false|accessdate=25 November 2014}}</ref>\n\nThe term was originally coined by software company Nastec Corporation of Southfield, Michigan in 1982 with their original integrated graphics and text editor GraphiText, which also was the first microcomputer-based system to use hyperlinks to cross-reference text strings in documents&mdash;an early forerunner of today\'s web page link.  GraphiText\'s successor product, DesignAid, was the first microprocessor-based tool to logically and semantically evaluate software and system design diagrams and build a data dictionary.\n\nUnder the direction of [[Albert F. Case, Jr.]] vice president for product management and consulting, and Vaughn Frick, director of product management, the DesignAid product suite was expanded to support analysis of a wide range of [[Structured Analysis and Design Technique|structured analysis and design methodologies]], including those of [[Ed Yourdon]] and [[Tom DeMarco]], [[Chris Gane (computer scientist)|Chris Gane]] & [[Trish Sarson]], Ward-Mellor (real-time) SA/SD and [[Warnier-Orr]] (data driven).<ref>{{cite journal|last1=Case|first1=Albert|title=Computer-aided software engineering (CASE): technology for improving software development productivity|journal=ACM SIGMIS Database|date=Fall 1985|volume=17|issue=1|pages=35–43|url=http://dl.acm.org/citation.cfm?id=1040698}}</ref>\n\nThe next entrant into the market was Excelerator from Index Technology in Cambridge, Mass.  While DesignAid ran on Convergent Technologies and later Burroughs Ngen networked microcomputers, Index launched Excelerator on the IBM PC/AT platform. While, at the time of launch, and for several years, the IBM platform did not support networking or a centralized database as did the Convergent Technologies or Burroughs machines, the allure of IBM was strong, and Excelerator came to prominence. Hot on the heels of Excelerator were a rash of offerings from companies such as Knowledgeware (James Martin, [[Fran Tarkenton]] and Don Addington), Texas Instrument\'s [[Information Engineering Facility|IEF]] and [[Andersen Consulting|Andersen Consulting\'s]] FOUNDATION toolset (DESIGN/1, INSTALL/1, FCP).\n\nCASE tools were at their peak in the early 1990s.<ref>{{cite news|last1=Yourdon|first1=Ed|title=Can XP Projects Grow?|url=https://books.google.com/books?id=_faqtO2fEbkC&pg=PA28&lpg=PA28&dq=CASE+tools+most+interest+90%27s&source=bl&ots=9WNDAYPU89&sig=vC_s1JtRyOwcHcCvyDici5H9Z7w&hl=en&sa=X&ei=lNd0VPr1De2rjAK6o4D4DA&ved=0CDIQ6AEwAw#v=onepage&q=CASE%20tools%20most%20interest%2090%27s&f=false|accessdate=25 November 2014|publisher=Computerworld|date=Jul 23, 2001}}</ref>  At the time [[IBM]] had proposed AD/Cycle, which was an alliance of software vendors centered on IBM\'s [[Software repository]] using [[IBM DB2]] in [[Mainframe computer|mainframe]] and [[OS/2]]:\n\n:\'\'The application development tools can be from several sources: from IBM, from vendors, and from the customers themselves. IBM has entered into relationships with [[Bachman Information Systems]], Index Technology Corporation, and [[KnowledgeWare|Knowledgeware]] wherein selected products from these vendors will be marketed through an IBM complementary marketing program to provide offerings that will help to achieve complete life-cycle coverage\'\'.<ref name="ADC_SAaA">"AD/Cycle strategy and architecture", IBM Systems Journal, Vol 29, NO 2, 1990; p. 172.</ref>\n\nWith the decline of the mainframe, AD/Cycle and the Big CASE tools died off, opening the market for the mainstream CASE tools of today. Many of the leaders of the CASE market of the early 1990s ended up being purchased by [[Computer Associates]], including IEW, IEF, ADW, Cayenne, and Learmonth & Burchett Management Systems (LBMS).  The other trend that led to the evolution of CASE tools was the rise of object-oriented methods and tools. Most of the various tool vendors added some support for object-oriented methods and tools.  In addition new products arose that were designed from the bottom up to support the object-oriented approach. Andersen developed its project Eagle as an alternative to Foundation. Several of the thought leaders in object-oriented development each developed their own methodology and CASE tool set: Jacobsen, Rumbaugh, [[Grady Booch|Booch]], etc. Eventually, these diverse tool sets and methods were consolidated via standards led by the [[Object Management Group]] (OMG). The OMG\'s [[Unified Modelling Language]] (UML) is currently widely accepted as the industry standard for object-oriented modeling.\n\n== CASE software ==\nA. Fuggetta classified CASE software into 3 categories:<ref name="AF_93">{{cite journal\n| author      = Alfonso Fuggetta\n|date=December 1993\n| title       = A classification of CASE technology\n| journal     = Computer\n| volume      = 26\n| issue       = 12\n| pages       = 25–38\n| doi         = 10.1109/2.247645\n| url         = http://www2.computer.org/portal/web/csdl/abs/mags/co/1993/12/rz025abs.htm\n| accessdate  = 2009-03-14\n}}\n</ref>\n# \'\'Tools\'\' support specific tasks in the [[software life-cycle]].\n# \'\'Workbenches\'\' combine two or more tools focused on a specific part of the software life-cycle.\n# \'\'Environments\'\' combine two or more tools or workbenches and support the complete software life-cycle.\n\n=== Tools ===\nCASE tools supports specific tasks in the software development life-cycle. They can be divided into the following categories:\n# Business and Analysis modeling. Graphical modeling tools. E.g., E/R modeling, object modeling, etc.\n# Development. Design and construction phases of the life-cycle. Debugging environments. E.g., [[GNU Debugger]].\n# Verification and validation. Analyze code and specifications for correctness, performance, etc.  \n# Configuration management. Control the check-in and check-out of repository objects and files. E.g., [[Source Code Control System|SCCS]], CMS.\n# Metrics and measurement. Analyze code for complexity, modularity (e.g., no "go to\'s"), performance, etc. \n# Project management. Manage project plans, task assignments, scheduling.\nAnother common way to distinguish CASE tools is the distinction between Upper CASE and Lower CASE. Upper CASE Tools support business and analysis modeling. They support traditional diagrammatic languages such as [[ER diagram]]s, [[Data flow diagram]], [[Structure chart]]s, [[Decision Tree]]s, [[Decision table]]s, etc. Lower CASE Tools support development activities, such as physical design, debugging, construction, testing, component integration, maintenance, and reverse engineering. All other activities span the entire life-cycle and apply equally to upper and lower CASE.<ref>Software Engineering: Tools, Principles and Techniques by Sangeeta Sabharwal, Umesh Publications</ref>\n\n=== Workbenches ===\nWorkbenches integrate two or more CASE tools and support specific software-process activities. Hence they achieve:\n*a homogeneous and consistent interface (presentation integration).\n*seamless integration of tools and tool chains (control and data integration).\n\nAn example workbench is Microsoft\'s [[Visual Basic]] programming environment. It incorporates several development tools: a GUI builder, smart code editor, debugger, etc. Most commercial CASE products tended to be such workbenches that seamlessly integrated two or more tools. Workbenches also can be classified in the same manner as tools; as focusing on Analysis, Development, Verification, etc. as well as being focused on upper case, lower case, or processes such as configuration management that span the complete life-cycle.\n\n=== Environments ===\nAn environment is a collection of CASE tools or workbenches that attempts to support the complete software process. This contrasts with tools that focus on one specific task or a specific part of the life-cycle. CASE environments are classified by Fuggetta as follows:<ref name="AF_93" />\n#Toolkits. Loosely coupled collections of tools. These typically build on operating system workbenches such as the Unix Programmer\'s Workbench or the VMS VAX set. They typically perform integration via piping or some other basic mechanism to share data and pass control. The strength of easy integration is also one of the drawbacks. Simple passing of parameters via technologies such as shell scripting can\'t provide the kind of sophisticated integration that a common repository database can. \n#Fourth generation. These environments are also known as 4GL standing for fourth generation language environments due to the fact that the early environments were designed around specific languages such as Visual Basic. They were the first environments to provide deep integration of multiple tools. Typically these environments were focused on specific types of applications. For example, user-interface driven applications that did standard atomic transactions to a relational database. Examples are Informix 4GL, and Focus.\n#Language-centered. Environments based on a single often object-oriented language such as the Symbolics Lisp Genera environment or VisualWorks Smalltalk from Parcplace. In these environments all the operating system resources were objects in the object-oriented language. This provides powerful debugging and graphical opportunities but the code developed is mostly limited to the specific language. For this reason, these environments were mostly a niche within CASE. Their use was mostly for prototyping and R&D projects. A common core idea for these environments was the [[model-view-controller]] user interface that facilitated keeping multiple presentations of the same design consistent with the underlying model. The MVC architecture was adopted by the other types of CASE environments as well as many of the applications that were built with them. \n#Integrated. These environments are an example of what most IT people tend to think of first when they think of CASE. Environments such as IBM\'s AD/Cycle, Andersen Consulting\'s FOUNDATION, the ICL [[CADES]] system, and DEC Cohesion. These environments attempt to cover the complete life-cycle from analysis to maintenance and provide an integrated database repository for storing all artifacts of the software process. The integrated software repository was the defining feature for these kinds of tools. They provided multiple different design models as well as support for code in heterogenous languages. One of the main goals for these types of environments was "round trip engineering": being able to make changes at the design level and have those automatically be reflected in the code and vice versa. These environments were also typically associated with a particular methodology for software development. For example, the FOUNDATION CASE suite from Andersen was closely tied to the Andersen Method/1 methodology.\n#Process-centered. This is the most ambitious type of integration. These environments attempt to not just formally specify the analysis and design objects of the software process but the actual process itself and to use that formal process to control and guide software projects. Examples are East, Enterprise II, Process Wise, Process Weaver, and Arcadia. These environments were by definition tied to some methodology since the software process itself is part of the environment and can control many aspects of tool invocation.\n\nIn practice, the distinction between workbenches and environments was flexible. Visual Basic for example was a programming workbench but was also considered a 4GL environment by many. The features that distinguished workbenches from environments were deep integration via a shared repository or common language and some kind of methodology (integrated and process-centered environments) or domain (4GL) specificity.<ref name="AF_93" />\n\n== Major CASE Risk Factors ==\nSome of the most significant risk factors for organizations adopting CASE technology include: \n* Inadequate standardization. Organizations usually have to tailor and adopt methodologies and tools to their specific requirements. Doing so may require significant effort to integrate both divergent technologies as well as divergent methods. For example, before the adoption of the UML standard the diagram conventions and methods for designing object-oriented models were vastly different among followers of Jacobsen, Booch, Rumbaugh, \n* Unrealistic expectations. The proponents of CASE technology—especially vendors marketing expensive tool sets—often hype expectations that the new approach will be a silver bullet that solves all problems. In reality no such technology can do that and if organizations approach CASE with unrealistic expectations they will inevitably be disappointed. \n* Inadequate training. As with any new technology, CASE requires time to train people in how to use the tools and to get up to speed with them. CASE projects can fail if practitioners are not given adequate time for training or if the first project attempted with the new technology is itself highly mission critical and fraught with risk. \n* Inadequate process control. CASE provides significant new capabilities to utilize new types of tools in innovative ways. Without the proper process guidance and controls these new capabilities can cause significant new problems as well.<ref>[http://ithandbook.ffiec.gov/it-booklets/development-and-acquisition/development-procedures/software-development-techniques/computer-aided-software-engineering.aspx Computer Aided Software Engineering]. In: \'\'FFIEC IT Examination Handbook InfoBase\'\'. Retrieved 3 Mar 2012.</ref>\n\n== See also ==\n* [[Data modeling]]\n* [[Domain-specific modeling]]\n* [[Method engineering]]\n* [[Model-driven architecture]]\n* [[Modeling language]]\n* [[Rapid application development]]\n* [[Model-based architecture]]\n\n== References ==\n{{reflist}}\n\n{{Authority control}}\n\n[[Category:Computer-aided software engineering tools|*]]\n[[Category:Data management]]']
['AnalytiX DS', '43846669', '{{Use dmy dates|date=September 2016}}\n{{Use Indian English|date=September 2016}}\n{{Orphan|date=September 2014}}\n\n{{Infobox company\n| name             =AnalytiX DS\n| logo             =[[File:AnalytiX DS Logo.png]]\n| type             =[[Private limited company|Private]]\n| location_city    =[[Chantilly, Virginia|Chantilly]], [[Virginia]]\n| location_country =[[United States]]\n| founder          ={{Unbulleted list|Mike Boggs|}} \n| foundation       =2006 \n| area_served      =Worldwide\n| key_people       =Madan K. ([[CEO]])<br />Mike Boggs ([[Chief Technology Officer|CTO]] & Founder)<br/>\nSam Benedict (VP-Sales & Marketing)<br/> John Carter (Director of Professional Services)\n| industry         =[[Software Company]]\n| services         =IT, [[business consulting]] and automation services\n| homepage         ={{URL|www.analytixds.com}}\n| intl             =yes\n}}\n\'\'\' AnalytiX Data Services \'\'\'  is a software vendor that provides specialized data mapping and ETL conversion tools for [[data integration]], [[data management]], [[enterprise application integration]] and [[big data]] software and services.<ref>{{cite web|title=Mapping Manager, the missing link in moving data around|url=http://www.bloorresearch.com/analysis/analytix-mapping-manager-missing-link-moving-data|publisher=Bloor|accessdate=17 September 2014}}</ref>  Headquarter\'s based in Chantilly, Virginia, AnalytiX DS has offices in Dallas, TX and Hyderabad, India and an international network of technical and services partners.<ref>{{cite web|title=AnalytiX DS Partners|url=http://analytixds.com/technology-partners/}}</ref>\n\nMichael Boggs, the CTO and founder of AnalytiX DS coined the term "Pre ETL Mapping" now it is widely used and accepted synonym for design phase of data integration. \n\nMapping Manager is the flagship product of AnalytiX DS, an agile Unified Platform designed to govern and accelerate data integration processes by eliminating manual processes and replacing them with software designed to automate, govern and accelerate manual processes.<ref>{{cite web|title=Mapping Manager|url=http://analytixds.com/amm/}}</ref>  Several versions of Mapping Manager were released since the release of its first version in 2006. \n\nAnalytiX DS in April 2016 launched Mapping Manager Version 7.0, a major release version which extends the unified platform for enterprise data mapping, governance and automation.<ref>{{cite web|title=AnalytiX Data Services announces the launch of major release of Mapping Manager Version 7.0|url=http://www.pr.com/press-release/680901|publisher= PR |accessdate= 27 July 2016}}</ref>The latest release has added plenty of features and revolutionary modules never before seen.\n\nWith every new release, out of box features and functionalities were introduced. Later, AnalytiX DS began to add new modules including Release Management, Reference Data Manager, Code Set Manager, CATfX, LiteSpeed Conversion, Code automation Templates for Data Vault, Mapping Manager Big Data Edition, Data Quality Assessment Manager(DQAM), Metadata Management, Data Vault-Code Gen Bundle, and Test Manager, which extends the tools capabilities above and beyond management of the data mapping process.\n\n[[Mapping Manager Big Data Edition]]:<ref>{{cite web|title=Mapping Manager Big Data Edition|url=http://analytixds.com/mapping-manager-bigdata-edition/}}</ref>  Helps automate the big data mapping process and provides a bridge between structured and unstructured data to meet big data challenges.\n\n[[Release Management]]:<ref>{{cite web|title=AnalytiX DS Release Manager|url=http://analytixds.com/arm/}}</ref> Helps track the release process approvals, audits and verifications through the approval process.\n\n[[Reference Data Manager]]:<ref>{{cite web|title=AnalytiX DS Reference Data Manager|url=http://analytixds.com/rdm/}}</ref>  Helps create database like structure to maintain all reference data. \n\n[[Code Set Manager]]:<ref>{{cite web|title=AnalytiX DS CodeSet Manager|url=http://analytixds.com/code-set-manager/}}</ref>  Helps drive the organization of user defined reference data and Code Sets across an enterprise.\n\n[[Customizable Code-Automation Framework (CATfX)]]:<ref>{{cite web|title=AnalytiX DS Customizable Code-Automation Framework (CATfX)|url=http://analytixds.com/catfx-code-automation-templates/}}</ref>  Helps automate manual coding and tasks for ETL integration and data profiling, Testing Automation and more.\n\n[[LiteSpeed Conversion]]:<ref>{{cite web|title=AnalytiX DS LiteSpeed Conversion|url=http://analytixds.com/litespeed-conversion/}}</ref>  Helps automate the conversion of ETL tool platforms through an automated framework.\n\n[[Code automation Templates for Data Vault]]:<ref>{{cite web|title=AnalytiX DS Code automation Templates for Data Vault|url=http://analytixds.com/data-vault-automation/}}</ref> Helps automatically generate the Hub ETL code, Link ETL code and the Satellite ETL code through your existing ETL Platform. \n\n[[Data Quality Assessment Manager(DQAM)]]:<ref>{{cite web|title=AnalytiX DS Data Quality Assessment Manager(DQAM)|url=http://analytixds.com/data-quality-assessment-manager/}}</ref>  Helps standardize and execute a formal data quality assessment methodology. \n\n[[Metadata Management]]:<ref>{{cite web|title=AnalytiX DS Metadata Management|url=http://analytixds.com/metadata-management/}}</ref> Helps metadata in Mapping Manager Unified Platform to be ported into third party metadata environments. \n\n[[Data Vault-Code Gen Bundle]]:<ref>{{cite web|title=AnalytiX DS Data Vault-Code Gen Bundle|url=http://analytixds.com/data-vault-code-gen-bundle/}}</ref>  Helps automate the code-generation process for building the Data Vault through Code automation Templates (CAT\'s). \n\n[[Test Manager]]:<ref>{{cite web|title=AnalytiX DS Test Manager|url=http://analytixds.com/test-manager/}}</ref> Helps Test Cases and Test SQL generation to be managed in a purpose built module for testing data mappings and ETL processes. \n\nRecently, AnalytiX DS expanded its presence in the US with the opening of a new office in Dallas. \n\nAnalytiX DS has been named to CIO Review\'s 20 Most PROMISING PRODUCTIVITY TOOLS SOLUTION PROVIDERS 2015.<ref>{{cite web|title=CIO Review\'s 20 Most PROMISING PRODUCTIVITY TOOLS SOLUTION PROVIDERS 2015|url=http://www.pr.com/press-release/662862|publisher= PR |accessdate= 18 March 2016}}</ref>AnalytiX DS is a platinum sponsor for WWDVC 2016.<ref>{{cite web|title=AnalytiX Data Services To Platinum-Sponsor for WWDVC 2016|url=http://wwdvc.com/sponsors/}}</ref>\n\n\n== References ==\n{{reflist}}\n\n==External links==\n* {{official website|http://www.analytixds.com/}}\n* {{YouTube|u=AnalytiXDS|AnalytiX DS}}\n* {{Facebook|AnalytiX.Data.Services|AnalytiX DS}}\n\n[[Category:Software companies of India]]\n[[Category:Data management]]\n[[Category:Extract, transform, load tools]]\n[[Category:Data mapping]]\n[[Category:Data warehousing products]]\n[[Category:International information technology consulting firms]]\n[[Category:Multinational companies headquartered in the United States]]']
['Category:Extract, transform, load tools', '44363181', "'''Extract, transform, load tools''' are software packages that facilitate the performing of [[Extract, transform, load|ETL]] tasks.\n\n[[Category:Data management]]\n[[Category:Data warehousing products]]"]
['Semantic query', '44626050', '\'\'\'Semantic queries\'\'\' allow for queries and analytics of associative and contextual nature. Semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic, semantic and structural information contained in data. They are designed to deliver precise results (possibly the distinctive selection of one single piece of information) or to answer more fuzzy and wide open questions through [[pattern matching]] and [[Reasoning system|digital reasoning]].\n\nSemantic queries work on [[named graphs]], [[Linked Data|linked-data]] or [[Semantic triple|triples]]. This enables the query to process the actual relationships between information and \'\'infer\'\' the answers from the \'\'network of data\'\'. This is in contrast to [[semantic search]], which uses [[semantics]] (the science of meaning) in [[Unstructured data|unstructured text]] to produce a better search result (see [[natural language processing]]).\n\nFrom a technical point of view semantic queries are precise relational-type operations much like a [[SQL|database query]]. They work on structured data and therefore have the possibility to utilize comprehensive features like operators (e.g. >, < and =), namespaces, [[pattern matching]], [[Type inheritance|subclassing]], [[transitive relation]]s, [[Semantic Web Rule Language|semantic rules]] and contextual [[Full-text index|full text search]]. The [[semantic web]] technology stack of the [[W3C]] is offering [[SPARQL]]<ref name="XML.com">{{cite web|url=http://www.xml.com/pub/a/2005/11/16/introducing-sparql-querying-semantic-web-tutorial.html |title=Introducing SPARQL: Querying the Semantic Web |publisher=XML.com|date=2005}}</ref><ref name="W3C">{{cite web|url=http://www.w3.org/TR/rdf-sparql-query |title=SPARQL Query Language for RDF |publisher=W3C|date=2008}}</ref> to formulate semantic queries in a syntax similar to [[SQL]]. Semantic queries are used in [[triplestore]]s, [[graph databases]], [[semantic wiki]]s, natural language and artificial intelligence systems.\n\n== Background ==\n\n[[Relational database]]s contain all relationships between data in an \'\'implicit\'\' manner only.<ref name="ACM-DL">{{cite web|url=http://portal.acm.org/citation.cfm?id=1646157 |title=Semantic queries in databases: problems and challenges |publisher=ACM Digital Library|date=2009}}</ref><ref name="ESWC">{{cite web|url=http://2012.eswc-conferences.org/sites/default/files/eswc2012_submission_357.pdf |title=Karma: A System for Mapping Structured Sources into the Semantic Web |publisher=eswc-conferences.org|date=2012}}</ref> For example, the relationships between customers and products (stored in two content-tables and connected with an additional link-table) only come into existence in a query statement ([[SQL]] in the case of relational databases) written by a developer. Writing the query demands exact knowledge of the [[database schema]].<ref name="IEEE">{{cite web|url=http://www-scf.usc.edu/~taheriya/papers/taheriyan14-icsc-paper.pdf |title=A Scalable Approach to Learn Semantic Models of Structured Sources |publisher=8th IEEE International Conference on Semantic Computing|date=2014}}</ref><ref name="AAAI">{{cite web|url=http://www.isi.edu/integration/papers/knoblock13-sbd.pdf |title=Semantics for Big Data Integration and Analysis |publisher=AAAI Fall Symposium on Semantics for Big Data|date=2013}}</ref>\n\n[[Linked Data|Linked-Data]] contain all relationships between data in an \'\'explicit\'\' manner. In the above example no query code needs to be written. The correct product for each customer can be fetched automatically. Whereas this simple example is trivial, the real power of linked-data comes into play when a \'\'network of information\'\' is created (customers with their geo-spatial information like city, state and country; products with their categories within sub- and super-categories). Now the system can automatically answer more complex queries and analytics that look for the connection of a particular location with a product category. The development effort for this query is omitted. Executing a semantic query is conducted by \'\'walking\'\' the network of information and finding matches (also called \'\'Data Graph Traversal\'\').\n\nAnother important aspect of semantic queries is that the type of the relationship can be used to incorporate intelligence into the system. The relationship between a customer and a product has a fundamentally different nature than the relationship between a neighbourhood and its city. The latter enables the semantic query engine to \'\'infer\'\' that a customer \'\'living in Manhattan is also living in New York City\'\' whereas other relationships might have more complicated patterns and "contextual analytics". This process is called inference or reasoning and is the ability of the software to derive new information based on given facts.\n\n== Articles ==\n\n* {{Cite web\n| last =  Velez\n| first = Golda\n| year = 2008\n| url = http://www.wallstreetandtech.com/data-management/showArticle.jhtml?articleID=208700210&pgno=2\n| title = Semantics Help Wall Street Cope With Data Overload\n| publisher = wallstreetandtech.com\n}}\n* {{Cite web\n| last =  Zhifeng\n| first = Xiao\n| year = 2009\n| url = http://adsabs.harvard.edu/abs/2009SPIE.7492E..60X\n| title = Spatial information semantic query based on SPARQL\n| publisher = International Symposium on Spatial Analysis\n}}\n* {{Cite web\n| last = Aquin\n| first = Mathieu\n| year = 2010\n| url = http://www.semantic-web-journal.net/sites/default/files/swj96_1.pdf\n| title = Watson, more than a Semantic Web search engine\n| publisher = Semantic Web Journal\n}}\n* {{Cite web\n| last =  Prudhommeaux\n| first = Eric\n| year = 2010\n| url = http://www.cambridgesemantics.com/semantic-university/sparql-vs-sql-intro\n| title = SPARQL vs. SQL - Introduction\n| publisher = Cambridge Semantics\n}}\n* {{Cite web\n| last = Dworetzky\n| first = Tom\n| year = 2011\n| url = http://www.ibtimes.com/how-siri-works-iphones-brain-comes-natural-language-processing-stanford-professors-teach-free-online\n| title = How Siri Works: iPhone\'s \'Brain\' Comes from Natural Language Processing\n| publisher = International Business Times\n}}\n* {{Cite web\n| last =  Horwitt\n| first = Elisabeth\n| year = 2011\n| url = http://www.computerworld.com/s/article/9209118/The_semantic_Web_gets_down_to_businessarticleID=208700210&pgno=2\n| title = The semantic Web gets down to business\n| publisher = computerworld.com\n}}\n* {{Cite web\n| last = Rodriguez\n| first = Marko\n| year = 2011\n| url = http://markorodriguez.com/2011/06/15/graph-pattern-matching-with-gremlin-1-1/\n| title = Graph Pattern Matching with Gremlin\n| publisher = markorodriguez.com on Graph Computing\n}}\n* {{Cite web\n| last = Sequeda\n| first = Juan\n| year = 2011\n| url = http://www.cambridgesemantics.com/semantic-university/sparql-nuts-and-bolts\n| title = SPARQL Nuts & Bolts\n| publisher = Cambridge Semantics\n}}\n* {{Cite web\n| last = Freitas\n| first = Andre\n| year = 2012\n| url = https://www.deri.ie/sites/default/files/publications/freitas_ic_12.pdf\n| title = Querying Heterogeneous Datasets on the Linked Data Web\n| publisher = IEEE Internet Computing\n}}\n* {{Cite web\n| last = Kauppinen\n| first = Tomi\n| year = 2012\n| url = http://linkedscience.org/tools/sparql-package-for-r/tutorial-on-sparql-package-for-r/\n| title = Using the SPARQL Package in R to handle Spatial Linked Data\n| publisher = linkedscience.org\n}}\n* {{Cite web\n| last = Lorentz\n| first = Alissa\n| year = 2013\n| url = http://www.wired.com/2013/04/with-big-data-context-is-a-big-issue/\n| title = With Big Data, Context is a Big Issue\n| publisher = Wired\n}}\n\n== See also ==\n\n* [[Dataspaces]]\n* [[Knowledge Representation]]\n* [[Linked Data]]\n* [[Ontology alignment]]\n* [[Semantic Integration]]\n* [[Semantic publishing]]\n* [[Semantics of Business Vocabulary and Business Rules]]\n* [[SPARQL]]\n\n== References ==\n{{reflist}}\n\n==External links==\n* [http://www.w3.org/standards/semanticweb/query W3C Semantic Web Standards - Query]\n\n[[Category:Data management]]\n[[Category:Query languages]]\n[[Category:Semantic Web]]']
['Tuple', '132729', '{{About|the mathematical concept|the musical term|Tuplet}}\n{{Redir|Octuple|the type of rowing boat|Octuple scull}}\n{{Redir|Duodecuple|the term in music|Twelve-tone technique}}\n\nA \'\'\'tuple\'\'\' is a finite ordered list of [[Element (mathematics)|elements]].  In [[mathematics]], an \'\'\'{{math|\'\'n\'\'}}-tuple\'\'\' is a [[sequence]] (or ordered list) of {{math|\'\'n\'\'}} elements, where {{math|\'\'n\'\'}} is a non-negative integer. There is only one 0-tuple, an empty sequence. An {{math|\'\'n\'\'}}-tuple is [[Recursive definition|defined inductively]] using the construction of an [[ordered pair]]. Tuples are usually written by listing the elements within parentheses "<math>(\\text{ })</math>" and separated by commas; for example, <math>(2, 7, 4, 1, 7)</math> denotes a 5-tuple. Sometimes other symbols are used to surround the elements, such as square brackets "[ ]" or angle brackets "< >". Braces "{ }" are only used in defining arrays in some programming languages such as [[Java (programming language)|Java]], but not in mathematical expressions, as they are the standard notation for [[set (mathematics)|sets]]. Tuples are often used to describe other mathematical objects, such as [[Vector (mathematics and physics)|vectors]]. In computer science, tuples are directly implemented as [[product type]]s in most [[functional programming|functional programming languages]].{{citation needed|date=January 2016}} More commonly, they are implemented as [[Record (computer science)|record types]], where the components are labeled instead of being identified by position alone.{{citation needed|date=January 2016}} This approach is also used in [[relational algebra]]. Tuples are also used in relation to programming the [[semantic web]]  with the [[Resource Description Framework]] (RDF). Tuples are also used in [[linguistics]]<ref>{{cite web|url=http://www.oxfordreference.com/view/10.1093/acref/9780199202720.001.0001/acref-9780199202720-e-2276|title=N‐tuple - Oxford Reference|work=oxfordreference.com|accessdate=1 May 2015}}</ref> and [[philosophy]].<ref>{{cite web|url=http://www.oxfordreference.com/view/10.1093/acref/9780199541430.001.0001/acref-9780199541430-e-2262|title=Ordered n-tuple - Oxford Reference|work=oxfordreference.com|accessdate=1 May 2015}}</ref>\n\n==Etymology==\nThe term originated as an abstraction of the sequence: single, double, triple, quadruple, quintuple, sextuple, septuple, octuple, ..., {{math|\'\'n\'\'}}‑tuple, ..., where the prefixes are taken from the [[Latin]] names of the numerals. The unique 0‑tuple is called the null tuple. A 1‑tuple is called a singleton, a 2‑tuple is called an ordered pair and a 3‑tuple is a triple or triplet. {{math|\'\'n\'\'}} can be any nonnegative [[integer]]. For example, a [[complex number]] can be represented as a 2‑tuple, a [[quaternion]] can be represented as a 4‑tuple, an [[octonion]] can be represented as an 8‑tuple and a [[sedenion]] can be represented as a 16‑tuple.\n\nAlthough these uses treat \'\'‑tuple\'\' as the suffix, the original suffix was \'\'‑ple\'\' as in "triple" (three-fold) or "decuple" (ten‑fold). This originates from  [[medieval Latin]]  \'\'plus\'\' (meaning "more") related to [[Greek language|Greek]] ‑πλοῦς, which replaced the classical and late antique  \'\'‑plex\'\' (meaning "folded"), as in "duplex".<ref>\'\'OED\'\', \'\'s.v.\'\' "triple", "quadruple", "quintuple", "decuple"</ref>\n\n===Names for tuples of specific lengths===\n\n{| class="wikitable"\n|-\n! Tuple Length <math>n</math> !! Name !! Alternative names\n|-\n| align="right" | 0 || empty tuple || unit / empty sequence\n|-\n| align="right" | 1 || single || [[Singleton_(mathematics)|singleton]] / monuple \n|-\n| align="right" | 2 || double || couple / (ordered) pair / dual / twin / product\n|-\n| align="right" | 3 || triple || treble / triplet / triad\n|-\n| align="right" | 4 || quadruple || quad\n|-\n| align="right" | 5 || quintuple || pentuple\n|-\n| align="right" | 6 || sextuple ||hextuple\n|-\n| align="right" | 7 || septuple ||heptuple\n|-\n| align="right" | 8 || octuple || \n|-\n| align="right" | 9 || nonuple || \n|-\n| align="right" | 10 || decuple || \n|-\n| align="right" | 11 || undecuple || hendecuple\n|-\n| align="right" | 12 || duodecuple || \n|-\n| align="right" | 13 || tredecuple || \n|-\n| align="right" | 14 || quattuordecuple || \n|-\n| align="right" | 15 || quindecuple ||\n|-\n| align="right" | 16 || sexdecuple ||\n|-\n| align="right" | 17 || septendecuple ||\n|-\n| align="right" | 18 || octodecuple ||\n|-\n| align="right" | 19 || novemdecuple ||\n|-\n| align="right" | 20 || vigintuple ||\n|-\n| align="right" | 21 || unvigintuple ||\n|-\n| align="right" | 22 || duovigintuple ||\n|-\n| align="right" | 23 || trevigintuple ||\n|-\n| align="right" | 24 || quattuorvigintuple ||\n|-\n| align="right" | 25 || quinvigintuple ||\n|-\n| align="right" | 26 || sexvigintuple ||\n|-\n| align="right" | 27 || septenvigintuple ||\n|-\n| align="right" | 28 || octovigintuple ||\n|-\n| align="right" | 29 || novemvigintuple ||\n|-\n| align="right" | 30 || trigintuple ||\n|-\n| align="right" | 31 || untrigintuple ||\n|-\n| align="right" | 40 || quadragintuple ||\n|-\n| align="right" | 50 || quinquagintuple ||\n|-\n| align="right" | 60 || sexagintuple ||\n|-\n| align="right" | 70 || septuagintuple ||\n|-\n| align="right" | 80 || octogintuple ||\n|-\n| align="right" | 90 || nongentuple ||\n|-\n| align="right" | 100 || centuple ||\n|-\n| align="right" | 1,000 || milluple ||\n|-\n|}\n\n==Properties==\nThe general rule for the identity of two {{math|\'\'n\'\'}}-tuples is\n: <math>(a_1, a_2, \\ldots, a_n) = (b_1, b_2, \\ldots, b_n)</math> [[if and only if]] <math>a_1=b_1,\\text{ }a_2=b_2,\\text{ }\\ldots,\\text{ }a_n=b_n.</math>\n\nThus a tuple has properties that distinguish it from a [[Set (mathematics)|set]].\n# A tuple may contain multiple instances of the same element, so {{break|\n}}tuple <math>(1,2,2,3) \\neq (1,2,3)</math>; but set <math>\\{1,2,2,3\\} = \\{1,2,3\\}</math>.\n# Tuple elements are ordered: tuple <math>(1,2,3) \\neq (3,2,1)</math>, but set <math>\\{1,2,3\\} = \\{3,2,1\\}</math>.\n# A tuple has a finite number of elements, while a set or a [[multiset]] may have an infinite number of elements.\n\n==Definitions==\n\nThere are several definitions of tuples that give them the properties described in the previous section.\n\n===Tuples as functions===\nIf we are dealing with sets, an {{math|\'\'n\'\'}}-tuple can be regarded as a [[Function (mathematics)#Definition|function]], {{math|\'\'F\'\'}},  whose domain is the tuple\'s implicit set of element indices, {{math|\'\'X\'\'}}, and whose codomain, {{math|\'\'Y\'\'}}, is the tuple\'s set of elements. Formally:\n: <math>(a_1, a_2, \\dots, a_n) \\equiv (X,Y,F)</math>\nwhere:\n: <math>\n    \\begin{align}\n      X & = \\{1, 2, \\dots, n\\}                       \\\\\n      Y & = \\{a_1, a_2, \\ldots, a_n\\}                \\\\\n      F & = \\{(1, a_1), (2, a_2), \\ldots, (n, a_n)\\}. \\\\\n    \\end{align}\n  </math>\nIn slightly less formal notation this says:\n:<math> (a_1, a_2, \\dots, a_n) := (F(1), F(2), \\dots, F(n)).</math>\n\n===Tuples as nested ordered pairs===\nAnother way of modeling tuples in Set Theory is as nested [[ordered pair]]s. This approach assumes that the notion of ordered pair has already been defined; thus a 2-tuple \n# The 0-tuple (i.e. the empty tuple) is represented by the empty set <math>\\emptyset</math>.\n# An {{math|\'\'n\'\'}}-tuple, with {{math|\'\'n\'\' > 0}}, can be defined as an ordered pair of its first entry and an {{math|(\'\'n\'\' − 1)}}-tuple (which contains the remaining entries when {{math|\'\'n\'\' > 1)}}:\n#: <math>(a_1, a_2, a_3, \\ldots, a_n) = (a_1, (a_2, a_3, \\ldots, a_n))</math>\nThis definition can be applied recursively to the {{math|(\'\'n\'\' − 1)}}-tuple:\n: <math>(a_1, a_2, a_3, \\ldots, a_n) = (a_1, (a_2, (a_3, (\\ldots, (a_n, \\emptyset)\\ldots))))</math>\n\nThus, for example:\n: <math>\n    \\begin{align}\n         (1, 2, 3) & = (1, (2, (3, \\emptyset)))      \\\\\n      (1, 2, 3, 4) & = (1, (2, (3, (4, \\emptyset)))) \\\\\n    \\end{align}\n  </math>\n\nA variant of this definition starts "peeling off" elements from the other end:\n# The 0-tuple is the empty set <math>\\emptyset</math>.\n# For {{math|\'\'n\'\' > 0}}:\n#: <math>(a_1, a_2, a_3, \\ldots, a_n) = ((a_1, a_2, a_3, \\ldots, a_{n-1}), a_n)</math>\nThis definition can be applied recursively:\n: <math>(a_1, a_2, a_3, \\ldots, a_n) = ((\\ldots(((\\emptyset, a_1), a_2), a_3), \\ldots), a_n)</math>\n\nThus, for example:\n: <math>\n    \\begin{align}\n         (1, 2, 3) & = (((\\emptyset, 1), 2), 3)      \\\\\n      (1, 2, 3, 4) & = ((((\\emptyset, 1), 2), 3), 4) \\\\\n    \\end{align}\n  </math>\n\n===Tuples as nested sets===\nUsing [[ordered pair#Kuratowski definition|Kuratowski\'s representation for an ordered pair]], the second definition above can be reformulated in terms of pure [[set theory]]:\n# The 0-tuple (i.e. the empty tuple) is represented by the empty set <math>\\emptyset</math>;\n# Let <math>x</math> be an {{math|\'\'n\'\'}}-tuple <math>(a_1, a_2, \\ldots, a_n)</math>, and let <math>x \\rightarrow b \\equiv (a_1, a_2, \\ldots, a_n, b)</math>. Then, <math>x \\rightarrow b \\equiv \\{\\{x\\}, \\{x, b\\}\\}</math>.  (The right arrow, <math>\\rightarrow</math>, could be read as "adjoined with".)\n\nIn this formulation:\n: <math>\n   \\begin{array}{lclcl}\n     ()      & &                     &=& \\emptyset                                    \\\\\n             & &                     & &                                              \\\\\n     (1)     &=& ()    \\rightarrow 1 &=& \\{\\{()\\},\\{(),1\\}\\}                          \\\\\n             & &                     &=& \\{\\{\\emptyset\\},\\{\\emptyset,1\\}\\}            \\\\\n             & &                     & &                                              \\\\\n     (1,2)   &=& (1)   \\rightarrow 2 &=& \\{\\{(1)\\},\\{(1),2\\}\\}                        \\\\\n             & &                     &=& \\{\\{\\{\\{\\emptyset\\},\\{\\emptyset,1\\}\\}\\},     \\\\\n             & &                     & & \\{\\{\\{\\emptyset\\},\\{\\emptyset,1\\}\\},2\\}\\}    \\\\\n             & &                     & &                                              \\\\\n     (1,2,3) &=& (1,2) \\rightarrow 3 &=& \\{\\{(1,2)\\},\\{(1,2),3\\}\\}                    \\\\\n             & &                     &=& \\{\\{\\{\\{\\{\\{\\emptyset\\},\\{\\emptyset,1\\}\\}\\}, \\\\\n             & &                     & & \\{\\{\\{\\emptyset\\},\\{\\emptyset,1\\}\\},2\\}\\}\\}, \\\\\n             & &                     & & \\{\\{\\{\\{\\{\\emptyset\\},\\{\\emptyset,1\\}\\}\\},   \\\\\n             & &                     & & \\{\\{\\{\\emptyset\\},\\{\\emptyset,1\\}\\},2\\}\\},3\\}\\}                                       \\\\\n    \\end{array}\n  </math>\n\n=={{anchor|n-tuple}}{{math|\'\'n\'\'}}-tuples of {{math|\'\'m\'\'}}-sets ==\n\nIn [[discrete mathematics]], especially [[combinatorics]] and finite [[probability theory]], {{math|\'\'n\'\'}}-tuples arise in the context of various counting problems and are treated more informally as ordered lists of length {{math|\'\'n\'\'}}.<ref>{{harvnb|D\'Angelo|West|2000|p=9}}</ref> {{math|\'\'n\'\'}}-tuples whose entries come from a set of {{math|\'\'m\'\'}} elements are also called \'\'arrangements with repetition\'\', \'\'permutations of a multiset\'\' and, in some non-English literature, \'\'[[Variation (disambiguation)#Mathematics|variations]] with repetition\'\'. The number of {{math|\'\'n\'\'}}-tuples of an {{math|\'\'m\'\'}}-set is {{math|\'\'m\'\'<sup>\'\'n\'\'</sup>}}. This follows from the combinatorial [[rule of product]].<ref>{{harvnb|D\'Angelo|West|2000|p=101}}</ref> If {{math|\'\'S\'\'}} is a finite set of [[cardinality]] {{math|\'\'m\'\'}}, this number is the cardinality of the {{math|\'\'n\'\'}}-fold [[Cartesian_product#Cartesian_power | Cartesian power]] {{math|\'\'S\'\' × \'\'S\'\' × ... \'\'S\'\'}}. Tuples are elements of this product set.\n\n== Type theory ==\n{{main|Product type}}\nIn [[type theory]], commonly used in [[programming language]]s, a tuple has a [[product type]]; this fixes not only the length, but also the underlying types of each component. Formally:\n: <math>(x_1, x_2, \\ldots, x_n) : \\mathsf{T}_1 \\times \\mathsf{T}_2 \\times \\ldots \\times \\mathsf{T}_n</math>\nand the [[Projection (mathematics)|projection]]s are term constructors:\n: <math>\\pi_1(x) : \\mathsf{T}_1,~\\pi_2(x) : \\mathsf{T}_2,~\\ldots,~\\pi_n(x) : \\mathsf{T}_n</math>\n\nThe tuple with labeled elements used in the [[#Relational_model|relational model]] has a [[Record (computer science)|record type]]. Both of these types can be defined as simple extensions of the [[simply typed lambda calculus]].<ref name="pierce2002">{{cite book|last=Pierce|first=Benjamin|title=Types and Programming Languages|publisher=MIT Press|year=2002|isbn=0-262-16209-1|pages=126–132}}</ref>\n\nThe notion of a tuple in type theory and that in set theory are related in the following way: If we consider the natural [[model theory|model]] of a type theory, and use the Scott brackets to indicate the semantic interpretation<!-- do not link; that article needs to be a dab first-->, then the model consists of some sets <math>S_1, S_2, \\ldots, S_n</math> (note: the use of italics here that distinguishes sets from types) such that:\n: <math>[\\![\\mathsf{T}_1]\\!] = S_1,~[\\![\\mathsf{T}_2]\\!] = S_2,~\\ldots,~[\\![\\mathsf{T}_n]\\!] = S_n</math>\nand the interpretation of the basic terms is:\n: <math>[\\![x_1]\\!] \\in [\\![\\mathsf{T}_1]\\!],~[\\![x_2]\\!] \\in [\\![\\mathsf{T}_2]\\!],~\\ldots,~[\\![x_n]\\!] \\in [\\![\\mathsf{T}_n]\\!]</math>.\n\nThe {{math|\'\'n\'\'}}-tuple of type theory has the natural interpretation as an {{math|\'\'n\'\'}}-tuple of set theory:<ref>Steve Awodey, [http://www.andrew.cmu.edu/user/awodey/preprints/stcsFinal.pdf \'\'From sets, to types, to categories, to sets\'\'], 2009, [[preprint]]</ref>\n: <math>[\\![(x_1, x_2, \\ldots, x_n)]\\!] = (\\,[\\![x_1]\\!], [\\![x_2]\\!], \\ldots, [\\![x_n]\\!]\\,)</math>\nThe [[unit type]] has as semantic interpretation the 0-tuple.\n\n==See also==\n{{Wiktionary|tuple}}\n* [[Arity]]\n* [[Exponential object]]\n* [[Formal language]]\n* [[Multidimensional Expressions#MDX data types|OLAP: Multidimensional Expressions]]\n* [[Prime k-tuple]]\n* [[Relation (mathematics)]]\n* [[Tuplespace]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n\n{{refbegin}}\n* {{citation|first1=John P.|last1=D\'Angelo|first2=Douglas B.|last2=West|title=Mathematical Thinking/Problem-Solving and Proofs|year=2000|edition=2nd|publisher=Prentice-Hall|isbn=978-0-13-014412-6}}\n* [[Keith Devlin]], \'\'The Joy of Sets\'\'. Springer Verlag, 2nd ed., 1993, ISBN 0-387-94094-4, pp.&nbsp;7–8\n* [[Abraham Adolf Fraenkel]], [[Yehoshua Bar-Hillel]], [[Azriel Lévy]], \'\'[https://books.google.com/books?q=Foundations+of+set+theory&btnG=Search+Books Foundations of set theory]\'\', Elsevier Studies in Logic Vol. 67, Edition 2, revised, 1973, ISBN 0-7204-2270-1, p.&nbsp;33\n* [[Gaisi Takeuti]], W. M. Zaring, \'\'Introduction to Axiomatic Set Theory\'\', Springer [[Graduate texts in mathematics|GTM]] 1, 1971, ISBN 978-0-387-90024-7, p.&nbsp;14\n* George J. Tourlakis, \'\'[https://books.google.com/books?as_isbn=9780521753746 Lecture Notes in Logic and Set Theory. Volume 2: Set theory]\'\', Cambridge University Press, 2003, ISBN 978-0-521-75374-6, pp.&nbsp;182–193\n\n{{refend}}\n{{Set theory}}\n\n<!--Interwikies-->\n\n<!--Categories-->\n{{Authority control}}\n[[Category:Data management]]\n[[Category:Mathematical notation]]\n[[Category:Sequences and series]]\n[[Category:Basic concepts in set theory]]\n[[Category:Type theory]]\n[[ar:زوج مرتب]]']
['Data architect', '46362818', "{{expert-subject|Computer science|talk=Copyright issues and need for expert attention}}\nA ''data architect'' is a practitioner of [[data architecture]], an information technology discipline concerned with designing, creating, deploying and managing an organization's data architecture. Data architects define how the data will be stored, consumed, integrated and managed by different data entities and IT systems, as well as any applications using or processing that data in some way.<ref>{{cite web|title=Definition of Data Architect|url=http://stage.web.techopedia.com/definition/29452/data-architect|website=Techopedia}}</ref>  It is closely allied with [[business architecture]] and is considered to be one of the four domains of [[enterprise architecture]].\n\n==Role==\nAccording to the Data Management Body of Knowledge,<ref>{{cite web|title=Data Management Body of Knowledge|url=http://www.dama.org/content/body-knowledge|publisher=Data Management Association}}</ref> the data architect “provides a standard common business vocabulary, expresses strategic data requirements, outlines high level integrated designs to meet these requirements, and aligns with enterprise strategy and related business architecture.”\n\nAccording to the Open Group Architecture Framework (TOGAF), a data architect is expected to set data architecture principles, create models of data that enable the implementation of the intended business architecture, create diagrams showing key data entities, and create an inventory of the data needed to implement the architecture vision.<ref name=TOGAF>{{cite book|title=The Open Group Architectural Framework (TOGAF 9.1)|publisher=The Open Group|location=Chapter 10 - Data Architecture|url=http://pubs.opengroup.org/architecture/togaf9-doc/arch/chap10.html|accessdate=1 March 2015}}</ref>\n\n==Responsibilities==\n# Organizes data at the macro level (i.e. which subject areas are managed in which goldensources) \n# Organizes data at the micro level, data models, for a new application. \n# Provides a logical data model as a standard for the goldensource and for consuming applications to inherit. \n# Provides a logical data model with elements and business rules needed for the creation of DQ rules.\n\n==Skills==\nBob Lambert, Director of Data Architecture at consulting firm CapTech, describes the necessary skills of a Data Architect as follows:<ref>{{cite web|last1=Lambert|first1=Bob|title=Skills of a Data Architect|url=http://www.captechconsulting.com/blog/bob-lambert/skills-the-data-architect|website=Captech}}</ref>\n\n* Foundation in systems development: the data architect should understand the system development life cycle; software project management approaches; and requirements, design, and test techniques. The data architect is asked to conceptualize and influence application and interface projects, and therefore must understand what advice to give and where to plug in to steer toward desirable outcomes.\n* Depth in data modeling and database design: This is the core skill of the data architect, and the most requested in data architect job descriptions. The effective data architect is sound across all phases of data modeling, from conceptualization to database optimization. In his experience this skill extends to SQL development and perhaps database administration.\n* Breadth in established and emerging data technologies: In addition to depth in established data management and reporting technologies, the data architect is either experienced or conversant in emerging tools like columnar and NoSQL databases, predictive analytics, data visualization, and unstructured data. While not necessarily deep in all of these technologies, the data architect hopefully is experienced in one or more, and must understand them sufficiently to guide the organization in understanding and adopting them.\n* Ability to conceive and portray the big data picture: When the data architect initiates, evaluates, and influences projects he or she does so from the perspective of the entire organization. The data architect maps the systems and interfaces used to manage data, sets standards for data management, analyzes current state and conceives desired future state, and conceives projects needed to close the gap between current state and future goals.\n* Ability to astutely operate in the organization: Well respected and influential, Able to emphasize methodology, modeling, and governance, Technologically and politically neutral, Articulate, persuasive, and a good salesperson, and Enthusiastic\n\n==References==\n{{reflist}}\n\n==See also==\n* [[Data Architecture]]\n* [[Information Architect]]\n* [[Enterprise Architecture]]\n\n[[Category:Data management]]\n[[Category:Data modeling]]\n[[Category:Data security]]"]
['Comparison of CDMI server implementations', '40002906', '\n{| class="wikitable"\n|-\n! Implementation !! SNIA Reference Implementation !! CDMI-Serve !! CDMI-Proxy !! CDMI for OpenStack\'s Swift !! CDMI-z !! onedata\n|-\n| Version || [http://www.snia.org/forums/csi/programs/CDMIportal 1.0e] || [https://github.com/koenbollen/cdmi-serve 238c28fc7c] || [https://github.com/livenson/vcdm 0.1] || [https://github.com/osaddon/cdmi f0e3ad9bac] || 1 || [http://packages.onedata.org/oneprovider-Linux.rpm 2.0]\n|-\n| [[CDMI]] Version || 1.0.2 || ? || 1.0.1 || ? || 1.0.2 || 1.0.2\n|-\n| colspan="7" align="center" | \'\'\'HTTP features\'\'\'\n|-\n| [[HTTPS]] || ? || ? || {{Yes}} || ? || ? || {{Yes}}\n|-\n| [[Basic authentication]] || ? || ? || {{Yes}} || ? || ? || ?\n|-\n| [[Digest authentication]] || ? || ? || {{Yes}} || ? || ? || ?\n|-\n| [[X.509|X.509 authentication]] || ? || ? || ? || ? || ? || {{Yes}}\n|-\n| X.509-VOMS authentication || ? || ? || ? || ? || ? || {{Yes}}\n|-\n| Token based authentication || ? || ? || ? || ? || ? || {{Yes}}\n|-\n| colspan="7" align="center" | \'\'\'Data access methods\'\'\'\n|-\n| [[Filesystem in Userspace|FUSE]] || ? || ? || ? || ? || ? || {{Yes}}\n|-\n| [[GridFTP]] || ? || ? || ? || ? || ? || {{No}}\n|-\n| [[iSCSI]] || {{Yes}} || ? || ? || ? || ? || {{No}}\n|-\n| [[WebDAV]] || ? || ? || ? || ? || ? || {{No}}\n|-\n| [[Network File System|NFS]] || ? || ? || ? || ? || ? || {{No}}\n|-\n| [[Browser user interface|BUI]] || ? || ? || ? || ? || ? || {{Yes}}\n|-\n| colspan="7" align="center" | \'\'\'System-Wide CDMI Capabilities\'\'\'\n|-\n| cdmi_domains || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false}}\n|-\n| cdmi_export_cifs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_dataobjects || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}\n|-\n| cdmi_export_iscsi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_export_nfs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_export_occi_iscsi || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_export_webdav || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_metadata_maxitems || 1024 || ? || ? || ? || 4096 || 1024\n|-\n| cdmi_metadata_maxsize || 4096 || ? || ? || ? || 4096 || 4096\n|-\n| cdmi_metadata_maxtotalsize || ∞ || ? || ? || ? || 1048576 || 1048576\n|-\n| cdmi_notification || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_logging || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_query || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_query_regex || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_query_contains || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_query_tags || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_query_value || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_queues || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_security_access_control || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_security_audit || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_security_data_integrity || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_security_encryption || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_security_immutability || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_security_sanitization || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_serialization_json || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_snapshots || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_references || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_object_move_from_local || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_object_move_from_remote || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_object_move_from_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_object_move_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_object_copy_from_local || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_object_copy_from_remote || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_object_access_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_post_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_post_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_deserialize_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_deserialize_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_serialize_dataobject_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_serialize_domain_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_serialize_container_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_serialize_queue_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_copy_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_copy_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_create_reference_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| colspan="7" align="center" | \'\'\'Data Object Capabilities\'\'\'\n|-\n| cdmi_read_value || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_read_value_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_read_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_modify_value || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}|| {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_modify_value_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_modify_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_modify_deserialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_delete_dataobject || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}\n|-\n| cdmi_acl || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_size || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_ctime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_atime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_mtime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_acount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_mcount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_assignedsize || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_data_dispersion || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_retention || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_autodelete || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_holds || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_encryption || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}\n|-\n| cdmi_geographic_placement || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_immediate_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_infrastructure_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_latency || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_RPO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_RTO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_sanitization_method || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}\n|-\n| cdmi_throughput || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_value_hash || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}\n|-\n| colspan="7" align="center" | \'\'\'Container Capabilities\'\'\'\n|-\n| cdmi_list_children || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}\n|-\n| cdmi_list_children_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_read_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_modify_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_modify_deserialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_snapshot || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_serialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_serialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_serialize_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_serialize_domain || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_deserialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_deserialize_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_deserialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_create_dataobject || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}\n|-\n| cdmi_post_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_post_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_create_container || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}\n|-\n| cdmi_create_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_create_reference || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_export_container_cifs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_export_container_nfs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_export_container_iscsi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_export_container_occi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_export_container_webdav || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_delete_container || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}\n|-\n| cdmi_move_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_copy_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_move_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_copy_dataobject" || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_acl || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_size || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_ctime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_atime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_mtime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}\n|-\n| cdmi_acount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_mcount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_assignedsize || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_data_dispersion || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_retention || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_autodelete || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_holds || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_encryption || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}\n|-\n| cdmi_geographic_placement || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_immediate_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_infrastructure_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_latency || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_RPO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_RTO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_sanitization_method || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}\n|-\n| cdmi_throughput || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_value_hash || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}\n|-\n| colspan="7" align="center" | \'\'\'Domain Object Capabilities\'\'\'\n|-\n| cdmi_create_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_delete_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_domain_summary || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_domain_members || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_list_children || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_read_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_modify_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_modify_deserialize_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_copy_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_deserialize_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_acl || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_size || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_ctime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_atime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_mtime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_acount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_mcount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_assignedsize || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_data_dispersion || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_retention || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_autodelete || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_holds || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_encryption || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}\n|-\n| cdmi_geographic_placement || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_immediate_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_infrastructure_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_latency || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_RPO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_RTO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_sanitization_method || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}\n|-\n| cdmi_throughput || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_value_hash || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}\n|-\n| colspan="7" align="center" | \'\'\'Queue Object Capabilities\'\'\'\n|-\n| cdmi_read_value || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_read_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_modify_value || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_modify_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_modify_deserialize_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_delete_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}\n|-\n| cdmi_move_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_copy_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_reference_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_acl || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_size || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_ctime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_atime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_mtime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_acount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_mcount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_assignedsize || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_data_dispersion || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_retention || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_autodelete || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_data_holds || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_encryption || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}\n|-\n| cdmi_geographic_placement || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_immediate_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_infrastructure_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}\n|-\n| cdmi_latency || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_RPO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_RTO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_sanitization_method || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}\n|-\n| cdmi_throughput || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}\n|-\n| cdmi_value_hash || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}\n|}\n\n{{DEFAULTSORT:CDMI server implementation comparison}}\n[[Category:Cloud storage]]\n[[Category:Data management]]']
['Chunked transfer encoding', '7061159', '{{refimprove|date=June 2014}}\n\'\'\'Chunked transfer encoding\'\'\' is a data transfer mechanism in version 1.1 of the [[Hypertext Transfer Protocol]] (HTTP) in which data is sent in a series of "chunks". It uses the [[List of HTTP header fields#transfer-encoding-response-header|Transfer-Encoding]] HTTP header in place of the [[Content-Length]] header, which the earlier version of the protocol would otherwise require.<ref>http://tools.ietf.org/html/rfc1945#section-7.2</ref> Because the Content-Length header is not used, the sender does not need to know the length of the content before it starts transmitting a response to the receiver. Senders can begin transmitting dynamically-generated content before knowing the total size of that content.\n\nThe size of each chunk is sent right before the chunk itself so that the receiver can tell when it has finished receiving data for that chunk. The data transfer is terminated by a final chunk of length zero.\n\nAn early form of the chunked encoding was proposed in 1994.<ref>{{cite web|last=Connolly|first=Daniel|title=Content-Transfer-Encoding: packets for HTTP|url=http://1997.webhistory.org/www.lists/www-talk.1994q3/1147.html|accessdate=13 September 2013|date=27 Sep 1994|id=&lt;9409271503.AA27488@austin2.hal.com&gt;}}</ref> Later it was standardized in HTTP 1.1.\n\n==Rationale==\nThe introduction of chunked encoding provided various benefits:\n\n* Chunked transfer encoding allows a server to maintain an [[HTTP persistent connection]] for dynamically generated content. In this case, the HTTP Content-Length header cannot be used to delimit the content and the next HTTP request/response, as the content size is as yet unknown. Chunked encoding has the benefit that it is not necessary to generate the full content before writing the header, as it allows streaming of content as chunks and explicitly signaling the end of the content, making the connection available for the next HTTP request/response.\n* Chunked encoding allows the sender to send additional header fields after the message body. This is important in cases where values of a field cannot be known until the content has been produced, such as when the content of the message must be digitally signed. Without chunked encoding, the sender would have to buffer the content until it was complete in order to calculate a field value and send it before the content.\n\n==Applicability==\nFor version 1.1 of the HTTP protocol, the chunked transfer mechanism is considered to be always and anyways acceptable, even if not listed in the [[List of HTTP header fields#te-request-header|TE]] (transfer encoding) request header field, and when used with other transfer mechanisms, should always be applied last to the transferred data and never more than one time. This transfer coding method also allows additional entity header fields to be sent after the last chunk if the client specified the "trailers" parameter as an argument of the TE field. The origin server of the response can also decide to send additional entity trailers even if the client did not specify the "trailers" option in the TE request field, but only if the metadata is optional (i.e. the client can use the received entity without them). Whenever the trailers are used, the server should list their names in the Trailer header field; 3 header field types are specifically prohibited from appearing as a trailer field:  [[List of HTTP header fields#transfer-encoding-response-header|Transfer-Encoding]], [[List of HTTP header fields#content-length-response-header|Content-Length]] and [[List of HTTP header fields#trailer-response-header|Trailer]].\n\n==Format==\nIf a <tt>Transfer-Encoding</tt> field with a value of "<tt>chunked</tt>" is specified in an HTTP message (either a request sent by a client or the response from the server), the body of the message consists of an unspecified number of chunks, a terminating chunk, trailer, and a final CRLF sequence (i.e. [[carriage return]] followed by [[line feed]]).\n\nEach chunk starts with the number of [[Octet (computing)|octets]] of the data it embeds expressed as a [[hexadecimal]] number in [[ASCII]] followed by optional parameters (\'\'chunk extension\'\') and a terminating CRLF sequence, followed by the chunk data. The chunk is terminated by CRLF.\n\nIf chunk extensions are provided, the chunk size is terminated by a semicolon and followed by the parameters, each also delimited by semicolons. Each parameter is encoded as an extension name followed by an optional equal sign and value. These parameters could be used for a running [[message digest]] or [[digital signature]], or to indicate an estimated transfer progress, for instance.\n\nThe terminating chunk is a regular chunk, with the exception that its length is zero. It is followed by the trailer, which consists of a (possibly empty) sequence of entity header fields. Normally, such header fields would be sent in the message\'s header; however, it may be more efficient to determine them after processing the entire message entity. In that case, it is useful to send those headers in the trailer.\n\nHeader fields that regulate the use of trailers are \'\'TE\'\' (used in requests), and \'\'Trailers\'\' (used in responses).\n\n==Use with compression==\n\nHTTP servers often use [[data compression|compression]] to optimize transmission, for example with <tt>Content-Encoding: [[gzip]]</tt> or <tt>Content-Encoding: [[deflate]]</tt>. If both compression and chunked encoding are enabled, then the content stream is first compressed, then chunked; so the chunk encoding itself is not compressed, and the data in each chunk is not compressed individually. The remote endpoint then decodes the stream by concatenating the chunks and uncompressing the result.\n\n==Example==\n\n===Encoded data===\nIn the following example, three chunks of length 4, 5 and 14 are shown. The chunk size is transferred as a hexadecimal number followed by \\r\\n as a line separator, followed by a chunk of data of the given size.\n\n<pre>\n4\\r\\n\nWiki\\r\\n\n5\\r\\n\npedia\\r\\n\nE\\r\\n\n in\\r\\n\n\\r\\n\nchunks.\\r\\n\n0\\r\\n\n\\r\\n\n</pre>\n\nNote: the chunk size indicates the size of the chunk data and excludes the trailing CRLF ("\\r\\n").<ref>http://skrb.org/ietf/http_errata.html</ref> In this particular example, the CRLF following "in" is counted toward the chunk size of 0xE (14). The CRLF in its own line is also counted toward the chunk size.\nThe period character at the end of "chunks" is the 14th character, so it is the\nlast data character in that chunk. The CRLF following the period is\nthe trailing CRLF, so it is not counted toward the chunk size of 0xE (14).\n\n===Decoded data===\n<pre>\nWikipedia in\n\nchunks.\n</pre>\n\n==See also==\n* [[List of HTTP header fields]]\n\n==References==\n{{Reflist}}\n{{Refbegin}}\n* See [http://tools.ietf.org/html/rfc7230#section-4.1 RFC 7230 section 4.1] for further details of chunked encoding.\n* The previous (obsoleted) version is at [https://tools.ietf.org/html/rfc2616#section-3.6.1 RFC 2616 section 3.6.1].\n{{Refend}}\n\n{{DEFAULTSORT:Chunked Transfer Encoding}}\n[[Category:Data management]]\n[[Category:Hypertext Transfer Protocol]]\n[[Category:Hypertext Transfer Protocol headers]]']
['Skyline operator', '46213978', 'The \'\'\'Skyline operator\'\'\' is used in a query and performs a filtering of results from a database so that it keeps only those objects that are not worse than any other.\n\nThis operator is an extension to [[SQL]] proposed by Börzsönyi et al.<ref name=borzsony2001skyline>{{cite journal|last1=Borzsonyi|first1=Stephan|last2=Kossmann|first2=Donald|last3=Stocker|first3=Konrad|title=The Skyline Operator|journal=Proceedings 17th International Conference on Data Engineering|date=2001|pages=421–430|doi=10.1109/ICDE.2001.914855}}</ref>  A classic example of application of the Skyline operator involves selecting a hotel for a holiday. The user wants the hotel to be both cheap and close to the beach. However, hotels that are close to the beach may also be expensive. In this case, the Skyline operator would only present those hotels that are not worse than any other hotel in both price and distance to the beach.\n\n== Proposed syntax ==\n\nTo give an example in SQL: Börzsönyi et al.<ref name=borzsony2001skyline/> proposed the following syntax for the Skyline operator:\n\n<source lang="sql">\nSELECT ... FROM ... WHERE ...\nGROUP BY ... HAVING ...\nSKYLINE OF [DISTINCT] d1 [MIN | MAX | DIFF],\n                 ..., dm [MIN | MAX | DIFF]\nORDER BY ...\n</source>\nwhere d<sub>1</sub>, ... d<sub>m</sub> denote the dimensions of the Skyline and MIN, MAX and DIFF specify whether the value in that dimension should be minimised, maximised or simply be different.\n\n== Implementation ==\nThe Skyline operator can be implemented directly in SQL using current SQL constructs, however this has been shown to be very slow.<ref name=borzsony2001skyline/> Other algorithms have been proposed that make use of divide and conquer, indices,<ref name=borzsony2001skyline/> [[MapReduce]]<ref>{{cite journal|last1=Mullesgaard|first1=Kasper|last2=Pedersen|first2=Jens Laurits|last3=Lu|first3=Hua|last4=Zhou|first4=Yongluan|title=Efficient Skyline Computation in MapReduce|journal=Proc. 17th International Conference on Extending Database Technology (EDBT)|date=2014|pages=37–48|url=http://www.openproceedings.eu/2014/conf/edbt/MullesgaardPLZ14.pdf}}</ref> and [[General-purpose computing on graphics processing units|general-purpose computing on graphics cards]].<ref>{{cite journal|last1=Bøgh|first1=Kenneth S|last2=Assent|first2=Ira|last3=Magnani|first3=Matteo|title=Efficient GPU-based skyline computation|journal=Proceedings of the Ninth International Workshop on Data Management on New Hardware|date=2013|pages=5:1–5:6|doi=10.1145/2485278.2485283}}</ref> Skyline queries on data streams (i.e. continuous skyline queries) have been studied in the context of parallel query processing on multicores, owing to their wide diffusion in real-time decision making problems and data streaming analytics.<ref>{{cite journal|last1=De Matteis|first1=Tiziano|last2=Di Girolamo|first2=Salvatore|last3=Mencagli|first3=Gabriele|title=Continuous skyline queries on multicore architectures|journal=Concurrency and Computation: Practice and Experience|date=25 August 2016|volume=28|issue=12|pages=3503–3522|doi=10.1002/cpe.3866}}</ref>\n\n==References==\n<references />\n\n[[Category:Data management]]\n[[Category:Query languages]]\n[[Category:Relational database management systems]]\n[[Category:SQL]]\n\n\n{{database-software-stub}}']
['Astroinformatics', '28326718', '\'\'\'Astroinformatics\'\'\' is an interdisciplinary field of study involving the combination of [[astronomy]], [[data science]], [[informatics]], and [[Information technology|information]]/[[Communications technologies|communications]] technologies.<ref name="astroinfo" /><ref name=pdf>[http://www.math.bas.bg/~nkirov/zip/SEEDI_astro_presentation.pdf Astroinformatics and digitization of astronomical heritage], Nikolay Kirov. The fifth SEEDI International Conference Digitization of cultural and scientific heritage, May 19–20, 2010, Sarajevo. Retrieved 1 November 2012.</ref>\n\n==Background==\n\nAstroinformatics is primarily focused on developing the tools, methods, and applications of [[computational science]], [[data science]], and [[statistics]] for research and education in data-oriented astronomy.<ref name="astroinfo">{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics: Data-Oriented Astronomy Research and Education|url=http://link.springer.com/article/10.1007%2Fs12145-010-0055-2|website=Journal of Earth Science Informatics, June 2010, Volume 3, Issue 1, pp 5-17|publisher=Springer Link, Netherlands|accessdate=11 January 2016}}</ref> Early efforts in this direction included [[data discovery]], [[metadata standards]] development, [[data modeling]], astronomical [[data dictionary]] development, [[data access]], [[information retrieval]],<ref>{{cite arXiv|last1=Borne|first1=Kirk|title=Science User Scenarios for a Virtual Observatory Design Reference Mission: Science Requirements for Data Mining|arxiv=astro-ph/0008307}}</ref> [[data integration]], and [[data mining]]<ref>{{cite web|last1=Borne|first1=Kirk|title=Scientific Data Mining in Astronomy|url=https://www.crcpress.com/Next-Generation-of-Data-Mining/Kargupta-Han-Yu-Motwani-Kumar/9781420085860|website=CRC Press, pp. 91-114|publisher=Taylor & Francis Group|accessdate=11 January 2016}}</ref> in the astronomical [[Virtual Observatory]] initiatives.<ref>{{cite web|last1=Borne|first1=Kirk|title=Distributed Data Mining in the National Virtual Observatory|url=http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=764620|website=SPIE Digital Library|publisher=SPIE|accessdate=11 January 2016}}</ref><ref name="VOdm" /><ref>{{cite web|last1=Laurino|first1=O.|title=Astroinformatics of galaxies and quasars: a new general method for photometric redshifts estimation|url=http://mnras.oxfordjournals.org/content/418/4/2165|website=Monthly Notices of the Royal Astronomical Society, vol.418, pp. 2165-2195|publisher=Oxford Journals|accessdate=12 January 2016|display-authors=etal}}</ref> Further development of the field, along with astronomy community endorsement, was presented to the [[National Research Council (United States)]] in 2009 in the Astroinformatics "State of the Profession" Position Paper for the 2010 [[Astronomy and Astrophysics Decadal Survey]].<ref>{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics: A 21st Century Approach to Astronomy|url=http://adsabs.harvard.edu/abs/2009astro2010P...6B|website=Astrophysics Data System|publisher=SAO/NASA|accessdate=11 January 2016}}</ref> That position paper provided the basis for the subsequent more detailed exposition of the field in the Informatics Journal paper \'\'\'Astroinformatics: Data-Oriented Astronomy Research and Education\'\'\'.<ref name="astroinfo" />\n\nAstroinformatics as a distinct field of research was inspired by work in the fields of [[Bioinformatics]] and [[Geoinformatics]], and through the [[eScience]] work<ref>{{cite web|title=\'Online Science\'|url=http://research.microsoft.com/en-us/um/people/gray/JimGrayTalks.htm|website=Talks by Jim Gray|publisher=Microsoft Research|accessdate=11 January 2015}}</ref> of [[Jim Gray (computer scientist)]] at [[Microsoft Research]], whose legacy was remembered and continued through the Jim Gray eScience Awards.<ref>{{cite web|title=Jim Gray eScience Award|url=http://research.microsoft.com/en-us/collaboration/focus/escience/jim-gray-award.aspx|website=Microsoft Research}}</ref>\n\nThough the primary focus of Astroinformatics is on the large worldwide distributed collection of digital astronomical databases, image archives, and research tools, the field recognizes the importance of legacy data sets as well—using modern technologies to preserve and analyze historical astronomical observations. Some Astroinformatics practitioners help to [[Digital data|digitize]] historical and recent astronomical observations and images in a large [[database]] for efficient retrieval through [[World wide web|web]]-based interfaces.<ref name=pdf/><ref>[http://www.casca.ca/lrp2010/Docs/LRPReports/astroinformatics_lrp.pdf Astroinformatics in Canada], Nicholas M. Ball, David Schade. Retrieved 1 November 2012.</ref> Another aim is to help develop new methods and software for astronomers, as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy.<ref>{{cite web|title=\'Astroinformatics\' helps Astronomers explore the sky|url=http://phys.org/news/2013-10-astroinformatics-astronomers-exploring-sky.html|website=Phys.org|publisher=Heidelberg University|accessdate=11 January 2015}}</ref>\n\nAstroinformatics is described as the \'\'\'Fourth Paradigm\'\'\' of astronomical research.<ref>{{cite web|title=The Fourth Paradigm: Data-Intensive Scientific Discovery|url=https://www.microsoft.com/en-us/research/publication/fourth-paradigm-data-intensive-scientific-discovery/|website=Microsoft Research}}</ref> There are many research areas involved with astroinformatics, such as data mining, machine learning, statistics, visualization, scientific data management, and semantic science.<ref name="VOdm">{{cite web|last1=Borne|first1=Kirk|title=Virtual Observations, Data Mining, and Astroinformatics|url=http://link.springer.com/referenceworkentry/10.1007/978-94-007-5618-2_9|website=Planets, Stars and Stellar Systems, Volume 2: Astronomical Techniques, Software, and Data, pp.403-443|publisher=Springer Link, Netherlands|accessdate=11 January 2015}}</ref> [[Data mining]] and [[machine learning]] play significant roles in Astroinformatics as a [[Scientific method|scientific research]] discipline due to their focus on "knowledge discovery from data" ([[data mining|KDD]]) and "learning from data".<ref>{{cite web|last1=Ball|first1=N.M.|last2=Brunner|first2=R.J.|title=Data Mining and Machine Learrning in Astronomy|url=http://www.worldscientific.com/doi/abs/10.1142/S0218271810017160|website=International Journal of Modern Physics D|publisher=World Scientific Publishing|accessdate=12 January 2016}}</ref><ref>{{cite web|last1=Borne|first1=Kirk|title=The LSST Data Mining Research Agenda|url=http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.3059074|website=Classification and Discovery in Large Astronomical Surveys, pp.347-351|publisher=American Institute of Physics|accessdate=12 January 2016}}</ref>\n\nThe amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the [[Large Synoptic Survey Telescope]] and into the exabytes with the [[Square Kilometre Array]].<ref>{{cite web|last1=Ivezić|first1=Ž.|title=Parametrization and Classification of 20 Billion LSST Objects|url=http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.3059076|website=Classification and Discovery in Large Astronomical Surveys, pp.359-365|publisher=American Institute of Physics|accessdate=12 January 2016|display-authors=etal}}</ref> This plethora of new data both enables and challenges effective astronomical research. Therefore, new approaches are required. In part, due to this data-driven science is becoming a recognized academic discipline. Consequently, astronomy (and other scientific disciplines) are developing sub-disciplines information and data intensive to an extent that these sub-disciplines are now becoming (or have already become) stand alone research disciplines and full-fledged academic programs. While many institutes of education do not boast an astroinformatics program, the most likely will in the near future.\n\n[[Informatics]] has been recently defined as "the use of digital data, information, and related services for research and knowledge generation". However the usual, or commonly used definition is "informatics is the discipline of organizing, accessing, integrating, and mining data from multiple sources for discovery and decision support." Therefore, the discipline of astroinformatics includes many naturally-related specialties including data modeling, data organization, etc. It may also include transformation and normalization methods for data integration and information visualization, as well as knowledge extraction, indexing techniques, information retrieval and data mining methods. Classification schemes (e.g., [[taxonomy (general)|taxonomies]], [[ontology (information science)|ontologies]], [[folksonomy|folksonomies]], and/or collaborative [[Tag (metadata)|tagging]]<ref>{{cite web|last1=Borne|first1=Kirk|title=Collaborative Annotation for Scientific Data Discovery and Reuse|url=http://www.asis.org/Bulletin/Apr-13/AprMay13_RDAP_Borne.html|website=Bulletin of the ASIS&T|publisher=American Society for Information Science and Technology|accessdate=11 January 2016}}</ref>) plus \'\'\'[[Astrostatistics]]\'\'\' will also be heavily involved. \'\'\'[[Citizen science]]\'\'\' projects (such as [[Galaxy Zoo]]) also contribute highly valued novelty discovery, feature meta-tagging, and object characterization within large astronomy data sets. All of these specialties enable scientific discovery across varied massive data collections, collaborative research, and data re-use, in both research and learning environments.\n\nIn 2012, two position papers<ref>{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics in a Nutshell|url=https://asaip.psu.edu/Articles/astroinformatics-in-a-nutshell|website=asaip.psu.edu|publisher=The Astrostatistics and Astroinformatics Portal, Penn State University|accessdate=11 January 2016}}</ref><ref>{{cite web|last1=Feigelson|first1=Eric|title=Astrostatistics in a Nutshell|url=https://asaip.psu.edu/Articles/astrostatistics-in-a-nutshell|website=asaip.psu.edu|publisher=The Astrostatistics and Astroinformatics Portal, Penn State University|accessdate=11 January 2016}}</ref> were presented to the Council of the [[American Astronomical Society]] that led to the establishment of formal working groups in Astroinformatics and [[Astrostatistics]] for the profession of [[astronomy]] within the USA and elsewhere.<ref>{{cite arXiv|last1=Feigelson|first1=E.|last2=Ivezić|first2=Ž.|last3=Hilbe|first3=J.|last4=Borne|first4=K.|title=New Organizations to Support Astroinformatics and Astrostatistics|arxiv=1301.3069}}</ref>\n\nAstroinformatics provides a natural context for the integration of education and research.<ref>{{cite web|last1=Borne|first1=Kirk|title=The Revolution in Astronomy Education: Data Science for the Masses|url=http://adsabs.harvard.edu/abs/2009astro2010P...7B|website=Astrophysics Data System|publisher=SAO/NASA|accessdate=11 January 2016}}</ref> The experience of research can now be implemented within the classroom to establish and grow \'\'\'[[Data Literacy]]\'\'\' through the easy re-use of data.<ref>{{cite web|title=Using Data in the Classroom|url=http://serc.carleton.edu/usingdata/index.html|website=Science Education Resource Center at Carleton College|publisher=National Science Digital Library|accessdate=11 January 2016}}</ref> It also has many other uses, such as repurposing archival data for new projects, literature-data links, intelligent retrieval of information, and many others.<ref>{{cite book|last1=Borne|first1=Kirk|title=Astroinformatics: Data-Oriented Astronomy|location=George Mason University, USA|url=http://www.iccs-meeting.org/iccs2009/PosterPapers/Poster-paper18.pdf|accessdate=January 21, 2015}}</ref>\n\n==Conferences==\n\n{| class="wikitable"\n|-\n! Year\n! Place\n! Link\n|-\n| 2016\n| [[Sorrento]], [[Italy]]\n| [http://www.iau.org/science/meetings/future/symposia/1158/]\n|-\n| 2015\n| [[Dubrovnik]], [[Dalmatia]]\n| [http://iszd.hr/AstroInfo2015/]\n|-\n| 2014\n| [[University of Chile]]\n| [http://eventos.cmm.uchile.cl/astro2014/]\n|-\n| 2013\n| [[Australia Telescope National Facility]], [[CSIRO]]\n| [http://www.atnf.csiro.au/research/workshops/2013/astroinformatics/]\n|-\n| 2012\n| [[Microsoft Research]]\n| [http://www.astro.caltech.edu/ai12/]\n|-\n| 2011\n| [[Sorrento]], [[Italy]]\n| [http://dame.dsf.unina.it/astroinformatics2011.html]\n|-\n| 2010\n| [[Caltech]]\n| [http://www.astro.caltech.edu/ai10/]\n|}\n\n==See also==\n\n*\'\'[[Astronomy and Computing]]\'\'\n*[[Astrophysics Data System]]\n*[[Astrophysics Source Code Library]]\n*[[Astrostatistics]]\n*[[Galaxy Zoo]]\n*[[International Astrostatistics Association]]\n*[[International Virtual Observatory Alliance]] (IVOA)\n*[[MilkyWay@home]]\n*[[Virtual Observatory]]\n*[[WorldWide Telescope]]\n*[[Zooniverse (citizen science project)|Zooniverse]]\n\n== External links ==\n\n* [http://www.adass.org/ Astronomical Data Analysis Software and Systems] (ADASS)\n* [https://asaip.psu.edu/ Astrostatistics and Astroinformatics Portal]\n* [https://asaip.psu.edu/organizations/iaa/iaa-working-group-of-cosmostatistics/ Cosmostatistics Initiative] (COIN)\n* [http://www.iau.org/science/scientific_bodies/commissions/B3/ Astroinformatics and Astrostatistics Commission of the International Astronomical Union]\n\n==References==\n{{reflist}}\n\n[[Category:Astronomy]]\n[[Category:Astrophysics]]\n[[Category:Big data]]\n[[Category:Computational astronomy]]\n[[Category:Data management]]\n[[Category:Information science by discipline]]\n[[Category:Applied statistics]]\n[[Category:Computational fields of study]]']
['Altitude3.Net', '47288071', "{{Infobox software\n| name                   = Altitude3.Net\n| logo                   = Altitude-en.png\n| screenshot             = \n| caption                = Altitude3.Net Dashboard\n| developer              = Nmédia Solutions\n| status                 = Active\n| released               = {{start date and age|2000|06|01}}\n| operating system       = \n| platform               = [[.NET Framework|.Net]]\n| genre                  = [[Content Management System]], [[Content Management Framework]]\n| alexa                  =\n| website                = {{URL|http://altitude3.net}}\n}}\n\n'''Altitude3.Net''' is an electronic business development platform that allows to create web and mobile solutions along with interactive communication strategies. The platform has the same functionalities <ref>{{cite web|language=fr|title = Pinpoint|url = https://pinpoint.microsoft.com/fr-CA/Companies/4296539037/Services|publisher = pinpoint.microsoft.com|accessdate = 2015-06-22}}</ref><ref>{{cite web|title=Altitude3.Net|url=http://www.cmsmatrix.org/matrix/cms-matrix/altitude-3.net|website=CMS Matrix|accessdate=27 July 2015}}</ref> than a content management system (CMS) and communicates with other systems (accounting systems, manufacturing management software (MRP), business management software (enterprise resource planning (ERP)), database, Excel files, XML, CSV or all other kinds of structural data).\n\nNmédia solutions developed Altitude3.Net in 2001 using Microsoft's .NET Framework technology. The platform is currently using the 4.5 version of Microsoft’s Framework.\n\n== History ==\nIn 2001, Nmédia solutions created the content management system Altitude<sup>mc</sup>.<ref>{{cite web|title=About|url=http://altitude3.net/home|website=Altitude3.Net|accessdate=27 July 2015}}</ref> As it went on, many versions were developed:\n* Altitude Moto and Altitude Auto (2001 to 2006);\n* Altitude 2 (2006);\n* Altitude3.Net (2010).\n\n== List of main functionalities ==\nThe Altitude3.Net platform is structured in many modules:<ref>{{cite web|title=Altitude advantages|url=http://altitude3.net/home|website=Altitude3.Net|accessdate=27 July 2015}}</ref>\n* Content management\n* contact management and mass-emailing\n* Control of advanced SEO parameters\n* Microsoft flexibility & computability\n* Security & access management\n* Security & permission management\n* E-commerce solutions: Centralized Product Management (CPM) services. This module includes several functionalities: interface for mass product modification, centralized coupon management, custom management by product group, inventory by store location, shopping cart, price & currency management, catalog management, centralized database, supplier management, product by media, product comparison tool (based on common characteristics), syncing accounting software inventory with Altitude3.Net\n* A [[Microsoft Azure]] solution (cloud computing)\n* Omnichannel marketing\n* Other functionalities : On-site search engines for meta data and documents (text, Word, Excel and PDF); HTML5 video player with descending compatibility; Integrated functions enabling an entire site to be generated in Hypertext Markup Language (HTML) or enabling to export all its data (DATA) and import it in any other CMS\n\n== Awards and recognitions ==\n* In 2010: Altitude3.Net is finalist in the IT category (software application) of the Mérites du français during the Francofête.\n* In 2011: Nmédia solutions wins the title of Web Development Partner of the Year awarded by the Microsoft Partner Network.<ref>{{cite news|title=Microsoft honore deux entreprises de la région|url=http://www.lapresse.ca/la-tribune/economie-et-innovation/201112/19/01-4479237-microsoft-honore-deux-entreprises-de-la-region.php|accessdate=27 July 2015|publisher=[[La Presse (Canadian newspaper)]]|date=19 December 2011|language=fr}}</ref><ref>{{cite news|title=Drummondville triomphe à Toronto|url=http://www.journalexpress.ca/Actualites/Economie/2011-12-05/article-2825943/Drummondville-triomphe-a-Toronto/1|accessdate=27 July 2015|publisher=Journal l'Express|date=5 December 2011|language=fr}}</ref>\n* In 2012: Altitude3.Net wins the Prix Franco awarded by the Drummondville Young Chamber of Commerce during its annual gala.<ref>{{cite web|title=Prix Franco 2012 Nmédia Solutions Inc.|url=http://www.jccd.ca/Concours-Prix-Franco/Prix-Franco-2012/NMedia-Solutions-Inc.aspx|website=Jeune Chambre de Commerce de Drummondville|accessdate=27 July 2015|language=fr}}</ref>\n* In 2015: The CPM module of Altitude3 is finalist in the Web Solutions category at the Octas.<ref>{{cite web|title=Les lauréats du concours des Octas 2015|url=http://www.actionti.com/microsites/octas/gagnants/nos-gagnants|website=Réseau Action TI|accessdate=27 July 2015|language=fr}}</ref><ref>{{cite news|title=Nmédia en lice aux Octas|url=http://www.journalexpress.ca/Actualites/2015-05-12/article-4144312/Nmedias-en-lice-aux-Octas/1|accessdate=27 July 2015|publisher=Journal l'Express|date=12 May 2015|language=fr}}</ref>\n\n== See also ==\n*[[List of content management systems]]\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* [http://altitude3.net Altitude3.Net's website]\n\n[[:Category:Content management systems]]\n[[:Category:Website management]]\n\n\n\n[[Category:Content management systems]]\n[[Category:Data management]]\n[[Category:Technical communication]]"]
['SQL injection', '526999', '{{Use mdy dates|date=February 2012}}\n[[File:KD SQLIA Classification 2010.png|thumb|alt=Classification of SQL injection attack vectors in 2010|A classification of SQL injection attacking vector as of 2010.]]\n\n\'\'\'SQL injection \'\'\' is a [[code injection]] technique, used  to [[Attack (computing)|attack]] data-driven applications, in which nefarious [[SQL]] statements are inserted into an entry field for execution (e.g. to dump the database contents to the attacker).<ref>{{cite web | url = http://technet.microsoft.com/en-us/library/ms161953%28v=SQL.105%29.aspx | title = SQL Injection | accessdate = 2013-08-04 | author = Microsoft | quote = SQL injection is an attack in which malicious code is inserted into strings that are later passed to an instance of SQL Server for parsing and execution. Any procedure that constructs SQL statements should be reviewed for injection vulnerabilities because SQL Server will execute all syntactically valid queries that it receives. Even parameterized data can be manipulated by a skilled and determined attacker.}}</ref> SQL injection must exploit a [[security vulnerability]] in an application\'s software, for example, when user input is either incorrectly filtered for [[string literal]] [[escape sequence|escape characters]] embedded in SQL statements or user input is not [[Strongly-typed programming language|strongly typed]] and unexpectedly executed. SQL injection is mostly known as an attack [[Vector (malware)|vector]] for websites but can be used to attack any type of SQL database.\n\nSQL injection attacks allow attackers to spoof identity, tamper with existing data, cause repudiation issues such as voiding transactions or changing balances, allow the complete disclosure of all data on the system, destroy the data or make it otherwise unavailable, and become administrators of the database server.\n\nIn a 2012 study, it was observed that the average web application received 4 attack campaigns per month, and retailers received twice as many attacks as other industries.<ref>{{cite web | url = http://www.imperva.com/docs/HII_Web_Application_Attack_Report_Ed4.pdf | title = Imperva Web Application Attack Report | accessdate = 2013-08-04 | author = Imperva | date = July 2012 | format = PDF | quote = Retailers suffer 2x as many SQL injection attacks as other industries. / While most web applications receive 4 or more web attack campaigns per month, some websites are constantly under attack. / One observed website was under attack 176 out of 180 days, or 98% of the time.}}</ref>\n\n==History==\n\nThe first public discussions of SQL injection started appearing around 1998;<ref>{{cite web |title= How Was SQL Injection Discovered? The researcher once known as Rain Forrest Puppy explains how he discovered the first SQL injection more than 15 years ago. |author= Sean Michael Kerner  |date= November 25, 2013 |url= http://www.esecurityplanet.com/network-security/how-was-sql-injection-discovered.html }}</ref> for example, a 1998 article in [[Phrack Magazine]].<ref>{{cite journal |title= NT Web Technology Vulnerabilities |author= Jeff Forristal (signing as rain.forest.puppy) |journal= [[Phrack Magazine]] |volume= 8 |issue= 54 (article 8) |date= Dec 25, 1998 |url= http://www.phrack.com/issues.html?issue=54&id=8#article }}</ref>\n\n==Form==\nSQL injection (SQLI) is considered one of the top 10 web application vulnerabilities of 2007 and 2010 by the [[OWASP|Open Web Application Security Project]].<ref>{{cite web|url=https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project |title=Category:OWASP Top Ten Project |publisher=OWASP |accessdate=2011-06-03}}</ref> In 2013, SQLI was rated the number one attack on the OWASP top ten.<ref>{{cite web|url=https://www.owasp.org/index.php/Top_10_2013-Top_10 |title=Category:OWASP Top Ten Project |publisher=OWASP |accessdate=2013-08-13}}</ref> There are four main sub-classes of SQL injection:\n* Classic SQLI\n* Blind or Inference SQL injection\n* [[Database management system]]-specific SQLI\n* Compounded SQLI\n\n:* SQL injection + insufficient authentication<ref>{{cite web|url=http://www.xiom.com/whid-2007-60 |title=WHID 2007-60: The blog of a Cambridge University security team hacked |publisher=Xiom |accessdate=2011-06-03}}</ref>\n:* SQL injection + [[DDoS]] attacks<ref>{{cite web|url=http://www.xiom.com/content/whid-2009-1-gaza-conflict-cyber-war |title=WHID 2009-1: Gaza conflict cyber war |publisher=Xiom |accessdate=2011-06-03}}</ref>\n:* SQL injection + [[DNS hijacking]]<ref>[http://www.xiom.com/whid-list/DNS%20Hijacking ] {{webarchive |url=https://web.archive.org/web/20090618125914/http://www.xiom.com/whid-list/DNS%20Hijacking |date=June 18, 2009 }}</ref>\n:* SQL injection + [[Cross-site scripting|XSS]]<ref>{{cite web|url=http://www.darkreading.com/security/management/showArticle.jhtml?articleID=211201482 |title=Third Wave of Web Attacks Not the Last |publisher=Dark Reading |accessdate=2012-07-29}}</ref>\n\nThe [[Storm Worm]] is one representation of Compounded SQLI.<ref>{{cite web|last=Danchev |first=Dancho |url=http://ddanchev.blogspot.com/2007/01/social-engineering-and-malware.html|title=Mind Streams of Information Security Knowledge: Social Engineering and Malware |publisher=Ddanchev.blogspot.com |date=2007-01-23 |accessdate=2011-06-03}}</ref>\n\nThis classification represents the state of SQLI, respecting its evolution until 2010—further refinement is underway.<ref>{{cite web|last=Deltchev|first=Krassen|title=New Web 2.0 Attacks|url=http://www.nds.ruhr-uni-bochum.de/teaching/theses/Web20/|work=B.Sc. Thesis|publisher=Ruhr-University Bochum|accessdate=February 18, 2010}}</ref>\n\n==Technical implementations==\n\n===Incorrectly filtered escape characters===\nThis form of SQL injection occurs when user input is not filtered for [[escape character]]s and is then passed into an SQL statement. This results in the potential manipulation of the statements performed on the database by the end-user of the application.\n\nThe following line of code illustrates this vulnerability:\n\n statement = "<source lang="sql" enclose="none">SELECT * FROM users WHERE name = \'</source>" + userName + "<source lang="sql" enclose="none">\';</source>"\n\nThis SQL code is designed to pull up the records of the specified username from its table of users. However, if the "userName" variable is crafted in a specific way by a malicious user, the SQL statement may do more than the code author intended. For example, setting the "userName" variable as:\n\n<pre>\' OR \'1\'=\'1</pre>\nor using comments to even block the rest of the query (there are three types of SQL comments<ref>{{citation |title= IBM Informix Guide to SQL: Syntax. Overview of SQL Syntax &gt; How to Enter SQL Comments |publisher= IBM |url= http://publib.boulder.ibm.com/infocenter/idshelp/v10/index.jsp?topic=/com.ibm.sqls.doc/sqls36.htm }}</ref>). All three lines have a space at the end:\n<pre>\' OR \'1\'=\'1\' --\n\' OR \'1\'=\'1\' ({\n\' OR \'1\'=\'1\' /* </pre>\n.\nrenders one of the following SQL statements by the parent language:\n\n<source lang="sql">SELECT * FROM users WHERE name = \'\' OR \'1\'=\'1\';</source>\n<source lang="sql">SELECT * FROM users WHERE name = \'\' OR \'1\'=\'1\' -- \';</source>\n\nIf this code were to be used in an authentication procedure then this example could be used to force the selection of every data field (*) from \'\'all\'\' users rather than from one specific user name as the coder intended,  because the evaluation of \'1\'=\'1\' is always true ([[short-circuit evaluation]]).\n\nThe following value of "userName" in the statement below would cause the deletion of the "users" table as well as the selection of all data from the "userinfo" table (in essence revealing the information of every user), using an [[API]] that allows multiple statements:\n\n a\';<source lang="sql" enclose="none">DROP TABLE users; SELECT * FROM userinfo WHERE \'t\' = \'t\'</source>\n\nThis input renders the final SQL statement as follows and specified:\n\n<source lang="sql">SELECT * FROM users WHERE name = \'a\';DROP TABLE users; SELECT * FROM userinfo WHERE \'t\' = \'t\';</source>\n\nWhile most SQL server implementations allow multiple statements to be executed with one call in this way, some SQL APIs such as [[PHP]]\'s <code>mysql_query()</code> function do not allow this for security reasons. This prevents attackers from injecting entirely separate queries, but doesn\'t stop them from modifying queries.\n\n===Incorrect type handling===\nThis form of SQL injection occurs when a \'\'\'user-supplied\'\'\' field is not [[strongly typed]] or is not checked for [[data type|type]] constraints. This could take place when a numeric field is to be used in a SQL statement, but the programmer makes no checks to validate that the user supplied input is numeric. For example:\n statement := "<source lang="sql" enclose="none">SELECT * FROM userinfo WHERE id = </source>" + a_variable + ";"\n\nIt is clear from this statement that the author intended a_variable to be a number correlating to the "id" field. However, if it is in fact a [[String (computer science)|string]] then the [[end-user]] may manipulate the statement as they choose, thereby bypassing the need for escape characters. For example, setting a_variable to\n\n<pre>1;DROP TABLE users</pre>\n\nwill drop (delete) the "users" table from the database, since the SQL becomes:\n\n<source lang="sql">SELECT * FROM userinfo WHERE id=1; DROP TABLE users;</source>\n\n===Blind SQL injection===\nBlind SQL Injection is used when a web application is vulnerable to an SQL injection but the results of the injection are not visible to the attacker. The page with the vulnerability may not be one that displays data but will display differently depending on the results of a logical statement injected into the legitimate SQL statement called for that page.\nThis type of attack has traditionally been considered time-intensive because a new statement needed to be crafted for each bit recovered, and depending on its structure, the attack may consist of many unsuccessful requests. Recent advancements have allowed each request to recover multiple bits, with no unsuccessful requests, allowing for more consistent and efficient extraction. <ref>{{cite web | url = http://howto.hackallthethings.com/2016/07/extracting-multiple-bits-per-request.html | title = Extracting Multiple Bits Per Request From Full-blind SQL Injection Vulnerabilities | publisher = Hack All The Things | accessdate = July 8, 2016 |archiveurl = https://web.archive.org/web/20160708190141/http://howto.hackallthethings.com/2016/07/extracting-multiple-bits-per-request.html |archivedate = July 8, 2016}}</ref> There are several tools that can automate these attacks once the location of the vulnerability and the target information has been established.<ref>{{cite web | url = http://www.justinclarke.com/archives/2006/03/sqlbrute.html | title = Using SQLBrute to brute force data from a blind SQL injection point | publisher = Justin Clarke | accessdate = October 18, 2008 |archiveurl = http://web.archive.org/web/20080614203711/http://www.justinclarke.com/archives/2006/03/sqlbrute.html <!-- Bot retrieved archive --> |archivedate = June 14, 2008}}</ref>\n\n====Conditional responses====\nOne type of blind SQL injection forces the database to evaluate a logical statement on an ordinary application screen. As an example, a book review website uses a [[query string]] to determine which book review to display. So the [[URL]] <code><nowiki>http://books.example.com/showReview.php?ID=5</nowiki></code> would cause the server to run the query\n<source lang="sql">SELECT * FROM bookreviews WHERE ID = \'Value(ID)\';</source>\nfrom which it would populate the review page with data from the review with [[Identifier|ID]] 5, stored in the [[Table (database)|table]] bookreviews. The query happens completely on the server; the user does not know the names of the database, table, or fields, nor does the user know the query string. The user only sees that the above URL returns a book review. A [[Hacker (computer security)|hacker]] can load the URLs <code><source lang="sql" enclose="none">http://books.example.com/showReview.php?ID=5 OR 1=1</source></code> and <code><source lang="sql" enclose="none">http://books.example.com/showReview.php?ID=5 AND 1=2</source></code>, which may result in queries\n<source lang="sql">SELECT * FROM bookreviews WHERE ID = \'5\' OR \'1\'=\'1\';\nSELECT * FROM bookreviews WHERE ID = \'5\' AND \'1\'=\'2\';</source>\nrespectively. If the original review loads with the "1=1" URL and a blank or error page is returned from the "1=2" URL, and the returned page has not been created to alert the user the input is invalid, or in other words, has been caught by an input test script, the site is likely vulnerable to a SQL injection attack as the query will likely have passed through successfully in both cases. The hacker may proceed with this query string designed to reveal the version number of [[MySQL]] running on the server: <code><source lang="mysql" enclose="none">http://books.example.com/showReview.php?ID=5 AND substring(@@version, 1, INSTR(@@version, \'.\') - 1)=4</source></code>, which would show the book review on a server running MySQL 4 and a blank or error page otherwise. The hacker can continue to use code within query strings to glean more information from the server until another avenue of attack is discovered or his or her goals are achieved.<ref>{{cite web|url=http://forum.intern0t.org/web-hacking-war-games/818-blind-sql-injection.html|title=Blind SQL Injection tutorial|author=macd3v|accessdate=6 December 2012}}</ref><ref>{{cite web|title=TDSS botnet: full disclosure|url=http://nobunkum.ru/analytics/en-tdss-botnet|accessdate=6 December 2012|author=Andrey Rassokhin|author2=Dmitry Oleksyuk }}</ref>\n\n===Second order SQL injection===\nSecond order SQL injection occurs when submitted values contain malicious commands that are stored rather than executed immediately.  In some cases, the application may correctly encode an SQL statement and store it as valid SQL.  Then, another part of that application without controls to protect against SQL injection might execute that stored SQL statement.  This attack requires more knowledge of how submitted values are later used.  Automated web application security scanners would not easily detect this type of SQL injection and may need to be manually instructed where to check for evidence that it is being attempted.\n\n==Mitigation==\nAn SQL injection is a well known attack and easily prevented by simple measures. After an apparent SQL injection attack on [[TalkTalk Group|Talktalk]] in 2015, the BBC reported that security experts were stunned that such a large company would be vulnerable to it.<ref>{{Cite web|title = Questions for TalkTalk - BBC News|url = http://www.bbc.com/news/technology-34636308|website = BBC News|accessdate = 2015-10-26|language = en}}</ref>\n\n===Parameterized statements===\n{{Main article|Prepared statement}}\nWith most development platforms, parameterized statements that work with parameters can be used  (sometimes called placeholders or [[bind variable]]s) instead of embedding user input in the statement. A placeholder can only store a value of the given type and not an arbitrary SQL fragment. Hence the SQL injection would simply be treated as a strange (and probably invalid) parameter value.\n\nIn many cases, the SQL statement is fixed, and each parameter is a [[Scalar (computing)|scalar]], not a [[Table (database)|table]]. The user input is then assigned (bound) to a parameter.<ref>{{cite web|title=SQL Injection Prevention Cheat Sheet|url=https://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet|publisher=Open Web Application Security Project|accessdate=3 March 2012}}</ref>\n\n====Enforcement at the coding level====\nUsing [[object-relational mapping]] libraries avoids the need to write SQL code. The ORM library in effect will generate parameterized SQL statements from object-oriented code.\n\n===Escaping===\nA straightforward, though error-prone way to prevent injections is to escape characters that have a special meaning in SQL. The manual for an SQL DBMS explains which characters have a special meaning, which allows creating a comprehensive [[Blacklist (computing)|blacklist]] of characters that need translation. For instance, every occurrence of a single quote (<code>\'</code>) in a parameter must be replaced by two single quotes (<code><nowiki>\'\'</nowiki></code>) to form a valid SQL string literal. For example, in [[PHP]] it is usual to escape parameters using the function <code>mysqli_real_escape_string();</code> before sending the SQL query:\n<source lang="php">\n$mysqli = new mysqli(\'hostname\', \'db_username\', \'db_password\', \'db_name\');\n$query = sprintf("SELECT * FROM `Users` WHERE UserName=\'%s\' AND Password=\'%s\'",\n                  $mysqli->real_escape_string($username),\n                  $mysqli->real_escape_string($password));\n$mysqli->query($query);\n</source>\n\nThis function prepends backslashes to the following characters: \\x00, \\n, \\r, \\, \', " and \\x1a.\nThis function is normally used to make data safe before sending a query to [[MySQL]].<ref>{{cite web|url=http://in2.php.net/manual/en/mysqli.real-escape-string.php|title=mysqli->real_escape_string - PHP Manual|publisher=PHP.net}}</ref><br /> There are other functions for many database types in PHP such as pg_escape_string() for [[PostgreSQL]]. The function <code>addslashes(string $str)</code> works for escaping characters, and is used especially for querying on databases that do not have escaping functions in PHP.  It returns a string with backslashes before characters that need to be quoted in database queries, etc. These characters are single quote (\'), double quote ("), backslash (\\) and NUL (the NULL byte).<ref>{{cite web|url=http://pl2.php.net/manual/en/function.addslashes.php|title=Addslashes - PHP Manual|publisher=PHP.net}}</ref><br />\nRoutinely passing escaped strings to SQL is error prone because it is easy to forget to escape a given string. Creating a transparent layer to secure the input can reduce this error-proneness, if not entirely eliminate it.<ref>{{cite web|url=http://www.xarg.org/2010/11/transparent-query-layer-for-mysql/|title=Transparent query layer for MySQL|publisher=Robert Eisele|date=November 8, 2010}}</ref>\n\n===Pattern check===\nInteger, float or boolean,string parameters can be checked if their value is valid representation for the given type. Strings that must follow some strict pattern (date, UUID, alphanumeric only, etc.) can be checked if they match this pattern.\n\n===Database permissions===\nLimiting the permissions on the database logon used by the web application to only what is needed may help reduce the effectiveness of any SQL injection attacks that exploit any bugs in the web application.\n\nFor example, on [[Microsoft SQL Server]], a database logon could be restricted from selecting on some of the system tables which would limit exploits that try to insert JavaScript into all the text columns in the database.\n<source lang="tsql">\ndeny select on sys.sysobjects to webdatabaselogon;\ndeny select on sys.objects to webdatabaselogon;\ndeny select on sys.tables to webdatabaselogon;\ndeny select on sys.views to webdatabaselogon;\ndeny select on sys.packages to webdatabaselogon;\n</source>\n\n==Examples==\n* In February 2002, Jeremiah Jacks discovered that Guess.com was vulnerable to an SQL injection attack, permitting anyone able to construct a properly-crafted URL to pull down 200,000+ names, credit card numbers and expiration dates in the site\'s customer database.<ref>{{cite web|url=http://www.securityfocus.com/news/346|title=Guesswork Plagues Web Hole Reporting|publisher=[[SecurityFocus]]|date=March 6, 2002}}</ref>\n* On November 1, 2005, a teenaged hacker used SQL injection to break into the site of a [[Taiwan]]ese information security magazine from the Tech Target group and steal customers\' information.<ref>{{cite web|url=http://www.xiom.com/whid-2005-46|title=WHID 2005-46: Teen uses SQL injection to break to a security magazine web site|publisher=[[Web Application Security Consortium]]|date=November 1, 2005|accessdate=December 1, 2009}}</ref>\n* On January 13, 2006, [[Russia]]n computer criminals broke into a [[Government of Rhode Island|Rhode Island government]] website and allegedly stole credit card data from individuals who have done business online with state agencies.<ref>{{cite web|url=http://www.xiom.com/whid-2006-3|title=WHID 2006-3: Russian hackers broke into a RI GOV website|publisher=[[Web Application Security Consortium]]|date=January 13, 2006|accessdate=May 16, 2008}}</ref>\n* On March 29, 2006, a hacker discovered an SQL injection flaw in an official [[Government of India|Indian government]]\'s [[Tourism in India|tourism]] site.<ref>{{cite web|url=http://www.xiom.com/whid-2006-27|title=WHID 2006-27: SQL Injection in incredibleindia.org|publisher=[[Web Application Security Consortium]]|date=March 29, 2006|accessdate=March 12, 2010}}</ref>\n* On June 29, 2007, a computer criminal defaced the [[Microsoft]] UK website using SQL injection.<ref>{{cite web|url=http://www.cgisecurity.net/2007/06/hacker-defaces.html|title=Hacker Defaces Microsoft U.K. Web Page|publisher=cgisecurity.net|author=Robert|date=June 29, 2007|accessdate=May 16, 2008}}</ref><ref>{{cite web|url=http://rcpmag.com/news/article.aspx?editorialsid=8762|title=Hacker Defaces Microsoft UK Web Page|publisher=Redmond Channel Partner Online|author=Keith Ward|date=June 29, 2007|accessdate=May 16, 2008}}</ref> UK website \'\'[[The Register]]\'\' quoted a Microsoft [[spokesperson]] acknowledging the problem.\n* On September 19, 2007 and January 26, 2009 the Turkish hacker group "m0sted" used SQL injection to exploit Microsoft\'s SQL Server to hack web servers belonging to [[McAlester Army Ammunition Plant]] and the [[United States Army Corps of Engineers|US Army Corps of Engineers]] respectively.<ref>{{cite web|url=http://www.informationweek.com/architecture/anti-us-hackers-infiltrate-army-servers/d/d-id/1079964|publisher=[[Information Week]]|title=Anti-U.S. Hackers Infiltrate Army Servers|date=May 29, 2009|accessdate=December 17, 2016}}</ref>\n* In January 2008, tens of thousands of PCs were infected by an automated SQL injection attack that exploited a vulnerability in application code that uses [[Microsoft SQL Server]] as the database store.<ref name="chinesefarm" />\n* In July 2008, [[Kaspersky Lab|Kaspersky]]\'s [[Malaysia]]n site was hacked by the "m0sted" hacker group using SQL injection.\n* On April 13, 2008, the [[Sex offender registries in the United States|Sexual and Violent Offender Registry]] of [[Oklahoma]] shut down its website for "[[routine maintenance]]" after being informed that 10,597 [[Social Security number]]s belonging to [[sex offender]]s had been downloaded via an SQL injection attack<ref>{{cite web|url=http://thedailywtf.com/Articles/Oklahoma-Leaks-Tens-of-Thousands-of-Social-Security-Numbers,-Other-Sensitive-Data.aspx|publisher=[[The Daily WTF]]|title=Oklahoma Leaks Tens of Thousands of Social Security Numbers, Other Sensitive Data|author=Alex Papadimoulis|date=April 15, 2008|accessdate=May 16, 2008}}</ref>\n* In May 2008, a [[server farm]] inside [[China]] used automated queries to [[Google Search|Google\'s search engine]] to identify [[Microsoft SQL Server|SQL server]] websites which were vulnerable to the attack of an automated SQL injection tool.<ref name="chinesefarm">{{cite web | url = http://www.pcworld.com/businesscenter/article/146048/mass_sql_injection_attack_targets_chinese_web_sites.html | title = Mass SQL Injection Attack Targets Chinese Web Sites | author = Sumner Lemon, IDG News Service | publisher = [[PC World (magazine)|PCWorld]] | date = May 19, 2008 | accessdate = May 27, 2008 }}</ref><ref name="attackspecifics">{{cite web | url = http://www.bloombit.com/Articles/2008/05/ASCII-Encoded-Binary-String-Automated-SQL-Injection.aspx | title = ASCII Encoded/Binary String Automated SQL Injection Attack |author=Michael Zino| date = May 1, 2008 }}</ref>\n* In 2008, at least April through August, a sweep of attacks began exploiting the SQL injection vulnerabilities of Microsoft\'s [[Internet Information Services|IIS web server]] and [[Microsoft SQL Server|SQL Server database server]]. The attack does not require guessing the name of a table or column, and corrupts all text columns in all tables in a single request.<ref name="broad_inject_specifics">{{cite web | url = http://hackademix.net/2008/04/26/mass-attack-faq/ | title = Mass Attack FAQ |author=Giorgio Maone| date = April 26, 2008 }}</ref>  A HTML string that references a [[malware]] [[JavaScript]] file is appended to each value. When that database value is later displayed to a website visitor, the script attempts several approaches at gaining control over a visitor\'s system. The number of exploited web pages is estimated at 500,000.<ref name="broad_inject_numbers">{{cite web | url = http://www.computerworld.com/article/2535473/security0/huge-web-hack-attack-infects-500-000-pages.html | title = Huge Web hack attack infects 500,000 pages |author=Gregg Keizer| date = April 25, 2008 |accessdate=October 16, 2015}}</ref>\n* On August 17, 2009, the [[United States Department of Justice]] charged an American citizen, [[Albert Gonzalez]], and two unnamed Russians with the theft of 130 million credit card numbers using an SQL injection attack. In reportedly "the biggest case of [[identity theft]] in American history", the man stole cards from a number of corporate victims after researching their [[Payment processor|payment processing system]]s. Among the companies hit were credit card processor [[Heartland Payment Systems]], convenience store chain [[7-Eleven|7&#8209;Eleven]], and supermarket chain [[Hannaford Brothers]].<ref>{{cite news |url=http://news.bbc.co.uk/2/hi/americas/8206305.stm |title=US man \'stole 130m card numbers\' |publisher=BBC |date=August 17, 2009 |accessdate=August 17, 2009}}</ref>\n* In December 2009, an attacker breached a [[RockYou]] plaintext database containing the [[Encryption|unencrypted]] usernames and passwords of about 32&nbsp;million users using an SQL injection attack.<ref>{{cite news | url=http://www.nytimes.com/external/readwriteweb/2009/12/16/16readwriteweb-rockyou-hacker-30-of-sites-store-plain-text-13200.html | title = RockYou Hacker - 30% of Sites Store Plain Text Passwords | work=New York Times | first=Jolie | last=O\'Dell | date=December 16, 2009 | accessdate=May 23, 2010}}</ref>\n*On July 2010, a South American security researcher who goes by the [[User (computing)|handle]] "Ch&nbsp;Russo" obtained sensitive user information from popular [[BitTorrent]] site [[The Pirate Bay]]. He gained access to the site\'s administrative control panel and exploited a SQL injection vulnerability that enabled him to collect user account information, including [[IP address]]es, [[MD5]] [[Cryptographic hash function|password hashes]] and records of which torrents individual users have uploaded.<ref>{{cite news |url=http://krebsonsecurity.com/2010/07/pirate-bay-hack-exposes-user-booty/ | title= The pirate bay attack | date=July 7, 2010 }}</ref>\n*From July 24 to 26, 2010, attackers from [[Japan]] and [[China]] used an SQL injection to gain access to customers\' credit card data from Neo Beat, an [[Osaka]]-based company that runs a large online supermarket site. The attack also affected seven business partners including supermarket chains Izumiya Co, Maruetsu Inc, and Ryukyu Jusco Co. The theft of data affected a reported 12,191 customers. As of August 14, 2010 it was reported that there have been more than 300 cases of credit card information being used by third parties to purchase goods and services in China.\n* On September 19 during the [[Swedish general election, 2010|2010 Swedish general election]] a voter attempted a code injection by hand writing SQL commands as part of a [[Write-in candidate|write&#8209;in]] vote.<ref>{{cite web|url=http://alicebobandmallory.com/articles/2010/09/23/did-little-bobby-tables-migrate-to-sweden |title=Did Little Bobby Tables migrate to Sweden? |publisher=Alicebobandmallory.com |accessdate=2011-06-03}}</ref>\n* On November 8, 2010 the British [[Royal Navy]] website was compromised by a Romanian hacker named TinKode using SQL injection.<ref>[http://www.bbc.co.uk/news/technology-11711478 Royal Navy website attacked by Romanian hacker] \'\'BBC News\'\', 8-11-10, Accessed November 2010</ref><ref>{{cite web|url=http://news.sky.com/skynews/Home/World-News/Stuxnet-Worm-Virus-Targeted-At-Irans-Nuclear-Plant-Is-In-Hands-Of-Bad-Guys-Sky-News-Sources-Say/Article/201011415827544 |title=Super Virus A Target For Cyber Terrorists\n|author=Sam Kiley |date=November 25, 2010 |accessdate=November 25, 2010}}</ref>\n* On February 5, 2011 [[HBGary]], a technology security firm, was broken into by [[LulzSec]] using a SQL injection in their CMS-driven website<ref>{{cite web|url=http://www.par-anoia.net/We_Are_Anonymous_Inside_the_Hacker_World_of_LulzSe.pdf|title=We Are Anonymous: Inside the Hacker World of LulzSec|publisher=Little, Brown and Company}}</ref>\n* On March 27, 2011, [http://www.mysql.com mysql.com], the official homepage for [[MySQL]], was compromised by a hacker using SQL blind injection<ref>{{cite web|url=http://blog.sucuri.net/2011/03/mysql-com-compromised.html|title=MySQL.com compromised|publisher=[[sucuri]]}}</ref>\n* On April 11, 2011, [[Barracuda Networks]] was compromised using an SQL injection flaw. [[Email address]]es and usernames of employees were among the information obtained.<ref>{{cite web|url=http://www.networkworld.com/news/2011/041211-hacker-breaks-into-barracuda-networks.html?hpg1=bn |title=Hacker breaks into Barracuda Networks database}}</ref>\n*Over a period of 4&nbsp;hours on April 27, 2011, an automated SQL injection attack occurred on [[Broadband Reports]] website that was able to extract 8% of the username/password pairs: 8,000 random accounts of the 9,000 active and 90,000 old or inactive accounts.<ref name="DSLReports">{{cite web|url=http://www.dslreports.com/forum/r25793356- |title=site user password intrusion info |publisher=Dslreports.com |accessdate=2011-06-03}}</ref><ref name="Cnet News">{{cite news|url=http://news.cnet.com/8301-27080_3-20058471-245.html|title=DSLReports says member information stolen|publisher=Cnet News|date=2011-04-28|accessdate=2011-04-29}}</ref><ref name="The Tech Herald">{{cite news|url=http://www.thetechherald.com/article.php/201117/7127/DSLReports-com-breach-exposed-more-than-100-000-accounts|title=DSLReports.com breach exposed more than 100,000 accounts|publisher=The Tech Herald|date=2011-04-29|accessdate=2011-04-29}}</ref>\n*On June 1, 2011, "[[hacktivist]]s" of the group [[LulzSec]] were accused of using SQLI to steal [[coupon]]s, download keys, and passwords that were stored in plaintext on [[Sony]]\'s website, accessing the personal information of a million users.<ref>{{citation |title= LulzSec hacks Sony Pictures, reveals 1m passwords unguarded | date= June 2, 2011 |work= electronista.com |url= http://www.electronista.com/articles/11/06/02/lulz.security.hits.sony.again.in.security.message/ }}</ref><ref>{{citation |title= LulzSec Hacker Arrested, Group Leaks Sony Database|author=Ridge Shan | date= June 6, 2011 |work= The Epoch Times |url=http://www.theepochtimes.com/n2/technology/lulzsec-member-arrested-group-leaks-sony-database-57296.html}}</ref>\n* In June 2011, [[PBS]] was hacked, mostly likely through use of SQL injection; the full process used by hackers to execute SQL injections was described in this [http://blog.imperva.com/2011/05/pbs-breached-how-hackers-probably-did-it.html Imperva] blog.<ref name="PBS Breached - How Hackers Probably Did It">{{cite news|url=http://blog.imperva.com/2011/05/pbs-breached-how-hackers-probably-did-it.html|title=Imperva.com: PBS Hacked - How Hackers Probably Did It|accessdate=2011-07-01}}</ref>\n* In May 2012, the website for \'\'[[Wurm Online]]\'\', a [[massively multiplayer online game]], was shut down from an SQL injection while the site was being updated.<ref>{{cite web|url=http://wurmonline.tumblr.com/post/22835329693/wurm-online-restructuring |title=Wurm Online is Restructuring |date=May 11, 2012}}</ref>\n* [[2012 Yahoo! Voices hack|In July 2012]] a hacker group was reported to have stolen 450,000 login credentials from [[Yahoo!]]. The logins were stored in [[plain text]] and were allegedly taken from a Yahoo [[subdomain]], [[Yahoo! Voices]]. The group breached Yahoo\'s security by using a "[[Set operations (SQL)#UNION operator|union]]-based SQL injection technique".<ref>Chenda Ngak. [http://www.cbsnews.com/8301-501465_162-57470956-501465/yahoo-reportedly-hacked-is-your-account-safe/ "Yahoo reportedly hacked: Is your account safe?"], CBS News. July 12, 2012. Retrieved July 16, 2012.</ref><ref>http://www.zdnet.com/450000-user-passwords-leaked-in-yahoo-breach-7000000772/</ref>\n* On October 1, 2012, a hacker group called "Team GhostShell" published the personal records of students, faculty, employees, and alumni from 53 universities including [[Harvard]], [[Princeton University|Princeton]], [[Stanford]], [[Cornell]], [[Johns Hopkins University|Johns Hopkins]], and the [[University of Zurich]] on [[Pastebin|pastebin.com]]. The hackers claimed that they were trying to "raise awareness towards the changes made in today’s education", bemoaning changing education laws in Europe and increases in [[College tuition in the United States|tuition in the United States]].<ref>{{cite news|last=Perlroth|first=Nicole|title=Hackers Breach 53 Universities and Dump Thousands of Personal Records Online|url=http://bits.blogs.nytimes.com/2012/10/03/hackers-breach-53-universities-dump-thousands-of-personal-records-online/|newspaper=New York Times|date=3 October 2012}}</ref>\n* In February 2013, a group of Maldivian hackers, hacked the website "UN-Maldives" using SQL Injection.\n* On June 27, 2013, hacker group "[[RedHack]]" breached Istanbul Administration Site.<ref>{{Cite news | title=RedHack Breaches Istanbul Administration Site, Hackers Claim to Have Erased Debts | url=http://news.softpedia.com/news/RedHack-Breaches-Istanbul-Administration-Site-Hackers-Claim-to-Have-Erased-Debts-364000.shtml}}</ref>  They claimed that, they’ve been able to erase people\'s debts to water, gas, Internet, electricity, and telephone companies. Additionally, they published admin user name and password for other citizens to log in and clear their debts early morning. They announced the news from Twitter.<ref>{{Cite news | title=Redhack tweet about their achievement | url=http://twitter.com/RedHack_EN/statuses/350461821456613376 }}</ref>\n* On November 4, 2013, hacktivist group "RaptorSwag" allegedly compromised 71 Chinese government databases using an SQL injection attack on the Chinese Chamber of International Commerce. The leaked data was posted publicly in cooperation with [[Anonymous (group)|Anonymous]].<ref>http://news.softpedia.com/news/Hackers-Leak-Data-Allegedly-Stolen-from-Chinese-Chamber-of-Commerce-Website-396936.shtml</ref>\n* On February 2, 2014, AVS TV had 40,000 accounts leaked by a hacking group called @deletesec <ref>http://www.maurihackers.info/2014/02/40000-avs-tv-accounts-leaked.html</ref>\n* On February 21, 2014, United Nations Internet Governance Forum had 3,215 account details leaked.<ref>http://www.batblue.com/united-nations-internet-governance-forum-breached/</ref>\n* On February 21, 2014, Hackers of a group called @deletesec hacked Spirol International after allegedly threatening to have the hackers arrested for reporting the security vulnerability. 70,000 user details were exposed over this conflict.<ref>http://news.softpedia.com/news/Details-of-70-000-Users-Leaked-by-Hackers-From-Systems-of-SPIROL-International-428669.shtml</ref>\n* On March 7, 2014, officials at Johns Hopkins University publicly announced that their Biomedical Engineering Servers had become victim to an SQL injection attack carried out by an Anonymous hacker named "Hooky" and aligned with hacktivist group "RaptorSwag". The hackers compromised personal details of 878 students and staff, posting a [http://pastebin.com/UG4fYnby press release] and the leaked data on the internet.<ref>http://articles.baltimoresun.com/2014-03-07/news/bs-md-hopkins-servers-hacked-20140306_1_engineering-students-identity-theft-server</ref>\n* In August 2014, [[Milwaukee]]-based computer security company Hold Security disclosed that it uncovered [[2014 Russian hacker password theft|a theft of confidential information]] from nearly 420,000 websites through SQL injections.<ref>Damon Poeter. [http://www.pcmag.com/article2/0,2817,2462057,00.asp \'Close-Knit\' Russian Hacker Gang Hoards 1.2 Billion ID Creds], \'\'PC Magazine\'\', August 5, 2014</ref> \'\'[[The New York Times]]\'\' confirmed this finding by hiring a security expert to check the claim.<ref>Nicole Perlroth. [http://www.nytimes.com/2014/08/06/technology/russian-gang-said-to-amass-more-than-a-billion-stolen-internet-credentials.html?_r=0 Russian Gang Amasses Over a Billion Internet Passwords], \'\'The New York Times\'\', August 5, 2014.</ref>\n* In October 2015, an SQL injection attack was used to steal the personal details of 156,959 customers from British telecommunications company [[TalkTalk Group|Talk Talk\'s]] servers, exploiting a vulnerability in a legacy web portal<ref>https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2016/10/talktalk-gets-record-400-000-fine-for-failing-to-prevent-october-2015-attack/</ref>\n\n==In popular culture==\n* Unauthorized login to web sites by means of SQL injection forms the basis of one of the subplots in [[J.K. Rowling]]\'s novel \'\'[[The Casual Vacancy]]\'\', published in 2012.\n* An \'\'[[xkcd]]\'\' cartoon involved a character "Robert\'); DROP TABLE students;--" named to carry out a SQL injection. As a result of this cartoon, SQL injection is sometimes informally referred to as \'Bobby Tables\'.<ref>{{cite web|last=Munroe|first=Randall|title=XKCD: Exploits Of A Mom|url=http://xkcd.com/327/|accessdate=26 February 2013}}</ref><ref>{{cite web|title=Bobby Tables: A guide to preventing SQL injection|url=http://bobby-tables.com/|accessdate=6 October 2013}}</ref>\n* In 2014, an individual in Poland legally renamed his business to \'\'<nowiki>Dariusz Jakubowski x\'; DROP TABLE users; SELECT \'1</nowiki>\'\' in an attempt to disrupt operation of spammers’ [[Web scraping|harvesting bots]].<ref>{{cite web|title=Jego firma ma w nazwie SQL injection. Nie zazdrościmy tym, którzy będą go fakturowali ;)|website=Niebezpiecznik|language=pl|date=11 September 2014|url=http://niebezpiecznik.pl/post/jego-firma-ma-w-nazwie-sql-injection-nie-zazdroscimy-tym-ktorzy-beda-go-fakturowali/|accessdate=26 September 2014}}</ref>\n* The 2015 game [[Hacknet]] has a hacking program called SQL_MemCorrupt. It is described as injecting a table entry that causes a corruption error in a SQL database, then queries said table, causing a SQL database crash and core dump.\n\n==See also==\n{{Portal|Software Testing}}\n* [[Code injection]]\n* [[Cross-site scripting]]\n* [[Metasploit Project]]\n* [[OWASP]] Open Web Application Security Project\n* [[SGML entity]]\n* [[Uncontrolled format string]]\n* [[w3af]]\n* [[Web application security]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* [http://www.techyfreaks.com/2012/05/manual-sql-injection-tutorial.html Manual Sql Injection Tutorial] By The Ajay Devgan\n* [http://www.websec.ca/kb/sql_injection SQL Injection Knowledge Base], by Websec.\n* [http://www.sqlinjectionwiki.com/ SQL Injection Wiki]\n* [http://projects.webappsec.org/SQL-Injection WASC Threat Classification - SQL Injection Entry], by the Web Application Security Consortium.\n* [https://docs.google.com/leaf?id=0BykNNUTb95yzYTRjMjNjMWEtODBmNS00YzgwLTlmMGYtNWZmODI2MTNmZWYw&sort=name&layout=list&num=50 Why SQL Injection Won\'t Go Away], by Stuart Thomas.\n* [http://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet SQL Injection Prevention Cheat Sheet], by OWASP.\n* [http://sqlmap.org/ sqlmap: automatic SQL injection and database takeover tool]\n* [http://go.microsoft.com/?linkid=9707610 SDL Quick security references on SQL injection] by Bala Neerumalla.\n* [http://arstechnica.com/information-technology/2016/10/how-security-flaws-work-sql-injection/ How security flaws work: SQL injection]\n* [https://www.netsparker.com/blog/web-security/sql-injection-cheat-sheet/ SQL Injection Cheat Sheet] by Netsparker\n\n[[Category:Data management]]\n[[Category:Injection exploits]]\n[[Category:SQL]]\n[[Category:Articles with example SQL code]]\n[[Category:Computer security exploits]]']
['Cambridge Semantics', '50373790', '{{Infobox company\n|name             = Cambridge Semantics\n|logo             =\n|logo_size        =200\n|type             = [[Private company|Private]]\n|genre            =\n|fate             =\n|predecessor      =\n|successor        =\n|foundation       = 2007\n|founder          = Sean Martin<br>Lee Feigenbaum<br>Simon Martin<br>Emmett Eldred\n|defunct          =\n|location_city    = [[Boston, MA]]\n|location_country = [[United States]]\n|locations        = (2) [[Boston, MA]] & [[San Diego, CA]]\n|area_served      =\n|key_people       =  Chuck Pieper (CEO)<br>Alok Prasad (President)\n|industry         =  [[Computer Software]]\n|products         =\n|production       =\n|services         =\n|revenue          = \n|operating_income =\n|net_income       =\n|aum              =\n|assets           =\n|equity           =\n|slogan           = The Smart Data Company\n|owner            =\n|num_employees    =\n|parent           =\n|divisions        =\n|subsid           =\n|homepage         =  {{URL|CambridgeSemantics.com}} \n|footnotes        =\n|intl             =\n}}\n\n\'\'\'Cambridge Semantics\'\'\' is a privately held company headquartered in [[Boston, Massachusetts]] with a West Coast office in [[San Diego, California]]. The company develops and sells a suite of smart data products for Data Management, Data Discovery and Enterprise Analytics.\n\n==History==\n\nCambridge Semantics was founded in 2007 by Sean Martin, Lee Feigenbaum, Simon Martin, Rouben Meschian and Emmett Eldred who all previously worked at [[IBM]]\'s Advanced Technology Internet Group.<ref>{{cite web|last1=Lynch|first1=Brendan|website=[[Boston Business Journal]]|title=Ex-IBMers aim at better search tech|url=http://www.bizjournals.com/boston/blog/mass-high-tech/2008/03/ex-ibmers-aim-at-better-search-tech.html|accessdate=27 April 2016}}</ref><ref>{{cite web|last1=Resende|first1=Patricia|title=With explosion of big data comes big growth for Cambridge Semantics|url=http://www.bizjournals.com/boston/blog/techflash/2015/02/with-explosion-of-big-data-comes-big-growth-for.html|website=[[Boston Business Journal]]|accessdate=27 April 2016}}</ref>\n\nIn 2012, Cambridge Semantics appointed Chuck Pieper as Chief Executive Officer. Prior to joining Cambridge Semantics, Pieper was Vice Chairman of Alternative Investments and Managing Director of [[Credit Suisse]] within the Asset Management Division.<ref>{{cite web|last1=Seiffert|first1=Don|title=Chuck Pieper named CEO at Cambridge Semantics|url=http://www.bizjournals.com/boston/blog/techflash/2012/12/chuck-pieper-named-ceo-at-cambridge.html|website=[[Boston Business Journal]]|accessdate=27 April 2016}}</ref>\n\nIn 2015, Cambridge Semantics formed an alliance with [[MarkLogic]].<ref>{{cite web|title=Cambridge Semantics and MarkLogic Partner to Advance Semantic-Driven Data Management|url=http://www.dbta.com/Editorial/News-Flashes/Cambridge-Semantics-and-MarkLogic-Partner-to-Advance-Semantic-Driven-Data-Management-106569.aspx|website=Dbta.com|accessdate=27 April 2016|language=en-US|date=24 September 2015}}</ref><ref>{{cite web|title=MarkLogic, Cambridge Semantics partner for NoSQL|url=http://www.kmworld.com/Articles/News/News/MarkLogic-Cambridge-Semantics-partner-for-NoSQL-106568.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=24 September 2015}}</ref>\n\nIn January 2016, Cambridge Semantics acquired SPARQL City and its [[graph database]] [[intellectual property]].<ref>{{cite web|last1=Leopold|first1=George|title=Cambridge Semantics Buys Graph Database Specialist|url=http://www.datanami.com/2016/01/14/cambridge-semantics-buys-graph-database-specialist/|website=Datanami|accessdate=27 April 2016|date=14 January 2016}}</ref>\n\n==Products==\n* Anzo Smart Data Platform is a platform for building unified information solutions based on a set of open data standards implemented using [[Semantic Web |Semantic Web Technologies]].<ref>{{cite web|last1=Bertolucci|first1=Jeff|title=Big Data + Semantic Web: Love At First Terabyte? - InformationWeek|url=http://www.informationweek.com/big-data/big-data-analytics/big-data-+-semantic-web-love-at-first-terabyte/d/d-id/1107520?|website=[[InformationWeek]]|accessdate=28 April 2016}}</ref><ref>{{cite web|last1=Shacklett|first1=Mary|title=A start to solving the enterprise data usage problem - TechRepublic|url=http://www.techrepublic.com/article/a-start-to-solving-the-enterprise-data-usage-problem/|website=[[TechRepublic]]|accessdate=28 April 2016}}</ref> It allows IT departments and their business users to quickly and flexibly access all of their diverse data for breakthrough insights.<ref>{{cite web|last1=Lawson|first1=Loraine|title=Cambridge Semantics Offers New Integration Tool|url=http://www.itbusinessedge.com/blogs/integration/cambridge-semantics-offers-new-integration-tool.html|website=IT Business Edge|accessdate=27 April 2016}}</ref><ref>{{cite web|title=Cambridge Semantics Launches Anzo Smart Data Integration|url=http://www.econtentmag.com/Articles/News/News-Item/Cambridge-Semantics-Launches-Anzo-Smart-Data-Integration-98007.htm|website=EContent Magazine|accessdate=27 April 2016|language=en-US|date=3 July 2014}}</ref><ref>{{cite web|title=The time for Smart Data has finally arrived: Cambridge Semantics Inc.|url=http://thesiliconreview.com/magazines/the-time-for-smart-data-has-finally-arrived-cambridge-semantics-inc/|website=The Silicon Review|accessdate=27 April 2016|language=en-US}}</ref><ref>{{cite web|last1=Kutz|first1=Erin|title=Cambridge Semantics, Looking to Put Microsoft Excel "On Steroids," Brings Intelligent Data Sorting to Non-Techies|url=http://www.xconomy.com/boston/2010/07/08/cambridge-semantics-looking-to-put-microsoft-excel-on-steroids-brings-intelligent-data-sorting-to-non-techies/|website=[[Xconomy]]|accessdate=27 April 2016|language=en-US|date=8 July 2010}}</ref><ref>{{cite web|last1=McNamara|first1=Paul|title=Book of Odds opening eyes to new probabilities|url=http://www.networkworld.com/article/2231870/data-center/book-of-odds-opening-eyes-to-new-probabilities.html|website=[[Network World]]|accessdate=28 April 2016}}</ref>\n* Anzo Smart Data Manager\n* Anzo Graph Query Engine\n* Anzo Smart Data Lake\n\n==Awards and recognition==\n\n* Cambridge Semantics named [[Software and Information Industry Association|SIIA]] [[CODiE Award]] 2016 finalist.<ref>{{cite web|title=2016 Finalists|url=https://www.siia.net/codie/2016-Finalists|website=Siia.net|accessdate=27 April 2016}}</ref>\n* Cambridge Semantics named [[KMWorld]]’s 2016 ‘100 Companies That Matter in Knowledge Management’<ref>{{cite web|title=KMWorld 100 COMPANIES That Matter in Knowledge Management|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-COMPANIES-That-Matter-in-Knowledge-Management-109344.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=1 March 2016}}</ref> and [[KMWorld]] Trend-Setting Products of 2015.<ref>{{cite web|last1=McKellar|first1=Hugh|title=KMWorld Trend-Setting Products of 2015|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2015-105783.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=1 September 2015}}</ref>\n* Cambridge Semantics named  2016 Bio-IT World Best of Show People\'s Choice Award Contenders<ref>{{cite web|title=2016 Bio-IT World Best of Show People\'s Choice Award Contenders|url=http://www.bio-itworld.com/2016/3/29/2016-best-of-show-peoples-choice-award-contenders.asp|website=Bio-IT World|accessdate=27 April 2016}}</ref> and 2015 Bio-IT best of show finalist.<ref>{{cite web|title=Bio-IT World Recognizes 2015 Best of Show Winners|url=http://www.bio-itworld.com/2015/4/27/bio-it-world-recognizes-2015-best-of-show-winners.html|website=Bio-IT World|accessdate=27 April 2016}}</ref>\n* CIO Review Recognizes Cambridge Semantics as 2015 Top 20 Tech Solution Provider for [[Pharmaceutical industry|Pharma and Life Sciences Industry]].<ref>{{cite web|title=20  Most Promising Pharma and Life Sciences Tech Solution Providers  20 15|url=http://pharma-life-sciences.cioreview.com/vendors/2015/20special1|website=CIOReview|accessdate=27 April 2016}}</ref><ref>{{cite web|title=Cambridge Semantics:Smart Data Management and Advanced Analytics for Pharma and Life Sciences|url=http://pharma-life-sciences.cioreview.com/vendor/2015/cambridge_semantics|website=CIOReview|accessdate=27 April 2016}}</ref>\n* Anzo Insider Trading Investigation and Surveillance named 2015 [[CODiE Award]] finalist.<ref>{{cite web|title=Finalists - 2015 SIIA CODiE Awards|url=https://www.siia.net/archive/codies/2015/finalists.asp|website=Siia.net|accessdate=27 April 2016}}</ref>\n* Cambridge Semantics Selected as Finalist for 2014 [[MIT Sloan]] CIO Symposium\'s Innovation Showcase.<ref>{{cite web|title=Lead Your Digital Enterprise Mit Sloan Cio|url=http://www.mitcio.com/wp-content/uploads/2015/12/mitcio_2014.pdf|accessdate=27 April 2016}}</ref>\n* Cambridge Semantics named [[Software and Information Industry Association|SIIA]] [[CODiE Award]] 2014 finalist.<ref>{{cite web|title=Finalists - 2014 SIIA CODiE Awards |url=http://archive.siia.net/codies/2014/finalist_detail.asp?ID=3 |website=Siia.net |accessdate=27 April 2016 }}{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n* Cambridge Semantics Win 2013 [[Software and Information Industry Association|SIIA]] [[CODiE Award]] for best business intelligence and analytics solution.<ref>{{cite web|title=2013 CODiE Award Winners|url=http://www.siia.net/archive/codies/2015/pw_2013.asp|website=Siia.net|accessdate=27 April 2016}}</ref>\n* Cambridge Semantics wins [[KMWorld]] 2012 Promise Award.<ref>{{cite web|title=KMWorld Promise Award Winner|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-2012-Promise-and-Reality-award-winners-and-finalists-85829.aspx|website=KMWorld Magazine|accessdate=27 April 2016|language=en-US|date=30 October 2012}}</ref>\n* Cambridge Semantics wins Best of Show at 2012 Bio-IT World Conference.<ref>{{cite web|title=2012 Best of Show Winners|url=http://www.bio-itworld.com/2012/04/26/2012-best-of-show-winners.html|website=Bio-IT World|accessdate=27 April 2016}}</ref>\n\n==References==\n{{reflist|2}}\n\n==External links==\n* [https://www.cambridgesemantics.com/ Official website]\n\n[[Category:Software companies based in Massachusetts]]\n[[Category:Companies established in 2007]]\n[[Category:Data management]]']
['Information repository', '13255720', "{{other uses|Knowledge base}}\n\nAn '''information repository''' is an easy way to deploy a secondary tier of [[data storage device|data storage]] that can comprise multiple, networked data storage technologies running on diverse [[operating system]]s, where data that no longer needs to be in primary storage is protected, classified according to captured [[metadata]], processed, de-duplicated, and then purged, automatically, based on data service level objectives and requirements. In information repositories, data storage resources are virtualized as composite storage sets and operate as a [[Federation (information technology)|federated]] environment.\n\nInformation repositories were developed to mitigate problems arising from [[data proliferation]] and eliminate the need for separately deployed data storage solutions because of the concurrent deployment of diverse storage technologies running diverse operating systems. They feature centralized management for all deployed data storage resources. They are self-contained, support heterogeneous storage resources, support resource management to add, maintain, recycle, and terminate media, track of off-line media, and operate autonomously.\n\n==Automated data management==\nSince one of the main reasons for the implementation of an Information repository is to reduce the maintenance workload placed on IT staff by traditional data storage systems, information repositories are automated. Automation is accomplished via polices that can process data based on time, events, data age, and data content. Policies manage the following:\n*File system space management\n*Irrelevant data elimination (mp3, games, etc.)\n*Secondary storage resource management\nData is processed according to media type, [[Storage virtualization|storage pool]], and [[data storage device|storage technology]].\n\nBecause information repositories are intended to reduce IT staff workload, they are designed to be easy to deploy and offer configuration flexibility, virtually limitless extensibility, redundancy, and reliable failover.\n\n==Data recovery==\nInformation repositories feature robust, client based data search and recovery capabilities that, based on permissions, enable end users to search the information repository, view information repository contents, including data on off-line media, and recover individual files or multiple files to either their original [[Computer network|network]] computer or another network computer.\n\n==References==\n*NGDC Conference: Understand advanced IT infrastructures, Protecting Information: Benefits of a Federated Information Repository as a Secondary Storage Tier. http://www.networkworld.com/ngdc/ \n*SNIA Enterprise Information World 2007 Conference: Benefits of a Federated Information Repository as a Secondary Storage Tier. http://www.enterpriseinformationworld.com/abstracts/benefits_federated_info.htm\n\n{{DEFAULTSORT:Information Repository}}\n[[Category:Information technology management|*]]\n[[Category:Content management systems|*]]\n[[Category:Data management]]\n[[Category:Data security]]\n[[Category:Records management]]"]
['StoredIQ', '51073850', '{{Underlinked|date=July 2016}}\n\n<!-- Don\'t mess with this line! --><!-- Write your article below this line -->\n\'\'\'StoredIQ\'\'\' was a company founded for [[information lifecycle management]] (ILM) of unstructured data. Founded in 2001 as Deepfile<ref>{{cite news|title=Deepfile Comes to the Surface|url=http://www.networkcomputing.com/storage/deepfile-comes-surface/865316998|publisher=Network Computing}}</ref> in [[Austin, Texas]] by Jeff Erramouspe, Jeff Bone, Russell Turpin, Rudy Rouhana, Laura Arbilla and Brett Funderburg.<ref>{{cite news|title=Enterprise file management made easy|url=http://www.networkworld.com/article/2332452/wireless/deepfile.html|publisher=Network World}}</ref> The company changed its name in 2005 to StoredIQ<ref>{{cite news|title=Deep file Becomes StoredIQ|url=http://www.networkcomputing.com/storage/deepfile-becomes-storediq/1788209585|publisher=Network Computing}}</ref> and continued to operate successfully for over a decade until it was acquired in 2012 by IBM.<ref>{{cite web|title=IBM Extends ILG Suite and Big Data Governance with StoredIQ Acquisition|url=http://public.dhe.ibm.com/software/data/sw-library/ecm-programs/Parity_Research_StoredIQ_Whitepaper.pdf|website=IBM}}</ref> It now serves as a platform for IBM\'s information life cycle governance, [[big data]] governance and [[enterprise content management]] technologies.<ref>{{cite web|title=StoredIQ is now an IBM Company|url=https://www-01.ibm.com/software/info/storediq/|website=IBM}}</ref>\n\nStoredIQ was awarded five patents by the USPTO. The first, originally filed in 2003, enabled unstructured data in file systems to be manipulated in a similar way to information stored in databases.<ref>{{cite web|title=Method and apparatus for managing file systems and file-based data storage|url=http://patents.justia.com/assignee/storediq-inc|website=JUSTIA Patents}}</ref> Subsequent patents only added to StoredIQ\'s market dominance by building upon the patented actionable file system with further enhancements specific to Enterprise Policy Management  and expanding the reach of StoredIQ\'s management capability all the way to individual desktops.<ref>{{cite web|title=Patents by Assignee Storediq, Inc.|url=http://patents.justia.com/assignee/storediq-inc|website=JUSTIA Patents}}</ref>\n\nIn 2008 StoredIQ was recognized as "Best in Compliance" by Network Products Guide.<ref>{{cite web|title=StoredIQ Wins Network Products Guide Award For Best In Compliance|url=http://www.datastorageconnection.com/doc/storediq-network-products-best-in-compliance-0001|publisher=Data Storage Connection}}</ref> At the same time, StoredIQ was being recognized as a "Top 5 Provider" by the prestigious Socha-Gelbmann eDiscovery survey.<ref>{{cite web|title=StoredIQ Recognized With "Top 5 Provider" Rating In Socha-Gelbmann eDiscovery Survey|url=http://www.datastorageconnection.com/doc/torediq-ediscovery-survey-storage-0001|publisher=Data Storage Connection}}</ref> This incredible breath of information governance capability is what originally drew the attention of [[EMC Corporation]], StoredIQ\'s first potential acquirer. Initially a strategic investor in StoredIQ, many experts{{Who|date=August 2016}} predicted an inevitable acquisition. However, the company shunned their first suitor; leaving EMC to acquire a competitor.<ref>{{cite web|title=EMC Acquires Kazeon, Stiffs StoredIQ|url=http://www.informationweek.com/software/information-management/emc-acquires-kazeon-stiffs-storediq/d/d-id/1082836?|publisher=Information Week}}</ref>\n\nThe company published a whitepaper titled \'\'The Truth About Big Data\'\'. This promotion combined with StoredIQ\'s patented, technology led to [[IBM]] selecting StoredIQ as the basis for some products.<ref>{{cite news|last1=Butta|first1=Tom|title=The Truth Behind IBM’s Plans to Acquire Big Data Company, StoredIQ|url=http://www.huffingtonpost.com/entry/ibm-storediq_b_2377339|publisher=Huffington Post|date=2012-12-31}}</ref>\n\n==References==\n{{reflist}}\n<!-- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. -->\n\n\n\n[[Category:Companies established in 2001]]\n[[Category:Companies based in Austin, Texas]]\n[[Category:Information technology management]]\n[[Category:Data management]]\n[[Category:Data warehousing]]']
['Atomicity (database systems)', '373991', "{{Other uses|Atomicity (disambiguation)}}\n\nIn [[database system]]s, '''atomicity''' (or '''atomicness'''{{Citation needed|date=February 2016}}; from [[Greek language|Greek]] ''atomos'', ''undividable'') is one of the [[ACID]] [[database transaction|transaction]] properties. An '''atomic transaction''' is an ''indivisible'' and ''irreducible'' series of database operations such that either ''all'' occur, or ''nothing'' occurs.<ref>{{cite web\n| accessdate = 2011-03-23\n| location = http://www.webopedia.com/\n| publisher = Webopedia\n| title = atomic operation\n| quote = An operation during which a processor can simultaneously read a location and write it in the same bus operation. This prevents any other processor or I/O device from writing or reading memory until the operation is complete.\n| url = http://www.webopedia.com/TERM/A/atomic_operation.html}}</ref> A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. As a consequence, the transaction cannot be observed to be in progress by another database client. At one moment in time, it has not yet happened, and at the next it has already occurred in whole (or nothing happened if the transaction was cancelled in progress).\n\nAn example of an atomic transaction is a monetary transfer from bank account A to account B. It consists of two operations, withdrawing the money from account A and saving it to account B. Performing these operations in an atomic transaction ensures that the database remains in a [[Data consistency|consistent state]], that is, money is not lost nor created if either of those two operations fail.<ref>{{cite web\n| url = http://archive.oreilly.com/pub/a/onjava/2001/11/07/atomic.html\n| title = Atomic File Transactions, Part 1 - O'Reilly Media\n| last = Amsterdam\n| first = Jonathan\n| website = archive.oreilly.com\n| access-date = 2016-02-28\n}}</ref>\n\n==Orthogonality==\nAtomicity does not behave completely [[Orthogonal (computing)|orthogonally]] with regard to the other [[ACID]] properties of the transactions. For example, [[Isolation (database systems)|isolation]] relies on atomicity to roll back changes in the event of isolation failures such as [[deadlock]]; [[Consistency (database systems)|consistency]] also relies on rollback in the event of a consistency-violation by an illegal transaction. Finally, atomicity itself relies on [[Durability (database systems)|durability]] to ensure the atomicity of transactions even in the face of external failures.\n\nAs a result of this, failure to detect errors and roll back the enclosing transaction may cause failures of isolation and consistency.\n\n==Implementation==\nTypically, systems implement Atomicity by providing some mechanism to indicate which transactions have started and which finished; or by keeping a copy of the data before any changes occurred ([[read-copy-update]]).  Several filesystems have developed methods for avoiding the need to keep multiple copies of data, using journaling (see [[journaling file system]]). Databases usually implement this using some form of logging/journaling to track changes. The system synchronizes the logs (often the [[metadata]]) as necessary once the actual changes have successfully taken place. Afterwards, crash recovery simply ignores incomplete entries. Although implementations vary depending on factors such as concurrency issues, the principle of atomicity — i.e. complete success or complete failure — remain.\n\nUltimately, any application-level implementation relies on [[operating system|operating-system]] functionality.  At the file-system level, [[POSIX]]-compliant systems provide [[system call]]s such as <code>open(2)</code> and <code>flock(2)</code> that allow applications to atomically open or lock a file. At the process level, [[POSIX Threads]] provide adequate synchronization primitives.\n\nThe hardware level requires [[linearizability|atomic operations]] such as [[Test-and-set]], [[Fetch-and-add]], [[Compare-and-swap]], or [[Load-Link/Store-Conditional]], together with [[memory barrier]]s.  Portable operating systems cannot simply block interrupts to implement synchronization, since hardware that lacks actual concurrent execution such as [[hyper-threading]] or [[multi-processing]] is now extremely rare.{{Citation needed|date=December 2016}}\n\nIn [[NoSQL (concept)|NoSQL]] [[data store]]s with eventual consistency, the atomicity is also weaker specified than in relational database systems, and exists only in ''row''s (i.e. [[Column family|column families]]).<ref>{{cite web\n| accessdate = 2011-03-23\n| author = Olivier Mallassi\n| date = 2010-06-09\n| location = http://blog.octo.com/en/\n| publisher = OCTO Talks!\n| title = Let’s play with Cassandra… (Part 1/3)\n| quote = Atomicity is also weaker than what we are used to in the relational world. Cassandra guarantees atomicity within a <code>ColumnFamily</code> so for all the columns of a row.\n| url = http://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13/}}</ref>\n\n==See also==\n* [[Atomic operation]]\n* [[Transaction processing]]\n* [[Long-running transaction]]\n* [[Read-copy-update]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Atomicity (Database Systems)}}\n[[Category:Data management]]\n[[Category:Transaction processing]]"]
['Metadata', '18933632', '{{pp-move-indef|small=yes}}\n{{use dmy dates|date=October 2016}}\n[[File:Schlagwortkatalog.jpg|thumb|200px|In the 2010s, metadata typically refers to digital forms; however, even traditional card catalogues from the 1960s and 1970s are an example of metadata, as the cards contain information about the books in the library (author, title, subject, etc.).]]\n\n\'\'\'Metadata\'\'\' is "[[data]] [information] that provides information about other data".<ref>http://www.merriam-webster.com/dictionary/metadata</ref> Three distinct types of metadata exist: \'\'\'descriptive metadata\'\'\', \'\'\'structural metadata\'\'\', and \'\'\'administrative metadata\'\'\'.<ref name="Metadata Basics Outline">{{cite web | url=http://marciazeng.slis.kent.edu/metadatabasics/types.htm | title=Metadata Types and Functions | publisher=NISO | date=2004 | accessdate=5 October 2016 | author=Zeng, Marcia}}</ref>\n\n* Descriptive metadata describes a resource for purposes such as discovery and identification. It can include elements such as title, abstract, author, and keywords.\n* Structural metadata is metadata about containers of metadata and indicates how compound objects are put together, for example, how pages are ordered to form chapters.\n* Administrative metadata provides information to help manage a resource, such as when and how it was created, file type and other technical information, and who can access it.<ref name="Understanding Metadata (2)">{{cite book | url=http://www.niso.org/publications/press/UnderstandingMetadata.pdf | title=Understanding Metadata | publisher=NISO Press | author=National Information Standards Organization (NISO) | year=2001 | isbn=1-880124-62-9|page=1}}</ref>\n\n== History ==\nMetadata was traditionally used in the [[library catalog|card catalogs]] of [[library|libraries]] until the 1980s, when libraries converted their catalog data to digital databases. In the 2000s, as digital formats are becoming the prevalent way of storing data and information, metadata is also used to describe digital data using [[metadata standards]].\n\nThere are different metadata standards for each different discipline (e.g., [[museum]] collections, [[digital audio file]]s, [[website]]s, etc.). Describing the [[Content (media)|contents]] and [[Context (computing)|context]] of data or [[computer file|data files]] increases its usefulness. For example, a [[web page]] may include metadata specifying what software language the page is written in (e.g., HTML), what tools were used to create it, what subjects the page is about, and where to find more information about the subject. This metadata can automatically improve the reader\'s experience and make it easier for users to find the web page online.<ref name="Practices in Using Metadata">{{cite web | url=http://www.library.illinois.edu/dcc/bestpractices/chapter_11_structuralmetadata.html | title=Best Practices for Structural Metadata | publisher=University of Illinois | date=15 December 2010 | accessdate=17 June 2016}}</ref> A [[CD]] may include metadata providing information about the musicians, singers and songwriters whose work appears on the disc.\n\nA principal purpose of metadata is to help users find relevant information and discover resources. Metadata also helps to organize electronic resources, provide digital identification, and support the archiving and preservation of resources. Metadata assists users in resource discovery by "allowing resources to be found by relevant criteria, identifying resources, bringing similar resources together, distinguishing dissimilar resources, and giving location information."<ref name = Understanding_Metadata/> Metadata of telecommunication activities including [[Internet]] traffic is very widely collected by various national governmental organizations. This data is used for the purposes of [[traffic analysis]] and can be used for mass [[surveillance]].<ref>https://www.schneier.com/essays/archives/2014/03/metadata_surveillanc.html</ref>\n\nIn many countries, the metadata relating to emails, telephone calls, web pages, video traffic, IP connections and cell phone locations are routinely stored by government organizations.<ref name="NSA_Watching">http://www.washingtonsblog.com/2014/03/nsa-recorded-every-single-call-one-country-country-america.html</ref>\n\n== Definition ==\nMetadata means "data about data". Although the "meta" prefix (from the [[Greek language|Greek]] [[preposition]] and [[prefix]] μετά-) means "after" or "beyond", it is used to mean "about" in [[epistemology]]. Metadata is defined as the data providing information about one or more aspects of the data; it is used to summarize basic information about data which can make tracking and working with specific data easier.<ref>{{cite web\n| title = A Guardian Guide to your Metadata\n| website = [[theguardian.com]]\n| publisher = [[Guardian News and Media Limited]]\n| date = 12 June 2013\n| url = https://www.theguardian.com/technology/interactive/2013/jun/12/what-is-metadata-nsa-surveillance#meta=0000000\n}}</ref> Some examples include:\n* Means of creation of the data\n* Purpose of the data\n* Time and date of creation\n* Creator or author of the data\n* Location on a [[computer network]] where the data was created\n* [[Technical standard|Standards]] used\n* File size\n\nFor example, a [[digital image]] may include metadata that describes how large the picture is, the color depth, the image resolution, when the image was created, the shutter speed, and other data.<ref>{{cite web|url=http://www.adeoimaging.com |title=ADEO Imaging: TIFF Metadata |accessdate=2013-05-20}}</ref> A text document\'s metadata may contain information about how long the document is, who the author is, when the document was written, and a short summary of the document. Metadata within web pages can also contain descriptions of page content, as well as key words linked to the content.<ref name="Rouse, M (2014)">{{cite web\n| last = Rouse\n| first = Margaret\n| title = Metadata\n| work = WhatIs\n| publisher = TechTarget\n| date = July 2014\n| url = http://whatis.techtarget.com/definition/metadata\n}}</ref> These links are often called "Metatags", which were used as the primary factor in determining order for a web search until the late 1990s.<ref name="Rouse, M (2014)"/> The reliance of metatags in web searches was decreased in the late 1990s because of "keyword stuffing".<ref name="Rouse, M (2014)"/> Metatags were being largely misused to trick search engines into thinking some websites had more relevance in the search than they really did.<ref name="Rouse, M (2014)"/>\n\nMetadata can be stored and managed in a [[database]], often called a [[metadata registry]] or [[metadata repository]].<ref>Hüner, K.; Otto, B.; Österle, H.: Collaborative management of business metadata, in: \'\'International Journal of Information Management\'\', 2011</ref> However, without context and a point of reference, it might be impossible to identify metadata just by looking at it.<ref>{{cite web|url=http://www.bls.gov/ore/pdf/st000010.pdf |title=Metadata Standards And Metadata Registries: An Overview |format=PDF |accessdate=2011-12-23}}</ref> For example: by itself, a database containing several numbers, all 13 digits long could be the results of calculations or a list of numbers to plug into an equation - without any other context, the numbers themselves can be perceived as the data. But if given the context that this database is a log of a book collection, those 13-digit numbers may now be identified as [[ISBN]]s - information that refers to the book, but is not itself the information within the book. The term "metadata" was coined in 1968 by Philip Bagley, in his book "Extension of Programming Language Concepts" where it is clear that he uses the term in the ISO 11179 "traditional" sense, which is "structural metadata" i.e. "data about the containers of data"; rather than the alternate sense "content about individual instances of data content" or metacontent, the type of data usually found in library catalogues.<ref name=Bagley>{{Cite journal\n|author=Philip Bagley\n|title=Extension of programming language concepts\n|date=November 1968\n| url = http://www.dtic.mil/dtic/tr/fulltext/u2/680815.pdf\n|publisher=University City Science Center\n|location=Philadelphia\n}}</ref><ref>"The notion of "metadata" introduced by Bagley". {{Cite journal\n | last = Solntseff\n | first = N+1\n | last2 = Yezerski\n | first2 = A\n | year = 1974\n | title = A survey of extensible programming languages\n | series = Annual Review in Automatic Programming\n | publisher = Elsevier Science Ltd\n | volume = 7\n | pages = 267–307\n | doi = 10.1016/0066-4138(74)90001-9\n}}</ref> Since then the fields of information management, information science, information technology, librarianship, and [[GIS]] have widely adopted the term. In these fields the word \'\'metadata\'\' is defined as "data about data".<ref name=NISO >{{Cite book\n| last = NISO\n| authorlink =NISO\n| title = Understanding Metadata\n| publisher = NISO Press\n| url = http://www.niso.org/publications/press/UnderstandingMetadata.pdf\n| isbn = 1-880124-62-9\n| accessdate = 5 January 2010 }}\n</ref>{{page needed|date=November 2016}} While this is the generally accepted definition, various disciplines have adopted their own more specific explanation and uses of the term.\n\n== Types ==\nWhile the metadata application is manifold, covering a large variety of fields, there are specialized and well-accepted models to specify types of metadata. [[Francis Bretherton|Bretherton]] & Singley (1994) distinguish between two distinct classes: structural/control metadata and guide metadata.<ref>{{Cite conference\n| first1 = F. P. | last1 = Bretherton | author1-link = Francis Bretherton\n|first2 = P.T. | last2 = Singley\n| title = Metadata: A User\'s View, Proceedings of the International Conference on Very Large Data Bases (VLDB)\n| pages = 1091–1094\n| publisher =\n| year = 1994}}\n</ref> \'\'Structural metadata\'\' describes the structure of database objects such as tables, columns, keys and indexes. \'\'Guide metadata\'\' helps humans find specific items and are usually expressed as a set of keywords in a natural language. According to [[Ralph Kimball]] metadata can be divided into 2 similar categories: technical metadata and business metadata. \'\'Technical metadata\'\' corresponds to internal metadata, and \'\'business metadata\'\' corresponds to external metadata. Kimball adds a third category, \'\'process metadata\'\'. On the other hand, NISO distinguishes among three types of metadata: descriptive, structural, and administrative.<ref name=NISO/>\n\n\'\'Descriptive metadata\'\' is typically used for discovery and identification, as information to search and locate an object, such as title, author, subjects, keywords, publisher. \'\'Structural metadata\'\' describes how the components of an object are organized. An example of structural metadata would be how pages are ordered to form chapters of a book. Finally, \'\'administrative metadata\'\' gives information to help manage the source. Administrative metadata refers to the technical information, including file type, or when and how the file was created. Two sub-types of administrative metadata are rights management metadata and preservation metadata. \'\'Rights management metadata\'\' explains intellectual property rights, while \'\'preservation metadata\'\' contains information to preserve and save a resource.<ref name = Understanding_Metadata>{{cite book|last=National Information Standards Organization|title=Understanding Metadata|year=2004|publisher=NISO Press|location=Bethesda, MD|isbn=1-880124-62-9|url=http://www.niso.org/publications/press/UnderstandingMetadata.pdf |author2=Rebecca Guenther |author3=Jaqueline Radebaugh|accessdate=2 April 2014}}</ref>{{page needed|date=November 2016}}\n\n== Structures ==\nMetadata (metacontent) or, more correctly, the vocabularies used to assemble metadata (metacontent) statements, is typically structured according to a standardized concept using a well-defined metadata scheme, including: [[metadata standards]] and [[Metadata modeling|metadata models]]. Tools such as [[Controlled vocabulary|controlled vocabularies]], [[Taxonomy (general)|taxonomies]], [[Thesaurus (information retrieval)|thesauri]], [[Data Dictionary|data dictionaries]], and [[Metadata registry|metadata registries]] can be used to apply further standardization to the metadata. Structural metadata commonality is also of paramount importance in [[data model]] development and in [[database design]].\n\n=== Syntax ===\nMetadata (metacontent) syntax refers to the rules created to structure the fields or elements of metadata (metacontent).<ref>{{cite web\n| last = Cathro\n| first = Warwick\n| authorlink =\n| title = Metadata: an overview\n| year = 1997\n| url = http://www.nla.gov.au/nla/staffpaper/cathro3.html\n| accessdate = 6 January 2010\n}}</ref> A single metadata scheme may be expressed in a number of different markup or programming languages, each of which requires a different syntax. For example, Dublin Core may be expressed in plain text, [[HTML]], [[XML]], and [[Resource Description Framework|RDF]].<ref>{{cite web\n| last = DCMI\n| authorlink =Dublin_Core_Metadata_Initiative\n| title = Semantic Recommendations\n| date =5 October 2009\n| url = http://dublincore.org/specifications/\n| accessdate = 6 January 2010\n}}</ref>\n\nA common example of (guide) metacontent is the bibliographic classification, the subject, the [[List of Dewey Decimal classes|Dewey Decimal class number]]. There is always an implied statement in any "classification" of some object. To classify an object as, for example, Dewey class number 514 (Topology) (i.e. books having the number 514 on their spine) the implied statement is: "<nowiki><book><subject heading><514></nowiki>. This is a subject-predicate-object triple, or more importantly, a class-attribute-value triple. The first two elements of the triple (class, attribute) are pieces of some structural metadata having a defined semantic. The third element is a value, preferably from some controlled vocabulary, some reference (master) data. The combination of the metadata and master data elements results in a statement which is a metacontent statement i.e. "metacontent = metadata + master data". All of these elements can be thought of as "vocabulary". Both metadata and master data are vocabularies which can be assembled into metacontent statements. There are many sources of these vocabularies, both meta and master data: UML, EDIFACT, XSD, Dewey/UDC/LoC, SKOS, ISO-25964, Pantone, Linnaean Binomial Nomenclature, etc. Using controlled vocabularies for the components of metacontent statements, whether for indexing or finding, is endorsed by [[ISO 25964]]: "If both the indexer and the searcher are guided to choose the same term for the same concept, then relevant documents will be retrieved."<ref>https://www.iso.org/obp/ui/#iso:std:iso:25964:-1:ed-1:v1:en</ref> This is particularly relevant when considering search engines of the internet, such as Google. The process indexes pages then matches text strings using its complex algorithm; there is no intelligence or "inferencing" occurring, just the illusion thereof.\n\n=== Hierarchical, linear and planar schemata ===\nMetadata schemata can be hierarchical in nature where relationships exist between metadata elements and elements are nested so that parent-child relationships exist between the elements.\nAn example of a hierarchical metadata schema is the [[Learning object metadata|IEEE LOM]] schema, in which metadata elements may belong to a parent metadata element.\nMetadata schemata can also be one-dimensional, or linear, where each element is completely discrete from other elements and classified according to one dimension only.\nAn example of a linear metadata schema is the [[Dublin Core Metadata Initiative|Dublin Core]] schema, which is one dimensional.\nMetadata schemata are often two dimensional, or planar, where each element is completely discrete from other elements but classified according to two orthogonal dimensions.<ref>{{cite web\n| title = Types of Metadata\n| publisher = [[University of Melbourne]]\n| date = 15 August 2006\n| url = http://www.infodiv.unimelb.edu.au/metadata/add_info.html\n| accessdate = 6 January 2010\n| archiveurl = https://web.archive.org/web/20091024112353/http://www.infodiv.unimelb.edu.au/metadata/add_info.html\n| archivedate = 2009-10-24}}</ref>\n\n=== Hypermapping ===\nIn all cases where the metadata schemata exceed the planar depiction, some type of [[hypermap]]ping is required to enable display and view of metadata according to chosen aspect and to serve special views. Hypermapping frequently applies to layering of geographical and geological information overlays.<ref>{{cite web |url=http://www.isprs.org/proceedings/XXXII/part4/kuebler51.pdf |title=THE DESIGN AND DEVELOPMENT OF A GEOLOGIC HYPERMAP PROTOTYPE |first1=Stefanie |last1=Kübler |first2=Wolfdietrich |last2=Skala |first3=Agnès |last3=Voisard}}</ref>\n\n=== Granularity ===\nThe degree to which the data or metadata is structured is referred to as its [[Data granularity|"granularity"]]. "Granularity" refers to how much detail is provided. Metadata with a high granularity allows for deeper, more detailed, and more structured information and enables greater levels of technical manipulation. A lower level of granularity means that metadata can be created for considerably lower costs but will not provide as detailed information. The major impact of granularity is not only on creation and capture, but moreover on maintenance costs. As soon as the metadata structures become outdated, so too is the access to the referred data. Hence granularity must take into account the effort to create the metadata as well as the effort to maintain it.\n\n== Standards ==\nInternational standards apply to metadata. Much work is being accomplished in the national and international standards communities, especially [[ANSI]] (American National Standards Institute) and [[International Organization for Standardization|ISO]] (International Organization for Standardization) to reach consensus on standardizing metadata and registries. The core metadata registry standard is [[International Organization for Standardization|ISO]]/[[International Electrotechnical Commission|IEC]] 11179 Metadata Registries (MDR), the framework for the standard is described in ISO/IEC 11179-1:2004.<ref>{{cite web\n  |url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=39438\n  |title=ISO/IEC 11179-1:2004 Information technology - Metadata registries (MDR) - Part 1: Framework\n  |publisher=Iso.org |date=2009-03-18 |accessdate=2011-12-23\n}}</ref> A new edition of Part 1 is in its final stage for publication in 2015 or early 2016. It has been revised to align with the current edition of Part 3, ISO/IEC 11179-3:2013<ref>{{cite web\n  |url=http://standards.iso.org/ittf/PubliclyAvailableStandards/c050340_ISO_IEC_11179-3_2013.zip\n  |title=ISO/IEC 11179-3:2013 Information technology-Metadata registries - Part 3: Registry metamodel and basic attributes\n  |publisher=iso.org|date=2014}}</ref> which extends the MDR to support registration of Concept Systems.\n(see [[ISO/IEC 11179]]). This standard specifies a schema for recording both the meaning and technical structure of the data for unambiguous usage by humans and computers. ISO/IEC 11179 standard refers to metadata as information objects about data, or "data about data". In ISO/IEC 11179 Part-3, the information objects are data about Data Elements, Value Domains, and other reusable semantic and representational information objects that describe the meaning and technical details of a data item. This standard also prescribes the details for a metadata registry, and for registering and administering the information objects within a Metadata Registry. ISO/IEC 11179 Part 3 also has provisions for describing compound structures that are derivations of other data elements, for example through calculations, collections of one or more data elements, or other forms of derived data. While this standard describes itself originally as a "data element" registry, its purpose is to support describing and registering metadata content independently of any particular application, lending the descriptions to being discovered and reused by humans or computers in developing new applications, databases, or for analysis of data collected in accordance with the registered metadata content. This standard has become the general basis for other kinds of metadata registries, reusing and extending the registration and administration portion of the standard.\n\nThe Geospatial community has a tradition of specialized [[geospatial metadata]] standards, particularly building on traditions of map- and image-libraries and catalogues. Formal metadata is usually essential for geospatial data, as common text-processing approaches are not applicable.\n\nThe [[Dublin Core]] metadata terms are a set of vocabulary terms which can be used to describe resources for the purposes of discovery. The original set of 15 classic<ref>{{cite web|url=http://dublincore.org/specifications/ |title=DCMI Specifications |publisher=Dublincore.org |date=2009-12-14 |accessdate=2013-08-17}}</ref> metadata terms, known as the Dublin Core Metadata Element Set<ref>{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |accessdate=2013-08-17}}</ref> are endorsed in the following standards documents:\n* IETF RFC 5013<ref>{{cite web |url= http://www.ietf.org/rfc/rfc5013.txt |title=The Dublin Core Metadata Element Set |author=J. Kunze, T. Baker |work=ietf.org |year=2007 |accessdate=17 August 2013}}</ref>\n* ISO Standard 15836-2009<ref>{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnumber=52142 |title=ISO 15836:2009 - Information and documentation - The Dublin Core metadata element set |publisher=Iso.org |date=2009-02-18 |accessdate=2013-08-17}}</ref>\n* NISO Standard Z39.85.<ref>{{cite web|url=http://www.niso.org/kst/reports/standards?step=2&gid=None&project_key=9b7bffcd2daeca6198b4ee5a848f9beec2f600e5 |title=NISO Standards - National Information Standards Organization |publisher=Niso.org |date=2007-05-22 |accessdate=2013-08-17}}</ref>\n\nAlthough not a standard, [[Microformat]] (also mentioned in the section [[Metadata#Metadata on the Internet|metadata on the internet]] below) is a web-based approach to semantic markup which seeks to re-use existing HTML/XHTML tags to convey metadata. Microformat follows XHTML and HTML standards but is not a standard in itself. One advocate of microformats, [[Tantek Çelik]], characterized a problem with alternative approaches: {{cquote|Here\'s a new language we want you to learn, and now you need to output these additional files on your server. It\'s a hassle. (Microformats) lower the barrier to entry.<ref name="Wharton000">{{cite web |title=What\'s the Next Big Thing on the Web? It May Be a Small, Simple Thing -- Microformats|work=Knowledge@Wharton |publisher=[[Wharton School of the University of Pennsylvania]] |date=2005-07-27 |url=http://knowledge.wharton.upenn.edu/index.cfm?fa=printArticle&ID=1247}}</ref>}}\n\n== Use ==\n\n=== Photographs ===\nMetadata may be written into a [[digital photo]] file that will identify who owns it, copyright and contact information, what brand or model of camera created the file, along with exposure information (shutter speed, f-stop, etc.) and descriptive information, such as keywords about the photo, making the file or image searchable on a computer and/or the Internet. Some metadata is created by the camera and some is input by the photographer and/or software after downloading to a computer. Most digital cameras write metadata about model number, shutter speed, etc., and some enable you to edit it;<ref>{{cite web|publisher=gurucamera.com|title=How To Copyright Your Photos With Metadata|url=https://gurucamera.com/copyright-photos-metadata/|work=Guru Camera}}</ref> this functionality has been available on most Nikon DSLRs since the [[Nikon D3]], on most new Canon cameras since the [[Canon EOS 7D]], and on most Pentax DSLRs since the Pentax K-3. Metadata can be used to make organizing in post-production easier with the use of key-wording. Filters can be used to analyze a specific set of photographs and create selections on criteria like rating or capture time.\n\nPhotographic Metadata Standards are governed by organizations that develop the following standards. They include, but are not limited to:\n* [[IPTC Information Interchange Model]] IIM (International Press Telecommunications Council),\n* [[International Press Telecommunications Council|IPTC]] Core Schema for XMP\n* [[Extensible Metadata Platform|XMP]] – Extensible Metadata Platform (an ISO standard)\n* [[Exchangeable image file format|Exif]] – Exchangeable image file format, Maintained by CIPA (Camera & Imaging Products Association) and published by JEITA (Japan Electronics and Information Technology Industries Association)\n* [[Dublin Core]] (Dublin Core Metadata Initiative – DCMI)\n* PLUS (Picture Licensing Universal System).\n* [http://www.loc.gov/standards/vracore/schemas.html VRA Core] (Visual Resource Association)<ref>{{cite web|title=VRA Core Support Pages|url=http://core.vraweb.org|website=Visual Resource Association Foundation|publisher=Visual Resource Association Foundation|accessdate=27 February 2016}}</ref>\n\n=== Telecommunications ===\nInformation on the times, origins and destinations of phone calls, electronic messages, instant messages and other modes of telecommunication, as opposed to message content, is another form of metadata. Bulk collection of this [[call detail record]] metadata by intelligence agencies has proven controversial after disclosures by [[Edward Snowden]] Intelligence agencies such as the NSA are keeping online metadata of millions of internet user for up to a year, regardless of whether or not they are persons of interest to the agency.\n\n=== Video ===\nMetadata is particularly useful in video, where information about its contents (such as transcripts of conversations and text descriptions of its scenes) is not directly understandable by a computer, but where efficient search of the content is desirable. There are two sources in which video metadata is derived: (1) operational gathered metadata, that is information about the content produced, such as the type of equipment, software, date, and location; (2) human-authored metadata, to improve search engine visibility, discoverability, audience engagement, and providing advertising opportunities to video publishers.<ref>{{cite web\n| last = Webcase\n| first = Weblog\n| authorlink =\n| title = Examining video file metadata\n| year = 2011\n| url = http://veresoftware.com/blog/?p=364\n| accessdate = 25 November 2015\n}}</ref> In today\'s society most professional video editing software has access to metadata. Avid\'s MetaSync and Adobe\'s Bridge are two prime examples of this.<ref>{{cite web\n| last = Oak Tree Press\n| authorlink =\n| title = Metadata for Video\n| year = 2011\n| url = http://veresoftware.com/blog/?p=364\n| accessdate = 25 November 2015\n}}</ref>\n\n=== Web pages ===\n[[Web page]]s often include metadata in the form of [[Meta element|meta tags]]. Description and keywords in meta tags are commonly used to describe the Web page\'s content. Meta elements also specify page description, key words, authors of the document, and when the document was last modified.<ref name="Rouse, M (2014)"/> Web page metadata helps search engines and users to find the types of web pages they are looking for.\n\n== Creation ==\nMetadata can be created either by automated information processing or by manual work. Elementary metadata captured by computers can include information about when an object was created, who created it, when it was last updated, file size, and file extension. In this context an \'\'object\'\' refers to any of the following:\n\n* A physical item such as a book, CD, DVD, a paper map, chair, table, flower pot, etc.\n* An electronic file such as a digital image, digital photo, electronic document, program file, database table, etc.\n\n=== Data virtualization ===\n{{main article|Data virtualization}}\nData virtualization has emerged in the 2000s as the new software technology to complete the virtualization "stack" in the enterprise. Metadata is used in data virtualization servers which are enterprise infrastructure components, alongside database and application servers. Metadata in these servers is saved as persistent repository and describe [[business object]]s in various enterprise systems and applications. Structural metadata commonality is also important to support data virtualization.\n\n=== Statistics and census services ===\nStandardization work has had a large impact on efforts to build metadata systems in the statistical community{{Citation needed|date=May 2013}}. Several metadata standards{{Which|date=May 2013}} are described, and their importance to statistical agencies is discussed. Applications of the standards{{Which|date=May 2013}} at the Census Bureau, Environmental Protection Agency, Bureau of Labor Statistics, Statistics Canada, and many others are described{{Citation needed|date=May 2013}}. Emphasis is on the impact a metadata registry can have in a statistical agency.\n\n=== Library and information science ===\n\nMetadata has been used in various ways as a means of cataloging items in libraries in both digital and analog format. Such data helps classify, aggregate, identify, and locate a particular book, DVD, magazine or any object a library might hold in its collection. Until the 1980s, many library catalogues used 3x5 inch cards in file drawers to display a book\'s title, author, subject matter, and an abbreviated [[Alphanumeric|alpha-numeric]] string ([[call number]]) which indicated the physical location of the book within the library\'s shelves. The [[Dewey Decimal Classification|Dewey Decimal System]] employed by libraries for the classification of library materials by subject is an early example of metadata usage. Beginning in the 1980s and 1990s, many libraries replaced these paper file cards with computer databases. These computer databases make it much easier and faster for users to do keyword searches. Another form of older metadata collection is the use by US Census Bureau of what is known as the "Long Form." The Long Form asks questions that are used to create demographic data to find patterns of distribution.<ref>{{cite web\n| title = AGLS Metadata Element Set - Part 2: Usage Guide - A non-technical guide to using AGLS metadata for describing resources\n| author = National Archives of Australia\n| year = 2002\n| url = http://www.naa.gov.au/records-management/publications/agls-element.aspx\n| accessdate = 17 March 2010}}\n</ref> [[library|Libraries]] employ metadata in [[library catalog]]ues, most commonly as part of an [[Library management system|Integrated Library Management System]]. Metadata is obtained by [[Library cataloguing#Cataloging rules|cataloguing]] resources such as books, periodicals, DVDs, web pages or digital images. This data is stored in the integrated library management system, [[Library management system|ILMS]], using the [[MARC standards|MARC]] metadata standard. The purpose is to direct patrons to the physical or electronic location of items or areas they seek as well as to provide a description of the item/s in question.\n\nMore recent and specialized instances of library metadata include the establishment of [[Digital library|digital libraries]] including [[eprint|e-print]] repositories and digital image libraries. While often based on library principles, the focus on non-librarian use, especially in providing metadata, means they do not follow traditional or common cataloging approaches. Given the custom nature of included materials, metadata fields are often specially created e.g. taxonomic classification fields, location fields, keywords or copyright statement. Standard file information such as file size and format are usually automatically included.<ref name=solodovnik>{{cite journal | last1 = Solodovnik | first1 = Iryna | year = 2011 | title = Metadata issues in Digital Libraries: key concepts and perspectives | journal = [[JLIS.it|JLIS.it: Italian Journal of Library, Archives and Information Science]] | volume = 2 | issue = 2 | publisher = University of Florence | doi = 10.4403/jlis.it-4663 | url = http://leo.cilea.it/index.php/jlis/article/view/4663 | accessdate = 29 June 2013}}</ref> Library operation has for decades been a key topic in efforts toward [[international standardization]]. Standards for metadata in digital libraries include [[Dublin Core]], [[Metadata Encoding and Transmission Standard|METS]], [[Metadata Object Description Schema|MODS]], [[Data Documentation Initiative|DDI]], [[Digital object identifier|DOI]], [[Uniform Resource Name|URN]], [[Preservation Metadata: Implementation Strategies|PREMIS]] schema, [[Ecological Metadata Language|EML]], and [[Protocol for Metadata Harvesting|OAI-PMH]]. Leading libraries in the world give hints on their metadata standards strategies.<ref>{{cite web |author=Library of Congress Network Development and MARC Standards Office |url=http://www.loc.gov/standards/metadata.html |title=Library of Congress Washington DC on metadata |publisher=Loc.gov |date=2005-09-08 |accessdate=2011-12-23}}</ref><ref>{{cite web |url=http://www.dnb.de/DE/Netzpublikationen/Ablieferung/MetadatenKernset/metadatenkernset_node.html |title=Deutsche Nationalbibliothek Frankfurt on metadata}}</ref>\n\n=== In museums ===\nMetadata in a museum context is the information that trained cultural documentation specialists, such as [[archivist]]s, [[librarian]]s, museum [[registrar (museum)|registrar]]s and [[curator]]s, create to index, structure, describe, identify, or otherwise specify works of art, architecture, cultural objects and their images.<ref name=":0">{{Cite journal |last=Zange|first=Charles S.|date=31 January 2015 |title=Community makers, major museums, and the Keet S\'aaxw: Learning about the role of museums in interpreting cultural objects |url=http://mw2015.museumsandtheweb.com/paper/community-makers-major-museums-and-the-keet-saaxw-learning-about-the-role-of-museums-in-interpreting-cultural-objects/ |publisher=Museums and the Web }}</ref><ref name=":1">{{Cite book |title=Cataloging cultural objects: a guide to describing cultural works and their images. Visual Resources Association |last=Baca |first=Murtha |publisher=Visual Resources Association |year=2006 |isbn=|location=|pages=}}</ref>{{page needed|date=November 2016}}<ref name=":2">{{Cite book|title=Introduction to Metadata: Second Edition. Los Angeles: Getty Information Institute|last=Baca|first=Murtha|publisher=Getty Information Institute|year=2008|isbn=|location=Los Angeles|pages=}}</ref>{{page needed|date=November 2016}} Descriptive metadata is most commonly used in museum contexts for object identification and resource recovery purposes.<ref name=":1" />\n\n==== Usage ====\nMetadata is developed and applied within collecting institutions and museums in order to:\n* Facilitate resource discovery and execute search queries.<ref name=":2" />\n* Create digital archives that store information relating to various aspects of museum collections and cultural objects, and serves for archival and managerial purposes.<ref name=":2" />\n* Provide public audiences access to cultural objects through publishing digital content online.<ref name=":1" /><ref name=":2" />\n\n==== Standards ====\nMany museums and cultural heritage centers recognize that given the diversity of art works and cultural objects, no single model or standard suffices to describe and catalogue cultural works.<ref name=":0" /><ref name=":1" /><ref name=":2" /> For example, a sculpted Indigenous artifact could be classified as an artwork, an archaeological artifact, or an Indigenous heritage item. The early stages of standardization in archiving, description and cataloging within the museum community began in the late 1990s with the development of standards such as [[Categories for the Description of Works of Art (CDWA)]], Spectrum, the [[Conceptual Reference Model (CIDOC)]], [[Cataloging Cultural Objects (CCO)]] and the [[CDWA Lite XML schema]].<ref name=":1" /> These standards use [[HTML]] and [[XML]] markup languages for machine processing, publication and implementation.<ref name=":1" /> The [[Anglo-American Cataloguing Rules (AACR)]], originally developed for characterizing books, have also been applied to cultural objects, works of art and architecture.<ref name=":2" /> Standards, such as the CCO, are integrated within a Museum\'s [[Collection Management System (CMS)]], a database through which museums are able to manage their collections, acquisitions, loans and conservation.<ref name=":2" /> Scholars and professionals in the field note that the "quickly evolving landscape of standards and technologies" create challenges for cultural documentarians, specifically non-technically trained professionals.<ref name=":3">{{Cite book|title=Linked Data for Libraries, Archives and Museums: How to Clean, Link and Publish Your Metadata|last=Hooland|first=Seth Van|last2=Verborgh|first2=Ruben|publisher=Facet|year=2014|isbn=|location=London|pages=}}</ref>{{page needed|date=November 2016}} Most collecting institutions and museums use a [[relational database]] to categorize cultural works and their images.<ref name=":2" /> Relational databases and metadata work to document and describe the complex relationships amongst cultural objects and multi-faceted works of art, as well as between objects and places, people and artistic movements.<ref name=":1" /><ref name=":2" /> Relational database structures are also beneficial within collecting institutions and museums because they allow for archivists to make a clear distinction between cultural objects and their images; an unclear distinction could lead to confusing and inaccurate searches.<ref name=":2" />\n\n==== Cultural objects and art works ====\nAn object\'s materiality, function and purpose, as well as the size (e.g., measurements, such as height, width, weight), storage requirements (e.g., climate-controlled environment) and focus of the museum and collection, influence the descriptive depth of the data attributed to the object by cultural documentarians.<ref name=":2" /> The established institutional cataloging practices, goals and expertise of cultural documentarians and database structure also influence the information ascribed to cultural objects, and the ways in which cultural objects are categorized.<ref name=":0" /><ref name=":2" /> Additionally, museums often employ standardized commercial collection management software that prescribes and limits the ways in which archivists can describe artworks and cultural objects.<ref name=":3" /> As well, collecting institutions and museums use [[Controlled vocabulary|Controlled Vocabularies]] to describe cultural objects and artworks in their collections.<ref name=":1" /><ref name=":2" /> [[Getty Vocabularies]] and the [[Library of Congress controlled vocabularies|Library of Congress Controlled Vocabularies]] are reputable within the museum community and are recommended by CCO standards.<ref name=":2" /> Museums are encouraged to use controlled vocabularies that are contextual and relevant to their collections and enhance the functionality of their digital information systems.<ref name=":1" /><ref name=":2" /> Controlled Vocabularies are beneficial within databases because they provide a high level of consistency, improving resource retrieval.<ref name=":1" /><ref name=":2" /> Metadata structures, including controlled vocabularies, reflect the [[Ontology (information science)|ontologies]] of the systems from which they were created. Often the processes through which cultural objects are described and categorized through metadata in museums do not reflect the perspectives of the maker communities.<ref name=":0" /><ref>{{Cite journal |last=Srinivasan |first=Ramesh |date=December 2006 |title=Indigenous, ethnic and cultural articulations of new media |url=http://ics.sagepub.com/content/9/4/497.abstract |journal=International Journal of Cultural Studies |volume=9 |issue=4 |doi=10.1177/1367877906069899}}</ref>\n\n==== Museums and the Internet ====\nMetadata has been instrumental in the creation of digital information systems and archives within museums, and has made it easier for museums to publish digital content online. This has enabled audiences who might not have had access to cultural objects due to geographic or economic barriers to have access to them.<ref name=":1" /> In the 2000s, as more museums have adopted archival standards and created intricate databases, discussions about [[Linked data|Linked Data]] between museum databases have come up in the museum, archival and library science communities.<ref name=":3" /> Collection Management Systems (CMS) and [[Digital asset management|Digital Asset Management]] tools can be local or shared systems.<ref name=":2" /> [[Digital humanities|Digital Humanities]] scholars note many benefits of interoperability between museum databases and collections, while also acknowledging the difficulties achieving such interoperability.<ref name=":3" />\n\n=== Law ===\n\n==== United States of America ====\n{{Globalize\n|date=March 2015\n}}\nProblems involving metadata in [[litigation]] in the [[United States]] are becoming widespread.{{when|date=February 2011}} Courts have looked at various questions involving metadata, including the [[discovery (law)|discoverability]] of metadata by parties. Although the Federal Rules of Civil Procedure have only specified rules about electronic documents, subsequent case law has elaborated on the requirement of parties to reveal metadata.<ref>{{Cite journal\n  | last = Gelzer | first = Reed D.\n  | title = Metadata, Law, and the Real World: Slowly, the Three Are Merging\n  | journal = Journal of AHIMA\n  | volume = 79\n  | issue = 2\n  | pages = 56–57, 64\n  | publisher = American Health Information Management Association\n  | date = February 2008\n  | url = http://library.ahima.org/xpedio/groups/public/documents/ahima/bok1_036537.hcsp?dDocName=bok1_036537\n  | accessdate = 8 January 2010}}</ref> In October 2009, the [[Arizona Supreme Court]] has ruled that metadata records are [[public record]].<ref>{{Cite news\n  | last = Walsh | first = Jim\n  | title = Ariz. Supreme Court rules electronic data is public record\n  | newspaper = The Arizona Republic\n  | location = Phoenix, Arizona\n  | date = 30 October 2009\n  | url = http://www.azcentral.com/arizonarepublic/local/articles/2009/10/30/20091030metadata1030.html\n  | accessdate = 8 January 2010\n}}</ref> Document metadata have proven particularly important in legal environments in which litigation has requested metadata, which can include sensitive information detrimental to a certain party in court. Using [[metadata removal tool]]s to "clean" or redact documents can mitigate the risks of unwittingly sending sensitive data. This process partially (see [[data remanence]]) protects law firms from potentially damaging leaking of sensitive data through [[electronic discovery]].\n\n====Australia====\n\nIn Australia the need to strengthen national security has resulted in the introduction of a new metadata storage law.<ref>Senate passes controversial metadata laws</ref> This new law means that both security and policing agencies will be allowed to access up to two years of an individual\'s metadata, supposedly to make it easier to stop any terrorist attacks and serious crimes from happening. In the 2000s, the law does not allow access to content of people\'s messages, phone calls or email and web-browsing history, but these provisions could be changed by the government.\n\n=== In healthcare ===\nAustralian medical research pioneered the definition of metadata for applications in health care. That approach offers the first recognized attempt to adhere to international standards in medical sciences instead of defining a proprietary standard under the [[World Health Organization]] (WHO) umbrella. The medical community yet did not approve the need to follow metadata standards despite research that supported these standards.<ref>M. Löbe, M. Knuth, R. Mücke [http://ceur-ws.org/Vol-559/Paper1.pdf TIM: A Semantic Web Application for the Specification of Metadata Items in Clinical Research], CEUR-WS.org, urn:nbn:de:0074-559-9</ref>\n\n=== Data warehousing ===\n[[Data warehouse]] (DW) is a repository of an organization\'s electronically stored data. Data warehouses are designed to manage and store the data. Data warehouses differ from [[business intelligence]] (BI) systems, because BI systems are designed to use data to create reports and analyze the information, to provide strategic guidance to management.<ref>Inmon, W.H. Tech Topic: What is a Data Warehouse? Prism Solutions. Volume 1. 1995.</ref> Metadata is an important tool in how data is stored in data warehouses. The purpose of a data warehouse is to house standardized, structured, consistent, integrated, correct, "cleaned" and timely data, extracted from various operational systems in an organization. The extracted data are integrated in the data warehouse environment to provide an enterprise-wide perspective. Data are structured in a way to serve the reporting and analytic requirements. The design of structural metadata commonality using a [[data modeling]] method such as [[entity relationship model]] diagramming is important in any data warehouse development effort. They detail metadata on each piece of data in the data warehouse. An essential component of a [[data warehouse]]/[[business intelligence]] system is the metadata and tools to manage and retrieve the metadata. [[Ralph Kimball]]<ref>{{Cite book\n  |last=Kimball |first=Ralph\n  |authorlink=Ralph Kimball\n  |title=The Data Warehouse Lifecycle Toolkit\n  |edition=Second\n  |location=New York |publisher=Wiley\n  |year=2008\n  |isbn=978-0-470-14977-5\n  |ref=harv\n  |pages=10, 115–117, 131–132, 140, 154–155\n}}</ref>{{page needed|date=November 2016}} describes metadata as the DNA of the data warehouse as metadata defines the elements of the [[data warehouse]] and how they work together.\n\n[[Ralph Kimball|Kimball]] et al.<ref>{{harvnb|Kimball|2008|pages=116–117}}</ref> refers to three main categories of metadata: Technical metadata, business metadata and process metadata. Technical metadata is primarily [[definitional]], while business metadata and process metadata is primarily descriptive. The categories sometimes overlap.\n* \'\'\'Technical metadata\'\'\' defines the objects and processes in a DW/BI system, as seen from a technical point of view. The technical metadata includes the system metadata, which defines the data structures such as tables, fields, data types, indexes and partitions in the relational engine, as well as databases, dimensions, measures, and data mining models. Technical metadata defines the data model and the way it is displayed for the users, with the reports, schedules, distribution lists, and user security rights.\n* \'\'\'Business metadata\'\'\' is content from the data warehouse described in more user-friendly terms. The business metadata tells you what data you have, where they come from, what they mean and what their relationship is to other data in the data warehouse. Business metadata may also serve as a documentation for the DW/BI system. Users who browse the data warehouse are primarily viewing the business metadata.\n* \'\'\'Process metadata\'\'\' is used to describe the results of various operations in the data warehouse. Within the [[Extract, transform, load|ETL]] process, all key data from tasks is logged on execution. This includes start time, end time, CPU seconds used, disk reads, disk writes, and rows processed. When troubleshooting the ETL or [[Information retrieval|query]] process, this sort of data becomes valuable. Process metadata is the fact measurement when building and using a DW/BI system. Some organizations make a living out of collecting and selling this sort of data to companies - in that case the process metadata becomes the business metadata for the fact and dimension tables. Collecting process metadata is in the interest of business people who can use the data to identify the users of their products, which products they are using, and what level of service they are receiving.\n\n=== On the Internet ===\nThe [[HTML]] format used to define web pages allows for the inclusion of a variety of types of metadata, from basic descriptive text, dates and keywords to further advanced metadata schemes such as the [[Dublin Core]], [[e-GMS]], and AGLS<ref>National Archives of Australia, AGLS Metadata Standard, accessed 7 January 2010, [http://www.naa.gov.au/records-management/create-capture-describe/describe/AGLS/index.aspx]</ref> standards. Pages can also be [[geotagging|geotagged]] with [[Geographic coordinate system|coordinates]]. Metadata may be included in the page\'s header or in a separate file. [[Microformat]]s allow metadata to be added to on-page data in a way that regular web users do not see, but computers, [[web crawler]]s and [[search engine]]s can readily access. Many search engines are cautious about using metadata in their ranking algorithms due to exploitation of metadata and the practice of search engine optimization, [[Search engine optimization|SEO]], to improve rankings. See [[Meta element]] article for further discussion. This cautious attitude may be justified as people, according to Doctorow,<ref>Metacrap: Putting the torch to seven straw-men of the meta-utopia http://www.well.com/~doctorow/metacrap.htm</ref> are not executing care and diligence when creating their own metadata and that metadata is part of a competitive environment where the metadata is used to promote the metadata creators own purposes. Studies show that search engines respond to web pages with metadata implementations,<ref>The impact of webpage content characteristics on webpage visibility in search engine results http://web.simmons.edu/~braun/467/part_1.pdf</ref> and Google has an announcement on its site showing the meta tags that its search engine understands.<ref>{{cite web|url=https://support.google.com/webmasters/answer/79812?hl=en/ |title=Meta tags that Google understands |publisher=Google.com |accessdate=2014-05-22}}</ref> Enterprise search startup [[Swiftype]] recognizes metadata as a relevance signal that webmasters can implement for their website-specific search engine, even releasing their own extension, known as Meta Tags 2.<ref>{{Cite web|url = https://swiftype.com/documentation/meta_tags2|title = Swiftype-specific Meta Tags |work=Swiftype Documentation |publisher=Swiftype |date = 3 October 2014 }}</ref>\n\n=== In broadcast industry ===\nIn [[broadcast]] industry, metadata is linked to audio and video [[broadcast media]] to:\n* \'\'identify\'\' the media: [[Media clip|clip]] or [[playlist]] names, duration, [[timecode]], etc.\n* \'\'describe\'\' the content: notes regarding the quality of video content, rating, description (for example, during a sport event, [[Index term|keywords]] like \'\'goal\'\', \'\'red card\'\' will be associated to some clips)\n* \'\'classify\'\' media: metadata allows to sort the media or to easily and quickly find a video content (a [[TV news]] could urgently need some [[archiving|archive content]] for a subject). For example, the BBC have a large subject classification system, [[Lonclass]], a customized version of the more general-purpose [[Universal Decimal Classification]].\n\nThis metadata can be linked to the video media thanks to the [[Video server#Broadcast automation|video servers]]. Most major broadcast sport events like [[FIFA World Cup]] or the [[Olympic Games]] use this metadata to distribute their video content to [[TV station]]s through [[Index term|keywords]]. It is often the host broadcaster<ref>{{cite web|url=http://www.hbs.tv/hostbroadcasting/ |title=HBS is the FIFA host broadcaster |publisher=Hbs.tv |date=2011-08-06 |accessdate=2011-12-23}}</ref> who is in charge of organizing metadata through its \'\'International Broadcast Centre\'\' and its video servers. This metadata is recorded with the images and are entered by metadata operators (\'\'loggers\'\') who associate in live metadata available in \'\'metadata grids\'\' through [[software]] (such as [[Multicam(LSM)]] or [[IPDirector]] used during the FIFA World Cup or Olympic Games).<ref>{{cite web|url=http://www.evs-global.com/01/MyDocuments/CS_BOB_EVScontributon_0808_ENG.pdf |title=Host Broadcast Media Server and Related Applications |format=PDF |accessdate=2013-08-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20111102235256/http://www.evs-global.com/01/MyDocuments/CS_BOB_EVScontributon_0808_ENG.pdf |archivedate=2 November 2011 }}</ref><ref>{{cite web|url=http://broadcastengineering.com/worldcup/fifa-world-cup-techonlogy-0610/ |title=logs during sport events |publisher=Broadcastengineering.com |accessdate=2011-12-23}}</ref>\n\n=== Geospatial ===\nMetadata that describes geographic objects in electronic storage or format (such as datasets, maps, features, or documents with a geospatial component) has a history dating back to at least 1994 (refer [http://libraries.mit.edu/guides/subjects/metadata/standards/fgdc.html MIT Library page on FGDC Metadata]). This class of metadata is described more fully on the [[geospatial metadata]] article.\n\n=== Ecological and environmental ===\nEcological and environmental metadata is intended to document the "who, what, when, where, why, and how" of data collection for a particular study. This typically means which organization or institution collected the data, what type of data, which date(s) the data was collected, the rationale for the data collection, and the methodology used for the data collection. Metadata should be generated in a format commonly used by the most relevant science community, such as [[Darwin Core]], [[Ecological Metadata Language]],<ref>[http://knb.ecoinformatics.org/software/eml/eml-2.0.1/index.html ] {{webarchive |url=https://web.archive.org/web/20110423161141/http://knb.ecoinformatics.org/software/eml/eml-2.0.1/index.html |date=23 April 2011 }}</ref> or [[Dublin Core]]. Metadata editing tools exist to facilitate metadata generation (e.g. Metavist,<ref>{{cite web|url=http://metavist.djames.net/ |title=Metavist 2 |publisher=Metavist.djames.net |accessdate=2011-12-23}}</ref> [[Mercury: Metadata Search System]], Morpho<ref>{{cite web|url=http://knb.ecoinformatics.org/morphoportal.jsp |title=KNB Data :: Morpho |publisher=Knb.ecoinformatics.org |date=2009-05-20 |accessdate=2011-12-23}}</ref>). Metadata should describe [[data provenance|provenance]] of the data (where they originated, as well as any transformations the data underwent) and how to give credit for (cite) the data products.\n\n=== Digital music ===\nWhen first released in 1982, Compact Discs only contained a Table Of Contents (TOC) with the number of tracks on the disc and their length in samples.[http://s3.amazonaws.com/academia.edu.documents/32801641/Morris_2012_-_Making_Music_Behave.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&Expires=1477195681&Signature=2TLmhapcR0M5eYsfMQ8FgG2TZa0%3D&response-content-disposition=inline%3B%20filename%3DMaking_music_behave_Metadata_and_the_dig.pdf][https://books.google.com/books?id=GkIaGZ0HWcMC&pg=PA48&source=gbs_toc_r&cad=4#v=onepage&q&f=false] Fourteen years later in 1996, a revision of the [[Compact Disc Digital Audio|CD Red Book]] standard added [[CD-Text]] to carry additional metadata.[http://web.ncf.ca/aa571/cdtext.htm] But CD-Text was not widely adopted. Shortly thereafter, it became common for personal computers to retrieve metadata from external sources (e.g. [[CDDB]], [[Gracenote]]) based on the TOC.\n\nDigital [[Sound recording and reproduction|audio]] formats such as [[digital audio file]]s superseded music formats such as [[cassette tape]]s and [[CDs]] in the 2000s. Digital audio files could be labelled with more information than could be contained in just the file name. That descriptive information is called the \'\'\'audio tag\'\'\' or audio metadata in general. Computer programs specializing in adding or modifying this information are called [[tag editor]]s. Metadata can be used to name, describe, catalogue and indicate ownership or copyright for a digital audio file, and its presence makes it much easier to locate a specific audio file within a group, typically through use of a search engine that accesses the metadata. As different digital audio formats were developed, attempts were made to standardize a specific location within the digital files where this information could be stored.\n\nAs a result, almost all digital audio formats, including [[mp3]], broadcast wav and [[AIFF]] files, have similar standardized locations that can be populated with metadata. The metadata for compressed and uncompressed digital music is often encoded in the [[ID3]] tag. Common editors such as [[TagLib]] support MP3, Ogg Vorbis, FLAC, MPC, Speex, WavPack TrueAudio, WAV, AIFF, MP4, and ASF file formats.\n\n=== Cloud applications ===\nWith the availability of [[Cloud computing|Cloud]] applications, which include those to add metadata to content, metadata is increasingly available over the Internet.\n\n== Administration and management ==\n\n=== Storage ===\nMetadata can be stored either \'\'internally\'\',<ref name=id3>{{cite web\n| first=Dan |last=O\'Neill\n| url=http://id3.org\n| title=ID3.org\n}}</ref> in the same file or structure as the data (this is also called \'\'embedded metadata\'\'), or \'\'externally\'\', in a separate file or field from the described data. A data repository typically stores the metadata \'\'detached\'\' from the data, but can be designed to support embedded metadata approaches. Each option has advantages and disadvantages:\n* Internal storage means metadata always travels as part of the data they describe; thus, metadata is always available with the data, and can be manipulated locally. This method creates redundancy (precluding normalization), and does not allow managing all of a system\'s metadata in one place. It arguably increases consistency, since the metadata is readily changed whenever the data is changed.\n* External storage allows collocating metadata for all the contents, for example in a database, for more efficient searching and management. Redundancy can be avoided by normalizing the metadata\'s organization. In this approach, metadata can be united with the content when information is transferred, for example in [[Streaming media]]; or can be referenced (for example, as a web link) from the transferred content. On the down side, the division of the metadata from the data content, especially in standalone files that refer to their source metadata elsewhere, increases the opportunities for misalignments between the two, as changes to either may not be reflected in the other.\n\nMetadata can be stored in either human-readable or binary form. Storing metadata in a human-readable format such as [[XML]] can be useful because users can understand and edit it without specialized tools.<ref name=Sutter>{{Cite book\n|first1=Robbie\n|last1=De Sutter\n|first2=Stijn\n|last2=Notebaert\n|first3=Rik\n|last3=Van de Walle\n|chapter=Evaluation of Metadata Standards in the Context of Digital Audio-Visual Libraries\n|title=Research and Advanced Technology for Digital Libraries: 10th European Conference, EDCL 2006\n|editor1-last=Gonzalo\n|editor1-first=Julio\n|editor2-last=Thanos\n|editor2-first=Constantino\n|editor3-last=Verdejo\n|editor3-first=M. Felisa\n|editor4-last=Carrasco\n|editor4-first=Rafael\n|date=September 2006\n|url =https://books.google.com/books?id=kU7Lqqowp54C&pg=PA226&cad=4#v=onepage\n|isbn= 978-3540446361\n|publisher=Springer\n|page=226\n}}</ref> However, text-based formats are rarely optimized for storage capacity, communication time, or processing speed. A binary metadata format enables efficiency in all these respects, but requires special software to convert the binary information into human-readable content.\n\n=== Database management ===\nEach [[relational database]] system has its own mechanisms for storing metadata. Examples of relational-database metadata include:\n* Tables of all tables in a database, their names, sizes, and number of rows in each table.\n* Tables of columns in each database, what tables they are used in, and the type of data stored in each column.\nIn database terminology, this set of metadata is referred to as the [[database catalog|catalog]]. The [[SQL]] standard specifies a uniform means to access the catalog, called the [[information schema]], but not all databases implement it, even if they implement other aspects of the SQL standard. For an example of database-specific metadata access methods, see [[Oracle metadata]]. Programmatic access to metadata is possible using APIs such as [[JDBC]], or SchemaCrawler.<ref name=schemacrawler>{{cite web\n| author=Sualeh Fatehi\n| url=http://schemacrawler.sourceforge.net/\n| title=SchemaCrawler\n| work=SourceForge\n}}</ref>\n\n== See also ==\n{{Div col||25em}}\n* [[Agris: International Information System for the Agricultural Sciences and Technology]]\n* [[Classification scheme]]\n* [[Crosswalk (metadata)]]\n* [[DataONE]]\n* [[Data Dictionary]] (aka metadata repository)\n* [[Dublin Core]]\n* [[Folksonomy]]\n* [[GEOMS – Generic Earth Observation Metadata Standard]]\n* [[Geospatial metadata]]\n* [[IPDirector]]\n* [[ISO/IEC 11179]]\n* [[Knowledge tag]]\n* [[Mercury: Metadata Search System]]\n* [[Meta element]]\n* [[IF-MAP|Metadata Access Point Interface]]\n* [[Metadata discovery]]\n* [[Metadata facility for Java]]\n* [[v:4-b: Metadata|Metadata from Wikiversity]]\n* [[Metadata publishing]]\n* [[Metadata registry]]\n* [[Metamathematics]]\n* [[METAFOR]] Common Metadata for Climate Modelling Digital Repositories\n* [[Microcontent]]\n* [[Microformat]]\n* [[Multicam (LSM)]]\n* [[Observations and Measurements]]\n* [[Ontology (computer science)]]\n* [[Official statistics]]\n* [[Paratext]]\n* [[Preservation Metadata]]\n* [[SDMX]]\n* [[Semantic Web]]\n* [[SGML]]\n* [[The Metadata Company]]\n* [[Universal Data Element Framework]]\n* [[Vocabulary OneSource]]\n* [[XSD]]\n{{Div col end}}\n\n== References ==\n{{Reflist|colwidth=30em}}\n\n== External links ==\n{{Wiktionary|metadata}}\n* [http://www.niso.org/apps/group_public/download.php/17446/Understanding%20Metadata \'\'Understanding Metadata: What is metadata, and what is it for?\'\'] — [[NISO]], 2017\n* [http://web.archive.org/web/20140522165110/http://www.theguardian.com/technology/interactive/2013/jun/12/what-is-metadata-nsa-surveillance#meta=1111111 "A Guardian guide to your metadata"] — \'\'[[The Guardian]]\'\', Wednesday 12 June 2013.\n* [http://www.well.com/~doctorow/metacrap.htm Metacrap: Putting the torch to seven straw-men of the meta-utopia] — [[Cory Doctorow]]\'s opinion on the limitations of metadata on the [[Internet]], 2001\n* [http://www.dataone.org DataONE] Investigator Toolkit\n* [http://www.informaworld.com/openurl?genre=journal&issn=1938-6389 \'\'Journal of Library Metadata\'\'], Routledge, Taylor & Francis Group, ISSN 1937-5034\n* [http://www.inderscience.com/ijmso \'\'International Journal of Metadata, Semantics and Ontologies\'\' (\'\'IJMSO\'\')], Inderscience Publishers, ISSN 1744-263X\n* {{webarchive |url=https://web.archive.org/web/20130126101115/http://www.metalounge.org/_literature_52579/Stephen_Machin_%E2%80%93_ON_METADATA_AND_METACONTENT |date=26 January 2013 |title=Metadata and metacontent }} (PDF, archived version)\n\n{{Software engineering}}\n{{Data warehouse}}\n\n{{Authority control}}\n\n[[Category:Data management]]\n[[Category:Records management]]\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]\n[[Category:Metadata| ]]\n[[Category:Technical communication]]\n[[Category:Business intelligence]]']
['Data storage device', '28174', '[[File:PersonalStorageDevices.agr.jpg|thumb|Many different consumer electronic devices can store data.]]\n[[File:EdisonPhonograph.jpg|thumb|Edison cylinder phonograph ca. 1899. The phonograph cylinder is a storage medium. The phonograph may be considered a storage device.]]\n[[File:Reel-to-reel recorder tc-630.jpg|thumb|On a reel-to-reel tape recorder (Sony TC-630), the recorder is data storage equipment and the magnetic tape is a data storage medium.]]\n[[File:RNA-comparedto-DNA thymineAndUracilCorrected.png|thumb|upright|[[RNA]] might be the oldest [[data]] storage medium.<ref>{{cite journal|title=The RNA World|journal=[[Nature (journal)|Nature]]|first=Walter|last=Gilbert|authorlink=Walter Gilbert|date=Feb 1986|pages=618|volume=319|doi=10.1038/319618a0|issue=6055|bibcode=1986Natur.319..618G}}</ref>]]\n\nA \'\'\'data storage device\'\'\' is a device for [[recording]] (storing) [[information]] (data). Recording can be done using virtually any form of [[energy]], spanning from manual muscle power in [[handwriting]], to acoustic vibrations in [[phonograph]]ic recording, to electromagnetic energy modulating [[magnetic tape]] and [[optical disc]]s.\n\nA storage device may hold information, process information, or both. A device that only holds information is a recording [[Medium (communication)|medium]]. Devices that process information (data storage equipment) may either access a separate portable (removable) recording medium or a permanent component to store and retrieve data.\n\nElectronic data storage requires electrical power to store and retrieve that data. Most storage devices that do not require [[Visual perception|vision]] and a brain to read data fall into this category.  Electromagnetic data may be stored in either an  analog [[data]] or [[digital data]] format on a variety of media. This type of data is considered to be [[Machine-readable medium|electronically encoded]] data, whether it is electronically stored in a  [[semiconductor]] [[Computer data storage|device]], for it is certain that a semiconductor device was used to record it on its medium. Most electronically processed data storage media (including some forms of [[computer data storage]]) are considered permanent (non-volatile) storage, that is, the data will remain stored when power is removed from the device. In contrast, most electronically stored information within most types of semiconductor (computer chips) [[microcircuit]]s are [[volatile memory]], for it vanishes if power is removed.\n\nExcept for [[barcode]]s, [[optical character recognition]] (OCR), and [[magnetic ink character recognition]] (MICR) data, electronic data storage is easier to revise and may be more cost effective than alternative methods due to smaller physical space requirements and the ease of replacing (rewriting) data on the same medium.<ref>{{Cite web|url=https://www.seas.gwu.edu/~shmuel/WORK/Differences/Chapter%203%20-%20Sources.pdf|title=The Difference between Electronic and Paper Documents|last=Rotenstreich|first=Shmuel|website=Seas.GWU.edu|publisher=The George Washington University|access-date=12 April 2016}}</ref>\n\n==Global capacity, digitization, and trends==\nIn a recent study in [[Science (journal)|\'\'Science\'\']] it was estimated that the world\'s technological capacity to store information in analog and digital devices grew from less than three (optimally compressed) [[exabyte]]s in 1986, to 295 (optimally compressed) [[exabyte]]s in 2007,<ref name="HilbertLopez2011">{{cite journal | last1 = Hilbert | first1 = Martin | last2 = López | first2 = Priscila | year = 2011 | title = The World\'s Technological Capacity to Store, Communicate, and Compute Information | journal = [[Science (journal)|Science]] | volume = 332 | issue = 6025| pages = 60–65 | doi=10.1126/science.1200970 | pmid=21310967}}; free access to the article through here: martinhilbert.net/WorldInfoCapacity.html</ref> and doubles roughly every three years.<ref name="Hilbertvideo2011">[http://ideas.economist.com/video/giant-sifting-sound-0 "video animation on The World’s Technological Capacity to Store, Communicate, and Compute Information from 1986 to 2010]</ref>\n\nIt is estimated that the year 2002 marked the beginning of the digital age for information storage, the year that marked the date when human kind started to store more information digitally than on analog storage devices.<ref name="HilbertLopez2011" />\n\n==See also==\n{{colbegin||22em}}\n* [[Archival science]]\n* [[Blank media tax]]\n* [[Computer data storage]]\n* [[Content format]]\n* [[Data transmission]]\n* [[Digital Data Storage|Digital Data Storage (DDS)]]\n* [[Digital preservation]]\n* [[Disk drive performance characteristics]]\n* [[Format war]]\n* [[Flip-flop (electronics)]]\n* [[IOPS]]\n* [[Library]]\n* [[Media controls]]\n* [[Medium format (film)]]\n* [[Memristor]]\n* [[Nanodot]]\n* [[Nonlinear medium]] ([[random access]])\n* [[Recording format]]\n* [[Semiconductor memory]]\n* [[Telecommunication]]\n{{colend}}\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* {{cite journal |last = Bennett |first=John C. | title = \'JISC/NPO Studies on the Preservation of Electronic Materials: A Framework of Data Types and Formats, and Issues Affecting the Long Term Preservation of Digital Material | publisher = British Library Research and Innovation Report 50 | year = 1997 | url = http://www.ukoln.ac.uk/services/papers/bl/jisc-npo50/bennet.html }}\n* [http://www.zetta.net/history-of-computer-storage/ History of Computer Storage from 1928 to 2013]\n* [http://www.remosoftware.com/info/history-of-storage-from-cave-paintings-to-electrons/ History of Storage from Cave Paintings to Electrons]\n* [[Plant-based digital data storage]]\n\n==External links==\n* [http://ns1758.ca/winch/winchest.html Historical Notes about the Cost of Hard Drive Storage Sp]\n* @[http://ns1758.ca/winch/winchest.html ace]\n* [https://www.securedatarecovery.com/infographics/the-evolution-of-data-storage The Evolution of Data Storage]\n\n{{Magnetic storage media}}\n{{Optical storage media}}\n{{Paper data storage media}}\n{{Primary storage technologies}}\n\n{{Authority control}}\n\n[[Category:Computer storage devices]]\n[[Category:Data management]]\n[[Category:Film and video technology]]\n[[Category:Media technology]]\n[[Category:Recording]]\n[[Category:Sound production technology]]\n[[Category:Storage media]]']
['DataverseNL', '52809243', "'''DataverseNL''' is data management service built on top of [[Dataverse]] data repository and jointly offered by participating institutions to the research community of the Netherlands. It's developed by the Dataverse Team at the Institute for Quantitative Social Science ([[IQSS]]) at [[Harvard University]] and [[Data Archiving and Networked Services]] (DANS) and housed and maintained by DANS team.\n\nWith DataverseNL, researchers and lecturers can store, share and register research data online, both during research and for up to ten years afterwards. Dataverse platform is used worldwide. In the Netherlands, DataverseNL was installed at the Universiteit Utrecht in 2010, after which it developed into a shared service of over other institutions. The data management is in the hands of the institutions; DANS has been managing the network since May 2014.\n\nAs of January 2017, DataverseNL offers access to more than 300 published studies.\n\n== Persistent identifier ==\nAt DataverseNL, each deposited dataset receives its own persistent identifier that allows information to remain accessible in the long term, even if its location changes. [[DANS]] has developed this functionality for Dataverse repository to use handle system for its persistent identifiers.\n\n== Open access ==\nBesides storing data, DataverseNL allows you to share them with other scientists. Researchers themselves determine who gets access to which materials and what their access rights are (user, contributor or curator). Researchers may choose any license for their data, including [[CC0]] (CC Zero Waiver) or [[ODBL]] (Open Database License).\n\n== How to use DataverseNL ==\n'''Depositing data'''<br />\n'''1''' Check to see if your university or institution is participating in DataverseNL.<br />\n'''2''' Prepare your data: select the relevant data files and check for any privacy-sensitive aspects. <br />\n'''3''' Log in at https://dataverse.nl. New users must first sign up for an account.<br />\n'''4''' Upload your data to studies within your dataverse.<br />\n'''5''' Describe your data and determine who gets access. <br />\n'''6''' Share the data by publishing them or allowing others access to your dataverse or studies.<br />\n\n'''Downloading data'''<br />\n'''1''' Search or browse, and if necessary refine your search results, until you have found the dataset you are looking for at https://dataverse.nl. <br />\n'''2''' Look at the metadata to determine if the dataset meets your requirements. <br />\n'''3''' Depending on the access category, you can go to the Data & Analysis tab and download the data. <br />\n'''4''' Check the ‘Data citation’ header to see the correct method for citing the data. <br />\n\n== Participating institutions ==\nDataverseNL is a shared service provided by the participating institutions and DANS. DANS performs back office tasks, including server and software maintenance and administrative support.\nThe participating institutions are responsible for managing the deposited data.\n\nAt the moment there are following participating institutions:\n* [[4TU]] Data lab\n*[[ 4TU.Centre for Research Data]]\n* [[Tilburg University]]\n* [[TiU]]\n* [[Universiteit Utrecht]]\n* [[Vrije Universiteit Amsterdam]]\n* [[Maastricht University]]\n* [[Protestantse Theologische Universiteit]]\n* [[Avans Hogeschool]]\n* [[Nederlands Instituut voor Ecologie]]\n* [[Rijksuniversiteit Groningen]]\n* [[Erasmus University Rotterdam]]\n* [[Hogeschool Windesheim]]\n\n==External links==\n*[https://www.dataverse.nl The DataverseNL Repository (Netherlands)]\n*[https://www.dans.knaw.nl Data Archiving and Networked Services]\n*[http://dataverse.org/ The Dataverse Project]\n\n[[Category:Data management]]\n[[Category:Open science]]\n[[Category:Open data]]\n[[Category:Open-access archives]]\n[[Category:Open access (publishing)]]\n[[Category:Academic publishing]]\n[[Category:Data publishing]]\n[[Category:Scholarly communication]]"]
['Category:Audio storage', '754789', '{{Commons category|Audio storage media}}\n{{Cat main|Sound recording and reproduction}}\n\n[[Category:Storage media]]\n[[Category:Electronic documents]]\n[[Category:Sound recording technology]]\n[[Category:Sound production technology]]']
['Information capture', '3547364', "{{refimprove|date=June 2016}}\n'''Information capture''' is the process of collecting paper [[documents]], [[form (document)|form]]s and e-documents, transforming them into accurate, retrievable, [[Digital data|digital]] information, and delivering the information into business applications and [[databases]] for immediate action.<ref>http://www.emc.com/collateral/advertorial/aiim-advertorial.pdf</ref>\n\n==See also==\n* [[Ibml]]\n\n==References==\n<references/>\n\n[[Category:Electronic documents]]\n\n\n{{database-stub}}"]
['Category:Office software', '21137368', '{{Commons category|Office suites}}\n[[Category:Office work]]\n[[Category:Business software]]\n[[Category:Electronic documents]]']
['Archival Resource Key', '24485224', 'An \'\'\'Archival Resource Key\'\'\' (\'\'\'ARK\'\'\') is a [[Uniform Resource Locator]] (URL) that is a multi-purpose [[persistent identifier]] for information objects of any type.  An ARK contains the label \'\'\'ark:\'\'\' after the URL\'s hostname, which sets the expectation that, when submitted to a web browser, the URL terminated by \'?\' returns a brief metadata record, and the URL terminated by \'??\' returns metadata that includes a commitment statement from the current service provider.  The ARK and its inflections (\'?\' and \'??\') gain access to three facets of a provider\'s ability to provide persistence.\n\nImplicit in the design of the ARK scheme is that persistence is purely \'\'\'a matter of service\'\'\' and not a property of a naming syntax.  Moreover, that a "persistent identifier" cannot be born persistent, but an identifier from any scheme may only be proved persistent over time.  The inflections provide information with which to judge an identifier\'s likelihood of persistence.\n\nARKs can be maintained and resolved locally using open source software such as [http://search.cpan.org/dist/Noid/ Noid (Nice Opaque Identifiers)] or via services such as [http://ezid.cdlib.org EZID] and the central [http://n2t.net N2T (Name-to-Thing)] resolver.\n\n== Structure ==\n <nowiki>[http://NMAH/]ark:/NAAN/Name[Qualifier]</nowiki>\n\n* NAAN: Name Assigning Authority Number - mandatory unique identifier of the organization that originally named the object\n* NMAH: Name Mapping Authority Host - optional and replaceable hostname of an organization that currently provides service for the object\n* Qualifier: optional string that extends the base ARK to support access to individual \'\'\'hierarchical\'\'\' subcomponents of an object,<ref>Hierarchy qualifiers begin with a slash character.</ref> and to \'\'\'variants\'\'\' (versions, languages, formats) of components.<ref>Variant qualifiers begin with a dot character.</ref>\n\n== Name Assigning Authority Numbers (NAANs) ==\nA complete NAAN registry<ref>[http://www.cdlib.org/services/uc3/naan_table.html Name Assigning Authority Number registry]</ref> is maintained by the [[California Digital Library]] and replicated at the [[Bibliothèque nationale de France|Bibliothèque Nationale de France]] and the [[National Library of Medicine|US National Library of Medicine]]. In 2015 it contained over 395 entries, some of which appear below.\n\n* 12025: [[National Library of Medicine]]\n* 12148: [[Bibliothèque Nationale de France]]\n* 13030: [[California Digital Library]]\n* 13038: [[World Intellectual Property Organization]]\n* 13960: [[Internet Archive]]\n* 14023: Revista de Arte, Ciência e Comunicação\n* 15230: [[Rutgers University]]\n* 17101: [[Centre for Ecology & Hydrology]]\n* 20775: [[University of California, San Diego]]\n* 21198: [[University of California Los Angeles]]\n* 25031: [[University of Kansas]]\n* 25593: [[Emory University]]\n* 25652: [[École nationale supérieure des mines de Paris]]\n* 26677: [[Library and Archives Canada]]\n* 27927: Portico/Ithaka Electronic-Archiving Initiative\n* 28722: [[University of California Berkeley]]\n* 29114: [[University of California San Francisco]]\n* 35911: [[IEEE]]\n* 39331: [[National Library of Hungary]]\n* 45487: Russian Linguistic Bulletin (Российский Лингвистический Бюллетень)\n* 48223: [[UNESCO]]\n* 52327: [[Bibliothèque et Archives Nationales du Québec]]\n* 61001: [[University of Chicago]]\n* 62624: [[New York University]]\n* 64269: [[Digital Curation Centre]]\n* 65323: [[University of Calgary]]\n* 67531: [[University of North Texas]]\n* 78319: [[Google]]\n* 78428: [[University of Washington]]\n* 80444: [[Northwest Digital Archives]]\n* 81055: [[British Library]]\n* 88435: [[Princeton University]]\n* 87925: [[University College Dublin]]\n\n== Generic Services ==\nThree generic ARK services have been defined. They are described below in protocol-independent terms. Delivering these services may be implemented through many possible methods given available technology (today’s or future).\n\n=== Access Service (access, location) ===\n*Returns (a copy of) the object or a redirect to the same, although a sensible object proxy may be substituted (for instance a table of contents instead of a large document).\n*May also return a discriminated list of alternate object locators.\n*If access is denied, returns an explanation of the object’s current (perhaps permanent) inaccessibility.\n\n=== Policy Service (permanence, naming, etc.) ===\n*Returns declarations of policy and support commitments for given ARKs.\n*Declarations are returned in either a structured metadata format or a human readable text format; sometimes one format may serve both purposes.\n*Policy subareas may be addressed in separate requests, but the following areas should be covered:\n**object permanence,\n**object naming,\n**object fragment addressing, and\n**operational service support.\n\n=== Description Service ===\n*Returns a description of the object. Descriptions are returned in either a structured metadata format or a human readable text format; sometimes one format may serve both purposes.\n*A description must at a minimum answer the \'\'\'who\'\'\', \'\'\'what\'\'\', \'\'\'when\'\'\', and \'\'\'where\'\'\' questions concerning an expression of the object.\n*Standalone descriptions should be accompanied by the modification date and source of the description itself.\n*May also return discriminated lists of ARKs that are related to the given ARK.\n\n== See also ==\n* [[Persistent identifier]]\n* [[Digital object identifier]] (DOI)\n* [[Handle System]] (Handle)\n* [[Persistent uniform resource locator]] (PURL)\n* [[Uniform resource name]] (URN)\n* [[Info URI scheme]]\n\n== Notes and references ==\n<references/>\n\n== External links ==\n* [http://www.cdlib.org/inside/diglib/ark/ ARK (Archival Resource Key)], [[California Digital Library]]\n* [https://confluence.ucop.edu/download/attachments/16744455/arkcdl.pdf Towards Electronic Persistence Using ARK Identifiers], California Digital Library\n* [http://tools.ietf.org/html/draft-kunze-ark  The ARK Identifier Scheme], [[Internet Engineering Task Force]]\n* [http://n2t.net Name-to-Thing Resolver]\n* [http://search.cpan.org/dist/Noid/ Noid (Nice Opaque Identifiers) open source software]\n* [http://ezid.cdlib.org EZID identifier manager]\n\n[[Category:Electronic documents]]\n[[Category:Identifiers]]\n[[Category:Index (publishing)]]']
['Quickstart guide', '12918613', "A '''quickstart guide''' is a short, simple introductory guide to a piece of equipment for many consumer electronics products (e.g. [[television]]) or recently, [[automobile]]s, [[mobile phone]]s, computers connection. \n\nWith the increase in complexity and functions with electronics products quickstart guides are created to get users quickly accustomed to the basic operations of the product. Complex or detailed operations are usually left in the full-length [[owner's manual]].\n\n{{Electronics-stub}}\n\n[[Category:Electronic documents]]"]
['Computable Document Format', '32785726', "{{Infobox file format\n| name                   = Computable Document Format\n| logo                   = [[Image:WolframCDFLogoSmall.png]]\n| icon                   = [[Image:WolframCDFLogoSmall.png]]\n| iconcaption            = \n| screenshot             =  \n| caption                =  \n| extension              = .cdf\n| mime                   = application/cdf\n| typecode               =  \n| uniform type           = com.wolfram.cdf\n| magic                  =  \n| owner                  = [[Wolfram Research]]\n| released               = {{Start date|2011|07|21}}<!-- {{Start date|YYYY|mm|dd|df=yes}} -->\n| latest release version = \n| latest release date    = <!-- {{Start date and age|YYYY|mm|dd|df=yes}} -->\n| genre                  =\n| container for          =\n| contained by           =\n| extended from          =\n| extended to            =\n| standard               =  \n| free                   =\n| url                    = [http://www.wolfram.com/cdf Computable Document Format]\n}}\n'''Computable Document Format''' ('''CDF''') is an electronic document format<ref>[http://www.telegraph.co.uk/technology/news/8561619/Wolfram-Alpha-creator-plans-to-delete-the-PDF.html Wolfram Alpha Creator plans to delete the PDF] The Telegraph (UK)</ref> designed to allow easy authoring<ref>[http://www.pcworld.com/businesscenter/article/236202/wolfram_makes_data_interactive.html Wolfram makes data interactive] PC World</ref> of dynamically generated interactive content. CDF is a published public format<ref>{{cite web|title=About CDFs|url=http://www.wolfram.com/cdf/faq/#aboutcdf|publisher=[[Wolfram Research]]}}</ref> created by [[Wolfram Research]].<ref name=thinq11/>\n\n==Features==\nComputable document format supports [[GUI]] elements such as sliders, menus and buttons. Content is updated using embedded computation in response to GUI interaction. Contents can include formatted text, tables, images, sounds and animations. CDF supports [[Mathematica]] typesetting and technical notation.<ref>[http://www.zdnet.com/blog/btl/wolfram-launches-new-document-format-meet-cdf/52917 Wolfram Launches new document format. Meet CDF] ZDNet</ref> Paginated layout, structured drill down layout and slide-show mode are supported. Styles can be controlled using a [[Cascading Style Sheets|cascading style sheet]].\n\n==Reading==\nCDF files can be read using a proprietary [[CDF Player]] with a restrictive license, which can be downloaded free of charge from Wolfram Research.<ref name=thinq11>[http://www.thinq.co.uk/2011/7/21/wolfram-punts-expanded-medium-technical-docs/ Wolfram punts expanded medium for technical docs] {{webarchive |url=https://web.archive.org/web/20110725121540/http://www.thinq.co.uk/2011/7/21/wolfram-punts-expanded-medium-technical-docs/ |date=July 25, 2011 }} ThinQ</ref>\n\n==Authoring==\nCDF Files can be created using [[Mathematica]].  Online authoring tools are planned.<ref name=thinq11/><ref>[http://www.cio.com.au/article/394473/wolfram_makes_data_interactive/ Wolfram makes data interactive] CIO, 21 July 2011</ref>\n\n==Uses==\nComputable Document Format has been used in electronic books by [[Pearson Education]],<ref>[http://www.schoollibraryjournal.com/slj/home/891371-312/wolfram_launches_pdf_killer.html.csp Wolfram launches PDF Killer] School Library Journal</ref><ref>[http://www.pearsonhighered.com/briggscochran1einfo/ Briggs Cochrane Calculus]</ref> to provide the content for the [[Wolfram Demonstrations Project]], and to add client-side interactivity to [[Wolfram Alpha]].<ref>[http://thenextweb.com/apps/2011/08/12/wolfram-alpha-adds-powerful-interactive-search-results/ WolframAlpha adds powerful interactive search results] The Next Web, 12 August 2011</ref><ref>[http://www.pcpro.co.uk/news/enterprise/368815/wolfram-launches-its-own-interactive-document-format Wolfram Launches its own interactive document format] PC Pro, July 2011</ref>\n\n== See also ==\n* [[List of numerical analysis software]]\n* [[Comparison of numerical analysis software]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.wolfram.com/cdf/ Wolfram Research CDF]\n* [http://www.wolfram.com/cdf-player/ CDF Player download]\n\n{{Graphics file formats}}\n{{Ebooks}} \n{{Wolfram Research}}<!--navbox-->\n\n[[Category:2011 introductions]]\n[[Category:Wolfram Research]]\n[[Category:Electronic documents]]\n[[Category:Open formats]]\n[[Category:Page description languages]]\n[[Category:Vector graphics]]\n[[Category:Computer file formats]]\n[[Category:Digital press]]"]
['Registry of Research Data Repositories', '41872647', '{{Infobox website\n| name = re3data.org\n| logo = Re3data Logo RGB 72dpi.png\n| logocaption = The logo of re3data.org, the online Registry of Research Data Repositories\n| registration = none\n| language = English\n| type = Online registry\n| owner = [[Karlsruhe Institute of Technology]], [[GFZ German Research Centre for Geosciences]], [[Berlin School of Library and Information Science]]\n| commercial = no\n| launch date = {{Start date and years ago|mf=yes|2013|05|28}}\n| current status  = Online\n| content license = Website: [[Creative Commons licenses|CC-BY]], Database: [[Creative Commons licenses|CC0]] \n| url = {{URL|http://www.re3data.org/}}\n}}\n\nThe \'\'\'Registry of Research Data Repositories\'\'\' (\'\'\'re3data.org\'\'\') is an [[Open Science]] tool that offers researchers, funding organizations, libraries and publishers an overview of existing international [[disciplinary repository|repositories]] for [[research data]].\n\n== Background ==\n\nre3data.org is a global registry of research data repositories from all academic disciplines. It provides an overview of existing research data repositories in order to help researchers to identify a suitable repository for their data and thus comply with requirements set out in data policies.<ref name=Pampel2013>{{cite journal|last=Pampel|first=Heinz|author2=Vierkant, Paul |author3=Scholze, Frank |author4=Bertelmann, Roland |author5=Kindling, Maxi |title=Making Research Data Repositories Visible: The re3data.org Registry|journal=PLoS ONE|date=4 November 2013|volume=8|issue=11|pages=e78080|doi=10.1371/journal.pone.0078080|pmid=24223762|url=http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0078080|accessdate=7 February 2014|bibcode=2013PLoSO...878080P|pmc=3817176|display-authors=etal}}</ref>\nThe registry was officially launched in May 2013.<ref name=Wellander2013>{{cite web | url=http://sparceurope.org/registry-of-research-data-repositories-launched-re3data-org/ | title=Registry of Research Data Repositories launched – re3data.org | author=Wellander, Janna | date= 4 June 2013 | work=SPARC Europe | accessdate= 5 February 2014}}</ref>\n\n== Content ==\n\nIn March 2014 the registry lists 634 research data repositories from around the world covering all academic disciplines. 586 of these are described in detail using the re3data.org schema.<ref>{{cite web|title=re3data.org – from Funding to Growing|url=http://www.re3data.org/2014/03/re3data-org-from-funding-to-growing/|work=re3data.org|accessdate=21 March 2014|date=19 March 2014}}</ref>\nThe project makes all metadata in the registry available for open use under the Creative Commons deed CC0.<ref>{{cite web|title=DataCite, re3data.org, and Databib Announce Collaboration|url=http://www.re3data.org/2014/03/datacite-re3data-org-databib-collaboration/|work=re3data|accessdate=25 March 2014}}</ref>\n[[File:DRYAD entry in re3data.org 2014-03-21.png|thumb|A screenshot of the DataDryad entry in re3data.org.]]\n\n== Features ==\n\nThe majority of the listed research data repositories are described in detail by a comprehensive schema, namely the re3data.org Schema for the Description of Research Data Repositories.<ref name=Vierkant>{{cite web|last=Vierkant|first=Paul|title=Schema for the description of research data repositories|work=GFZ Helmholtz-Zentrum Potsdam|accessdate=7 February 2014|author2=Spier, Shaked |author3=Rücknagel, Jessika |author4= et. al. |url=http://gfzpublic.gfz-potsdam.de/pubman/faces/viewItemFullPage.jsp?itemId=escidoc:301641|doi=10.2312/re3.004}}</ref>\nInformation icons support researchers to identify an adequate repository for the storage and reuse of their data.<ref name=Wellander2013 /><ref>{{cite web|last=Fenner|first=Martin|title=registry of research data repositories launched|url=http://blogs.plos.org/mfenner/2013/06/01/re3data-org-registry-of-research-data-repositories-launched/|work=PLOS Blog|publisher=Gobbledygook|accessdate=5 February 2014}}</ref>\n[[File:Journal.pone.0078080.g001.png|thumb|Aspects of a Research Data Repository with the corresponding icons used in re3data.org.]]\n\n== Inclusion criteria ==\n\nA repository is indexed when the minimum requirements for inclusion in re3data.org are met: the repository has to be run by a legal entity, such as a sustainable institution (e.g. library, university) and clearly state access conditions to the data and repository as well as the terms of use. Additionally, an English [[graphical user interface]] (GUI) plus a focus on research [[data]] is needed.<ref name=Vierkant />\n\n== Partners and Cooperation ==\n\nre3data.org is a joint project of the [[Berlin School of Library and Information Science]], the [[GFZ German Research Centre for Geosciences]] and the Library of the [[Karlsruhe Institute of Technology]] (KIT). The project is funded by the [[Deutsche Forschungsgemeinschaft|German Research Foundation]] (DFG).<ref name=Pampel2013 />\nThe project cooperates with other Open Science initiatives like Databib,<ref>{{cite web|last=Kratz|first=John|title=Finding Disciplinary Data Repositories with DataBib and re3data|url=http://datapub.cdlib.org/2014/03/03/finding-disciplinary-data-repositories-with-databib-and-re3data/|work=Data Pub|accessdate=21 March 2014|author2=Nicholls, Natsuko |date=3 March 2014}}</ref> BioSharing,<ref>{{cite web|title=Databases|url=http://www.biosharing.org/biodbcore|work=biosharing|accessdate=5 February 2014}}</ref> [[DataCite]]<ref>{{cite web|title=Resources|url=http://www.datacite.org/resources|work=DataCite|accessdate=5 February 2014}}</ref> and OpenAIRE.<ref>{{cite web|title=re3data.org and OpenAIRE sign Memorandum of Understanding|url=https://www.openaire.eu/it/component/content/article/481-re3data-and-openaire-sign-memorandum-of-understanding|work=OpenAIRE|accessdate=5 February 2014|date=21 October 2013}}</ref> Several publishers, research institutions and funders refer to re3data.org in their Editorial Policies and guidelines as a tool for the identification of suitable data repositories, e.g. [[Nature (journal)|Nature]],<ref>{{cite web|title=The paper trail|url=http://www.nature.com/news/the-paper-trail-1.13123|work=Nature|accessdate=5 February 2014|date=4 June 2013}}</ref> [[Springer Science+Business Media|Springer]]<ref>{{cite web|title=About SpringerPlus - Editorial policies|url=http://www.springerplus.com/about#editorialpolicies|work=SpringerPlus|accessdate=5 February 2014}}</ref> and the [[European Commission]].<ref>{{cite web|title=Guidelines on Open Access to Scientific Publications and Research Data in Horizon 2020|url=http://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-pilot-guide_en.pdf|publisher=European Commission|accessdate=20 March 2014}}</ref>\n\n== See also ==\n\n*[[Scientific data archiving]]\n*[[Data sharing]]\n*[[Data archive]]\n*[[Data library]]\n*[[Data curation]]\n\n== External links ==\n* [http://www.re3data.org/ Official website]\n\n== References ==\n\n{{reflist}}\n\n<!-- Just press the "Save page" button below without changing anything! Doing so will submit your article submission for review. Once you have saved this page you will find a new yellow \'Review waiting\' box at the bottom of your submission page. If you have submitted your page previously, either the old pink \'Submission declined\' template or the old grey \'Draft\' template will still appear at the top of your submission page, but you should ignore it. Again, please don\'t change anything in this text box. Just press the "Save page" button below. -->\n\n\n[[Category:Academic publishing]]\n[[Category:Electronic documents]]\n[[Category:Identifiers]]\n[[Category:Index (publishing)]]\n[[Category:Open science]]']
['Comparison of e-book formats', '12115370', 'The following is a \'\'\'comparison of e-book formats\'\'\' used to create and publish [[e-book]]s.\n\nThe [[EPUB]] format is the most widely supported vendor-independent [[XML]]-based (as opposed to [[Portable Document Format|PDF]]) e-book format; that is, it is supported by the largest number of e-Readers, including [[Kindle Fire|Amazon Kindle Fire]] (but not standard Kindle).<ref name="kdp.amazon.com">{{cite web|url=https://kdp.amazon.com/help?topicId=A2GF0UFHIYG9VQ |title=Amazon Kindle Direct Publishing: Get help with self-publishing your book to Amazon\'s Kindle Store |publisher=Kdp.amazon.com |date= |accessdate=2015-08-31}}</ref> See table below for details. \n{{TOC right}}\n\n==Format descriptions==\nFormats available include, but are not limited to:\n\n===Broadband eBooks (BBeB) ===\n{{main article |BBeB}}\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Sony media\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .lrf; .lrx\n|}\nThe digital book format originally used by [[Sony Corporation]].  It is a proprietary format, but some reader software for general-purpose computers, particularly under [[GNU Project|GNU]]/Linux (for example, [[Calibre (software)|Calibre]]\'s internal viewer<ref>{{Citation | title = About | url = http://calibre-ebook.com/about | publisher = Calibre}}</ref>), have the capability to read it.  The LRX file extension represents a [[Digital rights management|DRM]] encrypted eBook. More recently, Sony has converted its books from BBeB to EPUB and is now issuing new titles in EPUB.\n\n===Comic Book Archive file ===\n{{main article|Comic book archive}}\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| compressed images\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n|.cbr (RAR); .cbz (ZIP); .cb7 (7z); .cbt (TAR); .cba (ACE)\n|}\n\n===Compiled HTML ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| [[Microsoft Compiled HTML Help]]\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .chm\n|}\nCHM format is a proprietary format based on HTML. Multiple pages and embedded graphics are distributed along with [[metadata]] as a single compressed file. The indexing is both for keywords for full text search.\n\n===DAISY – ANSI/NISO Z39.86  ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| [[DAISY Digital Talking Book|DAISY]]\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n|\n|}\n\nThe Digital Accessible Information SYstem (DAISY) is an [[XML]]-based open standard maintained by the DAISY Consortium for people with [[print disabilities]].  DAISY has wide international support with features for multimedia, navigation and synchronization. A subset of the DAISY format has been adopted by law in the United States as the National Instructional Material Accessibility Standard (NIMAS), and K-12 textbooks and instructional materials are now required to be provided to students with disabilities.\n\nDAISY is already aligned with the EPUB technical standard, and is expected to fully converge with its forthcoming EPUB3 revision.<ref>{{cite web|url=http://www.daisy.org/z3986 |title=DAISY Standard &#124; DAISY Consortium |publisher=Daisy.org |date= |accessdate=2015-08-31}}</ref>\n\n===DjVu ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| DjVu\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[DjVu|djvu]]\n|}\nDjVu is a format specialized for storing scanned documents. It includes advanced compressors optimized for low-color images, such as text documents. Individual files may contain one or more pages. DjVu files cannot be re-flowed.\n\nThe contained page images are divided in separate layers (such as multi-color, low-resolution, background layer using [[lossy compression]], and few-colors, high-resolution, tightly compressed foreground layer), each compressed in the best available method. The format is designed to decompress very quickly, even faster than vector-based formats.\n\nThe advantage of DjVu is that it is possible to take a high-resolution scan (300–400 DPI), good enough for both on-[[screen reading]] and printing, and store it very efficiently. Several dozens of 300 DPI black-and-white scans can be stored in less than a megabyte.\n\n===DOC===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Microsoft Word\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[Doc (computing)|DOC]]\n|}\n\n[[Doc (computing)|DOC]] is a [[document]] file format that is directly supported by few ebook readers. Its advantages as an ebook format is that it can be easily converted to other ebook formats and it can be reflowed. It can be easily edited.\n\n===DOCX===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Microsoft Word (XML)\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[DOCX]]\n|}\n\n[[DOCX]] is a [[document]] file format that is directly supported by few ebook readers. Its advantages as an ebook format are that it can be easily converted to other ebook formats and it can be reflowed. It can be easily edited.\n\n=== EPUB ===\n{{Main article|EPUB}}\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| IDPF/EPUB\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .epub\n|}\n[[File:EPUB logo.svg|thumb|right|150px|The EPUB logo]]\nThe .epub or [[OEBPS]] format is a technical standard for e-books created by the [[International Digital Publishing Forum]] (IDPF).\n\nThe EPUB format has gained some popularity as a vendor-independent XML-based e-book format. The format can be read by the [[Kobo eReader]], [[BlackBerry]] devices, Apple\'s [[iBooks]] app running on [[Macintosh]] computers and [[IOS (Apple)|iOS]] devices, [[Google Play|Google Books]] app running on [[Android (operating system)|Android]] and iOS devices, Barnes & Noble [[Nook]], Amazon [[Kindle Fire]],<ref name="kdp.amazon.com"/> [[Sony Reader]], [[BeBook]], [[Cybook Gen3|Bookeen Cybook Gen3 (with firmware v2 and up)]], COOL-ER, [[Adobe Digital Editions]], [[Lexcycle Stanza]], BookGlutton, AZARDI, [[FBReader]], [[Aldiko]], [[CoolReader]], [[Mantano Reader]], [[Moon+ Reader]], the [[Mozilla Firefox]] [[Add-on (Mozilla)|add-on]] [[EPUBReader]], [[Okular]] and other reading apps.\n\n[[Adobe Digital Editions]] uses .epub format for its e-books, with [[digital rights management]] (DRM) protection provided through their proprietary ADEPT mechanism. The ADEPT framework and scripts have been reverse-engineered to circumvent this DRM system.<ref>{{cite web|author= |url=http://i-u2665-cabbages.blogspot.com/2009/02/circumventing-adobe-adept-drm-for-epub.html |title=i♥cabbages: Circumventing Adobe ADEPT DRM for EPUB |publisher=I-u2665-cabbages.blogspot.com |date=2009-02-18 |accessdate=2015-08-31}}</ref>\n\n===eReader ===\n;Formerly Palm Digital Media/Peanut Press\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Palm Media\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[PDB (Palm OS)|pdb]]\n|}\n\neReader is a [[freeware]] program for viewing Palm Digital Media electronic books which use the pdb format used by many Palm applications. Versions are available for [[Android (operating system)|Android]], [[BlackBerry]], [[IOS (Apple)|iOS]], [[Palm OS]] (not webOS), [[Symbian]], [[Windows Mobile]] Pocket PC/Smartphone, and [[OS X]]. The reader shows text one page at a time, as paper books do. eReader supports embedded hyperlinks and images. Additionally, the [[Lexcycle Stanza|Stanza]] application for the [[iPhone]] and [[iPod touch]] can read both [[encryption|encrypted]] and unencrypted eReader files.\n\nThe program supports features like bookmarks and footnotes, enabling the user to mark any page with a bookmark and any part of the text with a footnote-like commentary. Footnotes can later be exported as a Memo document.\n\nOn July 20, 2009, [[Barnes & Noble]] made an announcement<ref>{{cite web|url=http://www.barnesandnobleinc.com/press_releases/2009_july_20_ebookstore.html |title=Barnes & Noble Booksellers |publisher=Barnesandnobleinc.com |date=2009-07-20 |accessdate=2015-08-31}}</ref> implying that eReader would be the company\'s preferred format to deliver e-books. Exactly three months later, in a press release by [[Adobe Systems|Adobe]], it was revealed Barnes & Noble would be joining forces with the software company to standardize the EPUB and PDF eBook formats.<ref>{{cite press release\n | title = Barnes & Noble adopts open EPUB eBook Format, PDF and Adobe Content Server | publisher = [[Adobe Systems]] | date = 2009-10-20 | url = https://www.adobe.com/aboutadobe/pressroom/pressreleases/200910/AdobeandBarnesNobleJoinForcestoStandardizeeBookTechnology.html | accessdate = 2013-05-06}}</ref><ref>{{Citation|last=Rothman |first=David |title=‘Barnes & Noble adopts open EPUB eBook Format, PDF and Adobe Content Server’ |publisher=TeleRead |date=2009-10-20 |url=http://www.teleread.com/ebooks/barnes-noble-adopts-open-epub-ebook-format-pdf-and-adobe-content-server/ |accessdate=2013-05-06 |archiveurl=https://web.archive.org/web/20130506010320/http://www.teleread.com:80/ebooks/barnes-noble-adopts-open-epub-ebook-format-pdf-and-adobe-content-server/ |archivedate=2013-05-06 |deadurl=yes |df= }}</ref> Barnes & Noble e-books are now sold mostly in EPUB format.<ref>{{Citation | last = Bell | first = Ian | title = Barnes & Noble Adopts ePub Standard; Aligns With Adobe | publisher = [[Digital Trends]] | date = 2009-11-18 | url = http://www.digitaltrends.com/gadgets/barnes-aligns-with-adobe/ | accessdate = 2013-05-06}}</ref><ref>{{Citation|last=Meadows |first=Chris |title=Barnes & Noble quietly changes e-book format, neglects to tell consumers |publisher=TeleRead |date=2009-12-13 |url=http://www.teleread.com/drm/barnes-noble-quietly-changes-e-book-format-neglects-to-tell-consumers/ |accessdate=2013-05-06 |archiveurl=https://web.archive.org/web/20130130085503/http://www.teleread.com:80/drm/barnes-noble-quietly-changes-e-book-format-neglects-to-tell-consumers/ |archivedate=2013-01-30 |deadurl=yes |df= }}</ref><ref>{{Citation | last = James | first = Kendrick | title = Has Barnes & Noble Changed Its e-Book Format to ePUB? | publisher = [[GigaOM]] | date = 2009-12-14 | url = http://gigaom.com/2009/12/14/has-barnes-noble-changed-its-e-book-format-to-epub/ | accessdate = 2013-05-06}}</ref>\n\n===FictionBook (Fb2) ===\n\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| FictionBook\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[FictionBook|fb2]]\n|}\n\n[[FictionBook]]<ref>[http://haali.cs.msu.ru/pocketpc/FictionBook_description.html]  {{webarchive |url=https://web.archive.org/web/20070703204958/http://haali.cs.msu.ru/pocketpc/FictionBook_description.html |date=July 3, 2007 }}</ref> is a popular [[XML]]-based e-book format, supported by free readers such as [[FBReader]], [[Okular]], [[CoolReader]], [[Bebook]] and [[STDU Viewer]].\n\nThe FictionBook format does not specify the appearance of a document; instead, it describes its structure and semantics. All the ebook metadata, such as the author name, title, and publisher, is also present in the ebook file. Hence the format is convenient for automatic processing, indexing, and ebook collection management. This also is convenient to store books in it for later automatic conversion into other formats.\n\n===Founder Electronics ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Apabi Reader\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[XEB|xeb]]; .ceb\n|}\n[[APABI]] is a format devised by [[Founder Electronics]]. It is a popular format for Chinese e-books. It can be read using the [[Apabi Reader]] software, and produced using [[Apabi Publisher]]. Both .xeb and .ceb files are encoded binary files. The [[iLiad (E-book Reader)|Iliad]] e-book device includes an Apabi \'viewer\'.\n\n===Hypertext Markup Language ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Hypertext\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .htm; .html and typically auxiliary images, js and css\n|}\n[[HTML]] is the [[markup language]] used for most [[World Wide Web|web]] pages. E-books using HTML can be read using a [[Web browser]]. The specifications for the format are  available without charge from the [[W3C]].\n\nHTML adds specially marked meta-elements to otherwise plain text encoded using [[character set]]s like [[ASCII]] or [[UTF-8]]. As such, suitably formatted files can be, and sometimes are, generated \'\'by hand\'\' using a \'\'[[text editor|plain text editor]]\'\' or \'\'[[Source code editor|programmer\'s editor]]\'\'. Many \'\'HTML generator\'\' applications exist to ease this process and often require less intricate knowledge of the format details involved.\n\nHTML on its own is not a particularly efficient format to store information in, requiring more storage space for a given work than many other formats. However, several e-Book formats including the Amazon Kindle, Open eBook, Compiled HTML,  Mobipocket and EPUB store each book chapter in HTML format, then use [[ZIP (file format)|ZIP]] compression to compress the HTML data, images, metadata and style sheets into a single, significantly smaller, file.\n\nHTML files encompass a wide range of standards<ref>{{cite web|url=http://www.webstandards.org/learn/faq/ |title=Frequently Asked Questions (FAQ) - The Web Standards Project |publisher=Webstandards.org |date=2002-02-27 |accessdate=2015-08-31}}</ref> and displaying HTML files correctly can be complicated. Additionally many of the features supported, such as forms, are not relevant to e-books.\n\n===iBook (Apple) ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| iBook\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .ibooks\n|}\nThe .ibooks format is created with the free [[iBooks Author]] ebook layout software from [[Apple Inc.]]. This proprietary format is based on the [[EPUB]] standard, with some differences in the CSS tags used in an ibooks format file, thus making it incompatible with the EPUB specification. The End-User Licensing Agreement (EULA) that comes with iBooks Author states that "If you want to charge a fee for a work that includes files in the .ibooks format generated using iBooks Author, you may only sell or distribute such work through Apple". The "through Apple" will typically be in the Apple [[iBooks]] store. The EULA further states that "This restriction does not apply to the content of such works when distributed in a form that does not include files in the .ibooks format." Therefore, Apple has not included distribution restrictions in the iBooks Author EULA for ibooks format ebooks created in iBooks Author that are made available for free, and it does not prevent authors from re-purposing the content in other ebook formats to be sold outside the iBookstore. This software currently supports import and export functionally for three formats. ibook, Plain text and PDF. The iBooks Author 2.3 and later supports importing EPUB and export EPUB 3.0.<ref>{{Cite web|url=https://support.apple.com/en-us/HT204884|title=About ePubs created with iBooks Author|language=en-US|access-date=2016-09-25}}</ref>\n\n===IEC 62448===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| IEC 62448\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n|\n|}\n\nIEC 62448 is an international standard created by [[International Electrotechnical Commission]] (IEC), Technical Committee 100, Technical Area 10 (Multimedia e-publishing and e-book).\n\nThe current version of IEC 62448 is an umbrella standard that contains as appendices two concrete formats, XMDF of Sharp and BBeB of Sony. However, BBeB has been discontinued by Sony and the version of XMDF that is in the specification is out of date. The IEC TA10 group is discussing the next steps, and has invited the IDPF organization which has standardized [[EPUB]] to be a liaison. It is possible that the current version of EPUB and/or the forthcoming EPUB3 revision may be added to IEC 62448.  Meanwhile, a number of Japanese companies have proposed that IEC standardize a proposed new Japanese-centric file format that is expected to unify DotBook of Voyager Japan and XMDF of Sharp.  This new format has not been publicly disclosed as of November 2010 but it is supposed to cover basic representations for the Japanese language.  Technically speaking, this revision is supposed to provide a Japanese minimum set, a Japanese extension set, and a stylesheet language. These issues were discussed in the TC100 meeting held  in October 2010 but no decisions were taken besides offering the liaison status to IDPF.\n\n===INF (IBM) ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| IBM & Open Source\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .inf\n|}\n[[IBM]] created this e-book format and used it extensively for [[OS/2]] and other of its operating systems. The INF files were often digital versions of printed books that came with some bundles of OS/2 and other products. There were many other newsletters and monthly publications (e.g.: EDM/2) available in the INF format too.\n\nThe advantage of INF is that it is very compact and very fast. It also supports images, reflowed text, tables and various list formats. INF files get generated by compiling the markup text files — in the [[Information Presentation Facility]] (IPF) format — into binary files.\n\nOriginally only IBM created an INF viewer and compiler, but later open source viewers like NewView, DocView and others appeared. There is also an open source IPF compiler named WIPFC, created by the [[Open Watcom]] project.\n\n===KF8 (Amazon Kindle) ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Kindle\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .azw3; .azw; .kf8\n|}\nWith the release of the [[Kindle Fire]] reader in late 2011, [[Amazon.com]] also released [[Kindle Format 8]], their newest file format, also known as .AZW3. The .azw3 file format supports a subset of [[HTML5]] and [[CSS3]] features, with some additional nonstandard features; the new data is stored within a container which can also be used to store a Mobi content document, allowing limited backwards compatibility.<ref>{{cite web|url=http://www.amazon.com/gp/feature.html?docId=1000729511|title=Kindle Format 8 Overview|publisher=Amazon.com|year=2012}}</ref><ref>{{cite web|url=http://musingsandmarvels.com/2012/03/06/the-new-kindle-format-8-kf8/|title=The New Kindle Format KF8|publisher=Musings and Marvels:Learning the ins and outs of the publishing industry|date=2012-03-06|accessdate=2012-03-16}}</ref><ref>{{cite web|url=http://www.amazon.com/gp/feature.html/ref=amb_link_357613502_6?ie=UTF8&docId=1000729901&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=right-4&pf_rd_r=0GN9VRRB0NJ08VXGFKWK&pf_rd_t=1401&pf_rd_p=1343256942&pf_rd_i=1000729511|title=HTML5 tags supported by KF8|publisher=Amazon.com|accessdate=2012-03-16}}</ref>\n\nOlder [[Amazon Kindle|Kindle]] e-readers use the proprietary format, AZW. It is based on the [[Mobipocket]] standard, with a slightly different serial number scheme (it uses an [[asterisk]] instead of a [[dollar sign]]) and its own [[Digital rights management|DRM]] formatting. Because the ebooks bought on the Kindle are delivered over its wireless system called Whispernet, the user does not see the AZW files during the download process. The Kindle format is available on a variety of platforms, such as through the Kindle app for the various mobile device platforms.\n\n===Microsoft LIT ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Microsoft Reader\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[LIT (file format)|lit]]\n|}\nDRM-protected LIT files are only readable in the proprietary [[Microsoft Reader]] program, as the .LIT format, otherwise similar to Microsoft\'s [[Microsoft Compiled HTML Help|CHM]] format, includes [[Digital Rights Management]] features. Other third party readers, such as [[Lexcycle Stanza]], can read unprotected LIT files.\n\nThe Microsoft Reader uses patented [[ClearType]] display technology. In Reader navigation works with a keyboard, mouse, stylus, or through electronic bookmarks. The Catalog Library records reader books in a personalized "home page", and books are displayed with ClearType to improve readability.  A user can add annotations and notes to any page, create large-print e-books with a single command, or create free-form drawings on the reader pages.  A built-in dictionary allows the user to look up words.\n\nIn August 2011, Microsoft announced they were discontinuing both Microsoft Reader and the use of the .lit format for ebooks<ref>{{cite web|url=http://aazae.com/|title=Ebooks|work=Aazae}}</ref> at the end of August 2012, and ending sales of the format on November 8, 2011.<ref>"Microsoft is discontinuing Microsoft Reader effective August 30, 2012, which includes download access of the Microsoft Reader application from the Microsoft Reader website."[http://www.microsoft.com/reader/ Microsoft Reader] {{webarchive |url=https://web.archive.org/web/20050822035209/http://www.microsoft.com/reader/ |date=August 22, 2005 }}</ref>\n\n===Mobipocket ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Mobipocket\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| [[PRC (Palm OS)|.prc]]; .mobi\n|}\nThe [[Mobipocket]] e-book format is based on the [[Open eBook]] standard using [[XHTML]] and can include [[JavaScript]] and frames. It also supports native [[SQL]] queries to be used with embedded databases. There is a corresponding e-book reader.\n\nThe [[Mobipocket]] Reader has a home page library. Readers can add blank pages in any part of a book and add free-hand drawings. Annotations – highlights, bookmarks, corrections, notes, and drawings – can be applied, organized, and recalled from a single location. Images are converted to GIF format and have a maximum size of 64K,<ref>{{cite web|url=http://www.mobipocket.com/dev/article.asp?BaseFolder=creatorhome&File=image.htm |title=Mobipocket Developer Center - Importing Image files |publisher=Mobipocket.com |date= |accessdate=2015-08-31}}</ref> sufficient for mobile phones with small screens, but rather restrictive for newer gadgets. [[Mobipocket]] Reader has electronic bookmarks,  and a built-in dictionary.\n\nThe reader has a full screen mode for reading and support for many [[Personal digital assistant|PDAs]], [[Personal digital assistant|Communicators]], and [[Smartphone]]s. [[Mobipocket]] products support most Windows, Symbian, BlackBerry and Palm operating systems, but not the Android platform. Using WINE, the reader works under Linux or Mac OS X. Third-party applications like [[Okular]] and [[FBReader]] can also be used under Linux or Mac OS X, but they work only with unencrypted files.\n\nThe Amazon Kindle\'s AZW format is basically just the Mobipocket format with a slightly different serial number scheme (it uses an [[asterisk]] instead of a [[dollar sign]]), and .prc publications can be read directly on the Kindle.  The Kindle AZW format also lacks some Mobipocket features such as JavaScript.<ref>{{cite web|url=http://www.mobileread.com/forums/showpost.php?p=1299906&postcount=2 |title=MobileRead Forums - View Single Post - Javascript in mobi ebooks? |publisher=Mobileread.com |date=2010-12-29 |accessdate=2015-08-31}}</ref>\n\n[[Amazon.com|Amazon]] has developed an .epub to .mobi converter called KindleGen,<ref>{{cite web|url=http://www.mobipocket.com/dev/ |title=Mobipocket Developer Center |publisher=Mobipocket.com |date= |accessdate=2015-08-31}}</ref> and it supports IDPF 1.0 and IDPF 2.0 EPUB format.\n\n===Multimedia eBooks ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Eveda\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .exe or .html\n|}\nA [[multimedia ebook]] is [[media (communication)|media]] and [[book]] [[content (media and publishing)|content]] that utilizes a combination of different book [[content format]]s. The term can be used as a noun (a medium with multiple content formats) or as an adjective describing a medium as having multiple content formats.\n\nThe "multimedia ebook" term is used in contrast to media which only utilize traditional forms of printed or text books. Multimedia ebooks include a combination of [[Written language|text]], [[Audio file format|audio]], [[image]]s, [[video]], or [[interactive]] content formats.  Much like how a traditional book can contain images to help the text tell a story, a multimedia ebook can contain other elements not formerly possible to help tell the story.\n\nWith the advent of more widespread tablet-like computers, such as the [[smartphone]], some publishing houses are planning to make multimedia ebooks, such as Penguin.<ref>[http://paidcontent.co.uk/article/419-first-look-how-penguin-will-reinvent-books-with-ipad/ ] {{webarchive |url=https://web.archive.org/web/20100617170741/http://paidcontent.co.uk/article/419-first-look-how-penguin-will-reinvent-books-with-ipad/ |date=June 17, 2010 }}</ref>\n\n===Newton eBook ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Newton eBook\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .pkg\n|}\nCommonly known as an [[Apple Newton]] book; a single Newton package file can contain multiple books (for example, the three books of a trilogy might be packaged together). All systems running the Newton operating system (the most common include the Newton MessagePads, eMates, Siemens Secretary Stations, Motorola Marcos, Digital Ocean Seahorses and Tarpons) have built-in support for viewing Newton books. The Newton package format was released to the public by Newton, Inc. prior to that company\'s absorption into Apple Computer. The format is thus arguably open and various people have written readers for it (writing a Newton book converter has even been assigned as a university-level class project<ref>{{cite web|url=http://metcs.bu.edu/~feneric/cs331/Archives/Project2002/ |accessdate=July 6, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20060904191234/http://metcs.bu.edu/~feneric/cs331/Archives/Project2002/ |archivedate=September 4, 2006 }}</ref>).\n\nNewton books have no support for DRM or encryption. They do support internal links, potentially multiple tables of contents and indexes, embedded gray scale images, and even some scripting capability (for example, it\'s possible to make a book in which the reader can influence the outcome).<ref>{{cite web|url=http://tools.unna.org/wikiwikinewt/index.php/MakeNewtonEbooksIndex |title=WikiWikiNewt Undergoing Maintenance |publisher=Tools.unna.org |date= |accessdate=2015-08-31}}</ref> Newton books utilize [[Unicode]] and are thus available in numerous languages. An individual [[Newton book]] may actually contain multiple views representing the same content in different ways (such as for different screen resolutions).\n\n===Open Electronic Package===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Open eBook\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .opf\n|}\n\n[[Open eBook|OPF]] is an [[XML]]-based e-book format created by E-Book Systems; it has been superseded by the EPUB electronic publication standard.\n\n===Portable Document Format ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Portable Document Format\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[Portable Document Format|pdf]]\n|}\n\nInvented by [[Adobe Systems]], and first released in 1993, [[PDF]] became ISO 32000 in 2008. The format was developed to provide a platform-independent means of exchanging fixed-layout documents. Derived from [[PostScript]], but without language features like loops, PDF adds support for features such as compression, passwords, semantic structures and DRM. Because PDF documents can easily be viewed and printed by users on a variety of computer [[Platform (computing)|platforms]], they are very common on the [[World Wide Web]] and in document management systems worldwide. The current PDF specification, ISO 32000-1:2008, is available from ISO\'s website, and under special arrangement, without charge from Adobe.<ref>{{cite web|url=https://www.adobe.com/devnet/pdf/pdf_reference.html |title=PDF Reference and Adobe Extensions to the PDF Specification &#124; Adobe Developer Connection |publisher=Adobe.com |date=2007-01-29 |accessdate=2015-08-31}}</ref>\n\nBecause the format is designed to reproduce fixed-layout pages, re-flowing text to fit mobile device and e-book reader screens has traditionally been problematic. This limitation was addressed in 2001 with the release of PDF Reference 1.5 and "Tagged PDF",<ref>{{cite web|author= |url=http://www.planetpdf.com/enterprise/article.asp?ContentID=6067 |title=What is Tagged PDF? |publisher=Planet PDF |date= |accessdate=2015-08-31}}</ref> but 3rd party support for this feature was limited until the release of [[PDF/UA]] in 2012.\n\nMany products support creating and reading PDF files, such as Adobe Acrobat, [[PDFCreator]] and [[OpenOffice.org]], and several programming libraries such as [[iText]] and [[Formatting Objects Processor|FOP]]. Third party viewers such as [[xpdf]] and [[Nitro PDF]] are also available. Mac OS X has built-in PDF support, both for creation as part of the printing system and for display using the built-in Preview application.\n\nPDF files are supported by almost all modern e-book readers, tablets and smartphones. However, PDF reflow based on Tagged PDF, as opposed to re-flow based on the actual sequence of objects in the content-stream, is not yet commonly supported on mobile devices. Such Re-flow options as may exist are usually found under "view" options, and may be called "word-wrap".\n\n===Plain text files ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| text\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .txt\n|}\nThe first e-books in history were in [[text file|plain text]] (.txt) format, supplied for free by the [[Project Gutenberg]] community, but the format itself existed before the e-book era. The plain text format doesn\'t support digital rights management (DRM) or formatting options (such as different fonts, graphics or colors), but it has excellent portability as it is the simplest e-book encoding possible as a plain text file contains only [[ASCII]] or [[Unicode]] text (text files with [[UTF-8]] or [[UTF-16]] encoding are also popular for languages other than English). Almost all operating systems can read ASCII text files (e.g. Unix, Macintosh, Microsoft Windows, DOS and other systems) and newer operating systems support Unicode text files as well. The only potential for portability problems of ASCII text files is that operating systems differ in their preferred line ending convention and their interpretation of values outside the ASCII range (their character encoding). Conversion of files from one to another line-ending convention is easy with free software. DOS and Windows uses CRLF, Unix and Apple\'s OS X use LF, Mac OS up to and including OS 9 uses CR. By convention, lines are often broken to fit into 80 characters, a legacy of older terminals and consoles. Alternately, each paragraph may be a single line.\n\nThe size in bytes of a text file is simply the number of characters, including spaces, and with a new line counting for 1 or 2. For example, the [[Bible]], which is approximately 800,000 words, is about 4 MB.<ref name="bible">{{cite web|url=http://www.gutenberg.org/ebooks/10 |accessdate=January 10, 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20081205071232/http://www.gutenberg.org/ebooks/10 |archivedate=December 5, 2008 }}</ref>\n\n===Plucker ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Plucker\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n|.pdb\n|}\n[[Plucker]] is an Open Source [[free software|free]] mobile and desktop e-book reader application with its own associated file format and software to automatically generate Plucker files from text, PDF, HTML, or other document format files, web sites or RSS feeds.  The format is public and well-documented. Free readers are available for all kinds of desktop computers and many PDAs.\n\n===PostScript ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| PostScript\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[PostScript|ps]]\n|}\n[[PostScript]] is a [[page description language]] used in the electronic and [[desktop publishing]] areas for defining the contents and layout of a printed page, which can be used by a rendering program to assemble and create the actual output [[Raster graphics|bitmap]]. Many office printers directly support interpreting PostScript and printing the result. As a result, the format also sees wide use in the [[Unix]] world.<!-- IE if you don\'t want to fool around with output filters, ghostscript, and whatnot, get a postscript printer. Most Unix programs with specialized ``print\'\' functions output ps anyway (pity the firefox print renderer sucks so much). Don\'t see a way to comment on that here so left in a comment. Would be nice if (the higher end) ebook readers would add a ps interpreter, though -->\n\n===RTF===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| Rich Text Format\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .[[Rich Text Format|rtf]]\n|}\n\n[[Rich Text Format]] is a [[document]] file format that is supported by many ebook readers. Its advantages as an ebook format is that it is widely supported, and it can be reflowed. It can be easily edited. It can be easily converted to other ebook formats, increasing its support.\n\n===SSReader ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| SSReader\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .pdg\n|}\nThe digital book format used by a popular digital library company 超星数字图书馆<ref>[http://www.ssreader.com/downland_index.asp ]{{dead link|date=August 2015}}</ref> in China.  It is a proprietary raster image compression and binding format, with reading time OCR plug-in modules.  The company scanned a huge number of Chinese books in the China National Library and this becomes the major stock of their service.  The detailed format is not published.  There are also some other commercial e-book formats used in Chinese digital libraries.\n\n===Text Encoding Initiative ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| [[TEI Lite]]\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .xml{{Citation needed|date=August 2009}}\n|}\n[[TEI Lite]] is the most{{Citation needed|date=September 2010}} popular of the [[Text Encoding Initiative|TEI]]-based (and thus [[XML]]-based or [[Standard Generalized Markup Language|SGML]]-based) electronic text formats.\n\n===TomeRaider ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| TomeRaider\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| .tr2; .tr3\n|}\n\nThe [[TomeRaider]] e-book format is a proprietary format. There are versions of [[TomeRaider]] for Windows, Windows Mobile (aka Pocket PC), Palm, Symbian and iPhone. Several Wikipedias are available as [[Wikipedia:TomeRaider database|TomeRaider files]] with all articles unabridged, some even with nearly all images. Capabilities of the TomeRaider3 e-book reader vary considerably per platform: the Windows and Windows Mobile editions support full [[HTML]] and [[CSS]]. The Palm edition supports limited HTML (e.g., no tables, no fonts), and CSS support is missing. For Symbian there is only the older TomeRaider2  format, which does not render images or offer category search facilities. Despite these differences any TomeRaider e-book can be browsed on all supported platforms.  The Tomeraider website<ref name="tomeraider.com">{{cite web|url=http://www.tomeraider.com/ |title=tomeraider.com |publisher=tomeraider.com |date=2015-06-24 |accessdate=2015-08-31}}</ref> claims to have over 4000 e-books available, including free versions of the [[Internet Movie Database]] and Wikipedia.\n\n===Open XML Paper Specification ===\n{| style="text-align:left;"\n|-\n| style="background:#ddd; width:100px;"| \'\'Format\'\':\n| OpenXPS\n|-\n| style="background:#ddd;"| \'\'Published as\'\':\n| [[Open XML Paper Specification|.oxps, .xps]]\n|}\n\n\'\'\'Open XML Paper Specification\'\'\' (also referred to as \'\'\'OpenXPS\'\'\') is an open [[specification]] for a [[page description language]] and a fixed-document format. [[Microsoft]] developed it as the XML Paper Specification (XPS). In June 2009, [[Ecma International]] adopted it as international standard \'\'\'ECMA-388\'\'\'.<ref>{{cite web|url=http://www.ecma-international.org/publications/standards/Ecma-388.htm |title=Standard ECMA-388 |publisher=Ecma-international.org |date= |accessdate=2015-08-31}}</ref>\n\nThe format is intentionally restricted to sequences of:\nGlyphs (a fixed run of text),\nPaths (a geometry that can be filled, or stroked, by a brush), and\nBrushes (a description of a shaped brush used to in rendering paths).\n\nThis reduces the possibility of inadvertent introduction of malicious content and simplifies the implementation of compatible renderers.\n\n== Comparison tables ==\n\n=== Features ===\n{| class="wikitable sortable" style="text-align: center; width:75%;"\n|-\n! \'\'\'Format\'\'\'\n! [[Filename extension]]\n! DRM support\n! Image support\n! Table support\n! Sound support\n! Interactivity support\n! [[Word wrap]] support\n! [[Open standard|Open]] [[Open standard|standard]]\n! Embedded annotation support\n! Book- marking\n! Video support\n|-\n| [[Comic Book Archive]]\n| .cbr, .cbz, .cb7, .cbt, .cba\n| ?\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n| [[DjVu]]\n| .djvu\n| ?\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| ?\n|-\n| [[Doc (computing)|DOC]]\n| .doc\n| ?\n| {{yes}}\n| {{yes}}\n| ?\n| ?\n| {{yes}}\n| {{no}}\n| ?\n| ?\n| ?\n|-\n| [[DOCX]]\n| .docx\n| ?\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| ?\n| {{yes}}\n| {{no}}\n| ?\n| ?\n| {{yes}}\n|-\n| [[EPUB]] (IDPF)\n| .epub\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes-no}}<ref name="depreader" group="f">Depends on the eReader application</ref>\n| {{yes-no}}<ref name="depreader" group="f">Depends on the eReader application</ref>\n| {{yes}}<ref group="f">With ePub 3</ref>\n|-\n| [[FictionBook]]\n| .fb2\n| {{no}}\n| {{yes}}\n| {{yes-no}}<ref group="f">Table support added in FictionBook V2.1. Not supported in V2.0</ref>\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| ?\n| ?\n|-\n| [[HTML]]\n| .html\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group="f" name="html5">With HTML 5</ref>\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{yes}}<ref group="f" name="html5" />\n|-\n| [[iBooks]]\n| .ibook\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|-\n| [[Information Presentation Facility|INF]]\n| .inf\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| ?\n| {{yes}}\n| {{yes}}\n| {{yes-no}}<ref name="depreader" group="f">Depends on the eReader application</ref>\n| {{yes-no}}<ref name="depreader" group="f">Depends on the eReader application</ref>\n| {{no}}\n|-\n| [[Amazon Kindle|Kindle]]\n| .azw\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group="f">Supported in all except 1st Generation Kindle. (Support level is as it is in mobipocket)</ref><ref>{{cite web|author=Joshua Tallent |url=http://kindleformatting.com/blog/2009/02/kindle-2-review-formatting-perspective.php |title=Kindle 2 Review, the Formatting Perspective |publisher=Kindle Formatting |date=2009-02-25 |accessdate=2015-08-31}}</ref>\n| {{yes}}<ref group="f" name="iOS">Supported only in kindle for iPhone, iPod, iPad.</ref><ref name="amazon.com">{{cite web|url=http://www.amazon.com/b?ie=UTF8&node=2248263011 |title=Kindle Editions with Audio-Video: Kindle Store |publisher=Amazon.com |date= |accessdate=2015-08-31}}</ref>\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group="f" name="iOS"/><ref name="amazon.com"/>\n|-\n| [[Microsoft Reader]]\n| .lit\n| {{yes}}\n| {{yes}}\n| ?\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| ?\n| {{yes}}\n| ?\n|-\n| [[Mobipocket]]\n| .prc, .mobi\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| ?\n|-\n| [[Multimedia EBook]]\n| .exe\n| {{yes}}\n| {{yes}}\n| ?\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| ?\n|-\n| [[Newton Book]]\n| .pkg\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n|-\n| [[#eReader|eReader]]\n| .pdb\n| {{yes}}\n| {{yes}}\n| ?\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| ?\n|-\n| [[Plain text]]\n| .txt\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n| [[Plucker]]\n| .pdb\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| ?\n|-\n| [[Portable Document Format]]\n| .pdf\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes-no}}<ref group="f">"Reflow" is implemented by some readers.</ref><ref>{{cite web|url=https://www.adobe.com/uk/epaper/tips/acr5reflow/ |title=Reflow the contents of Adobe PDF documents: Tutorial |publisher=Adobe.com |date=2001-04-02 |accessdate=2015-08-31}}</ref>\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group="f">With Flash Embeded</ref>\n|-\n| [[PostScript]]\n| .ps\n| {{no}}\n| {{yes}}\n| ?\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| ?\n| ?\n| ?\n|-\n| [[Tome Raider]]\n| .tr2, .tr3\n| {{yes}}\n| {{yes}}\n| ?\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| ?\n| ?\n| ?\n|-\n| [[OpenXPS]]\n| .oxps, .xps\n| ?\n| {{yes}}\n| {{yes}}\n| ?\n| {{no}}\n| {{no}}\n| {{yes}}\n| ?\n| ?\n| ?\n|}\n<references group="f"/>\n\n=== Supporting platforms ===\n{| class="wikitable sortable" style="text-align: center; width:75%;"\n|-\n! \'\'\'Reader&nbsp;\'\'\'\n! Plain text\n! PDF\n! ePub\n! HTML\n! Mobi- Pocket\n! Fiction- Book (Fb2)\n! DjVu\n! Broadband eBook (BBeB)<ref group=h name=propr>Proprietary format</ref>\n! eReader<ref group=h name=propr/>\n! Kindle<ref group=h name=propr/>\n! WOLF<ref group=h name=propr/>\n! Tome Raider<ref group=h name=propr/>\n! Open eBook<ref group=h>Predecessor of ePUB</ref>\n! Comic Book\n! OpenXPS\n|-\n| Amazon Kindle&nbsp;1\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Amazon Kindle&nbsp;2,&nbsp;DX\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| [[Amazon Kindle]]&nbsp;3\n| {{yes}}\n| {{yes}}\n| {{no}}<ref group=h name=3part>Yes, if the Duokan alternate Kindle OS (third-party software add-on) is used.</ref>\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| [[Kindle Fire|Amazon Kindle Fire]]\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group=h>By adding epub capable apps, such as [[Aldiko]]</ref>\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Android Devices\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group=h name=firm>Requires latest firmware</ref><ref>{{cite web|url=http://ireader.over-blog.com/ |title=iReader |publisher=Ireader.over-blog.com |date= |accessdate=2015-08-31}}</ref>\n| {{yes}}\n| {{yes}}<ref group=h name=firm/><ref>{{cite web|url=https://code.google.com/p/vudroid/ |title=vudroid - Android djvu and pdf viewer - Google Project Hosting |publisher=Code.google.com |date= |accessdate=2015-08-31}}</ref>\n| {{no}}\n| {{yes}}<ref group=h name=firm/><ref>{{cite web|author= |url=http://www.barnesandnoble.com/u/nook-for-android/379002287 |title=Rise of the Android by Apps for Nook &#124; 2940147132807 &#124; NOOK App &#124; Barnes & Noble |publisher=Barnesandnoble.com |date= |accessdate=2015-08-31}}</ref>\n| {{yes}}\n| {{no}}\n| {{yes}}<ref group=h name=firm/><ref name="tomeraider.com"/>\n| {{yes}}<ref group=h name=firm/>\n| {{dunno}}\n| {{yes}}\n|-\n| Apple iOS Devices\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group=h name=firm/>\n| {{yes}}<ref group=h name=firm/>\n| {{yes}}<ref group=h name=firm/>\n| {{no}}\n| {{yes}}<ref group=h name=firm/>\n| {{yes}}<ref group=h name=firm/>\n| {{no}}\n| {{yes}}<ref group=h name=firm/>\n| {{yes}}<ref group=h name=firm/>\n| {{yes}}<ref group=h>With third party apps, such as CloudReader</ref>\n| {{dunno}}\n|-\n| Azbooka WISEreader\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| [[Barnes & Noble Nook]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| [[Barnes & Noble Nook Color]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Bookeen Cybook Gen3, Opus\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group=h name=epmb>Versions support either ePUB or MobiPocket</ref>\n| {{yes}}\n| {{yes}}<ref group=h name=epmb/>\n| {{yes}}<ref group=h>Only ePUB version and with FW 2.0+</ref>\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{dunno}}\n| {{dunno}}\n|-\n| COOL-ER Classic\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| [[Linux]] Operating System\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group=h>KDE\'s [[Okular]] supports fb2</ref>\n| {{yes}}\n| {{yes}}<ref group=h>[[Calibre (software)|Calibre]] supports lrf/lrx</ref>\n| {{partial|?}}\n| {{partial|?}}\n| {{partial|?}}\n| {{partial|?}}\n| {{partial|?}}\n| {{yes}}\n| {{yes}}\n|-\n| [[eSlick|Foxit eSlick]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Hanlin e-Reader&nbsp;V3\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Hanvon WISEreader\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| iRex iLiad\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Iriver Story\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{yes}}<ref group=h name=firm/>\n| {{yes}}<ref group=h name=firm/>\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| [[Kobo eReader]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{dunno}}\n|-\n| Nokia N900\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{dunno}}\n|-\n| NUUTbook&nbsp;2\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| OLPC XO, Sugar\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Onyx Boox 60\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Mac OS X\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{dunno}}\n| {{yes}}\n| {{yes}}\n| {{dunno}}\n| {{dunno}}\n| {{yes}}\n| {{dunno}}\n| {{dunno}}\n|-\n| TrekStor eBook Reader Pyrus<ref>{{cite web|url=http://www.trekstor.co.uk/detail-ebook-reader-en/product/ebook-reader-pyrus-mini.html |title=Home - SurfTabs, smart phones, MiniPCs, data storage, MP3-Player - TrekStor GmbH |publisher=Trekstor.co.uk |date= |accessdate=2015-08-31}}</ref>\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Windows\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref group=h>[[ICE Book Reader]] for Windows supports fb2</ref>\n| {{yes}}\n| {{dunno}}\n| {{yes}}\n| {{yes}}<ref group=h>DRM-protected publications are supported as of Kindle for PC v1.3.0</ref>\n| {{dunno}}\n| {{dunno}}\n| {{yes}}\n| {{dunno}}\n| {{yes}}<ref group="h">XP or later, not on Windows 2000</ref>\n|-\n| Pocketbook 301&nbsp;Plus, 302, 360°\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Pocketbook Aqua\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Sony Reader\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Viewsonic VEB612\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|-\n| Windows Phone 7\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{dunno}}\n| {{dunno}}\n|}\n<references group=h/>\n\n== See also ==\n* [[Comparison of e-book readers]]\n* [[Comparison of Android e-book reader software]] – includes software e-book readers for Android devices\n* [[Comparison of iOS e-book reader software]] – includes software e-book readers for iOS devices\n* [[ICUE]], a British company using mobile phone (cellphone) technology to deliver books and other publications\n\n== References ==\n;General information\n{{Refbegin}}\n* {{cite book|last=Cavanaugh|first=T W|title=The Digital Reader: Using E-Books in K-12 Education|year=2006|publisher=International Society for Technology in Education|location=Eugene, Oregon|isbn=1564842215}}\n* {{cite book|last=Chandler|first=S|title=From Entrepreneur to Infopreneur: Make Money with Books, EBooks, and Information Products|year=2010|publisher=John Wiley & Sons|location=Hoboken, New Jersey|isbn=1118044770}}\n* Cope, B., & Mason, D. (2002). Markets for electronic book products. C-2-C series, bk. 3.2. Altona, Vic: Common Ground Pub.\n* {{cite book|last=Henke|first=H|title=Electronic Books and Epublishing: A Practical guide for Authors.|year=2001|publisher=Springer|location=London|isbn=1852334355}}\n* Hanttula, D. (2001). Pocket PC handbook.\n* {{cite book|last=Rich|first=J|title=Self-Publishing For Dummies|year=2006|publisher=John Wiley & Sons|location=Hoboken, New Jersey|isbn=0470100370}}\n{{Refend}}\n;Footnotes\n{{Reflist|30em}}\n\n==External links==\n*[http://wiki.mobileread.com/wiki/Main_Page ebook reader articles at Mobile Read Wiki]\n*[http://digbib.ubka.uni-karlsruhe.de/volltexte/1000010574 Daisy 3: A Standard for Accessible Multimedia Books]\n*[https://www.eff.org/deeplinks/2009/12/e-book-privacy An E-Book Buyer\'s Guide to Privacy]\n\n{{Ebooks}}\n\n{{DEFAULTSORT:Comparison Of E-Book Formats}}\n[[Category:Electronic documents]]\n[[Category:Electronic publishing]]\n[[Category:Computing comparisons]]']
['TheSwizzle.com', '37690869', "{{Multiple issues|\n{{COI|date=August 2013}}\n{{refimprove|date=August 2013}}\n}}\n{{Infobox dot-com company\n| name   = TheSwizzle\n| logo         = \n[[File:TheSwizzle.com Logo.jpg|150x50px|TheSwizzle.com Logo]]\n| company_type   = [[Privately held company|Private]]\n| location_city    = [[New York City]],<ref>{{cite web|url=http://www.theswizzle.com |title=TheSwizzle |accessdate=2012-11-19}}</ref> [[New York (state)|New York]]\n| location_country = USA\n| foundation = 2010\n| founder                   = [[Scott Kurnit]]\n| registration              = Optional\n| current_status            = Inactive\n| industry       = [[advertising]], [[online advertising]], [[email]]\n| homepage       = [http://www.theswizzle.com/ www.theswizzle.com]\n}}\n'''TheSwizzle''' was a [[webmail]] tool that worked with existing email and enabled consumers to manage email subscriptions, primarily from commercial vendors.<ref>{{cite web |accessdate=November 19, 2012 |url=http://mashable.com/2012/10/03/swizzle-emails |title=The Swizzle Cleans Your Inbox By Combining Promo E-mails Into a Daily Digest |publisher=Mashable |date=October 25, 2012 |author=Veena Bissram }}</ref><ref>{{cite web |accessdate=November 19, 2012 |url=http://revision3.com/tzdaily/swizzle-junk-email |title=Clean Junk Mail From Your Inbox! |publisher=revision3 |date=October 25, 2012 |author=Veronica Belmont }}</ref><ref>{{cite web |accessdate=November 19, 2012 |url=http://www.pcmag.com/article2/0,2817,2411068,00.asp |title=Swizzle |publisher=pcmag |date=October 17, 2012 |author=Samara Lynn }}</ref> It was acquired by Mailstrom of 410 Labs in September 2014, and TheSwizzle.com subsequently shut down.<ref>{{Cite news|url=http://technical.ly/baltimore/2014/09/11/mailstrom-the-410-labs-email-helper-lands-ex-competitors-users-swizzle/|title=Mailstrom, the 410 Labs email helper, lands ex-competitor's users - Technical.ly Baltimore|date=2014-09-11|newspaper=Technical.ly Baltimore|language=en-US|access-date=2016-12-20}}</ref>\n\n==Features==\nThe product claims several features, including cleaning up users' inboxes by helping to unsubscribe from unwanted emails while at the same time, allowing receipt as well as searching among those commercially oriented emails an individual still wants to receive. By packaging these messages into a digest format, users can consolidate their email box.\n\n==History==\nThe Swizzle is a product of Keep Holdings, a consumer and brand engagement conglomerate of business units including Keep.com, [[AdKeeper]] and TheSwizzle.com.\nThe company was founded in March, 2010,<ref>{{cite web |accessdate=January 17, 2010 |url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=114604562 |title=AdKeeper, Inc. Snapshot |publisher=Bloomberg Businessweek |author=Staff }}</ref> by [[Scott Kurnit]], who serves as Chairman and CEO. Kurnit is best known as the founder of [[About.com]], which grew to a public market value of $1.7 billion, and was sold to Primedia for $724 million, in 2001. About.com is now owned by [[IAC (company)|IAC]].\n\n==See also==\n* [[Scott Kurnit]]\n* [[AdKeeper]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Official website|http://www.theswizzle.com/}}\n\n{{DEFAULTSORT:Swizzle}}\n[[Category:Electronic documents]]\n[[Category:Internet properties established in 2010]]"]
['Portable Document Format', '24077', '{{Redirect|PDF|3=}}\n{{Infobox file format\n| name                   = Portable Document Format\n| icon                   = [[File:Adobe PDF.svg|frameless|SVG logo|150px]]\n| iconcaption            = Adobe PDF icon\n| extension              = .pdf\n| mime                   = {{plainlist|\n* <code>application/pdf</code>,<ref name="rfc3778">{{citation |url=http://tools.ietf.org/html/rfc3778 |title=The application/pdf Media Type, RFC 3778, Category: Informational |year=2004}}</ref>\n* <code>application/x-pdf</code>\n* <code>application/x-bzpdf</code>\n* <code>application/x-gzpdf</code>\n }}\n| _nomimecode            = true\n| magic                  = <code>%PDF</code>\n| released               = {{Start date and age|1993|6|15}}\n| standard               = ISO 32000-1\n| free                   = Yes\n| url                    = {{URL|https://www.adobe.com/devnet/pdf/pdf_reference_archive.html}}\n| image                  = \n| typecode               = \'PDF \'<ref name="rfc3778" /> (including a single space)\n| uniform type           = com.adobe.pdf\n| owner                  = [[Adobe Systems]]\n| latest release version = 1.7\n| latest release date    = <!-- {{Start date and age|YYYY|mm|dd|df=yes}} -->\n| genre                  =\n| container for          =\n| contained by           =\n| extended from          =\n| extended to            = [[PDF/A]], [[PDF/E]], [[PDF/UA]], [[PDF/VT]], [[PDF/X]]\n}}\nThe \'\'\'Portable Document Format\'\'\' (\'\'\'PDF\'\'\') is a [[file format]] used to present [[document]]s in a manner independent of [[application software]], [[Computer hardware|hardware]], and [[operating system]]s.<ref name="pdf-ref-1.7">Adobe Systems Incorporated, [https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf PDF Reference, Sixth edition, version 1.23 (30 MB)], Nov 2006, p. 33.</ref> Each PDF file encapsulates a complete description of a fixed-layout flat document, including the text, [[font]]s, graphics, and other information needed to display it. <!-- Today, three dimensional objects can be embedded in PDF documents with Acrobat 3D using [[U3D]] or [[PRC (file format)|PRC]] and various other data formats.<ref name="3d#1" /><ref name="3d#2" /> -->\n\n<blockquote>A PDF file captures document text, fonts, images, and even formatting of documents from a variety of applications. You can e-mail a PDF document to your friends and it will look the same on their screens as it looks on yours, even if they have Apple computers and you have a PC.<ref>[http://techterms.com/definition/pdf TechTerms.com]</ref>\n</blockquote>\n\n== History and standardization ==\n\n{{main article|History and standardization of Portable Document Format}}\n\nPDF was developed in the early 1990s<ref>{{cite web|url=http://www.planetpdf.com/enterprise/article.asp?ContentID=6650|title=Adobe\'s Bob Wulff knows Acrobat and PDF -- inside and out}}</ref> as a way to share computer documents, including text formatting and inline images.<ref>{{cite web|url=http://www.planetpdf.com/planetpdf/pdfs/warnock_camelot.pdf|title=The Camelot Project}}</ref> It was among a number of competing formats such as [[DjVu]], [[Envoy (WordPerfect)|Envoy]], Common Ground Digital Paper, Farallon Replica and even [[Adobe Systems|Adobe]]\'s own [[PostScript]] format. In those early years before the rise of the [[World Wide Web]] and [[HTML]] documents, PDF was popular mainly in [[desktop publishing]] [[workflow]]s.\nAdobe Systems made the PDF specification available free of charge in 1993. PDF was a [[proprietary format]] controlled by Adobe, until it was officially released as an [[open standard]] on July 1, 2008, and published by the [[International Organization for Standardization]] as ISO 32000-1:2008,<ref name="iso-standard">{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51502 |title=ISO 32000-1:2008 - Document management – Portable document format – Part 1: PDF 1.7 |publisher=Iso.org |date=2008-07-01 |accessdate=2010-02-21}}</ref><ref>{{cite web|last=Orion |first=Egan |title=PDF 1.7 is approved as ISO 32000 |work=[[The Inquirer]] |publisher=[[The Inquirer]] |date=2007-12-05 |url=http://www.theinquirer.net/gb/inquirer/news/2007/12/05/pdf-approved-iso-32000 |accessdate=2007-12-05 |deadurl=yes |archiveurl=https://web.archive.org/web/20071213004627/http://www.theinquirer.net/gb/inquirer/news/2007/12/05/pdf-approved-iso-32000 |archivedate=December 13, 2007 }}</ref> at which time control of the specification passed to an ISO Committee of volunteer industry experts. In 2008, Adobe published a Public Patent License to ISO 32000-1 granting [[royalty-free]] rights for all patents owned by Adobe that are necessary to make, use, sell, and distribute PDF compliant implementations.<ref>{{citation |url=https://www.adobe.com/pdf/pdfs/ISO32000-1PublicPatentLicense.pdf |title=Public Patent License, ISO 32000-1: 2008 – PDF 1.7 |author=Adobe Systems Incorporated |year=2008 |accessdate=2011-07-06}}</ref>\n\nHowever, there are still some proprietary technologies defined only by Adobe, such as [[XFA|Adobe XML Forms Architecture]] (XFA) and [[JavaScript]] extension for Acrobat, which are referenced by ISO 32000-1 as [[normative]] and indispensable for the application of the ISO 32000-1 specification. These proprietary technologies are not standardized and their specification is published only on Adobe’s website.<ref>{{cite web |url=http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=SWD:2013:0224:FIN:EN:PDF |title=Guide for the procurement of standards-based ICT - Elements of Good Practice, Against lock-in: building open ICT systems by making better use of standards in public procurement |quote=Example: ISO/IEC 29500, ISO/IEC 26300 and ISO 32000 for document formats reference information that is not accessible by all parties (references to proprietary technology and brand names, incomplete scope or dead web links). |publisher=European Commission |date=2013-06-25 |accessdate=2013-10-20}}</ref><ref name="iso-meeting-n603">{{citation |url=http://pdf.editme.com/files/pdfREF-meetings/ISO-TC171-SC2-WG8_N0603_SC2WG8_MtgRept_SLC.pdf |title=ISO/TC 171/SC 2/WG 8 N 603 - Meeting Report |quote=XFA is not to be ISO standard just yet. ... The Committee urges Adobe Systems to submit the XFA Specification, XML Forms Architecture (XFA), to ISO for standardization ... The Committee is concerned about the stability of the XFA specification ... Part 2 will reference XFA 3.1 |date=2011-06-27}}</ref><ref>{{cite web |url=http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0069446.s001 |title=Embedding and publishing interactive, 3-dimensional, scientificfigures in Portable Document Format (PDF) files |quote=... the implementation of the U3D standard was not complete and proprietary extensions were used. |accessdate=2013-10-20}}</ref><ref name="rosenthol-adobe-2012">{{cite web |url=http://cdn.parleys.com/p/5148922a0364bc17fc56c6e5/iSUM2012_00_LRO_presentation.pdf |title=PDF and Standards |author=Leonard Rosenthol, Adobe Systems |year=2012 |accessdate=2013-10-20}}</ref><ref>{{citation |url=http://www.planetpdf.com/enterprise/article.asp?ContentID=Is_PDF_an_open_standard&page=1 |title=Is PDF an open standard? - Adobe Reader is the de facto Standard, not PDF |author=Duff Johnson |date=2010-06-10 |accessdate=2014-01-19}}</ref> Many of them are also not supported by popular third-party implementations of PDF.  So when organizations publish PDFs which use these proprietary technologies, they present accessibility issues for some users.\n\nIn 2014, ISO TC 171 voted to deprecate XFA for ISO 32000-2 ("Next-generation PDF").<ref name="DRAFT INTERNATIONAL">{{Cite web|url=http://www.iso.org/iso/catalogue_detail.htm?csnumber=63534|title=DRAFT INTERNATIONAL STANDARD ISO/DIS 32000-2|last=|first=|date=|website=|publisher=ISO|access-date=2016-08-04|quote=Editor’s note: XFA forms have been deprecated from ISO 32000-2 in accordance with the outcome of the letter ballot following the Pretoria meetings.}}</ref>\n\n== Technical foundations ==\nThe PDF combines three technologies:\n* A subset of the [[PostScript]] page description programming language, for generating the layout and graphics.\n* A [[font embedding|font-embedding]]/replacement system to allow fonts to travel with the documents.\n* A structured storage system to bundle these elements and any associated content into a single file, with [[data compression]] where appropriate.\n\n=== PostScript ===\n[[PostScript]] is a [[page description language]] run in an [[interpreter (computing)|interpreter]] to generate an image, a process requiring many resources. It can handle graphics and standard features of [[programming language]]s such as <code>if</code> and <code>loop</code> commands. PDF is largely based on PostScript but simplified to remove flow control features like these, while graphics commands such as <code>lineto</code> remain.\n\nOften, the PostScript-like PDF code is generated from a source PostScript file. The graphics commands that are output by the PostScript code are collected and [[Lexical analysis|tokenized]]. Any files, graphics, or fonts to which the document refers also are collected. Then, everything is compressed to a single file. Therefore, the entire PostScript world (fonts, layout, measurements) remains intact.\n\nAs a document format, PDF has several advantages over PostScript:\n* PDF contains tokenized and interpreted results of the PostScript source code, for direct correspondence between changes to items in the PDF page description and changes to the resulting page appearance.\n* PDF (from version 1.4) supports [[transparency (graphic)|graphic transparency]]; PostScript does not.\n* PostScript is an [[interpreted programming language]] with an implicit global state, so instructions accompanying the description of one page can affect the appearance of any following page. Therefore, all preceding pages in a PostScript document must be processed to determine the correct appearance of a given page, whereas each page in a PDF document is unaffected by the others. As a result, PDF viewers allow the user to quickly jump to the final pages of a long document, whereas a PostScript viewer needs to process all pages sequentially before being able to display the destination page (unless the optional PostScript [[Document Structuring Conventions]] have been carefully complied with).\n\n== Technical overview ==\n\n=== File structure ===\n\nA PDF file is a 7-bit ASCII file, except for certain elements that may have binary content.\nA PDF file starts with a header containing the [[magic number (programming)|magic number]] and the version of the format such as <code>%PDF-1.7</code>. The format is a subset of a COS ("Carousel" Object Structure) format.<ref>{{cite web|url=http://jimpravetz.com/blog/2012/12/in-defense-of-cos/|title=In Defense of COS, or Why I Love JSON and Hate XML|author=Jim Pravetz|work=jimpravetz.com}}</ref> A COS tree file consists primarily of \'\'objects\'\', of which there are eight types:<ref>Adobe Systems, PDF Reference, p. 51.</ref>\n* [[Boolean data type|Boolean]] values, representing \'\'true\'\' or \'\'false\'\'\n* Numbers\n* [[String (computer science)|Strings]], enclosed within parentheses (<code>(...)</code>), may contain 8-bit characters.\n* Names, starting with a forward slash (<code>/</code>)\n* [[Array data type|Array]]s, ordered collections of objects enclosed within square brackets (<code>[...]</code>)\n* [[Dictionary (data structure)|Dictionaries]], collections of objects indexed by Names enclosed within double pointy brackets (<code>&lt;&lt;...&gt;&gt;</code>)\n* [[Stream (computing)|Streams]], usually containing large amounts of data, which can be compressed and binary\n* The [[Pointer (computer programming)|null]] object\nFurthermore, there may be comments, introduced with the percent sign (<code>%</code>). Comments may contain 8-bit characters.\n\nObjects may be either \'\'direct\'\' (embedded in another object) or \'\'indirect\'\'. Indirect objects are numbered with an \'\'object number\'\' and a \'\'generation number\'\' and defined between the <code>obj</code> and <code>endobj</code> keywords. An index table, also called the cross-reference table and marked with the <code>xref</code> keyword, follows the main body and gives the byte offset of each indirect object from the start of the file.<ref>Adobe Systems, PDF Reference, pp. 39–40.</ref> This design allows for efficient [[random access]] to the objects in the file, and also allows for small changes to be made without rewriting the entire file (\'\'incremental update\'\'). Beginning with PDF version 1.5, indirect objects may also be located in special streams known as \'\'object streams\'\'. This technique reduces the size of files that have large numbers of small indirect objects and is especially useful for \'\'Tagged PDF\'\'.\n\nAt the end of a PDF file is a trailer introduced with the <code>trailer</code> keyword. It contains\n\n* a dictionary\n* an offset to the start of the cross-reference table (the table starting with the <code>xref</code> keyword)\n* and the <code>%%EOF</code> [[end-of-file]] marker.\n\nThe dictionary contains\n\n* a reference to the root object of the tree structure, also known as the \'\'catalog\'\'\n* the count of indirect objects in the cross-reference table\n* and other optional information.\n\nThere are two layouts to the PDF files: non-linear (not "optimized") and linear ("optimized"). Non-linear PDF files consume less disk space than their linear counterparts, though they are slower to access because portions of the data required to assemble pages of the document are scattered throughout the PDF file. Linear PDF files (also called "optimized" or "web optimized" PDF files) are constructed in a manner that enables them to be read in a Web browser plugin without waiting for the entire file to download, since they are written to disk in a linear (as in page order) fashion.<ref name="pdf-ref">{{cite web |url=https://www.adobe.com/devnet/pdf/pdf_reference.html |title=Adobe Developer Connection: PDF Reference and Adobe Extensions to the PDF Specification |publisher=Adobe Systems |accessdate=2010-12-13}}</ref> PDF files may be optimized using [[Adobe Acrobat]] software or [[QPDF]].\n\n=== Imaging model ===\nThe basic design of how [[graphics]] are represented in PDF is very similar to that of PostScript, except for the use of [[transparency (graphic)|transparency]], which was added in PDF 1.4.\n\nPDF graphics use a [[device independence|device-independent]] [[Cartesian coordinate system]] to describe the surface of a page. A PDF page description can use a [[matrix (mathematics)|matrix]] to [[scale (ratio)|scale]], [[rotate]], or [[Shear mapping|skew]] graphical elements. A key concept in PDF is that of the \'\'graphics state\'\', which is a collection of graphical parameters that may be changed, saved, and restored by a \'\'page description\'\'. PDF has (as of version 1.6) 24 graphics state properties, of which some of the most important are:\n* The \'\'current transformation matrix\'\' (CTM), which determines the coordinate system\n* The \'\'[[clipping path]]\'\'\n* The \'\'[[color space]]\'\'\n* The \'\'[[alpha compositing|alpha constant]]\'\', which is a key component of transparency\n\n==== Vector graphics ====\nAs in PostScript, [[vector graphics]] in PDF are constructed with \'\'paths\'\'. Paths are usually composed of lines and cubic [[Bézier curve]]s, but can also be constructed from the outlines of text. Unlike PostScript, PDF does not allow a single path to mix text outlines with lines and curves. Paths can be stroked, filled, or used for [[clipping path|clipping]]. Strokes and fills can use any color set in the graphics state, including \'\'patterns\'\'.\n\nPDF supports several types of patterns. The simplest is the \'\'tiling pattern\'\' in which a piece of artwork is specified to be drawn repeatedly. This may be a \'\'colored tiling pattern\'\', with the colors specified in the pattern object, or an \'\'uncolored tiling pattern\'\', which defers color specification to the time the pattern is drawn. Beginning with PDF 1.3 there is also a \'\'shading pattern\'\', which draws continuously varying colors. There are seven types of shading pattern of which the simplest are the \'\'axial shade\'\' (Type 2) and \'\'radial shade\'\' (Type 3). <!-- Pictures desperately needed here! -->\n\n==== Raster images ====\n[[Raster graphics|Raster images]] in PDF (called \'\'Image XObjects\'\') are represented by dictionaries with an associated stream. The dictionary describes properties of the image, and the stream contains the image data. (Less commonly, a raster image may be embedded directly in a page description as an \'\'inline image\'\'.) Images are typically \'\'filtered\'\' for compression purposes. Image filters supported in PDF include the general purpose filters\n* \'\'\'ASCII85Decode\'\'\' a filter used to put the stream into 7-bit [[ASCII]]\n* \'\'\'ASCIIHexDecode\'\'\' similar to ASCII85Decode but less compact\n* \'\'\'FlateDecode\'\'\' a commonly used filter based on the [[deflate]] algorithm defined in RFC 1951 (deflate is also used in the [[gzip]], [[Portable Network Graphics|PNG]], and [[ZIP (file format)|zip]] file formats among others); introduced in PDF 1.2; it can use one of two groups of predictor functions for more compact zlib/deflate compression: \'\'Predictor 2\'\' from the [[TIFF]] 6.0 specification and predictors (filters) from the [[Portable Network Graphics|PNG]] specification (RFC 2083)\n* \'\'\'LZWDecode\'\'\' a filter based on [[LZW]] Compression; it can use one of two groups of predictor functions for more compact LZW compression: \'\'Predictor 2\'\' from the TIFF 6.0 specification and predictors (filters) from the PNG specification\n* \'\'\'RunLengthDecode\'\'\' a simple compression method for streams with repetitive data using the [[run-length encoding]] algorithm and the image-specific filters\n* \'\'\'DCTDecode\'\'\' a [[lossy]] filter based on the [[JPEG]] standard\n* \'\'\'CCITTFaxDecode\'\'\' a [[lossless]] [[bi-level image|bi-level]] (black/white) filter based on the Group 3 or [[Group 4 compression|Group 4]] [[CCITT]] (ITU-T) [[fax]] compression standard defined in ITU-T [[T.4]] and T.6\n* \'\'\'JBIG2Decode\'\'\' a lossy or lossless bi-level (black/white) filter based on the [[JBIG2]] standard, introduced in PDF 1.4\n* \'\'\'JPXDecode\'\'\' a lossy or lossless filter based on the [[JPEG 2000]] standard, introduced in PDF 1.5\n\nNormally all image content in a PDF is embedded in the file. But PDF allows image data to be stored in external files by the use of \'\'external streams\'\' or \'\'Alternate Images\'\'. Standardized subsets of PDF, including [[PDF/A]] and [[PDF/X]], prohibit these features.\n\n==== Text ====\nText in PDF is represented by \'\'text elements\'\' in page content streams. A text element specifies that \'\'characters\'\' should be drawn at certain positions. The characters are specified using the \'\'encoding\'\' of a selected \'\'font resource\'\'.\n\n===== Fonts =====\nA font object in PDF is a description of a digital [[typeface]]. It may either describe the characteristics of a typeface, or it may include an embedded \'\'font file\'\'. The latter case is called an \'\'embedded font\'\' while the former is called an \'\'unembedded font\'\'. The font files that may be embedded are based on widely used standard digital font formats: \'\'\'[[PostScript fonts|Type 1]]\'\'\' (and its compressed variant \'\'\'CFF\'\'\'), \'\'\'[[TrueType]]\'\'\', and (beginning with PDF 1.6) \'\'\'[[OpenType]]\'\'\'. Additionally PDF supports the \'\'\'Type 3\'\'\' variant in which the components of the font are described by PDF graphic operators. <!--- Type 3 bit is awkward and should be cleaned up --->\n\n===== Standard Type 1 Fonts (Standard 14 Fonts) =====\nFourteen typefaces, known as the \'\'standard 14 fonts\'\', have a special significance in PDF documents:\n* [[Times Roman|Times]] (v3) (in regular, italic, bold, and bold italic)\n* [[Courier (typeface)|Courier]] (in regular, oblique, bold and bold oblique)\n* [[Helvetica]] (v3) (in regular, oblique, bold and bold oblique)\n* [[Symbol (typeface)|Symbol]]\n* [[Zapf Dingbats]]\nThese fonts are sometimes called the \'\'base fourteen fonts\'\'.<ref>{{cite web|url=http://desktoppub.about.com/od/glossary/g/base14fonts.htm|title=Desktop Publishing: Base 14 Fonts - Definition|work=About.com Tech}}</ref> These fonts, or suitable substitute fonts with the same metrics, should be available in most PDF readers. However, since Adobe Acrobat version 6, most of these fonts are not \'\'guaranteed\'\' to be available in the reader, and may only display correctly if the system has them installed.<ref name="aquarium">[http://www.planetpdf.com/planetpdf/pdfs/pdf2k/03e/merz_fontaquarium.pdf The PDF Font Aquarium]</ref> Fonts may be substituted if they are not embedded in a PDF.\n\n===== Encodings =====\nWithin text strings, characters are shown using \'\'character codes\'\' (integers) that map to glyphs in the current font using an \'\'encoding\'\'. There are a number of predefined encodings, including \'\'WinAnsi\'\', \'\'MacRoman\'\', and a large number of encodings for East Asian languages, and a font can have its own built-in encoding. (Although the WinAnsi and MacRoman encodings are derived from the historical properties of the [[Microsoft Windows|Windows]] and [[Macintosh]] operating systems, fonts using these encodings work equally well on any platform.) PDF can specify a predefined encoding to use, the font\'s built-in encoding or provide a lookup table of differences to a predefined or built-in encoding (not recommended with TrueType fonts).<ref>{{cite web|url=https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf |title=PDF Reference Sixth Edition, version 1.7, table 5.11}}</ref> The encoding mechanisms in PDF were designed for Type 1 fonts, and the rules for applying them to TrueType fonts are complex.\n\nFor large fonts or fonts with non-standard glyphs, the special encodings \'\'Identity-H\'\' (for horizontal writing) and \'\'Identity-V\'\' (for vertical) are used. With such fonts it is necessary to provide a \'\'ToUnicode\'\' table if semantic information about the characters is to be preserved.\n\n==== Transparency ====\nThe original imaging model of PDF was, like PostScript\'s, \'\'opaque\'\': each object drawn on the page completely replaced anything previously marked in the same location. In PDF 1.4 the imaging model was extended to allow transparency. When transparency is used, new objects interact with previously marked objects to produce blending effects. The addition of transparency to PDF was done by means of new extensions that were designed to be ignored in products written to the PDF 1.3 and earlier specifications. As a result, files that use a small amount of transparency might view acceptably in older viewers, but files making extensive use of transparency could be viewed incorrectly in an older viewer without warning.\n\nThe transparency extensions are based on the key concepts of \'\'transparency groups\'\', \'\'blending modes\'\', \'\'shape\'\', and \'\'alpha\'\'. The model is closely aligned with the features of [[Adobe Illustrator]] version 9. The blend modes were based on those used by [[Adobe Photoshop]] at the time. When the PDF 1.4 specification was published, the formulas for calculating blend modes were kept secret by Adobe. They have since been published.<ref>[https://www.adobe.com/content/dam/Adobe/en/devnet/pdf/pdfs/pdf_reference_archives/blend_modes.pdf PDF Blend Modes Addendum]</ref>\n\nThe concept of a transparency group in PDF specification is independent of existing notions of "group" or "layer" in applications such as Adobe Illustrator. Those groupings reflect logical relationships among objects that are meaningful when editing those objects,\nbut they are not part of the imaging model.\n\n=== Interactive elements ===\n\nPDF files may contain interactive elements such as annotations, form fields, video and Flash animation.\n\n\'\'\'Rich Media PDF\'\'\' is a term that is used to describe interactive content that can be embedded or linked to inside of a PDF. This content must be produced using the Flash file format. When Adobe bought Macromedia, the jewel of the company was Flash, and the Flash player was embedded inside Adobe Acrobat and Adobe Reader, removing the need for third-party plug-ins such as Flash, QuickTime, or Windows Media. Unfortunately, this caused a rift with Apple as QuickTime video was prohibited from PDF.  [[Rich Media]] expert [[Bob Connolly (Canadian film director)#Books, eBooks and Magazine Articles|Robert Connolly]] believes this event triggered the war between Apple and Adobe over the Flash iPhone/iPad dispute. Rich Media PDF will not operate in Apple\'s iOS devices such as the iPad, and interactivity is limited.\n\n\'\'\'Interactive Forms\'\'\' is a mechanism to add forms to the PDF file format.\n\nPDF currently supports two different methods for integrating data and PDF forms. Both formats today coexist in PDF specification:<ref name="iso32000">{{citation |url=https://www.adobe.com/devnet/acrobat/pdfs/PDF32000_2008.pdf |title=Document Management – Portable Document Format – Part 1: PDF 1.7, First Edition |author=Adobe Systems Incorporated |date=2008-07-01 |accessdate=2010-02-19}}</ref><ref>{{cite web |url=http://gnupdf.org/Forms_Data_Format |title=Gnu PDF - PDF Knowledge - Forms Data Format |archiveurl=https://web.archive.org/web/20130101054615/http://www.gnupdf.org/Forms_Data_Format |archivedate=2013-01-01 |accessdate=2010-02-19}}</ref><ref>{{cite web |url=http://livedocs.adobe.com/coldfusion/8/htmldocs/help.html?content=formsPDF_02.html |title=About PDF forms |accessdate=2010-02-19}}</ref><ref>{{cite web |url=http://forums.adobe.com/thread/301733 |title=Convert XFA Form to AcroForm? |year=2008 |accessdate=2010-02-19}}</ref>\n* \'\'\'AcroForms\'\'\' (also known as \'\'\'Acrobat forms\'\'\'), introduced in the PDF 1.2 format specification and included in all later PDF specifications.\n* \'\'\'[[XML Forms Architecture|Adobe XML Forms Architecture]] (XFA)\'\'\' forms, introduced in the PDF 1.5 format specification. The XFA specification is not included in the PDF specification, it is only referenced as an optional feature. Adobe XFA Forms are not compatible with AcroForms.<ref>{{cite web |url=http://partners.adobe.com/public/developer/tips/topic_tip2.html |title=Migrating from Adobe Acrobat forms to XML forms |accessdate=2010-02-22}}</ref>\n\n==== AcroForms ====\nAcroForms were introduced in the PDF 1.2 format. AcroForms permit using objects (\'\'e.g.\'\' [[text box]]es, [[Radio button]]s, \'\'etc.\'\') and some code (\'\'e.g.\'\' [[JavaScript]]).\n\nAlongside the standard PDF action types, interactive forms (AcroForms) support submitting, resetting, and importing data. The "submit" action transmits the names and values of selected interactive form fields to a specified uniform resource locator (URL). Interactive form field names and values may be submitted in any of the following formats, (depending on the settings of the action’s ExportFormat, SubmitPDF, and XFDF flags):<ref name="iso32000" />\n* HTML Form format (HTML 4.01 Specification since PDF 1.5; HTML 2.0 since 1.2)\n* Forms Data Format (FDF)\n* XML Forms Data Format (XFDF) (external XML Forms Data Format Specification, Version 2.0; supported since PDF 1.5; it replaced the "XML" form submission format defined in PDF 1.4)\n* PDF (the entire document can be submitted rather than individual fields and values). (defined in PDF 1.4)\n\nAcroForms can keep form field values in external stand-alone files containing key:value pairs. The external files may use Forms Data Format (FDF) and XML Forms Data Format (XFDF) files.<ref>{{cite web |url=http://kb2.adobe.com/cps/325/325874.html |title=Using Acrobat forms and form data on the web |author=Adobe Systems Incorporated |date=2007-10-15 |accessdate=2010-02-19}}</ref><ref name="xfdf">{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfdf_2.0.pdf |format=PDF |title=XML Forms Data Format Specification, version 2 |date=September 2007 |accessdate=2010-02-19}}</ref><ref name="fdf-exchange">{{citation |url=https://www.adobe.com/devnet/acrobat/pdfs/fdf_data_exchange.pdf |format=PDF |title=FDF Data Exchange Specification |date=2007-02-08 |accessdate=2010-02-19}}</ref> The usage rights (UR) signatures define rights for import form data files in FDF, XFDF and text ([[comma-separated values|CSV]]/[[delimiter-separated values|TSV]]) formats, and export form data files in FDF and XFDF formats.<ref name="iso32000" />\n\n===== Forms Data Format (FDF) =====\n{{Infobox file format\n| name                   = Forms Data Format (FDF)\n| icon                   =\n| logo                   =\n| screenshot             =\n| caption                =\n| extension              = .fdf\n| mime                   = application/vnd.fdf<ref>{{citation |url=http://www.iana.org/assignments/media-types/application/ |title=IANA Application Media Types - vnd.fdf |accessdate=2010-02-22}}</ref>\n| type code              = \'FDF\'\n| uniform type           =\n| magic                  =\n| owner                  = [[Adobe Systems]]\n| released               = {{Start date|1996}}<!-- {{Start date|YYYY|mm|dd|df=yes}} --> (PDF 1.2)\n| latest release version =\n| latest release date    = <!-- {{Start date and age|YYYY|mm|dd|df=yes}} -->\n| genre                  =\n| container for          =\n| contained by           =\n| extended from          = PDF\n| extended to            = XFDF\n| standard               = ISO 32000-1:2008\n| free                   = Yes\n| url                    =\n}}\n\nThe Forms Data Format (FDF) is based on PDF, it uses the same syntax and has essentially the same file structure, but is much simpler than PDF, since the body of an FDF document consists of only one required object. Forms Data Format is defined in the PDF specification (since PDF 1.2). The Forms Data Format can be used when submitting form data to a server, receiving the response, and incorporating into the interactive form. It can also be used to export form data to stand-alone files that can be imported back into the corresponding PDF interactive form. Beginning in PDF 1.3, FDF can be used to define a container for annotations that are separate from the PDF document they apply to. FDF typically encapsulates information such as [[X.509|X.509 certificates]], requests for certificates, directory settings, timestamp server settings, and embedded PDF files for network transmission.<ref name="fdf-exchange" /> The FDF uses the MIME content type application/vnd.fdf, filename extension .fdf and on Mac OS it uses file type \'FDF\'.<ref name="iso32000" /> Support for importing and exporting FDF stand-alone files is not widely implemented in free or freeware PDF software. For example, there is no import/export support in Evince, Okular, Poppler, KPDF or Sumatra PDF, however, Evince, Okular and Poppler support filling in of PDF Acroforms and saving filled data inside the PDF file. Import support for stand-alone FDF files is implemented in Adobe Reader; export and import support (including saving of FDF data in PDF) is for example implemented in Foxit Reader and PDF-XChange Viewer Free; saving of FDF data in a PDF file is also supported in pdftk.\n\n===== XML Forms Data Format (XFDF) =====\n{{Infobox file format\n| name                   = XML Forms Data Format (XFDF)\n| icon                   =\n| logo                   =\n| screenshot             =\n| caption                =\n| extension              = .xfdf\n| mime                   = application/vnd.adobe.xfdf<ref>{{citation |url=http://www.iana.org/assignments/media-types/application/vnd.adobe.xfdf |title=IANA Application Media Types - Vendor Tree - vnd.adobe.xfdf |accessdate=2010-02-22}}</ref>\n| type code              = \'XFDF\'\n| uniform type           =\n| magic                  =\n| owner                  = [[Adobe Systems]]\n| released               = {{Start date|2003|07|df=yes}} (referenced in PDF 1.5)\n| latest release version = 3.0\n| latest release date    = {{Start date and age|2009|08|df=yes}}\n| genre                  =\n| container for          =\n| contained by           =\n| extended from          = PDF, FDF, [[XML]]\n| extended to            =\n| standard               = No (under standardization as ISO/CD 19444-1<ref name="iso-xfdf">{{citation |url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?ics1=35&ics2=240&ics3=30&csnumber=64911 |title=ISO/CD 19444-1 - Document management - XML forms data format - Part 1: XFDF 3.0 |accessdate=2014-11-26}}</ref>)\n| free                   =\n| url                    = [https://partners.adobe.com/public/developer/en/xml/XFDF_Spec_3.0.pdf XFDF 3.0 specification]\n}}\n\nXML Forms Data Format (XFDF) is the XML version of Forms Data Format, but the XFDF implements only a subset of FDF containing forms and annotations. There are not XFDF equivalents for some entries in the FDF dictionary - such as the Status, Encoding, JavaScript, Pages keys, EmbeddedFDFs, Differences and Target. In addition, XFDF does not allow the spawning, or addition, of new pages based on the given data; as can be done when using an FDF file. The XFDF specification is referenced (but not included) in PDF 1.5 specification (and in later versions). It is described separately in \'\'XML Forms Data Format Specification\'\'.<ref name="xfdf" /> The PDF 1.4 specification allowed form submissions in XML format, but this was replaced by submissions in XFDF format in the PDF 1.5 specification. XFDF conforms to the XML standard. As of November 2014, XFDF 3.0 is in the ISO/IEC standardization process under the formal name \'\'ISO/CD 19444-1 - Document management - XML forms data format - Part 1: XFDF 3.0\'\'.<ref name="iso-xfdf"/>\n\nXFDF can be used the same way as FDF; e.g., form data is submitted to a server, modifications are made, then sent back and the new form data is imported in an interactive form. It can also be used to export form data to stand-alone files that can be imported back into the corresponding PDF interactive form. A support for importing and exporting XFDF stand-alone files is not widely implemented in free or freeware PDF software. Import of XFDF is implemented in Adobe Reader 5 and later versions; import and export is implemented in PDF-XChange Viewer Free; embedding of XFDF data in PDF form is implemented in pdftk (pdf toolkit).\n\n==== Adobe XML Forms Architecture (XFA) ====\n{{Main article|XFA|l1=XML Forms Architecture}}\nIn the PDF 1.5 format, [[Adobe Systems]] introduced a new, proprietary format for forms, namely Adobe XML Forms Architecture (XFA) forms. The XFA 2.02 is referenced in the PDF 1.5 specification (and also in later versions) but is described separately in \'\'Adobe XML Forms Architecture (XFA) Specification\'\', which has several versions.<ref name="xfa-adobe">{{cite web |url=http://partners.adobe.com/public/developer/xml/index_arch.html |title=Adobe XML Forms Architecture (XFA) |author=Adobe Systems Incorporated |accessdate=2010-02-19}}</ref> XFA specification is not included in ISO 32000-1 PDF 1.7 and is only referenced as an external proprietary specification created by Adobe. XFA was not standardized as an ISO standard. In 2011 the ISO Committee (TC 171/SC 2/WG 8) urged Adobe Systems to submit the XFA Specification for standardization.<ref name="iso-meeting-n603" />\n\nAdobe XFA Forms are not compatible with AcroForms. Adobe Reader contains "disabled features" for use of XFA Forms, that activate only when opening a PDF document that was created using enabling technology available only from Adobe.<ref>{{citation |url=https://www.adobe.com/products/eulas/pdfs/Reader_Player_AIR_WWEULA-Combined-20080204_1313.pdf |format=PDF |title=Adobe Reader - Software license agreement |accessdate=2010-02-19}}</ref><ref>{{cite web|url=https://www.adobe.com/go/readerextensions |title=LiveCycle Reader Extensions ES features and benefits |accessdate=2010-02-19 |deadurl=yes |archiveurl=https://web.archive.org/web/20091219163323/http://www.adobe.com/go/readerextensions |archivedate=December 19, 2009 }}</ref> The XFA Forms are not compatible with Adobe Reader prior to version 6.\n\nXFA forms can be created and used as PDF files or as XDP ([[XML Data Package]]) files. The format of an XFA resource in PDF is described by the XML Data Package Specification.<ref name="iso32000" /> The XDP may be a standalone document or it may in turn be carried inside a PDF document. XDP provides a mechanism for packaging form components within a surrounding XML container. An XDP can also package a PDF file, along with XML form and template data.<ref name="xfa-adobe" /> PDF may contain XFA (in XDP format), but also XFA may contain PDF.<ref name="xfa-adobe" /> When the XFA (XML Forms Architecture) grammars used for an XFA form are moved from one application to another, they must be packaged as an XML Data Package.<ref name="xfa25">{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfa_spec_2_5.pdf |format=PDF |title=XML Forms Architecture (XFA) Specification Version 2.5 |date=2007-06-08 |accessdate=2010-02-19}}</ref>\n\nWhen the PDF and XFA are combined, the result is a form in which each page of the XFA form overlays a PDF background. This architecture is\nsometimes referred to as XFAF (XFA Foreground). The alternative is to express all of the form, including boilerplate, directly in XFA (without using PDF, or only using "Shell PDF" which is a container for XFA with minimal skeleton of PDF markup, or using a pre-rendered depiction of a static XFA form as PDF pages). It is sometimes called \'\'full\'\' XFA.<ref name="xfa25" />\n\nStarting with PDF 1.5, the text contents of variable text form fields, as well as markup annotations may include formatting information (style information). These rich text strings are XML documents that conform to the rich text conventions specified for the XML Forms Architecture specification 2.02, which is itself a subset of the XHTML 1.0 specification, augmented with a restricted set of CSS2 style attributes.<ref name="iso32000" />\nIn PDF 1.6, PDF supports the rich text elements and attributes specified in the XML Forms Architecture (XFA) Specification, 2.2.\nIn PDF 1.7, PDF supports the rich text elements and attributes specified in the XML Forms Architecture (XFA) Specification, 2.4.<ref name="iso32000" />\n\nMost PDF processors do not handle XFA content. When generating a shell PDF it is recommended to include in the PDF markup a simple one-page PDF image displaying a warning message (e.g. "To view the full contents of this document, you need a later version of the PDF viewer.", etc.). PDF processors that can render XFA content should either not display the supplied warning page image or replace it quickly with the dynamic form content.<ref name="xfa33">{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfa_spec_3_3.pdf |title=XML Forms Architecture (XFA) Specification Version 3.3 |date=2012-01-09 |accessdate=2014-04-09}}</ref> Examples of PDF software with some support of XFA rendering include Adobe Reader for Windows, Linux, macOS (but not Adobe Reader Mobile for Android or iOS) or Nuance PDF Reader.\n\nIn 2014, ISO TC 171 voted to deprecate XFA for ISO 32000-2 ("Next-generation PDF").<ref name="DRAFT INTERNATIONAL" />\n\n=== Logical structure and accessibility ===\n\nA "tagged" PDF (ISO 32000-1:2008 14.8) includes document structure and semantics information to enable reliable text extraction and accessibility. Technically speaking, tagged PDF is a stylized use of the format that builds on the logical structure framework introduced in PDF 1.3. Tagged PDF defines a set of standard structure types and attributes that allow page content (text, graphics, and images) to be extracted and reused for other purposes.<ref>[http://www.planetpdf.com/enterprise/article.asp?ContentID=6067 What is Tagged PDF?]</ref>\n\nTagged PDF is not required in situations where a PDF file is intended only for print. Since the feature is optional, and since the rules for Tagged PDF as specified in ISO 32000-1 are relatively vague, support for tagged PDF amongst consuming devices, including assistive technology (AT), is uneven.<ref>{{cite web|url=http://www.washington.edu/doit/Stem/articles?1002|title=Is PDF accessible?|work=washington.edu}}</ref>\n\nAn [[AIIM]] project to develop an ISO-standardized subset of PDF specifically targeted at accessibility began in 2004, eventually becoming [[PDF/UA]].\n\n=== Security and signatures ===\n\nA PDF file may be encrypted for security, or digitally signed for authentication.\n\nThe standard security provided by Acrobat PDF consists of two different methods and two different passwords, \'\'user password\'\', which encrypts the file and prevents opening, and \'\'owner password\'\', which specifies operations that should be restricted even when the document is decrypted, which can include: printing, copying text and graphics out of the document, modifying the document, or adding or modifying text notes and [[Acroforms|AcroForm]] fields. The user password (controls opening) encrypts the file and requires [[password cracking]] to defeat, with difficulty depending on password strength and encryption method – it is potentially very secure (assuming good password and encryption method without known attacks). The owner password (controls operations) does not encrypt the file, and instead relies on client software to respect these restrictions, and is not secure. An "owner password" can be removed by many commonly available "PDF cracking" software, including some free online services.<ref>{{cite web|url=http://freemypdf.com/|title=FreeMyPDF.com - Removes passwords from viewable PDFs|work=freemypdf.com}}</ref> Thus, the use restrictions that a document author places on a PDF document are not secure, and cannot be assured once the file is distributed; this warning is displayed when applying such restrictions using Adobe Acrobat software to create or edit PDF files.\n\nEven without removing the password, most freeware or open source PDF readers ignore the permission "protections" and allow the user to print or make copy of excerpts of the text as if the document were not limited by password protection.<ref>{{cite web |url= http://www.macworld.com/article/1137343/pdf.html |title=Adobe admits new PDF password protection is weaker |author= Jeremy Kirk}}</ref><ref>{{cite web |url= http://www.cs.cmu.edu/~dst/Adobe/Gallery/PDFsecurity.pdf  |title= How secure is PDF |author=Bryan Guignard}}</ref><ref>{{cite web |url= http://www.planetpdf.com/planetpdf/pdfs/pdf2k/01W/merz_securitykeynote.pdf |title= PDF Security Overview: Strengths and Weaknesses }}</ref>\n\nThere are a number of commercial solutions including [[Adobe LiveCycle]] [[Adobe LiveCycle#LiveCycle Rights Management ES4|Rights Management]] and Locklizard PDF DRM<ref>{{cite web |url=http://www.infosecurity-magazine.com/news/locklizard-develops-zero-footprint-solution-for/ |title=LockLizard Develops Zero Footprint Solution for PDF Security }}</ref> that are more robust means of [[information rights management]]. Not only can they restrict document access but they also reliably enforce [[File system permissions|permissions]] in ways that the standard security handler does not.<ref>{{cite web |url=http://www.locklizard.com/pdf_security_drm/ |title=PDF DRM Security Software for Adobe Document Protection}}</ref>\n\n==== Usage rights ====\nBeginning with PDF 1.5, Usage rights (UR) signatures are used to enable additional interactive features that are not available by default in a particular PDF viewer application. The signature is used to validate that the permissions have been granted by a bona fide granting authority. For example, it can be used to allow a user:<ref name="iso32000" />\n* to save the PDF document along with modified form and/or annotation data\n* import form data files in FDF, XFDF and text (CSV/TSV) formats\n* export form data files in FDF and XFDF formats\n* submit form data\n* instantiate new pages from named page templates\n* apply a [[Digital data|digital]] [[signature]] to existing [[digital signature]] form field\n* create, delete, modify, copy, import, export annotations\n\nFor example, Adobe Systems grants permissions to enable additional features in Adobe Reader, using public-key [[cryptography]]. Adobe Reader verifies that the signature uses a [[Public key certificate|certificate]] from an Adobe-[[authorize]]d certificate authority. The PDF 1.5 specification declares that other PDF viewer applications are free to use this same mechanism for their own purposes.<ref name="iso32000" />\n\n=== File attachments ===\n\nPDF files can have document-level and page-level file attachments, which the reader can access and open or save to their local filesystem. PDF attachments can be added to existing PDF files for example using [[pdftk]]. Adobe Reader provides support for attachments, and [[poppler (software)|poppler]]-based readers like [[Evince]] or [[Okular]] also have some support for document-level attachments.\n\n=== Metadata ===\nPDF files can contain two types of metadata.<ref>[https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf Adobe PDF reference version 1.7], section 10.2</ref> The first is the Document Information Dictionary, a set of key/value fields such as author, title, subject, creation and update dates. This is stored in the optional Info trailer of the file. A small set of fields is defined, and can be extended with additional text values if required.\n\nIn PDF 1.4, support was added for Metadata Streams, using the [[Extensible Metadata Platform]] (XMP) to add XML standards-based extensible metadata as used in other file formats. This allows metadata to be attached to any stream in the document, such as information about embedded illustrations, as well as the whole document (attaching to the document catalog), using an extensible schema.\n\n== Intellectual property ==\n\nAnyone may create applications that can read and write PDF files without having to pay royalties to [[Adobe Systems]]; Adobe holds patents to PDF, but licenses them for [[royalty-free]] use in developing software complying with its PDF specification.<ref>{{cite web|url=http://partners.adobe.com/public/developer/support/topic_legal_notices.html|title=Developer Resources|work=adobe.com}}</ref>\n\n== Technical issues ==\n\n=== Accessibility ===\nPDF files can be created specifically to be accessible for disabled people.<ref>{{cite web |url=http://www.webaim.org/techniques/acrobat/ |title=PDF Accessibility |publisher=WebAIM |accessdate=2010-04-24}}</ref><ref>{{cite web |url=http://www.alistapart.com/articles/pdf_accessibility |title=Facts and Opinions About PDF Accessibility |author=Joe Clark |date=2005-08-22 |accessdate=2010-04-24}}</ref><ref>{{cite web |url=http://wac.osu.edu/pdf/ |title=Accessibility and PDF documents |publisher=Web Accessibility Center |accessdate=2010-04-24}}</ref><ref>{{cite web |url=http://www.bbc.co.uk/guidelines/futuremedia/accessibility/accessible_pdf.shtml |title=PDF Accessibility Standards v1.2 |accessdate=2010-04-24}}</ref><ref>{{citation |url=http://www.csus.edu/training/handouts/workshops/creating_accessible_pdfs.pdf |format=PDF |title=PDF Accessibility |publisher=California State University |accessdate=2010-04-24}}</ref> PDF file formats in use {{As of|2014|lc=on}} can include tags ([[XML]]), text equivalents, captions, audio descriptions, etc. Tagged PDF is required in the [[PDF/A]]-1a specification.<ref>{{citation |url=http://www.aiim.org/documents/standards/PDF-A/19005-1_FAQ.pdf |title=Frequently Asked Questions (FAQs) – ISO 19005-1:2005 – PDF/A-1, Date: July 10, 2006 |format=PDF |date=2006-07-10 |accessdate=2011-07-06}}</ref><ref name="pdfa1-tech">{{cite web |url=http://www.pdfa.org/doku.php?id=artikel:en:pdfa_a_look_at_the_technical-side |title=PDF/A – A Look at the Technical Side |accessdate=2011-07-06}}</ref> Some software can automatically produce tagged PDFs, but this feature is not always enabled by default.<ref>{{citation |url=http://help.libreoffice.org/Common/Export_as_PDF#PDF.2FA-1a |title=LibreOffice Help - Export as PDF |accessdate=2012-09-22}}</ref><ref>{{citation |url=http://www.oooninja.com/2008/01/generating-pdfa-for-long-term-archiving.html |title=Exporting PDF/A for long-term archiving |date=2008-01-11}}</ref> Leading [[screen reader]]s, including [[JAWS (screen reader)|JAWS]], [[Window-Eyes]], Hal, and [[Kurzweil Educational Systems|Kurzweil 1000 and 3000]] can read tagged PDFs aloud, as can later versions of the Acrobat and Acrobat Reader programs.<ref>{{cite web |url=http://help.adobe.com/en_US/Reader/8.0/help.html?content=WS58a04a822e3e50102bd615109794195ff-7d15.html |title=Adobe Reader 8 - Read a PDF with Read Out Loud |accessdate=2010-04-24}}</ref><ref>{{cite news |url=http://gadgetwise.blogs.nytimes.com/2009/04/10/tip-of-the-week-adobe-readers-read-aloud-feature/ |title=Tip of the Week: Adobe Reader’s ‘Read Aloud’ Feature |accessdate=2010-04-24 | work=The New York Times | date=2009-04-10 |first=J.D. |last=Biersdorfer}}</ref><ref>{{citation |url=https://www.adobe.com/accessibility/pdfs/accessing-pdf-sr.pdf |format=PDF |title=Accessing PDF documents with assistive technology: A screen reader user\'s guide |publisher=Adobe |accessdate=2010-04-24}}</ref> Moreover, tagged PDFs can be re-flowed and magnified for readers with visual impairments. Problems remain with adding tags to older PDFs and those that are generated from scanned documents. In these cases, accessibility tags and re-flowing are unavailable, and must be created either manually or with OCR techniques. These processes are inaccessible to some disabled people.\n\nOne of the significant challenges with PDF accessibility is that PDF documents have three distinct views, which, depending on the document\'s creation, can be inconsistent with each other. The three views are (i) the physical view, (ii) the tags view, and (iii) the content view. The physical view is displayed and printed (what most people consider a PDF document). The tags view is what screen readers and other assistive technologies use to deliver a high-quality navigation and reading experience to users with disabilities. The content view is based on the physical order of objects within the PDF\'s content stream and may be displayed by software that does not fully support the tags view, such as the Reflow feature in Adobe\'s Reader.\n\n[[PDF/UA]], the International Standard for accessible PDF based on ISO 32000-1 was published as ISO 14289-1 in 2012, and establishes normative language for accessible PDF technology.\n\n=== Viruses and exploits ===\n{{see also|Adobe Acrobat#Security}}\nPDF attachments carrying viruses were first discovered in 2001. The virus, named \'\'OUTLOOK.PDFWorm\'\' or \'\'Peachy\'\', uses [[Microsoft Outlook]] to send itself as an attachment to an Adobe PDF file. It was activated with Adobe Acrobat, but not with Acrobat Reader.<ref>Adobe Forums, [https://forums.adobe.com/thread/302989 Announcement: PDF Attachment Virus "Peachy"], 15 August 2001.</ref>\n\nFrom time to time, new vulnerabilities are discovered in various versions of Adobe Reader,<ref>{{cite web|url=https://www.adobe.com/support/security/#readerwin |title=Security bulletins and advisories |publisher=Adobe |date= |accessdate=2010-02-21}}</ref> prompting the company to issue security fixes. Other PDF readers are also susceptible. One aggravating factor is that a PDF reader can be configured to start automatically if a web page has an embedded PDF file, providing a vector for attack. If a malicious web page contains an infected PDF file that takes advantage of a vulnerability in the PDF reader, the system may be compromised even if the browser is secure. Some of these vulnerabilities are a result of the PDF standard allowing PDF documents to be scripted with JavaScript. Disabling JavaScript execution in the PDF reader can help mitigate such future exploits, although it does not protect against exploits in other parts of the PDF viewing software. Security experts say that JavaScript is not essential for a PDF reader, and that the security benefit that comes from disabling JavaScript outweighs any compatibility issues caused.<ref>[http://www.grc.com/sn/sn-187.txt Steve Gibson - SecurityNow Podcast]</ref> One way of avoiding PDF file exploits is to have a local or web service convert files to another format before viewing.\n\nOn March 30, 2010 security researcher Didier Stevens reported an Adobe Reader and Foxit Reader exploit that runs a malicious executable if the user allows it to launch when asked.<ref>{{cite web|url=http://blogs.pcmag.com/securitywatch/2010/03/malicious_pdfs_execute_code_wi.php|title=Malicious PDFs Execute Code Without a Vulnerability|work=PCMAG}}</ref>\n\n=== Usage restrictions and monitoring ===\n\nPDFs may be [[encrypted]] so that a password is needed to view or edit the contents. The PDF Reference defines both 40-bit and 128-bit encryption, both making use of a complex system of [[RC4]] and [[MD5]]. The PDF Reference also defines ways that third parties can define their own encryption systems for PDF.\n\nPDF files may also contain embedded [[digital rights management|DRM]] restrictions that provide further controls that limit copying, editing or printing. The restrictions on copying, editing, or printing depend on the reader software to obey them, so the security they provide is limited.\n\nThe PDF Reference has technical details for an end-user overview.<ref>{{cite web|url=http://createpdf.adobe.com/cgi-feeder.pl/help_security?BP=&LOC=en_US |title=Create Adobe PDF Online - Security Settings Help |publisher=Createpdf.adobe.com |date= |accessdate=2010-02-21}}</ref>  Like HTML files, PDF files may submit information to a web server. This could be used to track the [[IP address]] of the client PC, a process known as [[phoning home]]. After update 7.0.5 to Acrobat Reader, the user is notified "...&nbsp;via a dialogue box that the author of the file is auditing usage of the file, and be offered the option of continuing."<ref>[https://www.adobe.com/support/techdocs/332208.html New features and issues addressed in the Acrobat 7.0.5 Update (Acrobat and Adobe Reader for Windows and Mac OS)]</ref>\n\nThrough its [[Adobe LiveCycle|LiveCycle Policy Server]] product, Adobe provides a method to set security policies on specific documents. This can include requiring a user to authenticate and limiting the period during which a document can be accessed or amount of time a document can be opened while offline. Once a PDF document is tied to a policy server and a specific policy, that policy can be changed or revoked by the owner. This controls documents that are otherwise "in the wild." Each document open and close event can also be tracked by the policy server. Policy servers can be set up privately or Adobe offers a public service through Adobe Online Services. As with other forms of DRM, adherence to these policies and restrictions may or may not be enforced by the reader software being used.\n\n=== Default display settings ===\nPDF documents can contain display settings, including the page display layout and zoom level. Adobe Reader uses these settings to override the user\'s default settings when opening the document.<ref>{{cite web | title=Getting Familiar with Adobe Reader &gt; Understanding Preferences | url=http://www.adobepress.com/articles/article.asp?p=412914 | accessdate=2009-04-22}}</ref> The free Adobe Reader cannot remove these settings.\n\n== Content ==\nA PDF file is often a combination of [[vector graphics]], text, and [[bitmap graphics]]. The basic types of content in a PDF are:\n* Text stored as content streams (i.e., not text)\n* Vector graphics for illustrations and designs that consist of shapes and lines\n* Raster graphics for photographs and other types of image\n* Multimedia objects in the document\n\nIn later PDF revisions, a PDF document can also support links (inside document or web page), forms, JavaScript (initially available as plugin for Acrobat 3.0), or any other types of embedded contents that can be handled using plug-ins.\n\nPDF 1.6 supports interactive 3D documents embedded in the PDF - 3D drawings can be embedded using [[U3D]] or [[PRC (file format)|PRC]] and various other data formats.<ref name="3d#1">{{cite web|url=https://www.adobe.com/manufacturing/resources/3dformats/ |title=3D supported formats |publisher=Adobe |date=2009-07-14 |accessdate=2010-02-21}}</ref><ref name="3d#2">{{cite web|url=https://www.adobe.com/devnet/acrobat3d/ |title=Acrobat 3D Developer Center |publisher=Adobe |date= |accessdate=2010-02-21}}</ref>\n\nTwo PDF files that look similar on a computer screen may be of very different sizes. For example, a high resolution raster image takes more space than a low resolution one. Typically higher resolution is needed for printing documents than for displaying them on screen. Other things that may increase the size of a file is embedding full fonts, especially for Asiatic scripts, and storing text as graphics.\n\n== Software ==\n{{Details|List of PDF software}}\nPDF viewers are generally provided free of charge, and many versions are available from a variety of sources.\n\nThere are many software options for creating PDFs, including the PDF printing capabilities built into [[macOS]] and most [[Linux]] distributions, [[LibreOffice]], [[Microsoft Office 2007]] (if updated to [[Office 2007#Service Pack 2|SP2]]) and later,<ref>{{cite web |url=http://support.microsoft.com/kb/953195|title=Description of 2007 Microsoft Office Suite Service Pack 2 (SP2) |publisher=[[Microsoft]] |accessdate=2009-05-09}}</ref> [[WordPerfect]] 9, [[Scribus]], numerous PDF print drivers for [[Microsoft Windows]], the [[pdfTeX]] typesetting system, the [[DocBook]] PDF tools, applications developed around [[Ghostscript]] and [[Adobe Acrobat]] itself as well as [[Adobe InDesign]], [[Adobe FrameMaker]], [[Adobe Illustrator]], [[Adobe Photoshop]]. [[Google]]\'s online office suite [[Google Docs]] also allows for uploading and saving to PDF.\n\n[[Raster image processor]]s (RIPs) are used to convert PDF files into a [[raster graphics|raster format]] suitable for imaging onto paper and other media in printers, digital production presses and [[prepress]] in a process known as [[rasterisation]]. RIPs capable of processing PDF directly include the Adobe PDF Print Engine<ref>{{cite web|url=https://www.adobe.com/products/pdfprintengine/overview.html|title=Adobe PDF Print Engine|work=adobe.com}}</ref> from [[Adobe Systems]] and Jaws<ref>{{cite web|url=http://www.globalgraphics.com/products/jaws_rip/|title=Jaws® 3.0 PDF and PostScript RIP SDK|work=globalgraphics.com}}</ref> and the [[Harlequin RIP]] from [[Global Graphics]].\n\n=== Editing ===\n{{Expand section|date=July 2010|reason=[[hybrid PDF]], a variant of [[LibreOffice]] isn\'t mentioned}}\nThere is specialized software for editing PDF files, though the choices are much more limited and often more expensive than creating and editing standard editable document formats. Version 0.46 and later of [[Inkscape]] allows PDF editing through an intermediate translation step involving [[Poppler (software)|Poppler]].\n\n[[Serif PagePlus]] can open, edit and save existing PDF documents, as well as publishing of documents created in the package.\n\n[[Enfocus]] PitStop Pro, a plugin for Acrobat, allows manual and automatic editing of PDF files,<ref>{{cite web|url=http://www.enfocus.com/product.php?id=855|title=Preflight and edit PDF files in Acrobat|work=enfocus.com}}</ref> while the free Enfocus Browser makes it possible to edit the low-level structure of a PDF.<ref>{{cite web|url=http://www.enfocus.com/product.php?id=4530|title=Enfocus product overview - online store|work=enfocus.com}}</ref>\n\n[[Dochub]], is a free online PDF editing tool that can be used without purchasing anything.<ref>{{Cite web|title = DocHub|url = http://www.dochub.com|website = DocHub|accessdate = 2015-12-12}}</ref>\n\n=== Annotation ===\n{{See also|Comparison of notetaking software}}\n[[Adobe Acrobat]] is one example of proprietary software that allows the user to annotate, highlight, and add notes to already created PDF files. One UNIX application available as [[free software]] (under the [[GNU General Public License]]) is [[PDFedit]]. Another GPL-licensed application native to the unix environment is Xournal. Xournal allows for annotating in different fonts and colours, as well as a rule for quickly underlining and highlighting lines of text or paragraphs. Xournal also has a shape recognition tool for squares, rectangles and circles. In Xournal annotations may be moved, copied and pasted. The [[freeware]] [[Foxit Reader]], available for [[Microsoft Windows]], [[macOS]] and [[Linux]], allows annotating documents. Tracker Software\'s [[PDF-XChange Viewer]] allows annotations and markups without restrictions in its freeware alternative. [[Apple Inc.|Apple]]\'s [[macOS]]\'s integrated PDF viewer, Preview, does also enable annotations as does the freeware [[Skim (software)|Skim]], with the latter supporting interaction with [[LaTeX]], SyncTeX, and PDFSync and integration with [[BibDesk]] reference management software. Freeware [[Qiqqa]] can create an annotation report that summarizes all the annotations and notes one has made across their library of PDFs.\n\nFor mobile annotation, [[iAnnotate PDF]] (from Branchfire) and [[GoodReader]] (from Aji) allow annotation of PDFs as well as exporting summaries of the annotations.\n\nThere are also [[web annotation]] systems that support annotation in pdf and other documents formats, e.g., [[A.nnotate]], [[crocodoc]], WebNotes.\n\nIn cases where PDFs are expected to have all of the functionality of paper documents, ink annotation is required. Some programs that accept ink input from the mouse may not be responsive enough for handwriting input on a tablet. Existing solutions on the PC include [[PDF Annotator]] and [[Qiqqa]].\n\n=== Other ===\nExamples of PDF software as online services including [[Scribd]] for viewing and storing, [[Pdfvue]] for online editing, and [[Zamzar]] for conversion.\n\nIn 1993 the Jaws [[raster image processor]] from [[Global Graphics]] became the first shipping prepress RIP that interpreted PDF natively without conversion to another format. The company released an upgrade to their Harlequin RIP with the same capability in 1997.<ref>{{cite web |url= http://www.globalgraphics.com/products/harlequin-multi-rip |title=Harlequin MultiRIP|accessdate=2014-03-02}}</ref>\n\n[[Agfa-Gevaert]] introduced and shipped Apogee, the first prepress workflow system based on PDF, in 1997.\n\nMany commercial offset printers have accepted the submission of press-ready PDF files as a print source, specifically the PDF/X-1a subset and variations of the same.<ref>[http://www.prepressx.com/ Press-Ready PDF Files] "For anyone interested in having their graphic project commercially printed directly from digital files or PDFs." (last checked on 2009-02-10).</ref> The submission of press-ready PDF files are a replacement for the problematic need for receiving collected native working files.\n\nPDF was selected as the "native" [[metafile]] format for [[macOS|Mac OS X]], replacing the [[PICT]] format of the earlier [[classic Mac OS]]. The imaging model of the [[Quartz (graphics layer)|Quartz]] graphics layer is based on the model common to [[Display PostScript]] and PDF, leading to the nickname \'\'Display PDF\'\'. The Preview application can display PDF files, as can version 2.0 and later of the [[Safari (web browser)|Safari]] web browser. System-level support for PDF allows Mac OS X applications to create PDF documents automatically, provided they support the OS-standard printing architecture. The files are then exported in PDF 1.3 format according to the file header. When taking a screenshot under Mac OS X versions 10.0 through 10.3, the image was also captured as a PDF; later versions save screen captures as a [[Portable Network Graphics|PNG]] file, though this behaviour can be set back to PDF if desired.\n\nIn 2006 PDF was widely accepted as the standard print job format at the [[Open Source Development Labs]] Printing Summit. It is supported as a print job format by the [[CUPS|Common Unix Printing System]] and desktop application projects such as [[GNOME]], [[KDE]], [[Firefox]], [[Mozilla Thunderbird|Thunderbird]], [[LibreOffice]] and [[OpenOffice.org|OpenOffice]] have switched to emit print jobs in PDF.<ref>{{cite web|title=PDF as Standard Print Job Format|url=http://www.linuxfoundation.org/collaborate/workgroups/openprinting/pdf_as_standard_print_job_format|website=The Linux Foundation|publisher=[[Linux Foundation]]|accessdate=21 June 2016}}</ref>\n\nSome desktop printers also support direct PDF printing, which can interpret PDF data without external help. Currently, all PDF capable printers also support PostScript, but most PostScript printers do not support direct PDF printing.\n\nThe [[Free Software Foundation]] once considered one of their [[High priority free software projects|high priority projects]] to be "developing a free, high-quality and fully functional set of libraries and programs that implement the PDF file format and associated technologies to the ISO 32000 standard."<ref>On 2014-04-02, a note dated 2009-02-10 referred to [http://www.fsf.org/campaigns/priority.html Current FSF High Priority Free Software Projects] as a source. Content of the latter page, however, changes over time.</ref><ref>{{cite web |url=http://gnupdf.org/Goals_and_Motivations |title=Goals and Motivations |authors=GNUpdf contributors| publisher=\'\'GNUpdf\'\' |date=2007-11-28 |website=gnupdf.org |accessdate=2014-04-02 }}</ref> In 2011, however, the [[GNU PDF]] project was removed from the list of "high priority projects" due to the maturation of the [[Poppler (software)|Poppler library]],<ref>{{cite web|title=GNU PDF project leaves FSF High Priority Projects list; mission complete! |url=http://www.fsf.org/blogs/community/gnu-pdf-project-leaves-high-priority-projects-list-mission-complete|date=2011-10-06|first=Matt|last=Lee|publisher=Free Software Foundation|website=fsf.org|accessdate=2014-04-02}}</ref> which has enjoyed wider use in applications such as [[Evince]] with the [[GNOME]] desktop environment. Poppler is based on [[Xpdf]]<ref>[http://poppler.freedesktop.org/ Poppler homepage] "Poppler is a PDF rendering library based on the xpdf-3.0 code base." (last checked on 2009-02-10)</ref><ref>[http://cgit.freedesktop.org/poppler/poppler/tree/README-XPDF Xpdf license] "Xpdf is licensed under the GNU General Public License (GPL), version 2 or 3." (last checked on 2012-09-23).</ref> code base. There are also commercial development libraries available as listed in [[List of PDF software]].\n\nThe [[Apache PDFBox]] project of the [[Apache Software Foundation]] is an open source Java library for working with PDF documents. PDFBox is licensed under the [[Apache License]].<ref>[http://pdfbox.apache.org/ The Apache PDFBox project] . Retrieved 2009-09-19.</ref>\n\n== See also ==\n{{Portal|Software}}{{columns-list|2|\n* [[Open XML Paper Specification]]\n* [[Comparison of OpenXPS and PDF]]\n* [[DjVu]]\n* [[PAdES]], <small>PDF Advanced Electronic Signature</small>\n* [[Web document]]\n* [[XSL Formatting Objects]]\n}}\n\n== References ==\n{{Reflist|30em}}\n\n== Further reading ==\n* {{Cite book | last1 = Hardy | first1 = M. R. B. | last2 = Brailsford | first2 = D. F. | chapter = Mapping and displaying structural transformations between XML and PDF | title = Proceedings of the 2002 ACM symposium on Document engineering  - DocEng \'02 | pages = 95–102| year = 2002 | url = http://www.cs.nott.ac.uk/~dfb/Publications/Download/2002/Hardy02.pdf| doi = 10.1145/585058.585077| publisher = Proceedings of the 2002 ACM symposium on Document engineering| isbn = 1-58113-594-7}}\n*Standards\n** PDF 1.7 [http://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/pdf_reference_1-7.pdf]\n** PDF 1.6 (ISBN 0-321-30474-8)\n** PDF 1.4 (ISBN 0-201-75839-3)\n** PDF 1.3 (ISBN 0-201-61588-6)\n\n== External links ==\n{{Commons category|PDF}}\n* [http://www.quora.com/PDF-file-format/How-was-the-PDF-format-created How was the PDF format created? Quora]\n* [http://www.pdfa.org/ PDF Association] - The PDF Association is the industry association for software developers producing or processing PDF files.\n* [http://partners.adobe.com/public/developer/tips/topic_tip31.html Adobe PDF 101: Summary of PDF]\n* [https://www.adobe.com/print/features/psvspdf/ Adobe: PostScript vs. PDF] – Official introductory comparison of PS, EPS vs. PDF.\n* {{webarchive |url=https://web.archive.org/web/20110424013530/http://www.aiim.org/Resources/Archive/Magazine/2007-Jul-Aug/33448 |date=April 24, 2011 |title=\'\'PDF Standards....transitioning the PDF specification from a de facto standard to a de jure standard\'\' }} – Information about PDF/E and PDF/UA specification for accessible documents file format (archived by [[Wayback Machine|The Wayback Machine]])\n* [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=38920 ISO 19005-1:2005] the PDF/A-1 ISO standard published by the [[International Organization for Standardization]] (chargeable)\n* [https://www.adobe.com/devnet/pdf/pdf_reference.html PDF Reference and Adobe Extensions to the PDF Specification]\n* [http://www.mactech.com/articles/mactech/Vol.15/15.09/PDFIntro/ Portable Document Format: An Introduction for Programmers] – Introduction to PDF vs. PostScript and PDF internals (up to v1.3)\n* [http://www.planetpdf.com/enterprise/article.asp?ContentID=6519 The Camelot Paper] – the paper in which John Warnock outlined the project that created PDF\n* [http://river-valley.zeeba.tv/everything-you-wanted-to-know-about-pdf-but-were-afraid-to-ask/ Everything you wanted to know about PDF but were afraid to ask] - recording of talk by Leonard Rosenthol (Adobe Systems) at TUG 2007\n* [http://www.data2type.de/en/xml-xslt-xslfo/xsl-fo/ How to produce PDF with XSL-FO]\n* [http://pdfextractoronline.com/ PDF To Excel Converter]\n{{Graphics file formats}}\n{{Office document file formats}}\n{{ISO standards}}\n{{Ebooks}} <!--navbox-->\n\n[[Category:1993 introductions]]\n[[Category:Adobe Systems]]\n[[Category:Digital press]]\n[[Category:Electronic documents]]\n[[Category:Graphics file formats]]\n[[Category:ISO standards]]\n[[Category:Office document file formats]]\n[[Category:Open formats]]\n[[Category:Page description languages]]\n[[Category:Vector graphics]]']
['Scopus', '582311', '{{Use dmy dates|date=August 2013}}\n{{other uses}}\n{{infobox bibliographic database\n| title = Scopus\n| image = [[File:Scopus_type_logo.jpg]]\n| caption = \n| producer = [[Elsevier]]\n| country = \n| history = \n| languages = English\n| providers = \n| cost = Subscription\n| disciplines= \n| depth = \n| formats = \n| temporal = 1995-present\n| geospatial = Worldwide\n| number = 55 million\n| updates = \n| p_title = \n| p_dates = \n| ISSN = \n| web = http://www.scopus.com\n| titles = \n}}\n\'\'\'Scopus\'\'\' is a [[bibliographic database]] containing [[Abstract (summary)|abstracts]] and [[citation]]s for [[academic journal]] [[Article (publishing)|articles]]. It covers nearly 22,000 titles from over 5,000 publishers, of which 20,000 are [[peer review|peer-reviewed]] journals in the scientific, technical, medical, and social sciences (including arts and humanities).<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview |title=Scopus Content Overview |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> It is owned by [[Elsevier]] and is available online by [[subscription business model|subscription]]. Searches in Scopus also incorporate searches of patent databases.<ref>{{cite journal |doi=10.1001/jama.2009.1307 |title=Comparisons of Citations in Web of Science, Scopus, and Google Scholar for Articles Published in General Medical Journals |year=2009 |last1=Kulkarni |first1=A. V. |last2=Aziz |first2=B. |last3=Shams |first3=I. |last4=Busse |first4=J. W. |journal=[[JAMA (journal)|JAMA]] |volume=302 |issue=10 |pages=1092–6 |pmid=19738094}}</ref>\n\n==Overview==\n\nSince Elsevier is the owner of Scopus and is also one of the main international publishers of scientific journals, an independent and international Scopus Content Selection and Advisory Board was established to prevent a potential conflict of interest in the choice of journals to be included in the database and to maintain an open and transparent content coverage policy, regardless of publisher.<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview#content-policy-and-selection |title=Scopus Content Overview: Content Policy and Selection |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> The board consists of scientists and subject librarians.\n\nEvaluating ease of use and coverage of Scopus and the [[Web of Science]] (WOS), a 2006 study concluded that "Scopus is easy to navigate, even for the novice user. ... The ability to search both forward and backward from a particular citation would be very helpful to the researcher. The multidisciplinary aspect allows the researcher to easily search outside of his discipline" and "One advantage of WOS over Scopus is the depth of coverage, with the full WOS database going back to 1945 and Scopus going back to 1966. However, Scopus and WOS complement each other as neither resource is all inclusive."<ref>{{Cite journal |pmid=16522216 |year=2006 |last1=Burnham |first1=JF |title=Scopus database: A review |volume=3 |pages=1 |doi=10.1186/1742-5581-3-1 |pmc=1420322 |journal=Biomedical Digital Libraries}}</ref>\n\nScopus also offers author profiles which cover affiliations, number of publications and their [[bibliographic]] data, [[references]], and details on the number of citations each published document has received. It has [[alerts|alerting]] features that allows registered users to track changes to a profile and a facility to calculate authors\' [[h-index]].\n\nScopus IDs for individual authors can be integrated with the nonproprietary digital identifier [[ORCID]].<ref name="Scopus">{{cite web |url=http://orcid.scopusfeedback.com |title=Scopus2Orcid |publisher=Scopus |accessdate=7 May 2014}}</ref>\n\n==See also==\n*[[Source Normalized Impact per Paper]]\n*[[Web of Science]]\n\n== References ==\n{{reflist|30em}}\n\n== External links ==\n{{Wikidata property|P1153}}\n{{Wikidata property|P1154}}\n{{Wikidata property|P1155}}\n{{Wikidata property|P1156}}\n* {{Official website|http://www.scopus.com/}}\n\n{{Reed Elsevier}}\n\n[[Category:Bibliographic databases and indexes]]\n[[Category:Elsevier]]\n[[Category:Citation indices]]\n[[Category:Library cataloging and classification]]']
['Book Citation Index', '46862330', 'The \'\'\'Book Citation Index\'\'\' (\'\'\'BCI\'\'\', \'\'\'BKCI\'\'\') is an online subscription-based scientific [[citation index]]ing service maintained by [[Thomson Reuters]] and is part of the [[Web of Science|Web of Science Core Collection]].<ref>{{cite book|last1=Campbell|first1=Robert|last2=Pentz|first2=Ed|last3=Borthwick|first3=Ian|title=Academic and Professional Publishing|date=2012|publisher=Chandos Publishing|isbn=9781780633091|pages=247–248|url=https://books.google.com/books?id=IpRlAgAAQBAJ&pg=PA247&dq=%22Book+Citation+Index%22&hl=en&sa=X&ei=Ig9sVZ2QB4rLsASwyYHYBw&ved=0CGoQ6AEwCg#v=onepage&q=%22Book%20Citation%20Index%22&f=false|accessdate=1 June 2015}}</ref> It was first launched in 2011 and indexes over 60,000 editorially selected books, starting from 2005.<ref name=ATL>{{cite journal|title=Thomson reuters launches Book Citation Index|journal=Advanced Technology Libraries|date=11/01/2011|volume=40|issue=11|page=3|url=http://web.a.ebscohost.com.ezproxy2.library.drexel.edu/ehost/pdfviewer/pdfviewer?sid=3118f8aa-bb72-4992-b82e-be196198670d%40sessionmgr4002&vid=1&hid=4212|accessdate=1 June 2015}}</ref> Books in the index are electronic and print scholarly texts that contain articles based on [[original research]] and/or reviews of such literature.<ref name=ATL />\n\n==Content==\nThe index covers series and non-series books as long as they include full footnotes and the index has two separate editions, a Science edition and a Social Sciences & Humanities edition. The Science edition covers physics and chemistry, engineering, computing and technology, clinical medicine, life sciences, and agriculture and biology. Currently both series only contain books that date back to 2005.<ref>{{cite book|last1=Mann|first1=Thomas|title=The Oxford Guide to Library Research|date=2015|publisher=Oxford University Press|isbn=9780199394463|url=https://books.google.com/books?id=llVLBgAAQBAJ&pg=PT193&dq=%22Book+Citation+Index%22&hl=en&sa=X&ei=Ig9sVZ2QB4rLsASwyYHYBw&ved=0CF4Q6AEwCA#v=onepage&q=%22Book%20Citation%20Index%22&f=false|accessdate=1 June 2015}}</ref>\n\n==Reception==\nIn their 2014 book \'\'Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact\'\', [[Blaise Cronin]] and Cassidy R. Sugimoto noted that "for impact assessment of book-based fields, bibliometricians need a database with large numbers of books" and that while the Book Citation Index did meet this need, [[Google Books]] also fulfilled this purpose and was not only free, but was (at the time) more comprehensive for bibliometric analyses.<ref>{{cite book|last1=Cronin|first1=Blaise|last2=Sugimoto|first2=Cassidy R.|title=Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact|date=2014|publisher=MIT Press|isbn=9780262323291|pages=33, 289, 296|url=https://books.google.com/books?id=xxSaAwAAQBAJ&pg=PA296&dq=%22Book+Citation+Index%22&hl=en&sa=X&ei=Ig9sVZ2QB4rLsASwyYHYBw&ved=0CE8Q6AEwBQ#v=onepage&q=%22Book%20Citation%20Index%22&f=false|accessdate=1 June 2015}}</ref> A 2013 article in the \'\'[[Journal of the American Society for Information Science and Technology]]\'\' remarked on the index\'s opportunities and limitations. It stated that the "most significant limitations to this potential application are the high share of publications without address information, the inflation of publication counts, the lack of cumulative citation counts from different hierarchical levels, and inconsistency in citation counts between the cited reference search and the book citation index."<ref name=journal>{{cite journal|last1=Gorraiz|first1=Juan|last2=Purnell|first2=Philip J.|last3=Glänze|first3=Wolfgang|title=Opportunities for and limitations of the Book Citation Index|journal=Journal of the American Society for Information Science and Technology|date=July 2013|volume=64|issue=7|pages=1388–1398|doi=10.1002/asi.22875|url=http://onlinelibrary.wiley.com/doi/10.1002/asi.22875/full|accessdate=1 June 2015}}</ref> They also stated that the Book Citation Index was "a first step toward creating a reliable and necessary citation data source for monographs — a very challenging issue, because, unlike journals and conference proceedings, books have specific requirements, and several problems emerge not only in the context of subject classification, but also in their role as cited publications and in citing publications."<ref name=journal />\n\n==Further reading==\n*{{cite journal|last1=Torres-Salinas|first1=Daniel|last2=Robinson-Garcia|first2=Nicolas|last3=Miguel Campanario|first3=Juan|last4=Emilio|first4=Delgado López-Cózar|title=Coverage, field specialisation and the impact of scientific publishers indexed in the Book Citation Index|journal=Online Information Review|date=January 2014|volume=38|issue=1|pages=24–42|doi=10.1108/OIR-10-2012-0169|url=http://www.emeraldinsight.com.ezproxy2.library.drexel.edu/doi/full/10.1108/OIR-10-2012-0169|accessdate=1 June 2015}}\n*{{cite journal|last1=Torres-Salinas|first1=Daniel|last2=Rodriguez-Sánchez|first2=Rosa|last3=Robinson-Garcia|first3=Nicolas|last4=Fdez-Valdivia|first4=J|last5=García|first5=J.A.|title=Mapping Citation Patterns of Book Chapters in the Book Citation Index|journal=Journal of Informetrics|date=February 2013|volume=7|issue=2|pages=412–424|doi=10.1016/j.joi.2013.01.004|arxiv=1302.5544}}\n\n==References==\n{{reflist}}\n\n==External links==\n*{{official website|http://wokinfo.com/products_tools/multidisciplinary/bookcitationindex/}}\n\n[[Category:Bibliographic databases and indexes]]\n[[Category:Full text scholarly online databases]]\n[[Category:Thomson family]]\n[[Category:Thomson Reuters]]\n[[Category:Citation indices]]\n[[Category:Books]]']
['ScienceOpen', '48598824', '{{Multiple issues|\n{{notability|Organizations|date=February 2016}}\n{{refimprove|date=February 2016}}\n}}\n\n{{Infobox organization\n| name                = ScienceOpen\n| native_name         = \n| native_name_lang    = \n| named_after         = \n| image               =\n| image_size          = 300px\n| alt                 =  ScienceOpen\'s logo\n| caption             = \n| map                 = \n| map_size            = \n| map_alt             = \n| map_caption         = \n| map2                = \n| map2_size           = \n| map2_alt            = \n| map2_caption        = \n| abbreviation        = \n| motto               = \n| predecessor         = \n| merged              = \n| successor           = \n| formation           = <!-- use {{start date|YYYY|MM|DD|df=y}} -->\n| founder             = \n| founding_location   = \n| extinction          = <!-- use {{end date and age|YYYY|MM|DD}} -->\n| merger              = \n| type                = \n| tax_id              = <!-- or | vat_id = (for European organizations) -->\n| registration_id     = <!-- for non-profit org -->\n| status              = \n| purpose             = \n| headquarters        = [[Berlin]], [[Germany]]\n| location            = \n| coords              = <!-- {{coord|LAT|LON|display=inline,title}} -->\n| region              = \n| services            = \n| products            = \n| methods             = \n| fields              = \n| membership          = \n| membership_year     = \n| language            = \n| owner               = <!-- or | owners = -->\n| sec_gen             = \n| leader_title        = \n| leader_name         = \n| leader_title2       = \n| leader_name2        = \n| leader_title3       = \n| leader_name3        = \n| leader_title4       = \n| leader_name4        = \n| board_of_directors  = \n| key_people          = \n| main_organ          = \n| parent_organization = \n| subsidiaries        = \n| secessions          = \n| affiliations        = \n| budget              = \n| budget_year         = \n| revenue             = \n| revenue_year        = \n| disbursements       = \n| expenses            = \n| expenses_year       = \n| endowment           = \n| staff               = \n| staff_year          = \n| volunteers          = \n| volunteers_year     = \n| slogan              = \n| mission             = \n| website             = {{URL|scienceopen.com}}\n| remarks             = \n| formerly            = \n| footnotes           = \n}}\n\'\'\'ScienceOpen\'\'\' is a privately owned discovery and research network with three roles:  aggregation, [[Open access|open access publishing]] and the evaluation of scholarly literature in all scholarly disciplines.\n\n== History ==\nScienceOpen began in 2013<ref>{{Cite web|title = OA interviews: Alexander Grossmann, ScienceOpen - Open-access publishing - Research Information|url = http://www.researchinformation.info/features/feature.php?feature_id=473|website = www.researchinformation.info|accessdate = 2015-11-19}}</ref> when Alexander Grossmann, a professor of Publishing Management at the [[Leipzig University of Applied Sciences]] and former publishing director at scientific house [http://www.degruyter.com/dg/page/15/the-publishing-house De Gruyter], and Tibor Tscheke, president and CEO of content management systems company [http://ovitas.com/ Ovitas], decided to start a platform. Their idea was to allow researchers to share scientific information, both formally by publishing articles and participating in [https://futureofscipub.wordpress.com/open-post-publication-peer-review/ post-publication peer review], and informally by reviewing their colleagues’ work, providing endorsements and comments, and by updating their own papers.\n\nIts beta version was introduced in November 2013, and release 1.0 launched in May 2014.<ref>{{Cite web|title = ScienceOpen: the next wave of Open Access? - EuroScientist Webzine|url = http://www.euroscientist.com/scienceopen-next-wave-open-access|website = EuroScientist Webzine|accessdate = 2015-11-19|language = en-US}}</ref> As of September 2015 the site has 10 million articles and records<ref>{{Cite web|title = Open and Shut?: The OA Interviews: ScienceOpen’s Alexander Grossmann|url = http://poynder.blogspot.com/2015/11/the-oa-interviews-scienceopens.html|website = Open and Shut?|date = 2015-11-16|accessdate = 2015-11-19|first = Richard|last = Poynder}}</ref> from [[PubMed Central]], [[ArXiv]], [[PubMed]] and ScienceOpen, and a publicly available citation index which is free for researchers to use wherever they are and is provided at no cost to libraries, which in February 2016 was dubbed the Open Citation Index.<ref>http://blog.scienceopen.com/2016/02/the-open-citation-index/</ref> All content on the platform is available for post-publication peer review by scientific members with five or more peer-reviewed publications on their [[ORCID]], and all articles can be publicly commented on by members with one or more items.\n\nScienceOpen appoints members of the research community as Collection Editors<ref>{{Cite web|title = ScienceOpen Collections|url = http://about.scienceopen.com/scienceopen-collections/#more-390|website = About ScienceOpen|accessdate = 2015-11-19|language = en-US}}</ref> who curate articles from multiple publishers in any topic. [[Thieme Medical Publishers|Thieme]], a German medical publisher, mirrors three open access journals [https://www.scienceopen.com/collection/Thieme on the platform].\n\nThe organization is based in Berlin and has a technical office in Boston. It is a member of [[CrossRef]], [[ORCID]], the [[Open Access Scholarly Publishers Association]]<ref>{{Cite web|url=http://oaspa.org/member/scienceopen/|title=Member Record: ScienceOpen|last=|first=|date=|website=|publisher=|access-date=}}</ref> and the [[Directory of Open Access Journals]]. The company was designated as one of  “10 to Watch” by research advisory firm Outsell in its report “[http://img.en25.com/Web/CopyrightClearanceCenterInc/%7Bfc9f07ac-b2c9-4cd7-b763-2f21e0c6e94b%7D_Outsell_2015_Open_Access_Report.pdf Open Access 2015: Market Size, Share, Forecast, and Trends].”\n\nIn 2015, Tscheke provided further clarification of ScienceOpen’s focus on aggregation and filtering content.<ref>{{Cite web|title = There’s more to Open Access than APCs, right? – ScienceOpen Blog|url = http://blog.scienceopen.com/2015/10/theres-more-to-open-access-than-apcs-right/|website = blog.scienceopen.com|accessdate = 2015-11-19}}</ref>\n\n== Business model ==\nScienceOpen publishes articles of almost any type and from any research field, including the social sciences and humanities. This includes primary research articles, opinion papers, posters, case studies, negative results, and data publications. To fund article publication, ScienceOpen charges a publication fee ($800 as of this time of writing, in 2015) to be paid by the author or the author’s employer, funder or library. This is for a post-publication peer review process and publication after editorial control,<ref>{{Cite web|url=http://blog.scienceopen.com/2016/06/review-instructions-for-scienceopen/|title=Review Instructions for ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> and is facilitated through integration with [[ORCID]].<ref>{{Cite web|url=http://blog.scienceopen.com/2016/06/orcid-integration-at-scienceopen/|title=ORCID integration at ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> Additionally, authors can opt to use the \'Peer Review by Endorsement\' model,<ref>{{cite journal |last= Jan Velterop |date= 29 September 2015 |title= Peer review – issues, limitations, and future development |url= https://www.scienceopen.com/document?15&vid=1dcfbe69-c30c-4eaa-a003-948c9700da40 |journal= ScienceOpen Research |doi= 10.14293/S2199-1006.1.SOR-EDU.AYXIPS.v1 |access-date=14 June 2016}}</ref> in which peer review is conducted prior to submission, and for a fee of $400. Included in this fee are up to two article revisions within 12 months, with full version control. Revised versions have a new DOI so that it is easier to link back to cited versions. A partial or full fee waiver is available to those who demonstrate need. [[Poster session]] publishing is free. All published articles are published via a [[Creative Commons]] Attribution 4.0 International Public License.<ref>{{Cite web|url=http://about.scienceopen.com/open-access-explanation-of-cc-by-license/|title=Open Access License Agreement|last=|first=|date=|website=|publisher=|access-date=}}</ref>\n\nFrom 2016, ScienceOpen has started partnering with publishers to offer advanced indexing services. In June 2016 they partnered with [[SciELO]], the largest publisher in Latin America{{Cite web|title = ScienceOpen helps to put scientific research in a global context with more than 15 million article records – ScienceOpen Blog|url = http://blog.scienceopen.com/2016/06/scienceopen-helps-to-put-scientific-research-in-a-global-context-with-more-than-15-million-article-records/|website = blog.scienceopen.com|accessdate = 2016-06-14}}. Additional publishing partners include Higher Education Press<ref>{{Cite web|url=http://blog.scienceopen.com/2016/04/higher-education-press-indexing-partnership-with-scienceopen/|title=Higher Education Press indexing partnership with ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> and The Italian Society of Victimology.<ref>{{Cite web|url=http://blog.scienceopen.com/2016/06/welcome-to-the-italian-society-of-victimology/|title=Welcome to the Italian Society of Victimology|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> In September 2015, ScienceOpen hit the 10 million article record mark,<ref>{{Cite web|url=http://www.prnewswire.com/news-releases/scienceopen-hits-the-10-million-article-mark-527671151.html|title=ScienceOpen Hits the 10 Million Article Mark|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> and as of 27 June 2016 has more than 13 million records.\n\nEvery research article on ScienceOpen has a traceable genealogy through citations, a public peer review process, and social interaction tracked by [[altmetrics]], which they call research "context".<ref>{{Cite web|url=http://blog.scienceopen.com/2016/05/why-context-is-important-for-research/|title=Why ‘context’ is important for research|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> The technology behind the ScienceOpen platform is provided by Ovitas.\n\nScienceOpen are findable on [https://www.youtube.com/user/ScienceOpen YouTube], [https://www.linkedin.com/company/scienceopen-inc- LinkedIn], [https://www.facebook.com/scienceopen/ Facebook], and [https://twitter.com/science_open Twitter].\n\n== Publications ==\n* [https://www.scienceopen.com/collection/scienceopen_research ScienceOpen Research], ISSN [https://www.worldcat.org/search?q=n2%3A2199-1006&qt=results_page 2199-1006]\n* [https://www.scienceopen.com/collection/scienceopen_posters ScienceOpen Posters], ISSN [https://www.worldcat.org/search?q=n2%3A2199-8442&qt=results_page 2199-8442]\n\n== Headquarters ==\nScienceOpen has its headquarters located at Pappelallee 78-79, 10437 Berlin and its technical hub is at 60 Mall Rd., Burlington, Mass.\n\n== See also ==\n* [[Open Access Scholarly Publishers Association]] (OASPA)\n* [[Directory of Open Access Journals]] (DOAJ)\n* [[Registry of Open Access Repositories Mandatory Archiving Policies]] (ROARMAP)\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{official website|http://www.scienceopen.com}}\n\n[[Category:Open access publishers]]\n[[Category:Open access (publishing)]]\n[[Category:Scholarly communication]]\n[[Category:Citation indices]]']
['Category:Telephone directory publishing companies', '5154927', '[[Category:Publishing companies by medium]]\n[[Category:Directories]]\n[[Category:Directory assistance services]]\n[[Category:Telephone numbers]]']
['High Weirdness by Mail', '3350777', '{{italic title}}\n\'\'\'\'\'High Weirdness By Mail – A Directory of the Fringe: Crackpots, Kooks & True Visionaries\'\'\'\'\', by [[Ivan Stang]] (ISBN 0-671-64260-X) is a 1988 book dedicated to an examination of "weird culture" by actually putting the reader in touch with it by mail.\n\nThe book is divided into sections&mdash;"Weird Science," "UFO Contactees," "Drug Stuff," and others, and each section contains a variety of mini-articles describing organizations. Each organization article concludes with a mailing address (and in some cases, phone numbers).\n\nSeveral years after the book\'s publication, Stang reported on the [[newsgroup]] [[alt.slack]] that his inclusion of entries for [[white supremacist]] groups in the book caused his name to be mentioned by those groups as a possible target for retaliation.  (The book\'s commentaries on various [[hate group]]s were less than flattering.)  Stang reported this incident to the [[FBI]], but did not receive any actual harassment or threats from the groups in question.\n\nThe [[Association for Consciousness Exploration]] produced a follow-up lecture by Rev. Stang on cassette entitled \'\'High Weirdness by Mail\'\', recorded live at the 1993 WinterStar Symposium.\n\n== Controversy ==\n\n[[Bob Black]] claims that his review of \'\'High Weirdness By Mail\'\' was the cause of his being sent a small \'prank\' mail bomb. [http://www.inspiracy.com/black/bomb.html]\n\n== External links ==\n* [http://www.subgenius.com Home Page of the Church of the SubGenius]\n* [http://subgenius.com/hwbw.htm The Return of \'\'High Weirdness by Mail\'\']\n\n[[Category:1988 books]]\n[[Category:Church of the SubGenius]]\n[[Category:Directories]]\n\n{{Nonfiction-book-stub}}']
['Category:Personal information managers', '1444957', '{{Cat main|personal information manager}}\n\n[[Category:Directories]]\n[[Category:Planning]]\n[[Category:Personal life|Information managers]]\n[[Category:Application software]]']
['American Art Directory', '21099152', '{{Infobox Magazine\n| title           = American Art Directory\n| image_file      = American Art Directory logo, 1898.svg\n| image_size      = 140px\n| image_caption   = \'\'Frontispiece from 1898 volume\'\'\n| editor          = \n| editor_title    = \n| previous_editor = \n| staff_writer    = \n| frequency       = [[Annual publication|Annual]]\n| circulation     = \n| category        = \n| company         = \n| publisher       = \n| firstdate       = 1898\n| country         = \n| based           = \n| language        = [[English language|English]]\n| website         = http://www.americanartdir.com/\n| issn            = 0065-6968\n}}\n\nThe \'\'\'\'\'American Art Directory\'\'\'\'\' is a yearly publication covering [[art museum]]s, [[Arts centre|arts centers]], and [[Art school|art educational institutions]] as well as news, obituaries, book and magazine publications, etc. related to the artistic community in the [[United States]].  Established in 1898, it was originally entitled \'\'American Art Annual\'\'.\n\nArt consultant, advisor, author, and independent appraiser [[Alan Bamberger]] describes the Directory as "...a required reference for art museums, libraries, arts organizations, art schools, and corporations with art holdings."<ref>{{citation\n |first1=Alan S. \n |last1=Bamberger \n |title=Who\'s Who In American Art, Official Museum Directory, American Art Directory \n |publisher=ArtBusiness.com \n |accessdate=2009-01-14 \n |url=http://www.artbusiness.com/revs0608.html \n |deadurl=bot: unknown \n |archiveurl=http://www.webcitation.org/5dphZr5S6?url=http%3A%2F%2Fwww.artbusiness.com%2Frevs0608.html \n |archivedate=2009-01-15 \n |df= \n}}, archived by [[WebCite]] </ref>\n\nA yearly feature is the "Review of the Year" article discussing the touring exhibitions, commissions, grants to organizations, construction starts at museums and other facilities, and various other events that occur within the art community.\n\nInitially the directory was the work of the [[New York City|New York]] area artist [[Florence Nightingale Levy]] and published by [[Macmillan Publishers|The Macmillan Company]].<ref name="NYTObit">{{Citation\n  | title = NOTES AND NEWS.; Items Gathered During This Week\'s Tour of the Publishing Houses.\n  | newspaper = [[The New York Times]]\n  | date = April 1, 1899\n  | url = http://query.nytimes.com/gst/abstract.html?res=9407E1DF1538E433A25752C0A9629C94689ED7CF}}</ref>  The [[American Federation of Arts]], with which Mrs. Levy was associated and which she would later become the president of, was founded in 1909<ref>{{citation\n  | title = About the AFA\n  | publisher = [[American Federation of Arts]]\n  | accessdate = 2009-01-14\n  | url = http://www.afaweb.org/about/}}</ref> and in 1913 the directory became an official publication of that organization.<ref name="Torchbearers">{{citation\n  | first1 = Karen J.\n  | last1 = Blair\n  | authorlink1 = Karen J. Blair\n  | title = The Torchbearers: Women and Their Amateur Arts Associations in America, 1890-1930\n  | publisher = [[Indiana University Press]]\n  | location = [[Bloomington, Indiana|Bloomington]]\n  | year = 1994\n  | isbn = 978-0-253-31192-4\n  | page = 80\n  | oclc = 27677514\n  | url = https://books.google.com/books?id=wP5pq2aBYBAC&printsec=frontcover#PPA80,M1}}</ref>  It later became the independent publication it exists as currently.\n\nIn 1952 the \'\'American Art Annual\'\' was split into two separate publications, \'\'[[Who\'s Who in American Art]]\'\' and the \'\'American Art Directory\'\'.<ref>{{citation\n  | title = Who’s Who in American Art\n  | publisher = [[R. R. Bowker]]\n  | location = [[New York City|New York]]\n  | year = 1953\n  | issn = 0000-0191}}</ref>\n\n==References==\n{{Reflist}}\n<div style="text-align:center;height:3em;">&#160;</div>\n\n==External links==\n* [http://www.americanartdir.com/ Official website]\n\n[[Category:Books about visual art]]\n[[Category:Annual magazines]]\n[[Category:Magazines established in 1898]]\n[[Category:Directories]]\n[[Category:Arts in the United States]]\n[[Category:American arts magazines]]\n\n\n{{art-mag-stub}}\n{{US-arts-org-stub}}\n{{art-book-stub}}']
['EADP', '23508467', "{{refimprove|date=August 2014}}\n\nThe '''European Association of Directory and Database Publishers''', known as '''EADP''', was founded in 1966. EADP is the key representative for the [[Europe|European]] directory and database publishing sector. As such, EADP has 180 members from 36 countries and represents the interests of some 340 [[Telephone directory|directory]] publishers. The associations members and affiliate members include [[Publishing|publishers]] and stakeholders from the industry such as suppliers and vendors.<ref>{{cite web|title=About EADP|url=http://www.eadp.org/index.php?q=aboutus|accessdate=25 July 2013}}</ref>\n\nEADP's activities include:\n\n* Maintaining an up-to-date member directory\n* Facilitating an annual congress and a separate annual conference\n* Monitoring EU legal activities of relevance to the industry\n* Compiling an annual statistical report and benchmarking studies\n\nThe [[North America|North American]] counterpart to the EADP is the [[Yellow Pages Association]] (YPA).\n\n== References ==\n<references />\n\n==External links==\n*[http://www.ypassociation.org/ YPA web-site]\n*[http://www.eadp.org/ EADP web-site]\n\n{{DEFAULTSORT:Eadp}}\n[[Category:Companies established in 1966]]\n[[Category:Directories]]\n\n\n{{telephony-stub}}"]
['Pamyatnaya Knizhka', '26512894', "{{unreferenced|date=April 2010}}\n'''Pamyatnaya knizhka''' Memorial Book Памятная книжка is the title of official reference books of regions and towns in [[Russian empire]].\n\n== History ==\nThe books were annually published by local authorities in 89 [[gubernia]]s and regions of [[Russian Empire]] starting from the mid-1830s till 1917. They provide information on population and businesses in the course of over 60 years. Over two thousand books have been found.\n\n== Composition of Pamyatnaya knizhka ==\nThe books had some peculiarities in some [[gubernia]]s and not always comprised 4 main sections:\n* [[address-calendar]] (index of all local official institutions and their staff),\n* administrative reference book (information on administrative units in a [[gubernia]], post offices, roads, industrial and commercial enterprises, hospitals and chemists’, educational institutions, museums and libraries, book stores and print shops, periodicals, list of towns, major landowners etc.),\n* statistic data (statistic tables on population, farming, education, incomes, fires etc.);\n* historical background.\n\n== Research project ==\nThe [[Russian National Library]] is carrying out a research project devoted to Pamyatnaya knizhka. The project is supervised by Mrs. Nadezhda Balatskaya Надежда Михайловна Балацкая in the department Bibliografia and krayevedeniye of the [[Russian National Library]].\nThe project comprises official memorial books Pamyatnay knizhkas of all gubernias and regions of [[Russia]], including areas that are no longer within the [[Russian Federation]].\nThe main result of the project will be the publication of 15 volume bibliographic index book titled “Памятные книжки губерний и областей Российской империи: Указатель содержания”. Some of materials in work are available at [http://givc.ru national computer center]. Рабочие материалы проекта представляют огромный интерес для людей, занимающихся генеалогией. That is an important source for genealogic researchers.\n\n==External links==\n* [http://www.nlr.ru/pro/inv/mem_buks.htm the site of the project of the Russia’s National Library «Памятные книжки губерний и областей Российской империи»]\n* [http://chigirin.narod.ru/book.html#spravochniki some memorial books of Russian Empire in pdf at Russian national computer center]\n* [http://russian-family.ru/index.php?option=com_jdownloads&task=viewcategory&catid=23&Itemid=27 Memorial books of some gubernias]\n{{use dmy dates|date=December 2010}}\n\n[[Category:Russian Empire]]\n[[Category:Directories]]\n[[Category:Genealogy publications]]"]
['City directory', '30018447', "{{Multiple issues|\n{{Refimprove|date=July 2015}}\n{{Citation style|date=July 2015}}\n}}\n\nA '''city directory''' is a listing of residents, streets,  businesses, organizations or institutions, giving their location in a [[city]].  Antedating [[telephone directories]], they have been in use for centuries.\n\nExamples include [[Kelly's Directory]] and the [[Boston Directory]].\n\n==See also==\n* [[:de:Adressbuch]]\n\n==References==\n{{refbegin}}\n*{{cite journal |title=How Reliable is the Modern City Directory? | volume= 30| issue =2| pages =154–158|date=June 1986 |journal=Canadian Geographer |author=Richard Harris, Ben Moffat |doi=10.1111/j.1541-0064.1986.tb01040.x}}\n*{{cite web |url=http://www.ancestry.com/learn/library/article.aspx?article=4062 |title= City vs. Telephone Directories |work=[[Ancestry.com]] |author=George G. Morgan}}\n*{{cite book \n|author=A. V. Williams \n|title=The Development and Growth of City Directories \n|publisher=\n|location=Cincinnati\n|year=1913 \n|url=http://catalog.hathitrust.org/Record/008698693\n}}\n*{{cite book |author=Florence May Hopkins |title=Reference Guides that Should be Known and how to Use Them: Atlases; City Directories; Gazetteers  |publisher=The Willard Company |location= |year=1919  |pages= |isbn= |url=https://books.google.com/books?id=SPEVAAAAIAAJ |doi= |accessdate=}}\n{{refend}}\n\n==Further reading==\n* {{citation |title=Direct Me NYC 1786: A History of City Directories in the United States and New York City |author= Philip Sutton |year=2012 |publisher=New York Public Library |url= http://www.nypl.org/blog/2012/06/08/direct-me-1786-history-city-directories-US-NYC |work=NYPL Blogs }}\n\n[[Category:Directories]]"]
['Adelskalender (directory)', '24177992', '\'\'\'Adelskalender\'\'\' ({{lang-de|Directory of Nobility}}) is a comprehensive directory of the nobility of a country or area. The best known such directory is the German [[Almanach de Gotha]] ("The Gotha") and its successor, the [[Genealogisches Handbuch des Adels]].\n\n[[Category:Biographical dictionaries]]\n[[Category:European nobility]]\n[[Category:Genealogy publications]]\n[[Category:Directories]]\n\n\n{{royal-bio-book-stub}}\n{{bio-dict-stub}}\n\n[[cs:Adelskalender]]\n[[de:Adelskalender]]\n[[nl:Adelskalender]]\n[[no:Adelskalender]]\n[[fi:Aateliskalenteri]]\n[[sv:Adelskalender]]']
['Royal Almanac', '31485471', '{{unreferenced|date=May 2011}}\n\nThe \'\'\'Royal Almanac\'\'\' is a French administrative directory founded in 1683 by the [[bookseller]] Laurent d\'Houry, which appeared under this title from 1700 to 1792, and under other titles until 1919.\n\nHe presented each year in the official [[order of precedence]], the list of members of the [[royal family]] of [[France]], the princes of blood, and the main body of the kingdom, great crown officers, senior clerics, abbots of large abbeys (with income of each abbey), marshals of France, colonels and general officers, ambassadors and consuls of France, presidents of the main courts, state councilors, bankers, etc..\n\nDespite the fact that he could present indigestible because of the many lists that he was composed, he enjoyed a wide circulation with a readership consisting primarily of financial, political and all persons who had an interest in knowing the administrative organization of France.\n\nAlthough his edition is due to the initiative of a private publisher, included in the lists of the Almanac was a royal official and abuse were therefore punished. Thus, a [[Poitou|Poitevin]], Pierre Joly, was interned in the [[Bastille]] at the end of the eighteenth century to have usurped the banking profession by being registered as such in the Almanach Royal.\n\nHis edition was in regular format in-8 o editor with a binder leather adorned with a sprinkling of [[fleur de lys]] gold.\n\n==History==\n===Founded at the request of King===\nLaurent d\'Houry imagines a calendar or [[Almanac]] 1683. The first edition contained only a few pages with a calendar and omens for the coming year. The last edition in this form, in 1699, already shows some lists that foreshadow the upcoming Royal Almanac. Thus we find lists of counselors of state with their ordinary homes, the commissioners of the Board, auditors general and stewards of finances, the Chancellor, archbishops and bishops of France, universities, and the list of major exhibitions, sessions of the courts of [[Paris]] and the log of the Palace, and finally addresses the messengers and items indicating the day of departure.\n\nIn 1699, [[Louis XIV]] asked him what the author describes in detail his work. Here as his widow explains these beginnings :\n\n:"Louis XIV, who wanted this glorious memory Almanac, made him ask the author, who had the honor to present to Her Majesty\'s what induced him to give the title of Royal Almanac, & to make it his principal occupation of this work. "\n\nThe same year [[Louis Tribouillet]], chaplain of the king and canon of [[Meaux]], publishes its State of France. This book describes in detail the functioning of the Court of the King, all his ministers, the treatments they receive, the various expenses of the state, clergy, etc..\n\nThe first edition of the Almanac Royal appears in 1700, at the same time as another book, Calendar of the Court of John Colombat, one of the printers of the King. Parisians have a choice between three books with similar content: the Almanach Royal Houry, Calendar of the Court of Colombat and the State of France Tribouillet. At that time, the yard around Louis XIV is highly stratified and since the expansion of the [[Palace of Versailles]] in 1684, it continues to grow. In this context, recognition of peers is a valuable asset "if someone has just placed a new post is an overflow of praise in its favor during the floods and the Chapel (...) but c is that while envy and jealousy talk like adulation.. One can understand the need to maintain directories so that everyone can follow the evolution of all these people. The multiplicity of these publications so says the king\'s will want to "officially" referencing his [[courtier]]s to charges created to keep beside him at [[Palace of Versailles|Versailles]], and maintain the jealousies of each other.\n\nEven if the king gave his approval, publishing such a book is not without risk. In December 1708, Laurent d\'Houry is being prosecuted for having established a [[printing press]] in his house and forced to sell its equipment two months later. Then, in February 1716, he was imprisoned in the Bastille on complaint of the [[Earl of Stair]], the British Ambassador, "to be disrespectful in his almanac, the King George by not naming him not as king of England, or rather from Great Britain, and mention the king as son of Jacques II Stuart, exiled to St. Germain.\n\n===Affirmation of a monopoly===\nThe Almanac and the Royal Court Calendar coexist peacefully for ten years and a lot of money to their authors, but from 1710, Laurent d\'Houry integrates more and more topics like the book Colombat Biblio. The abscess broke out in 1717 when Houry Almanac releases its Abstract that will follow the format of the Calendar of the Court and simultaneously filing a [[lawsuit]] against its competitor. A Judgement of Solomon is made: if it is allowed to Houry now to continue the publication of its abstract and to counterfeit the Court Calendar, Colombat is obliged to freeze its calendar format and forbidden \'expand content. This stops any changes in the calendar of the Court, leaving the way open to the Almanac.\n\nUpon the death of Laurent Houry in 1725, his family is [[destitute]]. Revenues from sales of the Almanac are not sufficient to cover expenses of printing and bookselling. In these circumstances, his widow, Elizabeth Dubois, took over the business. Their son Charles-Maurice, who had hitherto been a mere [[proofreader]] of the Almanac, is trying to evict her mother and she is suing cons. It prepares the edition of 1726 but a ruling forbade him to publish it in his name alone. The ruling of 11 December 1726 forbids even disturb the affairs of his mother and to participate directly or indirectly to the development of the Almanac. That is why Charles-Maurice is mentioned as editor of the Almanac on the edition of 1726.\n\nIn 1731, she filed a new lawsuit against Colombat which increased its schedule despite the prohibition of 1717. Unsuccessful, she resumed the publication of the Abstract and Colombat complaint in turn, she then accepts to abandon the publication of the Abstract "if returned to Colombat format from 1718. "The disputes have become extinct with the death of the parties.\n\nThe privilege of the Almanac is about to expire, Charles-Maurice d\'Houry tries one last time to seize it, but a royal letter of 27 March 1744 confirmed definitively André-François Le Breton as sole heir.\n\n==A family of hegemony 131 years==\n===Directorate of André Le Breton===\nIn 1728, the widow of his grandson, Houry combines son André-François Le Breton, who was 18 years old and an orphan under the [[guardianship]] of Charles-Maurice d\'Houry. Andrew Francis had inherited, according to the will of the estate of Lawrence Houry, half of the rights of the Almanac, and his widow, the rest.\n\nUnder his leadership, the Almanac takes a new breath and adds new sections, which sometimes does not go without punishment. For example, in 1768, he has trouble with [[Voltaire]], who sent him a letter incendiary:\n\n:"I say as much to Le Breton, the Almanach royal printer: I\'ll pay him Almanac point that sold me this year. He had the rudeness to say that Mr. President... Mr. advisor..., remains in the cul-de-sac to Menard, in the cul-de-sac of the White Coats in the ass -de-sac de l\'Orangerie. (...) How can you say that president remains in a serious ass?"\n\nIn 1773, Le Breton moved his print shop in a wing of the former Hotel d\'Alegre, at 13 rue de Hautefeuille, he acquired William Louis Joly de Fleury and was previously occupied by Ambassador Portugal.\n\nIn the late eighteenth century, the weather is bad and bad [[wheat]] harvests. The price of this staple increases disproportionately. Recently, a rumor that the government would have the [[monopoly]] wheat, thus perpetuating the high cost of food. This rumor became official when accidentally in his edition of 1774, Le Breton added a "Treasurer\'s grain account of the King" in the person of Sr. Mirlavaud. The edition of the Almanac had yet been proofread and approved by the Chancellor, but was still sentenced to close his shop for 3 months and publish a revised edition, without the line in question,.\n\nIn 1777, Le Breton was again accused of inserting information deemed subversive. It has, according to its critics, cited "the Floral & Pranks Vergès & Vaucresson, among the prosecutors and attorneys general of the [[Parliament of Paris]], who had been involved in a reform of parliament made by Maupeou against the venality of Parliament desired by [[Louis XV]], but annulled by [[Louis XVI]]. In rebuke, Le Breton was sentenced to " carton "section on the Almanacs that had not yet been sold, and replacement cost, the Almanac issue of those who so request it."\n\nHe died on 4 October 1779 and his cousin, Charles-Laurent Houry, son of Charles-Maurice d\'Houry, took over the business.\n\n===French Revolution===\nThe privilege granted to the family of Houry for the Almanac has been threatened in 1789 when [[Camille Desmoulins]], in his speech at the Lantern to the Parisians, says it will cease in favor of Baldwin, another Parisian publisher. This threat has not been brought into effect since the Almanac remained in Houry. Looking at the publications of the time, we can nevertheless see that Baldwin got the impressions of the [[National Assembly (French Revolution)|National Assembly]] and other organs of state.\n\n===Last generation Houry===\nFollowing the death of Joan Nera, widow of Laurent-Charles Houry, the Almanac is echoed by Jean-François-Noël Debure, husband of Anne-Charlotte d\'Houry, their daughter. Debure is from a prominent and wealthy family of Parisian booksellers, especially combined with the Didot family. It is a printer since 1784 with the title\'s printer [[Duke of Orleans]].\n\nDebure takes time printing of the family of Houry, but his other business is in trouble and he is forced to file for [[bankruptcy]]. To keep the property inherited from his family, Anne-Charlotte d\'Houry hires a separation procedure. In November 1791, the bankruptcy is declared and it is opposed to the [[creditor]]s to preserve his legacy, and this opposition is futile and a ruling allows creditors to seize his furniture, but that does not appear to have been necessary because a subsequent decision allows him to recover property that creditors have not taken her husband.\n\nFrancois-Jean-Noel Debure dies 1802 in [[Loiret]]. However, it is focused died from 1795 through various sources. Maybe it was just left without leaving an address.\n\nStephen Lawrence Testu worked as a clerk in the family home Debure since 1788, and had gradually won the confidence of the household. Because of the absence of Mr. Debure, Anne-Charlotte is alone with his two son. Despite their age difference, he is 20 years younger than she, she married Testu July 1795. Testu few highlights its knowledge in the profession to convince him to transfer the management of printing. It accepts in 1797 and offered him priority over the rights of the Almanac in exchange for a perpetual [[Annuity (European financial arrangements)|annuity]] of 800 francs, then she completely abandons the Almanac. This influx of money seems to turn the head Testu who play games and learn to enjoy the easy life, neglects the direction of its establishment and constantly running out of money, he contracted many loans that gradually ruining his business. Relations were strained with his wife because he left the marital home in September 1801 and the only ties that bind the couple are now linked to multiple trials they s\'intentent.\n\nIn 1810, Testu secretly sold the rights of the Almanac in which he partnered with Guyot. Anne-Charlotte d\'Houry opposes this sale she saw as a usurpation, but loses the case in 1812. She gets in return a pension of 1 200 francs Testu does not pay. Indeed, a decree of 1820 declares the debtor more than 90 000 francs... In 1814, due to the large sums invested by Guyot in the case, an order confirming the owner of the Almanac, a copy of this order is also printed at the end of the following books. Testu still gets Guyot repayment of its debts and an annuity of 2,400 francs.\n\nGuyot dismisses Testu business in 1820 and continues even to pay his annuity. The latter, again running out of money turned in 1823 against his wife, calls it reaches the marital home and she pays all household expenses, or alternatively, that she pays rent of 6000 francs. Judges déboutent Testu the marital home, since he himself had deserted 22 years earlier and has no housing to offer his wife even though she already lives in a very beautiful but still require his wife, yet very rich, to pay him a pension of 1,800 francs by invoking the solidarity between spouses.\n\nThe [[hegemony]] of the family of Houry on the Almanac established in 1683 has finally ceased in 1814 when, by order, the company is transferred to the association Guyot-Testu. Anne-Charlotte d\'Houry died 22 July 1828 aged about 83 years.\n\n===Judgement of publishing===\nIn 1867, edition of the Almanac is transferred to the widow Berger-Levrault, who had already published the Yearbook of the French empire diplomatic, and military Yearbook of the French empire, both published as the Almanac according to documents provided by the administration.\n\nThe edition of the Almanac stops definitively in 1919 after four years of interruption due to war, the latter number includes the years 1915 to 1919. Not found in the literature the reasons that prompted the shutdown of publication, but it can be assumed that the combination of very large volume of the book (more than 1650 pages in 1900) and the hard times that the [[Economics]] and French policy at that time was to make the management of such a volume of information extremely complex and unprofitable for the publisher. It is also possible that the new government formed after elections in 1919 no longer supported the development of the Almanac.\n\n==Changing content==\n===The topics in the Almanac===\nThe Almanac or Calendar, as he was called in its early editions, was just a simple calendar which were associated topics on astronomical events, the days of fairs, the newspaper of the Palace, the residence of messengers The departure of the mails, the price of currencies and the list of collectors\' offices. After his presentation to the king in 1699, many items are constantly being added including the clergy, the royal family of France, then the families of other sovereign nations, officers, ambassadors, etc..\n\nIn 1705, Houry added to the list of knights of the Holy Spirit and peer and marshal of France. In 1707, it is the state of the clergy and, in 1712, the birth of kings, princes and princesses of Europe. After the death of Louis XIV, the Duke of Orleans, became the Regent, is added to the list of members of the royal family of the members of the [[House of Orleans]]. Later, he put more of his own, the full house of the Queen and princes.\n\nIt is not possible to describe all items contained in an almanac as there, so the contents of 1780 covers ten pages:\n\nThe Almanac also stands abreast of scientific advances. In the middle of the eighteenth century, improving the accuracy of clocks and many wealthy fans begin to observe and study the stars. It is indispensable to know precisely the difference between true solar time of [[sundial]]s, and mean solar time clocks, especially since the advent of clocks seconds. This is the equation of the pendulum, also called the equation of time, the table is added shortly before 1750.\n\nWith the [[French Revolution]], the Almanac exchange of title and its content is modified to match the new institutions.\n\nThe abolition of all distinctions requires overhaul the topics, timing of the vulgar era is replaced by the [[French Republican Calendar|Republican calendar]], the place reserved for kings and princes of Europe is replaced by a note on the friendly powers of France, the administrative organs of royal power are replaced by new ministries.\n\nThe content changes again with the reforms of the Consulate and the Empire, the Restoration, the [[Hundred Days]], the [[July Monarchy]], the [[French Second Republic|Second Republic]] followed the [[Second French Empire|Second Empire]] and the [[French Third Republic|Third Republic]] who sees the end of the edition of Almanac. In each case, the bindings are supplied with the times.\n\nAs the number of entries is increasing, the number of pages follows the same trend: they numbered one hundred in 1699, nearly five-hundred in 1760, and seven-hundred just before the French Revolution. The course of a thousand pages is taken in 1840 to over 1000-6-cents in 1900. On average, about thirty names are listed per page, the total number of people or places listed annually in the tens of thousands, but no table patronymic does quickly find a particular name.\n\nAll changes in the Almanac makes it a very useful book for historians that may follow, year after year, ministries and other administrative bodies, movements of people in these offices, and retail organization public services to a resident of Paris (such as places of mailboxes, timetables and fares for ticks and royal messengers ).\n\n===Chronology of the 237 years of publishing Almanacs===\nAfter the death of Laurent Houry, his descendants continued his work until 1814. The edition was continued until 1919. It would be tedious to describe in words the evolution of the Almanac of the 237 years that have elapsed since the first edition by Laurent d\'Houry in 1683, hence the choice of this table layout.\n\nThroughout its existence, the Almanac has crossed 11 schemes political editor changed the title 14 times and 9 times.\n\n==Publication==\n===Collecting information===\nSince its inception in 1700, following a royal demand, the Almanac invented by Laurent d\'Houry aims to be an official handbook.\n\nUntil the French Revolution, contributors are cordially invited to provide information to the bookseller, as pointed note of the printer in the first pages of the Almanac. In 1771, for example, we read in the Journal History of the Revolution that the Bar Association in the person of a certain Gerbier, asserted that "there would be no change in the order of the table, and that it would be printed in the Almanac as Royal was last year, leaving out only the dead."\n\nWith the French Revolution, the order was given to government to provide all information to the publisher. In 1802, Testu gets even exclusivity.\n\nLater, the collection of information for the Almanac is even part of the operating budget of the ministries and can be seen in order of December 31, 1844 signed by [[Louis-Philippe I]] "on the organization of the Ministry Administration Navy "Article 6 of which list the items in the budget" the formation of the Royal Almanac.\n\n====Typography====\nThe print quality improves significantly when Laurent d\'Houry became printer. It multiplies the bands and tail-lights to decorate and titles for sections. The Almanac is still very poor prints because the image is not its goal. Only that the reader can find are patterns explaining the oppositions of the planets and eclipses are present every year, and the map of [[departments of France]] editions of 1791 and 1792.\n\nDespite the short time to prepare the book, the printer treats the presentation and uses in the case of many variations in size and shape of characters for easy reading of long lists, special characters to emphasize certain lines, compositions tables or columns and clusters in braces.\n\nThat Le Breton, grand son of Laurent Houry, who brings more to the book. It increases greatly and restructures the Almanac, and also improves its presentation in order to preserve readability. Many notes are added to guide the reader and help in understanding the operation of certain administrative bodies.\n\nThe Almanacs modern nineteenth-century advantage of technological advances. Cartoon characters are modernized and the use of fonts to graphics customizable multiplies, sometimes to excess: we can count at least 7 in 11 fonts fonts differ on the cover page of the National Almanac 1850 printed by Guyot et Scribe!\n\nAnnouncements, the ancestors of [[advertising]], are introduced by the publisher Berger-Levrault in the late nineteenth century.\n\n====Well-to-shoot====\nThe deadline for submitting this information to the editor is set to "first ten days of October (or November). The last-minute changes are incorporated in an erratum end of the book. When they are too large, they may even delay the release. In late December, an event is sent to the administration for approving the content. This approval is required before the sale.\n\nIt leaves only two months to integrate the information of the year in the text of the previous edition and call all of the pages before submitting the book for the right to shoot. The editing step, at least for the test in 1706, has not been done with great care as can be seen by very many shells and mistakes which have crept into the [[table of contents]] presented in thumbnail to the right.\n\nOnce the administrative agreement obtained, it is inserted end of the book, the Almanac is stapled or bound and is then distributed to customers at the end of the year.\n\n===Printing===\nEarly Almanacs were not printed by Laurent d\'Houry. The Almanac of 1706 and is printed by Jacques Vincent, installed Huchette street, at the sign of the Angel. November 15, 1712, Laurent Houry became printer and immediately began printing his work. Then all the almanacs will be printed by their publishers.\n\n====Draw====\nThere is no source that explains the draw of the Almanac. The only figures available are the annual rents generated by sales.\n\nIn 1782, Mercier said a pension of more than 40 000 francs. Diderot, at the same time, puts the figure at 30,000 pounds. For a price of sale of 5 to 6 pounds, the draw must necessarily be greater than about 7500 almanacs.\n\nAround 1820, during the trials that have brought the widow and Debure Testu, income of the Almanac was estimated between 25 and 30 000 francs. In 1834, another almanac, the Almanac of France, said that its cost is 35 cents for a sale price of 50 cents. Booksellers then purchase the item at prices of 38 cents, to resell a suggested retail price of 50 cents. The publisher earns so 3 cents per pound sold, the bookseller earns 12 (minus shipping costs, dependent). If one considers - arbitrarily - a four Almanac is sold directly into the library Testu (priced at 10 francs 50) the remainder being passed through intermediaries, we can prorate that to generate an annual income of 30 000 francs Testu must sell approximately 25 000.\n\nIn the absence of more precise information, we can only estimate at about 15,000, the number of copies sold per year between the late eighteenth and early nineteenth century.\n\n====Binding====\nThe almanac is sold either stapled or bound by the printer. The [[paperback]] version allows the purchaser to connect his book as he wishes, and so it is possible to find books with bindings very ornate, with lace, weapons of families, many colors brightened or gilding Biblio 24, etc..\n\nThe bound version provided by the printer is usually presented in a bound in calf or Morocco,{{clarify|date=August 2012}} full, and lilies in the boxes back. With the revolution, the lilies are replaced with Phrygian caps in roundels Library 25.\n\n====Distribution====\nThe Almanac is normally available from the bookseller, but it can also be found in the province in other bookstores that serve as intermediaries, for example in 1816, at Pesche, bookseller at Le Mans Ref 17, or by correspondence through the [[Sorbonne]], as did Voltaire Ref 18.\n\n===Readership===\nThe Almanac has a very great interest because of the number of subjects it addresses the organization of the French administration. In 1785, Mairobert wrote that "the Almanach Royal is in the hands of everyone and is among the Princes, the King\'s office, the foreign ministers would cater Ref 19. Louis-Sebastien Mercier in a pamphlet, the Tableau de Paris that stands in 1782 Ref 20 explains that "Those who are thrown into the paths of ambition, study Almanac Royal with serious attention," "more a beautiful royal consult the almanac to see if her lover is a lieutenant or sergeant,... ", that" everyone is buying the almanac to find out exactly where they stand. "And finally" even Fontenelle said, that it was the book which contained the greatest truths."\n\nAdages use the almanac as a reference. According to Jean-François de La Harpe is "the only book to read to get rich is the Royal Almanac Ref 21, [[Jean-Joseph Regnault-Warin]] uses the phrase" having the memory of a Royal Almanac Ref 22 " or the Memoirs of the Academy of hawkers Ref 23 explains that "it is enough to read the Almanac for education."\n\nIn the eyes of [[justice]], the book can be used as a basis for comparison: during a police investigation in 1824 Ref 24, a defendant defends himself by explaining that the volume of documents he was accused of having carried "could be equal to that of a royal Almanac almanac or a related trade."\n\nWhether to have a certain level of resources to purchase this item, the customer extended beyond the financial and political world.\n\n==Competition==\nThe Royal Almanac is competing at its inception with the Almanac of the Court of Colombat who can not make it evolve since 1717. In fact, bibliographers consider that the Royal Almanac is one of the "oldest and most helpful Ref 25". If it essentially describes the royal court and the Parisian institutions, other major cities also have their almanacs, such as the city of Lyon equally voluminous Ref 26. The Almanac is however considered a reference book. In 1780, a notice of a bookseller named Desnos inserted at the end of the Gazette of 27 offers for courts Ref 8 pounds to "the statesman, letters, and generally all persons attached to the service of the King (... ) Almanach Royal, Calendar of the Court, said Colombat, Mignone Strennas-Note 22, Ref 28, the State Military Note 23, the four connected units, with shelves & stylus to write, which makes the closure ": the Royal Almanac ranks first in the collection.\n\n===The Court Calendar===\nSince 1717, the Calendar of the Court can not change, its sections are limited to an ephemeris of the celestial motions (30 years) increased by astronomical tables with sky conditions, and timing of the court to the family and royal house, lists of boards, departments and secretaries of state finances, births and deaths of kings, queens, princes and princesses of Europe, the knights of various orders, the archbishops and bishops of the kingdom and Cardinals of the Sacred College.\n\nIt is primarily sought for its ephemeris of the celestial motions and astronomical tables of events\n\n===The Almanac of Commerce===\nThe Almanac of Business, published by Sébastien Bottin in the eighteenth century contains, besides the addresses of shops in Paris, many useful statistics financiers. It is supplementary to the Almanach royal, which concerns only the French administration.\n\n===The State of France===\nSome have criticized the Almanach Royal of being a [[plagiarism]] of the State of France, another administrative directory, the first publication seems to have been made in 1619 and is still published in the middle of the eighteenth century Ref 30. However, the edition of 1736 of the State of France said it was a "periodical whose audience has applied for renewal from time to time, and had been published until 1699, 1702, 1708, 1712, 1718 and 1727 Ref 31. The latest editions of 1727 and 1736 five volumes contain over 500 pages each. Offices are described down to the smallest detail Note 25, the state of France is a companion volume of the Almanach royal use by those who wish to deepen their knowledge on the functioning of the French administration.\n\n==Examples of information contained in the Almanac==\nFurther details concerning the organization of the administration of the French state, and persons who occupied positions, many other topics are discussed in the Almanacs, for example in the eighteenth century:\n\n===The cost of construction in Paris===\nThis section is only found in the Almanacs of the early eighteenth century, and stops just before 1726.\n\nThere are prices for masonry, carpentry and joinery, roofing, locksmithing, painting and glazing that are usually in Paris, for example:\n\n:"Walls of circular pits, with layers of stone studded with low excess moilon quilted 18 inch thick, 22 pounds fquare fathom, and more in proportion to the depth of the wells, or other difficulties that may encounter."\n\nWith these data, the historian is able to quantify fully the construction of a building in Paris at that time.\n\n===The official ceremonies===\nThe Almanac explains in great detail some official ceremonies:\n* Opening Ceremony of the Annual Courthouse\n:"The Entry of Parliament is the day after the S. Martin, 12 November, which day Presidents in red dresses holding their furs & Mortar Note 26, & Gowns Gentlemen Consultants red, after attending the solemn Mass that are usually said by a Bishop in the grand hall of the Palace, receive oaths of Lawyers & Counsel. The first president made this day a speech to thank those who celebrated the Mass, which responds to him by another harangue Note 27."\n* Procession of the University, whose description takes three pages of the Almanac Note 28\n:"The Rector of the University at the end of its Rector, who regularly is only three months, indicating a general procession which assists the whole body. It is a ceremony that deserves to be seen. We will mark the place here What the doctors take the four faculties Note 29 that comprise the university, all the graduates of these faculties, with the Religious Orders Note 30. Procession from the Church of Religious Trinitarians, otherwise known as Maturin. (...) The procession is closed by the booksellers, papermakers, bookbinders, Parcherminiers, illuminators, writers swear by the University."\n\nThe detailed description of the ceremonies to stop mid-eighteenth century to make room for a still more comprehensive directory. A reference is then made towards the end of the book "guides for all kinds of ceremonies to be observed in the receipt of any office or employment whether in dress or in the Sword."\n\n===Transportation===\nTransportation of persons is ensured by the coaches, carriages, wagons and other carriages. Found in the Almanac schedules and rates of major roads.\n\nIn 1715, a passenger wishing to travel from Paris to [[Caen]] will go rue Saint-Denis on Monday at six o\'clock in the morning. He has previously "sent his clothes the night before early." Fifteen years later, the starting time is advanced to 5 am in summer and in 1750, the departure is 5 hours throughout the year. In 1780, two flights are scheduled Tuesdays and Fridays at 23:30 and the journey takes two days. A van, slower, except Sunday at noon and made the trip in four and a half days in summer and five days in winter. In 1790, transportation is now provided by the General Department of stagecoaches and mail royal France. Three coaches liaise on Tuesday, Thursday and Sunday and the van on Sunday. The departure is now Notre-Dame-des-Victoires.\n\nRates are rarely reported but in 1725 and 1761 is 18 pounds per person tournaments. He is 21 pounds in 1770 to reach 42 pounds in 1790 (fortunately for the traveler, it is stated that the "sleeping bag weighing 10 pounds is" free").\n\n===Company guards of the King Pumps===\nIn 1716, the king appoints François Perrier Dumouriez as Director General of public pump to remedy fire, without the audience is obliged to pay anything. In 1722, he founded the Compagnie des Gardes Pumps du Roy, under the direction of the same. This company later became the Brigade of firefighters in Paris Note 32.\n\nThe Almanac of 1719 lists these pumps and their wardens and deputy wardens. We then learned that a brigade is made up of four guards and four sub-custodians who are responsible for maintaining the material deposited in each district. What became three years later the Society of King\'s Guards Pumps were not at that time that 41 people, 17 pumps distributed in groups of 8 men and 4 or 3 pumps in the City Hall, the convent of the Grands Augustins The Carmelite convent in the Place Maubert, Convent of Mercy, and the Fathers of Little Place des Victoires, in addition to a pump at the Director General of the pumps, Rue Mazarine. Except Dumouriez guards pumps are not professional fire but shoemakers, carpenters, locksmiths, etc..\n\n==Considerations bibliophiles==\n===Availability===\nAlmanacs are found regularly in auctions and in the antiquarian booksellers. Given their importance documentary and the fact that there are beautiful copies, these books are particularly sought after by historians, writers, bibliophiles and enthusiasts.\n\nVolumes in the first round of the seventeenth century often exceed several thousand euros Ref 32, the other is generally negotiated between a few tens and five hundred euros, sometimes more, depending on their rarity, condition and quality bookbinding Note 33. Just over half are however available for free download on Gallica or Google Books:\n\n===Notes handwritten readers===\nAlmanacs contain some handwritten notes left by their readers. The value of the book can then be influenced upward or downward depending on the quality and content of these notes, and especially the person who wrote them - when you can identify it. They are usually found on page intentionally left blank for the ephemeris. Some of these notes can provide very interesting information, such as notes written on the page in August 1715 a copy of the BNF. It relates the circumstances of the death of Louis XIV, who was suffering from gangrene Note 34:\n\n:"We thought the death dez Roy Lundy 25. He marched\n:better a day or two quoyque hopeless. It\n:died after having suffered much and with great\n:Patience on Sunday September 1, r t is 8 pm Morning\n:M r le Duc d\'Orleans went to Parl t and was declared\n:Regent on 2. September e"\n\n==References==\n{{Reflist}}\n\n[[Category:Historiography of France]]\n[[Category:18th-century books]]\n[[Category:Directories]]']
['Almanach de Bruxelles', '36297227', "{{one source|date=July 2012}}\nThe '''''Almanach de Bruxelles''''' is a [[Belgian]] website that lists [[royal family|royal]] and [[nobility|noble]] [[dynasties]] out of [[Europe]] in the form of a database. \n\nIt was established in 1996 and lists around 2,690 world dynasties.<ref>créé en 1996, est le site de référence des dynasties en dehors de l'Europe...2.690 dynasties, beaucoup d'entre elles introuvables ailleurs [http://www.almanach.be/about/index.htm About the ''Almanach'']</ref>\n\n==See also==\n* ''[[Almanach de Gotha]]''\n* ''[[Almanach de Bruxelles (defunct)]]''\n\n==Sources==\n{{reflist}}\n\n==External links==\n*{{Official|www.almanach.be}}\n\n[[Category:Directories]]\n[[Category:Biographical dictionaries]]\n[[Category:European nobility]]\n[[Category:Genealogy publications]]\n\n\n{{website-stub}}\n{{royal-bio-book-stub}}\n{{bio-dict-stub}}"]
['R.L. Polk & Company', '20018380', '{{Advert|date=July 2009}}\n{{Infobox company\n|name = R.L. Polk & Company\n|logo =\n|type = Acquired by [[IHS Inc.]]\n|foundation = [[Detroit, MI]] {1870}\n|location_city =  [[Southfield, Michigan|Southfield]], MI\n|area_served = Worldwide\n|founder = [[Ralph Lane Polk]]\n|key_people = Stephen R. Polk\n* Chairman, President and CEO\nTim Rogers\n* President, Polk\nRichard Raines\n* President, CARFAX\nMichelle Goff\n* Senior Vice President/Chief Financial Officer\n|homepage = [http://www.polk.com www.polk.com]\n|industry = Automotive\n}}\n\n\'\'\'R. L. Polk & Company\'\'\' is a provider of [[automotive]] information and marketing [[solution]]s to the automotive industry, insurance companies, and related businesses.<ref name="usa.polk.com">[http://usa.polk.com/Company/WhoWeAre/ R. L. Polk & Company :: Our Company :: Who We Are :: Index] Retrieved on 10/31/08  {{webarchive |url=https://web.archive.org/web/20080702225115/http://usa.polk.com/Company/WhoWeAre/ |date=July 2, 2008 }}</ref>\n\nPolk was acquired by [[IHS Inc.]] on July 15, 2013 <ref>http://press.ihs.com/press-release/corporate-financial/ihs-completes-acquisition-rl-polk-co</ref> and is based in Southfield, Michigan with operations in several countries, including the United States, Canada, Germany, United Kingdom, France, Japan, China and Australia.<ref name="usa.polk.com"/>\n\n==Company history==\n[[Image:1880 spine Illinois State Gazetteer by Polk & Co.png|thumb|100px|left|Polk\'s \'\'Illinois State Gazetteer\'\', 1880]]\nR. L. Polk & Company was founded by [[Ralph Lane Polk]] in 1870 in Detroit, MI as a publisher of business directories. In 1872, the company first published a City Directory, for Evansville, Indiana, plus a listing of post offices in nine states. Additional directories followed in the ensuing years as the business grew.<ref name=heritage>[http://usa.polk.com/Company/Heritage/ R.L. Polk : Heritage]  {{webarchive |url=https://web.archive.org/web/20091229120305/http://usa.polk.com/Company/Heritage/ |date=December 29, 2009 }}</ref> claiming 1000 directories by 1960.<ref>{{cite book|title=Polk\'s Abilene (Taylor County, Texas) City Directory, 1960|date=1960|publisher=R. L. Polk & Co|page=7|url=http://texashistory.unt.edu/ark:/67531/metapth160223/m1/7/|accessdate=27 September 2014}}</ref>  Affiliates included the Polk-Husted Directory Co. of Oakland, California.<ref>{{cite book |url=https://books.google.com/books?id=TNlKAQAAIAAJ&pg=PA547 |title=Polk\'s San Jose City and Santa Clara County Directory |year=1907 }}</ref> In addition to city directories, the company published bank directories.\n\nIn 1907, R.L. Polk & Co. was publishing a "[[Gazetteer]]" Business directory for the State of Michigan and Windsor and Walkerville Ontario, as well as gazetteers for Alaska, Arkansas, California, Idaho, Illinois, Oklahoma, Indiana, Iowa, Kansas, Kentucky, Maryland, Minnesota, North Dakota, South Dakota, Montana, Missouri, Nevada, Oregon, Washington State, Pennsylvania, Tennessee, Texas, Utah, West Virginia, and Wisconsin.<ref>{{cite book|title=Michigan State Gazetteer and Business Directory|date=1907|publisher=R.L. Polk & Co.|location=Detroit|page=2|edition=1907-1908|url=https://books.google.com/books?id=absfAQAAMAAJ&lpg=PA250&dq=%22manitou%22%20steamship%20charlevoix&pg=PA81#v=onepage&q=%22manitou%22%20steamship%20charlevoix&f=false|accessdate=7 June 2016}}</ref>\n\nIn 1921, a conversation between Ralph Lane Polk II and [[Alfred P. Sloan]] (who later became president of General Motors) helped fuel R. L. Polk & Company\'s entry into the automotive industry. During the conversation, Sloan asked Polk to impartially tabulate and publish statistical information on cars and trucks in operation. R.L. Polk & Company launched its motor vehicle statistical operations in 1922, when the first car registration reports were published.<ref>http://web.archive.org/web/20071116145915/http://www.salesforce.com/customers/business-services/case-studies/rlpolk.jsp Retrieved on 11/4/08</ref> In 1922, R.L. Polk & Co. published its first Passenger Car Registration Report, covering 58 makes and accounting for 9.2 million passenger automobiles on America\'s highways.\n\nFrom 1951 to 1958, the company pioneered the use of electronic punch card tabulating equipment. In 1956, Polk\'s reporting services included monthly statistics on boats, business aircraft, motorcycles, commercial trailers, and recreational vehicles. In 1976, the National Vehicle Population Profile (NVPP) was introduced.\n\n===1990s===\nIn 1993, Polk combined their Canadian activities with Blackburn Marketing Services to form Blackburn / Polk Marketing Services Inc. (BPMSI).  Polk also acquired a 35% interest in CARFAX from Blackburn Marketing Services.  In 1995, Polk entered an alliance with Marketing Systems GmBH and acquired a substantial minority interest in The Ultimate Perspective (T.U.P).\n\nIn 1996, Polk completed acquisition of the Blackburn / Polk operations and renamed it Polk Canada Marketing Services Inc. (PCMSI).  This acquisition unified and strengthened their North American operations in Polk\'s strategy to be a global information services provider.  They also announced their first Automotive Loyalty Award winners.\n\nIn 1997, Polk acquired the MSS division of Automatic Data Processing\'s European Operations.\n\nIn 1999, Polk completed acquisition of CARFAX and sold Advertising Unlimited, Inc. to Norwood Promotional Products.\n\n===2000 and Beyond===\nIn 2000, Polk sells its Consumer Information Solutions (CIS) business units Direct Marketing, Data Information Services / Polk Verity, City Directory, and the Compusearch and Prospects Unlimited units of Polk Canada to Equifax.\n\nPolk launches Garage Predictors and Polk Canada, Inc. announces Polk Canada Net. Polk also completes its acquisition of Marketing Systems Group.\n\nRalph Lane Polk II is inducted into the prestigious Automotive Hall of Fame (AHF) located in Dearborn, Michigan in 2001. Stephen R. Polk is also a part of the AHF as a director<ref>http://ias.net/ahof/v1n3/ Retrieved on 11/5/08</ref> and R. L. Polk & Co. is also considered a Sapphire Level Supporter.<ref>http://ias.net/ahof/v1n3/ Retrieved 11/5/08</ref>\n\nIn 2002, Polk launches the Polk Vehicle Lifecycle System and the Polk Cross Sell is introduced.\n\nAlso in 2002, Ralph Lane Polk II is inducted into The Direct Marketing Association (DMA) Hall of Fame, the highest professional honor in direct and interactive marketing. DMA inducts into "The Hall of Fame" as many as four individuals each year for the significant impact these leaders have had on the growth of the direct and interactive process.<ref>http://www.the-dma.org/awards/halloffame.shtml Retrieved on 11/4/08</ref>\n\nIn 2003, PolkInsight is launched. Polk Total Market Predictor (Polk TMP) is also introduced.\n\nIn 2004, R. L. Polk & Company launches Polk Cross Sell Report and RLPTechnologies, a new wholly owned subsidiary, is established. Also, The [[Software Engineering Institute|Software Engineering Institute (SEI)]] awards R. L. Polk & Company with a Level II Capability Maturity Model Integrated (CMMI) rating.\n\nIn 2005, R. L. Polk & Company introduces the Polk Inventory Efficiency Award. The Polk Inventory Efficiency Award recognizes and rewards outstanding aftermarket companies for process improvements relative to inventory efficiency.<ref>http://www.reuters.com/article/pressRelease/idUS147801+21-May-2008+PRN20080521 Retrieved 11/14/08</ref>\n\nIn 2007, R. L. Polk & Co. acquire a majority interest in ROADTODATA, a rapidly growing supplier of automotive price and specifications data.<ref>http://japan.polk.com/News/LatestNews/R.+L.+Polk+and+ROADTODATA+Merge.htm Retrieved 12/26/08</ref>\n\nIn 2010, R. L. Polk & Company partners with Citytwist.<ref>https://www.ihs.com/Customer/citytwist-auto-excellence-award.html</ref>\n\nIn 2013, IHS, Inc announced a $1.4B purchase of R.L. Polk.<ref>http://www.mlive.com/auto/index.ssf/2013/06/information_company_ihs_to_pur.html</ref>\n\nThe company\'s business-to-business marketing services include PolkInsight, the National Vehicle Population Profile (NVPP), Blackburn / Polk Marketing Services Inc. (BPMSI), Polk Dealer Marketing Manager,<ref>http://google.com/search?q=cache:2uEMeAtCgckJ:findarticles.com/p/articles/mi_hb6674/is_/ai_n26650183+polk+and+Marketing+Systems+GmBH&hl=en&ct=clnk&cd=7&gl=us Retrieved on 11/4/08</ref> The Ultimate Perspective (T.U.P), Polk Canada Net, Polk Vehicle Lifecycle System, Polk CrossSell Reports,<ref>http://www.prnewswire.com/cgi-bin/stories.pl?ACCT=104&STORY=/www/story/01-27-2004/0002097303&EDATE= Retrieved on 11/4/08</ref> and Polk Total Market Predictor (Polk TMP).{{Citation needed|date=July 2009}}\n\n==CARFAX==\n\nThe Polk Company announced on August 2, 1999 that it had completed acquisition of [[Carfax (company)|Carfax]]. Polk had previously owned 35 percent of Carfax, in partnership with the Blackburn Group, Inc., of London, Ontario, [[Canada]], and has now acquired the remaining 65 percent.<ref name="theautochannel.com">http://www.theautochannel.com/articles/press/date/19990802/press027618.html Retrieved 11/7/08</ref> Carfax compiles vehicle histories from various sources, with about 75 percent of the information coming from Polk data.  Using the [[vehicle identification number]] (VIN), each history provides potential buyers with all available facts about a used car being considered for purchase.  This may include original use of the vehicle odometer records, number of owners, and other items that might affect a purchase decision.<ref name="theautochannel.com"/>\n\n==See also==\n* [[St. Louis City Directories]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite book |url=https://archive.org/search.php?query=creator%3A%22R.L.+Polk+%26+Co%22 |title=Directory of Directories |publisher=R.L. Polk & Co. |location=NY |year=1916 }}\n\n==External links==\n{{commons category|R.L. Polk & Co.}}\n* Internet Archive. [https://archive.org/search.php?query=creator%3A%22R.L.+Polk+%26+Co%22 Works published by R.L. Polk & Co.], various dates\n* Hathi Trust. [http://catalog.hathitrust.org/Search/Home?checkspelling=true&lookfor=%22polk+%26+co%22&type=publisher&sethtftonly=true&submit=Find Works related to R.L. Polk & Co.], various dates\n* OCLC WorldCat. [http://www.worldcat.org/search?q=au%3A%22polk+%26+co Works related to R.L. Polk & Co.], various dates\n\n{{DEFAULTSORT:Polk and Co.}}\n[[Category:Companies based in Detroit]]\n[[Category:Directories|polk]]\n[[Category:Publishing companies established in 1870]]\n[[Category:American companies established in 1870]]']
['Search.ch', '661012', '\'\'\'search.ch\'\'\' is a [[search engine]] and [[web portal]] for [[Switzerland]]. It was founded in 1995 <ref name="founding" /> by Rudolf Räber and Bernhard Seefeld as a regional search engine. In the following years many other services were added, such as a phonebook in 1999, a free [[Short message service|SMS]] service in 2000 (now reduced to only one free SMS per week).\n\nThe search technology is home grown. The user can restrict his search to regions of Switzerland, such as a [[cantons of Switzerland|canton]] or a [[city]]. The [[web crawler]] looks only at sites in the [[.ch]] and [[.li]] [[top-level domain]]s and a number of automatically and manually updated list of Swiss websites on other domains. The index is updated weekly.\n\n== External links ==\n* [http://www.search.ch/ search.ch]\n* [http://tel.search.ch/ tel.search.ch] phonebook\n* [http://map.search.ch/ map.search.ch] Swiss maps\n* [http://meteo.search.ch/ meteo.search.ch] Swiss weather\n* [http://news.search.ch/ news.search.ch] Swiss news\n* [http://timetable.search.ch/ timetable.search.ch] Swiss public transport timetable\n* [http://tv.search.ch/ tv.search.ch] Swiss TV programme\n* [http://kino.search.ch/ kino.search.ch] Swiss cinema programme\n* [http://sms.search.ch/ sms.search.ch] sms service\n* [http://immo.search.ch/ immo.search.ch] immo portal search service\n* [http://www.post.ch/ Swiss Post] <ref name="post" /> Swiss Post acquired search.ch\n* [http://www.tamedia.ch Tamedia] <ref name="tamedia" /> Tamedia akquired a 75% stake from Swiss Post\n\n==References==\n{{Reflist|refs=\n<ref name="founding">http://www.moneyhouse.ch/en/u/search_ch_ag_CH-130.0.009.911-2.htm</ref>\n<ref name="tamedia">http://about.search.ch/archives/2004/06/04/post-kauft-search-ch/</ref>\n<ref name="post">http://www.post.ch/post-startseite/post-konzern/post-medien/post-archive/2009/post-mm09-fruehzustellung/post-medienmitteilungen.htm</ref>\n}}\n\n{{DEFAULTSORT:Search.Ch}}\n[[Category:Web portals]]\n[[Category:Internet search engines]]\n[[Category:Directories]]']
['Category:Indexes', '1789509', "An '''index''' is something that points the reader to other [[information]]. This category is for articles about indexes.\n{{distinguish|Category:Index numbers}}\n{{Cat more|Index (publishing)}}\n{{For|Wikipedia content|Category:Wikipedia indexes}}\n\n[[Category:Index (publishing)]]\n[[Category:Publications]]\n[[Category:Directories]]"]
['Category:Directory services', '30937577', '{{cat main|Directory service}}\n\n[[Category:Directories]]\n[[Category:Computer access control protocols]]\n[[Category:Access control software]]\n[[Category:Network service]]\n[[Category:Database management systems]]']
['World Leaders', '233058', "{{italic title}}\n[[Image:ChiefsCover.jpg|right|200px|thumb|Cover of ''World Leaders'']]\n'''''World Leaders''''', also known as '''''Chiefs of State and Cabinet Members of Foreign Governments''''', is a [[public domain]] directory published weekly by the United States [[Central Intelligence Agency]]. It lists different state officials for each country of the world: the [[head of state]] and/or [[head of government]] and other [[cabinet minister]]s, the chief of the [[central bank]], and the [[ambassador]]s to the [[United Nations]] and the United States.\n\n==See also==\n*[[World-Check]]\n*[[List of current heads of state and government]]\n*[[National Security Agency academic publications]]\n*''[[International Who's Who]]''\n\n==External links==\n*[https://www.cia.gov/library/publications/world-leaders-1/ ''World Leaders'']\n\n[[Category:Central Intelligence Agency publications]]\n[[Category:Heads of state]]\n[[Category:Heads of government]]\n[[Category:Directories]]\n[[Category:Public domain databases]]\n\n{{US-gov-stub}}"]
['Encyclopedia of Associations', '44017130', '{{Italic title}}\nThe \'\'\'\'\'Encyclopedia of Associations\'\'\'\'\' (\'\'EA\'\') is a comprehensive directory of more than 20,000 [[Voluntary associations|associations]], [[Society|societies]], and other non-profit membership organizations in the United States of America.<ref>[http://www.gale.cengage.com/DirectoryLibrary/GML33507EA%20GDL.pdf Encyclopedia of Associations]</ref>\n\nOriginally titled the \'\'Encyclopedia of American Associations\'\', \'\'EA\'\' was created by [[Frederick Gale Ruffner, Jr.]] in 1954 while working as a market researcher in [[Detroit, Michigan]].<ref>[http://lj.libraryjournal.com/2014/08/publishing/gale-founder-frederick-ruffner-dies-at-88/#_ "Gale Founder Frederick Ruffner Dies at 88" \'\'Library Journal\'\'. – Retrieved October 3, 2014]</ref>\n\nMore than 140 scholarly articles have made use of \'\'EA\'\'.<ref>[http://www.unc.edu/~fbaum/papers/JSTOR-EA-annotated-bibliography.pdf "An Annotated Bibliography of Articles Using the \'\'Encyclopedia of Associations\'\'" - Retrieved October 3, 2014.]</ref>\n\nPast extracts from \'\'EA\'\' have included "Organized Obsessions" <ref>[http://lccn.loc.gov/92219621 - Library of Congress LCCN Permalink for 92219621]</ref> and the "Gale Encyclopedia of Business and Professional Associations".<ref>[http://lccn.loc.gov/95649648 - Library of Congress LCCN Permalink for 95649648]</ref>\n\nA detailed history of \'\'EA\'\' is available in an article in \'\'Distinguished Classics of Reference Publishing\'\'<ref>[https://archive.org/stream/DistinguishedClassicsOfReferencePublishing#page/n101/mode/2up - Tobin, Carol M. "The Book that Built Gale Research: The \'\'Encyclopedia of Associations\'\'."  \'\'Distinguished Classics of Reference Publishing\'\']</ref><ref>[http://lccn.loc.gov/91033629 - Library of Congress LCCN Permalink for 91033629]</ref>\n\n== See also ==\n* [[Gale Research]]\n* [[Frederick Gale Ruffner, Jr.]]\n\n== References ==\n<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using<ref></ref> tags, these references will then appear here automatically -->\n{{Reflist}}\n\n== External links ==\n* [http://gale.cengage.com/ Gale website]\n\n<!--- Categories --->\n[[Category:Directories]]\n[[Category:Specialized encyclopedias]]']
['Yachting Pages', '47332933', "{{Use dmy dates|date=September 2013}}\n{{Use British English|date= September 2013}}\n{{Infobox company\n| logo     = [[File:Yachting Pages Logo Black KNOCKOUT.svg]]\n| foundation       = [[Antibes]], [[France]] (2003)\n| location         = [[Bristol]], [[Somerset]]\n[[United Kingdom]]\n| key_people       = \n| num_employees    = 34 (2013)\n| industry         = [[Superyacht]]\n| homepage         = {{url|www.yachting-pages.com/}}\n}}\n'''''Yachting Pages''''' is a [[superyacht]] business with a range of products aimed at [[Captain (nautical)|captains]] and [[crew]], [[shipyards]], refit yards and all within the superyacht industry. ''Yachting Pages'' is available either in its original printed form, or online. Since the first edition of ''Yachting Pages'' was released in 2004, the book has grown rapidly into an established superyacht [[Trade directory|directory]].\n\nThe annual print directory is available in three separate editions:\n\n*''Mediterranean, Europe, Africa & Middle East''\n*''USA, the Americas & Caribbean''\n*''Australasia, Asia Pacific & Far East''\n\nThe [[Port]] Maps section at the front of every edition totals over 350 detailed maps of the world's superyacht [[marinas]]. Copies of the printed directory are hand delivered free of charge directly to superyachts by uniformed crew, and also to superyacht marinas and land-based superyacht businesses in over 92 countries.<ref> Yachtingpages.com </ref>\n\n'''The Company'''\n\n''Yachting Pages'' was founded in May 2003 from current CEO Steve Crowe's spare bedroom in [[Antibes]], [[France]], with only one other member of staff. The company is now based in [[Bristol]], [[United Kingdom]] with 34 staff members, many of whom are [[multi-lingual]]. \n\nThe first copy of ''Yachting Pages'' was launched at the Genoa Charter Show, in May 2004. \n\nSince then, growth of the business has created more products: ''Yachtingpages.com, Yachting Pages Refit, Yachting Pages Delivers and Superyacht Owners' Guide (SYOG).'' \n\n'''Awards'''\n\nQueens award for Enterprise: International Trade 2009. \nEADP European B2B Award 2009 \n\n==References==\n{{Reflist}}\n{{refimprove|date=August 2013}}\n\n==External links==\n* [http://www.yachting-pages.com/ Yachting Pages]\n\n[[Category:Directories]]\n[[Category:Yachting]]"]
['Prospective search', '3345817', '{{one source|date=April 2014}}\n\'\'\'Prospective search\'\'\', or \'\'\'persistent search\'\'\', is a method of [[Search engine technology|searching]] on the [[Internet]] where the query is given first and the information for the results are then acquired. This differs from traditional, or "retrospective", search such as [[search engines]], where the information for the results is acquired and then queried.<ref name=globalpr2005>{{cite web|url=http://www.globalprblogweek.com/2005/09/21/wyman-reputation-management/|title=Blogs & Prospective Search Technology for Corporate Reputation Management|year=2005|author=Bob Wyman|publisher=Global PR Blog Week website}}</ref>\n\n== Comparison to retrospective search ==\nRetrospective search starts by gathering the information, indexing it, then letting users query the information. The results don\'t change until the index is rebuilt, often months apart. Prospective search starts with the user\'s queries, gathers the information in a targeted way, indexing it and then providing the results as they arrive. Sometimes [[Ping blog|Ping Servers]] are used to gather notification of changes to websites so that the information received is as fresh as possible. Users can be notified in a number of ways of new results.\n\nProspective search is well suited to queries where the results change over time, such as the current news, [[blog]]s and trends.\n\n== See also ==\n* [[PubSub]]\n* [[Google Alerts]]\n* Google AppEngine Prospective Search Service<ref>https://code.google.com/appengine/docs/python/prospectivesearch/</ref> (deprecated as of December 1st 2015<ref>https://cloud.google.com/appengine/docs/deprecations/prospective_search?hl=en</ref>)\n* [[Selective dissemination of information]]\n* [[Superfeedr]] (\'tracker\' API<ref>http://blog.superfeedr.com/full-text-trackers/</ref>)\n\n== Quotes ==\n{{quote|Prospective search is emerging as a way of keeping up-to-date on any subject of interest. This technology constantly monitors relevant blogs and Web feeds for matches to users’ subscriptions and delivers results in real time. Thus, users are notified whenever something new appears on their subject of choice|Global PR Blog Week<ref name=globalpr2005/>}}\n\n==References==\n{{reflist}}\n\n[[Category:Internet search]]\n\n\n{{compu-prog-stub}}']
['Sponsored search auction', '23022154', "A '''sponsored search auction (SSA)''', also known as a '''keyword auction''', is an indispensable part of the [[business model]] of modern [[web host]]s. It refers to results from a search engine that are not output by the main search algorithm, but\nrather clearly separate advertisements paid for by third parties. These advertisements\nare typically related to the terms for which the user searched. They come in the form\nof a link and some information, with the hope that a search user will click on the link\nand buy something from the advertiser.\nIn sponsored search auctions, there are typically some fixed number of slots for advertisements and more advertisers that want these slots than there are slots. The advertisers\nhave different valuations for these slots and slots are a scarce resource, so an auction\nis done to determine how the slots will be assigned.\n\n==History==\nPrior to 1998, many advertisements were charged by impression, as it was the\neasiest metric to calculate. In 1998, GoTo.com, Inc debuted a pay-per-click charging\nsystem, with pricing and slot placement determined by an auction. GoTo used a first\nprice auction, where bidders were placed according to their bids and charged their bids\nwhen they won. GoTo faced bidders who were constantly changing their bid\nin response to new information and changing information from other bidders.\n\nCurrently, charging per action is a common pricing scheme in affiliate networks,\nsuch as the Amazon Associates Program.\n\nIn 2002, [[Google AdWords]] began using a second price auction to sell the single advertisement\nslot. Shortly thereafter, pages had multiple advertisements slots, which were allocated\nand sold via [[generalized second-price auction]] (GSP) auction, the natural generalization of a second price, single item, multi bidder\nauction.<ref>Hal Varian, Christopher Harris. The VCG Auction in Theory and Practice, In The\nAmerican Economic Review, Volume 104, Issue 5, pages 442-452. Elsevier B.V.,\n2014.</ref>\n\n==Auction Mechanisms==\n===Generalized Second Price Auction===\n[[Generalized second-price auction]] (GSP) is the most commonly used auction mechanism for sponsored search.\n\n====Untruthfulness====\nAn issue with GSP is that it's not a truthful auction and it is not the optimal strategy. To illustrate this, consider the following example.\n\nThere are three bidders with only two possible slots. The values of\neach bidders 1, 2, and 3 are $10, $5, and $3 respectively. Suppose that the first slot click\nthrough rate (CTR) is 300 and the second slot CTR is 290. If bidder 1 is truthful, he\nwould have to pay <math>p_1 = $5(300) = $1500</math> for a utility of <math>u_1 = $10(300)-$1500 = $1500</math>.\nHowever, if bidder 1 decides to lie and reports a value of $4 instead then his utility\nwould be <math>u_2 = $10(290) - $3(290) = $2030</math>. Notice that <math>u_2 > u_1</math> which makes GSP\nuntruthful and bidders have an incentive to lie.\n\n====Quality Variant====\nGoogle uses a minor variant of GSP to auction off advertisement slots. Potential\nadvertisements may be of varying quality. Suppose that there are two advertisements\nfor eggs. One advertisement simply fills its space with the word “egg” repeated over\nand over, while the other advertisement shows a picture of eggs, contains branding\ninformation, and mentions positive qualities about their eggs, such as cage-freeness.\nThe second advertisement may be thought of as having higher quality than the first\nadvertisement, being more useful to consumers, more likely to be clicked on, and more\nlikely to generate revenue for both the advertiser and Google. Advertisements that\nhave a history of high click through rates, are geographically targeted at the user, or\nhave a high quality landing page may also be thought of as having higher quality.<ref>Google AdWords, Check and understand Quality Score. support.google.com/adwords/answer/2454010</ref>\n\nGoogle assigns a numeric “quality” score <math>\\gamma_i</math> to each bidder <math>i</math>. Bidders, rather than\nbeing ordered purely by their bid, are instead ordered by rank, which is the product\nof their bid and quality score <math>\\gamma_1 b_1 \\geq \\gamma_2 b_2 \\geq \\dots \\geq \\gamma_n b_n</math> . Slots are still assigned in\ndecreasing rank order. Bidders are charged, rather than the bid of the bidder one rank\nlower (<math>p_i(b_i, b_{-i}) = b_{i+1}</math>), are charged the minimum price for which, if it was their bid,\nwould keep them in their current rank: <math>p_i(b_i, b_{-i}) = \\frac{\\gamma_{i+1}b_{i+1}}{\\gamma_i}</math>\n\n===Vickrey–Clarke–Groves Auction===\n[[Vickrey–Clarke–Groves auction]] (VCG) is a truthful auction optimizing social welfare. VCG is more complicated to explain than GSP and that might deter many websites from using a VCG auction mechanism even though it's truthful. However, some websites use VCG as their auction mechanism, most notably [[Facebook]].\n\n==See also==\n*[[Generalized second-price auction]]\n*[[Vickrey–Clarke–Groves auction]]\n\n==References==\n{{reflist}}\n\n[[Category:Internet search]]"]
['Real-time web', '23231423', '{{multiple issues|\n{{more footnotes|date=November 2010}}\n{{refimprove|date=November 2010}}\n{{essay|date=April 2015}}\n{{Buzzword|date=July 2011}}xxdxxx\n}}\n\nThe \'\'\'real-time web\'\'\' is a network web using technologies and practices that enable users to receive information as soon as it is published by its authors, rather than requiring that they or their software check a source periodically for updates.\n\n==Difference from real-time computing==\nThe real-time web is fundamentally different from [[real-time computing]] since there is no knowing when, or if, a response will be received. The information types transmitted this way are often short messages, status updates, news alerts, or links to longer documents. The content is often "soft" in that it is based on the [[social web]]—people\'s opinions, attitudes, thoughts, and interests—as opposed to hard news or facts.\n\n==(Old) True-realtime web (an "alternate" model)==\nFrom another point of view, the real-time web consists in making the client interface (or the web side; or the web layer) of a web application, to communicate continuously with the corresponding real-time server, during every user connection. As a fast pic of the client/server model, imagine each client object (each web module of the web [[GUI]] of an application) having its object class alive as a sub process (of its user session) in the server environment. In this scenario, the web is considered as the human entrance (interface) to the real-time environment: at each connected web URL, or Internet real-time zone, corresponds a different "front-end" web application. The real-time server acts as a [[logic network operating system]] for the programmable array of applications; handles the array of connected users for each application; attends for connections from real-world appliances and second level real-time servers. Applications behaviours and the intercommunication procedures between online services or applications, online users, and connected devices or appliances, are settled in the corresponding source code of each real-time service written in the real-time-interpreted programming language of the centric server.\n\nAs opposite to previous scenario, real-time web is exactly soft [[real-time computing]]: the round trip of a data ping-pong signal from the real-time server to the client must take about 1s (max) to be considered real-time and not to be annoying for humans (or users) during their connections.{{Citation needed|date=April 2016}} About the dispute between social web and real-time web, we can say real-time web is social by default and it is not true the contrary (WEB-r comes before Web 2.0). The WEB-r model is called [[true-realtime web]] to highlight the differences with the defective (de facto) model of real-time web generally perceived. From the industry point of view, this model of (general) real-time Internet can also be defined as [[electronic web]], that comes with the intrinsic meaning of not being limited to the web side of the Net, and with the direct reference to its server/rest-of-the-world perspective as a mechanism of a single clock.\n\n==History==\nExamples of real-time web are Facebook\'s newsfeed, and Twitter, implemented in social networking, search, and news sites. Benefits are said to include increased user engagement ("flow") and decreased server loads. In December 2009 real-time search facilities were added to [[Google Search]].<ref>{{cite web|url=http://googleblog.blogspot.com/2009/12/relevance-meets-real-time-web.html|title=Relevance meets the real-time web}}</ref>\n\nThe absolutely first realtime web implementation worldwide have been the WIMS true-realtime server and its web apps in 2001-2011 (WIMS = Web Interactive Management System); based on the WEB-r model of above; built in Java (serverside) and Adobe Flash (clientside). The true-realtime web model was born in 2000 at mc2labs.net by an Italian independent researcher.\n\n==Real-time search==\nA problem created by the rapid pace and huge volume of information created by real-time web technologies and practices is finding relevant information. One approach, known as \'\'\'real-time search\'\'\', is the concept of searching for and finding information online as it is produced. Advancements in web search technology coupled with growing use of [[social media]] enable online activities to be queried as they occur. A traditional [[web search]] [[Web crawler|crawls]] and [[Index (search engine)|indexes]] web pages periodically, returning results based on relevance to the search query. [[Google Real-Time Search]] was available in [[Google Search]] until July 2011.\n\n==See also==\n*[[Comet (programming)|Comet]]\n*[[Collaborative real-time editor]]\n*[[Firebase]]\n*[[Internet of Things|Internet of Things (IoT)]]\n*[[Meteor (web framework)|Meteor]]\n*[[Microblogging]]\n*[[Node.js]]\n*[[Prospective search]]\n*[[PubNub]]\n*[[Push technology|Push Technology]]\n*[[Scoopler]]\n*[[Vert.x]]\n*[https://www.syncano.io Syncano]\n\n==References==\n<references />\n\n==External links==\n*{{Cite news|url=https://www.theguardian.com/business/2009/may/19/google-twitter-partnership|title=Google \'falling behind Twitter\'|last=Wray|first=Richard|date=19 May 2009|work=The Guardian|accessdate=17 June 2009}}\n*{{Cite news|url=http://www.nytimes.com/2009/06/14/business/14digi.html|title=Hey, Just a Minute (or Why Google Isn\'t Twitter)|last=Stross|first=Randall|date=13 June 2009 |work=New York Times|accessdate=17 June 2009}}\n*{{Cite news|url=http://online.wsj.com/article/BT-CO-20090615-712397.html |title=Internet Giants Look For Edge In Real-Time Search |last=Morrison |first=Scott |date=15 June 2009 |work=Wall Street Journal |accessdate=17 June 2009 |deadurl=yes |archiveurl=https://web.archive.org/web/20090616204058/http://online.wsj.com/article/BT-CO-20090615-712397.html |archivedate=16 June 2009 }} \n*{{Cite news|url=http://www.readwriteweb.com/archives/explaining_the_real-time_web_in_100_words_or_less.php|title=Explaining the Real-Time Web in 100 Words or Less|last=Kirkpatrick|first=Marshall|date=22 September 2009|work=ReadWriteWeb}}\n\n{{Use dmy dates|date=October 2010}}\n\n{{DEFAULTSORT:Real-Time Web}}\n[[Category:Internet search]]\n[[Category:Real-time web| ]]']
['Notey', '49177882', "{{Orphan|date=August 2016}}\n\n{{Infobox Website\n| name           = Notey\n| logo           = [[Image:Notey (logo).png|220px]]\n| screenshot     = \n| caption        = \n| url            = [http://www.notey.com/ notey.com]\n| commercial     = \n| type           = Blog discovery platform\n| language       = English\n| registration   = \n| owner          = \n| launch date    = February 2015\n| current status = active\n| revenue        = \n| slogan         = \n| alexa          =  \n}}\n\n'''Notey''' is a [[:Category:Blog search engines|blog search]] and discovery platform founded in 2015 that helps users discover non-mainstream content and blogs. The platform ranks and features both bloggers and independent publishers on various topics including [[technology]], [[weddings]], [[sneakers]] and more than 500,000 others. Users can upvote articles they like, save them in notebooks and see what the community is reading.<ref>{{cite web | url=http://techcrunch.com/2015/02/17/notey | title=Notey Raises $1.6 Million For Its Topic-Focused Blog Directory  | publisher=Techcrunch | date=February 17, 2015 | accessdate=2016-01-21}}</ref>\n\nIn April 2015, Business Insider named Notey “one of the fastest growing startups in the world still flying under the radar”.<ref>{{cite web | url= http://uk.businessinsider.com/15-of-the-fastest-growing-b2b-startups-2015-4?op=1 | title=15 of the fastest growing startups in the world still flying under the radar  | publisher=Business Insider | date=April 17, 2015 | accessdate=2016-01-21}}</ref>\n\nThe company is based in Hong Kong and San Francisco.<ref>{{cite web | url= https://www.crunchbase.com/organization/notey | title=Notey &#124; CrunchBase | year=2015 | publisher=CrunchBase | accessdate=2016-01-21}}</ref> Its investors include [[Hugo Barra]], [[Ryan Holmes]], Shakil Khan and [[Steve Kirsch]].\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.notey.com/ Notey Home Page]\n\n{{Commons category|Internet search engines}}\n\n[[Category:Blog search engines]]\n[[Category:Websites|Search engines]]\n[[Category:Internet search]]\n[[Category:Aggregation websites]]\n[[Category:Search engine software]]\n\n\n{{searchengine-website-stub}}\n{{US-company-stub}}"]
['National Centre for Text Mining', '10795520', '{{Multiple issues|\n{{COI|date=December 2015}}\n{{advert|date=December 2015}}\n{{external links|date=December 2015}}\n}}\n\n{{Infobox academic division\n| name           = National Centre for Text Mining (NaCTeM)\n| image_name     = \n| image_alt      = National Centre for Text Mining \n| established    = 2004\n| type           = \n| parent         = [[School of Computer Science, University of Manchester]] \n| affiliation    = [[University of Manchester]]\n| city           = [[Manchester]]\n| country        = [[United Kingdom]]\n| director       = Prof. Sophia Ananiadou\n| website        = {{URL|www.nactem.ac.uk}}\n| logo           =<!-- Deleted image removed:  [[File:Nactem hires.tif|300px]] -->\n}}\n\nThe \'\'\'National Centre for Text Mining\'\'\' (\'\'\'NaCTeM\'\'\')<ref name="ariadne">{{cite journal| author=Ananiadou S| title=The National Centre for Text Mining: A Vision for the Future | journal=Ariadne | year= 2007 | issue= 53 | url=http://www.ariadne.ac.uk/issue53/ananiadou/  }}</ref> is a publicly funded [[text mining]] (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the [[United Kingdom]] academic community.\n\nThe [[software]] tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest – examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as [[Europe PubMed Central]].\n\nThe Centre is located in the [[Manchester Institute of Biotechnology]] and is operated and organized by the [[University of Manchester School of Computer Science]]. NaCTeM contributes expertise in [[natural language processing]] and [[information extraction]], including [[named-entity recognition]] and extractions of complex relationships (or events) that hold between named entitites, along with parallel and distributed data mining systems in biomedical and clinical applications.\n\n==Services==\n[http://www.nactem.ac.uk/software/termine/ \'\'\'TerMine\'\'\'] is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them.<ref name="multi-word">{{cite journal| author=Frantzi, K., Ananiadou, S. and Mima, H.| title=Automatic recognition of multi-word terms | journal=International Journal of Digital Libraries | year= 2007 | volume=3 |issue= 2 | pages= 117–132|  url=http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf }}</ref>\n\n[http://www.nactem.ac.uk/software/acromine/ \'\'\'AcroMine\'\'\'] finds all known expanded forms of [[acronyms]] as they have appeared in [[Medline]] entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in [[Medline]] and [[disambiguation (metadata)|disambiguates]] them.<ref name="pmid17050571">{{cite journal|vauthors=Okazaki N, Ananiadou S | title=Building an abbreviation dictionary using a term recognition approach. | journal=Bioinformatics | year= 2006 | volume= 22 | issue= 24 | pages= 3089–95 | pmid=17050571 | doi=10.1093/bioinformatics/btl534 | pmc= | url=http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&tool=sumsearch.org/cite&retmode=ref&cmd=prlinks&id=17050571  }}</ref>\n\n[http://www.nactem.ac.uk/medie/ \'\'\'Medie\'\'\'] is  an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from [[Medline]] abstracts <ref>{{cite conference |author=Miyao, Y., Ohta, T., Masuda, K., Tsuruoka, Y., Yoshida, K., Ninomiya, T. and Tsujii, J.|title=Semantic Retrieval for the Accurate Identification of Relational Concepts in Massive Textbases |year=2006 |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=1017–1024 |doi=10.3115/1220175.1220303}}</ref>\n\n[http://www.nactem.ac.uk/facta/ \'\'\'Facta+\'\'\'] is a [[Medline]]  search engine for finding associations between biomedical concepts.<ref name="pmid18772154">{{cite journal|vauthors=Tsuruoka Y, Tsujii J, Ananiadou S | title=FACTA: a text search engine for finding associated biomedical concepts | journal=Bioinformatics | year= 2008 | volume= 24 | issue= 21 | pages= 2559–60 | pmid=18772154 | doi=10.1093/bioinformatics/btn469 | pmc=2572701   }}</ref>\n\n[http://www.nactem.ac.uk/facta-visualizer/ \'\'\'Facta+ Visualizer\'\'\'] is a web application that aids in understanding FACTA+ search results through intuitive graphical visualisation.<ref>{{Cite journal| last1 = Tsuruoka| first1 = Y\n| last2 = Miwa| first2 = M| last3 = Hamamoto| first3 = K| last4 = Tsujii| first4 = J| last5 = Ananiadou| first5 = S\n | year = 2011| title = Discovering and visualizing indirect associations between biomedical concepts\n | journal = Bioinformatics| volume = 27| issue = 13 | pages = i111-9| publisher =  | jstor = | doi = 10.1093/bioinformatics/btr214 }}</ref>\n\n[http://www.nactem.ac.uk/software/kleio/ \'\'\'KLEIO\'\'\'] is a faceted semantic information retrieval system over [[Medline]] abstracts.\n\n[http://labs.europepmc.org/evf \'\'\'Europe PMC EvidenceFinder\'\'\'] helps users to explore facts that involve entities of interest within the full text articles of the [[Europe PubMed Central]] database.<ref>{{cite journal| author=The Europe PMC Consortium| title=Europe PMC: a full-text literature database for the life sciences and platform for innovation | journal=Nucleic Acids Research| year= 2014 | volume= 43 | issue=D1 | pages=D1042-D1048 | doi=10.1093/nar/gku1061 | pmid=25378340 | pmc=4383902}}</ref>\n\n[http://www.nactem.ac.uk/EvidenceFinderAnatomyMK/ \'\'\'EUPMC Evidence Finder for Anatomical entities with meta-knowledge\'\'\'] – similar to the Europe PMC EvidenceFinder, allowing exploration of facts involving anatomical entities within the full text articles of the [[Europe PubMed Central]] database.  Facts can be filtered according to various aspects of their interpretation (e.g., negation, certainly level, novelty).\n\n[http://www.nactem.ac.uk/info-pubmed/ \'\'\'Info-PubMed\'\'\'] provides information and graphical representation of biomedical interactions extracted from [[Medline]] using deep [[Semantic analysis (machine learning)|semantic parsing]] technology. This is supplemented with a term dictionary consisting of over 200,000 [[protein]]/[[gene]] names  and identification of [[disease]] types and [[organisms]].\n\n[http://www.nactem.ac.uk/ClinicalTrialProtocols/ \'\'\'Clinical Trial Protocols (ASCOT) \'\'\'] is an efficient, semantically-enhanced search application, customised for clinical trial documents.<ref>{{cite journal| author=Korkontzelos, I., Mu, T. and Ananiadou, S.| title=ASCOT: a text mining-based web-service for efficient search and assisted creation of clinical trials | journal=BMC Medical Informatics and Decision Making | year= 2012 | volume= 12 | issue= Suppl 1 | pages= S3 | doi=10.1186/1472-6947-12-S1-S3}}</ref>\n\n[http://www.nactem.ac.uk/hom/ \'\'\'History of Medicine (HOM)\'\'\'] is a semantic search system over historical medical document archives\n\n==Resources==\n\n[http://www.nactem.ac.uk/biolexicon/ \'\'\'BioLexicon\'\'\'] – a large-scale terminological resource for the biomedical domain.<ref>{{cite journal| author=Thompson, P., McNaught, J., Montemagni, S., Calzolari, N., del Gratta, R., Lee, V., Marchi, S., Monachini, M., Pezik, P., Quochi, V., Rupp, C. J., Sasaki, Y., Venturi, G., Rebholz-Schuhmann, D. and Ananiadou, S.| title=The BioLexicon: a large-scale terminological resource for biomedical text mining | journal=BMC Bioinformatics | year= 2011 | volume= 12 | pages=397 | doi=10.1186/1471-2105-12-397}}</ref>\n\n[http://www.nactem.ac.uk/genia/ \'\'\'GENIA\'\'\'] – a collection of reference materials for the development of biomedical text mining systems.\n\n[http://www.nactem.ac.uk/GREC/ \'\'\'GREC\'\'\']  – a semantically annotated corpus of [[Medline]] abstracts intended for training IE systems and/or resources which are used to extract events from biomedical literature.<ref>{{cite journal| author=Thompson, P., Iqbal, S. A., McNaught, J. and Ananiadou, S.| title=Construction of an annotated corpus to support biomedical information extraction| journal=BMC Bioinformatics| year= 2009| volume= 10 | pages=349| doi=10.1186/1471-2105-10-349}}</ref>\n\n[http://www.nactem.ac.uk/metabolite-corpus/ \'\'\'Metabolite and Enzyme Corpus\'\'\']  – a corpus of [[Medline]] abstracts annotated by experts with metabolite and enzyme names.\n\n[http://www.nactem.ac.uk/anatomy_corpora/ \'\'\'Anatomy Corpora\'\'\']  – A collection of corpora manually annotated with fine-grained, species-independent anatomical entities, to facilitate the development of text mining systems that can carry out detailed and comprehensive analyses of biomedical scientific text.<ref>{{cite journal| author=Pyysalo, S., Ohta, T., Miwa, M., Cho, H. -C., Tsujii, J. and Ananiadou, S.| title=Event extraction across multiple levels of biological organization| journal=Bioinformatics| year= 2012| volume= 28 | issue=18 |pages=i575-i581| doi=10.1093/bioinformatics/bts407}}</ref>\n<ref>{{cite journal|author1=Pyysalo, S.  |author2=Ananiadou, S. |lastauthoramp=yes | title=Anatomical Entity Mention Recognition at Literature Scale| journal=Bioinformatics| year= 2014| volume= 30 | issue=6 |pages=868–875| doi=10.1093/bioinformatics/btt580}}</ref>\n\n[http://www.nactem.ac.uk/meta-knowledge/ \'\'\'Meta-knowledge corpus\'\'\']  – an enrichment of the [http://www.nactem.ac.uk/genia/ \'\'\'GENIA Event corpus\'\'\'], in which events are enriched with various levels of information pertaining to their interpretation. The aim is to allow systems to be trained that can distinguish between events that factual information or experimental analyses, definite information from speculated information, etc.<ref>{{cite journal| author=Thompson, P., Nawaz, R., McNaught, J. and Ananiadou, S.| title=Enriching a biomedical event corpus with meta-knowledge annotation| journal=BMC Bioinformatics| year= 2011| volume= 12  |pages=393| doi=10.1186/1471-2105-12-393}}</ref>\n\n==Projects==\n\n[http://nactem.ac.uk/argo/ \'\'\'Argo\'\'\'] – The objective of the Argo project is to develop a workbench for analysing (primarily annotating) textual data. The workbench, which is accessed as a web application, supports the combination of elementary text-processing components to form comprehensive processing workflows. It provides functionality to manually intervene in the otherwise automatic process of annotation by correcting or creating new annotations, and facilitates user collaboration by providing sharing capabilities for user-owned resources. Argo benefits users such as text-analysis designers by providing an integrated environment for the development of processing workflows; annotators/curators by providing manual annotation functionalities supported by automatic pre-processing and post-processing; and developers by providing a workbench for testing and evaluating text analytics.\n\n[http://nactem.ac.uk/big_mechanism/ \'\'\'Big Mechanism\'\'\'] –  Big mechanisms are large, explanatory models of complicated systems in which interactions have important causal effects. Whilst the collection of big data is increasingly automated, the creation of big mechanisms remains a largely human effort, which is becoming made increasingly challenging, according to the fragmentation and distribution of knowledge. The ability to automate the construction of big mechanisms could have a major impact on scientific research. As one of a number of different projects that make up the big mechanism programme, funded by [[DARPA]], the aim is to assemble an overarching big mechanism from the literature and prior experiments and to utilise this for the probabilistic interpretation of new patient panomics data. We will integrate machine reading of the cancer literature with probabilistic reasoning across cancer claims using specially-designed ontologies, computational modeling of cancer mechanisms (pathways), automated hypothesis generation to extend knowledge of the mechanisms and a \'Robot Scientist\' that performs experiments to test the hypotheses. A repetitive cycle of text mining, modelling, experimental testing, and worldview updating is intended to lead to increased knowledge about cancer mechanisms.\n\n[http://nactem.ac.uk/copious/ \'\'\'COPIOUS\'\'\'] – This project aims to produce a knowledge repository of Philippine biodiversity by combining the domain-relevant expertise and resources of Philippine partners with the text mining-based big data analytics of the University of Manchester\'s National Centre for Text Mining. The repository will be a synergy of different types of information, e.g., taxonomic, occurrence, ecological, biomolecular, biochemical, thus providing users with a comprehensive view on species of interest that will allow them to (1) carry out predictive analysis on species distributions, and (2) investigate potential medicinal applications of natural products derived from Philippine species.\n\n[http://nactem.ac.uk/europepmc/ \'\'\'Europe PMC Project\'\'\'] – This is a collaboration with the Text-Mining group at the [[European Bioinformatics Institute]] (EBI) and [[Mimas (data centre)]], forming a work package in the [[Europe PubMed Central]] project (formerly UKPMC) hosted and coordinated by the [[British Library]]. Europe PMC, as a whole, forms a European version of the [[PubMed Central]] paper repository, in collaboration with the [[National Institutes of Health]] (NIH) in the United States. Europe PMC is funded by a consortium of key funding bodies from the biomedical research funders. The contribution to this major project is in the application of text mining solutions to enhance information retrieval and knowledge discovery. As such this is an application of technology developed in other NaCTeM projects on a large scale and in a prominent resource for the Biomedicine community.\n\n[http://nactem.ac.uk/DID-MIBIO/ \'\'\'Mining Biodiversity\'\'\'] – This project aims to transform the [[Biodiversity Heritage Library]] (BHL) into a next-generation social digital library resource to facilitate the study and discussion (via social media integration) of legacy science documents on biodiversity by a worldwide community and to raise awareness of the changes in biodiversity over time in the general public. The project integrates novel text mining methods, visualisation, crowdsourcing and social media into the BHL. The resulting digital resource will provide fully interlinked and indexed access to the full content of BHL library documents, via semantically enhanced and interactive browsing and searching capabilities, allowing users to locate precisely the information of interest to them in an easy and efficient manner.\n\n[http://nactem.ac.uk/text-mining-mrc/ \'\'\'Mining for Public Health\'\'\']  – This project aims to conduct novel research in text mining and machine learning to transform the way in which evidence-based public health (EBPH) reviews are conducted. The aims of the project are to develop new text mining unsupervised methods for deriving term similarities, to support screening while searching in EBPH reviews and to develop new algorithms for ranking and visualising meaningful associations of multiple types in a dynamic and iterative manner. These newly developed methods will be evaluated in EBPH reviews, based on implementation of a pilot, to ascertain the level of transformation in EBPH reviewing.\n\n==References==\n{{Reflist}}\n\n==External links==\n* http://www.nactem.ac.uk\n\n[[Category:Computational linguistics]]\n[[Category:Computer science organizations]]\n[[Category:Information retrieval organizations]]\n[[Category:Linguistics organizations]]\n[[Category:School of Computer Science, University of Manchester]]']
['European Summer School in Information Retrieval', '22254915', 'The \'\'\'European Summer School in Information Retrieval\'\'\' (\'\'\'ESSIR\'\'\') is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.\nThe aim of ESSIR is to give to its participants a common ground in different aspects of \'\'\'[[Information Retrieval]] (IR)\'\'\'. Maristella Agosti in 2008 stated that: "\'\'The term IR identifies the activities that a person – the user – has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need.\'\'"<ref>Agosti, M.: "Information Access using the Guide of User Requirements". In: \'\'Information Access through Search Engines and Digital Libraries\'\'. Agosti, M. ed., Springer-Verlag Berlin Heidelberg, pp. 1-12, (2008).</ref>\n\nIR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from [[computer science]] to [[information science]] and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the [[Internet]].\n\nESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the Internet.\n\nTwo books have been prepared as readings in IR from editions of ESSIR, the first one is \'\'Lectures on Information Retrieval\'\',<ref>Agosti, M., Crestani, F. and Pasi, G. (Eds): "Lectures on Information Retrieval". Revised Lectures of Third European Summer-School, ESSIR 2000 Varenna, Italy, September 11–15, 2000. LNCS Vol. 1980, Springer-Verlag, Berlin Heidelberg, 2001.</ref> the second one is \'\'Advanced Topics in Information Retrieval\'\'.<ref>Melucci, M., and Baeza-Yates, R. (Eds): "Advanced Topics in Information Retrieval". The Information Retrieval Series, Vol. 33, Springer-Verlag, Berlin Heidelberg, 2011.</ref>\n\n== ESSIR Editions ==\nESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by [http://www.dei.unipd.it/~agosti/ Maristella Agosti], [[University of Padua]], Italy and [[Nick Belkin]], [[Rutgers University]], U.S.A., for an Italian audience in 1989.\n\n{| class="wikitable" border="1"\n|-\n! Edition\n! Web Site\n! Location\n! Organiser(s)\n|-\n|  10th\n|  [http://mklab.iti.gr/essir2015/ ESSIR 2015]\n|  Thessaloniki, Greece\n|  Ioannis (Yiannis) Kompatsiaris, Symeon Papadopoulos, Theodora Tsikrika, and Stefanos Vrochidis\n|-\n|  9th\n|  [http://www.ugr.es/~essir2013/ ESSIR 2013]\n|  Granada, Spain\n|  Juan M. Fernadez-Luna and Juan F. Huete\n|-\n|  8th\n|  [http://essir.uni-koblenz.de/ ESSIR 2011]\n|  Koblenz, Germany\n|  Sergej Sizov and Steffen Staab\n|-\n|  7th\n|  [http://essir2009.dei.unipd.it/ ESSIR 2009]\n|  Padua, Italy\n|  Massimo Melucci and Ricardo Baeza-Yates\n|-\n|  6th\n|  [http://www.dcs.gla.ac.uk/essir2007/ ESSIR 2007]\n|  Glasgow, Scotland, United Kingdom\n|  Iadh Ounis and Keith van Rijsbergen\n|-\n|  5th\n|  [http://www.cdvp.dcu.ie/ESSIR2005/ ESSIR 2005]\n|  Dublin, Ireland\n|  Alan Smeaton\n|-\n|  4th\n|  [http://www-clips.imag.fr/mrim/essir03/main_essir.html ESSIR 2003]\n|  Aussois (Savoie), France\n|  Catherine Berrut and Yves Chiaramella\n|-\n|  3rd\n|  [http://www.itim.mi.cnr.it/Eventi/essir2000/index.htm ESSIR 2000]\n|  Varenna, Italy\n|  Maristella Agosti, Fabio Crestani, and Gabriella Pasi\n|-\n|  2nd\n|  [http://www.dcs.gla.ac.uk/essir/ ESSIR 1995]\n|  Glasgow, United Kingdom\n|  Keith van Rijsbergen\n|-\n|  1st\n|  [http://ims.dei.unipd.it/websites/essir/essir1990.html ESSIR 1990]\n|  Brixen, Italy\n|  Maristella Agosti\n|}\n\n==Notes==\n{{reflist}}\n\n==External links==\n* [http://ims.dei.unipd.it/websites/essir/home.html ESSIR presentation page of the IMS Research Group]\n* [http://ims.dei.unipd.it IMS Research Group, Department of Information Engineering – University of Padua, Italy]\n* [http://www.dei.unipd.it/ Department of Information Engineering – University of Padua, Italy]\n* [http://www.unipd.it/en/index.htm University of Padua, Italy]\n\n[[Category:Information retrieval organizations]]\n[[Category:Summer schools]]']
['Concept Searching Limited', '17770654', '{{Infobox company |\n  name   = Concept Searching Limited |\n  logo = [[Image:conceptSearching.jpg]] |\n  slogan = "Retrieval Just Got Smarter" |\n  type   =  [[Privately held company|Private]] |\n  foundation     = 2002|\n  location       = UK, United States |\n  area_served    = Global |\n  industry       = [[Information retrieval]] |\n  products       = conceptSearch<br/>conceptClassifier<br/>conceptClassifier for SharePoint<br/>conceptClassifier for SharePoint Online<br/>Taxonomy Manager<br/>Taxonomy Workflow |\n  homepage       = [http://www.conceptsearching.com/ www.conceptsearching.com]\n}}\n\n\'\'\'Concept Searching Limited\'\'\' is a [[software company]] which specializes in [[information retrieval]] software. It has products for [[Enterprise search]], Taxonomy Management and  [[Statistical classification]].\n\n==History==\nConcept Searching was founded in 2002 in the UK and now has offices in the USA and South Africa. In August 2003 the company introduced the idea of using [[Compound term processing]].<ref>[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN Lateral thinking in information retrieval] \'\'Information Management and Technology.\'\' 2003. vol 36; part 4, pp 169-173</ref><ref>[http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf] Lateral Thinking in Information Retrieval</ref>\n\nCompound term processing allows statistical information retrieval applications to perform matching using multi-word concepts. This can improve the quality of search results and also allows unstructured information to be automatically classified with semantic metadata.<ref>[http://airforcemedicine.afms.mil/711hswom/InterSymp2008/AFMS%20-%20InterSymp%202008.html] US Air Force Medical Service presentation at InterSymp-2008</ref>\n\nThe company\'s products run on the Microsoft [[.NET Framework|.NET]] platform. The products integrate with Microsoft [[SharePoint]] and many other platforms.<ref>[http://pinpoint.microsoft.com/en-US/partners/Concept-Searching-Inc-4297066101] Microsoft Partner Profile</ref>\n\nConcept Searching has developed the \'\'\'Smart Content Framework\'\'\', which is a toolset that provides an enterprise framework to mitigate risk, automate processes, manage information, protect privacy, and address compliance issues. The Smart Content Framework is used by many large organizations including 23,000 users at the [[NASA]] Safety Center <ref>[http://www.aiim.org/About/News/CS-NASA-Safety] NASA Safety Center using Smart Content Framework</ref>\n\n== Awards ==\n* 100 Companies that Matter in Knowledge Management 2009/2010/2011/2012/2013/2014/2015 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-COMPANIES-That-Matter-in-Knowledge-Management-102189.aspx |title=KMWorld Magazine}}</ref>\n* KMWorld Trend-Setting Products of 2009/2010/2011/2012/2013/2014/2015 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2015-105783.aspx |title=Trend-Setting Products}}</ref>\n\n==See also==\n* [[Compound term processing]]\n* [[Enterprise search]]\n* [[Full text search]]\n* [[Information retrieval]]\n* [[Concept Search]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.conceptsearching.com/ Company Website]\n\n[[Category:Information retrieval organizations]]\n[[Category:Privately held companies of the United Kingdom]]']
['Information Retrieval Specialist Group', '10218640', '{{Unreferenced|date=January 2010}}\n\nThe \'\'\'Information Retrieval Specialist Group\'\'\' (\'\'\'IRSG\'\'\') or \'\'\'BCS-IRSG\'\'\' is a Specialist Group of the [[British Computer Society]] concerned with supporting communication between researchers and practitioners, promoting the use of [[Information Retrieval]] (IR) methods in industry and raising public awareness. There is a newsletter called \'\'The Informer\'\', an annual European Conference (ECIR), and continual organisation and sponsorship of conferences, workshops and seminars. The current chair is Dr. Andy MacFarlane.{{Citation needed|date=January 2010}}\n\n==European Conference on Information Retrieval==\nOrganising [[European Conference on Information Retrieval|ECIR]] is one of the major activities of the Information Retrieval Specialist Group. The conference began in 1979 and has grown to become one of the major Information Retrieval conferences alongside [[Special Interest Group on Information Retrieval|SIGIR]] receiving hundreds of paper and poster submissions every year from around the world.{{Citation needed|date=January 2010}} ECIR was initially established by the IRSG under the name "Annual Colloquium on Information Retrieval Research", and held in the UK until 1997. It was renamed ECIR in 2003 to better reflect its status as an international conference.\n\n== External links ==\n* [http://irsg.bcs.org/ IRSG website]\n\n[[Category:Information retrieval organizations]]\n[[Category:BCS Specialist Groups]]']
['Gerard Salton Award', '1981660', 'The \'\'\'Gerard Salton Award\'\'\' is presented by the [[Association for Computing Machinery]] (ACM) [[Special Interest Group on Information Retrieval]] (SIGIR) every three years to an individual who has made "significant, sustained and continuing contributions to research in [[information retrieval]]". SIGIR also co-sponsors (with [[SIGWEB]]) the [[Vannevar Bush Award]], for the best paper at the [[Joint Conference on Digital Libraries]].\n\n==Chronological honorees and lectures==\n* 1983 - [[Gerard Salton]], [[Cornell University]] : "About the future of automatic information retrieval."\n* 1988 - [[Karen Spärck Jones]], [[University of Cambridge]] : "A look back and a look forward."\n* 1991 - [[Cyril Cleverdon]], [[Cranfield Institute of Technology]] : "The significance of the Cranfield tests on index languages."\n* 1994 - William S. Cooper, [[University of California, Berkeley]] : "The formalism of probability theory in IR: a foundation or an encumbrance?"\n* 1997 - [[Tefko Saracevic]], [[Rutgers University]] : "Users lost (summary): reflections on the past, future, and limits of information science." \n* 2000 - [[Stephen Robertson (computer scientist)|Stephen E. Robertson]], [[City University, London|City University London]] : "On theoretical argument in information retrieval."<BR>\'\'\'For ...\'\'\' \'\'"Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval."\'\'\n* 2003 - [[W. Bruce Croft]], [[University of Massachusetts Amherst]] : "Information retrieval and computer science: an evolving relationship."<BR>\'\'\'For ...\'\'\' \'\'"More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of [[Bayesian inference]] networks and [[language modelling]] for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems."\'\'\n* 2006 - [[C. J. van Rijsbergen]], [[University of Glasgow]] : \t"Quantum haystacks."\n* 2009 - [[Susan Dumais]], [[Microsoft Research]] : "An Interdisciplinary Perspective on Information Retrieval."\n* 2012 - [[Norbert Fuhr]], [[University of Duisburg-Essen]]: "Information Retrieval as Engineering Science."\n* 2015 - [[Nicholas J. Belkin]], [[Rutgers University]]: “People, Interacting with Information”\n\n==External links==\n* [http://www.acm.org/sigir/ ACM SIGIR homepage]\n* [http://www.sigir.org/awards/awards.html ACM SIGIR awards]\n\n[[Category:Association for Computing Machinery]]\n[[Category:Computer science awards]]\n[[Category:Information retrieval organizations]]']
['Category:Waymo', '52604208', '{{Commons category|Google}}\n{{Cat main|Waymo}}\n\n[[Category:Technology companies based in the San Francisco Bay Area]]\n[[Category:Companies based in Mountain View, California]]\n[[Category:Web portals]]\n[[Category:Information retrieval organizations]]\n[[Category:Alphabet Inc.]]\n[[Category:Wikipedia categories named after websites]]']
['AUTINDEX', '43739701', "{{multiple issues|\n{{COI|date=September 2014}}\n{{notability|Products|date=September 2014}}\n}}\n\n'''AUTINDEX''' is a commercial [[text mining]] software package based on sophisticated linguistics.<ref>Ripplinger, Bärbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen</ref><ref>Paul Schmidt, Mahmoud Gindiyeh & Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6–8 October 2009, Portugal</ref><ref>Paul Schmidt & Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19–20 November</ref>\n\n'''AUTINDEX''' resulting from research in [[information extraction]] <ref>Paul Schmidt, Thomas Bähr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen für die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt</ref><ref>Ursula Deriu, Jörn Lehmann & Paul Schmidt, 2009: ‚Erstellung einer Technik-Ontologie auf der Basis ausgefeilter Sprachtechnologie’. In: Proceedings Knowtech, Frankfurt</ref> is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing [[language technology]] since its foundation in 1985. IAI is an institute affiliated to [[Saarland University]] in Saarbrücken, Germany.\n\n'''AUTINDEX''' is the result of a number of research projects funded by the EU (Project BINDEX <ref>[//www.lrec-conf.org/proceedings/lrec2002/pdf/255.pdf]. Dieter Maas, Nuebel Rita, Catherine Pease, Paul Schmidt: Bilingual Indexing for Information Retrieval with AUTINDEX. LREC 2002.</ref>), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch <ref>[//www.l3s.de/AR07/layout/L3S-AR2007_screen.pdf]. Project LinSearch. P. 32.</ref> and WISSMER,<ref>[//www.wissmer.info/index.php/de/]. Project Wissmer.</ref> see also the reference to IAI-Webite.<ref>[//www.iai-sb.de/forschung/content/view/67/89/]. Wissmer-Project on IAI-Site.</ref>\n\nThe basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document.<ref>Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR – The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.–8. Oktober 2009, Portugal. 2009, S. 259 - 262.</ref> Ideally the system is integrated with a [[thesaurus]] that defines the standardised terms to be used for key word assignment.<br> \nAUTINDEX is used in library applications (e.g. integrated in [[dandelon.com]]) as well as in high quality (expert) information systems <ref>[//www.wti-frankfurt.de]. WTI Information system.</ref> and in document management and content management environments. <br> \n \nTogether with AUTINDEX a number of additional software comes along such as an integration with [[Apache Solr]] / [[Lucene]] to provide a complete [[information retrieval]] environment, a classification and [[categorisation]] system on the basis of a [[machine learning]] <ref>Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung, Logos Verlag, Berlin, 2013.</ref> software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called [[tag clouds]].<ref>[//www.wissmer.info]. Electro mobility information system.</ref>\n\n==See also==\n\n* [[Information retrieval]]\n* [[Linguistics]]\n* [[Knowledge Management]]\n* [[Natural Language Processing]]\n* [[Semantics]]\n\n== References ==\n{{reflist}}\n\n== Publications ==\n* Ripplinger, Bärbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen.\n* Paul Schmidt, Thomas Bähr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen für die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt.\n* Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR – The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.–8. Oktober 2009, Portugal. 2009, S. 259 - 262.\n* Paul Schmidt, Mahmoud Gindiyeh: ''Language Technology for Multilingual Information and Document Management.'' In: ''Proceedings of ASLIB.'' London, 19.–20. November 2009.\n* Rösener, Christoph, Ulrich Herb: ''Automatische Schlagwortvergabe aus der SWD für Repositorien.'' Zusammen mit Ulrich Herb in ''Proceedings.'' Berufsverband Information Bibliothek, Bibliothekartage. 97. Deutscher Bibliothekartag, Mannheim, 2008.\n* Svenja Siedle: ''Suchst du noch oder weißt du schon? Inhaltserschließung leicht gemacht mit automatischer Indexierung.'' In: ''tekom-Jahrestagung und tcworld conference 2013''\n* Michael Gerards, Adreas Gerards, Peter Weiland: ''Der Einsatz der automatischen Indexierungssoftware AUTINDEX im Zentrum für Psychologische Information und Dokumentation (ZPID).'' 2006 ([http://zpid.de/download/PSYNDEXmaterial/autindex.pdf Online] bei zpid.de, PDF-Datei)\n* Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung. Logos Verlag, Berlin, 2013.\n\n== External links ==\n* http://www.iai-sb.de/ Institute for Applied Information Sciences\n\n[[Category:Natural language processing]]\n[[Category:Information retrieval systems]]"]
['Locate (Unix)', '3522125', "{{lowercase}}\n'''<code>locate</code>''' is a [[Unix]] utility which serves to find [[computer file|file]]s on [[filesystem]]s. It searches through a prebuilt [[database]] of files generated by the <code>updatedb</code> command or by a [[Daemon (computing)|daemon]] and compressed using [[incremental encoding]]. It operates significantly faster than <code>[[find]]</code>, but requires regular updating of the database. This sacrifices overall efficiency (because of the regular interrogation of filesystems even when no user needs information) and absolute accuracy (since the database does not update in [[Real-time computing|real time]]) for significant speed improvements, particularly on very large filesystems.\n\n<code>locate</code> was first created in 1982.<ref>{{cite magazine|last=Woods|first=James A.|date=1983-01-15|title=Finding Files Fast|url=https://archive.org/stream/login-feb83/login_feb83_issue#page/n9/mode/2up|magazine=[[;login:]]|volume=8|issue=1|pages=8–10|publisher=[[Usenix]]|access-date=2016-03-27}}</ref>  The BSD and [[GNU Findutils]] versions derive from the original implementation.<ref>{{cite web|url=https://www.gnu.org/software/findutils/manual/html_node/find_html/Introduction.html#Introduction|title=Finding Files|date=2012-11-17|website=[[GNU]]|publisher=[[Free Software Foundation]]|access-date=2016-03-27|quote=GNU locate and its associated utilities were originally written by James Woods, with enhancements by David MacKenzie.}}</ref>  Their primary database is world-readable, so the index is built as an unprivileged user.\n\n<code>mlocate</code> (Merging Locate) and the earlier <code>slocate</code> (Secure Locate) use a restricted-access database, only showing filenames accessible to the user.<ref>{{cite web|url=http://carolina.mff.cuni.cz/~trmac/blog/mlocate/|archive-url=https://web.archive.org/web/20060411074142/http://carolina.mff.cuni.cz/~trmac/blog/mlocate/|archive-date=2006-04-11|title=mlocate|date=2005|author=Miloslav Trmač|access-date=2016-03-27|quote=...faster and does not trash the system caches as much...attempts to be compatible to GNU locate, when it does not conflict with slocate compatibility.|dead-url=yes}}</ref><ref>{{cite web|url=http://www.geekreview.org/slocate/|archive-url=https://web.archive.org/web/20050507092723/http://www.geekreview.org/slocate/|archive-date=2005-05-07|title=Secure Locate|date=1999|author=Kevin Lindsay|access-date=2016-03-27|quote=...will also check file permissions and ownership so that users will not see files they do not have access to.|dead-url=yes}}</ref>\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [https://www.gnu.org/software/findutils/findutils.html GNU Findutils]\n* [https://fedorahosted.org/mlocate/ mlocate]\n* {{man|1|locate|FreeBSD}}\n* {{man|1|locate|OpenBSD}}\n\nVariants:\n* [http://rlocate.sourceforge.net/ rlocate] - Variant using kernel module and daemon for continuous updates.\n* [http://www.kde-apps.org/content/show.php/KwickFind+(Locate+GUI+Frontend)?content=54817 KwickFind] - KDE GUI frontend for locate\n* [http://www.locate32.net/ Locate32 for Windows] - GPL'ed graphical Windows variant\n\n{{unix commands}}\n\n[[Category:GNU Project software]]\n[[Category:Unix file system-related software]]\n[[Category:Information retrieval systems]]\n\n\n{{Unix-stub}}"]
['Contextual Query Language', '9672320', '\'\'\'Contextual Query Language\'\'\' (CQL), previously known as \'\'\'Common Query Language\'\'\',<ref>[http://www.loc.gov/standards/sru/cql/spec.html CQL: the Contextual Query Language: Specifications] SRU: Search/Retrieval via URL, Standards, Library of Congress</ref> is a [[formal language]] for representing queries to [[information retrieval]] systems such as [[search engine]]s, [[bibliography|bibliographic catalogs]] and [[museum]] collection information. Based on the [[semantics]] of [[Z39.50]], its design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex [[query language]]s. It is being developed and maintained by the Z39.50 Maintenance Agency, part of the [[Library of Congress]].\n\n== Examples of query syntax ==\n\nSimple queries:\n\n<blockquote><tt>dinosaur<br/>\n"complete dinosaur"<br/>\ntitle = "complete dinosaur"<br/>\ntitle exact "the complete dinosaur"</tt></blockquote>\n\nQueries using [[Boolean logic]]:\n\n<blockquote><tt>dinosaur or bird<br/>\nPalomar assignment and "ice age"<br/>\ndinosaur not reptile<br/>\ndinosaur and bird or dinobird<br/>\n(bird or dinosaur) and (feathers or scales)<br/>\n"feathered dinosaur" and (yixian or jehol)</tt></blockquote>\n\nQueries accessing [[index (publishing)|publication indexes]]:\n\n<blockquote><tt>publicationYear < 1980<br/>\nlengthOfFemur > 2.4<br/>\nbioMass >= 100</tt></blockquote>\n\nQueries based on the proximity of words to each other in a document:\n\n<blockquote><tt>ribs prox/distance<=5 chevrons<br/>\nribs prox/unit=sentence chevrons<br/>\nribs prox/distance>0/unit=paragraph chevrons</tt></blockquote>\n\nQueries across multiple [[Dimension (data warehouse)|dimensions]]:\n\n<blockquote><tt>date within "2002 2005"<br/>\ndateRange encloses 2003</tt></blockquote>\n\nQueries based on [[Relevance (information retrieval)|relevance]]:\n\n<blockquote><tt>subject any/relevant "fish frog"<br/>\nsubject any/rel.lr "fish frog"</tt></blockquote>\n\nThe latter example specifies using a specific [[algorithm]] for [[logistic regression]].<ref>[http://srw.cheshire3.org/contextSets/rel/ Relevance Ranking Context Set version 1.1]</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.loc.gov/standards/sru/cql/ CQL home page]\n* [http://www.loc.gov/z3950/agency/ Z39.50 Maintenance Agency]\n* [http://zing.z3950.org/cql/intro.html A Gentle Introduction to CQL]\n\n{{Query languages}}\n\n{{USGovernment|sourceURL=http://www.loc.gov/standards/sru/cql/}}\n{{LOC-stub}}\n\n[[Category:Information retrieval systems]]\n[[Category:Library science]]\n[[Category:Library of Congress]]\n[[Category:Query languages]]\n[[Category:Knowledge representation languages]]']
['Comparison of enterprise search software', '41829011', '{{Cleanup-list|date=June 2014}}\n\nThe following tables compare the major [[List of enterprise search vendors|enterprise search software]] vendors in their classes.\n\n== General information ==\n\n{| class="wikitable sortable"\n|-\n! Product !! formerly k.a. !! Vendor !! [[Software release life cycle|Stable release]] !! Update!! Platforms !! API !! Target Customer !! Software License !! Open source !! Multilingual !! Website\n|-\n|Mindbreeze InSpire\n|Mindbreeze InSpire\n|Mindbreeze \n|2016 Summer Release\n|October 17th, 2016\n|Windows Server Linux\n|REST, SOAP, .NET, Java, Push API\n|Information Insight, Knowledge management for departments/organizations, Big Data Search & Analytics\n|?\n|No\n|Yes (including languages like CJK)\n|www.mindbreeze.com \n|-\n| [[Lookeen Desktop Search#Lookeen Server|Lookeen Server]]<ref>[http://www.lookeen-server.com/produkt/overview Lookeen Server Enterprise Search]</ref> || Lookeen Server || [[Axonic Informationssysteme GmbH]] || 1.3.1.1118 || April 2014 || Windows Server || [[.NET Framework|.NET]] || ? || ? || {{no}} || {{yes}} || http://www.lookeen-server.com\n|-\n| intergator || intergator Enterprise Search || [[intergator|interface projects GmbH]] || 5.3 || March 2016 || Windows Server, Linux Server || Java, Groovy und XML/JSON || Enterprise Search,Knowledge-Management, Content Analytics, Big Data || {{Yes}} || {{No}} || {{Yes}} || http://www.intergator.de \n|-\n| Coveo Enterprise Search || Coveo Platform || [[Coveo|Coveo Solutions Inc]] || 7.0 || February 6, 2014 || Windows || [[REST]], [[SOAP]], [[.NET Framework|.NET]] || Web customer service, customer interaction hubs || ? || {{no}} || {{yes|Yes, multilingual user interfaces}} || http://www.coveo.com/en/advanced-enterprise-search\n|-\n| 3RDi Search || 3RDi Search || The Digital Group Inc || 1.0 || NA || Generic || Supported || Enterprise Search, Knowledge Management, Big Data, BI, Analytics || Commercial || {{no}} || {{Yes}} || http://www.3rdisearch.com\n|-\n| Endeca Guided Search || ? || [[Oracle Corporation|Oracle]] || 6.2.2 || March 2012 || ? || ? || ? || ? || ? || ? || http://www.oracle.com/us/products/applications/commerce/endeca/endeca-guided-search/overview/index.html\n|-\n| EXALEAD CloudView || ? || [[Dassault Systèmes]] || R2014 || July 2013 || Windows Server, Linux Server || Push API (PAPI) || ? || ? || {{no}} || {{yes|Yes, <br />125 languages supported}} || http://www.3ds.com/products-services/exalead/\n|-\n|Datafari\n|\n|France Labs\n|2.2.1\n|April 2016\n|Linux Server, Windows (for test)\n|REST\n|Enterprise Search, Knowledge management, Big Data, BI, Analytics\n|No\n|Yes\n|Yes\n|<nowiki>http://www.datafari.com/en</nowiki>\n|-\n| FAST for SharePoint 2010 (F4SP) || [[Fast Search & Transfer]] || [[Microsoft]] || 2010 (SP1) || ? || Windows Server, SharePoint || [[.NET Framework|.NET]] || ? || ? || {{no}} || ? || ?\n|-\n| SharePoint 2013 || [[Fast Search & Transfer]] || [[Microsoft]] || 2010 (SP1) || ? || Windows Server, SharePoint || [[.NET Framework|.NET]] || ? || ? || {{no}} || ? || ?\n|-\n| [[Google Search Appliance]] (GSA) || ? || [[Google]] || 7.4 || March 2015 || ? || [[.NET Framework|.NET]], [[Java]] || ? || ? || ? || {{yes}} || https://www.google.com/enterprise/search/products/gsa.html\n|-\n| HP IDOL<ref>[http://www8.hp.com/us/en/software-solutions/information-data-analytics-idol/ HP IDOL]</ref> || . || [[Hewlett Packard Enterprise|HPE BigData]] || 11 || March 2016 || Windows, HP-UX, Linux, Solaris || [[SOAP]], [[REST]] || big data and analytics, Enterprise search, Video, Image or Audio analytics, Knowledge-Management, Info-Governance || Yes, licensed by volume. Starting on 250Gb of Metadata || No || {{yes}} || http://www8.hp.com/us/en/software-solutions/information-data-analytics-idol/\n|-\n| Haven Search OnDemand<ref>[http://search.havenondemand.com/ Haven Search OnDemand]</ref> || . || [[Hewlett Packard Enterprise|HPE BigData]] || 20160329|| March 2016 || SAAS Offering || [[REST]] || ? || Consumption-based pricing. || No || {{yes}} || http://search.havenondemand.com\n|-\n|-\n| [[Funnelback]] Search || Panoptic Search || [[Funnelback]] || 15.6 || June 2016 || SaaS Offering, Windows server, Linux server || [[REST]] || Enterprise search, website search, vertical search || Document- and server-based licensing || {{no}} || {{yes|Yes.  Multi-lingual support includes indexing, querying and localised UIs}} || https://www.funnelback.com\n|-\n| IBM Infosphere Data Explorer || [[Vivisimo|Vivisimo Velocity]] || [[IBM]] || ? || ? || ? || ? || big data and analytics projects || ? || ? || {{yes}} || http://www-01.ibm.com/software/data/information-optimization/\n|-\n| [[Swiftype]] Search || [[Swiftype]] || [[Swiftype]] || ? || September 2014 || Windows and Linux, MacOS || [[REST]] APIs || websites and mobile applications || ? No{{No}} || ? || https://swiftype.com\n|-\n| Lucidworks Fusion || N/A || [[Lucidworks|Lucidworks Inc.]] || 2.1.4 || March 2016 || Windows, Linux, MacOS || [[REST]] APIs || Enterprise search, online retail, search-based data analytics || Licensed by CPU cores || {{No}} ||{{yes}}|| https://lucidworks.com/products/fusion/\n\n|-\n| Perceptive Search/ISYS || Enterprise Server || [[Lexmark|Lexmark Perceptive Software]] || 4.2 || ? || Windows, Linux, Solaris, Mac OS, HP-UX and AIX || [[REST]], [[SOAP]] || ? || ? || ? || {{yes}} || http://www.perceptivesoftware.com/products/perceptive-search\n|-\n| Secure Enterprise Search (SES) || ? || [[Oracle Corporation|Oracle]] || 11.2.2.2 || January 2014 || Windows, Linux (Oracle, RedHat and SUSE, 32-bit and 64 bit), Solaris || ? || ? || ? || ? || ? || http://www.oracle.com/technetwork/search/oses/documentation/ses-096384.html\n|-\n| RAVN Connect ||   || RAVN Systems || 3.3 || February 2014 || Windows Server, Linux Server || REST API || ? || ? || {{no}} || {{yes}} || http://www.ravn.co.uk/capabilities/enterprise-search/\n|}\n\n== Features ==\n\n=== Content Collection & Indexing ===\n\nThis section compares the ability of the products to collect and index content, both textual and non textual, from different data source types and document types (formats).\n\n==== Indexing and connectivity ====\n\nThis is about indexing pipeline tools and processes; included connectors, support for connectors, etc.\n\n===== Web-based =====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server<ref>[http://www.lookeen-server.com/produkt/overview Lookeen Server Enterprise Search]</ref>!! HP IDOL<ref>[http://www.ndm.net/archiving/HP-Autonomy/information-connectivity Autonomy Information Connectivity]</ref>!! Coveo for Advanced Enterprise Search<ref>[http://www.coveo.com/en/platform-features-connect#connectorsSection Coveo connectors]</ref>!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Swiftype Search !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search<ref>[http://www.oracle.com/technetwork/search/oses/overview/ses11222ds-1969734.pdf SES 11.2.2.2 Datasheet - Oracle]</ref>!! RAVN Connect !! intergator !! Funnelback Search\n|-\n| [[HTTP]] || For crawling of Web servers. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || ? || ? || {{yes}} || {{yes}} ||  {{yes}}\n|-\n| [[HTTPS]] || For crawling of secured Web servers. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || {{yes}} || {{yes}} || {{yes}} || {{yes}} || {{yes}}  \n|-\n| [[XML]] || For indexing any XML-compliant data source. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}} \n|-\n|}\n\n===== File-based =====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2 !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search\n|-\n| [[NetWare File System|Netware file systems]]  || For incremental indexing of Netware file systems. || ? || {{yes}} || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{no}}\n|-\n| [[Samba]]/[[Unix File System|Unix file systems]] ||  || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Windows File System|Windows file systems]], Windows NT Filesystems (NTFS) ||  || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n|}\n\n===== Archiving/Directory =====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/what_connectors_are_available_with_ces.htm What Connectors Are Available With Coveo]</ref>!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search\n|-\n| LDAP || For indexing a company directory stored on a LDAP (v2 or v3) server. || ? ||  {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || {{yes}} || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}\n|-\n| [[Microsoft Active Directory]] || Supports Microsoft Active Directory. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}\n|-\n| [[Symantec Enterprise Vault]] || . || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{no}}\n|-\n|}\n\n===== Messaging =====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server || HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator\n|-\n| Akonix || . || ? || {{yes}} || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?\n|-\n| [[Facetime]] || . || ? || {{yes}}|| ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?\n|-\n| [[IBM Lotus Connections]] || . || ? || ? || {{yes}} || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}\n|-\n| [[IMAP]] || For indexing e-mail messages and attached files stored on an IMAP server. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}\n|-\n| [[IBM Lotus Notes]] || For indexing e-mail messages and attached files stored on a Lotus notes server. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CLN)}} \n|?|| ? || ? || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}\n|-\n| [[Microsoft Exchange Server|Microsoft Exchange]] || Ability to retrieve and index e-mail messages and attached files <br /> (Mailboxes, Public Folders). || {{yes}} || {{yes}} || {{yes|Yes, <br /> Exchange 2003/2007/2010/2013 Servers}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CXG)}} \n|Yes|| ? || ? || {{yes|Yes <br />(via Adhere Solutions partner)}} || ? || ? || ? || {{yes|Yes, <br /> Exchange 2003 Servers}} || {{yes}}|| {{yes}}\n|-\n| [[Microsoft Exchange Server|Microsoft Exchange Online]] || . || {{yes}} || ? || {{yes}} || ? || {{yes}} || . \n|Yes (IMAP)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}\n|-\n| [[NNTP]] || For real-time indexing of Usenet news groups. || ? ||  ? || ? || ? || ? || {{yes}} \n|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}\n|-\n| [[Gmail]] || . || ? || ? || {{yes}} || ? || {{yes}} || . \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}\n|-\n|}\n\n===== [[Content management system|CMS]], [[Document management system|DMS]] & Social =====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL<ref>[http://www8.hp.com/us/en/software-solutions/asset/software-asset-viewer.html?asset=1997065&module=1970565&docname=4AA5-8715ENW&page=1970341 KeyView IDOL - Product Brief]</ref>!! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/what_connectors_are_available_with_ces.htm What connectors are available with Coveo]</ref>!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator\n|-\n| [[Alfresco (software)|Alfresco]] || . || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, but no native support}} \n|Yes|| ? || ? || {{yes|Yes (via Incentro partner)}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}\n|-\n| [[EMC Documentum|EMC Documentum Server]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CDO)}} \n|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes}}  || {{yes}}|| {{yes}}\n|-\n| [[EPiServer|EpiServer]] || . || ? || {{yes}} || {{yes}} || ? || ? || {{no}} \n|?|| ? || ? || ? || ? || {{no}} || ? || ?  || ?|| {{yes}}\n|-\n| IBM Content Manager || . || ? || {{yes}} || ? || ? || {{yes}} || ? \n|?|| ? || ? || {{yes|Yes <br />(via Adhere Solutions partner)}} || ? || ? || ? || ?  || {{yes}}|| {{yes}}\n|-\n| IBM FileNet || . || ? || {{yes}} || ? || ? || {{yes}} || ? \n|Yes (P8)|| ? || ? || {{yes|Yes <br />(via Adhere Solutions partner)}} || ? || ? || ? || ?  || {{yes}}|| {{yes}}\n|-\n| [[WebSphere|IBM WebSphere]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, Informatica PowerCenter 9.x connectivity}} \n|?|| ? || ? || {{yes|Yes (via Adhere Solutions partner)}} || ? || ? || ? || ? || {{yes}}|| {{yes}}\n|-\n| Jalios || . || ? || ? || ? || ? || {{yes}} || {{yes|Yes <br /> (trigram:CJA)}} \n|?|| ? || ? || ? || ? || ? || ? || ?  || ?|| ?\n|-\n| [[SharePoint]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CXG)}} \n|Yes|| ? || ? || {{yes|Microsoft SharePoint Portal Server <br /> Microsoft SharePoint Services}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}\n|-\n| SharePoint Online || . || ? || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}\n|-\n| [[Dropbox (service)|Dropbox]] || . || ? || ? || {{yes}} || ? || ? || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}\n|-\n| Windows File Share || . || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}\n|-\n| [[Google Docs]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}}\n|-\n| Jive || . || ? || ? || {{yes}} || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| {{yes}}\n|-\n| [[Plumtree Software|Plumtree]] || . || ? || ? || {{yes}} || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || {{no}} || ? || ? || ?|| {{yes}}\n|-\n| Lithium || . || ? || ? || {{yes}} || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ?\n|-\n| [[Confluence]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || {{yes|Yes (via partner)}} || ? || {{yes}} || ? || ?  || ? || {{yes}} \n|-\n| [[Twitter]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} \n|-\n| [[Facebook]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? \n|No|| ? || ? || ? || ? || ? || ? || ?  || {{yes}}|| ? \n|-\n| [[LinkedIn]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ? \n|-\n|}\n\n===== Databases =====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://www.arnoldit.com/search-wizards-speak/coveo.html Coveo Solutions Inc., An Interview with Laurent Simoneau]</ref>!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search\n|-\n| [[IBM DB2]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter <br /> DB2 for Linux}} \n|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[JDBC]] ||  || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> translates [[SQL database]] fields <br /> into XML documents <br /> than are then indexed together <br />  with the document metadata}} \n|Yes|| ? || ? || Yes || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Microsoft SQL Server]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter }} \n|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[MySQL]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[ODBC]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter }} \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Oracle RDBMS]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter }} \n|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{?}}\n|-\n| [[Sybase]] || . || ? || {{yes}} || ? || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter }} \n|?|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n|}\n\n===== [[Customer relationship management|CRM]], [[Enterprise resource planning|ERP]], [[Product lifecycle management|PLM]], [[Business intelligence|BI]] =====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance<ref>[https://www.google.com/enterprise/marketplace/viewListing?productListingId=3905631+7212827882376498737 IBM DB2 Content Manager OD Connector for Google Search Appliance, by Adhere Solutions]</ref>!! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator\n|-\n| [[Salesforce.com|Salesforce]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CSF)}} \n|Yes (via partner)|| ? || ? || {{yes|Yes (via partner)}} || ? || {{yes}} || ? || ? || {{yes}} || ?\n|-\n| [[SAP Business Suite|SAP]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} \n|-\n| [[Siebel Systems|Siebel]]/Oracle || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, ETL:Informatica }} \n|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} \n|-\n| [[Microsoft Dynamics]] || . || ? || ? || {{yes}} || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} \n|-\n| [[Business Objects|Business Object]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} \n|-\n| [[IBM]] [[Cognos]] || . || ? || {{yes}} || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} \n|-\n| [[Informatica|Informatica PowerCenter]] || . || ? || ? || ? || ? || ? || {{yes|Yes <br /> (trigram:CJA)}} \n|?|| ? || ? || ? || ? || ? || ? || ? || ? || ? \n|-\n| [[MicroStrategy]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || ? || ? || {{yes|Yes (via partners)}} || ? || ? \n|-\n| [[Windchill (software)|PTC Windchill]] || . || ? || {{yes}} || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || ? ||{{yes}}\n|-\n| [[ENOVIA MatrixOne|ENOVIA]] || Support for MatrixOne/Enovia data. || ? || {{yes}} || ? || ? || ? || {{yes|Yes <br /> (trigram:CEN)}} \n|?|| ? || ? || {{yes|Yes (via partner)}} || ? || ? || ? || ? || ?|| {{yes}}\n|-\n|}\n\n===== Miscellaneous =====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator\n|-\n| [[Jira (software)|Jira]] || . || ? || ? || {{yes}} || ? || ? || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| {{yes}}\n|-\n| [[GitHub]] || . || ? || ? || ? || ? || ? || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| ?\n|-\n| [[Slack (software)|Slack]] || . || ? || ? || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| ?\n|-\n| [[Mantis Bug Tracker|Mantis]] || . || ? || ? || {{yes}} || ? || ? || ? \n|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}\n|-\n| [[JD Edwards]]/Oracle || EnterpriseOne, World. || ? || {{yes}} || ? || ? || ? || {{yes|Yes <br />ETL:Informatica PowerCenter }} \n|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?\n|-\n|-\n| [[PeopleSoft]]/Oracle || . || ? || {{yes}} || ? || ? || ? || {{yes|Yes <br />ETL:Informatica PowerCenter}} \n|?|| ? || ? || ? || ? || ? || ? || {{coming soon}} || ?|| ?\n|-\n|}\n\n==== Supported Formats ====\n\n{| class="wikitable sortable"\n|-\n! File type !! Description !! Lookeen Server !! HP IDOL  !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/CES/7.0/User/Supported_File_Formats.htm Coveo Platform 7, Supported File Formats]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView<ref>[http://3ds.exalead.com/software/common/pdfs/products/cloudview/Exalead-Connectors-and-Formats.pdf Exalead Cloudview Connectors + Formats]</ref>\n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search\n|-\n| Adobe [[PDF]] || Includes Adobe Acrobat or other PDF documents. || {{yes}} || {{yes}} || {{yes|Yes, <br /> Version 1.0 to 1.7}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Web pages || [[HTML]], [[XHTML]], etc. || {{yes}} || {{yes|Yes, <br /> versions v.3, 4, 5}} || {{yes|.asp, .aspx, .cgi, .col, <br /> .dochtml, .dothtml, .fphtml, <br /> .hta, .htm, .html, .jsp, .php, <br /> .pothtml, .ppthtml, .shtm, <br /> .shtml, .xlshtml}} || ? || {{yes}} || {{yes|Yes, <br /> versions v.4.01 and above, and [[XHTML]]}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[XML]] || Extensible Markup Language (.xml) || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> any [[Document type definition|DTD]]}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Text || Raw text. || {{yes}} || {{yes}} || {{yes|.ascx, .bat, .cmd, .config, <br /> .csv, .dic, .exc, .inf, .ini, <br /> .js, .jsl, .log, .nfo, .scp, <br /> .sdl, .sln, .txt, .vbdproj, <br /> .vbs, .vdp, .vdproj, .vjp, <br /> .vjsproj, .vjsprojdata, <br /> .wsdl, .wsf, .wtx, .xsd <br /> ANSI, ASCII, Unicode}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Microsoft Excel]] || Microsoft Excel Charts (.xls), <br /> Microsoft Excel XML (.xlsx, .xltm, .xltx) <br /> Others (.xlam, .xlb, .xlm, .xlsm) || {{yes}} || {{yes}} || {{yes|<br /> Version 5.0, 95(7.0), 97, 2000, XP, 2003, 2007, 2010. <br /> Indexes Excel 2010 attachments.}} || {{yes}} || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Microsoft Word]] || Microsoft Word (.doc) <br /> Microsoft Word XML (.docx, .dotx, .dotm) || {{yes}} || {{yes|Yes <br /> for Mac, Windows (multiple versions)}} || {{yes|Version 6.0, 6.0 (for MAC), 95 (7.0), 97, 98 (for MAC), 2000, <br /> XP, 2003, 2007, 2010 <br /> Indexes Word 2010 attachments.}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Microsoft PowerPoint]] || .pot, .potm, .potx, .ppam,  <br /> .pps, .ppsm, .ppsx, .ppt,  <br /> .pptm, .pptx || {{yes}} || {{yes}} || {{yes|Yes, <br /> Indexes PowerPoint 2010 attachments.}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Microsoft Access]] || MDB || ? || {{yes}} || ? || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Microsoft Project]] || MPP || ? || {{yes}} || ? || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{no}}\n|-\n| [[Microsoft Visio]] || VSD || ? || {{yes|Yes <br /> (multiple versions)}} || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| [[Microsoft Outlook]] || Message, archives, and templates (.msg, .oft, .pst) || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| [[MIME|MIME documents]] || Multipurpose Internet Mail Extension. || {{yes}} || {{yes|Yes, <br /> Microsoft Outlook Express Mac and PC (multiple versions) (.eml)}} || {{yes|.email, .eml, .ews, .mime <br /> MIME converter available with CES 7.0.5935+}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| [[StarOffice|Sun StarOffice]] || . || ? || ? || ? || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || ? || ? || {{yes}} || ?|| {{yes}} || {{no}}\n|-\n| Lotus 1-2-3 || . || ? || ? || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| Lotus Freehand || . || ? || ? || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| [[Corel WordPerfect]] || Corel WordPerfect Linux (.wps) <br /> Corel WordPerfect Macintosh (.wps) <br /> Corel WordPerfect Windows (.wo) <br /> Corel WordPerfect Windows (.wpd) || ? || {{yes}} || ? || ? || ? || {{yes|Yes, <br /> version 6 and 7}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| Archive files || . || ? || {{yes|7Z, DMG, HQX, BIZIP2, GZ, ISO, JAR, <br /> EMX, BIN, BKF, CAB, LZH/LHA <br /> ZIP, RAR, RTFD, TAR, Z, UUE <br /> multiple versions.}} || ? || ? || {{yes}} || {{yes|RAR, ZIP}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| RTF || Rich Text Format || {{yes}} || {{yes|Yes, <br />multiple versions.}} || ? || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Image files <br /> (text extraction) ||  || {{yes}} || ? || {{yes|Yes, <br /> .bmp, .jpeg, .max, .pcx/.dcx, <br /> .pdf, .png, .tiff, .tiff-fx <br /> requires the Optical Character Recognition (OCR) module.}} || ? || ? || ? \n|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ?  || {{yes}}|| {{yes}} || {{no}}\n|-\n| Images <br /> (metadata extraction) || Creation of thumbnail. || {{yes}} || {{yes}} || {{yes|.bmp, .emf, .exif, .gif, <br /> .icon, .jpeg, .png, .tiff, <br /> .wmf}} || ? || {{yes}} || {{yes|Yes, <br /> JPEG, PNG, GIF, PNG}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Audio <br /> (text extraction) ||  || ? || ? || {{yes|.aif, .aifc, .aiff, .asf, .au, <br /> .cda .mid, .midi, .mp1, .mp3, <br /> .mpga, .rmi, .snd, .wav , .wma <br /> Requires the Coveo Audio Video Search (CAVS) module.}} || ? || ? || ? \n|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| Audio (metadata) || Creation of thumbnail. || ? || ? || ? || ? || {{yes}} || {{yes|Yes, <br /> MP3, OGG.}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Video ||  || ? || {{yes|.264, video/3gpp, .3mm, .4mp ... <br /> .avi, .m1v, .mov, .mp2, ...}} || {{yes|.avi, .m1v, .mov, .mp2, .mp2v, <br /> .mpa, .mpeg, .mpg, .mpv2 .qt, <br /> .rec, .rm, .rnx, .wm, .wmv <br /> Requires the Coveo Audio Video Search (CAVS) module.}} || ? || ? || ? \n|Yes (metadata)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Metadata extraction.}}\n|-\n| MacroMedia Flash || . || ? || ? || ? || ? || {{yes|Yes, <br /> MacroMedia Flash text section and hypertext links}} || ? \n|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}\n|-\n| Autovue 2D & 3D CAD files || . || ? || ? || ? || ?  || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || {{yes|Yes, <br /> Open CAD files directly inside}} Autovue || ?|| {{yes}} || {{no}}\n|-\n| AutoCAD Drawing || AutoCAD Drawing (DWF), AutoCAD Drawing Exchange (DXF) || ? || {{yes|Yes, <br /> (multiple versions)}} || ? || ? || ? || {{yes}} \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| [[CATIA]] versions 4 and 5 || Drawing document (CATDrawing), Part document (CATPart), <br />  Assembly document (CATProduct), Model (V4 only) || ? || {{yes}} || ? || ? || ? || {{yes|Yes, <br /> any 5 version}} \n|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}\n|-\n| [[SolidWorks]] v1 || Drawing (.slddrw), Assembly (.sldasm), <br /> Part (.sldprt) || ? ||{{yes}} || ? || ? || ? || {{yes|Yes, <br /> 2003 to 2013 releases}} \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| [[Pro/ENGINEER]] || Part (.prt), Assembly (.asm) || ? || ? || ? || ? || ? || {{yes|Yes, <br /> any release from R16 to 2001, from WF1 to WF5}} \n|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}\n|-\n| CAD Open formats || . || ? || ? || ? || ? || ? || {{yes|Yes, <br /> [[IGES]] version 5.2 and 5.3, [[STEP-File]]}} \n|Yes (metadata from the DWG CAD format)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Metadata from DWG CAD}}\n|-\n|}\n\n==== Text Analytics ====\n\n{| class="wikitable sortable"\n|-\n! Linguistics !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance<ref>[http://www.bainsight.com/resources/Google-vs-Microsoft-FAST-Search-Whitepaper.pdf Microsoft and Google - BA Insight]</ref>!! IBM Infosphere Data Explorer !! Lucidworks Fusion!! Perceptive Search !! Secure Enterprise Search<ref>[http://docs.oracle.com/cd/E14507_01/admin.1112/e14130.pdf Oracle Secure Enterprise Search Administrator\'s Guide]</ref>!! RAVN Connect !! intergator !! Funnelback Search\n|-\n| Language detection || Ability to identify the languages at indexing time. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Synonyms/stemming  || Ability to treat as synonyms variations of keywords. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| {{yes}} || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Entity extraction|Named Entity Extraction]] || Ability to automatically extract entities such as persons, locations and organizations from indexed content. || ? || {{yes|Yes  <br /> named "Eduction"}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} || ? || {{yes|Yes <br/>With automatic entity linking}}|| {{yes}} || {{yes}}\n|-\n| Stop words || Ability to exclude stop words (e.g. \'an\', \'the\') in order to improve relevance. || {{yes}} || ? || {{yes}} || {{yes}} || {{yes}} || ? \n|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n|}\n\n==== Audio & Video Analytics ====\n\n{| class="wikitable sortable"\n|-\n! Multimedia !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator\n|-\n| Audio analytics || Ability to understand topics being discussed, genders and emotional tones of speech, music, etc. || ? || ? || ? || ? || ? || ? \n|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes|Yes <br/> speech, topics, music, keyword spotting }}|| ? \n|-\n| Video analytics || Ability to understand the content of the video without relying on metadata (e.g. key framing, facial identification, logo recognition, etc.) || ? || {{yes}} || ? || ? || ? || ? \n|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| ? \n|-\n| Image analytics || Ability to detect patterns in image (e.g. faces, bodies, gender, age range, expression, etc.) || ? || {{yes}} || ? || ? || ? || ? \n|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}\n|-\n|}\n\n=== Search Experience ===\n\nThis section compares the ability of the products to :\n* Enable the user to enter and execute the query\n* Present the data to the user within seconds after the query is parsed and processed so that the user can find what he seeks quickly and act on it.\n\n==== Search Language ====\n\n{| class="wikitable sortable"\n|-\n! Query Parser !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/User/search_prefix_and_operators.htm Search Prefixes and Operators]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect<ref>[http://www.ravn.co.uk/wp-content/uploads/2014/02/CORE-Whitepaper-W.V.pdf RAVN Connect, CORE White paper]</ref>!! intergator !! Funnelback Search <ref>https://docs.funnelback.com/query_language_help.html</ref>\n|-\n| Wildcard search || Does the system use the asterisk ("*") and question mark ("?") character as a wildcard? || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Fuzzy search || Does the system offer phonetic and approximate spelling search? (distinctions of syntax and semantics) || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Exact phrase search || Does the system enable to find words as a phrase? || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| [[Proximity search (text)|Proximity search]] || Support for advanced proximity operators - NEAR, BEFORE, AFTER. || ? || ? || {{yes}} || ? || ? || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Range search || Ability to match all terms which are lexically between square brackets ("[]") and curly braces ("{}"). || ? || ? || ? || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Boosting a term || Automatic bigram and trigram relevancy boosting. || ? || ? || {{yes}} || ? || ? || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Boolean search || Does the system interprets Boolean operators? || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Graph search || Does the system keep relationships between fields and allows searching for them (while enabling full-text search)? || ? || ? || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n|}\n\n==== Usability ====\n\n===== Search Query =====\n\nThis is the process of searching (querying).\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://www.coveo.com/en/news-releases/Coveo-Reports-Accelerated-Demand-for-Search-and-Relevance-Technology Coveo Reports Accelerated Demand for Search & Relevance Technology in 2013]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search\n|-\n| Auto-complete || Does the system provide an automatic query guidance in the search box while typing? || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}<ref>https://docs.funnelback.com/auto_completion_collection_cfg.html</ref>\n|-\n| Spell-checking || Does the system checks if the words in the query are spelled correctly and suggest corrections? || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}}|| {{yes}} <ref>https://docs.funnelback.com/spelling_suggestions.html</ref>\n|-\n| [[Federated search]] || The ability to send the same query simultaneously to several searchable sources. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? \n|Yes|| {{yes}} || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} || {{yes}}|| {{yes}}|| {{yes}}<ref>https://docs.funnelback.com/ui_modern_extra_searches_collection_cfg.html</ref>\n|-\n| Advanced search page || Does the system allow users to perform complex and sophisticated queries? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}|| {{yes}}\n|-\n|}\n\n===== Result-list =====\n\nThis is the process of scanning the content of any document directly from the result lists.\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL<ref>[http://www.ndm.net/archiving/pdf/20130902_PI_B_HP_AUTN_IDOL10_web.pdf Autonomy KeyView IDOL - Product Brief - Ndm.net]</ref>!! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance<ref>[http://static.googleusercontent.com/media/www.google.com/fr//support/enterprise/static/gsa/docs/admin/70/gsa_doc_set/quick_start/quick_start.pdf Getting the Most from Your Google Search Appliance]</ref>!! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator\n|-\n| Relevance ranking || Ability to find the highest quality and most relevant documents and bring them to the top of a search results list. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| {{yes}} || {{yes}} || {{yes|Yes <br /> Google Site Search factors in more than 100 variables <br /> for each query}} || ? || {{yes}} || {{yes}} || ? || {{yes}}|| {{yes}}\n|-\n| Find similar || Ability to find similar links. || {{yes}} || ? || ? || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}}\n|-\n| Hit highlighting || Ability to highlight query key terms within the document in search result. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| {{no}} || {{no}} || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}\n|-\n| Summarization <br /> (view as HTML) || Does the system offer content preview in the search result, so that users can judge relevance of results? || {{yes}} || {{yes}} || ? || ? || {{yes}} || {{yes}} \n|Yes|| {{partial|*only available for Office file types (Powerpoint, Excel, Visio, etc.)}} || {{partial}} || {{yes|Yes <br /> converts over 220 file formats into HTML}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}\n|-\n|  || Does the system enable to copy/paste from within the preview? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| {{no}} || {{no}} || {{yes}} || ? || ? || ? || ? || {{yes}}|| {{yes}}\n|-\n| Thumbnails and preview || Ability to generate thumbnails for a large amount of different file types. || ? || {{yes|Yes <br /> 100+ doc types}} || {{yes}} || ? || {{yes}} || {{yes|Yes, but low resolution for CATProducts <br /> no thumbnails for Pro/E assemblies}} \n|Yes|| {{partial|*only available for Office file types (.[[DOCX|DOCx]], .[[PPTX|PPTx]]) <br /> First page thumbnail preview}} || {{partial}} || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}\n|-\n| Full document graphical preview || Ability to access the content of any document without having to open a windows client application. || ? || ? || ? || ? || {{yes}} || {{yes|Yes <br /> regardless of the file\'s original application}} \n|Yes|| {{partial|*only available for PowerPoint file types}} || {{yes}} || ? || ? || ? || ? || ? || {{yes|Yes <br/> Asynchronous loading}}|| {{yes}}\n|-\n| Document comparison || Ability to compare. || ? ||{{yes|Yes <br /> for version management, signature identification, among other features}} || ? || ? || {{yes}} || {{yes}} \n|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}\n|-\n| Clustering || Ability to dynamically organize search results into groups. || ? || {{yes}} || ? || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || {{yes|Yes <br /> group search results by topic}} || ? || ? || ? || ? || {{yes}}|| {{yes}}\n|-\n| Sort by fields || Ability to sort all results by order of date or other attribute. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes|Yes, <br /> „hard sort‟}} || {{yes}}|| {{yes}}\n|-\n|}\n\n==== Faceted Navigation ====\n\nThis is the process of browsing the content by narrowing search results quickly in clicking filters that refine results based on related categories, so that users extract more meaning and insight from the content.\n\n{| class="wikitable sortable"\n|-\n! [[Faceted Search|Facets]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/user/about_facets.htm Coveo Facets]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator\n|-\n| Multiple filters || Does the system enable the user to filter results in selecting multiple facet values? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || {{yes}} || {{yes}} || ? || {{yes}}|| {{yes}}\n|-\n| Facet values and counts || Ability to display the term and the number of documents containing that term in the search results. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}\n|-\n| [[Faceted classification|Hierarchical]]- and range facets || . || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}\n|-\n| Date, number and string types || Ability to filter by date/time, number and string data types. || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}\n|-\n|}\n\n==== Social and collaborative ====\n\nThis is the process of asking social network.\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010<ref>[http://www.hcsolutions.at/produkte/ontolica/Documents/Ontolica_2010_feature_matrix.pdf Search Solution for Microsoft SharePoint]</ref>!! SharePoint 2013<ref>[http://www.slideshare.net/SurfRay/new-sharepoint-server-2013-search-features Introduction to SharePoint 2013 Search]</ref>!! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search<ref>[http://www.perceptivesoftware.com/images/psi_ds_perceptiveenterprisesearch.pdf Perceptive Enterprise Search - Product Datasheet]</ref>!! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search\n|-\n| Search result tagging || Ability to improve relevancy by creating or adding existing tags. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|No|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes|Yes<br/>Real time}}|| {{yes}} || {{no|No. Deprecated in v14.0.1}}\n|-\n| Tag searching || Ability to search for a tag. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Personalization/ Audience targeting || Ability to deliver more accurate targeted results. || ? || {{yes| Yes, <br /> browse histories, content contributors, and interactions, etc}}  || {{yes}} || ? || {{yes}} || ? \n|Yes|| {{yes}} || ? || {{yes| Yes, <br /> source, date, metadata and entities biaising }} || ? || ? || {{yes}} || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Expertise location || Ability to find experts in users organization by searching on related keywords. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> with [[Software development kit|SDK]]}} \n|Yes|| ? || ? || {{yes}} || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Saved search || Ability to save searches. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> with [[Software development kit|SDK]]}} \n|No|| {{no}} || {{no}} || ? || {{yes}} || ? || {{yes}} || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Saved alerts || Ability to save alerts in order to notified when new content matching your queries has been added to the system. || ? || ? || ? || ? || {{yes}} || ? \n|Yes|| ? || {{no}} || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| Saved RSS feeds || Ability to save RSS feeds. || ? || ? || {{yes}} || ? || {{yes}} || ? \n|No|| ? || {{no}} || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n|}\n\n==== Mobile support ====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari\n!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search\n|-\n| Mobile search || Does the system support mobile device access and search? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}\n|-\n| Mobile UI || Does the system detect the user device (desktop, smartphone, tablet, etc.) and adapt itself based on it?  || ? || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Admin UI and Default Public UI use responsive designs.}}\n|-\n| Geolocation || Does the system enable from the end-user’s geolocation to provide additional context to filter? || ? || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ? || {{yes}}\n|-\n| Compatibility || Is the system compatible with iOS. || ? || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{yes}}\n|-\n|  || Is the system compatible with Android? || ? || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{yes}}\n|-\n|  || Is the system compatible with Windows Phone? || ? || ? || {{yes}} || ? || {{yes}} || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || ?\n|-\n|}\n\n=== Administration & Architecture ===\n\nThis section compares the flexibility in the underlying architecture, application development, the scalability and the administrative services of the products.\n\n==== Management & Search analytics ====\n\nThis table is about the ability to report on usage and activity (most popular queries, documents not found, etc.)\n\n{| class="wikitable sortable"\n|-\n! [[Search analytics]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search<ref>[http://docs.oracle.com/cd/E35643_01/Workbench.211/pdf/WorkbenchUserGuide.pdf Endeca® Workbench: User\'s Guide - Oracle Documentation]</ref>!! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search\n|-\n| Web based administration interfaces <br /> (HTML) || . || ? || ? || {{yes}} || {{yes}} || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Search statistics || Does the system collect search statistics? || ? || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Search reports || Does the system enable to report search statistics? || ? || ? || {{yes}} || {{yes}} || {{yes}} || ? \n|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes|Yes, <br /> most popular queries}} || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Portal usage reports || Does the system enable to report on portal usage? || ? || ? || ? || {{yes|Yes, <br /> popular navigation}} || {{yes}} || {{yes|Yes <br /> with [[Software development kit|SDK]]}} \n|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| Document storage reports || . || ? || ? || {{yes}} || ? || {{yes}} || ? \n|Yes|| {{no}} || ? || ? || ? || ? || ? || {{yes|Yes, <br /> for documents not found}} || ?|| {{yes}} || {{yes}}\n|-\n| Custom reports || . || ? || ? || ? || ? || {{yes}} || {{yes|Yes <br /> with [[Software development kit|SDK]]}} \n|Yes|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n| Click Scoring || Ability to improve relevancy by enabling to track which results are most often clicked. || ? || ? || ? || ? || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}\n|-\n|}\n\n==== Interface flexibility ====\n\nThis is about tools to customize the interface,so that it adds value to any industry or business process.\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator !! Funnelback Search\n|-\n| Standard based open interface || Does the system support all client platforms? || ? || {{yes|Yes, <br /> HTTP and XML/JSON}} || ? || {{yes}} || {{yes}} || ? \n|Yes|| ? || ? || ? || ? || {{yes|Yes, <br /> XML, JSON and HTTP}} || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes.  HTML, JSON, XML, RSS and OpenSearch.}}\n|-\n| Page Layout Helper || Ability to change easily to global attributes (logo, fonts, header, and footer) and to the look of the Search Box and Search Results. || ? || ? || ? || ? || {{yes}} || ? \n|Yes|| {{no|No, action menu on search results not configurable}} || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}\n|-\n| Stylesheet editor || Ability to make more extensive changes using [[XSLT]] stylesheet. || ? || ? || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> Full customization with CSS or Java API & XML}}  || {{yes}}\n|Yes|| {{yes}} || ? || {{yes|Yes <br />XSLT stylesheet editor}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes|Yes. Full results customisation via [[FreeMarker]]<ref>https://docs.funnelback.com/freemarker.html</ref> and CSS/JS. Cached copies of XML templated via XSLT<ref>https://docs.funnelback.com/xslt_processing.html</ref>}}\n|-\n|}\n\n==== Scalability ====\n\n{| class="wikitable sortable"\n|-\n!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/coveo_scalability_model.htm Scalability]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator\n|-\n| Index capacity || How many total documents can be indexed into the system? || up to 10 Million || ? || ? || ? || ? || ? \n|?|| Up to 100 Million || Up to 100 Million || ? || ? || ? || ? || ? || Scalable to billions|| Scalable to billions\n|-\n| Indexing rate || How rapidly documents can be added or reprocessed into the index? || real time || ? || ? || ? || ? || ? \n|Depends on setup and hardware|| ? || ? || ? || ? || ? || ? || ? || per requirement, scalable || ?\n|-\n| Query-processing speed || How many queries per second ([[Queries per second|QPS]]) the engine can process? || ? || 2,000 <br /> across all indexed data with sub-second response times || ? || ? || ? || ? \n|Depends on setup and hardware|| ? || ? || ? || ? || ? || ? || scalable || || Depands on the datasource. (sub)-second times \n|-\n|}\n\n==== Platform readiness ====\n\n{| class="wikitable sortable"\n|-\n! [[Search analytics]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator\n|-\n| General system requirement || Minimum of available disk space. || 100 MB || ? || ? || ? || ? || 25 GB \n|?|| ? || ? || ? || ? || ? || ? || ? || 10 GB || 25 GB\n|-\n|  || Minimum RAM. || 8GB || ? || ? || ? || ? || 8 GB \n|?|| ? || ? || ? || ? || ? || ? || ? || 8 GB || 4 GB\n|-\n|  || Hard drives to store the data files. || ? || ? || ? || ? || ? || SCSI, SAS, SAN, over FC or SSD disks <br /> (as opposed to SATA disks) \n|?|| ? || ? || ? || ? || ? || ? || ? || optimised for throughput<br/> SCSI, SAS, SAN || SCSI, SAS, SAN, SSD, HDD\n|-\n|}\n\n=== Vendor Intangibles ===\n\nThis section compares each software investment.\n\n{| class="wikitable sortable"\n|-\n! Investment !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView \n!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect || intergator\n|-\n| Hardware costs || Total costs of servers. || 0 || ? || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || ? || Depands on the base licence\n|-\n| Installation costs || . || ? ||? || ? || ? || ? || ? \n|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?\n|-\n| License costs || . || ? ||? || ? || ? || ? || ? \n|0|| ? || ? || 500,000 Documents – $28,387 <br /> 1 million documents – $66,236 <br /> Upgrade from 1 million to 2 million documents – $1,971 <br /> 2 Million documents – $123,010 <br /> 2 Million documents – $123,010 <br /> 3 Million documents – $113, 548 <br /> 3 Million documents – $158,967 <br /> 5 Million documents – $433,766 <br /> 10 Million documents – $305,066 <br /> 10 Million – $423,913 <br /> 15 Million documents – $533,896 <br /> 15 Million documents with hot backup – $615,053 <br /> 30 Million documents with hot backup – $993,548 || ? || ? || ? || ? || ?|| ?\n|-\n| Annual Maintenance || Annual Maintenance fees. || ? || ? || ? || ? || ? || ? \n|Per server|| ? || ? || ? || ? || ? || ? || ?|| ?|| ?\n|-\n|}\n\n== References ==\n\n{{reflist}}\n\n== First version ==\n\n[[Category:Information retrieval systems]]']
['Trip (search engine)', '14069461', '{{Infobox software\n|name = Trip\n|logo =\n|screenshot =\n|caption =\n|developer = Trip Database Ltd\n|latest_release_version =\n|latest_release_date =\n|latest_preview_version =\n|latest_preview_date =\n|operating_system =\n|genre = [[Search engine]]\n|language =\n|license = [[Freeware]]\n|website = [http://www.tripdatabase.com Trip]\n}}\n\n\'\'\'Trip\'\'\' is a free [[Illness|clinical]] [[search engine]].  Its primary function is to help [[clinician]]s identify the best available evidence with which to answer clinical questions.  Its roots are firmly in the world of [[evidence-based medicine]].\n\n==History==\nThe site was created in 1997 as a search tool to help the staff of ATTRACT<ref>[http://www.attract.wales.nhs.uk ATTRACT]</ref> answer clinical questions for [[General practitioner|GP]]s in [[Gwent (county)|Gwent]], [[South Wales]].  Shortly afterwards \'\'[[Bandolier (journal)|Bandolier]]\'\' highlighted the Trip Database and this helped establish the site.  In 2003, after a period of steady growth, Trip became a subscription-only service.  This was abandoned In September 2006 and since then the growth in usage has been significant. Originally "Trip" stood for Turning Research Into Practice, but the system is now simply called Trip.<ref>{{cite web |url= http://www.tripdatabase.com/about |title=About |work=Trip |publisher=Trip Database Ltd |accessdate=3 April 2013}}</ref>\n\n==Process==\nThe core to Trip’s system is the identification and incorporation of new evidence.  The people behind Trip are heavily involved in clinical question answering systems (e.g., [[NLH Q&A Service]]).  Therefore, if resources are identified that are useful in the Q&A process they tend to be added to Trip.\n\n==Users==\nA site survey (September 2007) showed that the site was searched over 500,000 times per month, with 69% from [[health professional]]s and 31% from members of the public. Of the health professionals around 43% are doctors.  Most users come from either the United Kingdom or the United States.  In September 2008 the site was searched 1.4 million times. To date the site has been searched over 100 millions times.\n\n==Recent updates==\nAt the end of 2012 Trip had a major upgrade which saw significant new enhancements:\n\n* New content - widening the coverage\n* New design\n* Advanced search\n* PICO search - to help users formulate focused searches\n* Improved filtering\n* Search history/timeline - recording all a user activity on the site\n* Related articles\n\n==Education tracker==\nTrip has an education tracker which allows users to record their activity on Trip which can then be used, subject to local regulations, for revalidation/re-licensing.\n\n==Future areas of work==\nTrip is exploring numerous innovative technologies to improve the site, these include:\n* Link out to full-text articles via Trip.\n* RCT database.\n* Rapid (within a week) systematic review quality reviews.\n* Learning from users prior use of the site and that of similar users to improve search results.\n\n==Trip Answers==\nIn November 2008, Trip released a new website, Trip Answers.  This is a repository of clinical Q&As from a variety of Q&A services. At launch it had over 5,000 Q&As and currently has over 6,300.  This content has been integrated into Trip.\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.tripdatabase.com Trip]\n* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1852632/ Using the Turning Research Into Practice (TRIP) database: how do clinicians really search?] an evaluation of the website.\n* [http://libguides.lhl.uab.edu/content.php?pid=108596&sid=1099056 Reviews: From Systematic to Narrative] review of the site\n* [http://guides.library.manoa.hawaii.edu/content.php?pid=250484&sid=2157277 Evidence Based Pyramid] a pictorial representation of TRIP\'s approach to the evidence\n\n[[Category:Medical websites]]\n[[Category:Information retrieval systems]]']
['Dynatext', '14460441', '{{primary sources|date=October 2011}}\n\'\'\'DynaText\'\'\' is an [[SGML]] publishing tool. It was introduced in 1990, and was the first system to handle arbitrarily large SGML documents, and to render them according to multiple style-sheets that could be switched at will.\n\nDynaText and its Web sibling DynaWeb won multiple [[Seybold]] and other awards [http://xml.coverpages.org/ebt-award.html][http://xml.coverpages.org/dynaweb3-dvi.html], and there are eleven US Patents related to the DynaText technology: 5,557,722; 5,644,776; 5,708,806; 5,893,109; 5,983,248; 6,055,544; 6,101,511; 6,101,512; 6,105,044; 6,167,409; and 6,546,406.\n\nDynaText was developed by Electronic Book Technologies, Incorporated, of [[Providence, Rhode Island]]. EBT was founded by [[Louis Reynolds]], [[Steven DeRose]], [[Jeffrey Vogel]], and [[Andries van Dam]], and was sold to [[Inso]] corporation in 1996, when it had about 150 employees.\n\nDynaText heavily influenced stylesheet technologies such as [[DSSSL]] and [[CSS]], and [[XML]] chairman [[Jon Bosak]] cites EBT chief architect [[Steven DeRose]] as one of the originators of the notion of [[well-formed document|well-formedness]] formalized in [[XML]], as well as DynaText for influencing the design of Web browsers in general [http://www.ibiblio.org/bosak/cv.htm].\n\n[[Inso]] corporation went out of business in 2002.\n\n==Technology==\n\nDynaText accepted [[SGML]] as input, and built a binary representation of the structure (similar to [[Document Object Model|DOM]] for [[XML]], but persistent), as well as a full-text [[inverted index]] of the text, elements, and attributes. Customers typically distributed such compiled e-books on CD-ROM or via network servers. Later versions of DynaText could also read SGML on the fly, providing exactly the same interface.\n\nUnlike many prior systems, DynaText was not limited to any particular [[Document type definition|DTD]] (or [[XML schema|schema]]). Rather, customers could build style sheets in a simple language (also SGML-based), using properties very much like the later [[DSSSL]], [[CSS]], and [[XSL-FO]]. However, every property could have an expression as its value, which would be evaluated (if necessary) for each element the style applied to. Graphics, tables, formulae, and plug-ins could be included in documents.\n\nUnlike nearly all prior SGML systems, DynaText was not limited to documents that could fit in [[RAM]] on the viewing or serving computer system. Users commonly created documents in the tens to hundreds of MB. DynaText customers included aerospace, workstation and other computer industry firms, government, literary and technical publishers, and others.\n\nFull-text searches were based on an inverted index of words and other tokens (except for Japanese text, which was handled specially). Dynatext could report the number of "hits" for a given search, that occur within each section in the table of contents (by default, the table of contents appeared in a separate pane as an expandable outline, and clicking on any entry scrolled the full-text pane to the start of the corresponding section). Searches could also restrict hits to particular SGML element types, or sequences of types; refer to attributes; and use Boolean operators and parentheses. The "and" operator restricted its operands to occurring near each other, by default in the same paragraph or comparable element.\n\n==References==\n*[http://www.w3.org/History/19921103-hypertext/hypertext/Products/DynaText/Overview.html DynaText Notes] by [[Tim Berners-Lee]] (this note refers to a pre-release or very early release of DynaText).\n*{{cite journal\n | id = MS\n | last = Smith\n | first = MacKenzie\n | title = Review: DynaText: An Electronic Publishing System\n | journal = Computers and the Humanities\n | volume = 27\n | issue = 5/6\n | pages = 415–420\n | publisher = Springer\n | location =\n | date = 1993\n | jstor = http://www.jstor.org/stable/30204569\n | issn = 0010-4817\n }}\n\n*{{cite book\n | url = http://techpubs.sgi.com/library/dynaweb_docs/0630/SGI_EndUser/books/IIDWeb_UG/sgi_html/ch05.html\n | title = IRIS InSight™ DynaWeb™ User\'s Guide: Chapter 5. Introduction to the DynaText Search Language\n | publisher = Silicon Graphics, Inc.\n}} Document Number: 007-3229-001\n\n*{{cite journal\n | url = http://www.w3.org/Conferences/WWW4/ora_951122/112.html\n | title = DynaWeb: Interfacing Large SGML Repositories and the WWW\n | journal = Fourth International World Wide Web Conference: ``The Web Revolution\'\'\n | date = 1995\n | location = Boston\n | first = Gavin Thomas \n | last = Nicol\n}}\n\n[[Category:Information retrieval systems]]']
['Relevance feedback', '5818361', '\'\'\'Relevance [[feedback]]\'\'\' is a feature of some [[information retrieval]] systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.\n\n== Explicit feedback ==\n\nExplicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as [[Relevance (information retrieval)|relevance]] judgments.\n\nUsers may indicate relevance explicitly using a \'\'binary\'\' or \'\'graded\'\' relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", "somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the [[SearchWiki]] feature implemented by [[Google]] on their search website.\n\nThe relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known [[Rocchio Classification#Algorithm|Rocchio Algorithm]].\n\nA performance [[Metric (mathematics)|metric]] which became popular around 2005 to measure the usefulness of a ranking [[algorithm]] based on the explicit relevance feedback is [[NDCG]]. Other measures include [[Precision (information retrieval)|precision]] at \'\'k\'\' and [[Mean average precision#Mean average precision|mean average precision]].\n\n== Implicit feedback ==\n\nImplicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [http://www.scils.rutgers.edu/etc/mongrel/kelly-belkin-SIGIR2001.pdf]. There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response<ref>Jansen, B. J. and McNeese, M. D. 2005. [https://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_assistance_jasist2005.pdf Evaluating the effectiveness of and patterns of interactions with automated assistance in IR systems]. Journal of the American Society for Information Science and Technology. 56(14), 1480-1503</ref><ref>Kelly, Diane, and Jaime Teevan. "Implicit feedback for inferring user preference: a bibliography." ACM SIGIR Forum. Vol. 37. No. 2. ACM, 2003.</ref>\n\nThe key differences of implicit relevance feedback from that of explicit include [http://haystack.lcs.mit.edu/papers/kelly.sigirforum03.pdf]:\n\n# the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and\n# the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback\n\nAn example of this is [[dwell time (information retrieval)|dwell time]], which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.\nAnother example of this is the [[Surf Canyon]] [[browser extension]], which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.\n\n== Blind feedback ==\n\nPseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top "k" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:\n\n# Take the results returned by initial query as relevant results (only top k with k being between 10 and 50 in most experiments).\n# Select top 20-30 (indicative number) terms from these documents using for instance [[tf-idf]] weights.\n# Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.\n\nSome experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments.\n\nThis automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.<ref>Jinxi Xu and W. Bruce Croft, [http://portal.acm.org/citation.cfm?id=243202\'\'Query expansion using local and global document analysis\'\'], in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.</ref> Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task {{Citation needed|date=March 2011}}. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.<ref>Yuanhua Lv and ChengXiang Zhai, [http://portal.acm.org/citation.cfm?id=1835546\'\'Positional relevance model for pseudo-relevance feedback\'\'], in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.</ref> \nSpecifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.\n\nBlind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.\n\n== Using relevance information ==\n\nRelevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the [[Rocchio Classification#Algorithm|Rocchio Algorithm]].\n\n==Further reading==\n*[http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt Relevance feedback lecture notes] - Jimmy Lin\'s lecture notes, adapted from Doug Oard\'s\n*[http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html] - chapter from \'\'Modern Information Retrieval\'\'\n*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\n\n== References ==\n{{reflist|2}}\n\n[[Category:Internet search algorithms]]\n[[Category:Information retrieval evaluation]]\n[[zh:相关反馈]]']
['Discounted cumulative gain', '19542049', '\'\'\'Discounted cumulative gain\'\'\' (\'\'\'DCG\'\'\') is a measure of ranking quality. In [[information retrieval]], it is often used to measure effectiveness of [[World Wide Web|web]] [[search engine]] [[algorithm]]s or related applications. Using a [[Relevance (information retrieval)|graded relevance]] scale of documents in a search engine result set, DCG measures the usefulness, or \'\'gain\'\', of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom with the gain of each result discounted at lower ranks.<ref>Kalervo Jarvelin, Jaana Kekalainen: Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4), 422–446 (2002)</ref>\n\n== Overview ==\n\nTwo assumptions are made in using DCG and its related measures.\n\n# Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)\n# Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents.\n\nDCG originates from an earlier, more primitive, measure called Cumulative Gain.\n\n=== Cumulative Gain ===\n\nCumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position <math>p</math> is defined as:\n\n:<math> \\mathrm{CG_{p}} = \\sum_{i=1}^{p} rel_{i} </math>\n\nWhere <math>rel_{i}</math> is the graded relevance of the result at position <math>i</math>.\n\nThe value computed with the CG function is unaffected by changes in the ordering of search results. That is, moving a highly relevant document <math>d_{i}</math> above a higher ranked, less relevant, document <math>d_{j}</math> does not change the computed value for CG. Based on the two assumptions made above about the usefulness of search results, DCG is used in place of CG for a more accurate measure.\n\n=== Discounted Cumulative Gain ===\n\nThe premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position <math>p</math> is defined as:<ref name="stanfordireval">{{cite web|title=Introduction to Information Retrieval - Evaluation|url=http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf|publisher=Stanford University|accessdate=23 March 2014|date=21 April 2013}}</ref>\n\n:<math> \\mathrm{DCG_{p}} = \\sum_{i=1}^{p} \\frac{rel_{i}}{\\log_{2}(i+1)} = rel_1 + \\sum_{i=2}^{p} \\frac{rel_{i}}{\\log_{2}(i+1)} </math>\n\nPreviously there has not been  any theoretically sound justification for using a [[logarithm]]ic reduction factor<ref name=CMS2009>{{cite book | title=Search Engines: Information Retrieval in Practice |author1=B. Croft |author2=D. Metzler |author3=T. Strohman |year=2010 | publisher=\'\'Addison Wesley"}}</ref> other than the fact that it produces a smooth reduction. But Wang et al. (2013)<ref>Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, Tie-Yan Liu. 2013. A Theoretical Analysis of Normalized Discounted Cumulative Gain (NDCG) Ranking Measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).</ref> give theoretical guarantee for using the logarithmic reduction factor in NDCG. The authors show that for every pair of substantially different ranking functions, the NDCG can decide which one is better in a consistent manner.\n\nAn alternative formulation of DCG<ref>Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning (ICML \'05). ACM, New York, NY, USA, 89-96. DOI=10.1145/1102351.1102363 http://doi.acm.org/10.1145/1102351.1102363</ref> places stronger emphasis on retrieving relevant documents:\n\n:<math> \\mathrm{DCG_{p}} = \\sum_{i=1}^{p} \\frac{ 2^{rel_{i}} - 1 }{ \\log_{2}(i+1)} </math>\n\nThe latter formula is commonly used in industry including major web search companies<ref name="stanfordireval"/> and data science competition platform such as Kaggle.<ref>{{cite web|title=Normalized Discounted Cumulative Gain|url=https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain|accessdate=23 March 2014}}</ref>\n\nThese two formulations of DCG are the same when the relevance values of documents are [[binary function|binary]];<ref name=CMS2009/>{{rp|320}} <math>rel_{i} \\in \\{0,1\\}</math>.\n\nNote that Croft et al. (2010) and Burges et al. (2005) present the second DCG with a log of base e, while both versions of DCG above use a log of base 2.  When computing NDCG with the second formulation of DCG, the base of the log does not matter, but the base of the log does affect the value of NDCG for the first formulation.  Clearly, the base of the log affects the value of DCG in both formulations.<!-- Not very clear, does it affect or no the value of DCG? -->\n\n=== Normalized DCG ===\n\nSearch result lists vary in length depending on the [[Web search query|query]]. Comparing a search engine\'s performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of <math>p</math> should be normalized across queries. This is done by sorting all \'\'\'relevant\'\'\' documents in the corpus by their relative relevance, producing the maximum possible DCG through position <math>p</math>, also called Ideal DCG (IDCG) through that position. For a query, the \'\'normalized discounted cumulative gain\'\', or nDCG, is computed as:\n\n:<math> \\mathrm{nDCG_{p}} = \\frac{DCG_{p}}{IDCG_{p}} </math>,\n\nwhere:\n\n:<math> \\mathrm{IDCG_{p}} = \\sum_{i=1}^{|REL|} \\frac{ 2^{rel_{i}} - 1 }{ \\log_{2}(i+1)} </math>\n\nand |REL| represents the list of relevant documents (ordered by their relevance) in the corpus up to position p.\n\nThe nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engine\'s ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.\n\nThe main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial [[relevance feedback]] is available.\n\n== Example ==\n\nPresented with a list of documents in response to a search query, an experiment participant is asked to judge the relevance of each document to the query. Each document is to be judged on a scale of 0-3 with 0 meaning not relevant, 3 meaning highly relevant, and 1 and 2 meaning "somewhere in between". For the documents ordered by the ranking algorithm as\n\n:<math> D_{1}, D_{2}, D_{3}, D_{4}, D_{5}, D_{6} </math>\n\nthe user provides the following relevance scores:\n\n:<math> 3, 2, 3, 0, 1, 2 </math>\n\nThat is: document 1 has a relevance of 3, document 2 has a relevance of 2, etc. The Cumulative Gain of this search result listing is:\n\n:<math> \\mathrm{CG_{6}} = \\sum_{i=1}^{6} rel_{i} = 3 + 2 + 3 + 0 + 1 + 2 = 11</math>\n\nChanging the order of any two documents does not affect the CG measure. If <math>D_3</math> and <math>D_4</math> are switched, the CG remains the same, 11. DCG is used to emphasize highly relevant documents appearing early in the result list. Using the logarithmic scale for reduction, the DCG for each result in order is:\n\n\n{| class="wikitable" border="1"\n|-\n! <math>i</math>\n! <math>rel_{i}</math>\n! <math>\\log_{2}(i+1)</math>\n! <math> \\frac{rel_{i}}{\\log_{2}(i+1)} </math>\n|-\n| 1\n| 3\n| 1\n| 3\n|-\n| 2\n| 2\n| 1.585\n| 1.262\n|-\n| 3\n| 3\n| 2\n| 1.5\n|-\n| 4\n| 0\n| 2.322\n| 0\n|-\n| 5\n| 1\n| 2.585\n| 0.387\n|-\n| 6\n| 2\n| 2.807\n| 0.712\n|}\n\nSo the <math>DCG_{6}</math> of this ranking is:\n\n:<math> \\mathrm{DCG_{6}} = \\sum_{i=1}^{6} \\frac{rel_{i}}{\\log_{2}(i+1)} = 3 + 1.262 + 1.5 + 0 + 0.387 + 0.712 = 6.861</math>\n\nNow a switch of <math>D_3</math> and <math>D_4</math> results in a reduced DCG because a less relevant document is placed higher in the ranking; that is, a more relevant document is discounted more by being placed in a lower rank.\n\nThe performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized.\n\nTo normalize DCG values, an ideal ordering for the given query is needed. For this example, that ordering would be the [[Monotonic|monotonically decreasing]] sort of the relevance judgments provided by the experiment participant, which is:\n\n:<math> 3, 3, 2, 2, 1, 0 </math>\n\nThe DCG of this ideal ordering, or \'\'IDCG (Ideal DCG)\'\' , is then:\n\n:<math> \\mathrm{IDCG_{6}} = 7.141 </math>\n\nAnd so the nDCG for this query is given as:\n\n:<math> \\mathrm{nDCG_{6}} = \\frac{DCG_{6}}{IDCG_{6}} = \\frac{6.861}{7.141} = 0.961 </math>\n\n== Limitations ==\n# Normalized DCG metric does not penalize for bad documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,0 </math> respectively, both would be considered equally good even if the latter contains a bad result. One way to take into account this limitation is to use <math>1 - 2^{rel_{i}}</math> in the numerator for scores for which we want to penalize and <math>2^{rel_{i}} - 1</math> for all others. For example, for the ranking judgments <math>Excellent, Fair, Bad</math> one might use numerical scores <math>1,0,-1</math> instead of <math>2,1,0</math>.\n# Normalized DCG does not penalize for missing documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,1,1 </math> respectively, both would be considered equally good. One way to take into account this limitation is to enforce fixed set size for the result set and use minimum scores for the missing documents. In previous example, we would use the scores <math> 1,1,1,0,0 </math> and <math> 1,1,1,1,1 </math> and quote nDCG as nDCG@5.\n# Normalized DCG may not be suitable to measure performance of queries that may typically often have several equally good results. This is especially true when this metric is limited to only first few results as it is done in practice. For example, for queries such as "restaurants" nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive.\n\n== References ==\n{{Reflist|1}}\n\n[[Category:Information retrieval evaluation]]']
['Cranfield experiments', '20289869', "The '''Cranfield experiments''' were computer information retrieval experiments conducted by [[Cyril W. Cleverdon]] at [[Cranfield University]] in the 1960s, to evaluate the efficiency of indexing systems.<ref>Cleverdon, C. W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.</ref><ref>Cleverdon, C. W. (1967). The Cranfield tests on index language devices. Aslib Proceedings, 19(6), 173-194.</ref><ref>Cleverdon, C. W., & Keen, E. M. (1966). Factors determining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results. Cranfield, UK: Aslib Cranfield Research Project. \n</ref>\n\nThey represent the prototypical evaluation model of [[information retrieval]] systems, and this model has been used in large-scale information retrieval evaluation efforts such as the [[Text Retrieval Conference]] (TREC).\n\n==See also==\n*[[ASLIB]]\n*[[Information history]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://ir.dcs.gla.ac.uk/resources/test_collections/cran/ Cranfield 1400 corpus]\n\n[[Category:Experiments]]\n[[Category:Information retrieval evaluation]]\n\n\n{{database-stub}}"]
['Controlled vocabulary', '1850719', '{{refimprove|date=June 2012}}\n\n\'\'\'Controlled vocabularies\'\'\' provide a way to organize knowledge for subsequent retrieval.  They are used in [[subject indexing]] schemes, [[subject heading]]s, [[thesauri]],<ref>[https://web.archive.org/web/20101204132228/http://www.imresources.fit.qut.edu.au:80/vocab/ Controlled Vocabularies]  Links to examples of thesauri and classification schemes.</ref><ref>[https://web.archive.org/web/20090314094707/http://www.fao.org/aims/kos_list_type.htm Controlled Vocabularies]  Links to examples of thesauri and classification schemes used in the domain of Agriculture, Fisheries, Forestry etc.</ref> [[Taxonomy (general)|taxonomies]] and other forms of [[knowledge organization system]]s. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designers of the schemes, in contrast to natural language vocabularies, which have no such restriction.\n\n== In library and information science ==\n\nIn [[library and information science]] controlled vocabulary is a carefully selected list of [[word (linguistics)|word]]s and [[phrase]]s, which are used to [[Tag (metadata)|tag]] units of information (document or work) so that they may be more easily retrieved by a search.<ref>Amy Warner, [http://www.ischool.utexas.edu/~i385e/readings/Warner-aTaxonomyPrimer.html A taxonomy primer].</ref><ref>Karl Fast, Fred Leise and Mike Steckel, [http://boxesandarrows.com/what-is-a-controlled-vocabulary/]</ref> Controlled vocabularies solve the problems of [[homographs]], [[synonyms]] and [[polyseme]]s by a [[bijection]] between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.\n\nFor example, in the [[Library of Congress Subject Headings]] (a subject heading system that uses a controlled vocabulary), authorized terms—subject headings in this case—have to be chosen to handle choices between variant spellings of the same word (American versus British), choice among scientific and popular terms (\'\'cockroach\'\' versus \'\'Periplaneta americana\'\'), and choices between synonyms (\'\'automobile\'\' versus \'\'car\'\'), among other difficult issues.\n\nChoices of authorized terms are based on the principles of \'\'user warrant\'\' (what terms users are likely to use), \'\'literary warrant\'\' (what terms are generally used in the literature and documents), and \'\'structural warrant\'\' (terms chosen by considering the structure, scope of the controlled vocabulary).\n\nControlled vocabularies also typically handle the problem of [[homographs]], with qualifiers. For example, the term \'\'pool\'\' has to be qualified to refer to either \'\'swimming pool\'\' or the game \'\'pool\'\' to ensure that each authorized term or heading refers to only one concept.\n\nThere are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.\n\nHistorically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.\n\nFor example, the [[Library of Congress Subject Heading]] itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term "[[Hypernym|Broader term]]" and "[[Hyponym|Narrow term]]".\n\nThe [[terminology|terms]] are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document\'s text. Well known subject heading systems include the [[Library of Congress Subject Headings|Library of Congress system]], [[Medical Subject Headings|MeSH]], and [[Sears Subject Headings|Sears]]. Well known thesauri include the [[Art and Architecture Thesaurus]] and the [[Education Resources Information Center|ERIC]] Thesaurus.\n\nChoosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.\n\nControlled vocabulary elements (terms/phrases) employed as [[Tag (metadata)|tags]], to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as [[metadata]].\n\n== Indexing languages ==\n\nThere are three main types of indexing languages.\n\n* Controlled indexing language – only approved terms can be used by the indexer to describe the document\n* [[Natural language]] indexing language – any term from the document in question can be used to describe the document\n* Free indexing language – any term (not only from the document) can be used to describe the document\n\nWhen indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example, using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each document.\n\nIn recent years [[free text search]] as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is \'\'indexed\'\'). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.\n\nControlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce [[Relevance (Information Retrieval)|irrelevant]] items in the retrieval list. These irrelevant items ([[false positives]]) are often caused by the inherent ambiguity of [[natural language]]. Take the English word [[football (word)|\'\'football\'\']] for example. \'\'Football\'\' is the name given to a number of different [[team sport]]s. Worldwide the most popular of these team sports is [[Football (soccer)|association football]], which also happens to be called \'\'[[soccer]]\'\' in several countries. The word \'\'football\'\' is also applied to [[rugby football]] ([[rugby union]] and [[rugby league]]), [[American football]], [[Australian rules football]], [[Gaelic football]], and [[Canadian football]]. A search for \'\'football\'\' therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by [[Tag (metadata)|tagging]] the documents in such a way that the ambiguities are eliminated.\n\nCompared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually [[relevance|relevant]] to the search topic).\n\nIn some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you don\'t need to worry about searching for other terms that might be synonyms of that term.\n\nHowever, a controlled vocabulary search may also lead to unsatisfactory [[Recall (information retrieval)|recall]], in that it will fail to retrieve some documents that are actually relevant to the search question.\n\nThis is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer.\n\nAnother possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example, an article might mention football as a secondary focus, and the indexer might decide not to tag it with "football" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.\n\nOn the other hand, free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision.\n\nControlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the author\'s own words.\n\nThe use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.\n\nNumerous methodologies have been developed to assist in the creation of controlled vocabularies, including [[faceted classification]], which enables a given data record or document to be described in multiple ways.\n\n==Applications==\nControlled vocabularies, such as the [[Library of Congress Subject Headings]],  are an essential component of [[bibliography]], the study and classification of books. They were initially developed in [[library and information science]]. In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the [[Medical Subject Headings]] (MeSH) developed by the [[United States National Library of Medicine|U.S. National Library of Medicine]]. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup [[X.25]] networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first [[full text]] databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.\n\nIn large organizations, controlled vocabularies may be introduced to improve [[technical communication]]. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in [[technical writing]] and [[knowledge management]], where effort is expended to use the same word throughout a [[document]] or [[organization]] instead of slightly different ones to refer to the same thing.\n\nWeb searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a [[Semantic Web]], in which the content of Web pages is described using a machine-readable [[metadata]] scheme. One of the first proposals for such a scheme is the [[Dublin Core]] Initiative. An example of a controlled vocabulary which is usable for [[Web indexing|indexing web pages]] is [[Polythematic Structured Subject Heading System|PSH]].\n\nIt is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web.<ref>Cory Doctorow, [http://www.well.com/~doctorow/metacrap.htm Metacrap].</ref> To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page\'s contents. The [[eXchangeable Faceted Metadata Language]] (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on [[faceted classification]] principles.<ref>Mark Pilgrim, [http://petervandijck.com/xfml/ eXchangeable Faceted Metadata Language].</ref>\n\nControlled vocabularies of the [[Semantic Web]] define the concepts and relationships (terms) used to describe a field of interest or area of concern. For instance, to declare a person in a machine-readable format, a vocabulary is needed that has the formal definition of “Person”, such as the Friend of a Friend ([[FOAF]]) vocabulary, which has a Person class that defines typical properties of a person including, but not limited to, name, honorific prefix, affiliation, email address, and homepage, or the Person vocabulary of [[Schema.org]].<ref>{{cite web |url=http://schema.org/Person |title=The Person vocabulary of Schema.org |accessdate=13 March 2015}}</ref> Similarly, a book can be described using the Book vocabulary of [[Schema.org]]<ref>{{cite web |url=http://schema.org/Book |title=The Book vocabulary of Schema.org |accessdate=13 March 2015}}</ref> and general publication terms from the [[Dublin Core]] vocabulary,<ref>{{cite web |url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |accessdate=13 March 2015}}</ref> an event with the Event vocabulary of [[Schema.org]],<ref>{{cite web |url=http://schema.org/Event |title=The Event vocabulary of Schema.org |accessdate=13 March 2015}}</ref> and so on.\n\nTo use machine-readable terms from any controlled vocabulary, web designers can choose from a variety of annotation formats, including RDFa, [[Microdata (HTML)|HTML5 Microdata]], or [[JSON-LD]] in the markup, or [[Resource Description Framework|RDF]] serializations (RDF/XML, Turtle, N3, TriG, TriX) in external files.\n\n==See also==\n*[[Authority control]]\n*[[Controlled natural language]]\n*[[IMS VDEX|IMS Vocabulary Definition Exchange]]\n*[[Named-entity recognition]]\n*[[Nomenclature]]\n*[[Ontology (computer science)]]\n*[[Terminology]]\n*[[Thesaurus]]\n*[[Universal Data Element Framework]]\n*[[Vocabulary-based transformation]]\n\n==References==\n{{Reflist|2}}\n\n==External links==\n* [http://www.controlledvocabulary.com/ controlledvocabulary.com] — explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases.\n* [http://www.photo-keywords.com/ photo-keywords.com/] — useful guides to creating and editing your own controlled vocabulary suitable for image cataloging.\n* [http://www.niso.org/standards/resources/Z39-19.html ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies]\n\n{{Lexicography}}\n\n[[Category:Information retrieval techniques]]\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]\n[[Category:Technical communication]]\n[[Category:Semantic Web]]\n[[Category:Ontology (information science)]]\n[[Category:Controlled vocabularies]]\n[[Category:Information science]]']
['Vocabulary mismatch', '36749242', '{{refimprove|date=June 2015}}\n\'\'\'Vocabulary mismatch\'\'\' is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.\n\nFurnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.<ref>Furnas, G., et al, The Vocabulary Problem in Human-System Communication, Communications of the ACM, 1987, 30(11), pp. 964-971.</ref>  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on [[latent semantic indexing]].\n\nThe vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in [[information retrieval]].  Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the [[Binary Independence Model]].  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.<ref name="cnf">Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>\n\n== Techniques that solve mismatch ==\n\n* [[Stemming]]\n* [[Full-text indexing]] instead of only indexing keywords or abstracts\n* Indexing text on inbound links from other documents (or other social tagging\n* [[Query expansion]].  A 2012 study by Zhao and Callan<ref name="cnf"/> using expert created manual [[Conjunctive normal form]] queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. [[Rocchio algorithm|Rocchio expansion]].\n* Translation-based models\n\n== References ==\n\n{{Reflist}}\n\n[[Category:Linguistic research]]\n[[Category:Information retrieval techniques]]\n[[Category:Natural language processing]]']
['Tag (metadata)', '1707086', '{{hatnote|Not to be confused with [[Markup language]] or [[HTML element]] tags.}}\n[[File:Web 2.0 Map.svg|thumb|right|250px|A [[tag cloud]] with terms related to [[Web 2.0]]]]\n\nIn [[information system]]s, a \'\'\'tag\'\'\' is a non-hierarchical [[index term|keyword or term]] assigned to a piece of information (such as an [[Bookmark (World Wide Web)|Internet bookmark]], digital image, or [[computer file]]). This kind of [[metadata]] helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item\'s creator or by its viewer, depending on the system.\n\nTagging was popularized by websites associated with [[Web 2.0]] and is an important feature of many Web 2.0 services. It is now also part of some desktop software.\n\n==History==\n\nLabeling and tagging are carried out to perform functions such as aiding in [[Classification (machine learning)|classification]], marking ownership, noting boundaries, and indicating [[online identity]]. They may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. In the organization of information and objects, the use of textual keywords as part of identification and classification long  predates computers. However, computer based searching made the use of keywords a rapid way of exploring records.\n\n[[File:A Description of the Equator and Some Otherlands, collaborative hypercinema portal Upload page.jpg|thumb|A Description of the Equator and Some Otherlands, collaborative hypercinema portal, produced by documenta X, 1997. User upload page associating user contributed media with the term \'\'Tag\'\'.]] Online and Internet databases and early websites deployed them as a way for publishers to help users find content. In 1997, the collaborative portal "A Description of the Equator and Some Other Lands" produced by [[documenta]] X, Germany, coined the folksonomic term \'\'Tag\'\' for its co-authors and guest authors on its Upload page. In "The Equator" the term \'\'Tag\'\' for user-input was described as an \'\'abstract literal or keyword\'\' to aid the user. Turned out in Web 1.0 days, all "Otherlands" users defined singular \'\'Tags\'\', and did not share \'\'Tags\'\' at that point.\n\nIn 2003, the [[social bookmarking]] website [[Delicious (website)|Delicious]] provided a way for its users to add "tags" to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag.<ref>[http://flickr.com/photos/joshu/765809051/in/set-72157600740166824/ Screenshot of tags on del.icio.us] in 2004 and [http://flickr.com/photos/joshu/765817375/in/set-72157600740166824/ Screenshot of a tag page on del.icio.us], also in 2004, both published by [[Joshua Schachter]] on July 9, 2007.</ref> [[Flickr]] allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable.<ref>[http://www.adaptivepath.com/ideas/essays/archives/000519.php "An Interview with Flickr\'s Eric Costello"] by Jesse James Garrett, published on August 4, 2005. Quote: "Tags were not in the initial version of Flickr. Stewart Butterfield...liked the way they worked on del.icio.us, the social bookmarking application. We added very simple tagging functionality, so you could tag your photos, and then look at all your photos with a particular tag, or any one person’s photos with a particular tag."</ref> The success of Flickr and the influence of Delicious popularized the concept,<ref>An example is [http://www.adammathes.com/academic/computer-mediated-communication/folksonomies.html "Folksonomies - Cooperative Classification and Communication Through Shared Metadata"] by Adam Mathes, December 2004. It focuses on tagging in Delicious and Flickr.</ref> and other [[social software]] websites&nbsp;– such as [[YouTube]], [[Technorati]], and [[Last.fm]]&nbsp;– also implemented tagging. Other traditional and web applications have incorporated the concept such as "Labels" in [[Gmail]] and the ability to add and edit tags in [[iTunes]] or [[Winamp]].\n\nTagging has gained wide popularity due to the growth of social networking, photography sharing and bookmarking sites. These sites allow users to create and manage labels (or “tags”) that categorize content using simple keywords. The use of keywords as part of an identification and classification system long predates computers. In the early days of the web keywords meta tags were used by web page designers to tell search engines what the web page was about. Today\'s tagging takes the meta keywords concept and re-uses it. The users add the tags. The tags are clearly visible, and are themselves links to other items that share that keyword tag.\n\nKnowledge tags are an extension of [[Index term|keyword]] tags. They were first used by [[Jumper 2.0]], an [[open source]] [[Web 2.0]] software platform released by Jumper Networks on 29 September 2008.<ref>{{Citation|url=http://www.jumpernetworks.com/ NEWS-Jumper_Networks_Releases_Jumper_2.0_Platform.pdf|title=Jumper Networks Press Release for Jumper 2.0|publisher=Jumper Networks, Inc.|date=29 September 2008}}</ref> Jumper 2.0 was the first [[collaborative search engine]] platform to use a method of expanded tagging for [[knowledge capture]].\n\nWebsites that include tags often display collections of tags as [[tag cloud]]s. A user\'s tags are useful both to them and to the larger community of the website\'s users.\n\nTags may be a "bottom-up" type of classification, compared to [[hierarchy|hierarchies]], which are "top-down". In a traditional hierarchical system ([[Taxonomy (general)|taxonomy]]), the designer sets out a limited number of terms to use for classification, and there is one correct way to classify each item. In a tagging system, there are an unlimited number of ways to classify an item, and there is no "wrong" choice. Instead of belonging to one category, an item may have several different tags. Some researchers and applications have experimented with combining structured hierarchy and "flat" tagging to aid in information retrieval.<ref>[http://infolab.stanford.edu/~heymann/taghierarchy.html Tag Hierarchies], research notes by Paul Heymann.</ref>\n\n==Examples==\n\n===Within a blog===\nMany [[blog]] systems allow authors to add free-form tags to a post, along with (or instead of) placing the post into categories. For example, a post may display that it has been tagged with \'\'baseball\'\' and \'\'tickets\'\'. Each of those tags is usually a [[web link]] leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.\n\n===For an event===\nAn official tag is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a [[controlled vocabulary]].\n\n===In research===\nA researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, free form [[categorization|classification]] allows the author to manage what would otherwise be unwieldy amounts of information. Commercial, as well as some free computer applications are readily available to do this.\n\n==Special types==\n\n===Triple tags===\n{{see also|Microformat}}\nA \'\'\'triple tag\'\'\' or \'\'\'machine tag\'\'\' uses a special [[syntax]] to define extra [[semantic]] information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a [[namespace]], a [[wikt:predicate|predicate]], and a value. For example, "<nowiki>geo:long=50.123456</nowiki>" is a tag for the geographical [[longitude]] coordinate whose value is 50.123456. This triple structure is similar to the [[Resource Description Framework]] model for information.\n\nThe triple tag format was first devised for geolicious<ref>{{cite web |url=http://brainoff.com/weblog/2004/11/05/124 |title=geo.lici.us: geotagging hosted services |first1=Mikel |last1=Maron |date=November 5, 2004}}</ref> in November 2004, to map [[Delicious (website)|Delicious]] bookmarks, and gained wider acceptance after its adoption by [http://stamen.com/projects/mappr Mappr] and GeoBloggers<ref>[http://web.archive.org/web/20071011024028/http://geobloggers.com/archives/2006/01/11/advanced-tagging-and-tripletags/ Advanced Tagging and TripleTags] by Reverend Dan Catt, \'\'Geobloggers\'\', January 11, 2006.</ref> to map [[Flickr]] photos. In January 2007, [[Aaron Straup Cope]] at [[Flickr]] introduced the term \'\'machine tag\'\' as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.<ref>[https://www.flickr.com/groups/api/discuss/72157594497877875/ Machine tags], a post by Aaron Straup Cope in the Flickr API group, January 24, 2007.</ref>\n\nSpecialized metadata for geographical identification is known as \'\'[[geotagging]]\'\'; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using [[binomial nomenclature]].<ref>[https://www.flickr.com/groups/encyclopedia_of_life/rules/ Encyclopedia of Life use of machine tag], The Encyclopedia of Life project rules including the required use of a taxonomy machine tag, September 19, 2009.</ref>\n\n===Hashtags===\n{{main|Hashtag}}\nA hashtag is a kind of metadata tag marked by the prefix <code>#</code>, sometimes known as a "hash" symbol. This form of tagging is used on [[microblogging]] and [[social networking service]]s such as [[Twitter]], [[Facebook]], [[Google+]], [[VK (social network)|VK]] and [[Instagram]].\n\n===Knowledge tags===\nA knowledge tag is a type of [[metadata|meta-information]] that describes or defines some aspect of an information resource (such as a [[document]], [[digital image]], [[database table|relational table]], or [[web page]]). Knowledge tags are more than traditional non-hierarchical [[index term|keywords or terms]]. They are a type of [[metadata]] that captures knowledge in the form of descriptions, categorizations, classifications, [[semantics]], comments, notes, annotations, [[hyperdata]], [[hyperlinks]], or references that are collected in tag profiles. These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository. Knowledge tags are a [[knowledge management]] discipline that leverages [[Enterprise 2.0]] methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. It generally allows greater flexibility than other [[knowledge management]] classification systems.\n\nCapturing knowledge in tags takes many different forms, there is factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies).<ref>\n{{Citation\n | last=Wiig | first=K. M.\n | year= 1997\n | title=Knowledge Management: An Introduction and Perspective\n | journal=Journal of Knowledge Management\n | volume=1 | issue=1\n | pages=6–14\n | url=http://www.mendeley.com/c/67997727/Wiig-1997-Knowledge-Management-An-Introduction-and-Perspective/\n | doi=10.1108/13673279710800682\n}}\n</ref> These forms of [[knowledge]] often exist outside the data itself and are derived from personal experience, insight, or expertise.\n\nKnowledge tags, in fact, manifest themselves in any number of ways – conceptual knowledge tags describe procedures, lessons learned, and facts that are related to the information resource. [[Tacit knowledge]] tags, manifests itself through skills, habits or learning by doing and represent experience or organizational intelligence. Anecdotal knowledge, is a memory of a particular case or event that may not surface without context.<ref>\n{{citation\n | last=Getting | first=Brian\n | year= 2007\n | title=What Are "Tags" And What Is "Tagging?\n | publisher=Practical eCommerce\n | url=http://www.practicalecommerce.com/articles/589\n}}\n</ref>\n\nKnowledge can best be defined as information possessed in the mind of an individual: it is personalized or subjective information related to facts, procedures, concepts, interpretations, ideas, observations and judgments (which may or may not be unique, useful, accurate, or structurable). Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information. Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turn-over, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.<ref>\n{{Citation\n | last=Alavi | first=Maryam\n | last2=Leidner\n | year= 1999\n | title=Knowledge Management Systems: Issues, Challenges, and Benefits\n | journal=Communications of the Association for Information Systems\n | volume=1 | issue=7\n | url=http://www.belkcollege.uncc.edu/jpfoley/Readings/artic07.pdf\n}}\n</ref>\n\n==Advantages and disadvantages==\n{{procon|date=November 2012}}\n\nIn a typical tagging system, there is no explicit information about the meaning or [[semantics]] of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them.<ref name="Smith2008">Smith, Gene (2008). Tagging: People-Powered Metadata for the Social Web. Berkeley, CA: New Riders. ISBN 0-321-52917-0</ref> The flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.\n\nWhen users can freely choose tags (creating a [[folksonomy]], as opposed to selecting terms from a [[controlled vocabulary]]), the resulting metadata can include [[homonym]]s (the same tags used with different meanings) and [[synonym]]s (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject.<ref>Golder, Scott A. Huberman, Bernardo A. (2005).\n"[http://arxiv.org/abs/cs.DL/0508082 The Structure of Collaborative Tagging Systems]." Information Dynamics Lab, HP Labs. Visited November 24, 2005.</ref> For example, the tag "orange" may refer to the [[Orange (fruit)|fruit]] or the [[Orange (colour)|color]], and items related to a version of the [[Linux kernel]] may be tagged "Linux", "kernel", "Penguin", "software", or a variety of other terms. Users can also choose tags that are different [[inflection]]s of words (such as singular and plural),<ref>[http://keithdevens.com/weblog/archive/2004/Dec/24/SvP.tags Singular vs. plural tags in a tag-based categorization system] by Keith Devens, December 24, 2004.</ref> which can contribute to navigation difficulties if the system does not include [[stemming]] of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of "tag terms" within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies collectively develop a partial set of tagging conventions.\n\n===Complex system dynamics===\n\nDespite the apparent lack of control, research has shown that a simple form of shared vocabularies emerges in social bookmarking systems. Collaborative tagging exhibits a form of [[complex system]]s dynamics,<ref name="WWW07-ref">Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proceedings of the 16th International Conference on the World Wide Web (WWW\'07), Banff, Canada, pp. 211-220, ACM Press, 2007. Downloadable on [http://www2007.org/papers/paper635.pdf the conference\'s website]</ref> (or [[Self-organization|self organizing]] dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags that describe different resources (e.g., websites) converges over time to stable [[power law]] distributions.<ref name="WWW07-ref"/> Once such stable distributions form, simple vocabularies can be extracted by examining the [[correlation]]s that form between different tags.  This informal collaborative system of tag creation and management has been called a [[folksonomy]].\n\n===Spamming===\n\nTagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a [[YouTube]] video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items.<ref>[http://heymann.stanford.edu/tagspam.html Tag Spam], research notes by Paul Heymann.</ref> The number of tags allowed may also be limited to reduce spam.\n\n==Syntax==\nSome tagging systems provide a single [[text box]] to enter tags, so to be able to [[tokenize]] the string, a [[Wiktionary:separator|separator]] must be used. Two popular separators are the [[Space (punctuation)|space character]] and the [[comma]]. To enable the use of separators in the tags, a system may allow for higher-level separators (such as [[quotation mark]]s) or [[escape character]]s. Systems can avoid the use of separators by allowing only one tag to be added to each input [[Web widget|widget]] at a time, although this makes adding multiple tags more time-consuming.\n\nA syntax for use within [[HTML]] is to use the \'\'\'rel-tag\'\'\' [[microformat]] which uses the [[Rel attribute|\'\'rel\'\' attribute]] with value "tag" (i.e., <code>rel="tag"</code>) to indicate that the linked-to page acts as a tag for the current context.<ref>[http://microformats.org/wiki/rel-tag rel tag microformat specification], Microformats Wiki, January 10, 2005.</ref>\n\n==See also==\n{{colbegin||27em}}\n* [[Collective intelligence]]\n* [[Concept map]]\n* [[Enterprise 2.0]]\n* [[Enterprise bookmarking]]\n* [[Explicit knowledge]]\n* [[Faceted classification]]\n* [[Folksonomy]]\n* [[Information ecology]]\n* [[Knowledge representation]]\n* [[Knowledge transfer]]\n* [[Metaknowledge]]\n* [[Ontology (information science)]]\n* [[Organisational memory]]\n* [[Semantic web]]\n* [[SciCrunch]]\n* [[Tag cloud]]\n* [[Web 2.0]]\n{{colend}}\n\'\'\'Others\'\'\'\n{{colbegin||27em}}\n* [[Collective unconscious]]\n* [[Human-computer interaction]]\n* [[Social network aggregation]]\n* [[Enterprise social software]]\n* [[Expert system]]\n* [[Knowledge]]\n* [[Knowledge base]]\n* [[Knowledge worker]]\n* [[Management information system]]\n* [[Microformats]]\n* [[Social network]]\n* [[Social software]]\n* [[Sociology of knowledge]]\n* [[Tacit knowledge]]\n{{colend}}\n\n==References==\n{{reflist|30em}}\n\n\'\'\'General\'\'\'\n{{refbegin}}\n*{{Citation\n | surname1=Nonaka | given1=Ikujiro\n | year=1994\n | title= A dynamic theory of organizational knowledge creation\n | journal= Organization Science |volume=5 |issue=1\n | pages=14–37\n | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992\n | doi=10.1287/orsc.5.1.14\n}}\n*{{Citation\n | surname1=Wigg | given1=Karl M\n | year=1993\n | title= Knowledge Management Foundations: Thinking About Thinking: How People and Organizations Create, Represent and Use Knowledge\n | journal= Arlington: Schema Press\n | pages=153\n | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992\n}}\n*{{Citation\n | surname1=Alavi | given1=Maryam\n | surname2=Leidner | given2=Dorothy E.\n | year=1999\n | title=Knowledge management systems: issues, challenges, and benefits\n | journal=Communications of the AIS\n | volume=1| issue=2 | url=http://portal.acm.org/citation.cfm?id=374117\n}}\n*{{Citation\n | surname1=Kemsley | given1=Sandy\n | year=2009\n | title=Models, Social Tagging and Knowledge Management #BPM2009 #BPMS2’09\n | journal=BPM, Enterprise 2.0 and technology trends in business\n | url=http://www.column2.com/2009/09/models-social-tagging-and-knowledge-management-bpm2009-bpms209/\n}}\n{{refend}}\n\n==External links==\n* [http://www.inc.com/tech-blog/twitter-hashtag-techniques-for-businesses.html Hashtag Techniques for Businesses], Curt Finch. Inc Magazine. May 26, 2011.\n* [http://www.tbray.org/tmp/tag-urn.html A Uniform Resource Name (URN) Namespace for Tag Metadata].  Tim Bray.  Internet draft, expired August 5, 2007.\n\n{{Web syndication}}\n\n{{DEFAULTSORT:Tag (Metadata)}}\n[[Category:Collective intelligence]]\n[[Category:Computer jargon]]\n[[Category:Information retrieval techniques]]\n[[Category:Knowledge representation]]\n[[Category:Metadata]]\n[[Category:Reference]]\n[[Category:Web 2.0]]']
['Category:Substring indices', '33958933', '{{cat main|Substring index}}\n\n[[Category:String (computer science)]]\n[[Category:Algorithms on strings]]\n[[Category:String data structures]]\n[[Category:Database index techniques]]\n[[Category:Information retrieval techniques]]\n[[Category:Bioinformatics algorithms]]']
['Subsetting', '3231582', 'In research communities (for example, [[earth science]]s, [[astronomy]], [[business]], and [[government]]), \'\'\'subsetting\'\'\' is the process of retrieving just the parts of large files which are of interest for a specific purpose. This occurs usually in a client—server setting, where the extraction of the parts of interest occurs on the server before the data is sent to the client over a network. The main purpose of subsetting is to save bandwidth on the network and storage space on the client computer.\n\nSubsetting may be favorable for the following reasons:<ref name="Institute2012">{{cite book|author=SAS Institute|title=SAS/ETS 12.1 User\'s Guide|url=https://books.google.com/books?id=OE0UfAhit4kC&pg=PA70|date=1 August 2012|publisher=SAS Institute|isbn=978-1-61290-379-8|pages=70}}</ref>\n* restrict or divide the time range\n* select [[Cross-sectional data|cross section]]s of data\n* select particular kinds of [[time series]]\n* exclude particular observations\n\n==References==\n{{reflist}}\n\n\n==External links==\n*[http://www.subset.org/index.jsp Subset.org]\n\n[[Category:Information retrieval techniques]]\n\n{{Statistics-stub}}']
['Literature-based discovery', '31149053', '\'\'\'Literature-based discovery\'\'\' refers to the use of papers and other [[Academic publishing|academic publications]] (the "literature") to find new relationships between existing knowledge (the "discovery"). The technique was pioneered by [[Don R. Swanson]] in the 1980s and has since seen widespread use. \n\nLiterature-based discovery does not generate new knowledge through laboratory experiments, as is customary for [[empirical]] sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and "neglected".<ref>{{cite journal | last1 = Swanson | first1 = Don | year = 1988 | title = Migraine and Magnesium: Eleven Neglected Connections | url = | journal = Perspectives in Biology and Medicine | volume = 31 | issue = 4| pages = 526–557 | doi=10.1353/pbm.1988.0009}}</ref> It is marked by [[empiricism]] and [[rationalism]] in concert or [[consilience]].\n\n==Swanson linking==\n[[File:Swanson linking.jpg|thumb|Swanson linking example diagram]]\n\'\'Swanson linking\'\' is a term proposed in 2003<ref>Stegmann J, Grohmann G. Hypothesis generation guided by co-word clustering. Scientometrics. 2003;56:111–135. As quoted by Bekhuis</ref> that refers to connecting two pieces of knowledge previously thought to be unrelated.<ref>{{cite journal|last=Bekhuis|first=Tanja|title=Conceptual biology, hypothesis discovery, and text mining: Swanson\'s legacy|publisher=BioMed Central Ltd.|year=2006|pmc=1459187|pmid=16584552|doi=10.1186/1742-5581-3-2|volume=3|journal=Biomed Digit Libr|pages=2}}</ref> For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called "disjoint data"), the relationship between illness A and drug C may be unknown. \'\'Swanson linking\'\' aims to find these relationships and report them.\n\n==See also==\n*[[Arrowsmith System]]\n*[[Implicature]]\n*[[Latent semantic indexing]]\n*[[Metaphor]]\n\n==References==\n* Chen, Ran; Hongfei Lin & Zhihao Yang (2011). "Passage retrieval based hidden knowledge discovery from biomedical literature." \'\'Expert Systems with Applications: An International Journal\'\' (August, 2011), vol. 38, no. 8, pp.&nbsp;9958–9964.\n*:  \'\'\'Abstract\'\'\': [...] automatic extraction of the implicit biological relationship from biomedical literature contributes to building the biomedical hypothesis that can be explored further experimentally. This paper presents a passage retrieval based method which can explore the hidden connection from MEDLINE records. [...] Experimental results show this method can significantly improve the hidden knowledge discovery performance. @ [http://portal.acm.org/citation.cfm?id=1967763.1968003&coll=DL&dl=GUIDE&CFID=23143258&CFTOKEN=52033794 ACM DL]\n\n; Further readings\n* [[Patrick Wilson (librarian)|Wilson, Patrick]] (1977). \'\'Public Knowledge, Private Ignorance: Toward a Library and Information Policy\'\'. Greenwood Publishing Group. p.&nbsp;156. ISBN 0-8371-9485-7.\n\n; Footnotes\n{{reflist}}\n\n[[Category:Information retrieval techniques]]\n[[Category:Medical research]]\n\n\n{{science-stub}}']
['Compound term processing', '18046649', '\'\'\'Compound term processing\'\'\' refers to a category of techniques used in [[information retrieval]] applications to perform matching on the basis of [[compound term]]s. Compound terms are built by combining two or more simple terms; for example, "triple" is a single word term, but "triple heart bypass" is a compound term.\n\nCompound term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? Using this technique, a search for \'\'survival rates following a triple heart bypass in elderly people\'\' will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a [[concept search]], which itself uses compound term processing. This will extract the key concepts automatically (in this case "survival rates", "triple heart bypass" and "elderly people") and use these concepts to select the most relevant documents.\n\n== Techniques ==\n\nIn August 2003, [[Concept Searching Limited]] introduced the idea of using statistical Compound Term Processing.<ref>{{cite journal|url=http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf|title=Lateral Thinking in Information Retrieval|journal=INFORMATION MANAGEMENT AND TECHNOLOGY|volume=36 PART 4}} The British Library Direct catalogue entry can be found here:[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN]</ref>\n\nCLAMOUR is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information and statistics. CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling.<ref>[http://webarchive.nationalarchives.gov.uk/20040117000117/statistics.gov.uk/methods_quality/clamour/default.asp] National Statistics CLAMOUR project</ref>\n\n== History ==\n\nTechniques for probabilistic weighting of single word terms date back to at least 1976 in the landmark publication by [[Stephen Robertson (computer scientist)|Stephen E. Robertson]] and [[Karen Spärck Jones]].<ref>{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Spärck Jones | first2 = K. | authorlink2 = Karen Spärck Jones}}</ref> Robertson stated that the assumption of word independence is not justified and exists as a matter of mathematical convenience. His objection to the term independence is not a new idea, dating back to at least 1964 when H. H. Williams stated that "[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience".<ref>{{cite journal|last=WILLIAMS |first=J.H. |title=Results of classifying documents with multiple discriminant functions |url=http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=AD0612272 |journal= Statistical Association Methods for Mechanized Documentation, National Bureau of Standards |location=Washington |pp=217-224 |year=1965}}</ref>\n\nIn 2004, Anna Lynn Patterson filed patents on "phrase-based searching in an information retrieval system"<ref>{{patent|US|20060031195}}</ref> to which [[Google]] subsequently acquired the rights.<ref>[http://www.seobythesea.com/2012/02/google-acquires-cuil-patent-applications/ Google Acquires Cuil Patent Applications]</ref>\n\n== Adaptability ==\n\nStatistical compound term processing is more adaptable than the process described by Patterson. Her process is targeted at searching the [[World Wide Web]] where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to [[enterprise search]] applications where such [[A priori and a posteriori|a priori]] knowledge is not available.\n\nStatistical compound term processing is also more adaptable than the linguistic approach taken by the CLAMOUR project, which must consider the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent.\n\n== Applications ==\nCompound Term Processing allows information retrieval applications, such as [[search engines]], to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous.\n\nEarly search engines looked for documents containing the words entered by the user into the search box . These are known as [[keyword search]] engines. [[Boolean search]] engines add a degree of sophistication by allowing the user to specify additional requirements. For example, "Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen" uses the operators "NEAR", "AND", "OR" and "NOT" to specify that these words must follow certain requirements. A [[phrase search]] is simpler to use, but requires that the exact phrase specified appear in the results.\n\n==See also==\n* [[Concept Searching Limited]]\n* [[Enterprise search]]\n* [[Information retrieval]]\n\n== References ==\n{{Reflist|30em}}\n\n==  External links ==\n\n{{Natural Language Processing}}\n\n{{DEFAULTSORT:Compound Term Processing}}\n[[Category:Information retrieval techniques]]']
['Personalization', '1656760', '{{Multiple issues|\n{{cleanup-reorganize|date=June 2008}}\n{{refimprove|date=September 2015}}\n{{tone|date=September 2015}}\n}}\n\n\'\'\'Personalization\'\'\', sometimes known as \'\'\'customization\'\'\', consists of tailoring a service or a product to accommodate specific individuals, sometimes tied to groups or segments of individuals.  A wide variety of organizations use personalization to improve [[customer satisfaction]], digital sales conversion, marketing results, branding, and improved website metrics as well as for [[advertising]]. Personalization is a key element in [[social media]] and [[recommender system]]s.\n\n==Web pages==\n[[Web page]]s can be personalized based on the characteristics (interests, social category, context, etc.), actions (click on button, open a link, etc.), intent (make a purchase, check status of an entity), or any other parameter that can be identified and associated with an individual, therefore providing them with a tailored user experience.  Note that the experience is rarely simply accommodation of the user but a relationship between the user and the desires of the site designers in driving specific actions to achieve objectives (e.g. Increase sales conversion on a page).  The term \'\'customization\'\' is often used when the site only uses explicit data such as product ratings or user preferences.\n\nTechnically, web personalization can be achieved by associating a visitor segment with a predefined action. Customizing the user experience based on behavioural, contextual and technical data is proven to have a positive impact on conversion rate optimization efforts. Associated actions can range from changing the content of a webpage, presenting a modal display, presenting interstitials, triggering a personalized email or even automating a phone call to the user.\n\nAccording to a 2014 study from research firm Econsultancy, less than 30% of [[e-commerce]] websites have invested in the field of web personalization. However, many companies now offer services for web personalization as well as web and email recommendation systems that are based on personalization or anonymously-collected user behaviours.<ref name=behaviours>[http://online.wsj.com/article/SB10001424052748703294904575385532109190198.html \'\'Wall Street Journal\'\', “On the Web\'s Cutting Edge, Anonymity in Name Only”], August 4, 2010</ref> According to a study done by Compass, e-commerce websites that use personalization can see an increase in revenue of as much as 29%. <ref name=29%>[http://blog.compass.co/improving-ecommerce-retention-revenue-personalization/ \'\'Compass Blog\'\', “Improving Ecommerce Retention and Revenue with Personalization”], August 11, 2016</ref>\n\nThere are many categories of web personalization including\n# Behavioral\n# Contextual\n# Technical\n# Historic data\n# Collaboratively filtered\n\nThere are several camps in defining and executing web personalization.  A few broad methods for web personalization may include:\n# Implicit\n# Explicit\n# Hybrid\n\nWith implicit personalization, the web personalization is performed based on the different categories mentioned above. It can also be learned from direct interactions with the user based on implicit data, such as items purchased or pages viewed.<ref>{{cite web|last1=Flynn|first1=Lawrence|title=5 Things To Know About Siri And Google Now\'s Growing Intelligence|url=http://www.forbes.com/sites/parmyolson/2014/07/08/5-things-to-know-about-siri-and-google-nows-growing-intelligence/|website=Forbes}}</ref> With explicit personalization, the web page (or information system) is changed by the user using the features provided by the system. Hybrid personalization combines the above two approaches to leverage the \'\'best of both worlds\'\'.\n\nWeb personalization is can be linked to the notion of [[Adaptive hypermedia]] (AH). The main difference is that the former would usually work on what is considered an "open corpus hypermedia," whilst the latter would traditionally work on "closed corpus hypermedia." However, recent research directions in the AH domain take both closed and open corpus into account. Thus, the two fields are closely inter-related.\n\nPersonalization is also being considered for use in less overtly commercial applications to improve the user experience online.<ref>[[Jonathan Bowen|Bowen, J.P.]] and Filippini-Fantoni, S., [http://www.archimuse.com/mw2004/papers/bowen/bowen.html Personalization and the Web from a Museum Perspective]. In [[David Bearman]] and Jennifer Trant (eds.), \'\'[[Museums and the Web]] 2004: Selected Papers from an International Conference\'\', Arlington, Virginia, USA, 31 March – 3 April 2004. Archives & Museum Informatics, pages 63–78, 2004.</ref> Internet activist [[Eli Pariser]] has documented that search engines like [[Google]] and [[Yahoo! News]] give different results to different people (even when logged out). He also points out social media site [[Facebook]] changes user\'s friend feeds based on what it thinks they want to see. Pariser warns that these algorithms can create a "[[filter bubble]]" that prevents people from encountering a diversity of viewpoints beyond their own, or which only presents facts which confirm their existing views.\n\nOn an [[intranet]] or [[B2E]] [[Web portal#Enterprise Web portals|Enterprise Web portals]], personalization is often based on user attributes such as department, functional area, or role. The term "customization" in this context refers to the ability of users to modify the page layout or specify what content should be displayed.\n\n==Digital media==\nAnother aspect of personalization is the increasing prevalence of [[open data]] on the Web. Many companies make their data available on the Web via [[API]]s, web services, and [[open data]] standards. One such example is Ordnance Survey Open Data.<ref>{{cite news| url=https://www.theguardian.com/news/datablog/2010/apr/02/ordnance-survey-open-data | location=London | work=The Guardian | first1=Chris | last1=Thorpe | first2=Simon | last2=Rogers | title=Ordnance Survey opendata maps: what does it actually include? | date=2 April 2010}}</ref> Data made available in this way is structured to allow it to be inter-connected and re-used by third parties.<ref>{{cite web|url=http://www.cio.com/article/372363/Google_Opens_Up_Data_Center_For_Third_Party_Web_Applications |title=Google Opens Up Data Centre for Third Party Web Applications |publisher=Cio.com |date=2008-05-28 |accessdate=2013-01-16}}</ref>\n\nData available from a user\'s personal [[social graph]] can be accessed by third-party [[application software]] to be suited to fit the personalized [[web page]] or [[information appliance]].\n\nCurrent [[open data]] standards on the Web include:\n# [[Attention Profiling Mark-up Language]] (APML)\n# DataPortability\n# [[OpenID]]\n# [[OpenSocial]]\n\n== Mobile phones ==\n\nOver time mobile phones have seen an increased emphasis placed on user personalization. Far from the black and white screens and monophonic ringtones of the past, phones now offer interactive wallpapers and MP3 TruTones. In the UK and Asia, WeeMees have become popular. WeeMees are three-dimensional characters that are used as wallpaper and respond to the tendencies of the user. Video Graphics Array (VGA) picture quality allows people to change their background with ease without sacrificing quality. All of these services are downloaded through the provider with the goal to make the user feel connected to the phone.<ref>May, Harvey, and Greg Hearn. "The Mobile Phone as Media." International Journal of Cultural Studies 8.2 (2005): 195-211. Print.</ref>\n\n==Print media==\n{{main|Mail merge}}\n\nIn print media, ranging from [[magazine]]s to [[admail|promotional publication]]s, personalization uses databases of individual recipients\' information. Not only does the written document address itself by name to the reader, but the advertising is targeted to the recipient\'s demographics or interests using fields within the database, such as "first name", "last name", "company", etc.\n\nThe term "personalization" should not be confused with variable data, which is a much more granular method of marketing that leverages both images and text with the medium, not just fields within a database. Although personalized children\'s books are created by companies who are using and leveraging all the strengths of [[variable data printing|variable data printing (VDP)]]. This allows for full image and text variability within a printed book.\nWith the advent of online 3D printing services such as Shapeways and Ponoko we are seeing personalization enter into the realms of product design.\n\n== Promotional merchandise ==\nPromotional items ([[mug]]s, [[T-shirt]]s, [[keychain]]s, [[ball]]s etc.) are regularly personalized. Personalized children\'s storybooks—wherein the child becomes the [[protagonist]], with the name and image of the child personalized—are also popular. Personalized CDs for children also exist. With the advent of [[digital printing]], personalized calendars that start in any month, birthday cards, cards, e-cards, posters and photo books can also be obtained.\n\n== 3D printing ==\n3D printing is a production method that allows to create unique and personalized items on a global scale. Personalized apparel and accessories, such as jewellery, are increasing in popularity.<ref>{{cite web|url=http://www.jewellermagazine.com/Article.aspx?id=2167&h=New-jewellery-website-targets-|title=New jewellery website targets \'customisers\'|last=Weinman|first=Aaron|date=21 February 2012|publisher=Jeweller Magazine|language=|accessdate=6 January 2015}}</ref> This kind of customization is also relevant in other areas like Consumer Electronics<ref>{{Cite web|url=http://www.3ders.org/articles/20160121-philips-launches-worlds-first-personalized-3d-printed-face-shaver-for-limited-edition-run.html|title=Philips launches the world\'s first personalized, 3D printed face shaver for limited edition run|website=3ders.org|language=en-US|access-date=2016-03-02}}</ref> and Retail.<ref>{{Cite web|url=http://twikblog.twikit.com/belgian-3d-company-twikit-brings-3d-customization-french-retail/|title=Twikit brings 3D customization to French retail.|website=Twikit Blog {{!}} 3D Customization, 3D Printing|language=en-US|access-date=2016-03-02}}</ref> By combining 3D printing with complex software a product can easily be customized by an end-user.\n\n== Mass personalization ==\n\n{{tone|section|date=January 2011}}\n\nMass personalization is defined as custom tailoring by a company in accordance with its end users tastes and preferences.<ref>{{cite web|url=http://www.answers.com/personalization&r=67 |title=personalize: Definition, Synonyms from |publisher=Answers.com |date= |accessdate=2013-01-16}}</ref> From collaborative engineering perspective, mass customization can be viewed as collaborative efforts between customers and manufacturers, who have different sets of priorities and need to jointly search for solutions that best match customers\' individual specific needs with manufacturers\' customization capabilities.<ref>\tChen, S., Y. Wang and M. M. Tseng. 2009. Mass Customization as a Collaborative Engineering Effort. International Journal of Collaborative Engineering, 1(2): 152-167</ref> The main difference between mass customization and mass personalization is that customization is the ability for a company to give its customers an opportunity to create and choose product to certain specifications, but does have limits.<ref>Haag et al., \'\'Management Information Systems for the Information Age\'\', 3rd edition, 2006, page 331.</ref>\n\nA website knowing a user\'s location, and buying habits, will present offers and suggestions tailored to the user\'s demographics; this is an example of mass personalization. The personalization is not individual but rather the user is first classified and then the personalization is based on the group they belong to.<ref>{{cite news| url=http://www.telegraph.co.uk/foodanddrink/9808015/How-supermarkets-prop-up-our-class-system.html | location=London | work=The Daily Telegraph | first=Harry | last=Wallop | title=How supermarkets prop up our class system | date=2013-01-18}}</ref>\n\n[[Behavioral targeting]] represents a concept that is similar to mass personalization.\n\n== Predictive personalization ==\n\nPredictive personalization is defined as the ability to predict customer behavior, needs or wants - and tailor offers and communications very precisely.<ref>{{cite web|url=http://www.slideshare.net/jwtintelligence/jwt-10-trends-for-2013-executive-summary|title=10 Trends for 2013 Executive Summary: Definition, Projected Trends |publisher=JWTIntelligence.com |date= |accessdate=2012-12-04}}</ref>  Social data is one source of providing this predictive analysis, particularly social data that is structured.  Predictive personalization is a much more recent means of personalization and can be used well to augment current personalization offerings.\n\n== Map personalization ==\n{{Expand section|date=September 2015}}Digital [[Web mapping|web maps]] are also being personalized. [[Google Maps]] change the content of the map based on previous searches and other profile information.<ref>{{Cite web|title = The Next Frontier For Google Maps Is Personalization|url = http://social.techcrunch.com/2013/02/01/the-next-frontier-for-google-maps-is-personalization/|website = TechCrunch|accessdate = 2015-09-13|first = Frederic|last = Lardinois}}</ref> Technology writer [[Evgeny Morozov]] has criticized map personalization as a threat to [[public space]].<ref>{{Cite news|title = My Map or Yours?|url = http://www.slate.com/articles/technology/future_tense/2013/05/google_maps_personalization_will_hurt_public_space_and_engagement.html|newspaper = Slate|date = 2013-05-28|access-date = 2015-09-13|issn = 1091-2339|language = en|first = Evgeny|last = Morozov}}</ref>\n\n==See also==\n* [[Adaptation (computer science)]]\n* [[Mass customization]]\n* [[Adaptive hypermedia]]\n* [[Behavioral targeting]]\n* [[Bespoke]]\n* [[Collaborative filtering]]\n* [[Configurator]]\n* [[Personalized learning]]\n* [[Preorder economy]]\n* [[Real-time marketing]]\n* [[Recommendation system]]\n* [[User modeling]]\n\n==References==\n{{reflist|2}}\n\n==External links==\n* [http://www.iimcp.org International Institute on Mass Customization & Personalization which organizes MCP, a biannual conference on customization and personalization]\n* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] \'\'The Journal of Personalization Research\'\'\n\n[[Category:Human–computer interaction]]\n[[Category:World Wide Web]]\n[[Category:User interface techniques]]\n[[Category:Usability|Personas]]\n[[Category:Types of marketing]]\n[[Category:Information retrieval techniques]]']
['Cluster labeling', '25202953', 'In [[natural language processing]] and [[information retrieval]], \'\'\'cluster labeling\'\'\' is the problem of picking descriptive, human-readable labels for the clusters produced by a [[document clustering]] algorithm; standard clustering algorithms do not typically produce any such labels. Cluster labeling algorithms examine the contents of the documents per cluster to find a labeling that summarize the topic of each cluster and distinguish the clusters from each other.\n\n==Differential cluster labeling==\nDifferential cluster labeling labels a cluster by comparing term [[probability distribution|distributions]] across clusters, using techniques also used for [[feature selection]] in [[document classification]], such as [[mutual information]] and [[Pearson\'s chi-squared test|chi-squared feature selection]].  Terms having very low frequency are not the best in representing the whole cluster and can be omitted in labeling a cluster.  By omitting those rare terms and using a differential test, one can achieve the best results with differential cluster labeling.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. \'\'Introduction to Information Retrieval\'\'. Cambridge: Cambridge UP, 2008. \'\'Cluster Labeling\'\'. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html>.</ref>\n\n===Pointwise mutual information===\n\n{{Main article|Pointwise mutual information}}\n\nIn the fields of [[probability theory]] and [[information theory]], mutual information measures the degree of dependence of two [[random variables]].  The mutual information of two variables {{mvar|X}} and {{mvar|Y}} is defined as:\n\n<math>I(X, Y) = \\sum_{x\\in X}{ \\sum_{y\\in Y} {p(x, y)log_2\\left(\\frac{p(x, y)}{p_1(x)p_2(y)}\\right)}}</math>\n\nwhere \'\'p(x, y)\'\' is the [[joint probability|joint probability distribution]] of the two variables, \'\'p<sub>1</sub>(x)\'\' is the probability distribution of X, and \'\'p<sub>2</sub>(y)\'\' is the probability distribution of Y.\n\nIn the case of cluster labeling, the variable X is associated with membership in a cluster, and the variable Y is associated with the presence of a term.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. \'\'Introduction to Information Retrieval\'\'. Cambridge: Cambridge UP, 2008. \'\'Mutual Information\'\'. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html>.</ref>  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:\n\n<math>I(C, T) = \\sum_{c\\in {0, 1}}{ \\sum_{t\\in {0, 1}} {p(C = c, T = t)log_2\\left(\\frac{p(C = c, T = t)}{p(C = c)p(T = t)}\\right)}}</math>\n\nIn this case, \'\'p(C = 1)\'\' represents the probability that a randomly selected document is a member of a particular cluster, and \'\'p(C = 0)\'\' represents the probability that it isn\'t.  Similarly, \'\'p(T = 1)\'\' represents the probability that a randomly selected document contains a given term, and \'\'p(T = 0)\'\' represents the probability that it doesn\'t.  The [[joint probability|joint probability distribution function]] \'\'p(C, T)\'\' represents the probability that two events occur simultaneously.  For example, \'\'p(0, 0)\'\' is the probability that a document isn\'t a member of cluster \'\'c\'\' and doesn\'t contain term \'\'t\'\'; \'\'p(0, 1)\'\' is the probability that a document isn\'t a member of cluster \'\'c\'\' and does contain term \'\'t\'\'; and so on.\n\n===Chi-Squared Selection===\n{{Main article|Pearson\'s chi-squared test}}\nThe Pearson\'s chi-squared test can be used to calculate the probability that the occurrence of an event matches the initial expectations.  In particular, it can be used to determine whether two events, A and B, are [[statistically independent]].  The value of the chi-squared statistic is:\n\n<math>X^2 = \\sum_{a \\in A}{\\sum_{b \\in B}{\\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>\n\nwhere \'\'O<sub>a,b</sub>\'\' is the \'\'observed\'\' frequency of a and b co-occurring, and \'\'E<sub>a,b</sub>\'\' is the \'\'expected\'\' frequency of co-occurrence.\n\nIn the case of cluster labeling, the variable A is associated with membership in a cluster, and the variable B is associated with the presence of a term.  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:\n\n<math>X^2 = \\sum_{a \\in {0,1}}{\\sum_{b \\in {0,1}}{\\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>\n\nFor example, \'\'O<sub>1,0</sub>\'\' is the observed number of documents that are in a particular cluster but don\'t contain a certain term, and \'\'E<sub>1,0</sub>\'\' is the expected number of documents that are in a particular cluster but don\'t contain a certain term.\nOur initial assumption is that the two events are independent, so the expected probabilities of co-occurrence can be calculated by multiplying individual probabilities:<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. \'\'Introduction to Information Retrieval\'\'. Cambridge: Cambridge UP, 2008. \'\'Chi2 Feature Selection\'\'. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html>.</ref>\n\n\'\'E<sub>1,0</sub> = N * P(C = 1) * P(T = 0)\'\'\n\nwhere N is the total number of documents in the collection.\n\n==Cluster-Internal Labeling==\nCluster-internal labeling selects labels that only depend on the contents of the cluster of interest. No comparison is made with the other clusters.\nCluster-internal labeling can use a variety of methods, such as finding terms that occur frequently in the centroid or finding the document that lies closest to the centroid.\n\n===Centroid Labels===\n{{Main article|Vector space model}}\nA frequently used model in the field of [[information retrieval]] is the vector space model, which represents documents as vectors.  The entries in the vector correspond to terms in the [[vocabulary]]. Binary vectors have a value of 1 if the term is present within a particular document and 0 if it is absent. Many vectors make use of weights that reflect the importance of a term in a document, and/or the importance of the term in a document collection. For a particular cluster of documents, we can calculate the [[centroid]] by finding the [[arithmetic mean]] of all the document vectors.  If an entry in the centroid vector has a high value, then the corresponding term occurs frequently within the cluster.  These terms can be used as a label for the cluster.\nOne downside to using centroid labeling is that it can pick up words like "place" and "word" that have a high frequency in written text, but have little relevance to the contents of the particular cluster.\n\n===Contextualized centroid labels===\nA simple, cost-effective way of overcoming the above limitation is to embed the centroid terms with the highest weight in a graph structure that provides a context for their interpretation and selection.<ref>Francois Role, Moahmed Nadif. [http://dl.acm.org/citation.cfm?id=2574675 Beyond cluster labeling: Semantic interpretation of clusters’ contents using a graph representation.] Knowledge-Based Systems, Volume 56, January, 2014: 141-155</ref>\nIn this approach, a term-term co-occurrence matrix referred as <math>T_k</math> is first built for each cluster <math>S_k</math>. Each cell represents the number of times term <math>i</math> co-occurs with term <math>j</math> within a certain window of text (a sentence, a paragraph, etc.)\nIn a second stage, a similarity matrix <math>T_k^{sim}</math> is obtained by multiplying <math>T_k</math> with its transpose. We have <math>T_k^{sim}=T_k\' T_k=(t_{{sim}_{ij}})</math>. Being the dot product of two normalized vectors <math>\\tilde{t}_{i}</math> and <math>\\tilde{t}_{j}</math>, <math>t_{{sim}_{ij}}</math> denotes the cosine similarity between terms <math>i</math> and <math>j</math>. The so obtained <math>T_k^{sim}</math> can then be used as the weighted adjacency matrix of a term similarity graph. The centroid terms are part of this graph, and they thus can be interpreted and scored by inspecting the terms that surround them in the graph.\n\n===Title labels===\nAn alternative to centroid labeling is title labeling.  Here, we find the document within the cluster that has the smallest [[Euclidean distance]] to the centroid, and use its title as a label for the cluster.  One advantage to using document titles is that they provide additional information that would not be present in a list of terms.  However, they also have the potential to mislead the user, since one document might not be representative of the entire cluster.\n\n===External knowledge labels===\nCluster labeling can be done indirectly using external knowledge such as pre-categorized knowledge such as the one of Wikipedia.<ref>David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146</ref> In such methods, a set of important cluster text features are first extracted from the cluster documents. These features then can be used to retrieve the (weighted) K-nearest categorized documents from which candidates for cluster labels can be extracted. The final step involves the ranking of such candidates. Suitable methods are such that are based on a voting or a fusion process which is determined using the set of categorized documents and the original cluster features.\n\n=== Combining Several Cluster Labelers ===\nThe cluster labels of several different cluster labelers can be further combined to obtain better labels. \nFor example, [[Linear Regression]] can be used to learn an optimal combination of labeler scores.<ref>David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146</ref> A more sophisticated technique is based on a [[wikt:fusion|fusion]] approach and analysis of the cluster labels decision stability of various labelers.<ref>Haggai Roitman, Shay Hummel, Michal Shmueli-Scheuer. [http://dl.acm.org/citation.cfm?id=2609465 A fusion approach to cluster labeling.] SIGIR 2014: 883-886</ref>\n\n==External links==\n* [http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html Hierarchical Clustering]\n* [http://www.cs.cmu.edu/~callan/Papers/dgo06-puck.pdf Automatically Labeling Hierarchical Clusters]\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Cluster Labeling}}\n[[Category:Information retrieval techniques]]']
['Hashtag', '20819040', '{{Use mdy dates|date=May 2016}}\n{{Use American English|date=May 2016}}\n\n[[File:Global Summit to End Sexual Violence in Conflict (14203190979).jpg|thumb|A sign suggesting the use of a #TimeToAct hashtag at a 2014 conference]]\n\nA \'\'\'hashtag\'\'\' is a type of label or [[Tag (metadata)|metadata tag]] used on [[Social networking service|social network]] and [[microblogging]] services which makes it easier for users to find messages with a specific theme or content. Users create and use hashtags by placing the [[Number sign|hash character <code>#</code>]] (also known as the number sign or pound sign) in front of a word or unspaced phrase, either in the main text of a message or at the end. Searching for that hashtag will yield each message that has been tagged with it. A hashtag archive is consequently collected into a single stream under the same hashtag.<ref>{{Cite journal|last=Chang|first=Hsia-Ching|last2=Iyer|first2=Hemalata|title=Trends in Twitter Hashtag Applications: Design Features for Value-Added Dimensions to Future Library Catalogues|url=http://muse.jhu.edu/article/485537|journal=Library Trends|volume=61|issue=1|pages=248–258|doi=10.1353/lib.2012.0024|issn=1559-0682}}</ref> For example, on the [[photo sharing|photo-sharing]] service [[Instagram]], the hashtag \'\'#bluesky\'\' allows users to find all the posts that have been tagged using that hashtag.\n\nBecause of its widespread use, \'\'hashtag\'\' was added to the \'\'[[Oxford English Dictionary]]\'\' in June 2014.<ref>{{cite web |title=\'Hashtag\' added to the OED – but # isn\'t a hash, pound, nor number sign|url=http://www.theregister.co.uk/2014/06/13/hashtag_added_to_the_oed/|work=The Register|date=June 13, 2014}}</ref><ref>{{cite web |title=New words notes June 2014|url=http://public.oed.com/the-oed-today/recent-updates-to-the-oed/june-2014-update/new-words-notes-june-2014/|work=Oxford English Dictionary|date=June 2014}}</ref>  The term \'\'hashtag\'\' can also refer to the hash symbol itself when used in the context of a hashtag.<ref>{{cite web |title=Oxford English Dictionary – Hash|url=http://www.oed.com/view/Entry/389023#eid301493073|work=Oxford English Dictionary|date=June 2014}}</ref>\n\n== Origin and use ==\nThe [[Number sign|pound sign]] or [[Number sign|hash symbol]] was often used in [[information technology]] to highlight a special meaning. (It should be noted that the words "Pound Sign" in the UK refer specifically to currency "£" - extended ASCII character 156 - and not weight.) In 1970 for example, the number sign was used to denote \'\'immediate\'\' [[address mode]] in the assembly language of the [[PDP-11]]<ref>{{cite web|url=https://programmer209.wordpress.com/2011/08/03/the-pdp-11-assembly-language/ |title=PDP-11 assembly language |publisher=Programmer209.wordpress.com |date=August 3, 2011 |accessdate=August 25, 2014}}</ref> when placed next to a symbol or a number. In 1978, [[Brian Kernighan]] and [[Dennis Ritchie]] used \'\'#\'\' in the [[C (programming language)|C programming language]] for special keywords that had to be processed first by the [[C preprocessor]].<ref>{{cite book|title=[[The C Programming Language]]|authors=B.W. Kernighan & d. Ritchie|publisher=Prentice Hall|year=1978|pages=86 and 207|isbn=0-13-110163-3}}</ref> Since before the invention of the hashtag, the pound sign has been called the "hash symbol" in some countries outside of North America.<ref>{{cite book|last1=Bourke|first1=Jane|title=Communication Technology Resource Book|date=2004|publisher=Ready-Ed Publications|pages=19|url=https://books.google.com/books?id=gPNBTmxzpIIC&lpg=PA19&dq=hash%20key%20telephone&pg=PA19#v=onepage&q=hash&f=false|accessdate=November 7, 2014|isbn=978-1-86397-585-8}}</ref><ref>{{cite book|last1=Hargraves|first1=Orin|title=Mighty fine words and smashing expressions : making sense of transatlantic English|date=2003|publisher=Oxford Univ. Press|location=Oxford [u.a.]|isbn=978-0-19-515704-8|pages=33, 260|url=https://books.google.com/books?id=dUTdk93cq9UC&lpg=PA260&dq=hash%20telephone&pg=PA260#v=onepage&q=hash%20mark&f=false}}</ref>\n\nThe pound sign appeared and was used by people within [[Internet Relay Chat|IRC]] networks to label groups and topics.<ref>"Channel Scope". Section 2.2. RFC 2811</ref> Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an [[ampersand]] \'&\').<ref>{{cite IETF |title=Internet Relay Chat Protocol |rfc=1459 |sectionname=Channels |section=1.3 |page= |last1=Oikarinen |first1=Jarkko |authorlink1=Jarkko Oikarinen |last2=Reed |first2=Darren |authorlink2= |year=1993 |month=May |publisher=[[Internet Engineering Task Force|IETF]] |accessdate=June 3, 2014}}</ref>\n\nThe use of the pound sign in IRC inspired<ref>{{cite web|url=http://www.cmu.edu/homepage/computing/2014/summer/originstory.shtml |title=#OriginStory|publisher=Carnegie Mellon University|date=August 29, 2014}}</ref> [[Chris Messina (open source advocate)|Chris Messina]] to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network.<ref>{{cite news | url=http://www.nytimes.com/2011/06/12/fashion/hashtags-a-new-way-for-tweets-cultural-studies.html?_r=1&pagewanted=all | title=Twitter\'s Secret Handshake | work=The New York Times | date=June 10, 2011 | accessdate=July 26, 2011 | author=Parker, Ashley}}</ref> He posted the first hashtag on Twitter:\n{{quote |1=How do you feel about using # (pound) for groups. As in #barcamp [msg]? |author = Chris Messina |source = ("factoryjoe"), August 23, 2007<ref>{{cite web|url = https://twitter.com/factoryjoe/statuses/223115412|title = Twitter post|author = Chris Messina ("factoryjoe")|date = August 23, 2007<!-- 3:25 PM-->}}</ref> |width  = 50% |align  = center }}\nMessina’s suggestion to use the hashtag was not adopted by Twitter, but the practice took off after hashtags were widely used in tweets relating to the [[2007 San Diego forest fires]] in Southern California.<ref>[http://mashable.com/2013/10/08/what-is-hashtag/ What is hashtag?"], Mashable, 8 October 2013</ref><ref>https://factoryjoe.com/2007/10/22/twitter-hashtags-for-emergency-coordination-and-disaster-relief/</ref>\n\nAccording to Messina, he suggested use of the hashtag to make it easy for "lay" users to search for content and find specific relevant updates; they are for people who do not have the technological knowledge to navigate the site. Therefore, the hashtag “was created organically by Twitter users as a way to categorize messages." <ref>{{Cite journal|last=Scott|first=Kate|date=2015-05-01|title=The pragmatics of hashtags: Inference and conversational style on Twitter|url=http://www.sciencedirect.com/science/article/pii/S037821661500096X|journal=Journal of Pragmatics|volume=81|pages=8–20|doi=10.1016/j.pragma.2015.03.015}}</ref>\n\nInternationally, the hashtag became a practice of writing style for Twitter posts during the [[2009–2010 Iranian election protests]]; Twitter users inside and outside Iran used both English- and [[Persian language|Persian]]-language hashtags in communications during the events.<ref>{{cite news|title=The story of the hashtag began with Iranians|url=http://www.dw.de/حکایت-هشتگی-که-ایرانیان-آغاز-کردند/g-18012627|accessdate=March 12, 2015|publisher=Deutsche Welle Persian|date=2009}}</ref>\n\nThe first published use of the term "hash tag" was in a blog post by Stowe Boyd, "Hash Tags = Twitter Groupings,"<ref>{{cite web|url=http://stoweboyd.com/post/39877198249/hash-tags-twitter-groupings |title=Stowe Boyd, Hash Tags = Twitter Groupings |publisher=Stoweboyd.com |date= |accessdate=September 19, 2013}}</ref> on August 26, 2007, according to lexicographer [[Ben Zimmer]], chair of the American Dialect Society\'s New Words Committee.\n\nBeginning July 2, 2009,<ref>{{cite web|title=Twitter Makes Hashtags More #Useful|url=http://techcrunch.com/2009/07/02/twitter-makes-hashtags-more-useful/|accessdate=December 27, 2015}}</ref> Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced "[[Twitter#Trending topics|Trending Topics]]" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to [[spamming|spam]] the trending list and ensure that hashtags trend naturally.<ref>{{cite web|url=http://www.allisayis.com/the-secret-of-twitters-trending-hashtags-with-insight-and-tips/ |title=The Secret of Twitter\'s Trending Hashtags With Insight and Tips |publisher=AllISayIs.com |date= |accessdate=December 3, 2014}}</ref>\n\nAlthough the hashtag started out most popularly on Twitter as the main social media platform for this use, the use has extended to other social media sites including Instagram, Facebook, Flickr, Tumblr, and Google+.<ref>{{Cite web|url=http://www.cnn.com/2013/06/12/tech/social-media/facebook-hashtags/index.html|title=Facebook finally gets #hashtags - CNN.com|last=Mashable|first=By Christina Warren|website=CNN|access-date=2016-05-16}}</ref>\n\nIn China, microblogs [[Sina Weibo]] and [[Tencent Weibo]] use a double-hashtag #HashName# format, since the lack of spacing between [[Chinese characters]] necessitates a closing tag. In contrast, when using Chinese characters (and [[orthographies]] with similar spacing conventions) on [[Twitter]], users must insert spacing before and after the hashtagged element (e.g. \'我 #爱 你\' instead of \'我#爱你\')<ref>{{cite news|last1=Martin|first1=Rick|title=Twitter Rolls Out Hashtag Support for Japanese, Korean, Chinese, and Russian|url=https://www.techinasia.com/twitter-hashtag-languages/|accessdate=March 5, 2015|publisher=Tech in Asia|date=July 13, 2011}}</ref> or insert a [[zero-width non-joiner]] character before and after the hashtagged element, to retain a linguistically natural appearance, such as \'我#爱你\'.<ref>{{cite news|last1=International services team|title=Right-to-left languages on Twitter|url=https://blog.twitter.com/2012/right-to-left-languages-on-twitter|accessdate=March 5, 2015|publisher=Twitter|date=April 5, 2012}}</ref>\n\n== Style ==\nOn microblogging or social networking sites, hashtags can be inserted anywhere within a sentence, either preceding it, following it as a [[postscript]], or being included as a word within the sentence (e.g. "It is #sunny today").\n\nThe quantity of hashtags used in a post or tweet is just as important as the types of hashtags used. It is currently considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the "absolute maximum", and any contribution exceeding this risks "raising the ire of the community."<ref>{{cite web|title=What is a (#) Hashtag?|url=http://www.hashtags.org/how-to/history/what-is-a-hashtag/|publisher=Hashtags.org|accessdate=February 22, 2014}}</ref>\n\nAs well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or even suspended.<ref>{{cite web|title=The Twitter Rules|url=https://support.twitter.com/groups/56-policies-violations/topics/236-twitter-rules-policies/articles/18311-the-twitter-rules|publisher=Twitter, Inc.|accessdate=February 22, 2014}}</ref>{{failed verification|date=August 2014}}\n\n[[Jimmy Fallon]] and [[Justin Timberlake]] performed a sketch parodying the often misused and misunderstood usage of hashtags on \'\'[[Late Night with Jimmy Fallon]]\'\' in September 2013.<ref>{{cite web|author=The Tonight Show Starring Jimmy Fallon |url=https://www.youtube.com/watch?v=57dzaMaouXA |title="#Hashtag" with Jimmy Fallon & Justin Timberlake (Late Night with Jimmy Fallon) |publisher=YouTube |date=September 24, 2013 |accessdate=August 25, 2014}}</ref>\n\n== Function ==\n\n[[File:Seguir hashtags.png|upright=1.3|right|thumb|Search bar in the header of a social networking site, searching for most recent posts containing the hashtag #science]]\n\nHashtags are mostly used in unmoderated, ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can "trend" and attract more individual users to discussion. On Twitter, when a hashtag becomes extremely popular, it will appear in the "Trending Topics" area of a user\'s homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users. They cannot be "retired" from public usage, meaning that any given hashtag can theoretically be used in perpetuity. They do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes, as chosen by those who make use of them.\n\nHashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using #cakefestival rather than simply #cake. However, this can also make it difficult for topics to become "trending topics" because people often use different spelling or words to refer to the same topic.  In order for topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.\n\nHashtags also function as beacons in order for users to find and "follow" (subscribe) or "list" (organize into public contact lists) other users of similar interest.\n\nIn recent years, broadcasters such as [[Channel 4]] have employed the hashtag during the airing of programmes such as [[First Dates]] and [[The Undateables]]. Research has shown that audience numbers go up when individuals can be interactive - by tweeting while viewing a programme on TV.\n\nHashtags can be used on the social network [[Instagram]], by posting a picture and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic, such as #photography #iPhone #iphoneography, and therefore do not fulfill a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use.<ref>{{cite web|url=http://www.bbc.co.uk/news/technology-24842750 |title=Instagram banned hashtags | date = November 7, 2013|publisher=[[BBC.co.uk]] |accessdate=November 25, 2013}}</ref> The ban against certain hashtags has a consequential role in the way that particular [[subaltern]] communities are built and maintained on Instagram. Despite Instagram\'s content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.<ref>Olszanowski, M. (2014). "Feminist Self-Imaging and Instagram: Tactics of Circumventing Sensorship{{sic|hide=y}}". \'\'Visual Communication Quarterly,\'\' 21(1), 83-95. Retrieved February 8, 2015, from http://www.tandfonline.com/doi/abs/10.1080/15551393.2014.928154#.VNgGT7DF-7FF-7F</ref>\n\nHashtags are also used informally to express context around a given message, with no intent to categorize the message for later searching, sharing, or other reasons. One of the functions of the hashtag is to serve as a reflexive meta-commentary, which contributes to the idea of how written communication in new media can be paralleled to how pragmatic methodology is applied to speech.<ref>{{Cite web|url=http://www.linguistics.fi/julkaisut/SKY2014/Wikstrom.pdf|title=#srynotfunny: Communicative Functions of Hashtags on Twitter|last=Wilkström|first=Peter|date=2014|work=SKY Journal of Linguistics|publisher=|access-date=May 15, 2016}}</ref>\n\nThis can help express contextual cues or offer more depth to the information or message that appears with the hashtag. "My arms are getting darker by the minute. #toomuchfaketan". Another function of the hashtag can be used to express personal feelings and emotions. For example, with "It\'s Monday!! #excited #sarcasm" in which the adjectives are directly indicating the emotions of the speaker. It can also be used as a disclaimer of the information that the hashtag accompanies, as in, "BREAKING: US GDP growth is back! #kidding". In this case, the hashtag provides an essential piece of information in which the meaning of the utterance is changed entirely by the disclaimer hashtag. This may also be conveyed with #sarcasm, as in the previous example. Self-mockery is another informal function of the hashtag used by writers, as in this tweet: "Feeling great about myself till I met an old friend who now races at the Master\'s level. Yup, there\'s today\'s #lessoninhumility," where the informality of the hashtag provides commentary on the tweet itself.<ref name=":0">{{Cite web|url=http://www.skase.sk/Volumes/JTL28/pdf_doc/05.pdf|title=The ‘hashtag’: A new word or a new rule?|last=Caleffi|first=Paola-Maria|date=|website=Skase Journal of Theoretical Linguistics|publisher=|access-date=}}</ref>\n\n== Other uses ==\nThe feature has been added to other, non-short-message-oriented services, such as the user comment systems on [[YouTube]] and [[Gawker Media]]. In the case of the latter, hashtags for blog comments and directly submitted comments are used to maintain a more constant rate of user activity even when paid employees are not logged into the website.<ref>{{cite web|url = http://gawker.com/5382267/anarchy-in-the-machine-welcome-to-gawkers-open-forums|title = Anarchy in the Machine: Welcome to Gawker\'s Open Forums|author = Gabriel Snyder|publisher = Gawker|date = October 15, 2009<!-- 3:25 PM-->}}</ref><ref>{{cite web|url = http://www.niemanlab.org/2009/10/got-a-tip-gawker-media-opens-tag-pages-to-masses-expecting-chaos/|title = Got a #tip? Gawker Media opens tag pages to masses, expecting "chaos"|author = Zachary M. Seward|publisher = Nieman Journalism Lab|date = October 15, 2009 <!-- 8 a.m. -->}}</ref> Real-time search aggregators such as the former [[Google Real-Time Search]] also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the "river" of Twitter posts that can result from search terms or hashtags.{{citation needed|date=September 2014}}\n\n== Uses ==\n\n=== Broadcast media ===\n\nThe use of hashtags has extended to [[television]]{{nsmdns}}a concept that began rising in prominence in the early 2010s. Broadcasters may display a hashtag as an on-screen [[digital on-screen graphic|bug]], encouraging viewers to participate in a [[backchannel]] of discussion via social media prior to, during, or after the program. [[Television commercial]]s have sometimes contained hashtags for similar purposes.<ref>{{cite web|url = http://www.tvguide.com/News/New-TV-Screen-1032111.aspx|title = New to Your TV Screen: Twitter Hashtags|date = April 21, 2011<!-- 3:25 PM-->|author = Michael Schneider|publisher = TV Guide}}</ref> Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement.<ref>{{cite web|url = http://mashable.com/2012/12/03/mcdonalds-tv-ad-twitter-hashtag/|title = McDonald\'s Releases First TV Ad With Twitter Hashtag|date = Dec 3, 2012|author = Todd Wasserman|publisher = Mashable}}</ref>\n\nWhile personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames in order to receive mentions and replies to posts, usage of related or "branded" hashtags alongside Twitter usernames (e.g., [[The Ed Show|#edshow]] as well as [[Ed Schultz|@edshow]]) is increasingly encouraged as a microblogging style in order to "trend" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style in order to index select posts for live broadcast. Chloe Sladden, Twitter\'s director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. [[It\'s Always Sunny in Philadelphia|#SunnyFX]]) and instantaneous, "temporary" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts.<ref>{{cite web|url = http://www.fastcompany.com/1747437/twitter-tv-hashtag-tips-twitters-own-expert|title = Twitter TV Hashtag Tips From Twitter\'s Own Expert|author = Gregory Ferenstein|date = April 15, 2011|publisher = Fast Company}}</ref> Some have speculated that hashtags might take the place of (or co-exist with) the [[Nielsen ratings|Nielsen television ratings system]].<ref>{{cite web|url=http://www.ibtimes.com/twitter-chatter-correlates-tv-ratings-good-or-bad-news-nielsen-1144311 |title=Twitter Chatter Correlates With TV Ratings, But Is That Good Or Bad News For Nielsen? |work=International Business Times |date=March 22, 2013 |accessdate=September 19, 2013}}</ref>\n\nAn example of trending "temporary" hashtags garnering viewers during broadcasts is observed on \'\'[[The Tonight Show]]\'\' with [[Jimmy Fallon]], a variety [[talk show]] on [[NBC]]. Every Wednesday, Fallon hosts a segment on his show called "Tonight Show Hashtags," which engages viewers by inviting them via Twitter to post humorous stories based on a specific hashtag topic, such as #WhydidIsaythat, #Worstfirstdate, to #Onetimeinclass, reflecting on funny experiences in daily life. By using hashtags, Fallon creates a sense of community and solidarity among his viewers and draws a wider range of viewers through an online platform while they watch a classic, non-interactive television program. Because of its popularity, the "Tonight Show Hashtags" are usually the \'most tweeted hashtag\' on Twitter, which promotes the show. By engaging viewers with a lighthearted subject and simple hashtags, Fallon can gauge topical responses from viewers during broadcasts and also use the hashtags to brand his show.{{citation needed|date=April 2016}}\n\nThe increased usage of hashtags as brand promotion devices has been compared to the promotion of branded "[[Index term|keywords]]" by [[AOL]] in the late 1990s and early 2000s, as such keywords were also promoted at the end of television commercials and series episodes.<ref>{{cite web|url = http://techcrunch.com/2012/06/10/twitter-hashtag-pages-aol-keywords/|title = Twitter\'s Hashtag Pages Could Be The New AOL Keywords — But Better|author = Ryan Lawler|date = June 10, 2012|publisher = Techcrunch}}</ref>\n\nThe late-night television comedy [[game show]] [[@midnight]] with [[Chris Hardwick]] on [[Comedy Central]] features a daily game entitled "Hashtag Wars," in which three comedians compete against one another to come up with phrases based on a given hashtag theme.\n\nSome hashtags have become famous worldwide. For instance the slogan "\'\'[[Je suis Charlie]],\'\'" which was first used on Twitter as the hashtag #jesuischarlie and #iamcharlie to indicate solidarity with \'\'Charlie Hebdo\'\' offices attacked in Paris, spread to the internet at large.\n\n=== Purchasing ===\n\nSince February 2013 Twitter and [[American Express]] have collaborated to enable users to pay for discounted goods online by tweeting a special hashtag.<ref>{{cite news | first = Kelly | last = Heather | title = Twitter and Amex let you pay with a hashtag | date = February 12, 2013 | url = http://edition.cnn.com/2013/02/11/tech/social-media/twitter-hashtag-purchases/| work = CNN | accessdate = November 25, 2013}}</ref> American Express members can sync their card with Twitter and pay for offers by tweeting; American Express tweets a response to the member that confirms the purchase.<ref>{{cite web|url=https://sync.americanexpress.com/Twitter/Index |title=Sync with Twitter|publisher=Amex Sync |accessdate=November 25, 2013}}</ref>\n\n=== Event promotion ===\n\n[[File:Occupy for Rights.JPG|thumb|[[Stencil graffiti]] promoting the hashtag #OccupyForRights]]\n\nOrganized real-world events have used hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants in order to find each other, both on Twitter and, in many cases, during actual physical events.\n\nCompanies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.\n\nPolitical protests and campaigns in the early 2010s, such as [[Occupy Wall Street|#OccupyWallStreet]] and [[2011 Libyan civil war|#LibyaFeb17]], have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion.\n\n=== Consumer complaints ===\nHashtags are often used by consumers on social media platforms in order to complain about the customer service experience with large companies.  The term "bashtag" has been created to describe situations in which a user refers to a corporate social media hashtag in order to criticise the company or to tell others about poor customer service. For example, in January 2012, [[McDonald\'s]] created the #McDStories hashtag so that customers could share positive experiences about the restaurant chain. But, the marketing effort was cancelled after two hours when McDonald\'s received numerous complaint tweets rather than the positive stories they were anticipating.<ref>{{cite news | first = Alexis | last = Akwagyiram | title = Are Twitter and Facebook changing the way we complain? | date = May 17, 2012 | url = http://www.bbc.co.uk/news/uk-18081651 | work = BBC News | accessdate = June 12, 2012}}</ref>\n\n=== Sentiment analysis ===\nThe use of hashtags also reveals what feelings or sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is [[sarcasm|sarcastic]].<ref>{{cite journal|last=Maynard|title=Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis|journal=Proceedings of the Conference on Language Resources and Evaluation|year=2014}}</ref>—a difficult [[Artificial Intelligence|AI]] problem.<ref>http://www.huffingtonpost.com/entry/power-yourself-with-viral-marketing-become-a-hashtag_us_57bf13e6e4b06384eb3e7f1d?dxrywr9zcw30rizfr</ref>\n\n=== Sports ===\nThe YouTuber Spencer FC used the hashtag for the name and crest of his YouTube-based association football team, [[Hashtag United]].\n\n== In popular culture ==\nDuring the [[2011 Canadian leaders debates|April 2011 Canadian party leader debate]], [[Jack Layton]], then-leader of the [[New Democratic Party of Canada|New Democratic Party]], referred to [[Conservative Party of Canada|Conservative]] Prime Minister [[Stephen Harper]]\'s crime policies as "a hashtag fail" (presumably #fail).<ref>{{cite news|url = http://www.theglobeandmail.com/news/politics/jack-laytons-debatable-hashtag-fail/article576224/|title = Jack Layton\'s debatable \'hashtag\' #fail|author = Anna Mehler Paperny|publisher = The Globe and Mail|date = April 13, 2011 <!-- , 6:00 AM EDT --> }}</ref><ref>{{cite news|url = http://www.cbc.ca/news/politics/canadavotes2011/story/2011/04/13/cv-debate-twitter.html|title = Canadians atwitter throughout debate|date = April 13, 2011<!-- 3:25 PM-->|publisher = CBC News}}</ref>\n\nThe term "hashtag [[Hip hop music|rap]]", coined by [[Kanye West]],<ref>{{cite web |url = http://blogs.villagevoice.com/music/2010/11/the_ten_best_qu.php|title = The Ten Best Quotes From Kanye West\'s Epic Hot 97 Interview With Funkmaster Flex|author = Zach Baron|publisher = The Village Voice|date = November 3, 2010}}</ref> was developed in the 2010s to describe a style of rapping which, according to Rizoh of the \'\'[[Houston Press]],\'\' uses "three main ingredients: a metaphor, a pause, and a one-word [[punch line]], often placed at the end of a rhyme".<ref>{{cite web|url = http://blogs.houstonpress.com/rocks/2011/07/a_brief_history_of_hashtag_rap.php|title = A Brief History Of Hashtag Rap|author = Rizoh|publisher = Houston Press|date = July 7, 2011 <!-- at 9:00 AM --> }}</ref> Rappers [[Nicki Minaj]], [[Big Sean]], [[Drake (rapper)|Drake]], and [[Lil Wayne]] are credited with the popularization of hashtag rap, while the style has been criticized by [[Ludacris]], [[The Lonely Island]],<ref>{{cite web|url = http://www.tucsonweekly.com/TheRange/archives/2013/05/22/the-lonely-island-puts-hashtag-rap-in-its-place-looking-at-you-drake|title = The Lonely Island Puts Hashtag Rap In Its Place (Looking at You, Drake)|author = David Mendez|date = May 22, 2013 <!-- AT 11:43 AM --> |publisher = Tucson Weekly}}</ref> and various music writers.<ref>{{cite web|url = http://www.joplinglobe.com/enjoy/x1666506743/Jeremiah-Tucker-Hashtag-rap-is-2010s-lamest-trend|title = Jeremiah Tucker: Hashtag rap is 2010\'s lamest trend|author = Jeremiah Tucker|date = December 17, 2010|publisher = Joplin Globe}}</ref>\n\nOn September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a \'\'[[The New York Times|New York Times]]\'\' front page article regarding Twitter\'s [[initial public offering]].<ref>{{cite web\n| title = Twitter / nickbilton: My first byline on A1 of the …\n| url = https://twitter.com/nickbilton/status/378534272962793472/photo/1\n| accessdate = September 14, 2013\n }}</ref>\n\n[[Bird\'s Eye]] foods released in 2014 a shaped [[mashed potato]] food that included forms of @-symbols and hashtags, called "Mashtags".<ref>{{cite web|title=Birds Eye launches Mashtags – social media potato shapes|url=http://www.thegrocer.co.uk/fmcg/birds-eye-launches-mashtags-potato-shapes/354514.article|work=The Grocer}}</ref>\n\nIn May 2014, Twitter users began using the hashtag [[YesAllWomen|#YesAllWomen]] to raise awareness about personal experiences of [[sexism]] and [[violence against women]].<ref name="Nytimes">{{cite news |last=Medina| first=Jennifer | title = Campus Killings Set Off Anguished Conversation About the Treatment of Women | work = [[The New York Times]] | accessdate = September 23, 2014 | date = May 27, 2014 | url =http://www.nytimes.com/2014/05/27/us/campus-killings-set-off-anguished-conversation-about-the-treatment-of-women.html?ref=us&_r=0 }}</ref>\n\nIn September 2014, in response to the "[[blame the victim]]" public reactions to videotaped footage of [[NFL]] player [[Ray Rice]] assaulting his then-fiancée Janay Palmer in the elevator of an [[Atlantic City]] casino, Beverly Gooden shared on Twitter her own story of [[domestic abuse]], using the hashtag #WhyIStayed, and encouraged others to share theirs.<ref>{{cite news|work=Today|title=WhyIStayed: Woman behind Ray Rice-inspired hashtag writes to past self, other abuse victims|author=Gooden, Beverly|date=September 10, 2014| url= http://www.today.com/news/whyistayed-woman-behind-ray-rice-inspired-hashtag-writes-past-self-1D80139011}}</ref><ref>{{cite news|work=The Leonard Lopate Show|authors=Lopate, Leonard & Gooden, Beverly|title=#WhyIStayed|date=September 10, 2014}}</ref>\n\nHashtags have been used verbally to make a humorous point in informal conversations,<ref>[http://www.macmillandictionary.com/dictionary/british/hashtag]</ref> such as "I’m hashtag confused!"<ref name=":0" /> In August 2012, British journalist Tom Meltzer reported in \'\'[[The Guardian]]\'\' about a new [[hand gesture]] that mimicked the hashtag, sometimes called the "finger hashtag", in which both hands form a [[Peace sign#The V sign|peace sign]], and then the fingers are crossed to form the symbol of a hashtag.<ref>{{cite web |url=https://www.theguardian.com/technology/shortcuts/2012/aug/01/how-to-say-hashtag-fingers |title=How to say \'hashtag\' with your fingers |work=[[The Guardian]] |author=Tom Meltzer |date=August 1, 2012 |accessdate=March 20, 2014}}</ref> The emerging gesture was reported about in \'\'[[Wired (magazine)|Wired]]\'\' by [[Nimrod Kamer]],<ref>{{cite web |url=http://www.wired.co.uk/news/archive/2013-03/06/hashtags |title=Finger-Hashtags |work=[[Wired (magazine)|Wired]] |author=[[Nimrod Kamer]] |date=March 2013 |accessdate=March 20, 2014}}</ref> and during 2013, it was seen on TV as used by [[Jimmy Fallon]], and on \'\'[[The Colbert Report]],\'\' among other programs.<ref>{{cite web |url=http://www.dailydot.com/lol/finger-hashtag-jimmy-fallon-twitter/ |title=I invented finger hashtags—and I regret nothing |work=[[The Daily Dot]] |author=[[Nimrod Kamer]] |date=February 26, 2014 |accessdate=March 20, 2014}}</ref> Writing in 2015, Paola Maria Caleff considered this usage a [[fad]], but noted that people talking the way that they write was a consequence of computer-mediated communication.<ref name=":0" />\n\n=== Adaptations ===\n*Hashflags: In 2010, Twitter introduced "hashflags" during the 2010 World Cup in South Africa.<ref>{{cite web|author=|url=http://www.ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |title=Twitter Supports World Cup Fever with Hashflags |publisher=Ryanseacrest.com |date=June 11, 2010 |accessdate=August 5, 2015 |archiveurl=https://web.archive.org/web/20101129201517/http://ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |archivedate=November 29, 2010}}</ref> They reintroduced the feature on June 10, 2014, in time for the 2014 World Cup in Brazil,<ref>{{cite web|url=http://howto.digidefen.se/twitter/What-are-hashflags.php |title=What are Hashflags? |publisher=Howto.digidefen.se |date=June 10, 2014 |accessdate=August 25, 2014}}</ref><ref>{{cite web|author=Ben Woods |url=http://thenextweb.com/twitter/2014/06/10/twitter-brings-back-hashflags-just-time-world-cup-2014-kick/ |title=Twitter brings back hashflags just in time for World Cup 2014 kick-off |publisher=Thenextweb.com |date=June 10, 2014 |accessdate=August 25, 2014}}</ref> and then again on April 10, 2015, with UK political party logos for the 2015 UK General Election.<ref>{{cite web|title=Twitter just launched election hashflags|url=http://www.bbc.co.uk/newsbeat/article/32249518/twitter-just-launched-election-hashflags|website=BBC News|accessdate=April 15, 2015}}</ref> When a user tweets a hashtag consisting of the three letter country code of any of the 32 countries represented in the tournament, Twitter automatically embeds a flag emoticon for that country.\n*Cashtags: In 2009, [[StockTwits]] used [[ticker symbol]]s preceded by the [[dollar sign]] to create "cashtags".<ref name=Wong2012>{{cite journal |author=Wong, Matthew |title=VCs and Start-Ups Pin Their Hopes on Pinterest |date=2012-08-17 |work=[[The Wall Street Journal]] |url=http://blogs.wsj.com/venturecapital/2012/08/17/vcs-and-start-ups-pin-their-hopes-on-pinterest/ |accessdate=2013-05-28 }}</ref><ref name=Taylor2012>{{cite journal |author=Taylor, Colleen |title=Howard Lindzon on Why He Sold His Twitter Stock, And The \'Hijack\' Of StockTwits’ Cashtags [TCTV] |date=2012-07-01 |publisher=[[TechCrunch]] |url=http://techcrunch.com/2012/08/01/howard-lindzon-on-why-he-sold-his-twitter-stock-and-the-hijack-of-stocktwits-cashtags-tctv/ |accessdate=2013-05-09 }}</ref> In July 2012, Twitter adapted the hashtag style to make company ticker symbols preceded by the dollar sign clickable (as in [[Apple, Inc.|$AAPL]]), a method that Twitter dubbed the "cashtag".<ref>{{cite web|last=Kim |first=Erin |url=http://money.cnn.com/2012/07/31/technology/twitter-cashtag/ |title=Twitter unveils \'cashtags\' to track stock symbols – Jul. 31, 2012 |publisher=Money.cnn.com |date=July 31, 2012 |accessdate=November 12, 2013}}</ref><ref>{{cite web|author= |url=http://www.theverge.com/2012/7/30/3205284/twitter-stock-ticker-cashtag-links-official |title=Twitter makes stock symbol $ \'cashtag\' links official, following # and @ |publisher=The Verge |date=July 30, 2012 |accessdate=November 12, 2013}}</ref> This is intended to allow users to search posts discussing companies and their stocks. This is also used for discussion of currency fluctuations on twitter, eg. using #USDGBP or $USDGBP when mentioning the US Dollar\'s level expressed in Pounds Sterling.\n\n== References ==\n\n{{Reflist|30em}}\n\n== External links ==\n\n{{Commons category|Hashtags}}\n\n* [//tools.wmflabs.org/hashtags/search/artandfeminism Wikipedia internal hashtag search engine] – for hashtags used in edit summaries\n\n{{Microblogging}}\n{{Online social networking}}\n{{Web syndication}}\n{{authority control}}\n\n[[Category:Hashtags| ]]\n[[Category:2010s slang]]\n[[Category:Collective intelligence]]\n[[Category:Computer jargon]]\n[[Category:Information retrieval techniques]]\n[[Category:Knowledge representation]]\n[[Category:Metadata]]\n[[Category:Reference]]\n[[Category:Social media]]\n[[Category:Web 2.0]]\n[[Category:Twitter]]']
['Personalized search', '28010520', '{{Multiple issues|\n{{essay-like|date=January 2015}}\n{{original research|date=January 2015}}\n}}\n\n\'\'\'Personalized search\'\'\' refers to [[web search]] experiences that are tailored specifically to an individual\'s interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to [[personalizing]] search results, one involving modifying the user\'s query and the other re-ranking search results.<ref>{{cite journal|last=Pitokow|first=James|author2=Hinrich Schütze |author3=Todd Cass |author4=Rob Cooley |author5=Don Turnbull |author6=Andy Edmonds |author7=Eytan Adar |author8=Thomas Breuel |title=Personalized search|journal=Communications of the ACM |year=2002|volume=45|issue=9|pages=50–55|url=http://portal.acm.org/citation.cfm?doid=567498.567526}}</ref>\n\n==History==\n\n[[Google]] introduced personalized search in 2004 and it was implemented in 2005 to Google search. Google has personalized search implemented for all users, not only those with a Google account. There is not very much information on how exactly Google personalizes their searches; however, it is believed that they use user language, location, and [[web history]].<ref>{{cite conference | url=http://personalization.ccs.neu.edu/paper.pdf | title=Measuring Personalization of Web Search | year=2013 | archiveurl=https://web.archive.org/web/20130425195202/http://personalization.ccs.neu.edu/paper.pdf | archivedate=April 25, 2013 | deadurl=y|author1=Aniko Hannak|author2=Piotr Sapiezynski|author3=Arash Molavi Kakhki|author4=Balachander Krishnamurthy|author5=David Lazer|author6=Alan Mislove|author7=Christo Wilson}}</ref>\n\nEarly [[search engine]]s, like [[Google]] and [[AltaVista]], found results based only on key words. Personalized search, as pioneered by Google, has become far more complex with the goal to "understand exactly what you mean and give you exactly what you want."<ref name=Remerowski>{{cite AV media| last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Using mathematical algorithms, search engines are now able to return results based on the number of links to and from sites; the more links a site has, the higher it is placed on the page.<ref name=Remerowski/> Search engines have two degrees of expertise: the shallow expert and the deep expert. An expert from the shallowest degree serves as a witness who knows some specific information on a given event. A deep expert, on the other hand, has comprehensible knowledge that gives it the capacity to deliver unique information that is relevant to each individual inquirer.<ref name=Simpson>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969–982}}</ref> If a person knows what he or she wants than the search engine will act as a shallow expert and simply locate that information. But search engines are also capable of deep expertise in that they rank results indicating that those near the top are more relevant to a user\'s wants than those below.<ref name=Simpson/>\n\nWhile many search engines take advantage of information about people in general, or about specific groups of people, personalized search depends on a user profile that is unique to the individual. Research systems that personalize search results model their users in different ways. Some rely on users explicitly specifying their interests or on demographic/cognitive characteristics.<ref>{{cite journal|last=Ma|first=Z.|author2=Pant, G. |author3=Sheng, O. |title=Interest-based personalized search.|journal=ACM TOIS|year=2007|volume=25|issue=5}}</ref><ref>{{cite journal|last=Frias-Martinez|first=E.|author2=Chen, S.Y. |author3=Liu, X. |title=Automatic cognitive style identification of digital library users for personalization.|journal=JASIST|year=2007|volume=58|issue=2|pages=237–251|doi=10.1002/asi.20477}}</ref> However, user-supplied information can be difficult to collect and keep up to date. Others have built implicit user models based on content the user has read or their history of interaction with Web pages.<ref>{{cite journal|last=Chirita|first=P.|author2=Firan, C. |author3=Nejdl, W. |title=Summarizing local context to personalize global Web search|journal=SIGIR|year=2006|pages=287–296}}</ref><ref>{{cite journal|last=Dou|first=Z.|author2=Song, R. |author3=Wen, J.R. |title=A large-scale evaluation and analysis of personalized search strategies|journal=WWW|year=2007|pages=581–590}}</ref><ref>{{cite journal|last=Shen|first=X. |author2=Tan, B. |author3=Zhai, C.X.|title=Implicit user modeling for personalized search|journal=CIKM|year=2005|pages=824–831}}</ref><ref>{{cite journal|last=Sugiyama|first=K.|author2=Hatano, K. |author3=Yoshikawa, M. |title=Adaptive web search based on user profile constructed without any effort from the user|journal=WWW|year=2004|pages=675–684}}</ref><ref>{{cite journal|last=Teevan|first=J.|author2=Dumais, S.T. |author3=Horvitz, E. |title=Personalizing search via automated analysis of interests and activities|journal=SIGIR|year=2005|pages=415–422|url=http://people.csail.mit.edu/teevan/work/publications/papers/tochi10.pdf}}</ref>\n\nThere are several publicly available systems for personalizing Web search results (e.g., [[Google Personalized Search]] and [[Bing (search engine)|Bing]]\'s search result personalization<ref>{{cite web|last=Crook|first=Aidan, and Sanaz Ahari|title=Making search yours|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx|publisher=Bing|accessdate=14 March 2011}}</ref>). However, the technical details and evaluations of these commercial systems are proprietary. One technique Google uses to personalize searches for its users is to track log in time and if the user has enabled web history in his browser. If a user accesses the same site through a search result from Google many times, it believes that they like that page. So when users carry out certain searches, Google\'s personalized search algorithm gives the page a boost, moving it up through the ranks. Even if a user is signed out, Google may personalize their results because it keeps a 180-day record of what a particular web browser has searched for, linked to a cookie in that browser.<ref>{{cite web|last=Sullivan|first=Danny|title=Of "Magic Keywords" and Flavors Of Personalized Search At Google|url=http://searchengineland.com/flavors-of-google-personalized-search-139286|accessdate=21 April 2014}}</ref>\n\nIn order to better understand how personalized search results are being presented to the users, a group of researchers at Northeastern University compared an aggregate set of searches from logged in users against a [[control group]]. The research team found that 11.7% of results show differences due to personalization; however, this varies widely by [[Web search query|search query]] and result ranking position.<ref name=Briggs>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|date=24 June 2013|accessdate=21 April 2014}}</ref> Of various factors tested, the two that had measurable impact were being logged in with a Google account and the [[IP address]] of the searching users. It should also be noted that results with high degrees of personalization include companies and politics. One of the factors driving personalization is localization of results, with company queries showing store locations relevant to the location of the user. So, for example, if a user searched for "used car sales", Google may produce results of local car dealerships in their area. On the other hand, queries with the least amount of personalization include factual queries ("what is") and health.<ref name=Briggs/>\n\nWhen measuring personalization, it is important to eliminate background noise. In this context, one type of background noise is the carry-over effect. The carry-over effect can be defined as follows: when a user performs a search and follow it with a subsequent search, the results of the second search is influenced by the first search. A noteworthy point is that the top-ranked [[URL]]s are less likely to change based off personalization, with most personalization occurring at the lower ranks. This is a style of personalization based on recent search history, but it is not a consistent element of personalization because the phenomenon times out after 10 minutes, according to the researchers.<ref name=Briggs/>\n\n==The filter bubble==\n{{Main article|Filter bubble}}\n\nSeveral concerns have been brought up regarding personalized search. It decreases the likelihood of finding new information by [[bias]]ing search results towards what the user has already found. It introduces potential privacy problems in which a user may not be aware that their search results are personalized for them, and wonder why the things that they are interested in have become so relevant. Such a problem has been coined as the "filter bubble" by author [[Eli Pariser]]. He argues that people are letting major websites drive their destiny and make decisions based on the vast amount of data they\'ve collected on individuals. This can isolate users in their own worlds or "filter bubbles" where they only see information that they want to, such a consequence of "The Friendly World Syndrome". As a result, people are much less informed of problems in the developing world which can further widen the gap between the North (developed countries) and the South (developing countries).<ref name=Pariser>{{cite book| url=http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf | title=The Filter Bubble | archiveurl=https://web.archive.org/web/20131228150122/http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf|archivedate=December 28, 2013|author=E. Pariser|year=2011}}</ref>\n\nThe methods of personalization, and how useful it is to "promote" certain results which have been showing up regularly in searches by like-minded individuals in the same community. The personalization method makes it very easy to understand how the filter bubble is created. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.<ref>{{cite journal|last=Smyth|first=B.|title=Adaptive Information Access:: Personalization And Privacy |journal=International Journal of Pattern Recognition & Artificial Intelligence |year=2007|pages=183–205}}</ref>\n\nAn area of particular concern to some parts of the world is the use of personalized search as a form of control over the people utilizing the search by only giving them particular information ([[selective exposure]]). This can be used to give particular influence over highly talked about topics such as gun control or even gear people to side with a particular political regime in different countries.<ref name=Pariser/> While total control by a particular government just from personalized search is a stretch, control of the information readily available from searches can easily be controlled by the richest corporations. The biggest example of a corporation controlling the information is Google. Google is not only feeding you the information they want but they are at times using your personalized search to gear you towards their own companies or affiliates. This has led to a complete control of various parts of the web and a pushing out of their competitors such as how Google Maps took a major control over the online map and direction industry with MapQuest and others forced to take a backseat.<ref name="Consumer Watchdog">{{cite web| title=Traffic Report: How Google is squeezing out competitors and muscling into new markets|url= http://www.consumerwatchdog.org/resources/TrafficStudy-Google.pdf|date=2 June 2010|accessdate= 27 April 2014|work=Consumer Watchdog}}</ref>\n\nMany search engines use concept-based user profiling strategies that derive only topics that users are highly interested in but for best results, according to researchers Wai-Tin and Dik Lun, both positive and negative preferences should be considered. Such profiles, applying negative and positive preferences, result in highest quality and most relevant results by separating alike queries from unalike queries. For example, typing in \'apple\' could refer to either the fruit or the [[Macintosh]] computer and providing both preferences aids search engines\' ability to learn which apple the user is really looking for based on the links clicked. One concept-strategy the researchers came up with to improve personalized search and yield both positive and negative preferences is the click-based method. This method captures a user\'s interests based on which links they click on in a results list, while downgrading unclicked links.<ref>{{cite journal|last=Wai-Tin|first=Kenneth|author2=Dik Lun, L|title=Deriving concept-based user profiles from search engine logs|journal=IEEE Transactions on Knowledge and Data Engineering|year=2010|volume=22|issue=7|pages=969–982|doi=10.1109/tkde.2009.144}}</ref>\n\nThe feature also has profound effects on the [[search engine optimization]] industry, due to the fact that search results will no longer be ranked the same way for every user.<ref>[http://www.networkworld.com/news/2009/120709-google-personalized-results-could-be.html "Google Personalized Results Could Be Bad for Search"]. \'\'Network World\'\'. Retrieved July 12, 2010.</ref> An example of this is found in Eli Pariser\'s, The Filter Bubble, where he had two friends type in "BP" into Google\'s search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.<ref name=Pariser/>\n\nSome have noted that personalized search results not only serve to customize a user\'s search results, but also [[Advertising|advertisements]].  This has been criticized as an [[Expectation of privacy|invasion on privacy]].<ref>{{cite web|url=http://www.seooptimizers.com/search-engines-and-customized-results-based-on-your-internet-history.html|title=Search Engines and Customized Results Based on Your Internet History|publisher=SEO Optimizers|accessdate=27 February 2013}}</ref>\n\n==The case of Google==\n{{Main article|Google Personalized Search}}\n\nAn important example of search personalization is [[Google]]. There are a host of Google applications, all of which can be personalized and integrated with the help of a Google account. Personalizing search does not require an account. However, one is almost deprived of a choice, since so many useful Google products are only accessible if one has a Google account. The Google Dashboard, introduced in 2009, covers more than 20 products and services, including Gmail, Calendar, Docs, YouTube, etc.<ref>{{cite journal|last=Mattison|first=D.|title=Time, Space, And Google: Toward A Real-Time, Synchronous, Personalized, Collaborative Web. |journal=Searcher|year=2010|pages=20–31}}</ref> that keeps track of all the information directly under one\'s name. The free Google Custom Search is available for individuals and big companies alike, providing the Search facility for individual websites and powering corporate sites such as that of the \'\'[[New York Times]]\'\'. The high level of personalization that was available with Google played a significant part in helping remain the world\'s most favorite search engine.\n\nOne example of Google\'s ability to personalize searches is in its use of Google News. Google has geared its news to show everyone a few similar articles that can be deemed interesting, but as soon as the user scrolls down, it can be seen that the news articles begin to differ. Google takes into account past searches as well as the location of the user to make sure that local news gets to them first. This can lead to a much easier search and less time going through all of the news to find the information one want. The concern, however, is that the very important information can be held back because it does not match the criteria that the program sets for the particular user. This can create the "[[filter bubble]]" as described earlier.<ref name=Pariser/>\n\nAn interesting point about personalization that often gets overlooked is the privacy vs personalization battle. While the two do not have to be mutually exclusive, it is often the case that as one becomes more prominent, it compromises the other. Google provides a host of services to people, and many of these services do not require information to be collected about a person to be customizable. Since there is no threat of privacy invasion with these services, the balance has been tipped to favor personalization over privacy, even when it comes to search. As people reap the rewards of convenience from customizing their other Google services, they desire better search results, even if it comes at the expense of private information. Where to draw the line between the information versus search results tradeoff is new territory and Google gets to make that decision. Until people get the power to control the information that is being collected about them, Google is not truly protecting privacy.\nGoogle\'s popularity as a search engine and Internet browser has allowed it to gain a lot of power. Their popularity has created millions of usernames, which have been used to collect vast amounts of information about individuals. Google can use multiple methods of personalization such as traditional, social, geographic, IP address, browser, cookies, time of day, year, behavioral, query history, bookmarks, and more. Although having Google personalize search results based on what users searched previously may have its benefits, there are negatives that come with it.<ref>{{cite web|last=Jackson|first=Mark|title=The Future of Google\'s Search Personalization|url=http://searchenginewatch.com/article/2067001/The-Future-of-Googles-Search-Personalization|accessdate=29 April 2014}}</ref><ref>{{cite web|last=Harry|first=David|title=Search Personalization and the User Experience|url=http://searchenginewatch.com/article/2118126/Search-Personalization-the-User-Experience|accessdate=29 April 2014}}</ref>\nWith the power from this information, Google has chosen to enter other sectors it owned, such as videos, document sharing, shopping, maps, and many more. Google has done this by steering searchers to their own services offered as opposed to others such as MapQuest.\n\nUsing search personalization, Google has doubled its video market share to about eighty percent. The legal definition of a [[monopoly]] is when a firm gains control of seventy to eighty percent of the market. Google has reinforced this monopoly by creating significant barriers of entry such as manipulating search results to show their own services. This can be clearly seen with Google Maps being the first thing displayed in most searches.\n\nThe analytical firm Experian Hitwise stated that since 2007, MapQuest has had its traffic cut in half because of this. Other statistics from around the same time include Photobucket going from twenty percent of market share to only three percent, Myspace going from twelve percent market share to less than one percent, and ESPN from eight percent to four percent market share. In terms of images, Photobucket went from 31% in 2007 to 10% in 2010 and Yahoo Images has gone from 12% to 7%. It becomes apparent that the decline of these companies has come because of Google\'s increase in market share from 43% in 2007 to about 55% in 2009.\n\nIt can be said that Google is more dominant because they provide better services. However, Experian Hitwise has also created graphs to show the market share of about fifteen different companies at once. This has been done for every category for the market share of pictures, videos, product search, and more. The graph for product search is evidence enough for Google\'s influence because their numbers went from 1.3 million unique visitors to 11.9 unique visitors in one month. That kind of growth can only come with the change of a process.\n\nIn the end, there are two common themes with all of these graphs. The first is that Google\'s market share has a directly inverse relationship to the market share of the leading competitors. The second is that this directly inverse relationship began around 2007, which is around the time that Google began to use its "Universal Search" method.<ref>{{cite web|title=TRAFFIC REPORT:How Google is Squeezing out Competitors and Muscling into New Markets |url=https://courses.lis.illinois.edu/pluginfile.php/226148/mod_resource/content/1/TrafficStudy-Google.pdf|publisher=ConsumerWatchDog.org|accessdate=29 April 2014}}</ref>\n\n==Benefits==\n\nOne of the most critical benefits personalized search has is to improve the quality of decisions consumers make. The internet has made the transaction cost of obtaining information significantly lower than ever. However, human ability to process information has not expanded much.<ref name=Diehl>{{cite journal|author=Diehl, K.|year=2003|title=Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making|journal=Advances In Consumer Research|volume=30|issue=1|pages=166–169}}</ref> When facing overwhelming amount of information, consumers need a sophisticated tool to help them make high quality decisions. Two studies examined the effects of personalized screening and ordering tools, and the results show a [[positive correlation]] between personalized search and the quality of consumers\' decisions.\n\nThe first study was conducted by Kristin Diehl from the [[University of South Carolina]]. Her research discovered that reducing search cost led to lower quality choices. The reason behind this discovery was that \'consumers make worse choices because lower search costs cause them to consider inferior options.\' It also showed that if consumers have a specific goal in mind, they would further their search, resulting in an even worse decision.<ref name=Diehl/> The study by Gerald Haubl from the [[University of Alberta]] and Benedict G.C. Dellaert from [[Maastricht University]] mainly focused on recommendation systems. Both studies concluded that a personalized search and recommendation system significantly improved consumers\' decision quality and reduced the number of products inspected.<ref name=Diehl/>\n\n==Models==\n\nPersonalized search gains popularity because of the demand for more relevant information. Research has indicated low success rates among major search engines in providing relevant results; in 52% of 20,000 queries, searchers did not find any relevant results within the documents that Google returned.<ref>{{cite book|author1=Coyle, M.|author2=Smyth, B.|lastauthoramp=y|year=2007|chapter=Information recovery and discovery in collaborative web search|title=Advances in Information Retrieval|pp=356–367|doi=10.1007/978-3-540-71496-5_33|isbn=978-3-540-71494-1|series=Lecture Notes in Computer Science}}</ref> Personalized search can improve search quality significantly and there are mainly two ways to achieve this goal.\n\nThe first model available is based on the users\' historical searches and search locations. People are probably familiar with this model since they often find the results reflecting their current location and previous searches.\n\nThere is another way to personalize search results. In Bracha Shapira and Boaz Zabar\'s "Personalized Search: Integrating Collaboration and Social Networks", Shapira and Zabar focused on a model that utilizes a [[recommendation system]].<ref>{{cite journal|author1=Shapira, B.|author2=Zabar, B.|lastauthoramp=y|year=2011|title=Personalized search: Integrating collaboration and social networks|journal=Journal of the American Society for Information Science & Technology|volume=62|issue=1|pages=146–160|doi=10.1002/asi.21446}}</ref> This model shows results of other users who have searched for similar keywords. The authors examined keyword search, the recommendation system, and the recommendation system with social network working separately and compares the results in terms of search quality. The results show that a personalized search engine with the recommendation system produces better quality results than the standard search engine, and that the recommendation system with social network even improves more.\n\nRecent paper “[https://arxiv.org/abs/1612.03597 Search personalization with embeddings]” shows that a new embedding model for search personalization, where users are embedded on a topical interest space, produces better search results than strong learning-to-rank models.\n\n==Disadvantages==\n\nWhile there are documented benefits of the implementation of search personalization, there are also arguments against its use. The foundation of this argument against its use is because it confines internet users\' search engine results to material that aligns with the users\' interests and history. It limits the users\' ability to become exposed to material that would be relevant to the user\'s search query but due to the fact that some of this material differs from the user\'s interests and history, the material is not displayed to the user. Search personalization takes the objectivity out of the search engine and undermines the engine. "Objectivity matters little when you know what you are looking for, but its lack is problematic when you do not".<ref>{{cite journal|last=Simpson|first=Thomas W.|title=Evaluating Google As An Epistemic Tool|journal=Metaphilosophy|date=2012|volume=43.4|pages=426–445|doi=10.1111/j.1467-9973.2012.01759.x}}</ref> Another criticism of search personalization is that it limits a core function of the web: the collection and sharing of information. Search personalization prevents users from easily accessing all the possible information that is available for a specific search query.  Search personalization adds a bias to user\'s search queries. If a user has a particular set of interests or internet history and uses the web to research a controversial issue, the user\'s search results will reflect that. The user may not be shown both sides of the issue and miss potentially important information if the user\'s interests lean to one side or another. A study done on search personalization and its effects on search results in Google News resulted in different orders of news stories being generated by different users, even though each user entered the same search query. According to Bates, "only 12% of the searchers had the same three stories in the same order. This to me is prima facie evidence that there is filtering going on".<ref>{{cite journal|last=Bates|first=Mary Ellen|title=Is Google Hiding My News?|year=2011|journal=Online|volume=35|issue=6|pages=64}}</ref> If search personalization was not active, all the results in theory should have been the same stories in an identical order.\n\nAnother disadvantage of search personalization is that internet companies such as Google are gathering and potentially selling their users\' internet interests and histories to other companies. This raises a privacy issue concerning whether people are comfortable with companies gathering and selling their internet information without their consent or knowledge.  Many web users are unaware of the use of search personalization and even fewer have knowledge that user data is a valuable commodity for internet companies.\n\n==Sites that use it==\n\nE. Pariser, author of \'\'The Filter Bubble\'\', explains how there are differences that search personalization has on both [[Facebook]] and Google. Facebook implements personalization when it comes to the amount of things people share and what pages they "like". An individual\'s [[social interaction]]s, whose profile they visit the most, who they message or chat with are all indicators that are used when Facebook uses personalization. Rather than what people share being an indicator of what is filtered out, Google takes into consideration what we "click" to filter out what comes up in our searches. In addition, Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and users share what other people want to see. Even while [[tag (metadata)|tag]]ging photographs, Facebook uses personalization and [[face recognition]] that will automatically assign a name to face. In terms of Google, users are provided similar websites and resources based on what they initially click on. There are even other websites that use the filter tactic to better adhere to user preferences. For example, [[Netflix]] also judges from the users search history to suggest movies that they may be interested in for the future. There are sites like [[Amazon.com|Amazon]] and personal [[shopping site]]s also use other peoples history in order to serve their interests better. [[Twitter]] also uses personalization by "suggesting" other people to follow. In addition, based on who one "follows", "tweets" and "retweets" at, Twitter filters out suggestions most relevant to the user.  [[Mark Zuckerberg]], founder of Facebook, believed that people only have one identity. E. Pariser argues that is completely false and search personalization is just another way to prove that isn\'t true. Although personalized search may seem helpful, it is not a very accurate representation of any person. There are instances where people also search things and share things in order to make themselves look better. For example, someone may look up and share political articles and other intellectual articles. There are many sites being used for different purposes and that do not make up one person\'s [[Personal identity|identity]] at all, but provide false representations instead.<ref name=Pariser/>\n\n==Online shopping==\n{{main article|Online shopping}}\nSearch engines such as Google and Yahoo! utilize personalized search to attract possible customers to products that fit their presumed desires. Based on a large amount of collected data aggregated from an individual\'s web clicks, search engines can use personalized search to put advertisements that may pique the interest of an individual. Utilizing personalized search can help consumers find what they want faster, as well as help match up products and services to individuals within more specialized and/or niche markets. Many of these products or services that are sold via personalized online results would struggle to sell in [[brick-and-mortar]] stores. These types of products and services are called long tail items.<ref>{{cite journal|author=Badke, William|title=Personalization and Information Literacy|journal=Online|volume=36|issue=1|page=47|date=February 2012}}</ref> Using personalized search allows faster product and service discoveries for consumers, and reduces the amount of necessary advertisement money spent to reach those consumers. In addition, utilizing personalized search can help companies determine which individuals should be offered online coupon codes to their products and/or services. By tracking if an individual has perused their website, considered purchasing an item, or has previously made a purchase a company can post advertisements on other websites to reach that particular consumer in an attempt to have them make a purchase.\n\nAside from aiding consumers and businesses in finding one another, the search engines that provide personalized search benefit greatly. The more data collected on an individual, the more personalized results will be. In turn, this allows search engines to sell more advertisements because companies understand that they will have a better opportunity to sell to high percentage matched individuals then medium and low percentage matched individuals. This aspect of personalized search angers many scholars, such as William Badke and Eli Pariser, because they believe personalized search is driven by the desire to increase advertisement revenues. In addition, they believe that personalized search results are frequently utilized to sway individuals into using products and services that are offered by the particular search engine company or any other company in partnered with them. For example, Google searching any company with at least one brick-and-mortar location will offer a map portraying the closest company location using the Google Maps service as the first result to the query.<ref"Consumer Watchdog"/> In order to use other mapping services, such as MapQuest, a user would have to dig deeper into the results. Another example pertains to more vague queries. Searching the word "shoes" using the Google search engine will offer several advertisements to shoe companies that pay Google to link their website as a first result to consumer\'s queries.\n\n==References==\n{{reflist|30em}}\n\n{{DEFAULTSORT:Personalized search}}\n[[Category:Information retrieval techniques]]\n[[Category:Internet search engines|*Personalized search]]\n[[Category:Internet terminology]]\n[[Category:Personalized search| ]]']
['Enterprise search', '12101316', '{{original research|date=November 2015}}\n\'\'\'Enterprise search\'\'\' is the practice of making content from multiple enterprise-type sources, such as [[database]]s and [[intranet]]s, searchable to a defined audience.\n\n"Enterprise search" is used to describe the software of search information within an enterprise (though the search function and its results may still be public).<ref>{{cite web|url=http://www.aiim.org/What-is-Enterprise-Search|title=What is Enterprise Search?|publisher=}}</ref> Enterprise search can be contrasted with [[web search]], which applies search technology to documents on the open web, and [[desktop search]], which applies search technology to the content on a single computer.\n\nEnterprise search systems index data and documents from a variety of sources such as: [[file systems]], [[intranets]], [[document management system]]s, [[e-mail]], and [[databases]]. Many enterprise search systems integrate structured and unstructured data in their collections.<ref>[http://www.arma.org/bookstore/files/Delgado.pdf The New Face of Enterprise Search: Bridging Structured and Unstructured Information]</ref> Enterprise search systems also use access controls to enforce a security policy on their users.<ref>{{cite web|url=http://www.ideaeng.com/tabId/98/itemId/118/Mapping-Security-Requirements-to-Enterprise-Search.aspx|title=Security Requirements to Enterprise Search: part 1 - New Idea Engineering|publisher=}}</ref>\n\nEnterprise search can be seen as a type of [[vertical search]] of an enterprise.\n\n==Components of an enterprise search system==\nIn an enterprise search system, content goes through various phases from source repository to search results:\n\n=== Content awareness ===\nContent awareness (or "content collection") is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its [[API]]s. This model is used when realtime indexing is important. In the pull model, the software gathers content from sources using a connector such as a [[web crawler]] or a [[database]] connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content.<ref>{{cite web|url=http://www.information-management.com/issues/20_7/content_management_data_integration_indexing_metadata-10019105-1.html|title=Understanding Content Collection and Indexing|publisher=}}</ref>\n\n=== Content processing and analysis ===\nContent from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve [[Recall (information retrieval)|recall]] or [[Precision (information retrieval)|precision]]. These may include [[stemming]], [[lemmatization]], [[synonym]] expansion, [[entity extraction]], [[part of speech]] tagging.\n\nAs part of processing and analysis, [[tokenization (lexical analysis)|tokenization]] is applied to split the content into [[Lexical analysis#Token|tokens]] which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall.\n\n=== Indexing ===\nThe resulting text is stored in an [[Index (search engine)|index]], which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and [[term frequency]].\n\n=== Query processing ===\nUsing a web page, the user issues a [[Web search query|query]] to the system. The query consists of any terms the user enters as well as navigational actions such as [[faceted search|faceting]] and paging information.\n\n=== Matching ===\nThe processed query is then compared to the stored index, and the search system returns results (or "hits") referencing source documents that match. Some systems are able to present the document as it was indexed.\n\n==Differences from web search==\n{{unreferenced section|date=November 2015}}\nBeyond the difference in the kinds of materials being indexed, enterprise search systems also typically include functionality that is not associated with the mainstream [[web search engine]]s. These include:\n*Adapters to index content from a variety of repositories, such as [[databases]] and [[content management systems]].\n*[[Federated search]], which consists of\n# transforming a query and broadcasting it to a group of disparate databases or external content sources with the appropriate syntax,\n# merging the results collected from the databases,\n# presenting them in a succinct and unified format with minimal duplication, and\n# providing a means, performed either automatically or by the portal user, to sort the merged result set.\n*[[Enterprise bookmarking]], collaborative [[tag (metadata)|tagging]] systems for capturing knowledge about structured and semi-structured enterprise data.\n*[[Entity extraction]] that seeks to locate and classify elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n*[[Faceted search]], a technique for accessing a collection of information represented using a [[faceted classification]], allowing users to explore by filtering available information.\n*Access control, usually in the form of an [[Access control list]] (ACL), is often required to restrict access to documents based on individual user identities. There are many types of access control mechanisms for different content sources making this a complex task to address comprehensively in an enterprise search environment (see below).\n*[[Text clustering]], which groups the top several hundred search results into topics that are computed on the fly from the search-results descriptions, typically titles, excerpts (snippets), and meta-data.  This technique lets users navigate the content by topic rather than by the meta-data that is used in faceting. Clustering compensates for the problem of incompatible meta-data across multiple enterprise repositories, which hinders the usefulness of faceting.\n*[[User interfaces]], which in web search are deliberately kept simple in order not to distract the user from clicking on ads, which generates the revenue.  Although the business model for enterprise search could include showing ads, in practice this is not done.  To enhance end user productivity, enterprise vendors continually experiment with rich UI functionality which occupies significant screen space, which would be problematic for web search.\n\n==Relevance factors for enterprise search==\n{{unreferenced section|date=November 2015}}\nThe factors that determine the relevance of search results within the context of an enterprise overlap with but are different from those that apply to web search. In general, enterprise search engines cannot take advantage of the rich [[hyperlink|link structure]] as is found on the web\'s [[hypertext]] content, however, a new breed of Enterprise search engines based on a bottom-up [[Web 2.0]] technology are providing both a contributory approach and [[hyperlink]]ing within the enterprise. Algorithms like [[PageRank]] exploit hyperlink structure to assign authority to documents, and then use that authority as a query-independent relevance factor. In contrast, enterprises typically have to use other query-independent factors, such as a document\'s recency or popularity, along with query-dependent factors traditionally associated with [[information retrieval]] algorithms.  Also, the rich functionality of enterprise search UIs, such as clustering and faceting, diminish reliance on ranking as the means to direct the user\'s attention.\n\n==Access control: early binding vs late binding==\nSecurity and restricted access to documents is an important matter in enterprise search. There are two main approaches to apply restricted access: early binding vs late binding.<ref>[http://enterprisesearch.co/enterprise-search-document-access-control/ Enterprise Search: document access control]</ref>\n\n===Late binding===\nPermissions are analyzed and assigned to documents at query stage. Query engine generates a document set and before returning it to a user this set is filtered based on user access rights. It is costly process but accurate (based on user permissions at the moment of query).\n\n===Early binding===\nPermissions are analyzed and assigned to documents at indexing stage. It is much more effective than late binding, but could be inaccurate (user might be granted or revoked permissions between in the period between indexing and querying).\n\n==Search relevance testing options==\nSearch application relevance can be determined by following relevance testing options like<ref>[http://searchhub.org/2009/09/02/debugging-search-application-relevance-issues/  Debugging Search Application Relevance Issues]</ref>\n*Focus groups\n*Reference evaluation protocol (based on relevance judgements of results from agreed-upon queries performed against common document corpuses)\n*Empirical testing\n*[[A/B testing]]\n*Log analysis on a Beta production site\n*Online ratings\n\n==See also==\n*[[Collaborative search engine]]\n*[[Comparison of enterprise search software]]\n*[[Data defined storage]] \n*[[Enterprise bookmarking]]\n*[[Enterprise information access]]\n*[[Faceted search]]\n*[[Information extraction]]\n*[[Knowledge management]]\n*[[List of enterprise search vendors]]\n*[[List of search engines]]\n*[[Text mining]]\n*[[Vertical search]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Enterprise Search}}\n[[Category:Information retrieval genres]]']
['Category:Personalized search', '38954543', '{{catmore}}\n\n[[Category:Information retrieval genres]]\n[[Category:Internet search engines]]']
['Human–computer information retrieval', '14473878', '\'\'\'Human-computer information retrieval\'\'\' (\'\'\'HCIR\'\'\') is the study and engineering of [[information retrieval]] techniques that bring human intelligence into the [[search engine|search]] process. It combines the fields of [[human-computer interaction]] (HCI) and information retrieval (IR) and creates systems that improve search by taking into account the human context, or through a multi-step search process that provides the opportunity for human feedback.\n\n== History ==\n\nThis term \'\'human–computer information retrieval\'\' was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006.<ref name=march2006>[http://www.asis.org/Bulletin/Jun-06/marchionini.html Marchionini, G. (2006). Toward Human-Computer Information Retrieval Bulletin, in June/July 2006 Bulletin of the American Society for Information Science]</ref> Marchionini’s main thesis is that "HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy."\n\nIn 1996 and 1998, a pair of workshops at the [[University of Glasgow]] on [[information retrieval]] and [[human–computer interaction]] sought to address the overlap between these two fields. Marchionini notes the impact of the [[World Wide Web]] and the sudden increase in [[information literacy]] – changes that were only embryonic in the late 1990s.\n\nA few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the [[University of Maryland Human-Computer Interaction Lab]] in 2005, alternates between the [[Association for Computing Machinery]] [[Special Interest Group on Information Retrieval]] (SIGIR) and [[CHI (conference)|Special Interest Group on Computer-Human Interaction]] (CHI) conferences. Also in 2005, the [[European Science Foundation]] held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the [[Massachusetts Institute of Technology]].\n\n== Description ==\n\nHCIR includes various aspects of IR and HCI. These include [[exploratory search]], in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as "the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system."<ref name=ingwer1992>[http://vip.db.dk/pi/iri/index.htm Ingwersen, P. (1992). Information Retrieval Interaction. London: Taylor Graham.]</ref>\n\nA key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users.<ref>{{cite web|title=Mira working group (1996). Evaluation Frameworks for Interactive Multimedia Information Retrieval Applications|url=http://www.dcs.gla.ac.uk/mira/}}</ref>\n\nMost modern IR systems employ a [[ranking|ranked]] retrieval model, in which the documents are scored based on the [[probability]] of the document\'s [[relevance]] to the query.<ref>Grossman, D. and Frieder, O. (2004). Information Retrieval Algorithms and Heuristics. </ref> In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their [[Information retrieval#Average precision of precision and recall|mean average precision]] over a set of benchmark queries from organizations like the [[Text Retrieval Conference]] (TREC).\n\nBecause of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models – one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and [[Nicholas J. Belkin]]\'s 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of [[Information retrieval#Precision|precision]] and [[Information retrieval#Recall|recall]] but apply them to the results of multiple iterations of user interaction, rather than to a single query response.<ref name=koene1996>[http://sigchi.org/chi96/proceedings/papers/Koenemann/jk1_txt.htm Koenemann, J. and Belkin, N. J. (1996). A case for interaction: a study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: Common Ground (Vancouver, British Columbia, Canada, April 13–18, 1996). M. J. Tauber, Ed. CHI \'96. ACM Press, New York, NY, 205-212]</ref> Other HCIR research, such as Pia Borlund\'s IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.<ref name=borlund2003>[http://informationr.net/ir/8-3/paper152.html Borlund, P. (2003). The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Information Research, 8(3), Paper 152]</ref>\n\n== Goals ==\nHCIR researchers have put forth the following goals towards a system where the user has more control in determining relevant results.<ref name=march2006/><ref name=ipm2013>[https://dl.acm.org/citation.cfm?id=2504017 White, R., Capra, R., Golovchinsky, G., Kules, B., Smith, C., and Tunkelang, D. (2013). Introduction to Special Issue on Human-computer Information Retrieval. Journal of Information Processing and Management 49(5), 1053-1057]</ref>\n\nSystems should\n*no longer only deliver the relevant documents, but must also provide semantic information along with those documents\n*increase user responsibility as well as control; that is, information systems require human intellectual effort\n*have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases\n*aim to be part of information ecology of personal and [[Collective memory|shared memories]] and tools rather than discrete standalone services\n*support the entire information life cycle (from creation to preservation) rather than only the dissemination or use phase\n*support tuning by end users and especially by information professionals who add value to information resources\n*be engaging and fun to use\n\nIn short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process — the [[information professional]] — to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs).\n\n== Techniques ==\n\nThe techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift.\n\nMany [[search engines]] have features that incorporate HCIR techniques. [[Spelling suggestion]]s and [[query expansion|automatic query reformulation]] provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the user’s hands.\n\n[[Faceted search]] enables users to navigate information [[hierarchy|hierarchically]], going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional [[Taxonomy (general)|taxonomies]] in which the hierarchy of categories is fixed and unchanging. [[Faceted classification|Faceted navigation]], like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking.<ref>Hearst, M. (1999). User Interfaces and Visualization, Chapter 10 of Baeza-Yates, R. and Ribeiro-Neto, B., Modern Information Retrieval.</ref>\n\n[[Combinatorial search#Lookahead|Lookahead]] provides a general approach to penalty-free exploration. For example, various [[web applications]] employ [[Ajax (programming)|AJAX]] to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., [[metadata]] about the objects) and "snippets" of document text that are most pertinent to the words in the search query.\n\n[[Relevance feedback]] allows users to guide an IR system by indicating whether particular results are more or less relevant.<ref>Rocchio, J. (1971). Relevance feedback in information retrieval. In: Salton, G (ed), The SMART Retrieval System.</ref>\n\nSummarization and [[analytics]] help users digest the results that come back from the query. Summarization here is intended to encompass any means of [[aggregate data|aggregating]] or [[data compression|compressing]] the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is [[cluster analysis|clustering]], which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for "java" might return clusters for [[Java (programming language)]], [[Java|Java (island)]], or [[Java (coffee)]].\n\n[[information visualization|Visual representation of data]] is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of [[information visualization]] that allow users access to summary views of search results include [[tag clouds]] and [[treemapping]].\n\n== Related Areas ==\n* [[Exploratory Video Search]]\n\n== References ==\n\n<References/>\n\n==External links==\n*{{cite web|url=https://sites.google.com/site/hcirworkshop/ |title=Workshops on Human Computer Information Retrieval}}\n*{{cite web|url=http://www.chiir.org/ |title=ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)}}\n\n{{DEFAULTSORT:Human-computer information retrieval}}\n[[Category:Information retrieval genres]]\n[[Category:Human–computer interaction]]']
['Information retrieval applications', '13324645', 'Areas where [[information retrieval]] techniques are employed include (the entries are in alphabetical order within each category):\n\n==General applications of information retrieval==\n* [[Digital libraries]]\n*  [[Information filtering]]\n** [[Recommender systems]]\n*  Media search\n** Blog search\n** [[Image retrieval]]\n** [[3D retrieval]]\n** [[Music information retrieval|Music retrieval]]\n** News search\n** Speech retrieval\n** Video retrieval\n* [[Search engines]]\n** [[Site search]]\n** [[Desktop search]]\n** [[Enterprise search]]\n** [[Federated search]]\n** [[Mobile search]]\n** [[Social search]]\n** [[Web search engine|Web search]]\n\n==Domain specific applications of information retrieval==\n* Expert search finding\n* Genomic information retrieval\n* [[Geographic information retrieval]]\n*  Information retrieval for chemical structures\n* Information retrieval in [[software engineering]]\n* [[Legal information retrieval]]\n* [[Vertical search]]\n\n==Other retrieval methods==\nMethods/Techniques in which [[information retrieval]] techniques are employed include:\n* [[Adversarial information retrieval]]\n* [[Automatic summarization]]\n**[[Multi-document summarization]]\n* [[Compound term processing]]\n* [[Cross-language information retrieval|Cross-lingual retrieval]]\n* [[Document classification]]\n* [[Spam filtering]]\n* [[Question answering]]\n\n== See also ==\n* [[Information retrieval]]\n\n{{DEFAULTSORT:Information Retrieval Applications}}\n[[Category:Information retrieval genres|*]]']
['Dragomir R. Radev', '31253847', '\'\'\'Dragomir R. Radev\'\'\' is a [[University of Michigan]] computer science professor and [[Columbia University]] computer science adjunct professor working on [[natural language processing]] and [[information retrieval]]. From January 2017 he will join [[Yale University]] as a professor of computer science.\nHe is currently working on the fields of open domain [[question answering]],  [[multi-document summarization]], and the application of NLP in Bioinformatics and Political Science.\n\nRadev received his PhD in [[Computer Science]] from [[Columbia University]] in 1999. He is the secretary of [http://www.aclweb.org [[Association for Computational Linguistics|ACL]]] (2006–present) and associate editor of [http://www.jair.org JAIR].\n\n== Awards ==\nAs [[NACLO]] founder, Radev shared the [[Linguistic Society of America]] 2011 [http://www.lsadc.org/info/lsa-awards.cfm \'\'Linguistics, Language and the Public Award\'\']. He is the  Co-winner of the [http://polmeth.wustl.edu/about.php?page=awards Gosnell Prize (2006)].\n\nIn 2015 he was named a [[fellow]] of the [[Association for Computing Machinery]] "for contributions to natural language processing and computational linguistics."<ref>{{citation|url=http://www.acm.org/press-room/news-releases/2015/fellows-2015|title=ACM Fellows Named for Computing Innovations that Are Advancing Technology in the Digital Age|publisher=[[Association for Computing Machinery]]|year=2015|accessdate=2015-12-10}}.</ref>\n\n== IOL==\nRadev has served as the coach and led the US national team in the [[International Linguistics Olympiad|International Linguistics Olympiad (IOL)]] to several gold medals [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073][http://www.nsf.gov/news/news_summ.jsp?cntn_id=109891].\n\n== Books ==\n* Puzzles in Logic, Languages and Computation (2013) <ref>{{Cite web|url = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-34371-1|title = Puzzles in Logic, Languages and Computation|date = |accessdate = |website = |publisher = |last = |first = }}</ref>\n* Mihalcea and Radev (2011) [http://www.cambridge.org/gb/knowledge/isbn/item5980387/?site_locale=en_GB \'\'Graph-based methods for NLP and IR\'\']\n\n== Selected Papers ==\n* SIGIR 1995 Generating summaries of multiple news articles\n* ANLP 1997 Building a generation knowledge source using internet-accessible newswire\n* Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources\n* ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities\n* ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation\n* CIKM 2001 Mining the web for answers to natural language questions\n* AAAI 2002 Towards CST-enhanced summarization\n* ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project\n* Information Processing and Management 2004 Centroid-based summarization of multiple documents\n* J. of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization\n* J. of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web\n* Communications of the ACM 2005 NewsInEssence: summarizing online news topics\n* EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing\n* Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network\n* IEEE Intelligent Systems 2008 natural language processing and the web\n* NAACL 2009 Generating surveys of scientific paradigms\n* Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways\n* J. of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts\n* KDD 2010 Divrank: the interplay of prestige and diversity in information networks\n* American J. of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs\n* Arxiv 2011 The effect of linguistic constraints on the large scale organization of language\n* J. of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology\n\n==External links==\n* [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073 Team USA Brings Home the Linguistics Gold]\n* [http://www.eecs.umich.edu/eecs/about/articles/2011/Radev-LSA11.html Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award]\n* [http://www.eecs.umich.edu/eecs/about/articles/2010/Radev-Linguistics.html Dragomir Radev Coaches US Linguistics Team to Multiple Wins]\n* [http://www.eecs.umich.edu/eecs/about/articles/2009/Radev-ACM-DM.html Dragomir Radev Honored as ACM Distinguished Scientist]\n* [http://www.eecs.umich.edu/eecs/etc/news/shownews.cgi?428 Prof. Dragomir Radev Receives Gosnell Prize]\n\n== References ==\n{{reflist}}\n<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->\n*\n*\n*\n*\n\n\n{{DEFAULTSORT:Radev, Dragomir R.}}\n[[Category:Year of birth missing (living people)]]\n[[Category:Living people]]\n\n[[Category:Columbia School of Engineering and Applied Science alumni]]\n[[Category:American computer scientists]]\n[[Category:University of Michigan faculty]]\n[[Category:Natural language processing]]\n[[Category:Information retrieval researchers]]\n[[Category:Fellows of the Association for Computing Machinery]]']
['Stephen Robertson (computer scientist)', '24019253', "{{More footnotes|date=September 2012}}\n{{Infobox scientist\n| name = Stephen Robertson\n| image =\n| image_size = 100px\n| residence = United Kingdom\n| nationality = British\n| field = Computer science\n| alma_mater = Cambridge, City University, University College London\n| doctoral_advisor = B.C (Bertie) Brookes\n| doctoral_students = Ayse Göker, Andrew MacFarlane, Xiangji (Jimmy) Huang, Olga Vechtomova, Murat Karamuftuoglu, Micheline Beaulieu, Efthimis Efthimiadis, Anna Ritchie, Jagadeesh Gorla\n| known_for  = Work on information retrieval and inverse document frequency\n| prizes = [[Gerard Salton Award]] (2000), [[Tony Kent Strix award]] (1998), [[ACM Fellow]] (2013)\n| website = {{URL|http://staff.city.ac.uk/~sb317}}\n}}\n\n'''Stephen Robertson''' is a [[United Kingdom|British]] computer scientist. He is known for his work on [[information retrieval]]<ref>{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Spärck Jones | first2 = K. | authorlink2 = Karen Spärck Jones}}</ref> and the [[Okapi BM25]] weighting model.<ref>{{Cite journal | doi = 10.1016/S0306-4573(00)00015-7| title = A probabilistic model of information retrieval: Development and comparative experiments: Part 1| journal = Information Processing & Management| volume = 36| issue = 6| pages = 779–808| year = 2000| last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| last2 = Walker | first2 = S.| last3 = Robertson | first3 = S. E. | authorlink3 = Stephen Robertson (computer scientist)}}</ref><ref>{{Cite journal | doi = 10.1016/S0306-4573(00)00016-9| title = A probabilistic model of information retrieval: Development and comparative experiments: Part 2| journal = Information Processing & Management| volume = 36| issue = 6| pages = 809–840| year = 2000| last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| last2 = Walker | first2 = S. | last3 = Robertson | first3 = S. E. | authorlink3 = Stephen Robertson (computer scientist)}}</ref>\n\nAfter completing his undergraduate degree in mathematics at [[Cambridge university|Cambridge University]], he took an MS at [[City University, London|City University]], and then worked for [[ASLIB]]. He then studied for his PhD at [[University College London]] under the renowned statistician and scholar B. C. Brookes. He then returned to City University working there from 1978 until 1998 in the Department of [[Information Science]], continuing as a part-time professor and subsequently as professor emeritus. He is also a fellow of [[Girton College, Cambridge|Girton College]], Cambridge University.\n\nFrom 1998 to 2013 he worked in the Cambridge laboratory of [[Microsoft Research]], where he led a group investigating core search processes such as term weighting, document scoring and ranking algorithms, combining evidence from different sources, and metrics and methods for the evaluation and optimisation of search. Much of his work has contributed to the [[Microsoft]] [[Web search engine|search engine]] [[Bing (search engine)|Bing]]. He participated a number of times in the [[Text Retrieval Conference|TREC conference]].\n\n==References==\n{{Reflist}}\n\n== External links ==\n* {{cite book|last1=Robertson|first1=Stephen|last2=Zaragoza|first2=Hugo|title=The Probabilistic Relevance Framework: BM25 and Beyond|date=2009|publisher=NOW Publishers, Inc.|isbn=978-1-60198-308-4|url=http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf}}\n\n{{DEFAULTSORT:Robertson, Stephen}}\n[[Category:British computer scientists]]\n[[Category:Alumni of University College London]]\n[[Category:Fellows of Girton College, Cambridge]]\n[[Category:Living people]]\n[[Category:Alumni of City, University of London]]\n[[Category:Academics of City, University of London]]\n[[Category:Information retrieval researchers]]"]
['Jaime Teevan', '31120153', '{{Infobox scientist\n| name        = Jaime Teevan\n| image       = \n| caption     =\n| birth_date  = {{Birth year and age|1976}}\n| birth_place = \n| death_date  =\n| death_place =\n| nationality = \n| residence   = \n| fields      = [[Computer science]]<br/ >[[Human-Computer Interaction]]<br/ >[[Information Retrieval]]\n| work_institution = [[Microsoft Research]]\n| alma_mater  = [[Massachusetts Institute of Technology]]<br/>[[Yale University]]\n| known_for   =\n| doctoral_advisor = [[David Karger]]\n| awards = [[TR35]] (2009)<br/ >Borg Early Career Award (2014)<br/ >[[Karen Spärck Jones]] Award (2016)\n| website           = {{URL|http://teevan.org}} \n}}\n\n\'\'\'Jaime Teevan\'\'\' is an American [[computer scientist]] known for her research in [[human-computer interaction]] and [[information retrieval]]. She is particularly known for the work she has done on [[personalized search]]. According to the [[Technology Review]], Teevan "is a leader in using data about people\'s knowledge, preferences, and habits to help them manage information.<ref name="tr35">{{cite news|last=Kleiner|first=Kurt|title=TR35: Jaime Teevan, 32|url=http://www.technologyreview.com/tr35/Profile.aspx?Cand=T&TRID=778|accessdate=10 March 2011|newspaper=Technology Review|date=August 2009}}</ref>"\n\n==Biography==\nTeevan received and a [[Bachelor of Science|B.S.]] in [[Computer Science]] from [[Yale University]] and a Ph.D. and S.M. from [[MIT]].<ref>http://www.csail.mit.edu/~teevan/work/publications/theses/phd/thesis.pdf</ref><ref>http://www.csail.mit.edu/~teevan/work/publications/theses/masters/thesis.pdf</ref>\n\nShe is currently a researcher at [[Microsoft Research]] and an affiliate professor at the [[University of Washington]]. There she co-authored the first book on [[collaborative information seeking]],.<ref>{{cite book|last=Morris|first=Meredith Ringel and Teevan, Jaime|title=Collaborative Search: Who, What, Where, When, Why, and How|year=2010|publisher=Morgan and Claypool Publishers|isbn=1-60845-121-6|url=http://www.amazon.com/dp/1608451216/}}</ref> She also edited a book on [[Personal Information Management]] (PIM),<ref>{{cite book|editor=Jones, William |editor2=Teevan, Jaime|title=Personal Information Management|year=2007|publisher=University of Washington Press|isbn=0-295-98737-5|url=http://www.amazon.com/dp/0295987375}}</ref> \nedited a special issue of Communications of the ACM on the topic, and organized workshops on [[PIM (software)|PIM]] and query log analysis. She has published numerous technical papers, including several best papers, and was chair of the Web Search and Data Mining (WSDM) 2012 conference.\n\n==Awards==\n\nTeevan was named a Technology Review ([[TR35]]) 2009 Young Innovator for her research on [[personalized search]]<ref name="tr35" /> and received the [[CRA-W]] Borg Early Career Award (BECA) in 2014.<ref name="borg">{{cite news|last=Knies|first=Rob|title=Researcher Teevan Wins Borg Early Career Award|url=http://blogs.technet.com/b/inside_microsoft_research/archive/2014/04/22/researcher-teevan-wins-borg-early-career-award.aspx|accessdate=28 April 2014|newspaper=Inside Microsoft Research|date=April 2014}}</ref> In 2016 she received the [[Karen Spärck Jones]] award from the [[British Computer Society]] for her "technically strong and exceptionally creative contributions to the intersection of information retrieval, user experience and social media." <ref>http://irsg.bcs.org/ksjaward.php</ref>\n\n==Personal==\nTeevan is married to Alexander Hehmeyer.<ref>{{cite news|title=WEDDINGS; Jaime Teevan, Alexander Hehmeyer|url=http://www.nytimes.com/2002/06/16/style/weddings-jaime-teevan-alexander-hehmeyer.html|accessdate=14 September 2015|newspaper=New York Times|date=June 16, 2002}}</ref>\nThe couple live in [[Bellevue, Washington]]\nand have four children.<ref>{{cite news|last=Vanderkam|first=Laura|title=Women with Big Jobs and Big Families: Balancing Really Isn\'t That Hard|url=http://fortune.com/2015/06/06/women-with-big-jobs-and-big-families-balancing-really-isnt-that-hard/|accessdate=14 September 2015|newspaper=Fortune|date=6 June 2015}}</ref>\nTeevan is an advocate for helping researchers successfully integrate parenthood and academic efforts.<ref name="borg" />\n\n==References==\n<references />\n\n==External links==\n* [http://teevan.org/ Professional home page]\n\n{{DEFAULTSORT:Teevan, Jaime}}\n[[Category:People in information technology]]\n[[Category:Information retrieval researchers]]\n[[Category:Human–computer interaction researchers]]\n[[Category:Women computer scientists]]\n[[Category:Microsoft employees]]\n[[Category:Living people]]\n[[Category:Yale University alumni]]\n[[Category:Massachusetts Institute of Technology alumni]]\n[[Category:University of Washington faculty]]\n[[Category:1976 births]]']
['Suggested Upper Merged Ontology', '247601', 'The \'\'\'Suggested Upper Merged Ontology\'\'\' or \'\'\'SUMO\'\'\' is an [[Upper ontology (information science)|upper ontology]] intended as a foundation [[ontology (computer science)|ontology]] for a variety of computer information processing systems. It was originally developed by the Teknowledge Corporation and now is maintained by [http://www.articulatesoftware.com Articulate Software]. SUMO is [[open source]].\n\nSUMO originally concerned itself with meta-level concepts (general entities that do not belong to a specific problem domain), and thereby would lead naturally to a categorization scheme for encyclopedias.  It has now been considerably expanded to include a mid-level ontology and dozens of domain ontologies.\n\nSUMO was first released in December 2000. It defines a hierarchy of \'\'SUMO classes\'\' and related rules and relationships. These are formulated in a version of the language [[SUO-KIF]] which has a [[LISP]]-like syntax. A [[Map (mathematics)|mapping]] from [[WordNet]] [[synsets]]  to SUMO has also been defined.  \n\nSUMO is organized for interoperability of automated [[reasoning engine]]s. To maximize compatibility, [[logical schema|schema]] designers can try to assure that their [[naming convention]]s use the same meanings as SUMO for identical words (for example, "agent" or "process").  SUMO has an associated open source [[Sigma knowledge engineering environment]].\n\n==See also==\n* [[Semantic translation]]\n* [[Upper ontology]]\n\n== External links ==\n* [http://www.ontologyportal.org/ Main page for SUMO]\n* [http://suo.ieee.org/ Home page of the IEEE Standard Upper Ontology working group]\n* The [http://sigmakee.sourceforge.net Sigma] reasoning system for SUMO\n* [http://54.183.42.206:8080/sigma/Browse.jsp?kb=SUMO Online browser for SUMO]\n* [http://www.adampease.org/professional/ Adam Pease, current Technical Editor of the standard]\n[[Category:Java platform software]]\n[[Category:Knowledge representation]]\n[[Category:Ontology (information science)]]\n[[Category:Open data]]\n[[Category:Knowledge bases]]\n{{Compu-AI-stub}}']
['Ontic', '2788896', '{{The Works of Aristotle}}\nIn [[philosophy]], \'\'\'ontic\'\'\' (from the [[Greek language|Greek]] {{lang|grc|ὄν}}, genitive {{lang|grc|ὄντος}}: "of that which is") is physical, real, or factual existence.\n\n"Ontic" describes what is there, as opposed to the nature or properties of that being. To illustrate:\n\n*[[Roger Bacon]], observing that all languages are built upon a common grammar, stated that they share a foundation of ontically anchored linguistic structures.\n*[[Martin Heidegger]] posited the concept of \'\'Sorge\'\', or caring, as the fundamental concept of the [[intentionality|intentional being]], and presupposed an ontological significance that distinguishes [[ontology|ontological]] being from mere "thinghood" of an ontic being. He uses the [[German language|German]] word "[[Dasein]]" for a being that is capable of ontology, that is, [[recursivity|recursively]] comprehending [[property (philosophy)|properties]] of the very fact of its own Being. For Heidegger, "ontical" signifies concrete, specific realities, whereas "ontological" signifies deeper underlying structures of reality. Ontological objects or subjects have an ontical dimension, but they also include aspects of being like self-awareness, evolutionary vestiges, future potentialities, and networks of relationship.<ref>{{cite web|title=Ontico-Ontological Distinction|url=http://www.blackwellreference.com/public/tocnode?id=g9781405106795_chunk_g978140510679516_ss1-33|publisher=Blackwell Reference|accessdate=26 February 2015}}</ref><ref>{{cite web|last1=Duffy|first1=Michael|title=The Ontological and the Ontic|url=http://mikejohnduff.blogspot.com/2007/08/ontological.html|accessdate=26 February 2015}}</ref>\n* [[Nicolai Hartmann]] distinguishes among ontology, ontics, and metaphysics: (i) ontology concerns the categorical analysis of entities by means of the knowledge categories able to classify them, (ii) ontics refers to a pre-categorical and pre-objectual connection which is best expressed in the relation to transcendent acts, and (iii) metaphysics is that part of ontics or that part of ontology which concerns the residue of being that cannot be rationalized further according to categories.\n\n== Usage in philosophy of science ==\n[[Harald Atmanspacher]] writes extensively about the philosophy of science, especially as it relates to [[Chaos theory]], [[determinism]], [[Causality|causation]], and [[stochastic process|stochasticity]]. He explains that "\'\'ontic\'\' states describe all properties of a physical system exhaustively. (\'Exhaustive\' in this context means that an \'\'ontic\'\' state is \'precisely the way it is,\' without any reference to [[epistemic]] knowledge or ignorance.)"{{ref|autonumber}}\n\nIn an earlier paper, Atmanspacher portrays the difference between an [[epistemic]] perspective of a [[system]], and an ontic perspective:\n\n:Philosophical [[discourse]] traditionally distinguishes between [[ontology]] and [[epistemology]] and generally enforces this distinction by keeping the two subject areas separated. However, the relationship between the two areas is of central importance to [[physics]] and [[philosophy of physics]]. For instance, many [[measurement]]-related problems force us to consider both our [[knowledge]] of the [[Classical mechanics|states]] and [[observables]] of a [[system]] ([[epistemic perspective]]) and its states and observables, independent of such knowledge (ontic perspective). This applies to [[quantum|quantum systems]] in particular.{{ref|autonumber}}\n\n== Usage in philosophy of critical realism ==\nThe [[United Kingdom|British]] [[philosopher]] [[Roy Bhaskar]], who is closely associated with the philosophical [[Cultural movement|movement]] of [[Critical realism (philosophy of the social sciences)|Critical Realism]] writes:\n:"I differentiate the \'ontic\' (\'ontical\' etc.) from the \'ontological\'. I employ the former to refer to\n\n:# whatever pertains to being generally, rather than some distinctively philosophical (or scientific) theory of it (ontology), so that in this sense, that of the \'\'\'ontic<sub>1</sub>\'\'\', we can speak of the ontic presuppositions of a work of art, a [[joke]] or a strike as much as a [[epistemology|theory of knowledge]]; and, within this [[rubric]], to\n:# the [[intransitive]] [[object (philosophy)|object]]s of some specific, [[historically determinate]], [[scientific investigation]] (or set of such investigations), the \'\'\'ontic<sub>2</sub>\'\'\'.\n\n:"The ontic<sub>2</sub> is always specified, and only identified, by its relation, as the intransitive object(s) of some or other (denumerable set of) particular transitive process(es) of enquiry. It is cognitive process-, and level-specific; whereas the ontological (like the ontic<sub>1</sub>) is not."{{ref|autonumber}}\n\n[[Ruth Groff]] offers this expansion of Bhaskar\'s note above:\n:"\'ontic<sub>2</sub>\' is an abstract way of denoting the [[object-domain]] of a particular [[scientific]] area, field, or inquiry. E.g.: [[molecules]] feature in the ontic<sub>2</sub> of chemistry. He\'s just saying that the scientific undertaking ITSELF is not one of the objects of said, most narrowly construed, immediate object-domain. So [[chemistry]] itself is not part of the ontic<sub>2</sub> of chemistry."\n\n==See also==\n*[[Ding an sich#Noumenon and the thing-in-itself|Ding an sich]]\n*[[Ontologism]]\n*[[Physical ontology]]\n*[[Substance theory]]\n\n==References==\n{{Reflist}}\n# {{Note|autonumber}} Atmanspacher, Dr. H., and Primas, H., 2003 [2005], "Epistemic and Ontic [[Quantum]] [[Reality|Realities]]", in Khrennikov, A (Ed.), \'\'Foundations of Probability and Physics\'\' ([[American Institute of Physics]] 2005, pp 49&ndash;61, Originally published in \'\'Time, Quantum and Information\'\', edited by Lutz Castell and Otfried Ischebeck, Springer, Berlin, 2003, pp 301&ndash;321\n# {{Note|autonumber}} Atmanspacher, Harald (2001) \'\'[[Determinism]] Is Ontic, Determinability is [[Epistemic]]\'\' ([http://philsci-archive.pitt.edu/archive/00000939/00/determ.pdf [[University of Pittsburgh]] Archives])\n# {{Note|autonumber}} Bhaskar, R.A., 1986, \'\'Scientific Realism and Human Emancipation\'\' (London: Verso), pp 36 and 37, as quoted by [[Howard Engelskirchen]] in the [http://archives.econ.utah.edu/archives/bhaskar/2001m11/msg00015.htm Bhaskar mailing list archive]\n{{Continental philosophy}}\n{{wiktionary}}\n\n[[Category:Concepts in metaphysics]]\n[[Category:Knowledge representation]]\n[[Category:Martin Heidegger]]\n[[Category:Modal logic]]\n[[Category:Ontology]]\n[[Category:Philosophy of science]]\n[[Category:Reality]]']
['John F. Sowa', '102392', '{{Infobox person\n | name             = John F. Sowa\n | image            =\n | image_size       = \n | caption          = \n | birth_name       = John Florian Sowa\n | birth_date       = {{Birth date and age|mf=yes|1940|1|1}}\n | birth_place      = \n | death_date       = \n | death_place      = \n | death_cause      = \n | resting_place    = \n | residence        = [[Croton-on-Hudson, New York]]\n | nationality      = \n | other_names      =\n | known_for        = [[Conceptual graph]]s\n | education        = [[Massachusetts Institute of Technology]] BS 1962, [[Harvard University]] MA 1966, [[Vrije Universiteit Brussel]] PhD 1999\n | alma_mater       = \n | employer         = \n | occupation       = Computer Scientist\n | boards           = \n | religion         = \n | spouse           = [[Cora Angier Sowa]]\n | children         = \n | parents          = \n | relations        =\n | callsign         = \n | awards           = \n | signature        =\n | website          = {{URL|http://www.jfsowa.com/|JFSowa.com}}\n| \n}}\n\'\'\'John Florian Sowa\'\'\' (born 1940) is an American [[computer scientist]], an expert in [[artificial intelligence]] and [[computer design]], and the inventor of [[conceptual graph]]s.<ref>[[Kecheng Liu]] (2000) \'\'Semiotics in Information Systems Engineering\'\'. p.54 states: \'\'Conceptual graphs are devised as a language of knowledge representation by Sowa (1984), based on philosophy, psychology and linguistics. Knowledge in conceptual graph form is highly structured by modelling specialised facts that can be subjected to generalised reasoning.</ref><ref>Marite Kirikova (2002) \'\'Information Systems Development: Advances in Methodologies, Components, and Management\'\'. p.194. states: \'\'The original theory of conceptual graphs was introduced by Sowa (Sowa, 1984 ). A conceptual graph is a finite, connected, bipartite graph. It includes notions of concepts, relations, and actors...\'\'</ref>\n\n== Biography ==\nSowa received a BS in mathematics from [[Massachusetts Institute of Technology]] in 1962, an MA in applied mathematics from [[Harvard University]] in 1966, and a PhD in [[computer science]] from the [[Vrije Universiteit Brussel]] in 1999 on a dissertation titled "Knowledge Representation: Logical, Philosophical, and Computational Foundations".<ref>Andreas Tolk, Lakhmi C. Jain (2011) \'\'Intelligent-Based Systems Engineering\'\'. p.xxi</ref>\n\nSowa spent most of his professional career at [[International Business Machines|IBM]], which started in 1962 at IBM\'s applied mathematics group. Over the decades he has researched and developed emerging fields of [[computer science]] from compiler, programming languages, and system architecture<ref name="SoZa92">John F. Sowa and [[John Zachman]] (1992). [http://www.research.ibm.com/journal/sj/313/sowa.pdf "Extending and Formalizing the Framework for Information Systems Architecture"] In: \'\'IBM Systems Journal\'\', Vol 31, no.3, 1992. p. 590-616.</ref> to artificial intelligence and knowledge representation. In the 1990s Sowa was associated with IBM Educational Center in New York. Over the years he taught courses at the IBM Systems Research Institute, [[Binghamton University]], [[Stanford University]], [[Linguistic Society of America]] and [[Université du Québec à Montréal]]. He is a fellow of the [[Association for the Advancement of Artificial Intelligence]].\n\nAfter early retirement at IBM Sowa in 2001 cofounded VivoMind Intelligence, Inc. with [[Arun K. Majumdar]]. With this company he was developing data-mining and database technology, more specific high-level "[[ontology|ontologies]]" for [[artificial intelligence]] and automated [[natural language understanding]]. Currently Sowa is working with [http://kyndi.com/ Kyndi Inc.], also founded by Majumdar. \n\nJohn Sowa is married to the philologist Cora Angier Sowa,<ref>Cora Angier Sowa (1984) \'\'Traditional themes and the Homeric hymns\'\'. p.iv</ref> and they live in [[Croton-on-Hudson, New York]].\n\n== Work ==\nSowa\'s research interest since the 1970s were in the field of [[artificial intelligence]], [[expert systems]] and [[database query]] linked to natural languages.<ref name="SoZa92"/> In his work he combines ideas from numerous disciplines and eras modern and ancient, for example, applying ideas from [[Aristotle]], the medieval [[Scholastics]] to [[Alfred North Whitehead]] and including [[logical schema|database schema]] theory, and incorporating the model of analogy of Islamic scholar [[Ibn Taymiyyah]] in his works.<ref>[http://www.jfsowa.com/pubs/analog.htm Analogical Reasoning]</ref>\n\n=== Conceptual graph ===\n{{main|Conceptual graph}}\nSowa invented conceptual graphs, a graphic notation for logic and natural language, based on the structures in [[semantic network]]s and on the [[existential graph]]s of [[Charles Sanders Peirce|Charles S. Peirce]]. He published the concept in the 1976 article "Conceptual graphs for a data base interface" in the \'\'IBM Journal of Research and Development\'\'.<ref>{{cite journal |last=Sowa |authorlink = |first= John F. |date=July 1976 |title=Conceptual Graphs for a Data Base Interface |journal=IBM Journal of Research and Development |volume=20 |issue=4 |pages=336–357 |url=http://www.research.ibm.com/journal/rd/204/ibmrd2004E.pdf |ref=harv |doi=10.1147/rd.204.0336}}</ref> He further explained in the 1983 book \'\'Conceptual structures: information processing in mind and machine\'\'.\n\nIn the 1980s this theory has "been adopted by a number of research and development groups throughout the world.<ref name="SoZa92"/> International conferences on conceptual graphs have been held for over a decade since before 1992.{{citation needed|date=November 2012}}\n\n==={{anchor|law of standards}}Sowa\'s law of standards===\nIn 1991, Sowa first stated his \'\'Law of Standards\'\': \n: "Whenever a major organization develops a new system as an official [[Technical standard|standard]] for X, the primary result is the widespread adoption of some simpler system as a [[de facto]] standard for X."<ref>[http://www.jfsowa.com/computer/standard.htm Law of Standards]</ref> \nLike [[Gall\'s law]], The Law of Standards is essentially an argument in favour of underspecification. Examples include:\n\n*The introduction of [[PL/I]] resulting in [[COBOL]] and [[FORTRAN]] becoming the de facto standards for scientific and business programming\n*The introduction of [[Algol-68]] resulting in [[Pascal (programming language)|Pascal]] becoming the de facto standard for academic programming\n*The introduction of the [[Ada (programming language)|Ada language]] resulting in [[C (programming language)|C]] becoming the de facto standard for [[United States Department of Defense|DoD]] programming\n*The introduction of [[OS/2]] resulting in [[Microsoft Windows|Windows]] becoming the de facto standard for [[desktop OS]]\n*The introduction of [[X.400]] resulting in [[SMTP]] becoming the de facto standard for [[electronic mail]]\n*The introduction of [[X.500]] resulting in [[LDAP]] becoming the de facto standard for [[directory services]]\n\n== Publications ==\n* 1984. \'\'Conceptual Structures - Information Processing in Mind and Machine\'\'. The Systems Programming Series, Addison-Wesley<ref>[http://conceptualstructures.org/ Conceptual Structures Home Page]. Retrieved Nov 23, 2012.</ref>\n* 1991. \'\'Principles of Semantic Networks\'\'. Morgan Kaufmann.\n* {{Cite journal| editor1-last = Mineau | editor1-first = Guy W| editor2-last = Moulin | editor2-first = Bernard| editor3-last = Sowa | editor3-first = John F | editor3-link = John F. Sowa| title = Conceptual Graphs for Knowledge Representation| doi = 10.1007/3-540-56979-0| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 699| year = 1993| isbn = 978-3-540-56979-4}}\n* 1994. \'\'International Conference on Conceptual Structures (2nd : 1994 : College Park, Md.)\tConceptual structures, current practices : Second International Conference on Conceptual Structures, ICCS\'94, College Park, Maryland, USA, August 16–20, 1994 : proceedings\'\'. William M. Tepfenhart, Judith P. Dick, John F. Sowa, eds.\n*{{Cite journal| editor1-last = Ellis | editor1-first = Gerard| editor2-last = Levinson | editor2-first = Robert| editor3-last = Rich | editor3-first = William| editor4-last = Sowa | editor4-first = John F | editor4-link = John F. Sowa| doi = 10.1007/3-540-60161-9| title = Conceptual Structures: Applications, Implementation and Theory| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 954| year = 1995| isbn = 978-3-540-60161-6}}\n*{{Cite journal| editor1-last = Lukose | editor1-first = Dickson| editor2-last = Delugach | editor2-first = Harry| editor3-last = Keeler | editor3-first = Mary| editor4-last = Searle | editor4-first = Leroy| editor5-last = Sowa | editor5-first = John | editor5-link = John F. Sowa| doi = 10.1007/BFb0027865| title = Conceptual Structures: Fulfilling Peirce\'s Dream| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 1257| year = 1997| isbn = 3-540-63308-1}}\n* 2000. \'\'Knowledge representation : logical, philosophical, and computational foundations\'\', Brooks Cole Publishing Co., Pacific Grove<ref>[http://www.jfsowa.com/krbook/ Knowledge Representation: Logical, Philosophical, and Computational Foundations] at jfsowa.com. Retrieved Nov 23, 2012.</ref>\n\n;Articles, a selection<ref>{{DBLP|name=John F. Sowa}}</ref>\n*{{Cite journal| last1 = Sowa | first1 = J. F. | author1-link = John F. Sowa| title = Conceptual Graphs for a Data Base Interface| doi = 10.1147/rd.204.0336| journal = IBM Journal of Research and Development| volume = 20| issue = 4| pages = 336–357| date=July 1976 }}\n*{{Cite journal| last1 = Sowa | first1 = J. F. | author1-link = John F. Sowa| last2 = Zachman | first2 = J. A.| doi = 10.1147/sj.313.0590| title = Extending and formalizing the framework for information systems architecture| journal = IBM Systems Journal| volume = 31| issue = 3| pages = 590–616| year = 1992}}\n* 1992. "[http://www.jfsowa.com/cg/cgif.htm Conceptual Graph Summary]"; In: T.E. Nagle et. al. (Eds.). \'\'Conceptual Structures: Current Research and Practice\'\'. Chichester: Ellis Horwood. \n* 1995. "Top-level ontological categories." in: \'\'International journal of human-computer studies\'\'. Vol. 43, Iss. 5–6, Nov. 1995, pp.&nbsp;669–685\n* 2006. "Semantic Networks". In: \'\'Encyclopedia of Cognitive Science.\'\'. John Wiley & Sons.\n\n== References ==\n{{reflist}}\n\n== External links ==\n{{Wikiquote}}\n* [http://www.jfsowa.com/ John F. Sowa] homepage\n\n{{Authority control}}\n\n{{DEFAULTSORT:Sowa, John}}\n[[Category:1940 births]]\n[[Category:Artificial intelligence researchers]]\n[[Category:Knowledge representation]]\n[[Category:Living people]]\n[[Category:People from Croton-on-Hudson, New York]]\n[[Category:Harvard University alumni]]\n[[Category:Binghamton University faculty]]']
['Brinkler classification', '3388492', '\'\'\'Brinkler Classification\'\'\' is the [[library classification]] system  of [[Bartol Brinkler]] described in his article "The Geographical Approach to Materials in the Library of Congress Subject Headings".<ref>Brinkler, Bartol. The geographical approach to materials in the Library of Congress subject headings: report of a study project. s.l.: s.n., 1960. [Accession No: {{OCLC|3853830}}].</ref> The geographical aspect of a subject may be conveyed through three types of headings labeled A, B, and C. Heading A uses a primary topical description with geographical subdivisions (e.g. Art—Paris).  Type B uses a place-name for the main heading with a topical subdivision (e.g. Paris—Description). C headings use a geographical description of a phrase (e.g. Paris Literature).  \n\nBrinkler explores what type of heading is more useful to a [[patron]], and he finds that it depends on the level of familiarity a patron has with a topic and what approach they take when searching for resources on their topic. Ideally readers will either be looking for everything on a particular topic, or everything regarding a particular place. Bartol Brinkler investigates a system of classification that will best serve these two ideal types of patrons.  He finds working with Type A headings will best assist a patron who is more topic oriented, while using Type B headings is preferable for those who are primarily interested in one place. \n\nHowever this is problematic in practice. One possibility is to assign Type A and Type B headings to every resource, but the cataloguing cost would be high.  A system that aids readers regardless of their approach to a topic involves using cross-references (e.g. Canada—Botany, See Botany—Canada).  Admitting that see and see also references would require more work on the part of librarians, Bartol Brinkler notes that librarians must keep in mind "...readers do not have the same knowledge [of classification] and do need all the help they can get..."{{Citation needed|date=March 2008}}\n\n==References==\n{{reflist}}\n*Brinkler, Bartol. The geographical approach to materials in the Library of Congress subject headings. Library Resources & Technical Services 6, no. 1 (Winter 1962): 49-64.\n\n==External links==\n*[http://hcl.harvard.edu/libraries/#widener  Harvard University. Widener Library.]\n*[http://www.loc.gov/catdir/cpso/lcco/lcco.html Library of Congress Classification Outline.]\n*[http://www.princeton.edu/~paw/memorials/memorials_1930s/memorials_1937.html Princeton Alumni Weekly: Memorials 1937.]\n\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]\n[[Category:Classification systems]]']
['Semantic interoperability', '7233280', '{{multiple issues|\n{{underlinked|date=January 2013}}\n{{more footnotes|date=February 2011}}\n}}\n\n\'\'\'Semantic interoperability\'\'\' is the ability of [[computer]] systems to exchange [[data]] with unambiguous, shared meaning. [[Semantic]] interoperability is a requirement to enable machine computable logic, inferencing, knowledge discovery, and data federation between information systems.<ref>NCOIC, [https://www.ncoic.org/technology/deliverables/scope/ "SCOPE"], [https://www.ncoic.org/home \'\'Network Centric Operations Industry Consortium\'\'], 2008</ref>\n\nSemantic interoperability is therefore concerned not just with the packaging of data ([[syntax]]), but the simultaneous transmission of the meaning with the data (semantics).  This is accomplished by adding data about the data (metadata), linking each data element to a controlled, shared vocabulary.  The meaning of the data is transmitted with the data itself, in one self-describing "information package" that is independent of any information system.  It is this shared vocabulary, and its associated links to an ontology, which provides the foundation and capability of machine interpretation, inferencing, and logic.\n\nSyntactic interoperability is a prerequisite for semantic interoperability.  Syntactic interoperability refers to the packaging and transmission mechanisms for data.  In healthcare, HL7 has been in use for over thirty years (which predates the internet and web technology), and uses the pipe character (|) as a data delimiter. The current internet standard for document markup is XML, which uses "< >" as a data delimiter.  The data delimiters convey no meaning to the data other than to structure the data.  Without a data dictionary to translate the contents of the delimiters, the data remains meaningless.  While there are many attempts at creating data dictionaries and information models to associate with these data packaging mechanisms, none have been practical to implement.  This has only perpetuated the ongoing "babelization" of data and inability to exchange of data with meaning.\n\nSince the introduction of the Semantic Web concept by [[Tim Berners-Lee]] in 1999,<ref>{{cite book |last=Berners-Lee |first=Tim |authorlink=Tim Berners-Lee |author2=Fischetti, Mark |title=[[Tim Berners Lee#Weaving the Web|Weaving the Web]] |publisher=[[HarperSanFrancisco]] |year=1999 |pages=chapter 12 |isbn=978-0-06-251587-2 |nopp=true }}</ref> there has been growing interest and application of the W3C (World Wide Web Consortium, [[WWWC]]) standards to provide web-scale semantic data exchange, federation, and inferencing capabilities.\n\n== Semantic as a function of syntactic interoperability ==\n\nSyntactic interoperability, provided by for instance [[XML]] or the [[SQL]] standards, is a pre-requisite to semantic.  It involves a common data format and common protocol to structure any data so that the manner of processing the information will be interpretable from the structure.  It also allows detection of syntactic errors, thus allowing receiving systems to request resending of any message that appears to be garbled or incomplete.  No semantic communication is possible if the syntax is garbled or unable to represent the data.  However, information represented in one syntax may in some cases be accurately translated into a different syntax.  Where accurate translation of syntaxes is possible, systems using different syntaxes may also interoperate accurately.  In some cases the ability to accurately translate information among systems using different syntaxes may be limited to one direction, when the formalisms used have different levels of \'\'expressivity\'\' (ability to express information).\n\nA single ontology containing representations of every term used in every application is generally considered impossible, because of the rapid creation of new terms or assignments of new meanings to old terms.  However, though it is impossible to anticipate \'\'every\'\' concept that a user may wish to represent in a computer, there is the possibility of finding some finite set of "primitive" concept representations that can be combined to create any of the more specific concepts that users may need for any given set of applications or ontologies.  Having a foundation ontology (also called \'\'[[upper ontology]]\'\') that contains all those primitive elements would provide a sound basis for general semantic interoperability, and allow users to define any new terms they need by using the basic inventory of ontology elements, and still have those newly defined terms properly interpreted by any other computer system that can interpret the basic foundation ontology.  Whether the number of such primitive concept representations is in fact finite, or will expand indefinitely, is a question under active investigation.  If it is finite, then a stable foundation ontology suitable to support accurate and general semantic interoperability can evolve after some initial foundation ontology has been tested and used by a wide variety of users.  At the present time, no foundation ontology has been adopted by a wide community, so such a stable foundation ontology is still in the future.\n\n== Words and Meanings ==\n\nOne persistent misunderstanding recurs in discussion of semantics - the confusion of words and meanings.  The meanings of words change, sometimes rapidly. But a formal language such as used in an ontology can encode the meanings (semantics) of concepts in a form that does not change.  In order to determine what is the meaning of a particular word (or term in a database, for example) it is necessary to label each fixed concept representation in an ontology with the word(s) or term(s) that may refer to that concept.  When multiple words refer to the same (fixed) concept, in language this is called synonymy; when one word is used to refer to more than one concept, that is called ambiguity.  Ambiguity and synonymy are among the factors that make computer understanding of language very difficult.  The use of words to refer to concepts (the meanings of the words used)is very sensitive to the context and the purpose of any use for many human-readable terms.  The use of ontologies in supporting semantic interoperability is to provide a fixed set of concepts whose meanings and relations are stable and can be agreed to by users.  The task of determining which terms in which contexts (each database is a different context) then is separated from the task of creating the ontology, and must be taken up by the designer of a database, or the designer of a form for data entry, or the developer of a program for language understanding.  When a word used in some interoperability context changes its meaning, then to preserve interoperability it is necessary to change the pointer to the ontology element(s) that specifies the meaning of that word.\n\n== Knowledge representation requirements and languages ==\n\nA knowledge representation language may be sufficiently expressive to describe nuances of meaning in well understood fields.  There are at least five levels of complexity of these{{specify|date=June 2014}}.\n\nFor general [[semi-structured data]] one may use a general purpose language such as XML.<ref>[http://www.cs.umd.edu/projects/plus/SHOE/pubs/extreme2000.pdf XML as a tool for Semantic Interoperability] Semantic Interoperability on the Web, Jeff Heflin and James Hendler</ref>\n\nLanguages with the full power of first-order predicate logic may be required for many tasks.\n\nHuman languages are highly expressive, but are considered too ambiguous to allow the accurate interpretation desired, given the current level of human language technology.   In human languages the same word may be used to refer to different concepts (ambiguity), and the same concept may be referred to by different words (synonymy).\n\n== Prior agreement not required ==\n{{confusing|section|date=February 2016}}\n\nSemantic interoperability may be distinguished from other forms of interoperability by considering whether the information transferred has, in its communicated form, all of the meaning required for the receiving system to interpret it correctly, even when the algorithms used by the receiving system are unknown to the sending system.  Consider sending one number:\n\nIf that number is intended to be the sum of money owed by one company to another, it implies some action or lack of action on the part of both those who send it and those who receive it.\n\nIt may be correctly interpreted if sent in response to a specific request, and received at the time and in the form expected.  This correct interpretation does not depend only on the number itself, which could represent almost any of millions of types of quantitative measure, rather it depends strictly on the circumstances of transmission.  That is, the interpretation depends on both systems expecting that the algorithms in the other system use the number in exactly the same sense, and it depends further on the entire envelope of transmissions that preceded the actual transmission of the bare number.  By contrast, if the transmitting system does not know how the information will be used by other systems, it is necessary to have a shared agreement on how information with some specific meaning (out of many possible meanings) will appear in a communication.  For a particular task, one solution is to standardize a form, such as a request for payment; that request would have to encode, in standardized fashion, all of the information needed to evaluate it, such as: the agent owing the money, the agent owed the money, the nature of the action giving rise to the debt, the agents, goods, services, and other participants in that action; the time of the action; the amount owed and currency in which the debt is reckoned; the time allowed for payment; the form of payment demanded; and other information.  When two or more systems have agreed on how to interpret the information in such a request, they can achieve semantic interoperability \'\'for that specific type of transaction\'\'.  For semantic interoperability generally, it is necessary to provide standardized ways to describe the meanings of many more things than just commercial transactions, and the number of concepts whose representation needs to be agreed upon are at a minimum several thousand.\n\n== Ontology research ==\n\nHow to achieve semantic interoperability for more than a few restricted scenarios is currently a matter of research and discussion.  For the problem of General Semantic Interoperability, some form of foundation ontology (\'[[upper ontology]]\') is required that is sufficiently comprehensive to provide the defining concepts for more specialized ontologies in multiple domains.  Over the past decade more than ten foundation ontologies have been developed, but none have as yet been adopted by a wide user base.\n\nThe need for a single comprehensive all-inclusive ontology to support Semantic Interoperability can be avoided by designing the common foundation ontology as a set of basic ("primitive") concepts that can be combined to create the logical descriptions of the meanings of terms used in local domain ontologies or local databases.  This tactic is based on the principle that:\n\n\'\'\'If:\'\'\'\n<pre style="white-space:pre-wrap;">\n(1) the meanings and usage of the primitive ontology elements in the foundation ontology are agreed on, and \n(2) the ontology elements in the  domain ontologies are constructed as logical\ncombinations of the elements in the foundation ontology,\n</pre>\n\'\'\'Then:\'\'\'\n<pre style="white-space:pre-wrap;">\nThe intended meanings of the domain ontology elements can be computed automatically using an FOL reasoner, by any system that accepts the meanings of the elements in the foundation ontology, and has both the foundation ontology and the logical specifications of the elements in the domain ontology.\n</pre>\n\'\'\'Therefore:\'\'\'\n<pre style="white-space:pre-wrap;">\nAny system wishing to interoperate accurately with another system need transmit only the data to be communicated, plus any logical descriptions of terms used in that data that were created locally and are not already in the common foundation ontology.\n</pre>\n\nThis tactic then limits the need for prior agreement on meanings to only those ontology elements in the common Foundation Ontology (FO).  Based on several considerations, this is likely to be fewer than 10,000 elements (types and relations).\n\nIn practice, together with the FO focused on representations of the primitive concepts, a set of domain extension ontologies to the FO with elements specified using the FO elements will likely also be used.  Such pre-existing extensions will ease the cost of creating domain ontologies by providing existing elements with the intended meaning, and will reduce the chance of error by using elements that have already been tested.  Domain extension ontologies may be logically inconsistent with each other, and that needs to be determined if different domain extensions are used in any communication.\n\nWhether use of such a single foundation ontology can itself be avoided by sophisticated mapping techniques among independently developed ontologies is also under investigation.\n\n== Importance==\n\nThe practical significance of semantic interoperability has been measured by several studies that estimate the cost (in lost efficiency) due to lack of semantic interoperability.  One study,<ref>[http://content.healthaffairs.org/cgi/content/full/hlthaff.w5.10/DC1 Jan Walker, Eric Pan, Douglas Johnston, Julia Adler-Milstein, David W. Bates and Blackford Middleton, \'\'The Value of Healthcare Information Exchange and Interoperability\'\' Health Affairs, 19 January 2005]</ref> focusing on the lost efficiency in the communication of healthcare information, estimated that US$77.8 billion per year could be saved by implementing an effective interoperability standard in that area.  Other studies, of the construction industry<ref>[http://www.bfrl.nist.gov/oae/publications/gcrs/04867.pdf Microsoft Word - 08657 Final Rpt_8-2-04.doc<!-- Bot generated title -->]</ref> and of the automobile manufacturing supply chain,<ref>http://www.nist.gov/director/prog-ofc/report99-1.pdf</ref> estimate costs of over US$10 billion per year due to lack of semantic interoperability in those industries.  In total these numbers can be extrapolated to indicate that well over US$100 billion per year is lost because of the lack of a widely used semantic interoperability standard in the US alone.\n\nThere has not yet been a study about each policy field that might offer big cost savings applying semantic interoperability standards. But to see which policy fields are capable of profiting from semantic interoperability see \'[[Interoperability]]\' in general. Such policy fields are [[eGovernment]], health, security and many more. The EU also set up the [[Semantic Interoperability Centre Europe]] in June 2007.\n\n==See also==\n*[[Interoperability]], Interoperability generally\n*[[Semantic Computing]]\n*[[Upper ontology (computer science)]], Discussion of using an \'\'upper ontology\'\'.\n*[[Conceptual interoperability|Levels of Conceptual Interoperability]], A discussion describing an interoperability spectrum in the context of exchange of Modeling and Simulation information, in which \'\'semantic interoperability \'\' is not defined as fully independent of context, as described here.\n*[[UDEF]], Universal Data Element Framework\n\n==External links==\n*[http://colab.cim3.net/cgi-bin/wiki.pl?OntologyTaxonomyCoordinatingWG/OntacGlossary the ONTACWG Glossary Other definitions of Semantic Interoperability]\n*[http://marinemetadata.org/guides/vocabs/cvchooseimplement/cvsemint MMI Guide: Achieving Semantic Interoperability]\n\n==References==\n{{reflist|2}}\n\n[[Category:Knowledge representation]]\n[[Category:Technical communication]]\n[[Category:Information science]]\n[[Category:Ontology (information science)]]\n[[Category:Computing terminology]]\n[[Category:Telecommunication theory]]\n[[Category:Interoperability]]']
['F-logic', '4880312', '\'\'\'F-logic\'\'\' ([[frame (data structure)|frame]] [[Logic programming|logic]]) is a [[knowledge representation]] and [[ontology language]]. F-logic combines the advantages of conceptual modeling with object-oriented, frame-based languages and offers a declarative, compact and simple syntax, as well as the well-defined semantics of a logic-based language. \nFeatures include, among others, object identity, complex objects, [[inheritance (computer science)|inheritance]], [[polymorphism (computer science)|polymorphism]], query methods, [[encapsulation (computer science)|encapsulation]]. F-logic stands in the same relationship to [[object-oriented programming]] as classical [[predicate calculus]] stands to [[relational database]] programming.\n\nF-logic was developed by [[Michael Kifer]] at [[Stony Brook University]] and [[Georg Lausen]] at the [[University of Mannheim]]. F-logic was originally developed for deductive databases, but is now most frequently used for semantic technologies, especially the [[Semantic Web]]. F-logic is considered as one of the formalisms for [[Ontology (information science)|ontologies]], but [[description logic]] (DL) is more popular and accepted, as is the DL-based [[Web Ontology Language|OWL]].\n\nA development environment for F-logic was developed in the NeOn project and is also used in a range of applications for information integration, [[question answering]] and [[semantic search]]. Prior to the version 4 of Protégé ontology editor, F-Logic is supported as one of the two kinds of ontology.\n\nThe frame syntax of the [[Rule Interchange Format]] Basic Logic Dialect (RIF BLD) standardized by the [[World Wide Web Consortium]] is based on F-logic; RIF BLD however does not include [[non-monotonic reasoning]] features of F-logic.<ref name="Krötzsch2010">{{cite book|author=M. Krötzsch|title=Description Logic Rules|url=https://books.google.com/books?id=Z8h7AgAAQBAJ&pg=PA10|year=October 2010|publisher=IOS Press|isbn=978-1-61499-342-1|page=10}}</ref>\n\nIn contrast to [[description logic]] based ontology formalism the semantics of F-logic are normally that of a [[closed world assumption]] as opposed to DL\'s [[open world assumption]]. Also, F-logic is generally [[Undecidable problem|undecidable]]{{Citation needed|date=May 2014}}, whereas \nthe [[SHOIN|SHOIN description logic]] that [[Web Ontology Language|OWL DL]] is based on is decidable. However it is possible to represent more expressive statements in F-logic than are possible with description logics.\n\nThe most comprehensive description of F-logic appears in.<ref>M. Kifer, G. Lausen, J. Wu (1995). \'\'Foundations of Object-Oriented and Frame-Based Languages]\'\', Journal of ACM, May 1995. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3586 PDF]</ref> The preliminary paper <ref>M. Kifer and G. Lausen (1989). \'\'F-logic: a higher-order language for reasoning about objects, inheritance, and scheme\'\', ACM SIGMOD Conference, 1989. [http://dl.acm.org/citation.cfm?id=66939 PDF]</ref> <ref>M. Kifer and G. Lausen (1997). \'\'F-logic: a higher-order language for reasoning about objects, inheritance, and scheme\'\', re-issued 1997. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.7149 PDF]</ref> has won the 1999 [http://www.sigmod.org/sigmod-awards/sigmod-awards#time Test of Time Award] from [[SIGMOD|ACM SIGMOD]]. A follow-up paper <ref>M. Kifer, W. Kim, Y. Sagiv (1992). \'\'Querying object-oriented databases\'\', ACM SIGMOD Conference, 1992. [http://dl.acm.org/citation.cfm?doid=130283.130342 PDF]</ref> has won the 2002 [http://www.sigmod.org/sigmod-awards/sigmod-awards#time Test of Time Award] from [[SIGMOD|ACM SIGMOD]].\n\n== F-logic syntax ==\n\nClasses and individuals may be defined in F-logic as follows\n man::person.\n woman::person.\n brad:man.\n angelina:woman.\nThis states, that "men and women are persons" and that "Brad is a man", and "Angelina is a woman".\n\nStatements about classes and individuals may be made as follows\n person[hasSon=&gt;man].\n brad[hasSon-&gt;&gt;{maddox,pax}].\n married(brad,angelina).\nThis defines that "the son of a person is a man", "Maddox and Pax are the sons of Brad" and "Brad and Angelina are married". Note that <code>-&gt;&gt;</code> is used for sets of values.\n\nIn addition it is possible to represent axioms in F-logic in the following manner\n man(X) &lt;- person(X) AND NOT woman(X).\n FORALL X, Y &lt;- X:person[hasFather->Y] &lt;- Y:man[hasSon -> X].\nThese mean "X is a man if X is a person but not a woman" and "if X is the son of Y then Y is the father of X".\n\nThe [[Flora-2]] system introduced a number of changes to the syntax of F-logic, making it more suitable for a knowledge representation and reasoning system as opposed to just a theoretical logic. In particular, variables became prefixed with a ?-mark, the distinction between functional and multi-valued properties was dropped and replaced by cardinality constraints, plus other important changes.\n\n==F-logic based Languages==\n* [[Flora-2]] is an extension of F-logic with [[HiLog]], [[Transaction logic]], and [[defeasible reasoning]].\n* [http://pathlp.sourceforge.net/ PathLP] is a full logic programming language based on F-logic.\n* [http://dbis.informatik.uni-freiburg.de/index.php?project=Florid FLORID] is a C++ — based implementation\n* [http://www.wsmo.org/wsml/ Web Services Modeling Language (WSML)]\n* [http://www.daml.org/services/swsl/ Semantic Web Services Language (SWSL)]\n* [[ObjectLogic]] language is based on F-logic; [[OntoStudio]] is an ObjectLogic implementation by [[semafora systems GmbH]] (former [[Ontoprise GmbH]]).\n\n== References ==\n{{reflist}}\n\n[[Category:Knowledge representation]]\n[[Category:Semantic Web]]\n[[Category:Logic programming languages]]\n[[Category:Declarative programming languages]]']
['Category:Microformats', '10220551', '{{Cat main|Microformat}}\n\n\n\n[[Category:Knowledge representation]]\n[[Category:Metadata publishing]]\n[[Category:Metadata]]\n[[Category:Semantic HTML]]\n[[Category:Semantic Web]]\n[[Category:Domain-specific knowledge representation languages]]']
['Category:Argument mapping', '11461295', '{{Cat main|Argument map}}\n\n[[Category:Knowledge representation]]\n[[Category:Informal arguments]]']
['Yale shooting problem', '2778728', 'The \'\'\'Yale shooting problem\'\'\' is a conundrum or scenario in formal situational [[logic]] on which early logical solutions to the [[frame problem]] fail. The name of this problem derives from its inventors, [[Steve Hanks]] and [[Drew McDermott]], working at [[Yale University]] when they proposed it. In this scenario, Fred (later identified as a [[turkey (bird)|turkey]]) is initially alive and a gun is initially unloaded. Loading the gun, waiting for a moment, and then shooting the gun at Fred is expected to kill Fred. However, if [[inertia]] is formalized in logic by minimizing the changes in this situation, then it cannot be uniquely proved that Fred is dead after loading, waiting, and shooting. In one solution, Fred indeed dies; in another (also logically correct) solution, the gun becomes mysteriously unloaded and Fred survives.\n\nTechnically, this scenario is described by two [[fluent (artificial intelligence)|fluents]] (a fluent is a condition that can change [[truth value]] over time): <math>alive</math> and <math>loaded</math>. Initially, the first condition is true and the second is false. Then, the gun is loaded, some time passes, and the gun is fired. Such problems can be formalized in logic by considering four time points <math>0</math>, <math>1</math>, <math>2</math>, and <math>3</math>, and turning every fluent such as <math>alive</math> into a predicate <math>alive(t)</math> depending on time. A direct formalization of the statement of the Yale shooting problem in logic is the following one:\n\n: <math>alive(0)</math>\n: <math>\\neg loaded(0)</math>\n: <math>true \\rightarrow loaded(1)</math>\n: <math>loaded(2) \\rightarrow \\neg alive(3)</math>\n\nThe first two formulae represent the initial state. The third formula formalizes the effect of loading the gun at time <math>0</math>. The fourth formula formalizes the effect of shooting at Fred at time <math>2</math>. This is a simplified formalization in which action names are neglected and the effects of actions are directly specified for the time points in which the actions are executed. See [[situation calculus]] for details.\n\nThe formulae above, while being direct formalizations of the known facts, do not suffice to correctly characterize the domain. Indeed, <math>\\neg alive(1)</math> is consistent with all these formulae, although there is no reason to believe that Fred dies before the gun has been shot. The problem is that the formulae above only include the effects of actions, but do not specify that all fluents not changed by the actions remain the same. In other words, a formula <math>alive(0) \\equiv alive(1)</math> must be added to formalize the implicit assumption that loading the gun \'\'only\'\' changes the value of <math>loaded</math> and not the value of <math>alive</math>. The necessity of a large number of formulae stating the obvious fact that conditions do not change unless an action changes them is known as the [[frame problem]].\n\nAn early solution to the frame problem was based on minimizing the changes. In other words, the scenario is formalized by the formulae above (that specify only the effects of actions) and by the assumption that the changes in the fluents over time are as minimal as possible. The rationale is that the formulae above enforce all effect of actions to take place, while minimization should restrict the changes to exactly those due to the actions.\n\nIn the Yale shooting scenario, one possible evaluation of the fluents in which the changes are minimized is the following one.\n\n{| cellpadding="5"\n| <math>alive(0)</math>\n| <math>alive(1)</math> \n| <math>alive(2)</math> \n| <math>\\neg alive(3)</math>\n|-\n| <math>\\neg loaded(0)</math>\n| <math>loaded(1)</math>\n| <math>loaded(2)</math>\n| <math>loaded(3)</math>\n|}\n\nThis is the expected solution. It contains two fluent changes: <math>loaded</math> becomes true at time 1 and <math>alive</math> becomes false at time 3. The following evaluation also satisfies all formulae above.\n\n{| cellpadding="5"\n| <math>alive(0)</math>\n| <math>alive(1)</math> \n| <math>alive(2)</math> \n| <math>alive(3)</math>\n|-\n| <math>\\neg loaded(0)</math>\n| <math>loaded(1)</math>\n| <math>\\neg loaded(2)</math>\n| <math>\\neg loaded(3)</math>\n|}\n\nIn this evaluation, there are still two changes only: <math>loaded</math> becomes true at time 1 and false at time 2. As a result, this evaluation is considered a valid description of the evolution of the state, although there is no valid reason to explain <math>loaded</math> being false at time 2. The fact that minimization of changes leads to wrong solution is the motivation for the introduction of the Yale shooting problem.\n\nWhile the Yale shooting problem has been considered a severe obstacle to the use of logic for formalizing dynamical scenarios, solutions to it are known since the late 1980s. One solution involves the use of [[predicate completion]] in the specification of actions: according to this solution, the fact that shooting causes Fred to die is formalized by the preconditions: \'\'alive\'\' and \'\'loaded\'\', and the effect is that \'\'alive\'\' changes value (since \'\'alive\'\' was true before, this corresponds to \'\'alive\'\' becoming false). By turning this implication into an \'\'if and only if\'\' statement, the effects of shooting are correctly formalized. (Predicate completion is more complicated when there is more than one implication involved.)\n\nA solution proposed by [[Erik Sandewall]] was to include a new condition of occlusion, which formalizes the “permission to change” for a fluent. The effect of an action that might change a fluent is therefore that the fluent has the new value, and that the occlusion is made (temporarily) true. What is minimized is not the set of changes, but the set of occlusions being true. Another constraint specifying that no fluent changes unless occlusion is true completes this solution.\n\nThe Yale shooting scenario is also correctly formalized by the [[Ray Reiter|Reiter]] version of the [[situation calculus]], the [[fluent calculus]], and the [[action description language]]s.\n\nIn 2005, the 1985 paper in which the Yale shooting scenario was first described received the [[AAAI Classic Paper award]]. In spite of being a solved problem, that example is still sometimes mentioned in recent research papers, where it is used as an illustrative example (e.g., for explaining the syntax of a new logic for reasoning about actions), rather than being presented as a problem.\n\n==See also==\n\n* [[Circumscription (logic)]]\n* [[Frame problem]]\n* [[Situation calculus]]\n\n==References==\n\n* M. Gelfond and V. Lifschitz (1993). Representing action and change by logic programs. \'\'Journal of Logic Programming\'\', 17:301–322.\n* S. Hanks and D. McDermott (1987). Nonmonotonic logic and temporal projection. \'\'Artificial Intelligence\'\', 33(3):379–412.\n* J. McCarthy (1986). Applications of circumscription to formalizing common-sense knowledge. \'\'Artificial Intelligence\'\', 28:89–116.\n* T. Mitchell and H. Levesque (2006). The 2005 AAAI Classic Paper awards. "AI Magazine", 26(4):98–99.\n* R. Reiter (1991). The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression. In Vladimir Lifschitz, editor, \'\'Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy\'\', pages 359–380. Academic Press, New York.\n* E. Sandewall (1994). \'\'Features and Fluents\'\'. Oxford University Press.\n\n[[Category:Logic programming]]\n[[Category:Knowledge representation]]\n[[Category:1987 introductions]]']
['Preferential entailment', '3011353', '\'\'\'Preferential entailment\'\'\' is a [[non-monotonic logic]] based on selecting only [[model (logic)|model]]s that are considered the most plausible. The plausibility of models is expressed by an ordering among models called a preference relation, hence the name preference entailment.\n\nFormally, given a [[propositional formula]] <math>F</math> and an ordering over propositional models <math>\\leq</math>, preferential [[entailment]] selects only the models of <math>F</math> that are minimal according to <math>\\leq</math>. This selection leads to a non-monotonic inference relation: <math>F \\models_\\text{pref} G</math> holds if and only if all minimal models of <math>F</math> according to <math>\\leq</math> are also models of <math>G</math>.<ref name="s87">{{citation|last=Shoham|first=Y.|year=1987|contribution=Nonmonotonic logics: Meaning and utility|title=Proc. of the 10th Int. Joint Conf. on Artificial Intelligence (IJCAI’87)|pages=388–392|url=http://ijcai.org/Past%20Proceedings/IJCAI-87-VOL1/PDF/079.pdf}}.</ref>\n\n[[Circumscription (logic)|Circumscription]] can be seen as the particular case of preferential entailment when the ordering is based on containment of the sets of variables assigned to true (in the propositional case) or containment of the extensions of predicates (in the first-order logic case).<ref name="s87"/>\n\n==See also==\n* [[Rational consequence relation]]\n\n==References==\n{{reflist}}\n\n[[Category:Logic in computer science]]\n[[Category:Knowledge representation]]\n[[Category:Non-classical logic]]']
["Allen's interval algebra", '10483232', '{{Use dmy dates|date=June 2013}}\n\'\'For the type of boolean algebra called interval algebra, see [[Boolean algebra (structure)#Examples|Boolean algebra (structure)]]\'\'\n\n\'\'\'Allen\'s interval algebra\'\'\' is a [[calculus (disambiguation)|calculus]] for [[spatial-temporal reasoning|temporal reasoning]] that was introduced by [[James F. Allen]] in 1983.\n\nThe calculus defines possible relations between time intervals and provides a composition table that can be used as a basis\nfor reasoning about temporal descriptions of events.\n\n==Formal description==\n\n=== Relations ===\n\nThe following 13 base relations capture the possible relations between two intervals.\n\n{| class="wikitable"\n !Relation\n !Illustration\n !Interpretation\n |-\n |<math>X \\,\\mathrel{\\mathbf{<}}\\, Y</math>\n<math>Y \\,\\mathrel{\\mathbf{>}}\\, X</math>\n |[[Image:Allen calculus before.png|X takes place before Y]]\n |X takes place before Y\n |-\n |<math>X \\,\\mathrel{\\mathbf{m}}\\, Y</math>\n<math>Y \\,\\mathrel{\\mathbf{mi}}\\, X</math>\n |[[Image:Allen calculus meet.png|X meets Y]]\n |X meets Y (\'\'i\'\' stands for \'\'\'\'\'i\'\'\'nverse\'\')\n |-\n |<math>X \\,\\mathrel{\\mathbf{o}}\\, Y</math>\n<math>Y \\,\\mathrel{\\mathbf{oi}}\\, X</math>\n |[[Image:Allen calculus overlap.png|X overlaps with Y]]\n |X overlaps with Y\n |-\n |<math>X \\,\\mathrel{\\mathbf{s}}\\, Y</math>\n<math>Y \\,\\mathrel{\\mathbf{si}}\\, X</math>\n |[[Image:Allen calculus start.png|X starts with Y]]\n |X starts Y\n |-\n |<math>X \\,\\mathrel{\\mathbf{d}}\\, Y</math>\n<math>Y \\,\\mathrel{\\mathbf{di}}\\, X</math>\n |[[Image:Allen calculus during.png|X during Y]]\n |X during Y\n |-\n |<math>X \\,\\mathrel{\\mathbf{f}}\\, Y</math>\n<math>Y \\,\\mathrel{\\mathbf{fi}}\\, X</math>\n |[[Image:Allen calculus finish.png|X finishes with Y]]\n |X finishes Y\n |-\n |<math>X \\,\\mathrel{\\mathbf{=}}\\, Y</math>\n |[[Image:Allen calculus equal.png|X is equal to Y]]\n |X is equal to Y\n\n |}Using this calculus, given facts can be formalized and then used for automatic reasoning. Relations between intervals are formalized as sets of base relations.\n\nThe sentence\n: \'\'During dinner, Peter reads the newspaper. Afterwards, he goes to bed.\'\'\nis formalized in Allen\'s Interval Algebra as follows:\n\n<math>\\mbox{newspaper } \\mathbf{\\{ \\operatorname{d}, \\operatorname{s}, \\operatorname{f} \\}} \\mbox{ dinner}</math>\n\n<math>\\mbox{dinner } \\mathbf{\\{ \\operatorname{<}, \\operatorname{m} \\}} \\mbox{ bed}</math>\n\nIn general, the number of different relations between n intervals is 1, 1, 13, 409, 23917, 2244361... [http://oeis.org/A055203 OEIS A055203]. The special case shown above is for n=2.\n\n===Composition of relations between intervals===\nFor reasoning about the relations between temporal intervals, Allen\'s Interval Algebra provides a [[Relation composition|composition]] table. Given the relation between <math>X</math> and <math>Y</math> and the relation between <math>Y</math> and <math>Z</math>, the composition table allows for concluding about the relation between <math>X</math> and <math>Z</math>. Together with a [[Inverse relation|converse]] operation, this turns Allen\'s Interval Algebra into a [[relation algebra]].\n\nFor the example, one can infer <math>\\mbox{newspaper } \\mathbf{\\{ \\operatorname{<}, \\operatorname{m} \\}} \\mbox{ bed}</math>.\n\n==Extensions==\nAllen\'s Interval Algebra can be used for the description of both temporal intervals and spatial configurations. For the latter use, the relations are interpreted as describing the relative position of spatial objects. This also works for three-dimensional objects by listing the relation for each coordinate separately.\n\n==Implementation==\n* [https://code.google.com/p/allenintervalrelationships/ A simple java library implementing the concept of Allen\'s temporal relations and the path consistency algorithm]\n\n==See also==\n* [[Temporal logic]]\n* [[Logic]]\n* [[Region Connection Calculus]].\n* [[Spatial relation]] (analog)\n* [[Commonsense reasoning]]\n\n==References==\n* James F. Allen: \'\'Maintaining knowledge about temporal intervals\'\'. In: \'\'Communications of the ACM\'\'. 26 November 1983. ACM Press. pp.&nbsp;832–843, ISSN 0001-0782\n* [[Bernhard Nebel]], Hans-Jürgen Bürckert: \'\'Reasoning about Temporal Relations: A Maximal Tractable Subclass of Allen\'s Interval Algebra.\'\' In: \'\'Journal of the ACM\'\' 42, pp.&nbsp;43–66. 1995.\n* Peter van Beek, Dennis W. Manchak: \'\'The design and experimental analysis of algorithms for temporal reasoning.\'\' In: \'\'Journal of Artificial Intelligence Research\'\' 4, pp.&nbsp;1–18, 1996.\n\n[[Category:Knowledge representation]]\n[[Category:Constraint programming]]']
['Vivification', '15440345', "'''Vivification''' is an operation on a [[description logic]] knowledge base to improve performance of a [[semantic reasoner]].  Vivification replaces a [[Logical disjunction|disjunction]] of concepts <math>C_1 \\sqcup C_2 \\ldots \\sqcup C_n</math> by the ''[[least common subsumer]]'' of the concepts <math>C_1,C_2,\\ldots C_n</math>.\n\nThe goal of this operation is to improve the performance of the reasoner by replacing a complex set of concepts with a single concept which subsumes the original concepts. \n\nFor example, consider the example given in (Cohen 92):  Suppose we have the concept <math>\\textrm{PIANIST(Jill)} \\vee \\textrm{ORGANIST(Jill)}</math>.  This concept can be vivified into a simpler concept <math>\\textrm{KEYBOARD-PLAYER(Jill)}</math>.  This summarization leads to an approximation that may not be exactly equivalent to the original.\n\n== An approximation ==\n\n[[Knowledge base]] vivification is not necessarily exact. If the reasoner is operating under the [[open world assumption]] we may get surprising results.  In the previous example, if we replace the disjunction with the vivified concept,  we will arrive at a surprising results.  \n\nFirst, we find that the reasoner will no longer classify Jill as either a pianist or an organist.  Even though <math>\\textrm{ORGANIST}</math> and <math>\\textrm{PIANIST}</math> are the only two sub-classes, under the OWA we can no longer classify Jill as playing one or the other.  The reason is that there may be another keyboard instrument (e.g. a harpsichord) that Jill plays but which does not have a specific subclass.   \n\n== References ==\n# Cohen, W.W., Borgida, A., Hirsh, H., Computing Least Common Subsumers in Description Logics, In: Proc. AAAI-92, AAAI Press/The MIT Press, 1992, pages 754--760. [http://citeseer.ist.psu.edu/cohen92computing.html CiteSeer]\n# Baader, F., Kusters, R., Wolter F., Extensions to Description Logics. In F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P.F. Patel-Schneider, editors, The Description Logic Handbook: Theory, Implementation, and Applications. Cambridge University Press, 2003. http://citeseer.ist.psu.edu/baader03basic.html\n{{Wikt|vivification}}\n\n[[Category:Knowledge representation]]"]
['BCM Classification', '17952329', "The '''British Catalogue of Music Classification''' (BCM Classification)<ref>The British Catalogue of Music Classification / compiled for the Council of the British National Bibliography Ltd. by E. J. Coates, F.L.A.  London : Council of the British National Bibliography, 1960</ref> is a [[faceted classification]] that was commissioned from E. J. Coates by the Council of the British National Bibliography to organize the content of the British Catalogue of Music.<ref>{{cite web|url=https://archive.org/details/britishcatalogue001781mbp |title=Internet Archive: Details: The British Catalogue Of Music 1960 |publisher=Archive.org |date= |accessdate=2016-10-06}}</ref> The published schedule (1960) was considerably expanded by Patrick Mills of the British Library up until its use was abandoned in 1998. Entries in the catalogue were organized by BCM classmark from the catalogue's inception in 1957 until 1982. From that year the British Catalogue of Music (which from 1974 onward was published by [[The British Library]]) was organized instead by [[Dewey Decimal Classification]] number, though BCM classmarks continued to be added to entries up to the 1998 annual cumulation.\n\nThe schedule is divided into two main parts: A-B representing Musical literature and C-Z representing Music — Scores and Parts.  There are also seven auxiliary tables dealing with various sub-arrangements, sets of ethnic/locality subdivisions and chronological reference points.\n\nThe notation is retroactive using uppercase alphabetic characters omitting I and O, with the addition of slash / and parentheses ( ) which have specific anteriorizing functions.  Retroactive notation requires that the classifier combines terms in reverse schedule order. This has the benefit of producing a compact notation by removing the need for facet indicators.\n\nThe schedule at A (Music Literature) parallels that from the Scores and Parts schedules thus Choral Music is at D while books about Choral Music are at AD; Harp Music is at TQ so books on harp music are at ATQ.  The schedule at B accommodates books about specific composers and music in non-European traditions.\n\nAs a fully faceted scheme after the ideas of [[S. R. Ranganathan]], BCM class numbers are capable of being chain-indexed, allowing index access to each step of the hierarchy.\n\nBCM classification had a strong influence on Russell Sweeney's so-called Phoenix Dewey 780 schedule<ref>DDC Dewey Decimal Classification : proposed revision of 780 music / prepared under the direction of Russell Sweeney and John Clews with assistance from Winton E. Mathews, Jr. Albany N.Y. : Forest Press, 1980. ISBN 0-910608-25-3</ref> which in turn influenced the 780 Music schedule in the 20th edition of [[Dewey Decimal Classification]].  The music schedule of the second edition of the Bliss Classification<ref>{{cite web|url=http://library.music.indiana.edu/tech_s/mla/facacc.rev |title=Archived copy |accessdate=2008-06-15 |deadurl=yes |archiveurl=https://web.archive.org/web/20080512000152/http://library.music.indiana.edu:80/tech_s/mla/facacc.rev |archivedate=2008-05-12 |df= }}</ref> is also strongly influenced by BCM.\n\nThis classification system is still in use at a number of libraries, including the [[State Library of Western Australia]]<ref>{{cite web|url=http://www.slwa.wa.gov.au/find/guides/music/general_information/british_catalogue_of_music_classification_scheme |title=(accessed 2015-12-17) |publisher=Slwa.wa.gov.au |date=2013-08-20 |accessdate=2016-10-06}}</ref> and the Library at [[Edith Cowan University]].<ref>{{cite web|url=http://ecu.au.libguides.com/c.php?g=410622&p=2797056 |title=(accessed 2015-12-17) |publisher=Ecu.au.libguides.com |date=2016-08-05 |accessdate=2016-10-06}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]"]
['Category:Knowledge representation software', '20966976', '[[Category:Knowledge representation]]\n[[Category:Application software]]']
['Pretext', '8280463', 'A \'\'\'pretext\'\'\' (adj: \'\'\'pretextual\'\'\') is an excuse to do something or say something that is not accurate. Pretexts may be based on a half-truth or developed in the context of a misleading fabrication. Pretexts have been used to conceal the true purpose or rationale behind actions and words.\n\nIn [[Law of the United States|US law]], a pretext usually describes false reasons that hide the true intentions or motivations for a legal action. If a party can establish a [[prima facie]] case for the proffered evidence, the opposing party must prove that these reasons were "pretextual" or false. This can be accomplished by directly demonstrating that the motivations behind the presentation of evidence is false, or indirectly by evidence that the motivations are not "credible".<ref name=uslegal>{{cite web|title=Pretext Law & Legal Definition|url=http://definitions.uslegal.com/p/pretext/|publisher=uslegal.com|accessdate=13 March 2013}}</ref> In\n\'\'Griffith v. Schnitzer\'\', an employment discrimination case, a jury award was reversed by a [[Court of Appeals]] because the evidence was not sufficient that the defendant\'s reasons were "pretextual". That is, the defendant\'s evidence was either undisputed, or the plaintiff\'s  was "irrelevant subjective assessments and opinions".<ref>[http://www.omwlaw.com/wp-content/uploads/2013/01/Defining-Pretext-In-Discrimination-Cases.pdf Defining "pretext" in discrimination cases] by Karen Sutherland (2013)</ref>\n\nA "pretextual" arrest by law enforcement officers is one carried out for illegal purposes such as to conduct an unjustified [[search and seizure]].<ref>[http://assets.wne.edu/161/8_note_Criminal.pdf Criminal law - Pretextual arrests and alternatives to the objective tests] by Robert D. Snook</ref><ref name=oday>{{cite web|last=O\'Day|first=Kathleen M.|title=Pretextual traffic stops: injustice for minority drivers|url=http://academic.udayton.edu/race/03justice/s98oday.htm|publisher=The University of Dayton School of Law|accessdate=13 March 2013}}</ref>\n\n[[File:Marbleboot.jpg|thumb|right|140px|[[Marble Boat]] on [[Kunming Lake]] near Beijing.]]\nAs one example of pretext, in the 1880s, the Chinese government raised money on the pretext of modernizing the Chinese navy. Instead, these funds were diverted to repair a ship-shaped, two-story pavilion which had been originally constructed for [[Empress Xiaoshengxian|the mother]] of the [[Qianlong Emperor]]. This pretext and the Marble Barge are famously linked with [[Empress Dowager Cixi]]. This architectural [[folly]], known today as the [[Marble Boat]] (\'\'Shifang\'\'), is "moored" on Lake Kunming in what the empress renamed the "Garden for Cultivating Harmony" (\'\'Yiheyuan\'\').<ref>Min, Anchee. (2007). [https://books.google.com/books?id=_H8TYkS84E4C&pg=PA155&dq=marble+barge&client=firefox-a#PPA155,M1  \'\'The Last Empress,\'\' pp. 155-156;]</ref>\n\nAnother example of pretext was demonstrated in the speeches of the Roman orator [[Cato the Elder]] (234-149 BC). For Cato, every public speech became a pretext for a comment about Carthage. The Roman statesman had come to believe that the prosperity of ancient Carthage represented an eventual and inevitable danger to Rome. In the Senate, Cato famously ended every speech by proclaiming his opinion that [[Carthage]] had to be destroyed (\'\'[[Carthago delenda est]]\'\'). This oft-repeated phrase was the ultimate conclusion of all logical argument in every oration, regardless of the subject of the speech. This pattern persisted until his death in 149, which was the year in which the Third Punic War began. In other words, any subject became a pretext for reminding his fellow senators of the dangers Carthage represented.<ref>Hooper, William Davis \'\'et al.\'\' (1934). [http://penelope.uchicago.edu/Thayer/E/Roman/Texts/Cato/De_Agricultura/Introduction*.html   "Introduction,"] in Cato\'s \'\'De Agricultura\'\' (online version of Loeb edition).</ref>\n\n==Uses in warfare==\n\n[[File:Hokoji-Bell-M1767.jpg|thumb|right|140px|Temple bell at [[Hōkō-ji (Kyoto)|Hōkō-ji]].]][[File:Hokoji-BellDetail-M1767.jpg|thumb|right|140px|Inscription on bell at Hokoji in Kyoto]]\nThe early years of Japan\'s [[Tokugawa shogunate]] were unsettled, with warring factions battling for power. The causes for the fighting were in part pretextural, but the outcome brought diminished armed conflicts after the [[Siege of Osaka]] in 1614-1615.\n\n* \'\'\'1614\'\'\' (\'\'Keichō 19\'\'): The Shogun vanquished Hideyori and set fire to [[Osaka Castle]], and then he returned for the winter to [[Edo]].<ref name="t410">Titsingh, [https://books.google.com/books?id=18oNAAAAIAAJ&pg=PP9&dq=nipon+o+dai+itsi+ran#PRA1-PA410,M1  p. 410.]</ref>\n* \'\'\'August 24, 1614\'\'\' (\'\'Keichō 19, 19th day of the 7th month\'\'): A new bronze bell for the Hōkō-ji was cast successfully [http://oldphoto.lb.nagasaki-u.ac.jp/en/target.php?id=4771][http://oldphoto.lb.nagasaki-u.ac.jp/en/target.php?id=3093]; but despite the dedication ceremony planning, Ieyasu forbade any further actions concerning the great bell:\n::"[T]he tablet over the Daibutsu-den and the bell bore the inscription \'\'"Kokka ankō"\'\' (meaning "the country and the house, peace and tranquility"), and at this [[Tokugawa Ieyasu]] affect to take umbrage, alleging that it was intended as a curse on him for the character 安 (\'\'an,\'\' "peace") was placed between the two characters composing his own name 家康 (\'\'"ka-kō",\'\' "house tranquility") [suggesting subtly perhaps that peace could only be attained by Ieyasu\'s dismemberment?] ... This incident of the inscription was, of course, a mere pretext, but Ieyasu realized that he could not enjoy the power he had usurped as long as Hideyori lived, and consequently, although the latter more than once Hideyori dispatched his vassal Katagiri Kastumoto to Ieyasu\'s residence ([[Sunpu Castle]]) with profuse apologies, Ieyasu refused to be placated."<ref>Ponsonby-Fane, Richard. (1956). \'\'Kyoto, the Old Capital of Japan,\'\' p. 292; Titsingh, [https://books.google.com/books?id=18oNAAAAIAAJ&pg=PP9&dq=nipon+o+dai+itsi+ran#PRA1-PA410,M1  p. 410.]</ref>\n* \'\'\'October 18, 1614\'\'\' (\'\'Keichō 19, 25th day of the 10th month\'\'): A strong earthquake shook Kyoto.<ref name="t410"/>\n* \'\'\'1615\'\'\' (\'\'Keichō 20\'\'): Osaka Summer Battle begins.\n\nThe next two-and-a-half centuries of Japanese history were comparatively peaceful under the successors of Tokugawa Ieyasu and the [[bakufu]] government he established.\n\n===United States===\n*During the War of 1812, US President [[James Madison]] was often accused of using impressment of American sailors by the [[Royal Navy]] as a pretext to invade [[Canada]].\n\n{{main|Pearl Harbor advance-knowledge debate}}\n*Some have argued that United States President [[Franklin D. Roosevelt]] used the [[attack on Pearl Harbor]] by Japanese forces on December 7, 1941 as a pretext to enter [[World War II]].<ref>Bernstein, Richard. [http://www.nytimes.com/1999/12/15/books/books-of-the-times-on-dec-7-did-we-know-we-knew.html "On Dec. 7, Did We Know We Knew?"] \'\'New York Times.\'\' December 15, 1999.</ref> American soldiers and supplies had been assisting British and Soviet operations for almost a year by this point, and the United States had thus "chosen a side", but due to the political climate in the States at the time and some campaign promises made by Roosevelt that he would not send American boys to fight in foreign wars. Roosevelt could not declare war for fear of public backlash. The attack on Pearl Harbor united the American people\'s resolve against the Axis powers and created the bellicose atmosphere in which to declare war.\n* Critics have accused United States President [[George W. Bush]] of using the [[September 11th, 2001 attacks]] and faulty intelligence about the existence of [[weapons of mass destruction]] as a pretext for the [[Iraq war|war in Iraq]].<ref>Borger, Julian. (2006). [https://www.theguardian.com/world/2006/sep/07/usa.books  "Book says CIA tried to provoke Saddam to war,"] \'\'The Guardian\'\' (London). 7 September 2006.</ref>\n\n==Social engineering==\n{{main|Social engineering (security)}}\nA type of [[Social engineering (security)|social engineering]] called [[Social engineering (security)#Pretexting|pretexting]] uses a pretext to elicit information fraudulently  from a target. The pretext in this case includes research into the identity of a certain authorized person or personality type in order to establish legitimacy in the mind of the target.<ref>[[Federal Trade Commission]] (FTC):  [http://www.ftc.gov/bcp/edu/pubs/consumer/credit/cre10.shtm  "Pretexting: Your Personal Information Revealed."] February 2006.</ref>\n\n==See also==\n{{wiktionary}}\n\n* [[Plausible deniability]]\n* [[Proximate cause]]\n* [[Causes of the Franco-Prussian War]]\n\n==Notes==\n{{reflist|2}}\n\n==References==\n* [[James Bamford|Bamford]], James. (2004). [https://books.google.com/books?id=VuOxAAAACAAJ&dq=Pretext+for+War:+9/11,+Iraq,+and+the+Abuse+of+America%27s+Intelligence+Agencies&client=firefox-a  \'\'Pretext for War: 9/11, Iraq, and the Abuse of America\'s Intelligence Agencies.\'\']  New York: [[Doubleday Books]]. ISBN 978-0-385-50672-4; [http://www.worldcat.org/oclc/55068034?referer=di&ht=edition OCLC 55068034]\n* [[Cato the Elder|Cato]], Marcus Porcius. [https://books.google.com/books?id=D2mxAAAAIAAJ&q=Hooper+and+De+Agricultura&dq=Hooper+and+De+Agricultura&lr=&client=firefox-a&pgis=1 \'\'On Agriculture\'\' (\'\'De agricultura\'\')] trans,   William Davis Hooper and Harrison Boyd Ash. Cambridge: [[Harvard University Press]]. [http://www.worldcat.org/oclc/230499252 OCLC  230499252]\n* [[Michael Isikoff|Isikoff]], Michael and [[David Corn]]. 2006. [https://books.google.com/books?id=Sa14AAAAMAAJ&q=hubris&dq=hubris&pgis=1  \'\'Hubris: The Inside Story of Spin, Scandal, and the Selling of the Iraq War\'\']  New York: [[Crown Publishers]]. ISBN 978-0-307-34681-0\n* Min, Anchee. (2007). [https://books.google.com/books?id=_H8TYkS84E4C&client=firefox-a  \'\'The Last Empress.\'\'] New York: [[Houghton Mifflin Harcourt]]. ISBN 978-0-618-53146-2\n* [[Richard Ponsonby-Fane|Ponsonby-Fane]], Richard Arthur Brabazon. (1956). \'\'Kyoto, the Old Capital of Japan,\'\' Kyoto: Ponsonby Memorial Society.\n* [[Robert Stinnett|Stinnett]] Robert B. (2001). [https://books.google.com/books?id=Q2UKN5daNHYC \'\'Day of Deceit: The Truth about FDR and Pearl Harbor\'\'] New York: [[Simon & Schuster]]. ISBN 978-0-7432-0129-2\n* [[Isaac Titsingh|Titsingh]], Isaac. (1834). [Siyun-sai Rin-siyo/[[Hayashi Gahō]], 1652], \'\'[[Nipon o daï itsi ran]]; ou, [https://books.google.com/books?id=18oNAAAAIAAJ&dq=nipon+o+dai+itsi+ran  Annales des empereurs du Japon.\'\']  Paris: [[Royal Asiatic Society|Oriental Translation Fund of Great Britain and Ireland]].\n\n[[Category:Propaganda techniques]]\n[[Category:Knowledge representation]]\n[[Category:Cognition]]\n[[Category:Attack on Pearl Harbor]]\n[[Category:Social engineering (computer security)]]']
['AGRIS', '23241698', "'''AGRIS''' (International System for Agricultural Science and Technology) is a global public domain database with more than 8 million structured bibliographical records on agricultural science and technology. The database is maintained by [[CIARD]], and its content is provided by more than 150 participating institutions from 65 countries. The AGRIS Search system,<ref>{{cite web|url=http://agris.fao.org |title=agris.fao.org |publisher=agris.fao.org |date= |accessdate=2016-03-14}}</ref> allows scientists, researchers and students to perform sophisticated searches using keywords from the [[AGROVOC]] thesaurus, specific journal titles or names of countries, institutions, and authors.\n\n== Early AGRIS years ==\nAs [[information management]] flourished in the 1970s, the AGRIS [[metadata]] [[Text corpus|corpus]] was developed to allow its users to have free access to knowledge available in agricultural science and technology. AGRIS was developed to be an international cooperative system to serve both developed and [[developing countries]].\n\nWith the advent of the Internet, along with the promises offered by [[open access (publishing)|open access]] publishing, there was growing awareness that the management of agricultural science and technology information, would have various facets: [[Technical standard|standards]] and methodologies for [[interoperability]] and facilitation of knowledge exchange; tools to enable information management specialists to process data; information and knowledge exchange across countries. Common [[interoperability]] criteria were thus adopted in its implementation, and the AGRIS AP [[metadata]] was accordingly created in order to allow exchange and retrieval of Agricultural information Resources.<ref>{{cite web|url=http://www.fao.org/docrep/008/ae909e/ae909e00.htm |title=The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology Guidelines on Best Practices for Information Object Description |publisher=Fao.org |date= |accessdate=2013-07-09}}</ref>\n\n== AGRIS 2.0 ==\nAGRIS covers the wide range of subjects related to agriculture science and technology, including forestry, animal husbandry, aquatic sciences and fisheries, human nutrition, and extension. Its content includes unique grey literature such as unpublished scientific and technical reports, theses, conference papers, government publications, and more. A growing number (around 20%) of bibliographical records have a corresponding full text document on the web which can easily be retrieved by Google.\n\nOn 5th December 2013 AGRIS 2.0 was released. AGRIS 2.0 is at the same time:\n\n# A collaborative network of more than 150 institutions from 65 countries, maintained by FAO of the UN, promoting free access to agricultural information.\n# A multilingual bibliographic database for agricultural science, fuelled by the AGRIS network, containing more than 8 million records largely enhanced with AGROVOC, FAO’s multilingual thesaurus covering all areas of interest to FAO, including food, nutrition, agriculture, fisheries, forestry, environment etc.\n# A mash-up web application that links the bibliographic AGRIS knowledge to related resources on the web using the [[Linked Open Data]] methodology. An AGRIS mashup page (e.g. http://agris.fao.org/agris-search/search.do?recordID=QM2008000025 ) is a web page where an AGRIS resource is displayed together with relevant knowledge extracted from external data sources (as the World Bank, DBPedia, and Nature). The availability of external data sources is not under AGRIS control. Thus, if an external data source is temporary unreachable, it won’t be displayed in AGRIS mashup pages.\n\nAccess to the AGRIS Repository is provided through the AGRIS Search Engine.<ref>{{cite web|url=http://agris.fao.org/ |title=agris.fao.org |publisher=agris.fao.org |date= |accessdate=2013-07-09}}</ref>  As such, it:\n# enables retrieval of bibliographic records contained in the AGRIS Repository,\n# allows users to perform either full-text or fielded, parametric and assisted queries.\n\nAGRIS data was converted to RDF and the resulting linked dataset created some 200 million triples.\nAGRIS is also registered in the Data Hub at http://thedatahub.org/dataset/agris\n\nThe AGRIS partners contributing to the AGRIS Database use several formats for exchanging data, including simple DC, from [[OAI-PMH]] systems.\nThe AGRIS AP format is anyway adopted directly by:\n# Open Archive Initiative (OAI) partners: Scielo, Viikki Science Library\n# BIBSYS, Norway, National Library of Portugal, Wageningen UR Library.\n# ''National networks'': NARIMS<ref>[http://www.arc.sci.eg/ arc.sci.eg]</ref> in [[Egypt]], [[PhilAgriNet]] in [[Philippines]], [[KAINet]] in [[Kenya]], NAC in [[Thailand]], GAINS in [[Ghana]].\n# ''National institutional repositories'': Russia, Belarus, Uruguay, Spain, Iran.\n# ''Information service providers'': [[Wolters Kluwer]], [[NISC]], CGIR, [[CGIAR]], [[Agriculture Network Information Center|AgNIC]], GFIS.\n# ''Database systems/tools'': AgriOceanDspace,<ref>{{cite web|url=http://aims.fao.org/agriocean-dspace |title=AgriOcean DSpace &#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-09}}</ref> NewGenlib, WebAGRIS, NERAKIN, AgriDrupal.<ref>{{cite web|url=http://aims.fao.org/tools/agridrupal |title=AgriDrupal &#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-09}}</ref>\n\n=== AGRIS under the CIARD umbrella ===\nFalling under the umbrella of CIARD,<ref>{{cite web|url=http://www.ciard.net |title=What is CIARD? &#124; Coherence in Information for Agricultural Research for Development |publisher=Ciard.net |date= |accessdate=2013-07-09}}</ref> a joint initiative co-led by the CGIAR,<ref>{{cite web|last=Rijsberman |first=Frank |url=http://www.cgiar.org |title=CGIAR Home |publisher=Cgiar.org |date=2013-07-04 |accessdate=2013-07-09}}</ref> GFAR<ref>{{cite web|url=http://www.egfar.org |title=EGFAR web Site |publisher=Egfar.org |date= |accessdate=2013-07-09}}</ref> and [[FAO]], the new AGRIS aims to promote the sharing and management of agricultural science and technology information through the use of common [[Technical standard|standards]] and methodologies. These will incorporate [[Web 2.0]] features, in order to make the search experience as comprehensive, intuitive and far-reaching as possible for users of the new AGRIS.\n\nFurthermore, the new AGRIS will also leverage the data and infrastructure of one of CIARD's projects: the CIARD RING. An acronym standing for Routemap to Information Nodes and Gateways (RING), the CIARD RING project is led by [[Global Forum on Agricultural Research|GFAR]] and it aims to:\n\n* give an overview of the current offer of information services in ARD; as well as\n* support those who want to implement new services.\n\nA directory of ARD (Agricultural Research for Development) information services will allow the monitoring, describing and classifying of existing services, whilst benchmarking them against [[interoperability]] criteria, to ensure for maximum outreach and global availability.\n\n== See also ==\n* [[Agricultural Information Management Standards]]\n* [[AgMES]]\n* [[Agricultural Ontology Service]]\n* [[AGROVOC]]\n* [[Information management]]\n* [[IMARK]]\n* [[Disciplinary repository]]\n\n== References ==\n{{Reflist|2}}\n\n== Other publications ==\n* [http://f1000research.com/articles/4-432/v2 Discovering, Indexing and Interlinking Information Resources (F1000research 2015)]\n* [http://f1000research.com/articles/4-110/v1 AGRIS: providing access to agricultural research data exploiting open data on the web (F1000research 2015)]\n* [http://iospress.metapress.com/content/l15562xk70234n79/fulltext.pdf Migrating bibliographic datasets to the Semantic Web: The AGRIS case (Semantic Web journal 2013)]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n* [https://web.archive.org/web/20140908155657/http://eprints.rclis.org/21112/1/2013_EFITA%20Pushing_Pulling.pdf Pushing, Pulling, Harvesting, Linking - Rethinking Bibliographic Workflows for the Semantic Web (EFITA-2013)] \n* [http://www.fao.org/documents/advanced_s_result.asp?FORM_C=AND&SERIES=339 Agricultural Information and Knowledge Management Papers]\n* [http://www.fao.org/docrep/008/ae909e/ae909e00.htm The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology Guidelines on Best Practices for Information Object Description]\n* [http://www.fao.org/kce/consultations/coaim/coaim-2000/en/ Consultations on Agricultural Management (COAIM)]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n* [http://www.fao.org/docrep/x7936e/x7936e00.htm First Consultation on Agricultural Information Management (2000 COAIM Report)]\n* [http://www.fao.org/docrep/meeting/005/y7963e/Y7963e00.htm#Top Report on the Second Consultation on Agricultural Information Management (2002 COAIM Report)]\n* [http://departments.agri.huji.ac.il/economics/gelb-agris-10.pdf#AGRIS 1968-1994: Insights and Lessons]\n\n== External links ==\n* [http://agris.fao.org '''AGRIS''']\n* [http://agris.fao.org/agris-search/agrisMap.do AGRIS network map]\n* [http://agrovoc.fao.org/axis/services/SKOSWS?wsdl AGROVOC Web services]\n* [http://aims.fao.org/ Agricultural Information Management Standards (AIMS)]\n* [http://www.fao.org Food and Agriculture Organization of the United Nations (FAO) Web site]\n* [http://www.ciard.net Coherence in Information for Agricultural Research for Development (CIARD) Wed site]\n* [http://www.fao.org/nems/rss/rss_nems_results.asp?owner=615&status=10&dateto=31/12/2006&lang=en&sites=1 RSS feed of news and events]\n\n{{DEFAULTSORT:Agris}}\n[[Category:Agricultural organizations]]\n[[Category:Interoperability]]\n[[Category:Knowledge representation]]\n[[Category:Metadata]]\n[[Category:Food and Agriculture Organization]]\n[[Category:Public domain databases]]\n[[Category:Agricultural databases]]"]
['GNOWSYS', '450307', '{{Refimprove|date=June 2011}}\n{{Infobox software\n|  name = GNOWSYS\n|  logo = [[Image:Gnowsys-logo.png|100px]]\n|  developer = [[GNU|The GNU Project]]\n|  latest_release_version = 1.0 rc1\n|  operating_system = [[Cross-platform]]\n|  genre = [[Semantic web|Semantic computing]]\n|  license = [[GNU General Public License|GPL]]\n|  website = [https://www.gnu.org/software/gnowsys/ www.gnu.org/software/gnowsys/]\n}}\n\'\'\'GNOWSYS\'\'\' (Gnowledge Networking and Organizing system) is a specification for a generic [[distributed network]] based memory/[[knowledge management]]. It is developed as an application for developing and maintaining [[semantic web]] content. It is written in [[Python (programming language)|Python]]. It is implemented as a [[Django (web framework)|Django]] app.\n\nThe memory of GNOWSYS is designed as a node-oriented space. A node is described by other nodes to which it has links. The nodes are organized and processed according to a complex data structure called the neighborhood.<ref name="gnu">[https://www.gnu.org/software/gnowsys/] GNOWSYS: A Kernel for Semantic Computing.</ref>\n\n==Applications==\n\nThe application can be used for web-based knowledge representation and content management projects, for developing structured knowledge bases, as a collaborative authoring tool, suitable for making electronic glossaries, dictionaries and encyclopedias, for managing large web sites or links, developing an online catalogue for a library of any thing including books, to make ontologies, classifying and networking any objects, etc. This tool is also intended to be used for an on-line tutoring system with dependency management between various concepts or software packages.  For example, the dependency relations between [[Debian GNU/Linux]] packages have been represented by the [http://www.gnowledge.org/search_debmap?val=1 gnowledge portal].\n\n==Component Classes==\nThe kernel is designed to provide support to persistently store highly granular nodes of knowledge representation like terms, predicates and very complex propositional systems like arguments, rules, axiomatic systems, loosely held paragraphs, and more complex structured and consistent compositions. All the component classes in GNOWSYS are classified according to complexity into three groups, where the first two groups are used to express all possible well formed formulae permissible in a first order logic.<ref name="conceptPaper">[http://www.hbcse.tifr.res.in/gn/concept_paper.pdf GNOWSYS: A System for Semantic Computing ]</ref>\n\n===Terms===\n‘Object’, ‘Object Type’ for declarative knowledge, ‘Event’, ‘Event Type’, for temporal objects, and ‘Meta Types’ for expressing upper ontology. The\nobjects in this group are essentially any thing about which the [[knowledge engineer]] intends to express and store in the knowledge base, i.e., they are the objects of discourse. The instances of these component classes can be stored with or without expressing ‘instance of’ or ‘sub-class of’ relations among them.\n\n===Predicates===\nThis group consists of ‘Relation’, and ‘Relation Type’ for expressing declarative knowledge, and ‘Function’ and ‘Function Type’ for expressing procedural knowledge. This group is to express qualitative and quantitative relations among the various instances stored in the knowledge base. While instantiating the predicates can be characterized by their logical properties of relations, quantifiers and cardinality as monadic predicates\nof these predicate objects.\n\n===Structures===\n‘System’, ‘Encapsulated Class’, ‘Program’, and ‘Process’, are other base classes for complex structures, which can be combined iteratively to produce more complex systems. The component class ‘System’ is to store in the knowledge base a set of propositions composed into ontologies, axiomatic systems, complex systems like say a human body, an artifact like a vehicle etc., with or without consistency check. An ‘Encapsulated Class’ is to com-\npose declarative and behavioural objects in a flexible way to build classes. A ‘Program’ is not only to store the logic of any complete program or a component class, composed from the already available behavioural instances in the knowledge base with built-in connectives (conditions, and loops), but also execute them as web services. A ‘Process’ is to structure temporal objects with sequence, concurrency, synchronous or asynchronous specifications.\n\nEvery node in the database keeps the neighbourhood information, such as its super-class, sub-class, instance-of, and other relations, in which the object has a role, in the form of predicates. This feature makes computation of drawing graphs and inferences, on the one hand, and dependency and navigation paths on the other hand very easy.  All the data and metadata is indexed in a central catalogue making query and locating resources efficient.\n\n==References==\n{{Reflist}}\n\n==External links==\n{{Portal|Free software}}\n* [http://www.gnowledge.org/ Welcome to Gnowledge!]\n* [https://www.gnu.org/software/gnowsys/ GNOWSYS is part of the GNU project.]\n\n{{GNU}}\n\n{{DEFAULTSORT:Gnowsys}}\n[[Category:Cross-platform free software]]\n[[Category:Free network-related software]]\n[[Category:GNU Project software]]\n[[Category:Knowledge representation]]\n[[Category:Semantic Web]]']
['Linguistic value', '15299080', ':\'\'For the similar but unrelated term in linguistics see \'\' [[Linguistic variable]]\n\nIn [[artificial intelligence]], [[operations research]], and related fields, a \'\'\'Linguistic value\'\'\', (for some authors \'\'\'Linguistic variable\'\'\') is a [[natural language]] term which is derived using quantitative or qualitative reasoning such as with probability and statistics or [[fuzzy sets and systems]].<ref>\'\'Fuzzy Logic for Business and Industry\'\' Earl Cox, Charles River Media, pp188,214,302,306,352  1995 ISBN 1-886801-01-0</ref><ref>\'\'The Fuzzy Systems Handbook, Second Edition\'\' Earl Cox, Academic Press, 1999 ISBN 0-12-194455-7 Ch 6 Fuzzy Reasoning, &sect; 1 The Role of Linguistic Variables</ref>\n<ref>\'\'On the Modeling of Linguistic Information using Random Sets\'\' Hung T. Nguyen p. 242 in Readings in Fuzzy Sets for Intelligent Systems. Morgan Kaufmann 1993. Dubois, Prade, and Yager eds. </ref>\n<ref>\'\'Fuzzy Sets And The Social Nature of Truth\'\' J. Goguen. CS UCLA p. 49-67 in Advances in Fuzzy Sets and Systems, North Holland, 1979. &sect; 2.3 \'\'Linguistic Truth Values\'\'. ISBN 0-444-85372-3</ref>\n\n==Example of Linguistic Value==\n\nFor example, if a shuttle heat shield is deemed of having a linguistic value of a "very low" percentage of damage in re-entry, based upon knowledge from experts in the field, that probability would be given a value of say, 5%.  From there on out, if it were to be used in an equation, the variable of percentage of damage will be at 5% if it deemed very low percentage.\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Linguistic Value}}\n[[Category:Knowledge representation]]\n\n\n{{AI-stub}}']
['Ontology alignment', '4696039', '\'\'\'Ontology alignment\'\'\', or \'\'\'ontology matching\'\'\', is the process of determining correspondences between [[concept]]s in [[ontologies]]. A set of correspondences is also called an alignment. The phrase takes on a slightly different meaning, in [[computer science]], [[cognitive science]] or [[philosophy]].\n\n==Computer Science==\n\nFor [[computer scientist]]s, concepts are expressed as labels for data.  Historically, the need for ontology alignment arose out of the need to [[data integration|integrate]] heterogeneous [[database]]s, ones developed independently and thus each having their own data vocabulary.  In the [[Semantic Web]] context involving many actors providing their own [[ontology (information science)|ontologies]], ontology matching has taken a critical place for helping heterogeneous resources to interoperate. Ontology alignment tools find classes of data that are "[[semantic equivalence|semantically equivalent]]," for example, "Truck" and "Lorry."  The classes are not necessarily logically identical.  According to Euzenat and Shvaiko (2007),<ref name="Euzenat Shvaiko">Jérôme Euzenat and Pavel Shvaiko. 2007. [http://book.ontologymatching.org Ontology matching], Springer-Verlag, 978-3-540-49611-3.</ref> there are three major dimensions for similarity: syntactic, external, and semantic.  Coincidentally, they roughly correspond to the dimensions identified by Cognitive Scientists below.  A number of tools and frameworks have been developed for aligning ontologies, some with inspiration from Cognitive Science and some independently.\n\nOntology alignment tools have generally been developed to operate on [[database schema]]s,<ref>J. Berlin and A. Motro. 2002. [http://www.dit.unitn.it/~accord/RelatedWork/Matching/Berlin_caise02.pdf Database Schema Matching Using Machine Learning with Feature Selection]. Proc. of the 14th International Conference on Advanced Information Systems Engineering, pp. 452-466</ref> [[XML schema]]s,<ref name="coma">D. Aumueller, H. Do, S. Massmann, E. Rahm. 2005. [http://www.dit.unitn.it/~p2p/RelatedWork/Matching/COMA++-SIGMOD05.pdf Schema and ontology matching with COMA++]. Proc. of the 2005 International Conference on Management of Data, pp. 906-908</ref> [[Taxonomy (general)|taxonomies]],<ref>S. Ponzetto, R. Navigli. 2009. [http://ijcai.org/papers09/Papers/IJCAI09-343.pdf "Large-Scale Taxonomy Mapping for Restructuring and Integrating Wikipedia"]. Proc. of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009), Pasadena, California, pp. 2083-2088.</ref> [[formal language]]s, [[entity-relationship model]]s,<ref>A. H. Doan, A. Y. Halevy. [http://pages.cs.wisc.edu/~anhai/papers/si-survey-db-community.pdf Semantic integration research in the database community: A brief survey]. AI magazine, 26(1), 2005</ref> [[dictionary|dictionaries]], and other label frameworks. They are usually converted to a graph representation before being matched. \nSince the emergence of the Semantic Web, such graphs can be represented in the [[Resource Description Framework]] line of languages by triples of the form <subject, predicate, object>, as illustrated in the [[Notation 3]] syntax.\nIn this context, aligning ontologies is sometimes referred to as "ontology matching".\n\nThe problem of Ontology Alignment has been tackled recently by trying to compute matching first and mapping (based on the matching) in an automatic fashion. Systems like [[DSSim]], X-SOM<ref name="curino-xsom2007">{{cite journal|author=Carlo A. Curino and Giorgio Orsi and Letizia Tanca |title=X-SOM: A Flexible Ontology Mapper |url=http://www.polibear.net/blog/wp-content/uploads/2008/01/orsi-xsomflexmap.pdf |journal=International Workshop on Semantic Web Architectures for Enterprises (SWAE\'07) in conjunction with the 18th International Conference on Database and Expert Systems Applications (DEXA\'07) |year=2007 |format= |deadurl=yes |archiveurl=https://web.archive.org/web/20120213104823/http://www.polibear.net/blog/wp-content/uploads/2008/01/orsi-xsomflexmap.pdf |archivedate=February 13, 2012 }}</ref> or COMA++ obtained at the moment very high precision and recall.<ref name="coma" /> The [http://oaei.ontologymatching.org Ontology Alignment Evaluation Initiative] aims to evaluate, compare and improve the different approaches.\n\nMore recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on [[Minimal mappings|Minimal Mappings]]. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).\n\n=== Formal Definition ===\nGiven two ontologies <math>i=\\langle C_{i}, R_{i}, I_{i}, A_{i}\\rangle</math> and <math>j=\\langle C_{j}, R_{j}, I_{j}, A_{j}\\rangle</math>{{clarify|Give a (link to a) formal definition of \'ontology\' first, such that the meaning of C, R, I, and A is explained.|date=January 2017}} we can define different type of (inter-ontology) relationships among their terms. \nSuch relationships will be called, all together, alignments and can be categorized among different dimensions:\n\n* similarity vs logic: this is the difference between matchings (predicating about the [[semantic similarity|similarity]] of ontology terms), and mappings ([[logical axiom]]s, typically expressing [[logical equivalence]] or inclusion among ontology terms)\n* atomic vs complex: whether the alignments we considered are [[bijection|one-to-one]], or can involve more terms in a query-like formulation (e.g., [[data integration|LAV/GAV]] mapping)\n* homogeneous vs heterogeneous: do the alignments predicate on terms of the same type (e.g., classes are related only to classes, individuals to individuals, etc.) or we allow heterogeneity in the relationship?\n* type of alignment: the semantics associated to an alignment. It can be [[Hierarchy#Subsumptive containment hierarchy|subsumption]], [[logical equivalence|equivalence]], [[disjointness]], [[part-of]] or any user-specified relationship.\n\nSubsumption, atomic, homogeneous alignments are the building blocks to obtain richer alignments, and have a well defined semantics in every Description Logic. \nLet\'s now introduce more formally ontology matching and mapping.\n\nAn atomic homogeneous \'\'\'matching\'\'\' is an alignment that carries a similarity degree <math>s\\in [0,1]</math>, describing the similarity of two terms of the input ontologies <math>i</math> and <math>j</math>.\nMatching can be either \'\'computed\'\', by means of heuristic algorithms, or \'\'[[inference|inferred]]\'\' from other matchings.\n\nFormally we can say that, a matching is a quadruple <math>m=\\langle id, t_{i}, t_{j}, s\\rangle</math>, where <math>t_{i}</math> and <math>t_{j}</math> are homogeneous ontology terms, <math>s</math> is the similarity degree of <math>m</math>. \nA (subsumption, homogeneous, atomic) mapping is defined as a pair <math>\\mu=\\langle t_{i}, t_{j}\\rangle</math>, where <math>t_{i}</math> and <math>t_{j}</math> are homogeneous ontology terms.\n\n==Cognitive Science==\n\nFor [[cognitive scientist]]s interested in ontology alignment, the "concepts" are nodes in a [[semantic network]] that reside in brains as "conceptual systems."  The focal question is: if everyone has unique experiences and thus different semantic networks, then how can we ever understand each other?  This question has been addressed by a model called ABSURDIST (Aligning Between Systems Using Relations Derived Inside Systems for Translation). Three major dimensions have been identified for similarity as equations for "internal similarity, external similarity, and mutual inhibition."<ref>R. Goldstone and B. Rogosky. 2002. [http://courses.media.mit.edu/2003spring/mas963/goldstone.pdf Using relations within conceptual systems to translate across conceptual systems]. Cognition 84, pp. 295–320.</ref>\n\nOntology alignment is closely related to [[analogy formation]], where "concepts" are variables in logic expressions.\n\n==Ontology alignment methods==\nTwo sub research fields have emerged in recent years in ontology mapping, namely monolingual ontology mapping and cross-lingual ontology mapping. The former refers to the mapping of ontologies in the same natural language, whereas the latter refers to "the process of establishing relationships among ontological resources from two or more independent ontologies where each ontology is labelled in a different natural language".<ref>Bo Fu, Rob Brennan, Declan O\'Sullivan, A Configurable Translation-Based Cross-Lingual Ontology Mapping System to adjust Mapping Outcomes. Journal of Web Semantics, Volume 15, 15-36, ISSN 1570-8268, 2012 [http://www.sciencedirect.com/science/article/pii/S1570826812000704].</ref> Existing matching methods in monolingual ontology mapping are discussed in Euzenat and Shvaiko (2007).<ref name="Euzenat Shvaiko"/> Current approaches to cross-lingual ontology mapping are presented in Fu et al. (2011).<ref>Fu B., Brennan R., O\'Sullivan D., Using Pseudo Feedback to Improve Cross-Lingual Ontology Mapping [http://www.springerlink.com/content/a214858426kgm750/]. In Proceedings of the 8th Extended Semantic Web Conference (ESWC 2011), LNCS 6643, pp.336-351, Heraklion, Greece, May 2011.</ref>\n\n==Philosophy==\n\nFor philosophers, much like cognitive scientists, the interest is in the nature of "understanding."  The roots of discourse, however, may be traced to [[radical interpretation]].\n\n==Visualization Tools (links obsolete)==\n*[http://www.mondeca.com/content/download/718/6964/file/ITM_ALIGN_en.pdf ITM Align: semi-automated ontology alignment]\n*[http://cs.uga.edu/~uthayasa/Optima/Optima.html Optima: A Visual Ontology Alignment Tool]\n*[http://www.stanford.edu/~sfalc/cogz/cogz.html CogZ: Cognitive Support and Visualization for Human-Guided Mapping Systems]\n*[http://agreementmaker.org AgreementMaker: Efficient Matching for Large Real-World Schemas and Ontologies]\n*[http://bio-mixer.appspot.com/ Biomixer]: A Web-based Collaborative Ontology Visualization Tool.\n*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6882028 SDI(Semantic Data Integration) Tool]: A Semantic Mapping Representation and Generation Tool Using UML for System Engineers\n\n==See also==\n* [[Ontology (computer science)]]\n* [[Rule Interchange Format]] (RIF)\n* [[Data conversion]]\n* [[Semantic Integration]]\n* [[Semantic matching]]\n* [[Minimal mappings|Minimal Mappings]]\n* [[Semantics|Interpretation]] "An interpretation can be the part of a presentation or portrayal of information altered in order to conform to a specific set of symbols."\n* [[Graph isomorphism]]\n* [[Unification (computer science)]] (as [[Semantic unification]])\n* [[Semantic integration]]\n\n==References==\n{{Reflist|colwidth=35em}}\n\n== Further reading ==\n*[http://www.ontologymatching.org/publications.html Collection of surveys and research papers related to ontology mapping, matching, and alignment]\n* [http://www.atl.external.lmco.com/projects/ontology/ The Ontology Alignment Source]\n* [http://cognitrn.psych.indiana.edu/rgoldsto/pdfs/cogsci2002.pdf ABSURDIST]\n* [http://ontologymatching.org/publications.html Ontologymatching.org: Surveys, Approaches, and Themes]\n* [http://knoesis.wright.edu/library/publications/iswc10_paper218.pdf Ontology Alignment for Linked Open Data]\n* [http://sites.google.com/site/bschopman/master-thesis/master.pdf Instance-based Ontology Matching by Instance Enrichment]\n* Noy, N. F. (2004). "Semantic integration: a survey of ontology-based approaches." SIGMOD Rec. 33(4): 65-70.\n\n[[Category:Ontology (information science)]]\n[[Category:Semantic Web]]\n[[Category:Knowledge engineering]]\n[[Category:Information science]]\n[[Category:Knowledge representation]]']
['AGROVOC', '5465574', '\'\'\'AGROVOC\'\'\' (a portmanteau of agriculture and vocabulary) is a multilingual controlled vocabulary covering all areas of interest to the [[Food and Agriculture Organization of the United Nations]] (FAO), including food, nutrition, agriculture, fisheries, forestry and the environment. The vocabulary consists of over 32,000 concepts with up to 40,000 terms in 23 languages: Arabic, Chinese, Czech, English, French, German, Hindi, Hungarian, Italian, Japanese, Korean, Lao, Malay, Persian, Polish, Portuguese, Russian, Slovak, Spanish, Telugu, Thai, Turkish and Ukrainian. It is a collaborative effort, edited by a community of experts and coordinated by FAO.\n\nAGROVOC is made available by FAO as an [[Resource Description Framework|RDF]]/[[SKOS]]-XL concept scheme and published as a [[linked data]] set aligned to 16 other vocabularies.\n\n==History==\nAGROVOC was first published at the beginning of the 1980s by FAO in English, Spanish and French to serve as a controlled vocabulary to index publications in agricultural science and technology, especially for [[AGRIS]].\n\nIn the 1990s, AGROVOC abandoned paper printing and went digital with data storage handled by a relational database. In 2004, preliminary experiments with expressing AGROVOC into the [[Web Ontology Language]](OWL) took place. At the same time a web based editing tool was developed, then called WorkBench, nowadays VocBench. In 2009 AGROVOC became an SKOS resource.\n\nToday, AGROVOC is available in 23 languages as an SKOS-XL concept scheme and published as a Linked Open Data (LOD) set aligned to 16 other data sets related to agriculture.\n\n==Users==\nAGROVOC is used by researchers, librarians and information managers for indexing, retrieving and organizing data in agricultural information systems and web pages.<ref>[http://aims.fao.org/standards/agrovoc/uses-agrovoc AGROVOC Uses]</ref> Within the context of the [[Semantic Web]] also new users are emerging, like software developers and ontology builders.\n\n==Access==\nAGROVOC is accessible in various ways:\n\n* Online: Search <ref>[http://aims.fao.org/standards/agrovoc/functionalities/search AGROVOC search]</ref> and browse AGROVOC on the [[Agricultural Information Management Standards]] (AIMS) website. \n* Download: RDF-SKOS (AGROVOC only or AGROVOC LOD).<ref>[https://aims-fao.atlassian.net/wiki/display/AGV/Releases AGROVOC Releases]</ref> \n* Live: SPARQL endpoint <ref>[http://202.45.139.84:10035/catalogs/fao/repositories/agrovoc AGROVOC SPARQL endpoint]</ref> and the AGROVOC Web services.<ref>[https://aims-fao.atlassian.net/wiki/display/AGV/Releases AGROVOC releases]</ref>\n\n==Maintenance==\nThe AGROVOC team, located at FAO Headquarter, coordinates the editorial activities related to the maintenance of AGROVOC. The actual maintenance is carried out by a community of editors and institutions<ref>[http://aims.fao.org/standards/agrovoc/community AGROVOC people]</ref> for each of the 23 language versions.\n\nThe tool used by the community to edit and maintain AGROVOC is Vocbench, which was designed to meet the needs of the Semantic Web and linked data environments. VocBench provides tools and functionalities that facilitate both collaborative editing and multilingual terminology. It also includes administration and group management features that permit flexible roles for maintenance, validation and quality assurance.\n\nFAO also facilitates the technical maintenance of AGROVOC, including its publication as a LOD resource. Technical support is provided by the University of Tor Vergata<ref>[http://art.uniroma2.it/ Tor Vergata University]</ref> (Rome, Italy) which leads the technical development of VocBench. The technical infrastructure for the online publication of AGROVOC is hosted by MIMOS Berhad<ref>[http://www.mimos.my/ MIMOS Berhad]</ref> (Kuala Lumpur, Malaysia).\n\n==Structure==\nAll 32,000+ concepts of the AGROVOC thesaurus are hierarchically organized under 25 top concepts. AGROVOC top concepts are very general and high level concepts, like “activities”, “organisms”, “location”, “products” etc. More than half of the total number of concepts (20,000+) fall under the top concept “organism”, which confirms how AGROVOC is largely oriented towards the agricultural sector.\n\nAGROVOC is an RDF/SKOS-XL concept scheme, meaning the conceptual and terminological level are separated. The basic notions for such a concept scheme are: concepts, their labels and relations.\n\n*\'\'\'Concepts\'\'\' \nConcepts are anything we want to represent or “talk about” in our domain. Concepts are represented by terms. A concept could also be considered as the set of all terms used to express it in various languages.\nIn SKOS, concepts are formalized as skos:Concept, identified by dereferenceable URIs (= URL). For example, the AGROVOC concept with URI http://aims.fao.org/aos/agrovoc/c_12332 is for \'\'maize\'\'.\n\n*\'\'\'Terms\'\'\' \nTerms are the actual terms used to name a concept. For example \'\'maize\'\', \'\'maïs\'\', \'\'玉米\'\', \'\'ข้าวโพด\'\' are all terms used to refer to the same concept in English, French, Chinese and Hindi respectively.\n\nAGROVOC terms are expressed by means of the SKOS extension for labels, SKOS-XL. The predicates used are:\nskosxl:prefLabel, used for preferred terms (“descriptors” in thesaurus terminology), and \nskosxl:altLabel, used for non- preferred terms.\n\n*\'\'\'Relations\'\'\'\nIn SKOS, hierarchical relations between concepts are expressed by the predicates skos:broader, skos:narrower. They correspond to the classical thesaurus relations broader/narrower (BT/NT).\n\nNon-hierarchical relations express a notion of “relatedness” between concepts. AGROVOC uses the SKOS relation skos:related (corresponding to the classical thesaurus RT), and a specific vocabulary of relations called Agrontology.<ref>[https://aims-fao.atlassian.net/wiki/display/AGV/Agrontology Agrontology]</ref>\n\nAGROVOC also allows for relations between labels (i.e. terms), thanks to the SKOS-XL extension to SKOS.\n\n==Linked data==\nAGROVOC is available as a linked data set and is aligned (linked) with 16 vocabularies related to agriculture (see table down below). The linked data version of AGROVOC is exposed as RDF and HTML, through a content-negotiation mechanism. It is also exposed through a SPARQL endpoint.\n\nThe advantage of having a thesaurus like AGROVOC published as LOD is that once thesauri are linked, the resources they index are linked as well. A good example is AGRIS, a mash-up web application that links the AGRIS bibliographic repository (indexed with AGROVOC) to related web resources (indexed with vocabularies linked to AGROVOC).\n\n{| class="wikitable"\n|-\n! Resource !! Topics !! Linked concpets !! Languages !! Linked data !! Type of link\n|-\n| ASFA || Fisheries|| 1784|| || || skos:closeMatch\n|-\n| FAO Biotechnology Glossary || Biotechnologies|| 810|| EN, ES, FR, +3 more|| Yes|| skos:closeMatch\n|-\n| Chinese Agriculture Thesaurus (CAT)|| Agriculture|| || || Yes|| skos:closeMatch\n|-\n| EARTh|| Environment|| 1363 || EN+|| Yes|| skos:closeMatch\n|-\n| EUROVOC|| General EU || 1,297 || EN, ES, FR + 21 more || Yes || skos:exactMatch\n|-\n| GEMET || Environment || 1,191 || EN, ES, FR + 30 more || Yes|| skos:exactMatch\n|-\n| Library of Congress Subject Headings (LCSH)|| General || 1,093 || EN || Yes || skos:exactMatch\n|-\n| NAL Thesaurus || Agriculture || 13,390 || EN,ES || Yes || skos:exactMatch\n|-\n| RAMEAU Répertoire d\'autorité-matière encyclopedique et alphabetique unifie  || General || 686 || FR || Yes || skos:exactMatch\n|-\n| STW - Thesaurus for Economics || Economy || 1,136 || EN, DE || Yes || skos:exactMatch\n|-\n| TheSoz - Thesaurus for the Social Sciences || Social sciences || 846 || EN,DE || Yes || skos:exactMatch\n|-\n| Geopolical Ontology || Geopolitical entities || 253 || AR, CH, EN, ES, FR, RU || Yes || skos:exactMatch\n|-\n| Dewey Decimal Classification (DDC) || General || 409 || EN, ES, FR + 8 more || Yes || skos:exactMatch\n|-\n| DBpedia || General || 10,989 || EN, ES, FR + 8 more || Yes || skos:exactMatch\nskos:closeMatch\n|-\n| SWD (Schlagwortnormdatei)|| General || 6,245 || DE || Yes || skos:exactMatch\nskos:closeMatch\nskos:broadMatch\nskos:narrowMatch\n|-\n| GeoNames || Geographical entities || 212 || EN, ES, FR + 63 more || Yes || skos:exactMatch\n|}\n\n==Copyright and license==\nThe copyright for the AGROVOC thesaurus content in English, French, Russian and Spanish stays with FAO and is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.<ref>[http://creativecommons.org/licenses/by-sa/3.0/ Creative Commons Attribution ShareAlike 3.0 Unported]</ref> For any other language, the copyright rests with the institution responsible for its production.\n\n==Related links==\n* [[Agricultural Information Management Standards]]\n* [[AGRIS]]\n* [[Food and Agriculture Organization]]\n* [[Geopolitical ontology]]\n\n==External links==\n* [http://agris.fao.org/ AGRIS]\n* [http://aims.fao.org/standards/agrovoc AGROVOC]\n* [http://aims.fao.org/ AIMS]\n* [http://www.fao.org/home/en/ FAO]\n* [https://www.w3.org/2001/sw/wiki/VocBench VocBench/Agricultural Ontology Server]\n\n==Further reading==\n* [http://aims.fao.org/standards/agrovoc/publications AGROVOC Publications]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Agrovoc}}\n[[Category:Agricultural databases]]\n[[Category:Knowledge representation]]\n[[Category:Ontology (information science)]]\n[[Category:Food and Agriculture Organization]]\n[[Category:Thesauri]]']
['Knowledge value chain', '5118075', 'A \'\'\'[[knowledge value]] chain\'\'\' is a sequence of intellectual tasks by which [[knowledge workers]] build their employer\'s unique competitive advantage <ref>Carlucci, D., Marr, B. and Schiuma, G. (2004) \'The knowledge value chain: how intellectual capital impacts on business performance\', \'\'Int J. Technology Management\'\', Vol. 27, Nos. 6/7, pp. 575&ndash;690  [http://www.som.cranfield.ac.uk/som/dinamic-content/research/cbp/2004,%20Knowledge%20Value%20Chain%20(IJTM%2027,%206-7,%20Carlucci,%20Marr,%20Schiuma).pdf (pdf)]</ref> and/or social and environmental benefit. As an example, the components of a research and development project form a knowledge value chain.\n\nProductivity improvements in a knowledge value chain may come from [[knowledge integration]] in its original sense of data systems consolidation. Improvements also flow from the knowledge integration that occurs when [[knowledge management]] techniques are applied to the continuous improvement of a business process or processes.<ref>[http://www.edgeltd.com/performance-consultants-services/edge_service.php?service=3 Canada Edge Performance Consultants] - official page</ref>\n\nThe term first started coming into common use around 1999, appearing in management-related talks and papers.<ref>[http://www.aurorawdc.com/kmworld99.htm 1999 KMWorld conference program], listing Powell\'s talk on "The Knowledge Value Chain - Aligning       Knowledge Workers with Competitive Strategy"</ref><ref>[http://www.ingentaconnect.com/content/mcb/026/2000/00000019/00000009/art00003 "Knowledge value chain"],\'\'The Journal of Management Development\'\',            Volume 19, Number 9, 2000, pp. 783&ndash;794(12)</ref><ref>Tim Powell, "Knowledge Value Chain", May 2001, Proceeding of 22nd National Online Meeting, Information Today ([http://www.knowledgeagency.com/pdf_center/Knowledge_Value_Chain.pdf pdf)]</ref> It was registered as a trademark in 2004 by TW Powell Co., a [[Manhattan]] company.<ref>[http://www.knowledgeagency.com TW Powell Co. website]</ref><ref>U.S. Trademark, December 2004. 2,912,705</ref>\n\n\'\'\'Knowledge value chain processes\'\'\'\n*Knowledge acquisition\n*Knowledge storage\n*Knowledge dissemination\n*Knowledge application\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Knowledge Value Chain}}\n[[Category:Knowledge representation]]']
['VoID', '20451641', "{{Other uses|Void (disambiguation)}}\n{{Multiple issues|\n{{Expert-subject|Internet|date=November 2008}}\n{{Technical|date=November 2008}}\n{{Unreferenced|date=December 2008}}\n{{Orphan|date=February 2009}}\n{{Underlinked|date=November 2013}}\n}}\n\nThe '''Vocabulary of Interlinked Datasets''' ('''VoID''') is an [[Resource Description Framework|RDF]] vocabulary, and a set of instructions, that enables the discovery and usage of [[Linked Data|linked data]] sets. A linked dataset is a collection of data, published and maintained by a single provider, available as RDF on the Web, where at least some of the resources in the dataset are identified by dereferencable URIs.\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.w3.org/TR/void/ Describing Linked Datasets with the VoID Vocabulary, W3C TR]\n* [http://semanticweb.org/wiki/VoID VoID at Semantic Web Wiki]\n\n{{Semantic Web}}\n\n{{DEFAULTSORT:Void}}\n[[Category:Metadata]]\n[[Category:Semantic Web]]\n[[Category:Knowledge representation]]\n[[Category:XML-based standards]]"]
['ISO 15926', '4724116', 'The \'\'\'ISO 15926\'\'\' is a standard for [[data integration]], sharing, exchange, and hand-over between [[computer system]]s.\n\nThe title, "\'\'Industrial automation systems and integration&mdash;Integration of life-cycle data for process plants including oil and gas production facilities\'\'",  is regarded too narrow by the present ISO 15926 developers. Having developed a [[generic data model]] and Reference Data Library for process plants, it turned out that this subject is already so wide, that actually any state information may be modelled with it.\n\n== History ==\nIn 1991 a European Union [[European Strategic Program on Research in Information Technology|ESPRIT]]-, named ProcessBase, started. The focus of this research project was to develop a [[data model]] for lifecycle information of a facility that would suit the requirements of the process industries. At the time that the project duration had elapsed, a consortium of companies involved in the process industries had been established: [[EPISTLE]] (European Process Industries STEP Technical Liaison Executive). Initially individual companies were members, but later this changed into a situation where three national consortia were the only members: PISTEP (UK), POSC/Caesar (Norway), and USPI-NL (Netherlands). (later PISTEP merged into POSC/Caesar, and USPI-NL was renamed to USPI).\n\nEPISTLE took over the work of the ProcessBase project. Initially this work involved a standard called ISO 10303-221 (referred to as "[[ISO_10303|STEP]] AP221"). In that AP221 we saw, for the first time, an Annex M with a list of standard instances of the AP221 data model, including types of objects. These standard instances would be for reference and would act as a knowledge base with knowledge about the types of objects.\nIn the early nineties EPISTLE started an activity to extend Annex M to become a library of such object classes and their relationships: STEPlib. In the STEPlib activities a group of approx. 100 domain experts from all three member consortia, spread over the various expertises (e.g. Electrical, Piping, Rotating equipment, etc.), worked together to define the "core classes".\n\nThe development of STEPlib was extended with many additional classes and relationships between classes and published as [[Open Source]] data. Furthermore, the concepts and relation types from the AP221 and ISO 15926-2 data models were also added to the STEPlib dictionary. This resulted in the development of [[Gellish English]], whereas STEPlib became the [[Gellish English dictionary]]. Gellish English is a structured subset of natural English and is a [[modeling language]] suitable for [[knowledge modeling]], [[product modeling]] and [[data exchange]]. It differs from conventional modeling languages ([[meta language]]s) as used in information technology as it not only defines generic concepts, but also includes an English dictionary. The semantic expression capability of Gellish English was significantly increased by extending the number of relation types that can be used to express knowledge and information.\n\nFor modelling-technical reasons POSC/Caesar proposed another standard than [[ISO 10303]], called ISO 15926. EPISTLE (and ISO) supported that proposal, and continued the modelling work, thereby writing Part 2 of ISO 15926. This Part 2 has official ISO IS (International Standard) status since 2003.\n\nPOSC/Caesar started to put together their own RDL (Reference Data Library). They added many specialized classes, for example for [[American National Standards Institute|ANSI]] (American National Standards Institute) pipe and pipe fittings. Meanwhile STEPlib continued its existence, mainly driven by some members of USPI. Since it was clear that it was not in the interest of the industry to have two libraries for, in essence, the same set of classes, the Management Board of EPISTLE decided that the core classes of the two libraries shall be merged into Part 4 of ISO 15926. This merging process has been finished. Part 4 should act as reference data for part 2 of ISO 15926 as well as for ISO 10303-221 and replaced its Annex M. On June 5, 2007 ISO 15926-4 was signed off as a TS (Technical Specification).\n\nIn 1999 the work on an earlier version of Part 7 started. Initially this was based on [[XML Schema (W3C)|XML Schema]] (the only useful W3C Recommendation available then), but when [[Web Ontology Language|Web Ontology Language (OWL)]] became available it was clear that provided a far more suitable environment for Part 7. Part 7 passed the first ISO ballot by the end of 2005, and an implementation project started. A formal ballot for TS (Technical Specification) was planned for December 2007. However, it was decided then to split Part 7 into more than one part, because the scope was too wide.\n\n== The standard ==\n{{External links|date=May 2012}}\nISO 15926 has eleven parts (as of June 2009):\n\n* Part 1 [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=29556] - Introduction, information concerning engineering, construction and operation of production facilities is created, used and modified by many different organizations throughout a facility\'s lifetime. The purpose of ISO 15926 is to facilitate integration of data to support the lifecycle activities and processes of production facilities.\n* Part 2 [http://www.stanford.edu/group/narratives/classes/08-09/CEE215/ReferenceLibrary/FIATECH%20ISO%2015926/ISO/pack/ISO%2015926%20part2%20pack/ECM4.5/lifecycle_integration_schema.html]- Data Model. a generic 4D model that can support all disciplines, supply chain company types and life cycle stages, regarding information about functional requirements, physical solutions, types of objects and individual objects as well as activities.\n* Part 3 - Reference data for geometry and topology.\n* Parts 4 [http://data.posccaesar.org/rdl/] - Reference Data, the terms used within facilities for the process industry.\n* Part 7 [http://www.15926.org] - Integration of life-cycle data for process plants including oil and gas production facilities - Part 7: Implementation methods for the integration of distributed systems: Template methodology.\n* Part 8 [http://www.15926.org] - Integration of life-cycle data for process plants including oil and gas production facilities - Part 8: Implementation methods for the integration of distributed systems: [[Web Ontology Language]] (OWL/RDF) implementation.\n* Part 9 (in development)- Implementation standards, with the focus on Façades, standard web servers, web services, and security.\n* Part 10 (in development)- Test Methods.\n* Part 11 (in development)- Industrial Usage Guidelines.\n* Part 12 (in development)- Life cycle integration ontology in [[Web Ontology Language]] (OWL2).\n* Part 13 (in development)- Integrated lifecycle asset planning.\n\n=== Description ===\nThe model and the library are suitable for representing lifecycle information about technical installations and their components.\n\nThey can also be used for defining the terms used in product catalogs in [[e-commerce]]. Another, more limited, use of the standard is as a reference classification for harmonization purposes between shared databases and product catalogues that are not based on ISO 15926.\n\nThe purpose of ISO 15926 is to provide a [[Lingua franca|Lingua Franca]] for computer systems, thereby integrating the information produced by them. Although set up for the process industries with large projects involving many parties, and involving plant operations and maintenance lasting decades, the technology can be used by anyone willing to set up a proper vocabulary of reference data in line with Part 4.\n\nIn Part 7 the concept of Templates is introduced. These are semantic constructs, using Part 2 entities, that represent a small piece of information. These constructs then are mapped to more efficient classes of n-ary relations that interlink the Nodes that are involved in the represented information.\n\nIn Part 8 the data model of Part 2 is mapped to OWL, and so are, in concept, the Reference Data of Part 4 and the templates of Part 7. For validation and reasoning purposes all are represented in First-Order Logic as well.\n\nIn Part 9 these Node and Template instances are stored in Façades. A Façade is an RDF [[quad store]], set up to a standard schema and an API. Any Façade only stores the data for which the Façade owner is responsible.\n\nEach participating computer system maps its data from its internal format to such ISO-standard Node and Template instances. These are stored in a System Façade, each system its own Façade.\n\nData can be "handed over" from one Façade to another in cases where data custodianship is handed over (e.g. from a contractor to a plant owner, or from a manufacturer to the owners of the manufactured goods). Hand-over can be for a part of all data, whilst maintaining full referential integrity.\n\nFaçades can be set up for the consolidation of data by handing over data produced by various participating computer systems and stored in their System Façades. Examples are: a Façade for a project discipline, a project, a plant).\n\nDocuments are user-definable. They are defined in [[XML Schema]] and they are, in essence, only a structure containing cells that make reference to instances of Templates. This represents a view on all lifecycle data: since the data model is a 4D (space-time) model, it is possible to present the data that was valid at any given point in time, thus providing a true historical record. It is expected that this will be used for Knowledge Mining.\n\nData can be queried by means of [[SPARQL]]. In any implementation a restricted number of Façades can be involved, with different access rights. This is done by means of creating a CPF Server (= Confederation of Participating Façades). An [[Ontology (computer science)|Ontology]] Browser allows for access to one or more Façades in a given CPF, depending on the access rights.\n\n== Projects and applications ==\n{{External links|date=May 2012}}\nThere are a number of projects working on the extension of the ISO 15926 standard in different application areas.\n\n=== Capital-intensive projects ===\n\nWithin the application of Capital Intensive projects, some cooperating implementation projects are running:\n\n* The EDRC Project of [http://www.fiatech.org FIATECH] [http://www.fiatech.org/images/stories/projects/Project_Resumes/EDRC_Resume_v8_Sept_13_2013.pdf Capturing Equipment Data Requirements Using ISO 15926 and Assessing Conformance]. [http://techinvestlab.ru/EDRCDemo Example data and videos.]\n* The ADI Project of [http://www.fiatech.org FIATECH], to build the tools (which will then be made available in the public domain)\n** The tools and deliverables can be seen on the ISO 15926 knowledge base: [http://15926.org]\n* The IDS Project of [http://www.posccaesar.org POSC Caesar Association], to define product models required for data sheets\n* A joint ADI-IDS project is the [[ISO 15926 WIP]]\n* The DEXPI project: The objective of DEXPI is to develop and promote a general standard for the process industry covering all phases of the lifecycle of a (petro-)chemical plant, ranging from specification of functional requirements to assets in operation. See more at [http://www.dexpi.org dexpi.org]\n\n=== Upstream Oil and Gas industry ===\n\nThe [[Norwegian Oil Industry Association]] (OLF) has decided to use ISO 15926 (also known as the [[Oil and Gas Ontology]]) as the instrument for integrating data across disciplines and business domains for the [[Upstream (oil industry)|Upstream Oil and Gas industry]]. It is seen as one of the enablers of what has been called the next (or second) generation of [[Integrated operations]], where a better integration across companies is the goal.<ref>{{cite web |url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf |title=Integrated Operations and the Oil and Gas Ontology |author=The Norwegian Oil Industry Association (OLF) |accessdate=2009-05-06 }}</ref>\n\nThe following projects are currently running (May 2009):\n\n* The [[Integrated Operations in the High North|Integrated Operations in the High North (IOHN)]] project is working on extending ISO 15926 to handle real-time data transmission and (pre-)processing to enable the next generation of [[Integrated operations|Integrated Operations]].\n* The [http://trac.posccaesar.org/wiki/EW Environment Web] project to include environmental reporting terms and definitions  as used in [http://www.epim.no EPIM]\'s [http://www.epim.no/default.asp?id=945 EnvironmentWeb] in ISO 15926.\n\nFinalised projects include:\n\n* The [http://trac.posccaesar.org/wiki/IIP Integrated Information Platform (IIP)] project working on establishing a real-time information pipeline based on open standards. It worked among others on:\n** [http://www.posccaesar.org/wiki/NcsDdr Daily Drilling Report (DDR)] to including all terms and definitions in ISO 15926. This standard became mandatory on February 1, 2008<ref>{{cite web |url=http://www.npd.no/English/Produkter+og+tjenester/Skjemaer/CDRS_reporting_oct_2007.htm |title=Drilling Reporting to the authorities |author=Norwegian Petroleum Directorate |accessdate=2009-05-05 }}</ref> for reporting on the [[Norwegian Continental Shelf]] by the [[Norwegian Petroleum Directorate|Norwegian Petroleum Directorate (NPD)]] and [http://www.ptil.no/main-page/category9.html Safety Authority Norway (PSA)]. NPD says that the quality of the reports has improved considerably since.\n** [http://www.posccaesar.org/wiki/NcsDpr Daily Production Report (DPR)] to including all terms and definitions in ISO 15926. This standard was tested successfully on the [[Valhall oil field|Valhall]] ([[BP]]-operated) and [http://www.statoilhydro.com/en/ouroperations/explorationprod/ncs/aasgard/pages/default.aspx Åsgard] ([[StatoilHydro]]-operated) fields offshore Norway. The terminology and XML schemata developed have also been included in [http://www.Energistics.org Energistics]’ [[PRODML]] standard.\n\n== Some technical background ==\nOne of the main requirements was (and still is) that the scope of the data model covers the entire lifecycle of a facility (e.g. oil refinery) and its components (e.g. pipes, pumps and their parts, etc.). Since such a facility over such a long time entails many different types of activities on a myriad of different objects it became clear that a generic and data-driven data model would be required.\n\nA simple example will illustrate this. There are thousands of different types of physical objects in a facility (pumps, compressors, pipes, instruments, fluids, etc). Each of these has many properties. If all combinations would be modelled in a "hard-coded" fashion, the number of combinations would be staggering, and unmanageable.\n\nThe solution is a "template" that represents the semantics of: "This object has a property of  X yyyy" (where yyyy is the unit of measure). Any instance of that template refers to the applicable reference data:\n* physical object (e.g. my Induction Motor)\n* indirect property type (e.g. the class "cold locked rotor time")\n* base property type (here: time)\n* scale (here: seconds)\n\nWithout being able to make reference to those classes, via the Internet, it will be impossible to express this information.\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [http://15926.org 15926.org]: A forum for ISO 15926 discussions and team collaboration.\n* [http://iringug.org/wiki/index.php?title=Main_Page iringug.org]: -An online community of users, companies, and organizations that have common interest in solutions that implement ISO 15926 reference data and protocols.\n* [http://iringtoday.com iringtoday.com]: - An online ISO 15926 thought leadership community geared toward engineering management.\n* [http://techinvestlab.ru/ISO15926en .15926 Editor] Open source software to view, edit and verify ISO 15926 data.\n* [http://wings.buffalo.edu/philosophy/ontology/bfo/west.pdf Against Idiosyncrasy in Ontology Development]: A critical study of ISO 15926 and of the claims made on its behalf.\n* [http://www.matthew-west.org.uk/publications/ResponseToBarrySmithCommentsOnISO15926.pdf A Response to "Against Idiosyncrasy in Ontology Development"]: A rebuttal of "Against Idiosyncracy in Ontology Development".\n\n{{ISO standards}}\n\n{{DEFAULTSORT:Iso 15926}}\n[[Category:ISO standards|#15926]]\n[[Category:Semantic Web]]\n[[Category:Knowledge engineering]]\n[[Category:Technical communication]]\n[[Category:Information science]]\n[[Category:Ontology (information science)]]\n[[Category:Knowledge representation]]']
['Concept map', '698226', '{{for|concept maps in [[generic programming]]|Concept (generic programming)}}\n[[File:Electricity Concept Map.gif|thumb|An Electricity Concept Map, an example of a concept map]]\n{{InfoMaps}}\nA \'\'\'concept map\'\'\' or \'\'\'conceptual diagram\'\'\' is a [[diagram]] that depicts suggested relationships between [[concept]]s.<ref>Peter J. Hager,Nancy C. Corbin. \'\'Designing & Delivering: Scientific, Technical, and Managerial Presentations,\'\' 1997, . 163.</ref> It is a graphical tool that [[instructional designer]]s, [[engineer]]s, [[Technical communication|technical writers]], and others use to organize and structure [[knowledge]].\n\nA concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in linking phrases such as \'\'causes\'\', \'\'requires\'\', or \'\'contributes to\'\'.<ref name=theory>[[Joseph D. Novak]] & Alberto J. Cañas (2006). [http://cmap.ihmc.us/Publications/ResearchPapers/TheoryCmaps/TheoryUnderlyingConceptMaps.htm "The Theory Underlying Concept Maps and How To Construct and Use Them"], [[Institute for Human and Machine Cognition]]. Accessed 24 Nov 2008.</ref>\n\nThe technique for [[Visualization (graphic)|visualizing]] these relationships among different concepts is called \'\'concept mapping\'\'. Concept maps define the [[Ontology (information science)|ontology]] of computer systems, for example with the [[object-role modeling]] or  [[Unified Modeling Language]] formalism.\n\n== Overview ==\nA concept map is a way of representing relationships between [[idea]]s, [[image]]s, or [[word]]s in the same way that a [[sentence diagram]] represents the grammar of a sentence, a road map represents the locations of highways and towns, and a [[circuit diagram]] represents the workings of an electrical appliance. In a concept map, each word or phrase connects to another, and links back to the original idea, word, or phrase. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. An example of the use of concept maps is provided in the context of learning about types of fuel.{{clarify|why is this noteworthy?|date=November 2016}}<ref name="CMP">[http://www.energyeducation.tx.gov/pdf/223_inv.pdf CONCEPT MAPPING FUELS]. Accessed 24 Nov 2008.</ref>\n\nConcept maps were developed{{whom|date=November 2016}} to enhance meaningful learning in the sciences{{fact|date=November 2016}}. A well-made concept map grows within a \'\'context frame\'\' defined by an explicit "focus question", while a [[mind map]] often has only branches radiating out from a central picture. Some research evidence suggests that the brain stores knowledge as productions (situation-response conditionals) that act on [[declarative memory]] content, which is also referred to as chunks or propositions.<ref>Anderson, J. R., & Lebiere, C. (1998). The atomic components of thought. Mahwah, NJ: Erlbaum.</ref><ref>Anderson, J. R., Byrne, M. D., Douglass, S., Lebiere, C., & Qin, Y. (2004). An Integrated Theory of the Mind. Psychological Review, 111(4), 1036&ndash;1050.</ref> Because concept maps are constructed to reflect organization of the declarative memory system, they facilitate sense-making and meaningful learning on the part of individuals who make concept maps and those who use them.\n\n== Differences from other visualizations ==\n\n\'\'\'[[Topic map]]s:\'\'\'  Concept maps are rather similar to topic maps in that both allow to concepts or topics via [[graph (data structure)|graphs]]. Among the various schema and techniques for visualizing ideas, processes, and organizations, concept mapping, as developed by [[Joseph D. Novak|Joseph Novak]] is unique in its philosophical basis, which "makes concepts, and propositions composed of concepts, the central elements in the structure of knowledge and construction of meaning."<ref>Novak, J.D. & Gowin, D.B. (1996). Learning How To Learn, Cambridge University Press: New York, p. 7.</ref>\n\n\'\'\'[[mind mapping|Mind maps]]:\'\'\'  Both concept maps and topic maps can be contrasted with [[mind mapping]], which is often restricted to radial hierarchies and [[tree structure]]s. Another contrast between concept mapping and mind mapping is the speed and spontaneity when a mind map is created. A mind map reflects what you think about a single topic, which can focus group brainstorming. A concept map can be a map, a system view, of a real (abstract) system or set of concepts. Concept maps are more free form, as multiple hubs and clusters can be created, unlike mind maps, which fix on a single two centered approach.\n\n== History ==\nThe technique of concept mapping was developed by [[Joseph D. Novak]] and his research team at [[Cornell University]] in the 1970s as a means of representing the emerging science knowledge of students.<ref>{{cite web|url=http://www.ihmc.us/users/user.php?UserID=jnovak|title=Joseph D. Novak|publisher=Institute for Human and Machine Cognition (IHMC)|accessdate=2008-04-06}}</ref> It has subsequently been used as a tool to increase meaningful learning in the sciences and other subjects as well as to represent the expert knowledge of individuals and teams in education, government and business. Concept maps have their origin  in the learning movement called [[constructivism (learning theory)|constructivism]]. In particular, constructivists hold that learners actively construct knowledge.\n\nNovak\'s work is based on the cognitive theories of [[David Ausubel]], who stressed the importance of prior knowledge in being able to learn (or \'\'assimilate\'\') new concepts: "The most important single factor influencing learning is what the learner already knows. Ascertain this and teach accordingly."<ref>Ausubel, D. (1968) Educational Psychology: A Cognitive View. Holt, Rinehart & Winston, New York.</ref> Novak taught students as young as six years old to make concept maps to represent their response to focus questions such as "What is water?" "What causes the seasons?" In his book \'\'Learning How to Learn\'\', Novak states that a "meaningful learning involves the assimilation of new concepts and propositions into existing cognitive structures."\n\nVarious attempts have been made to conceptualize the process of creating concept maps. Ray McAleese, in a series of articles, has suggested that mapping is a process of \'\'off-loading\'\'. In this 1998 paper, McAleese draws on the work of Sowa<ref>Sowa, J.F., 1983. \'\'Conceptual structures: information processing in mind and machine\'\', Addison-Wesley.</ref> and a paper by Sweller & Chandler.<ref>Sweller, J. & Chandler, P., 1991. Evidence for Cognitive Load Theory. \'\'Cognition and Instruction\'\', 8(4), p.351-362.</ref> In essence, McAleese suggests that the process of making knowledge explicit, using \'\'nodes\'\' and \'\'relationships\'\', allows the individual to become aware of what they know and as a result to be able to modify what they know.<ref>McAleese,R (1998) \'\'\'The Knowledge Arena\'\'\' as an Extension to the Concept Map: Reflection in Action, \'\'Interactive Learning Environments\'\', \'\'\'6,3,p.251-272\'\'\'.</ref> Maria Birbili applies that same idea to helping young children learn to think about what they know.<ref>Birbili, M. (2006) [http://ecrp.uiuc.edu/v8n2/birbili.html "Mapping Knowledge: Concept Maps in Early Childhood Education"], \'\'Early Childhood Research & Practice\'\', \'\'8(2)\'\', Fall 2006</ref> The concept of the \'\'knowledge arena\'\' is suggestive of a virtual space where learners may explore what they know and what they do not know.\n\n==Use==\n[[Image:Conceptmap.png|thumb|450px|Example concept map created using the IHMC CmapTools computer program.]]\nConcept maps are used to stimulate the generation of ideas, and are believed to aid [[creativity]].<ref name="theory"/> Concept mapping is also sometimes used for [[brain-storming]]. Although they are often personalized and idiosyncratic, concept maps can be used to communicate complex ideas.\n\nFormalized concept maps are used in [[software design]], where a common usage is [[Unified Modeling Language]] diagramming amongst similar conventions and development methodologies.\n\nConcept mapping can also be seen as a first step in [[ontology (computer science)|ontology]]-building, and can also be used flexibly to represent formal argument.\n\nConcept maps are widely used in education and business. Uses include:\n*[[Note taking]] and summarizing gleaning key concepts, their relationships and hierarchy from documents and source materials\n*New knowledge creation: e.g., transforming [[tacit knowledge]] into an organizational resource, mapping team knowledge\n*Institutional knowledge preservation (retention), e.g., eliciting and mapping expert knowledge of employees prior to retirement\n*Collaborative knowledge modeling and the transfer of expert knowledge\n*Facilitating the creation of shared vision and shared understanding within a team or organization\n*Instructional design: concept maps used as [[David Ausubel|Ausubelian]] "advance organizers" that provide an initial conceptual frame for subsequent information and learning.\n*Training: concept maps used as [[David Ausubel|Ausubelian]] "advanced organizers" to represent the training context and its relationship to their jobs, to the organization\'s strategic objectives, to training goals.\n*Communicating complex ideas and arguments\n*Examining the symmetry of complex ideas and arguments and associated terminology\n*Detailing the entire structure of an idea, [[train of thought]], or line of argument (with the specific goal of exposing faults, errors, or gaps in one\'s own reasoning) for the scrutiny of others.\n*Enhancing [[metacognition]] (learning to learn, and thinking about knowledge)\n*Improving language ability\n*Assessing learner understanding of learning objectives, concepts, and the relationship among those concepts\n*Lexicon development\n\n==See also==\n{{list|date=November 2016}}\n{{colbegin}}\n* [[Argument map]]\n* [[Cognitive map]]\n* [[Conceptual graphs]]\n* [[Conceptual framework]]\n* [[Idea networking]]\n* [[Knowledge visualization]]\n* [[List of concept- and mind-mapping software]]\n* [[Mental model]]\n* [[Mind map]]\n* [[Radial tree]]\n* [[Entity-relationship model]]\n* [[Nomological network]]\n* [[Semantic web]]\n* [[Topic Maps]]\n* [[Educational psychology]]\n* [[Educational technology]]\n* [[Morphological analysis (problem-solving)|Morphological analysis]]\n* [[Wicked problem]]\n* [[Object role modeling]]\n* [[Personal knowledge base]]\n* [[Semantic network]]\n* [[Olog]]\n* [[Pathfinder network]]\n* [[Sensemaking]]\n{{colend}}\n\n==References==\n{{reflist|2}}\n\n== Further reading ==\n* {{cite book |last= Novak |first= J.D. |title= Learning, Creating, and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations |publisher= Routledge |edition= 2nd |date= 2009 |isbn= 9780415991858 }}\n<!-- |publisher= Lawrence Erlbaum Associates |location= Mahwah |date= 1998 |edition= 1st -->\n* {{cite book |last1= Novak |first1= J.D. |last2= Gowin |first2= D.B. |title= Learning How to Learn |publisher= Cambridge University Press |location= Cambridge |date= 1984 |isbn= 9780521319263 }}\n\n== External links ==\n{{commons|Concept map}}\n{{wikiversity|Concept mapping}}\n* [http://www.mind-mapping.org/images/walt-disney-business-map.png Example of a concept map from 1957] by Walt Disney.\n\n{{Mindmaps}}\n\n{{DEFAULTSORT:Concept Map}}\n[[Category:Concepts]]\n[[Category:Constructivism (psychological school)]]\n[[Category:Diagrams]]\n[[Category:Educational technology]]\n[[Category:Graph drawing]]\n[[Category:Knowledge representation]]\n[[Category:Note-taking]]\n[[Category:Visual thinking]]']
['Folksonomy', '23219749', 'A \'\'\'folksonomy\'\'\' is a system in which users apply public [[Tag (metadata)|tags]]  to online items, typically to aid them in re-finding those items. This can give rise to a classification system based on those tags and their frequencies, in contrast to a [[Taxonomy (general)|taxonomic]] classification specified by the owners of the content when it is published.<ref>{{cite news\n | title = Folksonomies. Indexing and Retrieval in Web 2.0.\n | url = https://books.google.com/books?id=Aeib_wy18gkC&printsec=frontcover&dq=folksonomies.+Indexing+and+Retrieval+in+Web+2.0#v=onepage&q&f=false\n | first = Isabella\n | last = Peters\n | work = Berlin: De Gruyter Saur\n | year = 2009\n }}</ref><ref>{{cite news\n | title = Folksonomy\n | first = Daniel H.\n | last = Pink\n | authorlink =\n | url = http://www.nytimes.com/2005/12/11/magazine/11ideas1-21.html\n | work = New York Times\n | date = 11 December 2005\n | accessdate = 14 July 2009\n }}</ref> This practice is also known as \'\'\'collaborative tagging\'\'\',<ref>Lambiotte, R, and M Ausloos. 2005. Collaborative tagging as a tripartite network. http://arxiv.org/abs/cs.DS/0512090.</ref><ref>{{cite web|last1=Borne|first1=Kirk|title=Collaborative Annotation for Scientific Data Discovery and Reuse|url=http://www.asis.org/Bulletin/Apr-13/AprMay13_RDAP_Borne.html|website=Bulletin of Association for Information Science and Technology|publisher=ASIS&T|accessdate=26 May 2016}}</ref> \'\'\'social classification\'\'\', \'\'\'social indexing\'\'\', and \'\'\'social tagging\'\'\'. However, these terms have slightly different meanings than folksonomy. Folksonomy was originally “the result of personal free tagging of information [...] for one’s own retrieval.”.<ref>Vander Wal, Thomas. "Folksonomy Coinage and Definition". Retrieved October 25, 2015 from <nowiki>http://www.vanderwal.net/folksonomy.html</nowiki></ref> \'\'\'Social tagging\'\'\' is the application of tags in an open online environment where the tags of other users are available to others. \'\'\'Collaborative tagging\'\'\' (also known as group tagging) is tagging performed by a group of users. This type of folksonomy is commonly used in cooperative and collaborative projects such as research, content repositories, and social bookmarking.\n\nThe term was coined by [[Thomas Vander Wal]] in 2004<ref>{{cite news\n | title = Folksonomy Coinage and Definition\n | url = http://www.vanderwal.net/folksonomy.html\n | first = Tomas\n | last = Vander Wal\n | date = 11 December 2005\n }}</ref><ref>Vander Wal, T. (2005). "[http://www.vanderwal.net/random/category.php?cat=153 Off the Top: Folksonomy Entries]." Visited November 5, 2005. See also: Smith, Gene. "[https://web.archive.org/web/20040828035712/http://atomiq.org/archives/2004/08/folksonomy_social_classification.html Atomiq: Folksonomy: social classification]." Aug 3, 2004. Retrieved January 1, 2007.</ref><ref>http://vanderwal.net/folksonomy.html Origin of the term</ref> as a [[portmanteau]] of \'\'[[Volk (German word)|folk]]\'\' and \'\'[[Taxonomy (general)|taxonomy]]\'\'. Folksonomies became popular as part of [[social software]] applications such as [[social bookmarking]] and photograph annotation that enable users to collectively classify and find information via shared tags. Some websites include [[tag cloud]]s as a way to visualize tags in a folksonomy.<ref>{{Cite journal\n | last1 = Lamere | first1 = Paul\n | title = Social Tagging And Music Information Retrieval\n | journal = Journal of New Music Research\n | volume = 37\n | issue = 2\n | pages = 101–114\n | date = June 2008\n | url = http://www.informaworld.com/smpp/content~db=all~content=a906001732\n | doi = 10.1080/09298210802479284 }}</ref>\n\nFolksonomies can be used for K-12 education, business, and higher education. More specifically, folksonomies may be implemented for social bookmarking, teacher resource repositories, e-learning systems, collaborative learning, collaborative research, and professional development.\n\n==Benefits and disadvantages==\nFolksonomies are a trade-off between traditional centralized classification and no classification at all,<ref>Gupta, M., et al., \'\'An Overview of Social Tagging and Applications, in Social Network Data Analytics\'\', C.C. Aggarwal, Editor. 2011, Springer. p. 447-497.</ref> and have several advantages:<ref>Quintarelli, E., \'\'Folksonomies: power to the people\'\'. 2005.</ref><ref>Mathes, A., \'\'Folksonomies - Cooperative Classification and Communication Through Shared Metadata\'\'. 2004.</ref><ref>Wal, T.V. \'\'Folksonomy\'\'. 2007</ref>\n* tagging is easy to understand and do, even without training and previous knowledge in classification or indexing\n* the vocabulary in a folksonomy directly reflects the user’s vocabulary\n* folksonomies are flexible, in the sense that the user can add or remove tags\n* tags consist of both popular content and long-tail content, enabling users to browse and discover new content even in narrow topics\n* tags reflect the user’s conceptual model without cultural, social, or political bias\n* enable the creation of communities, in the sense that users who apply the same tag have a common interest\n* folksonomies are multi-dimensional, in the sense that users can assign any number and combination of tags to express a concept\n\nThere are several disadvantages with the use of tags and folksonomies as well,<ref>Kipp, M. and D.G. Campbell, \'\'Patterns and Inconsistencies in Collaborative Tagging Systems: An Examination of Tagging Practices\'\'. Proceedings Annual General Meeting of the American Society for Information Science and Technology, 2006.</ref> and some of the advantages (see above) can lead to problems. For example, the simplicity in tagging can result in poorly applied tags.<ref>Hayman, S., \'\'Folksonomies and Tagging: New developments in social bookmarking\'\', in Proceedings of Ark Group Conference: Developing and Improving Classification Schemes, 2007, Sydney. 2007: Sydney.</ref> Further, while controlled vocabularies are exclusionary by nature,<ref>Kroski, E., The Hive Mind: \'\'Folksonomies and User-Based Tagging. 2005\'\'</ref> tags are often ambiguous and overly personalized.<ref>Guy, M. and E. Tonkin, \'\'Folksonomies: Tidying up Tags?\'\' D-Lib Magazine, 2006. 12(Number 1): p. 1-15.</ref> Users apply tags to documents in many different ways and tagging systems also often lack mechanisms for handling synonyms, acronyms and homonyms, and they also often lack mechanisms for handling spelling variations such as misspellings, singular/plural form, conjugated and compound words. Some tagging systems do not support tags consisting of multiple words, resulting in tags like “viewfrommywindow”. Sometimes users choose specialized tags or tags without meaning to others.\n\n==Elements and types==\nA folksonomy emerges when users tag content or information, such web pages, photos, videos, podcasts, tweets, scientific papers and others. Strohmaier et al.<ref>Strohmaier, M., C. Körner, and R. Kern, \'\'Understanding why users tag: A survey of tagging motivation literature and results from an empirical study\'\'. Web Semantics: Science, Services and Agents on the World Wide Web, 2012. 17: p. 1-11.</ref> elaborate the concept: the term “tagging” refers to a "voluntary activity of users who are annotating resources with term-so-called \'tags\' – freely chosen from an unbounded and uncontrolled vocabulary". Others explain tags as an unstructured textual label <ref>Ames, M.N.M., \'\'Why We Tag: Motivations for Annotation in Mobile and Online Media\'\', in SIGCHI conference on Human factors in computing systems. 2007, ACM Press: New York, NY, USA. p. 971-980.</ref> or keywords,<ref>Guy, M. and E. Tonkin, Folksonomies: \'\'Tidying up Tags?\'\' D-Lib Magazine, 2006. 12(Number 1): p. 1-15.</ref> and that they appear as a simple form of metadata.<ref>Brooks, C.H. and N. Montanez, \'\'Improved annotation of the blogosphere via autotagging and hierarchical clustering\'\', in WWW \'06: Proceedings of the 15th international conference on World Wide Web. 2006, ACM Press: New York, NY, USA. p. 625-632.</ref>\n\nFolksonomies consist of three basic entities: users, tags, and resource. Users create tags to mark resources such as: web pages, photos, videos, and podcasts. These tags are used to manage, categorize and summarize online content. This collaborative tagging system also uses these tags as a way to index information, facilitate searches and navigate resources. Folksonomy also includes a set of URLs that are used to identify resources that have been referred to by users of different websites. These systems also include category schemes that have the ability to organize tags at different levels of granularity.<ref name="Berlin, B. 1992">Berlin, B. (1992). Ethnobiological Classification. Princeton: Princeton University Press.</ref>\n\nVander Wal identifies two types of folksonomy: broad and narrow.<ref name="Vander Wal">{{cite web |title=Explaining and Showing Broad and Narrow Folksonomies |url=http://www.vanderwal.net/random/entrysel.php?blog=1635 | last = Vander Wal | first=Thomas |accessdate= 2013-03-05}}</ref>  A broad folksonomy arises when multiple users can apply the same tag to an item, providing information about which tags are the most popular. A narrow folksonomy occurs when users, typically fewer in number and often including the item\'s creator, tag an item with tags that can each be applied only once.  While both broad and narrow folksonomies enable the searchability of content by adding an associated word or phrase to an object, a broad folksonomy allows for sorting based on the popularity of each tag, as well as the tracking of emerging trends in tag usage and developing vocabularies.<ref name="Vander Wal"/>\n\nAn example of a broad folksonomy is [[Delicious (website)|del.icio.us]],  a website where users can tag any online resource they find relevant with their own personal tags. The photo-sharing website [[Flickr]] is an oft-cited example of a narrow folksonomy.\n\n==Folksonomy vs. taxonomy==\n\'Taxonomy\' refers to a hierarchical categorization in which relatively well-defined classes are nested under broader categories. A \'\'folksonomy\'\' establishes categories (each tag is a category) without stipulating or necessarily deriving a hierarchical structure of parent-child relations among different tags. (Work has been done on techniques for deriving at least loose hierarchies from clusters of tags.<ref>{{cite journal|last1=Laniado|first1=David|title=Using WordNet to turn a folksonomy into a hierarchy of concepts|journal=CEUR Workshop Proceedings|volume=314|issue=51|url=http://ceur-ws.org/Vol-314/51.pdf|accessdate=7 August 2015}}</ref>)\n\nSupporters of folksonomies claim that they are often preferable to taxonomies because folksonomies democratize the way information is organized, they are more useful to users because they reflect current ways of thinking about domains, and they express more information about domains.<ref>{{cite web|last1=Weinberger|first1=David|title=Folksonomy as Symbol|url=http://www.hyperorg.com/blogger/?p=6254|website=Joho the Blog|accessdate=7 August 2015}}</ref> Critics claim that folksonomies are messy and thus harder to use, and can reflect transient trends that may misrepresent what is known about a field.\n\nAn empirical analysis of the complex dynamics of tagging systems, published in 2007,<ref name="WWW07-ref" >Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proc. International Conference on World Wide Web, ACM Press, 2007.</ref> has shown that consensus around stable distributions and shared vocabularies does emerge, even in the absence of a central [[controlled vocabulary]]. For content to be searchable, it should be categorized and grouped. While this was believed to require commonly agreed on sets of content describing tags (much like keywords of a journal article), some research has found that in large folksonomies common structures also emerge on the level of categorizations.<ref name="TWEB-ref" >V. Robu, H. Halpin, H. Shepherd [http://portal.acm.org/citation.cfm?id=1594173.1594176 Emergence of consensus and shared vocabularies in collaborative tagging systems], ACM Transactions on the Web (TWEB), Vol. 3(4), art. 14, 2009.</ref>\nAccordingly, it is possible to devise mathematical [[models of collaborative tagging]] that allow for translating from personal tag vocabularies (personomies) to the vocabulary shared by most users.<ref>Robert Wetzker, Carsten Zimmermann, Christian Bauckhage, and Sahin Albayrak [http://portal.acm.org/citation.cfm?id=1718487.1718497 I tag, you tag: translating tags for advanced user models], Proc. International Conference on Web Search and Data Mining, ACM Press, 2010.</ref>\n\nFolksonomy is unrelated to [[folk taxonomy]], a cultural practice that has been widely documented in anthropological and [[folkloristics|folkloristic]] work. Folk taxonomies are culturally supplied, intergenerationally transmitted, and relatively stable classification systems that people in a given culture use to make sense of the entire world around them (not just the [[Internet]]).<ref name="Berlin, B. 1992"/>\n\nThe study of the structuring or classification of folksonomy is termed \'\'folksontology\'\'.<ref>{{cite web | url=http://www.heppnetz.de/files/vandammeheppsiorpaes-folksontology-semnet2007-crc.pdf | title=FolksOntology: An Integrated Approach for Turning Folksonomies into Ontologies | accessdate=April 20, 2012 | author=Van Damme, Céline|display-authors=etal}}</ref> This branch of [[ontology (information science)|ontology]] deals with the intersection between highly structured taxonomies or hierarchies and loosely structured folksonomy, asking what best features can be taken by both for a system of classification. The strength of flat-tagging schemes is their ability to relate one item to others like it. Folksonomy allows large disparate groups of users to collaboratively label massive, dynamic information systems. The strength of taxonomies are their browsability: users can easily start from more generalized knowledge and target their queries towards more specific and detailed knowledge.<ref>Trattner, C., Körner, C., Helic, D.: [http://www.christophtrattner.info/pubs/iknow2011.pdf Enhancing the Navigability of Social Tagging Systems with Tag Taxonomies]. In Proceedings of the 11th International Conference on Knowledge Management and Knowledge Technologies, ACM, New York, NY, USA, 2011</ref> Folksonomy looks to categorize tags and thus create browsable spaces of information that are easy to maintain and expand.\n\n== Social tagging for knowledge acquisition ==\nSocial tagging for knowledge acquisition is the specific use of tagging for finding and re-finding specific content for an individual or group. Social tagging systems differ from traditional taxonomies in that they are community-based systems lacking the traditional hierarchy of taxonomies. Rather than a top-down approach, social tagging relies on users to create the folksonomy from the bottom up.<ref name=":0">Held, C., & Cress, U. (2009). Learning by Foraging: The impact of social tags on knowledge acquisition. In Learning in the synergy of multiple disciplines (pp. 254-266). Springer Berlin Heidelberg.</ref>\n\nCommon uses of social tagging for knowledge acquisition include personal development for individual use and collaborative projects. Social tagging is used for knowledge acquisition in secondary, post-secondary, and graduate education as well as personal and business research. The benefits of finding/re-finding source information are applicable to a wide spectrum of users. Tagged resources are located through search queries rather than searching through a more traditional file folder system.<ref>Fu, W. (2008). The microstructures of social tagging: a rational model. In: Proceedings of the ACM 2008 Conference on Computer Supported Cooperative Work, pp. 229–238. ACM, New York.</ref> The social aspect of tagging also allows users to take advantage of metadata from thousands of other users.<ref name=":0" />\n\nUsers choose individual tags for stored resources. These tags reflect personal associations, categories, and concepts. All of which are individual representations based on meaning and relevance to that individual. The tags, or keywords, are designated by users. Consequently, tags represent a user’s associations corresponding to the resource. Commonly tagged resources include videos, photos, articles, websites, and email.<ref name=":1">Kimmerle, J., Cress, U., & Held, C. (2010). The interplay between individual and collective knowledge: technologies for organisational learning and knowledge building. Knowledge Management Research & Practice, 8(1), 33-44.</ref> Tags are beneficial for a couple of reasons. First, they help to structure and organize large amounts of digital resources in a manner that makes them easily accessible when users attempt to locate the resource at a later time. The second aspect is social in nature, that is to say that users may search for new resources and content based on the tags of other users. Even the act of browsing through common tags may lead to further resources for knowledge acquisition.<ref name=":0" />\n\nTags that occur more frequently with specific resources are said to be more strongly connected. Furthermore, tags may be connected to each other. This may be seen in the frequency in which they co-occur. The more often they co-occur, the stronger the connection. Tag clouds are often utilized to visualize connectivity between resources and tags. Font size increases as the strength of association increases.<ref name=":1" />\n\nTags show interconnections of concepts that were formerly unknown to a user. Therefore, a user’s current cognitive constructs may be modified or augmented by the metadata information found in aggregated social tags. This process promotes knowledge acquisition through cognitive irritation and equilibration. This theoretical framework is known as the co-evolution model of individual and collective knowledge.<ref name=":1" />\n\nThe co-evolution model focuses on cognitive conflict in which a learner’s prior knowledge and the information received from the environment are dissimilar to some degree.<ref name=":0" /><ref name=":1" /> When this incongruence occurs, the learner must work through a process cognitive equilibration in order to make personal cognitive constructs and outside information congruent. According to the coevolution model, this may require the learner to modify existing constructs or simply add to them.<ref name=":0" /> The additional cognitive effort promotes information processing which in turn allows individual learning to occur.<ref name=":1" />\n\nA Canadian university study of instructors\' use of folksonomy tools in a learning objects repository identified critical success factors, and affirmed the applicability of Zipf\'s [[Principle of least effort|Principle of Least Effort]], concluding that a major benefit of "the folksonomical approach to knowledge management... is the fact that it is developed and maintained by the users of that body of knowledge," fostering "the dual outcome of creating a more viable knowledge management tool while strengthening the bonds of the user community."<ref>{{Cite book|url=http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60566-368-5.ch045|title=Critical Success Factors in the Development of Folksonomy-Based Knowledge Management Tools|last=Owen|first=Kenneth|last2=Willis|first2=Robert|date=2010|publisher=IGI Global|year=|isbn=|location=|pages=509–518|language=English|doi=10.4018/978-1-60566-368-5.ch045|hdl=http://hdl.handle.net/10613/3176|quote=|via=VIUSpace|hdl-access=free}}</ref>\n\n==Examples of folksonomies==\n\n* [[Twitter]] [[hashtag]]s\n* Many libraries\' online catalogs<ref>Steele, T. (2009).  The new cooperative cataloging.  Library Hi Tech, 27 (1), 68-77</ref><ref>Corey A. Harper and [[Barbara B. Tillett]], [https://scholarsbank.uoregon.edu/dspace/bitstream/1794/3269/1/ccq_s Library of Congress controlled vocabularies and their application to the Semantic Web]</ref>\n* [[Delicious (website)|del.icio.us]]: public tagging service\n* [[Flickr]]: shared photos\n* [[Steam (software)|Steam]] video game store\n* [[Mendeley]]: social reference management software\n* [[StumbleUpon]]: content discovery engine\n* [[Diigo]]: [[social bookmarking]] website\n*  The [[World Wide Web Consortium]]\'s [[Annotea]] project with user-generated tags in 2002.\n* [[Instagram]]: online photo-sharing and social networking service\n* [[WordPress]]: blogging tool and Content Management System\n* [[Pinterest]]: photosharing and publishing website\n\n==See also==\n{{div col|colwidth=30em}}\n* [[Collective intelligence]]\n* [[Enterprise bookmarking]]\n* [[Faceted classification]]\n* [[Semantic similarity]]\n* [[Thesaurus]]\n* [[Weak ontology]]\n* [[Wiki]]\n{{div col end}}\n\n==References==\n{{Reflist|30em|refs = Bateman, S., Brooks, C., McCalla, G., & Brusilovsky, P. (2007, May). Applying collaborative tagging to e-learning. In Proceedings of the 16th International World Wide Web Conference (WWW2007).\n\nCivan, A., Jones, W., Klasnja, P., & Bruce, H. (2008). Better to organize personal information by folders or by tags?: The devil is in the details.Proceedings of the American Society for Information Science and Technology,45(1), 1-13.\n \nFu, W. (2008). The microstructures of social tagging: a rational model. In: Proceedings of the ACM 2008 Conference on Computer Supported Cooperative Work, pp. 229–238. ACM, New York.\n\nGuy, M. and E. Tonkin, Folksonomies: Tidying up Tags? D-Lib Magazine, 2006. 12(Number 1): p. 1-15.\n\nGupta, M., et al., An Overview of Social Tagging and Applications, in Social Network Data Analytics, C.C. Aggarwal, Editor. 2011, Springer. p. 447-497\n \nHeld, C., & Cress, U. (2009). Learning by Foraging: The impact of social tags on knowledge acquisition. In Learning in the synergy of multiple disciplines (pp. 254-266). Springer Berlin Heidelberg.\n\nHayman, S., Folksonomies and Tagging: New developments in social bookmarking, in Proceedings of Ark Group Conference: Developing and Improving Classification Schemes, 2007, Sydney. 2007: Sydney.\n \nKimmerle, J., Cress, U., & Held, C. (2010). The interplay between individual and collective knowledge: technologies for organisational learning and knowledge building. Knowledge Management Research & Practice, 8(1), 33-44.\n\nKipp, M. and D.G. Campbell, Patterns and Inconsistencies in Collaborative Tagging Systems: An Examination of Tagging Practices. Proceedings Annual General Meeting of the American Society for Information Science and Technology, 2006.\n\nKroski, E., The Hive Mind: Folksonomies and User-Based Tagging. 2005.\n \nLavoué, É. (2011). Social tagging to enhance collaborative learning. In Advances in Web-Based Learning-ICWL 2011 (pp. 92-101). Springer Berlin Heidelberg.\n\nMathes, A., Folksonomies - Cooperative Classification and Communication Through Shared Metadata. 2004.\n\nQuintarelli, E., Folksonomies: power to the people. 2005.\n\nVander Wal, Thomas. "Folksonomy Coinage and Definition". Retrieved October 25, 2015 from http://www.vanderwal.net/folksonomy.html\n\nWeinberger, D. (2007). Everything is miscellaneous: The power of the new digital disorder. Times Books, New York.\n \n}}\n\n==Additional references==\n* [http://www.nytimes.com/2005/12/11/magazine/11ideas1-21.html "Folksonomy"], [[The New York Times]], 2005-12-11\n* [http://www.wired.com/science/discoveries/news/2005/02/66456 "Folksonomies Tap People Power"], [[Wired News]], 2005-02-01\n* {{cite journal | journal = [[Information Services & Use]] | title = Folksonomy and science communication | issue = 27 | year = 2007 | pages = 97–103 | url = http://wwwalt.phil-fak.uni-duesseldorf.de/infowiss/admin/public_dateien/files/1/1194272247inf_servic.pdf }}{{spaced ndash}} Folksonomies as a tool for professional scientific databases\n* [http://www.hyperorg.com/blogger/misc/taxonomies_and_tags.html "The Three Orders"]: 2005 explanation of tagging and folksonomies\n\n==External links==\n* [http://www.socialtagging.org/ SocialTagging.org] provides short definitions of key terms related to tagging and folksonomies\n* [http://www.vanderwal.net/folksonomy.html Vanderwal\'s definition of folksonomy]\n* [http://www.vanderwal.net/random/entrysel.php?blog=1750 Vanderwal\'s take on Wikipedia\'s definition of folksonomy]\n*[http://er.educause.edu/articles/2011/9/classroom-collaboration-using-social-bookmarking-service-diigo Classroom Collaboration Using Social Bookmarking Service Diigo]\n\n{{Web syndication}}\n{{Semantic Web}}\n\n[[Category:Collective intelligence]]\n[[Category:Folksonomy| ]]\n[[Category:Knowledge representation]]\n[[Category:Metadata]]\n[[Category:Semantic Web]]\n[[Category:Social bookmarking]]\n[[Category:Taxonomy]]\n[[Category:Web 2.0 neologisms]]\n[[Category:Sociology of knowledge]]']
['Personal knowledge base', '33562977', '{{about|knowledge management software|the general concept|Personal knowledge management}}\n{{Copypaste|url=http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-997-05.pdf|date=June 2016}}\nA \'\'\'personal knowledge base\'\'\' (\'\'\'PKB\'\'\') is an electronic tool used to express, capture, and later retrieve the personal knowledge of an individual. It differs from a traditional [[database]] in that it contains subjective material particular to the owner, that others may not agree with nor care about. Importantly, a PKB consists primarily of knowledge, rather than [[information]]; in other words, it is not a collection of documents or other sources an individual has encountered, but rather an expression of the distilled knowledge the owner has extracted from those sources.<ref name = "Davies 2005"/><ref name = "Davies 2011"/>\n\n== Definition ==\n\nThe term \'\'personal knowledge base\'\' was mentioned as early as the 1980s,<ref name = "Brooks 1985"/><ref name = "Kruger 1986"/><ref name = "Forman 1988"/><ref name = "Smith 1991"/> but the term came to prominence when it was described at length in publications by computer scientist Stephen Davies and colleagues,<ref name = "Davies 2005"/><ref name = "Davies 2011"/> who defined the term as follows:{{efn|An earlier version of this article incorrectly stated that the term \'\'personal knowledge base\'\' was coined in 2011; in fact, Stephen Davies and colleagues wrote a paper on the subject in 2005,<ref name = "Davies 2005"/> and publications by other authors had mentioned the term as early as the 1980s.<ref name = "Brooks 1985"/><ref name = "Kruger 1986"/><ref name = "Forman 1988"/><ref name = "Smith 1991"/> Much of the present article closely follows the publications by Davies and colleagues.}}\n\n* \'\'\'personal\'\'\': a PKB is intended for private use, and its contents are custom-tailored to the individual. It contains trends, relationships, categories, and personal observations that its owner perceives but which no one else may agree with. It can be shared, just as one can explain one\'s own opinion to a hearer, but it is not jointly \'\'owned\'\' by anyone else any more than explaining one\'s opinion to a friend causes the friend to own one\'s mind.\n* \'\'\'knowledge\'\'\': a PKB contains knowledge, not merely information. Its purpose is not simply to aggregate all the information sources one has seen, but to preserve the knowledge that one has \'\'learned\'\' from those sources. When a user returns to a PKB to retrieve knowledge she has stored, she is not merely pointed back to the original documents, where she must relocate, reread, reparse, and relearn the relevant passages. Instead, she is returned to the distilled version of the particular truth she is seeking, so that the mental model she originally had in mind can be easily reformed.\n* \'\'\'base\'\'\': a PKB is a consolidated, integrated knowledge store. It is a reflection of its owner\'s memory, which, as Bush and many others have observed, can freely associate any two thoughts together, without restriction. Hence a PKB does not attempt to partition a user\'s field of knowledge into multiple segments that cannot reference one another. Rather, it can connect any two concepts without regard for artificial boundaries, and acts as a single, unified whole.\n\n=== Contrast with other classes of systems ===\n\nThe following classes of systems \'\'cannot\'\' be classified as PKBs:<ref name = "Davies 2005"/><ref name = "Davies 2011"/>\n\n* collaborative efforts to build a universal objective space (as opposed to an individual\'s personal knowledge.) The World Wide Web itself is in this category, as were its predecessors HyperTIES<ref name = "Schneiderman 1987"/> and Xanadu,<ref name = "Nelson 1987"/> Web categorization systems like the [[Open Directory Project]], and collaborative information collections like [[Wikipedia]].\n* search systems like Enfish and the Stuff I\'ve Seen project<ref name="Dumais et al 2003" /> that index and search one\'s information sources on demand, but do not give the user the ability to craft and express personal knowledge.\n* tools whose goal is to produce a design artifact rather than to maintain knowledge for its own sake. Systems like ART<ref name="Nakakoji et al 2000" /> and Writing Environment<ref name="Smith et al 1987" /> use intermediate knowledge representations as a means to an end, abandoning them once a final artifact has been produced, and hence are not suitable as PKBs.\n* systems that focus on capturing transient information, rather than archiving knowledge that has long-term value. Examples would be Web logs<ref name="Godwin-Jones 2003"/> and e-diaries.<ref name="Kovalainen et al 1998" />  Tools whose information domain is mostly limited to [[time management]] tasks (calendars, action items, contacts, etc.) rather than "general knowledge". Blandford and [[Thomas R.G. Green|Green]]<ref name="Blandford and Green 2001" /> and Palen<ref name="Palen 1999" /> give excellent surveys; common commercial examples would be [[Microsoft Outlook]], [[IBM Lotus Notes|Lotus Notes]], and [[Evolution (software)|Novell Evolution]].\n* similarly, tools developed for a specific domain, such as bibliographic research rather than for "general knowledge".\n\n==== Personal information management ====\n\nPKM is similar to [[personal information management]], but is a distinct topic based on the "information" vs. "knowledge" difference. PKBs are about recording and managing the knowledge one derives from documents, whereas PIM is more about managing and retrieving the documents themselves.<ref name = "Davies 2005"/><ref name = "Davies 2011"/>\n\n== Historical influences ==\n\nNon-electronic personal knowledge bases have probably existed in some form since the dawn of written language: [[Leonardo da vinci#Journals and notes|Da Vinci\'s notebooks]] are a famous example. More commonly, card files and personal annotated libraries have served this function in the pre-electronic age.\n\n=== Bush\'s Memex ===\n\nUndoubtedly the most famous early formulation of an electronic PKB was [[Vannevar Bush]]\'s description of the "[[Memex]]" in 1945.<ref name="Bush 1945" /> Bush surveyed the post-World-War-II landscape and laid out what he viewed as the most important forthcoming challenges to humankind in \'\'[[The Atlantic Monthly]]\'\'. The Memex was a theoretical (never implemented) design or a system to help tackle the [[information overload]] problem, already formidable in 1945. In Bush\'s own words:\n\n<blockquote>Consider a future device for individual use, which is a sort of mechanized private file and library. ... [A] device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.  </blockquote>\n\nBush envisioned collaborative aspects as well, and even a world-wide system that scientists could freely consult.{{Citation needed|date=November 2012}} But an important emphasis throughout the article was on expanding our own powers of recollection: "Man needs to mechanize his record more fully," he says, if he is not to "become bogged down...by overtaxing his limited memory". With the Memex, the user could "add marginal notes and comments," and "build a trail of his interest" through the larger information space. She could share trails with friends, identify related works, and create personal annotations. Bush\'s Memex would give each individual the ability to create, categorize, classify, and relate his own set of information corresponding to his unique personal viewpoint.  Much of that information would in fact consist of bits and pieces from public documents, just as the majority of the knowledge inside our own heads has been imbibed from what we read and hear. But the Memex also allowed for the specialized recording of information that each individual perceived and needed to retain. The idea of supplementing our memory" was not a one-size-fits-all proposition, since no two people have the same interests, opinions, or memories.  Instead, it demanded a subjective expression of knowledge, unique to each individual.\n\n=== Graphical knowledge capture tools ===\n\nGreat emphasis is placed on the pictorial nature of diagrams to represent abstract knowledge; the use of spatial layout, color, and images is said to strengthen understanding and promote creativity. Each of the three primary schools—[[mind map]]ping, [[concept map]]ping, and [[cognitive map]]ping—prescribes its own data model and procedures, and each boasts a number of software applications designed specifically to create compatible diagrams.\n\n==== Mind mapping ====\n[[Mind map]]ping was promoted by pop psychologist [[Tony Buzan]] in the 1960s, and commands the allegiance of an impressive number of adherents worldwide.  A mind map is essentially nothing more than a visual outline, in which a main idea or topic is written in the center of the diagram, and subtopics radiate outwards in increasing levels of specificity. The primary value is in the freeform, spatial layout (rather than a sequential, numbered outline), the ability for a software application to hide or reveal select levels of detail, and as mentioned above, graphical adornments. The basic data model is a [[Tree (graph theory)|tree]], rather than a [[Graph (discrete mathematics)|graph]], with all edges implicitly labeled "supertopic/subtopic". Numerous tools are available for constructing mind maps.\n\n==== Concept mapping ====\n\n[[Concept map]]s were developed by Cornell Professor [[Joseph D. Novak|Joseph Novak]],<ref name="Novak 2003" /> and based on [[David Ausubel]]\'s assimilation theory of learning.<ref name="Ausubel 1968" /> An essential tenet is that newly encountered knowledge must be related to one\'s prior knowledge in order to be properly understood. Concept maps help depict such connections graphically. Like mind maps, they feature evocative words or phrases in boxes connected by lines. There are two principal differences, however: first, a concept map is properly a graph, not a tree, permitting arbitrary links between nodes rather than only parent/child relationships; and second, the links are labeled to identify the nature of the inter-concept relationship, typically with a verb phrase. In this way, the links on a diagram can be read as English sentences, with the upstream node as the subject and the downstream node as the direct object of the sentence.\n\nThere are many applications available that could be used for drawing these diagrams, not all of which directly acknowledge their support for concept maps in particular.<ref name="Canas et al 2005"/><ref name="Gaines and Shaw 1995" />\n\nA concept map is virtually identical to the notion of a "[[semantic network]]",<ref name="Woods 1985" /> which has served as a cornerstone for much artificial intelligence work since its inception. Semantic networks, too, are directed graphs in which the nodes represent concepts and labeled edges the relationships between them. Much psychology research has strengthened the idea that the human mind internalizes knowledge in something very like this sort of framework. This likely explains the ease with which concept mapping techniques have been adopted by the uninitiated, since concept maps and semantic networks can be considered equivalent.\n\n==== Cognitive mapping ====\n\n[[Cognitive mapping]], developed by Fran Ackermann and Colin Eden at the University of Strathclyde, uses the same data model as does concept mapping, but with a new set of techniques. In cognitive maps, element names have two parts, separated by an ellipsis that is read "as opposed to" in order to further clarify the semantics of the node. ("Cold...hot" is different from "cold...freezing," for example.) Links are of three types—causal, temporal, connotative—the first of which is the most common and is read as "may lead to". Generally cognitive mapping is best suited to domains involving arguments and [[decision making]]. Cognitive mapping is not nearly as widespread as the other two paradigms. Together, these and related methods have brought into the mainstream the idea of breaking down knowledge into its fundamental elements, and representing them graphically. Students and workers from widely diverse backgrounds have experienced success in better articulating and examining their own knowledge, and in discovering how it relates to what else they know. Although architectural considerations prevent any of these tools from functioning as bona fide PKBs, the ideas they have contributed to a front-end interface mechanism cannot be overestimated.\n\n=== Hypertext systems ===\n\nMany in the hypertext community [[Hypertext#History|reference]] Vannevar Bush\'s article as the cornerstone of their heritage. Hence the development of hypertext techniques, while seldom applied specifically towards PKB solutions, is important.  There have basically been three types of hypertext systems: those that exploit features of non-linear text to create a dynamic, but coherent "hyperdocument";<ref name="Schneiderman 1987" /><ref name="Goodman 1988" /> those\nthat prescribe ways of linking existing documents together for navigation and expression of affinities;<ref name="Davis et al 1993" /><ref name="Garrett et al 1986" /><ref name="Pearl 1989" /> and those that use the hypertext model specifically to model abstract knowledge. Though the first and especially the second category have dominated research efforts (and public enthusiasm) over the past several decades, it is this third class that is closest in spirit to the original vision of hypertext by its founders.\n\nIn a similar vein to [[Vannevar Bush|Bush]], [[Doug Engelbart]]\'s focus was to develop computer systems to "help people think better".  He sought data models that more closely paralleled the human thought process, and settled on using hypertext as a way to represent and store abstract human knowledge. Although his "[[NLS (computer system)|Augment]]" system underwent many changes, the original purpose closely aligned with that of PKBs.<ref name="Engelbart 1953"/>\n\nMore recently, Randall Trigg\'s TextNet<ref name="Trigg and Weiser 1986" /> and [[NoteCards]]<ref name="Halasz et al 1987" /> systems further explored this idea. TextNet revolved around "primitive pieces of text connected with typed links to form a network similar in many ways to a semantic network".<ref name="Conklin and Begeman 1988"/> Though text-centric, it was clear that Trigg\'s goal was to model the associations between primitive ideas and hence to reflect the mind\'s understanding. "By using...structure, meaning can be extracted from the relationships between chunks (small pieces of text) rather than from the words making them up."<ref name="Trigg and Weiser 1986" /> The subsequent [[NoteCards]] effort was similarly designed to "formulate, structure, compare, and manage ideas". It was useful for "analyzing information, constructing models, formulating arguments, designing artifacts, and generally processing ideas".\n\nConklin and Begeman\'s [[gIBIS]] system was another early effort into true knowledge representation, specifically for the field of design deliberations and arguments.<ref name="Conklin and Begeman 1988"/> The project lived on in the later project QuestMap<ref name="Selvin 1999" /> and the more modern [[Compendium (software)|Compendium]], which has been primarily used for capturing group knowledge expressed in face-to-face meetings. In all these cases, systems use semantic hypertext in an attempt to capture shared knowledge in its most basic form. Other examples of knowledge-based hypertext tools include Mental Link,<ref name="Dede and Jayaram 1990" /> Aquanet,<ref name="Marshall et al 1991" /> and SPRINT,<ref name="Carlson and Ram 1990" /> as well\nas a few current commercial tools such as [[PersonalBrain]] and [[Tinderbox (application software)|Tinderbox]]<ref name="Bernstein 2003" /> and open source tools such as [[TiddlyWiki]].\n\n=== Note-taking applications ===\n\n[[Electronic Notetaking|Note-taking applications]] allow a user to create snippets of text and then organize or categorize them in some way. These tools can be used to form PKBs that are composed of such text snippets.\n\nMost of these tools are based on a [[Tree (graph theory)|tree]] hierarchy, in which the user can write pages of notes and then organize them into sections and subsections. The higher level sections or chapters often receive a colored tab exactly as a physical three-ring notebook might. Other designers eschew the tree model for a more flexible category-based approach (see section [[#Data models|data models]]). The primary purpose of all these tools is to offer the benefits of freeform note-taking with none of the deficiencies: users are free to brainstorm and jot down anything from bullet points to polished text, while still being able to search, rearrange, and restructure the entire notebook easily.\n\nAn important subcategory of note-taking tools is outliners (e.g., [[OmniOutliner]]), or applications specifically designed to organize ideas in a hierarchy. These tools typically show a two-pane display with a tree-like navigation widget in the left-pane and a list of items in the right-pane. Topics and subtopics can be rearranged, and each outline stored in its own file. Modern outliners feature the ability to add graphics and other formatting to an item, and even hyper links to external websites or documents. The once abandoned (but now resurrected) Ecco system was among the first to allow items to have typed attributes, displayed in columns. This gives the effect of a custom spreadsheet per topic, with the topic\'s items as rows and the columns as attributes. It allows the user to gracefully introduce structure to their information as it is identified.\n\nOf particular interest are applications optimized for subsuming portions of an information space realm into a PKB, where they can be clustered and arranged according to the user\'s own perceptions. The Virtual Notebook System (VNS)<ref name="Burger et al 1991" /> was one of the first to emphasize this. VNS was designed for sharing information among scientists at the Baylor College of Medicine; a user\'s "personal notebook" could make references to specific sections of a "community notebook," and even include arbitrary segments of other documents through a cut-and-paste mechanism.\n\n=== Document management systems ===\n{{main|Document management system}}\n\nAnother influence on PKBs are systems whose primary purpose is to help users organize documents, rather than personal knowledge derived from those documents. Such systems do not encode subjective knowledge per se, but they do create a personal knowledge base of sorts by allowing users to organize and cross-reference their information artifacts.\n\nThese efforts provide alternative indexing mechanisms to the limited "directory path and file name" approach. Presto<ref name="Dourish et al 1999" /> replaces the directory hierarchy entirely with attributes that users assign to files. These key-value pairs represent user-perceived properties of the documents, and are used as a flexible means for retrieval and organization. William Jones\' Memory Extender<ref name="Jones 1986" /> was similar in spirit, but it dynamically varied the "weight" of a file\'s keywords according to the user\'s context and perceived access patterns. In [[Haystack (MIT project)|Haystack]],<ref name="Adar et al 1999" /> users—in conjunction with automated software agents—build a graph-based network of associative links through which documents can be retrieved.\n\nMetadata and multiple\ncategorization can also be applied to provide multiple retrieval paths customized to the way the individual thinks and works with their information sources. WebTop<ref name="Wolber et al 2002" /> allowed the user to create explicit links between documents, but then also merged these user-defined relationships with other types of associations. These included the hyperlinks contained in the documents, associations implied by structural relationships, and content similarities discovered by text analysis. The idea was that any way in which items can be considered "related" should be made available to the user for help with retrieval.\n\nA subclass of these systems integrate the user\'s personal workspace with a search facility, blurring the distinction between information retrieval and information organization.  SketchTrieve,<ref name="Hendry and Harper 1997" /> DLITE,<ref name="Cousins et al 1997" /> and Garnet<ref name="Buchanan et al 2004" /> each materialized elements from the retrieval domain (repositories, queries, search results) into tangible, manipulatable screen objects. These could be introduced directly into a spatial layout that also included the information sources themselves. These systems can be seen as combining a spatial hypertext interface as in VIKI<ref name="Marshall and Shipman 1995" /> with direct access to digital library search facilities.  NaviQue<ref name="Furnas and Rauch 1998" /> was largely in the same vein, though it incorporated a powerful similarity engine to proactively aid the user in organization. CYCLADES<ref name="Renda and Straccia 2005" /> let users organize Web pages into folders, and then attempted to infer what each folder "means" to that user, based on a statistical textual analysis of its contents. This helps users locate other items similar to what\'s already in a folder, learn what other users have found interesting and have grouped together, etc.\n\nAll of these document management systems are principally concerned with organizing objective information sources rather than the expression of subjective knowledge. Yet their methods are useful to consider with respect to PKB systems, because such a large part of our knowledge comprises things we remember, assimilate, and repurpose from objective sources. Search environments like SketchTrieve, as well as snippet gatherers like YellowPen, address an important need in [[knowledge management]]: bridging the divide between the subjective and objective realms, so that the former can make reference to and bring structure to the latter.\n\n== Claims and benefits ==\n\nPKB systems make various claims about the advantages of using them. These can be classified as follows:<ref name = "Davies 2005"/><ref name = "Davies 2011"/>\n\n* \'\'\'Knowledge generation and formulation.\'\'\' Here the emphasis is on procedure, not persistence; it is the act of simply using the tool to express one\'s knowledge that helps, rather than the ability to retrieve it later.\n* \'\'\'[[Knowledge capture]].\'\'\' PKBs do not merely allow one to express knowledge, but also to capture it before it elusively disappears. Often the emphasis is on a streamlined user interface, with few distractions and little encumbrance.  The point is to lower the burden of jotting down one\'s thoughts so that neither task nor thought process is interrupted.\n* \'\'\'Knowledge organization.\'\'\' A 2003 study on note-taking habits found that "better organization" was the most commonly desired improvement in people\'s own information recording practices.<ref name="Hayes et al 2003" />\n* \'\'\'Knowledge management and [[knowledge retrieval|retrieval]].\'\'\' Perhaps the most critical aspect of a PKB is that the knowledge it stores is permanent and accessible, ready to be retrieved at any later time.\n* \'\'\'[[Knowledge integration|Integrating]] heterogeneous sources.\'\'\' Recognizing that the knowledge people form comes from a variety of different places, many PKB systems emphasize that the information from diverse sources and of different types can be integrated into a single database and interface.\n\n== Data models ==\n{{see|Data model}}\nPKB systems can be compared along a number of different axes, the most important of which is the underlying data model they support. This is what prescribes and constrains the nature of the knowledge they can contain: what types of knowledge elements are allowed, how they can be structured, and how the user perceives them and can interact with them.<ref name = "Davies 2005"/><ref name = "Davies 2011"/>\n\nThree aspects of data models can be identified: the \'\'structural framework\'\', which prescribes rules about how knowledge elements can be structured and interrelated; the \'\'knowledge elements\'\' themselves, or basic building blocks of information that a user creates and works with; and \'\'schema\'\', which involves the level of formal semantics introduced into the data model.\n\n=== Structural frameworks ===\n\nThe following structural frameworks have been featured in one or more prominent PKB systems.\n\n==== Tree ====\n\nSystems that support a [[Tree (data structure)|tree]] model allow knowledge elements to be organized into a containment hierarchy, in which each element has one and only one "parent". This takes advantage of the mind\'s natural tendency to classify objects into groups, and to further break up each classification into subclassifications. It also mimics the way that a document can be broken up into chapters, sections, and subsections.  It tends to be natural for users to understand.\n\nAll of the applications for creating Buzan [[Mind Map|mind maps]] are based on a tree model, because a mind map \'\'is\'\' a tree. Each mind map has a "root" element in the center of the diagram (often called a "main topic") from which all other elements emanate as descendents.  Every knowledge element has one and only one place in this structure. Some tools, such as [[MindManager]], extend this paradigm by introducing "floating topics", which are not anchored to the hierarchy, and permitting "crosslinks" to arbitrary topics, similar to those in concept maps.\n\nOther examples of tree-based systems are most personalized search interfaces,<ref name="Renda and Straccia 2005" /><ref name="Di Giacomo et al 2001" /><ref name="Reyes-Farfan and Sanchez 2003"/> outliners, and most of the "notebook-based" note-taking systems. By allowing them to partition their notes into sections and subsections, note-taking tools channel users into a tree hierarchy. In recognition of this confining limitation, many of these tools also permit a kind of "crosslink" between items, or employ some form of transclusion (see below) to allow items to co-exist in several places. The dominant paradigm in such tools, however, remains the simple parent-child hierarchy.\n\n==== Graph ====\n\nGraph-based systems allow users to create knowledge elements and then to interconnect them in arbitrary ways. The elements of a [[Graph (discrete mathematics)|graph]] are traditionally called "vertices," and connected by "arcs," though the terminology used by graph-based systems varies widely (see Table 1) and the hypertext community normally uses the terms "nodes" and "links". There are no restrictions on how many arcs one vertex can have with others, no notion of a "parent/child" relationship between vertices (unless the user chooses to label an arc with those semantics), and normally no "root" vertex. In many systems, arcs can optionally be labeled with a word or phrase indicating the nature of the relationship, and adorned with arrowheads on one or both ends to indicate navigability. (Neither of these adornments is necessary with a tree, since all relationships are implicitly labeled "parent/child" and are navigable from parent to child.) A graph is a more general form of a tree, and hence a strictly more powerful form of expression.\n\n{| class="wikitable"\n|+Terminology employed by a sampling of graph-based knowledge tools.\n! System !! Vertex !! Arc !! Graph\n|-\n|[[Axon Idea Processor]]||object||link||diagram\n|-\n|Banxia Decision Explorer||concept||link||view\n|-\n|[[Compendium (software)|Compendium]]||node||link||view\n|-\n|[[Haystack (MIT project)|Haystack]]||needle||tie||bale\n|-\n|Idea Graph||idea||connection||ideagraph\n|-\n|Knowledge Manager||concept||relation||map\n|-\n|[[MyLifeBits]]||resource||link/annotation||story\n|-\n|[[NoteCards]]||note card||link||browser\n|-\n|[[PersonalBrain]]||thought||link||brain\n|-\n|RecallPlus||idea||association||diagram\n|-\n|SMART Ideas||symbol||connector||level\n|}\n\nThis model is the defining characteristic of hypertext systems<ref name="Halasz and Schwartz 1994" /> including many of those used for document management.<ref name="Wolber et al 2002" /><ref name="Adar et al  1999" /> It is also the underpinning of all concept-mapping tools, whether they actually acknowledge the name "concept maps"<ref name="Canas et al 2005" /><ref name="Gaines and Shaw 1995" /> or advertise themselves simply as tools to draw knowledge diagrams. As mentioned previously, graphs draw their power from the fact that humans are thought to model knowledge as graphs (or equivalently, semantic networks) internally. In fact, it could be argued that all human knowledge can be ultimately reduced to a graph of some kind, which argues strongly for its sufficiency as a structural framework.<ref name="Quillian 1968" /><ref name="Nosek and Roth 1990" />\n\nAn interesting aspect of graph-based systems is whether or not they require a \'\'[[Connectivity (graph theory)|fully connected]]\'\' graph.  A fully connected graph is one in which every vertex can be reached from any other by simply performing enough arc traversals. There are no "islands" of vertices that are severed from each other. Most graph-based tools allow non-fully-connected graphs: knowledge elements are added to the system, and connected arbitrarily to each other, without constraint.  But a few tools, such as [[PersonalBrain]] and [[Compendium (software)|Compendium]], actually require a single network of information in which every knowledge element must be indirectly connected to every other. If one attempts to remove the last link that connects a body of nodes to the original root, the severed elements are either "forgotten" or else moved to a deleted objects heap where they can only be accessed by restoring a connection to the rest of the graph.\n\nSome hypertext systems<ref name="Garrett et al 1986" /><ref name="Delisle and Schwartz 1986" /> add precision to the basic linking mechanism by allowing nodes to reference not only other nodes, but sections within nodes.<ref name="Halasz and Schwartz 1994" /> This ability is especially useful if the nodes themselves contain sizeable content, and also for PKB elements making reference to fragments of objective sources.\n\n==== Tree plus graph ====\n\nAlthough graphs are a strict superset of trees, trees offer some important advantages in their own right: simplicity, familiarity, ease of navigation, and the ability to conceal details at any level of abstraction. Indeed, the problem of "disorientation" in hypertext navigation<ref name="Conklin and Begeman 1988"/><ref name="Mantei 1982" /> largely disappears with the tree model; one is never confused about "where one is" in the larger structure, because traversing the parent hierarchy gives the context of the larger surroundings. For this reason, several graph-based systems have incorporated special support for trees as well, to combine the advantages of both approaches.  For instance, in concept mapping techniques, a generally hierarchical paradigm is prescribed, after which users are encouraged to identify "crosslinks" between distant concepts. Similarly, some systems using the mind mapping paradigm permit arbitrary relationships between nodes.\n\nOne of the earliest systems to combine tree and graph primitives was TEXTNET,<ref name="Trigg and Weiser 1986" /> which featured two types of nodes: "chunks" (which contained content to be browsed and organized) and "table of contents" nodes (or "tocs".) Any node could freely link to any other, permitting an unrestricted graph. But a group of tocs could be combined to form a tree-like hierarchy that bottomed out in various chunk nodes. In this way, any number of trees could be superimposed upon an arbitrary graph, allowing it to be viewed and browsed as a tree, with all the requisite advantages. Strictly speaking, a network of tocs formed a [[Directed acyclic graph|DAG]] rather than a tree. This means that a "chunk" could be represented in multiple places in the tree, if two different traversal paths ended up referring to the same chunk. A DAG is essentially the result of applying transclusion to the tree model. This is also true of NoteCards. NoteCards<ref name="Halasz et al 1987" /> offered a similar mechanism, using "FileBoxes" as the tree component that was overlaid upon the semantic network of notecards.\n\nBrown University\'s IGD project explored various ways to combine and display unrestricted graphs with hierarchy, and used a visual metaphor of spatial containment to convey both graph and tree structure.<ref name="Feiner 1988" /> Their notion of "link inheritance" simplifies the way in which complex dual structures are displayed while still faithfully depicting their overall trends. Commercially, both [[PersonalBrain]] and Multicentrix<ref name="Koy 1997" /> provide explicit support for parent/child relationships in addition to arbitrary connections between elements, allowing tree and graph notions to coexist.  Some note-taking tools, while essentially tree-based, also permit crosslinks between notes.\n\n==== Spatial ====\nSome designers have shunned links between elements altogether, favoring instead spatial positioning as the sole organizational paradigm. Capitalizing on the human\'s tendency to implicitly organize through clustering, making piles, and spatially arranging, some tools offer a 2D workspace for placing and grouping items. This provides a less formal (and perhaps less intimidating) way for a user to gradually introduce structure into a set of items as it is discovered.\n\nThis approach originated from the spatial hypertext community, demonstrated in various projects,<ref name="diSessa and Abelson 1986" /> and VIKI/VKB<ref name="Marshall and Shipman 1995" /><ref name="Shipman et al 2000" /> With these programs, users place information items on a canvas and can manipulate them to convey organization imprecisely. Some project<ref name="Marshall and Shipman 1995" /> could infer the structure from a user\'s freeform layout: a spatial parser examines which items have been clustered together, colored or otherwise adorned similarly, etc., and makes judgments about how to turn these observations into machine-processible assertions. While others (Pad<ref name="Perlin and Fox 1993" />) allowed users to view different objects in varying levels of detail as they panned around the workspace.\n\nCertain note-taking tools<ref name="Burger et al 1991" /><ref name="Akscyn et al 1987" /> combine an overarching tree structure with spatial freedom on each "frame" or "page". Users can access a particular page of the notebook with basic search or tree navigation facilities, and then lay out notes and images on the page as desired. Many graph-based approaches (such as concept mapping tools) also allow for arbitrary spatial positioning of elements. This allows both kinds of relationships to be expressed: explicit links and less formal expression through creative use of the screen.\n\n==== Categories ====\n\nIn category-based structural frameworks, rather than being described in terms of their relationships to other elements (as with a tree or graph), items are simply grouped together in one or more categories, indicating that they have something in common. This scheme is based on the branch of pure mathematics called [[set theory]], in which each of a body of objects either has, or does not have, membership in each of some number of sets. There is normally no restriction as to how many different categories a given item can belong to, as is the case with mathematical sets.\n\nUsers may think of categories as collections, in which the category somehow encloses or "owns" the items within it. Indeed, some systems depict categories in this fashion, such as the Vista interface<ref name="Dourish et al 1999" /> where icons standing for documents are enclosed within ovals that represent categories. This is merely a convention of display, however, and fundamentally, categories are the same as simple keywords.\n\nThe most popular application to embrace the category approach was the original [[Lotus Agenda|Agenda]].<ref name="Kaplan et al 1990" /> All information retrieval in Agenda was performed in terms of category membership. Users specified queries that were lists of categories to include (or exclude), and only items that satisfied those criteria were displayed. Agenda was particularly sophisticated in that the categories themselves formed a tree hierarchy, rather than a flat namespace. Assigning an item to a category also implicitly assigned it to all ancestors in the hierarchy.\n\n[[Personal Knowbase]] is a more modern commercial product based solely on a keyword (category) paradigm, though it uses a simple flat keyword structure rather than an inheritance hierarchy like Agenda. [[Haystack (MIT project)|Haystack]]<ref name="Adar et al 1999" /> and [[Open Source Applications Foundation|Chandler]] are other information management tools which use categorization in important ways. William Jones\' Memory Extender<ref name="Jones 1986" /> took an artificial intelligence twist on the whole notion of keywords/categories, by allowing an item\'s keywords to be weighted, and adjusted over time by both the user and the system. This allowed the strength of category membership to vary dynamically for each of an item\'s assignments, in an attempt to yield more precise retrieval.\n\n==== Chronological ====\n\nYale University\'s Lifestreams project<ref name="Fertig et al 1996" /> used timestamps as the principal means of organization and retrieval of personal documents. In Fertig et al.\'s own words:\n\n<blockquote>A [[lifestreaming|lifestream]] is a time-ordered stream of documents that functions as a diary of your electronic life; every document you create is stored in your lifestream, as are the documents other people send you. The tail of your stream contains documents from the past, perhaps starting with your electronic birth certificate.  Moving away from the tail and toward the present, your stream contains more recent documents such as papers in progress or the latest electronic mail you\'ve received...</blockquote>\n\nDocuments are thus always ordered and accessed chronologically. Metadata-based queries on the collection produce "substreams," or chronologically ordered subsets of the original documents. The rationale for time-based ordering is that "time is a natural guide to experience; it is the attribute that comes closest to a universal skeleton-key for stored experience".<ref name="Freeman and Gelernter 1996" /> Whether chronology is our principal or even a common natural coding mechanism psychologically can be debated. But since any PKB system can easily create such an index, it seems worthwhile to follow Lifestreams\' lead and allow the user to sort and retrieve based on time, as many systems have done. If nothing else, it relieves the user from having to create names for knowledge elements, since the timestamp is always an implicit identifying mark. PlanPlus, based on the Franklin-Covey planner system, is also chronologically modeled, and a number of products based on other data models<ref name="Kaplan et al 1990" /> offer chronological indexing in addition to their core paradigm.\n\n==== Aquanet\'s framework ====\n\nThough advertised as a hypertext system, Marshall \'\'et al.\'\'\'s Aquanet<ref name="Marshall et al 1991" /> went far beyond the traditional node-link graph model.  Knowledge expressed in Aquanet is centered around "relations," or n-ary links between objects in which the semantics of each participant in the relation is specified by the relation type. Each type of relation specifies a physical display (i.e., how it will be drawn on the screen, and the spatial positioning of each of its participants), and a number of "slots" into which participants can be plugged. Each participant in a relation can be either a base object, or another relation. Users can thus define a schema of relation types, and then build a complex semantic model out of relations and objects. Since relation types can be specified to associate any number of nodes (instead of just two, as in the graph model), this potentially allows more complex relationships to be expressed.\n\nIt should be noted, however, that the same effect can be achieved in the basic graph model by simply taking the n-ary relations and "reifying" them (i.e., turning them into nodes in their own right.) For instance, suppose we define a relation type "assassination," with slot types of "assassin," "victim," "location," and "weapon". We could then create a relation based on this type where the participants are "John Wilkes Booth," "Abraham Lincoln," "Ford\'s Theatre," and "derringer". This allows us to express a complex relationship between multiple objects in Aquanet. But we can express the same knowledge with the basic graph model by simply creating a node called "Lincoln\'s assassination" and then creating typed links between that node and the other four labeled "assassin," "victim," etc. Aquanet\'s biggest achievement in this area is the ability to express the schema of relation types, so that the types of objects an "assassination" relation can connect are consistent and enforced.\n\n=== Knowledge elements ===\n\nThere are several options for specifying what knowledge elements consist of, and what kind of internal structure, if any, they possess:\n\n# \'\'\'Word/phrase/concept\'\'\'.  Most systems engineered for knowledge representation encourage structures to be composed of very simple elements, usually words or phrases.  This is in the spirit of both mind mapping and concept mapping, where users are encouraged to use simple phrases to stand for mental concepts.\n# \'\'\'Free text notes\'\'\'. Nearly all systems permit large amounts of free text to exist in the PKB, either as the contents of the elements themselves (NoteCards,<ref name="Halasz et al 1987" /> Hypercard,<ref name="Goodman 1988" /> TreePad) or attached to elements as separate, supplementary pages (Agenda,<ref name="Kaplan et al 1990" /> Zoot, HogBay).\n# \'\'\'Links to an information space\'\'\'.  Since a user\'s knowledge base is to correspond to her mental perceptions, it seems profitable for the PKB to point to entities in the information space from which she formed those perceptions.  Many systems do in fact allow their knowledge elements to point to the original sources in some way. There are three common techniques:\n##The knowledge element actually \'\'represents\'\' an original source.  This is the case for document management systems (WebTop,<ref name="Wolber et al 2002" /> MyLifeBits,<ref name="Gemmell et al 2002" /> Haystack<ref name="Adar et al 1999" />), integrated search facilities (NaviQue,<ref name="Furnas and Rauch 1998" /> CYCLADES<ref name="Renda and Straccia 2005" />), VIKI/VKB.<ref name="Marshall and Shipman 1995" /><ref name="Shipman et al 2000" />  Tinderbox<ref name="Bernstein 2003" /> will also allow one of its notes to be a URL, and the user can control whether its contents should be captured once, or "auto-fetched" as to receive constant web updates.  Many systems, in addition to storing a page of free text for each knowledge element, also permit any number of hyperlinks to be attached to a knowledge element (e.g., [[FreeMind|Freemind]], [[PersonalBrain]], Inspiration). VNS,<ref name="Burger et al  1991" /> which allows users to point to a community notebook page from within their personal notebook, gives similar functionality.\n## The knowledge element is a repurposed snippet from an original source. This is potentially the most powerful form, but is rare among fully featured PKB systems.  Cartagio, Hunter-Gatherer,<ref name="Schraefel et al 2002" /> and YellowPen all allow Web page excerpts to be assimilated and organized, although they primarily only do that, without allowing them to easily be combined with other subjective knowledge. DEVONThink and MyBase\'s WebCollect plug-in add similar functionality to their more general-purpose, tree-based information managers. Both of these systems, when a snippet is captured, archive the entire Web page locally so it can be returned to later. The user interfaces of CircusPonies and StickyBrain have been heavily optimized towards grabbing information from other applications and bringing them into the PKB without disturbing the user\'s workflow.\n# \'\'\'Composites\'\'\' Some programs allow a user to embed knowledge elements (and perhaps other information as well) inside a knowledge element to form an implicit hierarchy. Trees by themselves fall into this category, of course, since each node in the tree can be considered a "composite" of its content and children. But a few graph-based tools offer composite functionality as well. In Aquanet,<ref name="Marshall et al  1991" /> "relations" form the fundamental means of connection, and the units that are plugged into a relation can be not only objects, but other relations as well. This lends a recursive quality to a user\'s modeling. VIKI/VKB\'s spatial environment offers "subspaces" which let a user partition their visual workspace into subregions, whose internal contents can be viewed at a glance from the parent. Boxer\'s<ref name="diSessa and Abelson 1986" /> paradigm is similar. Tinderbox is a graph-based tool that supports hierarchical composite structures, and [[Compendium (software)|Compendium]] extends this even further by allowing transclusion of "views" as well as of nodes. Unlike the other tools, in Compendium the composite hierarchy does not form a [[Directed acyclic graph|DAG]], but rather an arbitrary graph: view A can appear on view B, and B can in turn appear on A. The user\'s intuitive notion of "inside" must be adapted somewhat in this case.\n\n=== Schema ===\n\nIn the context of PKBs, "schema" means the ability for a user to specify types and introduce structure to aspects of the data model. It is a form of metadata whereby more precise semantics can be applied to various elements of the system. This facilitates more formal knowledge expression, ensures consistency across items of the same kind, and can better allows automated agents to process the information.\n\nBoth knowledge elements, and links, can contain various aspects of schema.\n\n==== Schema for knowledge elements ====\n\n===== Types, and related schema =====\n\nIn a PKB, a "[[type system]]" allows users to specify that a knowledge element is a member of a specific class or category or items, to provide a built-in method of organization and retrieval. Generally speaking, systems can make knowledge elements untyped, rigidly typed, or flexibly typed. In addition, they can incorporate some notion of inheritance among elements and their types.  There is a distinction between types and categories here. A category-based scheme, typically allows any number of categories/keywords to be assigned to an item. There are two differences between this and the notion of type. First, items are normally restricted to being of a single type, and this usually indicates a more intrinsic, permanent property of an item than simply its presence in a category collection.  (For example, one could imagine an item called "XYZ Corporation" shifting into and out of categories like "competitors", "overseas distributors," or "delinquent debtors" over time, but its core type of "company" would probably be static for all time.) Second, types often carry structural specifications with them: if an item is of a given type, this means it will have values for certain attributes appropriate to that type.  Some systems that do not allow typing offer the ability to approximate this function through categories.\n\nUntyped elements are typical among informal knowledge capture tools, since they are designed to stimulate brainstorming and help users discover their nascent mental models.  These tools normally want to avoid forcing the user to commit to structure prematurely.  Most mind mapping and many concept mapping tools are in this category: a concept is simply a word or phrase, with no other semantic information (e.g., [[Visual Mind]]). Note-taking tools also usually take this approach, with all units of information being of the same type "note".\n\nAt the other extreme are tools which, like older relational database technology, require all items to be declared as of a specific type when they are created. Often this type dictates the internal structure of the element. These tools are better suited to domains in which the structure of knowledge to be captured is predictable, well-understood, and known in advance. For PKB systems, they are probably overly restrictive. KMap<ref name="Gaines and Shaw 1995" /> and Compendium are examples of tools that allow (and require) each item to be typed; in their case, the type controls the visual appearance of the item, rather than any internal structure.\n\nIn between these two poles are systems that permit typed and untyped elements to co-exist. NoteTaker is such a product; it holds simple free-text pages of notes, without any structure, but also lets the user define "templates" with predefined fields that can be used to instantiate uniformly structured forms. TreePad has a similar feature. Some other systems blur the distinction between typed and untyped, allowing the graceful introduction of structure as it is discovered. VKB,<ref name="Shipman et al 2000" /> for example, supports an elegant, flexible typing scheme, well suited to PKBs.  Items in general consist of an arbitrary number of [[attribute–value pair]]s. But when consistent patterns emerge across a set of objects, the user can create a type for that group, and with it a list of expected attributes and default values. This structure can be selectively overridden by individual objects, however, which means that even objects assigned to a particular type have flexible customization available to them. Tinderbox offers an alternate way of achieving this flexibility, as described below.\n\nFinally, the [[object-oriented]] notion of [[Inheritance (computer science)|type inheritance]] is available in a few solutions. The different card types in NoteCards are arranged into an inheritance hierarchy, so that new types can be created as extensions of old. Aquanet extends this to multiple inheritance among types; the "slots" that an object contains are those of its type, plus those of all supertypes. SPRINT and Tinderbox also use a frame-based approach, and allow default values for attributes to be inherited from supertypes. This way, an item need not define values for all its attributes explicitly: unless overridden, an item\'s slot will have the shared, default value for all items of that type.\n\n===== Other forms of schema =====\n\nIn addition to the structure that is controlled by an item\'s type, other forms of metadata and schema can be applied to knowledge elements.\n\n* \'\'\'Keywords\'\'\'. Many systems let users annotate items with user-defined keywords. Here the distinction between an item\'s contents and the overall knowledge structure becomes blurred, since an item keyword could be considered either a property of the item, or an organizational mechanism that groups it into a category with like items. Systems using the category data model (e.g., Agenda) can employ keywords for the latter purpose. Some systems based on other data models also use keywords to achieve category-like functionality.\n* \'\'\'Attribute/value pairs\'\'\'. Arbitrary attribute/value pairs can also be attached to elements in many systems, which gives a PKB the ability to define semantic structure that can be queried.  Frame-based systems like SPRINT and Aquanet are examples, as well as NoteTaker, VKB, and Tinderbox. MindPad[AKS-Labs 2005] is notable for taking the basic concept mapping paradigm and introducing schema to it via its "model editor". As mentioned earlier, adding user-defined attribute/value pairs to the items in an outliner yields spreadsheet-like functionality, as in Ecco and [[OmniOutliner]]. Some systems feature attribute/value pairs, but only in the form of system-defined attributes, not user-defined ones.\n* \'\'\'Knowledge element appearance\'\'\'. Some tools modify a knowledge element\'s visual appearance on the screen in order to convey meaning to the user. SMART Ideas and [[Visual Mind]] let the user freely choose each element\'s icon from a variety of graphics, while KMap<ref name="Gaines and Shaw 1995" /> ties the icon directly to its underlying type. Other graphical aspects that can be modified include color (VIKI<ref name="Marshall and Shipman 1995" />), the set of attributes shown in a particular context (VKB<ref name="Shipman et al 2000" />), and the spatial positioning of objects in a relation (Aquanet<ref name="Marshall et al 1991" />).\n\n==== Schema for links ====\n\nIn addition to prescribing schema for knowledge elements, many systems allow some form of information to be attached to the links that connect them.\n\nIn most of the early hypertext systems, links were unnamed and untyped, their function being merely to associate two items in an unspecified manner. The mind mapping paradigm also does not name links, but for a different reason: the implicit type of every link is one of generalization/specialization, associating a topic with a subtopic. Hence specifying types for the links would be redundant, and labeling them would clutter the diagram.\n\nConcept mapping prescribes the naming of links, such that the precise nature of the relationship between two concepts is made clear. As mentioned above, portions of a concept map are meant to be read as English sentences, with the name of the link serving as a verb phrase connecting the two concepts. Numerous systems thus allow a word or phrase to decorate the links connecting elements.\n\nNamed links can be distinguished from \'\'typed\'\' links, however. If the text attached to a link is an arbitrary string of characters, unrelated to that of any other link, it can be considered the link name. Some systems, however, encourage the re-use of link names that the user has defined previously. In [[PersonalBrain]], for instance, before specifying the nature of a link, the user must create an appropriate "link type" (associated with a color to be used in presentation) in the system-wide database, and then assign that type to the link in question. This promotes consistency among the names chosen for links, so that the same logical relationship types will hopefully have the same tags throughout the knowledge base. This feature also facilitates searches based on link type, among other things. Other systems, especially those suited for specific domains such as decision modeling ([[gIBIS]]<ref name="Conklin and Begeman 1988" /> and Banxia Decision Explorer), predefine a set of link types that can be assigned (but not altered) by the user.\n\nSome more advanced systems allow links to bear attribute/value pairs themselves, and even embedded structure, similar to those of the items they connect. In Haystack<ref name="Adar et al 1999" /> this is the case, since links ("ties") and nodes ("needles") are actually defined as subtypes of a common type ("straw").\n\nKMap similarly defines a link as a subclass of node, which allows links to represent n-ary relationships between nodes, and enables recursive structure within a link itself. It is unclear how much value this adds in knowledge modeling, or how often users take advantage of such a feature. Neptune<ref name="Delisle and Schwartz 1986" /> and Intermedia<ref name="Garrett et\nal 1986" /> are two older systems that also support attributes for links, albeit in a simpler manner.\n\nAnother aspect of links that generated much fervor in the early hypertext systems was that of link \'\'precision\'\': rather than merely connecting one element to another, systems like Intermedia defined anchors within documents, so that a particular snippet within a larger element could be linked to another snippet. The Dexter model<ref name="Halasz and Schwartz 1994" /> covers this issue in detail. For PKB purposes, this seems to be most relevant as regards links to the objective space, as discussed previously. If the PKB truly contains knowledge, expressed in appropriately fine-grained parts, then link precision between elements in the knowledge base is much less of a consideration.\n\nThis discussion on links has only considered connections between knowledge elements in the system, where the system has total control over both ends of the connection. As described in the previous section, numerous systems provide the ability to "link" from a knowledge element inside the system to some external resource: a file or a URL, say. These external links typically cannot be enhanced with any additional information, and serve only as convenient retrieval paths, rather than as aspects of knowledge representation.\n\n== Architecture ==\n\nThe idea of a PKB gives rise to some important architectural considerations. While not constraining the nature of what knowledge can be expressed, the architecture nevertheless affects more mundane matters such as availability and workflow. But even more importantly, the system\'s architecture determines whether it can truly function as a lifelong, integrated knowledge store—the "base" aspect of the personal knowledge base defined above.\n\n=== File-based ===\n\nTraditionally, most electronic PKB systems have employed a simple storage mechanism based on flat files in a filesystem. This is true of virtually all of the mind mapping tools ([[MindManager]]), concept mapping tools, and even a number of hypertext tools (NoteCards,<ref name="Halasz et al 1987" /> Hypercard,<ref name="Goodman 1988" /> Tinderbox<ref name="Bernstein 2003" />). Typically, the main "unit" of a user\'s knowledge design—whether that be a mind map, a concept map, an outline, or a "notebook"—is stored in its own file somewhere in the filesystem. The application can find and load such files via the familiar "File | Open..." paradigm, at which point it typically maintains the entire knowledge structure in memory.\n\nThe advantage of such a paradigm is familiarity and ease of use; the disadvantage is a possibly negative influence on knowledge formulation. Users must choose one of two basic strategies: either store all of their knowledge in a single file; or else break up their knowledge and store it across a number of different files, presumably according to subject matter and/or time period. The first choice can result in scalability problems—consider how much knowledge a user might collect over a decade, if they stored things related to their personal life, hobbies, relationships, reading materials, vacations, academic course notes, multiple work-related projects, future planning, etc. It seems unrealistic to keep adding this kind of volume to a single, ever-growing multi-gigabyte file. The other option, however, is also constraining: each bit of knowledge can be stored in only one of the files (or else redundantly, which leads to synchronization problems), and the user is forced to choose this at knowledge capture time.\n\n=== Database-based ===\n\nIf a PKB\'s data is stored in a database system, then knowledge elements reside in a global space, which allows any idea to relate to any other: now a user can relate a book he read on productivity not only to other books on productivity, but also to "that hotel in Orlando that our family stayed in last spring," because that is where he remembers having read the book. Though such a relationship may seem "out of bounds" in traditional knowledge organization, it is exactly the kind of retrieval path that humans often employ in retrieving memories.<ref name="Lorayne and Lucas 1974" /><ref name="Anderson 1990" /><ref name="Conway et al 1991" /> The database architecture enables a PKB to truly form an integrated knowledge base, and contain the full range of relationships.\n\nAgenda<ref name="Kaplan et al 1990" /> and [[gIBIS]]<ref name="Conklin and Begeman 1988"/> were two early tools that subsumed a database backend in their architecture. More recently, the MyLifeBits project<ref name="Gemmell et al 2002" /> uses Microsoft SQL Server as its storage layer, and [[Compendium (software)|Compendium]] interfaces with the open source MySQL database.  A few note-taking applications also store information in an integrated database rather than in user-named files. The only significant drawback to this architectural choice (other than the modest footprint of the database management system) is that data is more difficult to copy and share across systems.  This is one true advantage of files: it is a simple matter to copy them across a network, or include them as an e-mail attachment, where they can be read by the same application on a different machine. This problem is solved by some of the following architectural choices.\n\n=== Client–server ===\n\nDecoupling the actual knowledge store from the PKB user interface can achieve architectural flexibility. As with all client-server architectures, the benefits include load distribution, platform interoperability, data sharing, and ubiquitous availability.  Increased complexity and latency are among the liabilities, which can indeed be considerable factors in PKB design.\n\nOne of the earliest and best examples of a client-server knowledge base was the Neptune hypertext system.<ref name="Delisle and Schwartz 1986" /> Neptune was tailored to the task of maintaining shared information within software engineering teams, rather than to personal knowledge storage, but the elegant implementation of its "Hypertext Abstract Machine" (HAM) was a significant and relevant achievement. The HAM was a generic hypertext storage layer that provided node and link storage and maintained version history of all changes. Application layers and user interfaces were to be built on top of the HAM. Architecturally, the HAM provided distributed network access so that client applications could run from remote locations and still access the central store. Another, more recent example, is the Scholarly Ontologies Project<ref name="Uren et al  2004" /><ref name="Sereno et al 2005" /> whose ClaiMapper and ClaiMaker components form a similar distributed solution in order to support collaboration.\n\nThese systems implemented a distributed architecture primarily in order to share data among colleagues. For PKBs, the prime motive is rather user mobility. This is a key consideration, since if a user is to store all of their knowledge into a single integrated store, they will certainly need access to it in a variety of settings.  MyBase Networking Edition is one example of how this might be achieved. A central server hosts the user\'s data, and allows network access from any client machine. Clients can view the knowledge base from within the MyBase application, or through a Web browser (with limited functionality.)\n\nThe Haystack project<ref name="Adar et al 1999" /> outlines a three-tiered architecture, which allows the persistent store, the Haystack data model itself, and the clients that access it to reside on separate machines. The interface to the middle tier is flexible enough that a number of different persistent storage models can be used, including relational databases, semistructured databases, and object-oriented databases. Presto\'s architecture<ref name="Dourish et al 1999" /> exhibits similar features.\n\n==== Web-based ====\n\nA variation of the client-server approach is Web-based systems, in which the client system consists of nothing but a (possibly enhanced) browser. This gives the same ubiquitous availability that client-server approaches do, while minimizing (or eliminating) the setup and installation required on each client machine.\n\nKMap<ref name="Gaines and Shaw 1995" /> was one of the first knowledge systems to integrate with the World Wide Web. It allowed concept maps to be shared, edited, and remotely stored using the HTTP protocol. Concept maps were still created using a standalone client application for the Macintosh, but they could be uploaded to a central server, and then rendered in browsers as "clickable GIFs". Clicking on a concept within the map image in the browser window would have the same navigation effect as clicking on it locally inside the client application.  The user\'s knowledge expressions are stored on a central server in nearly all cases, rather than locally on the browser\'s machine.\n\n=== Handheld devices ===\n\nLastly, mobile devices are a possible PKB architecture. Storing all of one\'s personal knowledge on a PDA would solve the availability problem, of course, and even more completely than would a client-server or web-based architecture.  The safety of the information is an issue, since if the device were to be lost or destroyed, the user could face irrevocable data loss; this is easily remedied, however, by periodically synchronizing the device\'s contents with a host computer.\n\nMost handheld applications are simple note-taking software, with far fewer features than their desktop counterparts. BugMe! is an immensely popular note-taking tool that simply lets users enter text or scribble onto "notes" (screenfulls of space) and then organize them in primitive ways. Screen shots can be captured and included as graphics, and the tool features an array of drawing tools, clip art libraries, etc. The value add for this and similar tools is purely the size and convenience of the handheld device, not the ability to manage large amounts of information.\n\nPerhaps the most effective use of a handheld architecture would be as a satellite data capture and retrieval utility. A user would normally employ a fully functional desktop application for personal knowledge management, but when "on the go," they could capture knowledge into a compatible handheld application and upload it to their PKB at a later convenient time. To enable mobile knowledge retrieval, either select information would need to be downloaded to the device before the user needed it, or else a wireless client-server solution could deliver any part of the PKB on demand. This is essentially the approach taken by software like KeySuite, which supplements a feature-rich desktop information management tool (e.g. [[Microsoft outlook|Microsoft Outlook]]) by providing access to that information on the mobile device.\n\n== See also ==\n* [[Commonplace book]]\n* [[Lifelog]]\n* [[Notetaking]]\n** [[Comparison of notetaking software]]\n* [[Outliner]]\n* [[Personal knowledge management]]\n* [[Personal wiki]]\n** {{section link|List of wiki software|Personal wiki software}}\n* {{section link|Tag (metadata)|Knowledge tags}}\n\n== Notes ==\n{{notelist}}\n\n== References ==\n{{reflist|30em|\nrefs=\n<!-- Converted to LDR format\n     using [[User:PleaseStand/References segregator]] -->\n\n<ref name = "Davies 2005">Davies, S., Velez-Morales, J. and King, R. [http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-997-05.pdf Building the Memex sixty years later: trends and directions in personal knowledge bases]. Technical Report CU-CS-997-05. Boulder, Colorado: Department of Computer Science, University of Colorado at Boulder, August 2005.</ref>\n\n<ref name = "Davies 2011">Davies, S. Still Building the Memex. \'\'Communications of the ACM\'\', vol. 53, issue 2, February 2011, 80-88.</ref>\n\n<ref name = "Brooks 1985">Brooks, T. New technologies and their implications for local area networks. \'\'Computer Communications\'\', vol. 8, no. 2, 1985, 82-87.</ref>\n\n<ref name = "Kruger 1986">Krüger, G. Future information technology—motor of the "information society". in \'\'Employment and the Transfer of Technology\'\'. Berlin: Springer, 1986, 39-52.</ref>\n\n<ref name = "Forman 1988">Forman, G. Making intuitive knowledge explicit through future technology. in \'\'Constructivism in the Computer Age\'\'. Hillsdale, New Jersey: L. Erlbaum, 1988, 83-101.</ref>\n\n<ref name = "Smith 1991">Smith, C.F. Reconceiving hypertext. in \'\'Evolving Perspectives on Computers and Composition Studies: Questions for the 1990s\'\'. Urbana, Illinois: National Council of Teachers of English, 1991, 224-260.</ref>\n\n<ref name="Schneiderman 1987">Schneiderman, B., User interface design for the Hyperties electronic encyclopedia. in \'\'Proceedings of the ACM Conference on Hypertext\'\'. Chapel Hill, North Carolina, 1987, 189-194.</ref>\n\n<ref name = "Nelson 1987">Nelson, T.H. \'\'Literary machines: the report on, and of, Project Xanadu concerning word processing, electronic publishing, hypertext, thinkertoys, tomorrow\'s intellectual revolution, and certain other topics including knowledge, education and freedom\'\'. Swarthmore, Pennsylvania: Theodor H. Nelson, 1987.</ref>\n\n<ref name="Dumais et al 2003">Dumais, S.T., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R. and Robbins, D.C. Stuff I\'ve Seen: a system for personal information retrieval and re-use. in \'\'Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval\'\'. Toronto, Canada, 2003, 72-79.</ref>\n\n<ref name="Nakakoji et al 2000">Nakakoji, K., Yamamoto, Y., Takada, S. and Reeves, B.N. Two-dimensional spatial positioning as a means for reflection in design. in \'\'Proceedings of the Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques\'\'. New York: ACM, 2000, 145-154.</ref>\n\n<ref name="Smith et al 1987">Smith, J.B., Weiss, S.F. and Ferguson, G.J. A hypertext writing environment and its cognitive basis. in \'\'Proceedings of the ACM Conference on Hypertext\'\'. Chapel Hill, North Carolina, 1987, 195-214.</ref>\n\n<ref name="Godwin-Jones 2003">Godwin-Jones, B. Blogs and wikis: environments for on-line collaboration. \'\'Language Learning and Technology\'\', vol. 7, no. 2 (May 2003), 12-16.</ref>\n\n<ref name="Kovalainen et al 1998">Kovalainen, M., Robinson, M. and Auramaki, E. Diaries at work. in \'\'Proceedings of the 1998 ACM Conference on Computer Supported Collaborative Work\'\', Seattle, Washington, 1998, 49-58.</ref>\n\n<ref name="Blandford and Green 2001">Blandford, A.E. and [[Thomas R.G. Green|Green, T.R.G.]]. Group and individual time management tools: what you get is not what you need. \'\'Personal and Ubiquitous Computing\'\', vol. 5, no. 4 (December 2001), 213-230.</ref>\n\n<ref name="Palen 1999">Palen, L. Social, individual and technological issues for groupware calendar systems. In \'\'Proceedings of the SIGCHI conference on Human Factors in Computing Systems\'\', pp. 17-24. ACM, 1999.</ref>\n\n<ref name="Bush 1945">Bush, V. As we may think. \'\'The Atlantic Monthly\'\', July 1945, 101-108.</ref>\n\n<ref name="Novak 2003">Novak, J.D. The theory underlying concept maps and how to construct them. Institute for Human and Machine Cognition, University of West Florida, 2003.</ref>\n\n<ref name="Ausubel 1968">Ausubel, D.P. \'\'Educational Psychology: A Cognitive View\'\'. New York: Holt, Rinehart, and Winston, 1968</ref>\n\n<ref name="Canas et al 2005">Cañas, A.J., Hill, G., Carff, R., Suri, N., Lott, J., Gomez, G., Eskridge, T.C., Arroyo, M. and Carvajal, R. CmapTools: a knowledge modeling and sharing environment. in \'\'Proceedings of the First International Conference on Concept Mapping\'\', Pamplona, Spain, 2005, 125-133</ref>\n\n<ref name="Woods 1985">Woods, W.A. What\'s in a Link: Foundations for Semantic Networks. in Brachman, R.J. and Levesque, J. eds. \'\'Readings in Knowledge Representation\'\', Morgan Kaufmann, 1985.</ref>\n\n<ref name="Goodman 1988">Goodman, D. \'\'The Complete Hypercard Handbook\'\'. New York: Bantam Books, 1988.</ref>\n\n<ref name="Garrett et al 1986">Garrett, L.N., Smith, K.E. and Meyrowitz, N. Intermedia: Issues, strategies, and tactics in the design of a hypermedia document system. in \'\'Proceedings of the Conference on Computer-Supported Cooperative Work\'\', 1986, 163-174.</ref>\n\n<ref name="Davis et al 1993">Davis, H., Hall, W., Heath, I., Hill, G. and Wilkins, R. MICROCOSM: an open hypermedia environment for information integration. in \'\'Proceedings of the INTERCHI Conference on Human Factors in Computing Systems\'\'. ACM Press, 1993.</ref>\n\n<ref name="Pearl 1989">Pearl, A. Sun\'s Link Service: a protocol for open linking. in \'\'Proceedings of the Second Annual ACM Conference on Hypertext\'\', Pittsburgh, Pennsylvania, 1989, 137-146.</ref>\n\n<ref name="Engelbart 1953">Engelbart, D.C. A conceptual framework for the augmentation of man\'s intellect. in Howerton, P.W. ed. \'\'Vistas in Information Handling\'\', Spartan Books, Washington, D.C., 1963, 1-29.</ref>\n\n<ref name="Trigg and Weiser 1986">Trigg, R.H. and Weiser, M. TEXTNET: a network-based approach to text handling. \'\'ACM Transactions on Information Systems\'\', vol. 4, no. 1, 1986, 1-23.</ref>\n\n<ref name="Halasz et al 1987">Halasz, F.G., Moran, T.P. and Trigg, R.H. NoteCards in a Nutshell. \'\'ACM SIGCHI Bulletin\'\', 17, 1986, 45-52.</ref>\n\n<ref name="Conklin and Begeman 1988">Conklin, J. and Begeman, M.L. gIBIS: a hypertext tool for exploratory policy discussion. in \'\'Proceedings of the 1988 ACM Conference on Computer-supported Cooperative Work\'\', Portland, Oregon, 1988, 140-152.</ref>\n\n<ref name="Dede and Jayaram 1990">Dede, C.J. and Jayaram, G. Designing a training tool for imaging mental models. Air Force Human Resources Laboratory, Brooks Air Force Base, Texas, 1990.</ref>\n\n<ref name="Marshall et al 1991">Marshall, C., Halasz, F.G., Rogers, R.A. and Janssen, W.C. Aquanet: a hypertext took to hold your knowledge in place. in \'\'Proceedings of the Third Annual ACM Conference on Hypertext\'\', San Antonio, Texas, 1991, 261-275.</ref>\n\n<ref name="Carlson and Ram 1990">Carlson, D.A. and Ram, S. HyperIntelligence: the next frontier. \'\'Communications of the ACM\'\', vol. 33, no. 3, 1990, 311-321.</ref>\n\n<ref name="Bernstein 2003">Bernstein, M. Collages, composites, construction. in \'\'Proceedings of the Fourteenth ACM Conference on Hypertext and Hypermedia\'\', Nottingham, UK, August 2003, 121-123.</ref>\n\n<ref name="Kaplan et al 1990">Kaplan, S.J., Kapor, M.D., Belove, E.J., Landsman, R.A. and Drake, T.R. Agenda: a personal information manager. \'\'Communications of the ACM\'\', vol. 33, no. 7, 1990, 105-116.</ref>\n\n<ref name="Burger et al 1991">Burger, A.M., Meyer, B.D., Jung, C.P. and Long, K.B. The virtual notebook system. in \'\'Proceedings of the Third Annual ACM Conference on Hypertext\'\', San Antonio, Texas, 1991, 395-401.</ref>\n\n<ref name="Schraefel et al 2002">Schraefel, M.C., Zhu, Y., Modjeska, D., Widgdor, D. and Zhao, S. Hunter Gatherer: interaction support for the creation and management of within-Webpage collections. in \'\'Proceedings of the Eleventh International Conference on the World Wide Web\'\', 2002, 172-181.</ref>\n\n<ref name="Dourish et al 1999">Dourish, P., Edwards, W.K., LaMarca, A. and Salisbury, M. Presto: an experimental architecture for fluid interactive document spaces. \'\'ACM Transactions on Computer-Human Interaction\'\', 6, 2, 133-161.</ref>\n\n<ref name="Jones 1986">Jones, W.P. The Memory Extender personal filing system. in \'\'Proceedings of the SIGCHI Conference on Human Factors in Computing Systems\'\', Boston, Massachusetts, 1986, 298-305.</ref>\n\n<ref name="Adar et al 1999">Adar, E., Karger, D. and Stein, L.A. Haystack: per-user information environments. in \'\'Proceedings of the Eighth International Conference on Information Knowledge Management\'\', Kansas City, Missouri, 1999, 413-422.</ref>\n\n<ref name="Wolber et al 2002">Wolber, D., Kepe, M. and Ranitovic, I. Exposing document context in the personal web. in Proceedings of the 7th International Conference on Intelligent User Interfaces, San Francisco, California, 2002, 151-158.</ref>\n\n<ref name="Hendry and Harper 1997">Hendry, D.G. and Harper, D.J. An informal information-seeking environment. \'\'Journal of the American Society for Information Science\'\', vol. 48, no. 11, 1997, 1036-1048.</ref>\n\n<ref name="Cousins et al 1997">Cousins, S.B., Paepcke, A., Winograd, T., Bier, E.A. and Pier, K. The digital library integrated task environment (DLITE). in \'\'Proceedings of the Second ACM International Conference on Digital Libraries\'\', Philadelphia, Pennsylvania, 1997, 142-151.</ref>\n\n<ref name="Buchanan et al 2004">Buchanan, G., Blandford, A.E., Thimbleby, H. and Jones, M. Integrating information seeking and structuring: exploring the role of spatial hypertext in a digital library. in \'\'Proceedings of the Fifteenth ACM Conference on Hypertext and Hypermedia\'\', Santa Cruz, California, 2004, 225-234.</ref>\n\n<ref name="Marshall and Shipman 1995">Marshall, C. and Shipman, F. Spatial hypertext: designing for change. \'\'Communications of the ACM\'\', vol. 38, no. 8, 1995, 88-97.</ref>\n\n<ref name="Furnas and Rauch 1998">Furnas, G.W. and Rauch, S.J. Considerations for information environments and the NaviQue workspace. in \'\'Proceedings of the ACM Conference on Digital Libraries\'\', 1998, 79-88.</ref>\n\n<ref name="Renda and Straccia 2005">Renda, M.E. and Straccia, U. A personalized collaborative digital library environment: a model and an application. \'\'Information Processing and Management: an International Journal\'\', vol. 41, no. 1, 2005, 5-21.</ref>\n\n<ref name="Hayes et al 2003">Hayes, G., Pierce, J.S. and Abowd, G.D. Practices for capturing short important thoughts. in \'\'CHI \'03 Extended Abstracts on Human Factors in Computing Systems\'\', Ft. Lauderdale, Florida, 2003, 904-905.</ref>\n\n<ref name="Reyes-Farfan and Sanchez 2003">Reyes-Farfan, N. and Sanchez, J.A. Personal spaces in the context of OAI. in \'\'Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries\'\', 2003, 182-183.</ref>\n\n<ref name="Halasz and Schwartz 1994">Halasz, F.G. and Schwartz, M. The Dexter hypertext reference model. \'\'Communications of the ACM\'\', vol. 37, no. 2, February 1994, 30-39.</ref>\n\n<ref name="Adar et al 1999">Adar, E., Karger, D. and Stein, L.A. Haystack: per-user information environments. in \'\'Proceedings of the Eighth International Conference on Information Knowledge Management\'\', Kansas City, Missouri, 1999, 413-422.</ref>\n\n<ref name="Gaines and Shaw 1995">Gaines, B.R. and Shaw, M.L.G. Concept maps as hypermedia components. \'\'International Journal of Human Computer Studies\'\', vol. 43, no. 3, 1995, 323-361.</ref>\n\n<ref name="Quillian 1968">Quillian, M.R. Semantic memory. in \'\'Semantic Information Processing\'\', Cambridge, Massachusetts: MIT Press, 1968, 227-270.</ref>\n\n<ref name="Nosek and Roth 1990">Nosek, J.T. and Roth, I. A comparison of formal knowledge representationschemes as communication tools: predicate logic vs semantic network. \'\'International Journal of Man-Machine Studies\'\', vol. 33, no. 2, 1990, 227-239.</ref>\n\n<ref name="Delisle and Schwartz 1986">Delisle, N. and Schwartz, M. Neptune: a hypertext system for CAD applications. in \'\'Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data\'\', Washington, D.C., 1986, 132-143.</ref>\n\n<ref name="Mantei 1982">Mantei, M.M. \'\'Disorientation behavior in person–computer interaction\'\'. Ph.D. thesis. Communications Department, University of Southern California, 1982.</ref>\n\n<ref name="Feiner 1988">Feiner, S. Seeing the forest for the trees: hierarchical display of hypertext structure. in \'\'Proceedings of the ACM SIGOIS and IEEECS TC-OA 1988 Conference on Office Information Systems\'\', Palo Alto, California, 1988, 205-212.</ref>\n\n<ref name="Koy 1997">Koy, A.K. Computer aided thinking. in \'\'Proceedings of the 7th International Conference on Thinking\'\', Singapore, 1997.</ref>\n\n<ref name="Selvin 1999">Selvin, A.M. Supporting collaborative analysis and design with hypertext functionality. \'\'Journal of Digital Information\'\', vol. 1, no. 4, 1999.</ref>\n\n<ref name="Di Giacomo et al 2001">Di Giacomo, M., Mahoney, D., Bollen, J., Monroy-Hernandez, A. and Meraz, C.M.R. MyLibrary, a personalization service for digital library environments. In \'\'DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries\'\', June 2001.</ref>\n\n<ref name="Akscyn et al 1987">Akscyn, R., McCracken, D. and Yoder, E. KMS: a distributed hypermedia system for managing knowledge in organizations. in \'\'Proceedings of the ACM Conference on Hypertext\'\', Chapel Hill, North Carolina, 1987.</ref>\n\n<ref name="Fertig et al 1996">Fertig, S., Freeman, E. and Gelernter, D. Lifestreams: An alternative to the desktop metaphor. in \'\'Proceedings of the Conference on Human Factors in Computing Systems (CHI96)\'\', Vancouver, British Columbia, 1996, 410-411.</ref>\n\n<ref name="Freeman and Gelernter 1996">Freeman, E. and Gelernter, D. Lifestreams: a storage model for personal data. ACM SIGMOD Record, 25, 1.(March 1996), 80-86.</ref>\n\n<ref name="Gemmell et al 2002">Gemmell, J., Bell, G., Lueder, R., Drucker, S. and Wong, C. MyLifebits: Fulfilling the Memex vision. in \'\'Proceedings of the 2002 ACM Workshops on Multimedia\'\', 2002, 235-238.</ref>\n\n<ref name="Shipman et al 2000">Shipman, F., Hsieh, H. and Airhart, R. Analytic workspaces: supporting the emergence of interpretation in the Visual Knowledge Builder. Department of Computer Science and Center for the Study of Digital Libraries, Texas A&M University, 2000.</ref>\n\n<ref name="diSessa and Abelson 1986">diSessa, A.A. and Abelson, H. Boxer: a reconstructible computational medium. \'\'Communications of the ACM\'\', vol. 29, no. 9, 1986, 859-868.</ref>\n\n<ref name="Anderson 1990">Anderson, J.R. \'\'Cognitive Psychology and Its Implications\'\', 3rd Ed. New York: W.H. Freeman, 1990.</ref>\n\n<ref name="Lorayne and Lucas 1974">Lorayne, H. and Lucas, J. \'\'The Memory Book\'\'. New York: Stein and Day, 1974.</ref>\n\n<ref name="Conway et al 1991">Conway, M.A., Kahney, H., Bruce, K. and Duce, H. Imaging objects, routines, and locations. in Logie, R.H. and Denis, M. eds. \'\'Mental Images in Human Cognition\'\', New York: Elsevier Science Publishing, 1991, 171-182.</ref>\n\n<ref name="Uren et al 2004">Uren, V., Buckingham Shum, S., Li, G. and Bachler, M. Sensemaking tools for understanding research literatures: design, implementation, and user evaluation. Knowledge Media Institute, The Open University, 2004, 1-42.</ref>\n\n<ref name="Sereno et al 2005">Sereno, B., Buckingham Shum, S. and Motta, E. ClaimSpotter: an environment to support sensemaking with knowledge triples. in \'\'Proceedings of the International Conference on Intelligent User Interfaces\'\', San Diego, California, 2005, 1999-1206.</ref>\n\n<ref name="Perlin and Fox 1993">Perlin, K. and Fox, D. Pad: an alternative approach to the computer interface. in \'\'Proceedings of the 20th annual conference on computer graphics and interactive techniques\'\', 1993, 57-64.</ref>\n}}\n\n{{Computable knowledge}}\n\n[[Category:Knowledge representation]]']
['Dublin Core', '8742', '{{Use dmy dates|date=July 2012}}\nThe \'\'\'Dublin Core Schema\'\'\' is a small set of vocabulary terms that can be used to describe web resources (video, images, web pages, etc.), as well as physical resources such as books or CDs, and objects like artworks.<ref>{{cite web|url=http://dublincore.org/documents/dcmi-type-vocabulary/index.shtml |title=DCMI Metadata Terms |publisher=Dublincore.org |accessdate=5 April 2013}}</ref> The full set of Dublin Core metadata terms can be found on the Dublin Core Metadata Initiative (DCMI) website.<ref>{{cite web|url=http://dublincore.org/documents/dcmi-terms/ |title=DCMI Metadata Terms |publisher=Dublincore.org |accessdate=5 April 2013}}</ref> The original set of 15 classic<ref>{{cite web|url=http://dublincore.org/specifications/ |title=DCMI Specifications |publisher=Dublincore.org |date=14 December 2009 |accessdate=5 April 2013}}</ref> metadata terms, known as the Dublin Core Metadata Element Set<ref name="DCMES">{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |accessdate=5 April 2013}}</ref> are endorsed in the following standards documents:\n\n* IETF RFC 5013<ref>[http://www.ietf.org/rfc/rfc5013.txt The Dublin Core Metadata Element Set], Dublin Core Metadata Initiative, August 2007</ref>\n* ISO Standard 15836-2009<ref>{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnumber=52142 |title=ISO 15836:2009 - Information and documentation - The Dublin Core metadata element set |publisher=Iso.org |date=18 February 2009 |accessdate=5 April 2013}}</ref>\n* NISO Standard Z39.85<ref>{{cite web|url=http://www.niso.org/kst/reports/standards?step=2&gid=None&project_key=9b7bffcd2daeca6198b4ee5a848f9beec2f600e5 |title=NISO Standards - National Information Standards Organization |publisher=Niso.org |date=22 May 2007 |accessdate=5 April 2013}}</ref>\n\nDublin Core Metadata may be used for multiple purposes, from simple resource description, to combining metadata vocabularies of different [[Metadata#Metadata standards|metadata standards]], to providing interoperability for metadata vocabularies in the [[Linked Data]] cloud and [[Semantic Web]] implementations.\n\n== Background ==\n"Dublin" refers to [[Dublin, Ohio]], USA where the schema originated during the 1995 invitational OCLC/NCSA Metadata Workshop,<ref>[http://dublincore.org/workshops/dc1/ OCLC/NCSA Metadata Workshop]</ref> hosted by the [[Online Computer Library Center]] (OCLC), a library consortium based in Dublin, and the [[National Center for Supercomputing Applications]] (NCSA).  "Core" refers to the metadata terms as "broad and generic being usable for describing a wide range of resources".<ref name="DCMES"/> The semantics of Dublin Core were established and are maintained by an international, cross-disciplinary group of professionals from [[librarianship]], [[computer science]], [[text encoding]], [[museum]]s, and other related fields of scholarship and practice.\n\nStarting in 2000, the Dublin Core community focused on "[[application profile]]s" &ndash; the idea that metadata records would use Dublin Core together with other specialized vocabularies to meet particular implementation requirements. During that time, the World Wide Web Consortium\'s work on a generic data model for metadata, the [[Resource Description Framework]] (RDF), was maturing. As part of an extended set of DCMI Metadata Terms, Dublin Core became one of the most popular vocabularies for use with RDF, more recently in the context of the Linked Data movement.<ref>{{cite web|title=DCMI Metadata Basics|publisher=dublincore.org/metadata-basics/}}</ref>\n\nThe \'\'\'Dublin Core Metadata Initiative\'\'\' (DCMI)<ref>{{cite web|url=http://dublincore.org/ |title=DCMI Home: Dublin Core® Metadata Initiative (DCMI) |publisher=Dublincore.org |date= |accessdate=2015-12-04}}</ref> provides an open forum for the development of interoperable online [[metadata standards]] for a broad range of purposes and of business models. DCMI\'s activities include consensus-driven working groups, global conferences and workshops, standards liaison, and educational efforts to promote widespread acceptance of metadata standards and practices. In 2008, DCMI separated from OCLC and incorporated as an independent entity.<ref>{{cite web | title=OCLC Research and the Dublin Core Metadata Initiative | url=http://www.oclc.org/research/activities/past/orprojects/dublincore/default.htm | accessdate=21 April 2010}}</ref>\n\nCurrently, any and all changes that are made to the Dublin Core standard, are reviewed by a DCMI Usage Board within the context of a DCMI Namespace Policy (DCMI-NAMESPACE). This policy describes how terms are assigned and also sets limits on the amount of editorial changes allowed to the labels, definitions, and usage comments.<ref>{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |date= |accessdate=2015-12-04}}</ref>\n\n== Levels of the standard ==\nThe Dublin Core standard originally includes two levels: Simple and Qualified. \'\'\'Simple Dublin Core\'\'\' comprised 15 elements; \'\'\'Qualified Dublin Core\'\'\' included three additional elements (Audience, Provenance and RightsHolder), as well as a group of element refinements (also called qualifiers) that could refine the semantics of the elements in ways that may be useful in resource discovery.\n\nSince 2012 the two have been incorporated into the DCMI Metadata Terms as a single set of terms using the [[Resource Description Framework]] (RDF).<ref name="dublincore1">{{cite web|url=http://dublincore.org/documents/dcmi-terms/ |title=DCMI Metadata Terms |publisher=Dublincore.org |date= |accessdate=2015-12-04}}</ref> The full set of elements is found under the namespace http://purl.org/dc/terms/. Because the definition of the terms often contains domains and ranges, which may not be compatible with the pre-RDF definitions used for the original 15 Dublin Core elements, there is a separate namespace for the original 15 elements as previously defined: http://purl.org/dc/elements/1.1/.<ref>[http://dublincore.org/documents/dces/ Dublin Core Metadata Element Set, version 1.1]</ref>\n\n=== Dublin Core Metadata Element Set Version 1.1===\nThe original \'\'\'Dublin Core Metadata Element Set\'\'\' consists of 15 metadata elements:<ref name="DCMES"/>\n# Title\n# Creator\n# Subject\n# Description\n# Publisher\n# Contributor\n# Date\n# Type\n# Format\n# Identifier\n# Source\n# Language\n# Relation\n# Coverage\n# Rights\n\nEach Dublin Core element is optional and may be repeated. The DCMI has established standard ways to refine elements and encourage the use of encoding and vocabulary schemes. There is no prescribed order in Dublin Core for presenting or using the elements. The Dublin Core became ISO 15836 standard in 2006 and is used as a base-level data element set for the description of learning resources in the [[ISO/IEC 19788]]-2 Metadata for learning resources (MLR) &ndash; Part 2: Dublin Core elements, prepared by the [[ISO/IEC JTC1/SC36|ISO/IEC JTC1 SC36]].\n\nFull information on element definitions and term relationships can be found in the Dublin Core Metadata Registry.<ref name="registry">[http://dcmi.kc.tsukuba.ac.jp/dcregistry/ Dublin Core Metadata Registry]</ref>\n\n==== Example of code ====\n: {{code|2=html4strict|1=<meta name="DC.Format" content="video/mpeg; 10 minutes">}}\n: {{code|2=html4strict|1=<meta name="DC.Language" content="en" >}}\n: {{code|2=html4strict|1=<meta name="DC.Publisher" content="publisher-name" >}}\n: {{code|2=html4strict|1=<meta name="DC.Title" content="HYP" >}}\n\n==== An example of use [and mention] of D.C. (by [[WebCite]]) ====\n\nAt the web page which serves as the "archive" form for [[WebCite]],<ref name="WebCite_archive_form_(web_page)">{{cite web\n| url          = http://webcitation.org/archive\n| title        = WebCite® archive form\n| quote        = Metadata (optional)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These are Dublin Core elements. [...]\n| publisher    = [[WebCite]]\n}}\n</ref> it says, in part: "Metadata (optional)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These are Dublin Core elements. [...]".\n\n=== Qualified Dublin Core (deprecated in 2012<ref>{{cite web|url=http://dublincore.org/documents/2000/07/11/dcmes-qualifiers/ |title=Dublin Core Qualifiers |publisher=Dublincore.org |date= |accessdate=2015-12-04}}</ref>)===\nSubsequent to the specification of the original 15 elements, an ongoing process to develop exemplary terms extending or refining the Dublin Core Metadata Element Set (DCMES) was begun. The additional terms were identified, generally in working groups of the Dublin Core Metadata Initiative, and judged by the DCMI Usage Board to be in conformance with principles of good practice for the qualification of Dublin Core metadata elements.\n\nElements refinements make the meaning of an element narrower or more specific. A refined element shares the meaning of the unqualified element, but with a more restricted scope. The guiding principle for the qualification of Dublin Core elements, colloquially known as the \'\'Dumb-Down Principle\'\',<ref>[http://dublincore.org/workshops/dc8/dcgrammar/tsld008.html Dumb-Down Principle for qualifiers]</ref> states that an application that does not understand a specific element refinement term should be able to ignore the qualifier and treat the metadata value as if it were an unqualified (broader) element. While this may result in some loss of specificity, the remaining element value (without the qualifier) should continue to be generally correct and useful for discovery.\n\nIn addition to element refinements, Qualified Dublin Core includes a set of recommended encoding schemes, designed to aid in the interpretation of an element value. These schemes include controlled vocabularies and formal notations or parsing rules. A value expressed using an encoding scheme may thus be a token selected from a controlled vocabulary (for example, a term from a classification system or set of subject headings) or a string formatted in accordance with a formal notation, for example, "2000-12-31" as the ISO standard expression of a date. If an encoding scheme is not understood by an application, the value may still be useful to \'\'\'human reader\'\'\'.\n\n\'\'\'Audience, Provenance\'\'\' and \'\'\'RightsHolder\'\'\' are elements, but not part of the Simple Dublin Core 15 elements. Use Audience, Provenance and RightsHolder only when using Qualified Dublin Core.\nDCMI also maintains a small, general vocabulary recommended for use within the element Type. This vocabulary currently consists of 12 terms.<ref name="registry"/>\n\n=== DCMI Metadata Terms ===\nThe Dublin Core Metadata Initiative (DCMI) Metadata Terms is the current set of the Dublin Core vocabulary.<ref name="dublincore1"/> This set includes the fifteen terms of the Dublin Core Metadata Element Set (in \'\'italic\'\'), as well as the qualified terms. Each term has a unique URI in the namespace http://purl.org/dc/terms, and all are defined as [[Resource Description Framework|RDF]] properties.\n\n{{columns-list|4|\n*abstract\n*accessRights\n*accrualMethod\n*accrualPeriodicity\n*accrualPolicy\n*alternative\n*audience\n*available\n*bibliographicCitation\n*conformsTo\n*\'\'contributor\'\'\n*\'\'coverage\'\'\n*created\n*\'\'creator\'\'\n*\'\'date\'\'\n*dateAccepted\n*dateCopyrighted\n*dateSubmitted\n*\'\'description\'\'\n*educationLevel\n*extent\n*\'\'format\'\'\n*hasFormat\n*hasPart\n*hasVersion\n*\'\'identifier\'\'\n*instructionalMethod\n*isFormatOf\n*isPartOf\n*isReferencedBy\n*isReplacedBy\n*isRequiredBy\n*issued\n*isVersionOf\n*\'\'language\'\'\n*license\n*mediator\n*medium\n*modified\n*provenance\n*\'\'publisher\'\'\n*references\n*\'\'relation\'\'\n*replaces\n*requires\n*\'\'rights\'\'\n*rightsHolder\n*\'\'source\'\'\n*spatial\n*\'\'subject\'\'\n*tableOfContents\n*temporal\n*\'\'title\'\'\n*\'\'type\'\'\n*valid\n}}\n\n== Syntax ==\nSyntax choices for Dublin Core metadata depends on a number of variables, and "one size fits all" prescriptions rarely apply. When considering an appropriate syntax, it is important to note that Dublin Core concepts and semantics are designed to be syntax independent and are equally applicable in a variety of contexts, as long as the metadata is in a form suitable for interpretation both by machines and by human beings.\n\nThe \'\'\'Dublin Core Abstract Model\'\'\'<ref>[http://dublincore.org/documents/abstract-model/ Dublin Core Abstract Model]</ref> provides a reference model against which particular Dublin Core encoding guidelines can be compared, independent of any particular encoding syntax. Such a reference model allows implementers to gain a better understanding of the kinds of descriptions they are trying to encode and facilitates the development of better mappings and translations between different syntax.\n\n== Some applications ==\nOne [[Document Type Definition]] based on Dublin Core is the [http://www.ibiblio.org/osrt/omf/ Open Source Metadata Framework] (OMF) specification. OMF is in turn used by [[Rarian]] (superseding [[ScrollKeeper]]), which is used by the [[GNOME]] desktop and [[KDE]] help browsers and the ScrollServer documentation server. [[PBCore]] is also based on Dublin Core. The [[Zope]] [[Zope Content Management Framework|CMF\'s]] Metadata products, used by the [[Plone (content management system)|Plone]], [[ERP5]], the Nuxeo CPS [[Content management system]]s, [[SimpleDL]], and [[FedoraCommons]] also implement Dublin Core. The [[EPUB]] [[e-book]] format uses Dublin Core metadata in the [[OPF (file format)|OPF file]].<ref>{{cite web|url=http://www.idpf.org/epub/20/spec/OPF_2.0_latest.htm#Section2.2|title=Open Packaging Format (OPF) 2.0.1 – 2.2: Publication Metadata|publisher=[[International Digital Publishing Forum]]|accessdate=12 September 2013}}</ref> [[eXo Platform]] also implements Dublin Core.\n\nDCMI also maintains a list of projects using Dublin Core<ref>{{cite web|url=http://dublincore.org/projects/|title=DCMI Projects - Alphabetical|publisher=DCMI|accessdate=15 March 2013}}</ref> on its website.\n\n== See also ==\n* [[Metadata registry]]\n* [[Metadata Object Description Schema]]\n* [[Wikiversity:Digital Libraries/Metadata|Metadata from Wikiversity]]\n* [[Semantic Web]]\n* [[Ontology (information science)]]\n* [[Open Archives Initiative]]\n* [[Controlled vocabulary]]\n* [[Interoperability]]\n* [[Asset Description Metadata Schema]] ([http://www.w3.org/TR/vocab-adms/ ADMS]), a metadata standard maintained by the [[World Wide Web Consortium]] for describing semantic standards. Implemented on Joinup.<ref>{{cite web|url=https://joinup.ec.europa.eu/catalogue/all?filters=bs_current_version:true{{!}}Joinup |title=Joinup &#124; Joinup |publisher=Joinup.ec.europa.eu |date=2015-10-22 |accessdate=2015-12-04}}</ref> \n* [[Metadata Encoding and Transmission Standard]] (METS), maintained by the [[Library of Congress]] for the [[Digital Library Federation]]\n* [[Preservation Metadata: Implementation Strategies]] (PREMIS)\n\n=== Related software ===\n* [[Dublin Core Meta Toolkit]] (Conversion of Access, MySQL, or CSV data to DublinCore metadata)\n* [[Fedora (software)|Fedora]] repository architecture and Project (An open-source software system capable of implementing [[OAI-PMH]] (and thus Dublin Core).\n* [[Omeka]], A free, open-source, unqualified Dublin-Core compliant web-publishing system for digital archives.\n* The [http://archiviststoolkit.org/ Archivist\'s Toolkit] is a self-described as an "Archival Data Management system" able to work with the Dublin Core format. It will soon be merged with [[Archon (software)|Archon]], which is ambiguous as to its OAI support.\n* [[ICA-AtoM]], a web-based archival description/publication software that can serve as an OAI-PMH repository and uses OAI-PMH as the main language for remote data exchange\n\n== References ==\n{{Reflist|2}}\n\n== Further reading ==\n* {{cite book |title= Organising Knowledge in a Global Society |last= Harvey  |first= Ross |authorlink= |author2=Philip Hider |year= 2004 |publisher= Charles Sturt University |location= Wagga Wagga NSW |isbn= 1-876938-66-8 |page= |pages= |url= |accessdate=}}\n* [https://www.inf.unibz.it/courses/images/stories/2005_2006/Digital_Libraries/dini-less-5-6.ppt "Lecture slides about Dublin Core"], by Luca Dini, lecturer at the [[Free University of Bolzano]]\n\n== External links ==\n* [http://dublincore.org/ Dublin Core Metadata Initiative]\n* [http://wiki.dublincore.org/index.php/User_Guide Dublin Core usage guide]\n* [http://xml.coverpages.org/ni2005-03-21-a.html Dublin Core Metadata Initiative Publishes DCMI Abstract Model] (\'\'Cover Pages\'\', March 2005)\n* [http://www.loc.gov/standards/mods/v3/mods-userguide-3-0.html Metadata Object Description Schema (MODS)]\n*[http://www.dublincoregenerator.com/ The Dublin Core Generator: A tool for generating Dublin Core code]\n*[http://library.kr.ua/dc/dceditunie.html The Dublin Core Generator-Editor: Free tool for extracting-editing Dublin Core HTML code]\n\n{{Semantic Web}}\n{{Authority control}}\n\n[[Category:Archival science]]\n[[Category:Bibliography file formats]]\n[[Category:Digital libraries]]\n[[Category:Information management]]\n[[Category:Interoperability]]\n[[Category:ISO standards]]\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]\n[[Category:Metadata standards]]\n[[Category:Museology]]\n[[Category:Records management]]\n[[Category:Reference models]]\n[[Category:Semantic Web]]']
['BabelNet', '37291130', '{{Infobox software\n |name = BabelNet\n |logo = [[File:BabelNet_logo.svg|140px|BabelNet logo.]]\n |screenshot =\n |caption = Wikipedia Extraction\n |developer = \n |released = \n |latest_release_version = BabelNet 3.7\n |latest_release_date = August 2016\n |operating_system = {{Flatlist|\n* [[Virtuoso Universal Server]]\n* [[Lucene]]\n}}\n |genre = {{Flatlist|\n* [[Machine-readable dictionary|Multilingual encyclopedic dictionary]]\n* [[Linked data]]\n}}\n |programming language = \n |license = [[Attribution-NonCommercial-ShareAlike 3.0 Unported]]\n |website = {{URL|babelnet.org}}\n |alexa   = \n}}\n\n\'\'\'BabelNet\'\'\' is a [[Multilinguality|multilingual]] lexicalized [[semantic network]] and [[Ontology (information science)|ontology]] developed at the Linguistic Computing Laboratory in the Department of Computer Science of the [[Sapienza University of Rome]].<ref name="NavigliPonzetto12">R. Navigli and S. P Ponzetto. 2012. [http://dx.doi.org/10.1016/j.artint.2012.07.001 BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network]. Artificial Intelligence, 193, Elsevier, pp. 217-250.</ref><ref>R. Navigli, S. P. Ponzetto. [http://www.aclweb.org/anthology/P/P10/P10-1023.pdf BabelNet: Building a Very Large Multilingual Semantic Network]. Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July 11–16, 2010, pp. 216–225.</ref> BabelNet was automatically created by linking Wikipedia, to the most popular computational [[Dictionary|lexicon]] of the [[English language]], [[WordNet]]. The integration is performed by means of an automatic mapping and by filling in lexical gaps in resource-poor [[language]]s with the aid of [[statistical machine translation]]. The result is an "encyclopedic dictionary" \nthat provides [[concept]]s and [[Named entity|named entities]] [[Lexicalization|lexicalized]] in many languages and connected with large amounts of [[Semantic relation#Relationships|semantic relations]]. Additional lexicalizations and definitions are added by linking to free-license wordnets, [[OmegaWiki]], the English [[Wiktionary]], [[Wikidata]], [[FrameNet]], [[VerbNet]] and others. Similarly to WordNet, BabelNet groups [[word]]s in different languages into sets of [[synonyms]], called \'\'Babel [[synsets]]\'\'. For each Babel synset, BabelNet provides short definitions (called [[Definition|glosses]]) in many languages harvested from both WordNet and Wikipedia.\n\n[[File:The BabelNet structure.png|thumb|600px|BabelNet is a multilingual semantic network obtained as an integration of WordNet and Wikipedia.]]\n\n==Statistics of BabelNet==\n\n{{As of|2016|08}}, BabelNet (version 3.7) covers 271 [[language]]s, including all European languages, most [[Asian language]]s, and [[Latin]]. BabelNet 3.7 contains almost 14 million synsets and about 746 million [[word sense]]s (regardless of their language). Each Babel synset contains 2 synonyms per language, i.e., word senses, on average. The semantic network includes all the lexico-semantic relations from WordNet ([[Hyponymy|hypernymy and hyponymy]], [[meronymy]] and [[holonymy]], [[antonymy]] and [[synonymy]], etc., totaling around 364,000 relation edges) as well as an underspecified relatedness relation from Wikipedia (totaling around 380 million relation edges).<ref name="NavigliPonzetto12" /> Version 3.7 also associates about 11 million images with Babel synsets and provides a Lemon [[Resource Description Framework|RDF]] encoding of the resource,<ref>M. Ehrmann, F. Cecconi, D. Vannella, J. McCrae, P. Cimiano, R. Navigli. [http://www.lrec-conf.org/proceedings/lrec2014/pdf/810_Paper.pdf Representing Multilingual Data as Linked Data: the Case of BabelNet 2.0]. Proc. of the 9th Language Resources and Evaluation Conference (LREC 2014), Reykjavik, Iceland, 26–31 May 2014.</ref> available via a [[SPARQL endpoint]]. 2.67 million synsets are assigned domain labels.\n\n==Applications==\n\nBabelNet has been shown to enable multilingual [[Natural Language Processing]] applications. The lexicalized [[knowledge]] available in BabelNet has been shown to obtain state-of-the-art results in:\n\n* [[semantic relatedness]]<ref>R. Navigli and S. Ponzetto. 2012. [http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/download/5112/5126 BabelRelate! A Joint Multilingual Approach to Computing Semantic Relatedness]. Proc. of the 26th AAAI Conference on Artificial Intelligence (AAAI 2012), Toronto, Canada, pp. 108-114.</ref><ref>J. Camacho-Collados, M. T. Pilehvar and R. Navigli. [http://aclweb.org/anthology/N/N15/N15-1059.pdf NASARI: a Novel Approach to a Semantically-Aware Representation of Items]. Proc. of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2015), Denver, Colorado (US), 31 May-5 June 2015, pp. 567-577.</ref>\n* multilingual [[Word Sense Disambiguation]]<ref>R. Navigli and S. Ponzetto. [http://www.aclweb.org/anthology/D12-1128 Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation]. Proc. of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP 2012), Jeju, Korea, July 12–14, 2012, pp. 1399-1410.</ref>\n* multilingual Word Sense Disambiguation and [[Entity Linking]] with the [[Babelfy]] system<ref>A. Moro, A. Raganato, R. Navigli. [http://www.transacl.org/wp-content/uploads/2014/05/54.pdf Entity Linking meets Word Sense Disambiguation: a Unified Approach]. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 231-244, 2014.</ref>\n* [[game with a purpose|video games with a purpose]]<ref>D. Jurgens, R. Navigli. {{webarchive|url=http://web.archive.org/web/20150103085712/http://www.transacl.org/wp-content/uploads/2014/10/421-camera-ready.pdf |date=January 3, 2015 |title=It\'s All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation}}. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 449-464, 2014.</ref>\n\n==Prizes and acknowledgments==\nBabelNet received the [http://www.meta-net.eu/meta-prize META prize] 2015 for "groundbreaking work in overcoming language barriers through a multilingual lexicalised semantic network and ontology making use of heterogeneous data sources". \n\nBabelNet featured prominently in a [[TIME magazine]]\'s article<ref>Katy Steinmetz. [http://time.com/4327440/redefining-the-modern-dictionary/ Redefining the modern dictionary], TIME magazine, vol. 187, 23 maggio 2016, pp. 20-21.</ref> about the new age of innovative and up-to-date lexical knowledge resources available on the Web. The article describes in some detail how BabelNet is playing a leading role in the 21st century scenario.\n\n==See also==\n* [[Babelfy]]\n* [[EuroWordNet]]\n* [[Knowledge acquisition]]\n* [[Linguistic Linked Open Data]]\n* [[OmegaWiki]]\n* [[Semantic network]]\n* [[Semantic relatedness]]\n* [[Wikidata]]\n* [[Wiktionary]]\n* [[Word sense disambiguation]]\n* [[Word sense induction]]\n* [[UBY]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{Official website|http://babelnet.org}}\n* [http://babelnet.org/sparql SPARQL endpoint]\n* [http://www.meta-net.eu/meta-prize META prize]\n\n[[Category:Lexical databases]]\n[[Category:Knowledge bases]]\n[[Category:Ontology (information science)]]\n[[Category:Knowledge representation]]\n[[Category:Computational linguistics]]\n[[Category:Artificial intelligence]]\n[[Category:Online dictionaries]]\n[[Category:Multilingualism]]']
['SMW+', '21890258', '{{Infobox software\n|name= SMW+\n|screenshot=[[File:Fr Smw plus screenshot.png|300px|SMW+]]\n|caption=\n|developer=[http://www.diqa-pm.com/en/Main_Page DIQA-Projektmanagement GmbH]\n|latest release version = 1.7.0\n|latest release date = {{release date|2012|04|25}}\n|discontinued = yes\n|operating_system= [[Cross-platform]]\n|programming language=[[PHP]]\n|database=[[MySQL]]\n|genre=[[MediaWiki]] extension\n|license=[[GNU General Public License|GPL]]\n|website=[http://semanticweb.org/wiki/SMW%2B SMW+ homepage]\n}}\n\n\'\'\'SMW+\'\'\' is an [[open source]] [[Software suite|software bundle]] composed of the [[wiki software|wiki application]] [[MediaWiki]] along with a number of its extensions, that was developed by the [[Germany|German]] software company [[Ontoprise GmbH]] from 2007 to 2012. In 2012, Ontoprise GmbH filed for bankruptcy<ref>{{cite web |url=http://www.econo.de/no_cache/nachrichten/einzelansicht/article/ontoprise-stellt-insolvenzantrag.html|title=Ontoprise stellt Insolvenzantrag |trans_title=Ontoprise starts insolvency proceedings |language=German |date=3 May 2012 |publisher=econo |accessdate=30 July 2012}}</ref> and went out of business. DIQA-Projektmanagement GmbH, a start-up founded by former Ontoprise employees,<ref>{{cite web |author=Michael Erdmann |title=SMW+ is dead, long live SMW+ |url=http://sourceforge.net/mailarchive/message.php?msg_id=29589354 |work=[[MediaWiki]] users mailing list |date=25 July 2012 |accessdate=30 July 2012}}</ref> now offers support for the software in SMW+, though under the name "[http://diqa-pm.com/en/DataWiki DataWiki]".\n\n== Details ==\n\nSMW+\'s extensions include, most notably, [[Semantic MediaWiki]] and the [http://semanticweb.org/wiki/Halo_Extension Halo Extension]. Cumulatively, SMW+ functions as a [[semantic wiki]], and is also meant to serve as an [[enterprise wiki]] for use within companies, for applications such as [[knowledge management]] and [[project management]].\n\nThe SMW+ platform was available in a number of formats including a Windows installer, Linux installer and [[VMware]] image.\n\nSMW+ emerged from [[Project Halo]], a research project meant to provide a platform for collaborative knowledge engineering for [[Subject matter expert|domain experts]] in the [[biology]], [[chemistry]] and [[physics]] at the first stage.\n\nSMW+ is used by the [[Intergovernmental Oceanographic Commission]] of [[UNESCO]] to power an online encyclopedia for [[oceanography]].{{citation needed|date=April 2013}}\n\n== References ==\n{{reflist}}\n* [http://smwplus.net/index.php/Business_applications_with_SMW%2B \'\'Business applications with SMW+, a Semantic Enterprise Wiki\'\']. Michael Erdmann, Daniel Hansch.\n* [http://smwplus.net/index.php/Practical_applications_of_Semantic_MediaWiki_in_commercial_environments \'\'Practical applications of Semantic MediaWiki in commercial environments - Case Study: semantic-based project management\'\']. Daniel Hansch, Hans-Peter Schnurr. Presented at the ESTC 2009.\n* [http://swui.webscience.org/SWUI2008CHI/Pfisterer.pdf \'\'User-Centered Design and Evaluation of Interface Enhancements to the Semantic MediaWiki\'\']. Frederik Pfisterer, Markus Nitsche, Anthony Jameson and Catalin Barbu. Presented at the CHI2008 (Computer Human Interaction Conference).\n* [http://www.academypublisher.com/jetwi/vol1/no1/jetwi01019496.pdf \'\'Semantic Wikis: A Comprehensible Introduction with Examples from the Health Sciences\'\']. [[Maged N. Kamel Boulos]]. Journal of Emerging Technologies in Web Intelligence, Vol. 1, No. 1, August 2009\n* [http://smwplus.net/index.php/Towards_a_Collaborative_Semantic_Wiki-based_Approach_to_IT_Service_Management \'\'Towards a Collaborative Semantic Wiki-based Approach to IT Service Management\'\']. Frank Kleiner, Andreas Abecker. Proceedings of I-SEMANTICS ’09.\n\n==External links==\n* [http://semanticweb.org/wiki/SMW%2B SMW+] on semanticweb.org\n* [http://www.semanticweb.com/main/semantic_mediawiki_development_picks_up_steam_138918.asp Article at semanticweb.com about SMW+]\n* [http://videolectures.net/iswc08_greaves_swfttsotsw/ "Semantic Wikis: Fusing the two strands of the Semantic Web"] - Talk given by Mark Greaves at the ISWC 2008\n\n{{DEFAULTSORT:Smw}}\n[[Category:Semantic wiki software]]\n[[Category:Knowledge representation]]\n[[Category:Free software programmed in PHP]]\n[[Category:MediaWiki extensions]]']
['Chow–Liu tree', '12680566', '[[File:Chow-liu.png|thumb|400 px|A first-order dependency tree representing the product on the left.]]\n\nIn probability theory and statistics \'\'\'Chow–Liu tree\'\'\' is an efficient method for constructing a second-[[Orders of approximation|order]] product approximation of a [[joint probability distribution]], first described in a paper by {{Harvtxt|Chow|Liu|1968}}.  The goals of such a decomposition, as with such [[Bayesian networks]] in general, may be either [[data compression]] or [[inference]].\n\n==The Chow–Liu representation==\nThe Chow–Liu method describes a [[joint probability distribution]] <math>P(X_{1},X_{2},\\ldots,X_{n})</math> as a product of second-order conditional and marginal distributions.  For example, the six-dimensional distribution <math>P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})</math> might be approximated as\n\n:<math>\nP^{\\prime\n}(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})=P(X_{6}|X_{5})P(X_{5}|X_{2})P(X_{4}|X_{2})P(X_{3}|X_{2})P(X_{2}|X_{1})P(X_{1})\n</math>\n\nwhere each new term in the product introduces just one new variable, and the product can be represented as a first-order dependency tree, as shown in the figure.  The Chow–Liu algorithm (below) determines which conditional probabilities are to be used in the product approximation.   In general, unless there are no third-order or higher-order interactions, the Chow–Liu approximation is indeed an \'\'approximation\'\', and cannot capture the complete structure of the original distribution.  {{Harvtxt|Pearl|1988}} provides a modern analysis of the Chow–Liu tree as a [[Bayesian network]].\n\n==The Chow–Liu algorithm==\nChow and Liu show how to select second-order terms for the product approximation so that, among all such second-order approximations (first-order dependency trees), the constructed approximation <math>P^{\\prime}</math> has the minimum [[Kullback–Leibler distance]] to the actual distribution <math>P</math>, and is thus the \'\'closest\'\' approximation in the classical [[information theory|information-theoretic]] sense. The Kullback–Leibler distance between a second-order product approximation and the actual distribution is shown to be\n\n:<math>\nD(P\\parallel P^{\\prime })=-\\sum I(X_{i};X_{j(i)})+\\sum\nH(X_{i})-H(X_{1},X_{2},\\ldots ,X_{n})\n</math>\n\nwhere <math>I(X_{i};X_{j(i)})</math> is the [[mutual information]] between variable <math>X_{i}</math> and its parent <math>X_{j(i)}</math> and <math>H(X_{1},X_{2},\\ldots ,X_{n})</math> is the [[joint entropy]] of variable set <math>\\{X_{1},X_{2},\\ldots ,X_{n}\\}</math>.   Since the terms <math>\\sum H(X_{i})</math> and  <math>H(X_{1},X_{2},\\ldots ,X_{n})</math> are independent of the dependency ordering in the tree, only the sum of the pairwise [[mutual information]]s, <math>\\sum I(X_{i};X_{j(i)})</math>, determines the quality of the approximation. Thus, if every branch (edge) on the tree is given a weight corresponding to the mutual information between the variables at its vertices, then the tree which provides the optimal second-order approximation to the target distribution is just the \'\'maximum-weight tree\'\'. The equation above also highlights the role of the dependencies in the approximation: When no dependencies exist, and the first term in the equation is absent, we have only an approximation based on first-order marginals, and the distance between the approximation and the true distribution is due to the redundancies that are not accounted for when the variables are treated as independent. As we specify second-order dependencies, we begin to capture some of that structure and reduce the distance between the two distributions.\n\nChow and Liu provide a simple algorithm for constructing the optimal tree; at each stage of the procedure the algorithm simply adds the maximum [[mutual information]] pair to the tree.  See the original paper, {{Harvtxt|Chow|Liu|1968}}, for full details. A more efficient tree construction algorithm for the common case of sparse data was outlined in {{Harvtxt|Meilă|1999}}.\n\nChow and Wagner proved in a later paper {{Harvtxt|Chow|Wagner|1973}} that the learning of the Chow–Liu tree is consistent given samples (or observations) drawn i.i.d. from a tree-structured distribution. In other words, the probability of learning an incorrect tree decays to zero as the number of samples tends to infinity. The main idea in the proof is the continuity of the mutual information in the pairwise marginal distribution. Recently, the exponential rate of convergence of the error probability was provided.<ref name="Tan">A Large-Deviation Analysis for the Maximum-Likelihood Learning of Tree Structures. V. Y. F. Tan, A. Anandkumar, L. Tong and A. Willsky. In the International symposium on information theory (ISIT), July 2009.</ref>\n\n==Variations on Chow–Liu trees==\nThe obvious problem which occurs when the actual distribution is not in fact a second-order dependency tree can still in some cases be addressed by fusing or aggregating together densely connected subsets of variables to obtain a "large-node" Chow–Liu tree {{Harv|Huang|King|2002}}, or by extending the idea of greedy maximum branch weight selection to non-tree (multiple parent) structures {{Harv|Williamson|2000}}. (Similar techniques of variable substitution and construction are common in the [[Bayes network]] literature, e.g., for dealing with loops.  See {{Harvtxt|Pearl|1988}}.)\n\nGeneralizations of the Chow–Liu tree are the so-called [[t-cherry junction trees]]. It is proved that the t-cherry junction trees provide a better or at least as good approximation for a  discrete multivariate probability distribution as the Chow–Liu tree gives.\nFor the third order t-cherry junction tree see {{Harv|Kovács|Szántai|2010}}, for the \'\'k\'\'th-order t-cherry junction tree see {{Harv|Szántai|Kovács|2010}}. The second order t-cherry junction tree is in fact the Chow–Liu tree.\n\n==See also==\n*[[Bayesian network]]\n*[[Knowledge representation]]\n\n==Notes==\n{{reflist}}\n\n==References==\n{{refbegin|2}}\n*{{Citation\n | last=Chow | first=C. K. | last2=Liu | first2=C.N.\n | title=Approximating discrete probability distributions with dependence trees\n | journal=IEEE Transactions on Information Theory\n | volume=IT-14  | issue=3 | year=1968 | pages=462–467 | url= | doi=10.1109/tit.1968.1054142}}.\n*{{Citation\n | last=Huang | first=Kaizhu  | last2=King | first2=Irwin\n | last3=Lyu | first3=Michael R.  | year= 2002\n | chapter=Constructing a large node Chow–Liu tree based on frequent itemsets\n |editor1=Wang, Lipo |editor2=Rajapakse, Jagath C. |editor3=Fukushima, Kunihiko |editor4=Lee, Soo-Young |editor5=Yao, Xin | title=Proceedings of the 9th International Conference on Neural Information Processing ({ICONIP}\'02)\n | place=[[Singapore]] | url= | accessdate= | pages=498–502}}.\n*{{Citation\n | last=Pearl | first=Judea\n | title=Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference\n | publisher=[[Morgan Kaufmann]] | place=[[San Mateo, CA]] | year=1988}}\n*{{Citation\n | last=Williamson | first=Jon | year= 2000\n | chapter=Approximating discrete probability distributions with Bayesian networks\n | title=Proceedings of the International Conference on Artificial Intelligence in Science and Technology\n | place=[[Tasmania]] | accessdate= | pages=16–20}}.\n*{{Citation\n | last=Meilă | first=Marina | year= 1999\n | chapter=An Accelerated Chow and Liu Algorithm: Fitting Tree Distributions to High-Dimensional Sparse Data\n | title=Proceedings of the Sixteenth International Conference on Machine Learning\n | publisher=Morgan Kaufmann | accessdate= | pages=249–257}}.\n*{{Citation\n | last=Chow | first=C. K. | last2=Wagner | first2=T.\n | title=Consistency of an estimate of tree-dependent probability distribution\n | journal=IEEE Transactions on Information Theory\n | volume=IT-19  | issue=3 | year=1973 | pages=369–371 | doi=10.1109/tit.1973.1055013}}.\n*{{Citation\n | last=Kovács | first=E. | last2=Szántai | first2=T.\n | title=On the approximation of a discrete multivariate probability distribution using the new concept of t-cherry junction tree\n | journal=Lecture Notes in Economics and Mathematical Systems\n | volume=633, Part 1 | year=2010 | pages= 39–56 | doi=10.1007/978-3-642-03735-1_3}}.\n*{{Citation\n | last=Szántai | first=T. | last2=Kovács | first2=E.\n | title=Hypergraphs as a mean of discovering the dependence structure of a discrete multivariate probability distribution\n | journal=Annals of Operations Research | year=2010 | pages= }}.\n{{refend}}\n\n{{DEFAULTSORT:Chow-Liu tree}}\n[[Category:Knowledge representation]]']
['Knowledge representation and reasoning', '16920', '\'\'\'Knowledge representation and reasoning\'\'\' (\'\'\'KR\'\'\') is the field of [[artificial intelligence]] (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a [[natural language]]. Knowledge representation incorporates findings from psychology{{citation needed|date=February 2016}} about how humans solve problems and represent knowledge in order to design [[Formalism (mathematics)|formalisms]] that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from [[logic]] to automate various kinds of \'\'reasoning\'\', such as the application of rules or the relations of [[Set theory|sets]] and [[subset]]s.\n\nExamples of knowledge representation formalisms include [[Semantic network|semantic nets]], [[systems architecture]], [[Frame (artificial intelligence)|Frames]], Rules, and [[Ontology (information science)|ontologies]]. Examples of [[automated reasoning]] engines include [[inference engine]]s, [[automated theorem proving|theorem prover]]s, and classifiers.\n\n== History ==\n\nThe earliest work in computerized knowledge representation was focused on general problem solvers such as the [[General Problem Solver]] (GPS) system developed by [[Allen Newell]] and [[Herbert A. Simon]] in 1959. These systems featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal.\n\nIn these early days of AI, general search algorithms such as [[A*]] were also developed.  However, the amorphous problem definitions for systems such as GPS meant that they worked only for very constrained toy domains (e.g. the "[[blocks world]]"). In order to tackle non-toy problems, AI researchers such as [[Ed Feigenbaum]] and [[Rick Hayes-Roth|Frederick Hayes-Roth]] realized that it was necessary to focus systems on more constrained problems.\n\nIt was the failure of these efforts that led to the [[cognitive revolution]] in psychology and to the phase of AI focused on knowledge representation that resulted in [[expert systems]] in the 1970s and 80s, [[Production system (computer science)|production systems]], [[frame language]]s, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.\n\nExpert systems gave us the terminology still in use today where AI systems are divided into a Knowledge Base with facts about the world and rules and an inference engine that applies the rules to the [[knowledge base]] in order to answer questions and solve problems. In these early systems the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.<ref>{{cite book|last=Hayes-Roth|first=Frederick|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|author2=Donald Waterman |author3=Douglas Lenat }}</ref>\n\nIn addition to expert systems, other researchers developed the concept of [[Frame language|Frame based languages]] in the mid 1980s. A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. [[natural language understanding|understanding natural language]] and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.\n\nIt wasn\'t long before the frame communities and the rule-based researchers realized that there was synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined Frames and Rules. One of the most powerful and well known was the 1983 [[Knowledge Engineering Environment]] (KEE) from [[IntelliCorp (software)|Intellicorp]]. KEE had a complete rule engine with [[forward chaining|forward]] and [[backward chaining]]. It also had a complete frame based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from [[Symbolics]], [[Xerox]], and [[Texas Instruments]].<ref>{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}</ref>\n\nThe integration of Frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time as this was occurring, there was another strain of research which was less commercially focused and was driven by mathematical logic and automated theorem proving.  One of the most influential languages in this research was the [[KL-ONE]] language of the mid 80\'s. KL-ONE was a [[frame language]] that had a rigorous semantics, formal definitions for concepts such as an [[Is-a|Is-A relation]].<ref>{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}</ref> KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn\'t formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).<ref>{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=87683&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}</ref>\n\nAnother area of knowledge representation research was the problem of common sense reasoning.  One of the first realizations from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent.  Basic principles of common sense physics, causality, intentions, etc. An example is the [[Frame problem]], that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world it is essential to represent this kind of knowledge. One of the most ambitious programs to tackle this problem was Doug Lenat\'s [[Cyc]] project. Cyc established its own Frame language and had large numbers of analysts document various areas of common sense reasoning in that language. The knowledge recorded in Cyc included common sense models of time, causality, physics, intentions, and many others.<ref>{{cite book|last=Lenat|first=Doug|title=Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project|publisher=Addison-Wesley|isbn=978-0201517521|author2=R. V. Guha |date=January 1990}}</ref>\n\nThe starting point for knowledge representation is the \'\'knowledge representation hypothesis\'\' first formalized by [[Brian Cantwell Smith|Brian C. Smith]] in 1985:<ref>{{cite book|last=Smith|first=Brian C.|title=Readings in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages=31–40|editor=Ronald Brachman and Hector J. Levesque|chapter=Prologue to Reflections and Semantics in a Procedural Language}}</ref>\n\n<blockquote>\'\'Any mechanically embodied intelligent process will be {{sic|comprised |hide=y|of}} structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge.\'\'</blockquote>\n\nCurrently one of the most active areas of knowledge representation research are projects associated with the [[Semantic web]]. The semantic web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the semantic web creates large [[ontologies]] of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future semantic web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.\n\nRecent projects funded primarily by the [[Defense Advanced Research Projects Agency]] (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The [[Resource Description Framework]] (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The [[Web Ontology Language]] (OWL) provides additional levels of semantics and enables integration with classification engines.<ref>{{cite journal|last=Berners-Lee|first=Tim |author2=James Hendler |author3=Ora Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34–43}}</ref><ref>{{cite web|url=http://www.w3.org/2001/sw/BestPractices/SE/ODSD/|title=A Semantic Web Primer for Object-Oriented Software Developers|last1=Knublauch|first1=Holger|last2=Oberle|first2=Daniel|last3=Tetlow|first3=Phil|last4=Wallace|first4=Evan|publisher=[[W3C]]|date=2006-03-09|accessdate=2008-07-30}}</ref>\n\n== Overview ==\nKnowledge-representation is the field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems. The justification for knowledge representation is that conventional [[procedural code]] is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in [[expert systems]].\n\nFor example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.\n\nKnowledge representation goes hand in hand with [[automated reasoning]] because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all [[knowledge representation language]]s have a reasoning or inference engine as part of the system.<ref>{{cite book|last=Hayes-Roth|first=Frederick|pages=6–7|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|author2=Donald Waterman |author3=Douglas Lenat }}</ref>\n\nA key trade-off in the design of a knowledge representation formalism is that between expressivity and practicality. The ultimate knowledge representation formalism in terms of expressive power and compactness is First Order Logic (FOL).  There is no more powerful formalism than that used by mathematicians to define general propositions about the world. However, FOL has two drawbacks as a knowledge representation formalism:  ease of use and practicality of implementation.  First order logic can be intimidating even for many software developers. Languages which do not have the complete formal power of FOL can still provide close to the same expressive power with a user interface that is more practical for the average developer to understand. The issue of practicality of implementation is that FOL in some ways is too expressive. With FOL it is possible to create statements (e.g. quantification over infinite sets) that would cause a system to never terminate if it attempted to verify them.\n\nThus, a subset of FOL can be both easier to use and more practical to implement. This was a driving motivation behind rule-based expert systems. IF-THEN rules provide a subset of FOL but a very useful one that is also very intuitive.  The history of most of the early AI knowledge representation formalisms; from databases to semantic nets to theorem provers and production systems can be viewed as various design decisions on whether to emphasize expressive power or computability and efficiency.<ref>{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|page=49|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning|quote=The good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable.}}</ref>\n\nIn a key 1993 paper on the topic, Randall Davis of [[Massachusetts Institute of Technology|MIT]] outlined five distinct roles to analyze a knowledge representation framework:<ref>{{cite journal|last=Davis|first=Randall|author2=Howard Shrobe |author3=Peter Szolovits |title=What Is a Knowledge Representation?|journal=AI Magazine|date=Spring 1993|volume=14|issue=1|pages=17–33|url=http://www.aaai.org/ojs/index.php/aimagazine/article/view/1029/947}}</ref> \n* A knowledge representation (KR) is most fundamentally a surrogate, a substitute for the thing itself, used to enable an entity to determine consequences by thinking rather than acting, i.e., by reasoning about the world rather than taking action in it.\n* It is a set of ontological commitments, i.e., an answer to the question: In what terms should I think about the world?\n* It is a fragmentary theory of intelligent reasoning, expressed in terms of three components: (i) the representation\'s fundamental conception of intelligent reasoning; (ii) the set of inferences the representation sanctions; and (iii) the set of inferences it recommends.\n* It is a medium for pragmatically efficient computation, i.e., the computational environment in which thinking is accomplished. One contribution to this pragmatic efficiency is supplied by the guidance a representation provides for organizing information so as to facilitate making the recommended inferences.\n* It is a medium of human expression, i.e., a language in which we say things about the world."\n\nKnowledge representation and reasoning are a key enabling technology for the [[Semantic web]]. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today it will be possible to define logical queries and find pages that map to those queries.<ref>{{cite journal|last=Berners-Lee|first=Tim |author2=James Hendler |author3=Ora Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34–43}}</ref> The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the [[Subsumption relation|subsumption]] relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever changing and evolving information space of the Internet.<ref>{{cite web|last=Macgregor|first=Robert|title=Retrospective on Loom|url=http://www.isi.edu/isd/LOOM/papers/macgregor/Loom_Retrospective.html|work=isi.edu|publisher=Information Sciences Institute|accessdate=10 December 2013|date=August 13, 1999}}</ref>\n\nThe Semantic web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The [[Resource Description Framework]] (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The [[Web Ontology Language]] (OWL) adds additional semantics and integrates with automatic classification reasoners.<ref>{{cite web|url=http://www.w3.org/2001/sw/BestPractices/SE/ODSD/|title=A Semantic Web Primer for Object-Oriented Software Developers|last1=Knublauch|first1=Holger|last2=Oberle|first2=Daniel|last3=Tetlow|first3=Phil|last4=Wallace|first4=Evan|publisher=[[W3C]]|date=2006-03-09|accessdate=2008-07-30}}</ref>\n\n== Characteristics ==\nIn 1985, Ron Brachman categorized the core issues for knowledge representation as follows:<ref>{{cite book|last=Brachman|first=Ron|title=Readings in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages=XVI-XVII|editor=Ronald Brachman and Hector J. Levesque|chapter=Introduction}}</ref> \n*Primitives. What is the underlying framework used to represent knowledge? [[Semantic network]]s were one of the first knowledge representation primitives. Also, data structures and algorithms for general fast search. In this area there is a strong overlap with research in data structures and algorithms in computer science. In early systems the Lisp programming language which was modeled after the [[lambda calculus]] was often used as a form of functional knowledge representation. Frames and Rules were the next kind of primitive. Frame languages had various mechanisms for expressing and enforcing constraints on frame data. All data in frames are stored in slots. Slots are analogous to relations in entity-relation modeling and to object properties in object-oriented modeling. Another technique for primitives is to define languages that are modeled after [[First Order Logic]] (FOL). The most well known example is Prolog but there are also many special purpose theorem proving environments. These environments can validate logical models and can deduce new theories from existing models. Essentially they automate the process a logician would go through in analyzing a model. Theorem proving technology had some specific practical applications in the areas of software engineering. For example, it is possible to prove that a software program rigidly adheres to a formal logical specification.\n*Meta-Representation.  This is also known as the issue of [[Reflection (computer programming)|reflection]] in computer science. It refers to the capability of a formalism to have access to information about its own state. An example would be the meta-object protocol in [[Smalltalk]] and [[CLOS]] that gives developers run time access to the class objects and enables them to dynamically redefine the structure of the knowledge base even at run time. Meta-representation means the knowledge representation language is itself expressed in that language. For example, in most Frame based environments all frames would be instances of a frame class. That class object can be inspected at run time so that the object can understand and even change its internal structure or the structure of other parts of the model. In rule-based environments the rules were also usually instances of rule classes. Part of the meta protocol for rules were the meta rules that prioritized rule firing. \n*[[Completeness (logic)|Incompleteness]]. Traditional logic requires additional axioms and constraints to deal with the real world as opposed to the world of mathematics. Also, it is often useful to associate degrees of confidence with a statement. I.e., not simply say "Socrates is Human" but rather "Socrates is Human with confidence 50%". This was one of the early innovations from [[expert system]]s research which migrated to some commercial tools, the ability to associate certainty factors with rules and conclusions. Later research in this area is known as [[Fuzzy Logic]].<ref>{{cite journal|last=Bih|first=Joseph|title=Paradigm Shift: An Introduction to Fuzzy Logic|journal=IEEE POTENTIALS|year=2006|url=http://www.cse.unr.edu/~bebis/CS365/Papers/FuzzyLogic.pdf|accessdate=24 December 2013}}</ref> \n*Definitions and [[Universals]] vs. facts and defaults.  Universals are general statements about the world such as "All humans are mortal". Facts are specific examples of universals such as "Socrates is a human and therefore mortal". In logical terms definitions and universals are about universal quantification while facts and defaults are about existential quantifications. All forms of knowledge representation must deal with this aspect and most do so with some variant of set theory, modeling universals as sets and subsets and definitions as elements in those sets. \n*[[Non-monotonic logic|Non-Monotonic reasoning]]. Non-monotonic reasoning allows various kinds of hypothetical reasoning. The system associates facts asserted with the rules and facts used to justify them and as those facts change updates the dependent knowledge as well. In rule based systems this capability is known as a [[truth maintenance system]].<ref>{{cite journal|last=Zlatarva|first=Nellie|title=Truth Maintenance Systems and their Application for Verifying Expert System Knowledge Bases|journal=Artificial Intelligence Review|year=1992|volume=6|pages=67–110|url=http://link.springer.com/article/10.1007%2FBF00155580#page-2|accessdate=25 December 2013|doi=10.1007/bf00155580}}</ref> \n*[[Functional completeness|Expressive Adequacy]]. The standard that Brachman and most AI researchers use to measure expressive adequacy is usually First Order Logic (FOL). Theoretical limitations mean that a full implementation of FOL is not practical. Researchers should be clear about how expressive (how much of full FOL expressive power) they intend their representation to be.<ref>{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages = 41–70|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning}}</ref>\n*Reasoning Efficiency. This refers to the run time efficiency of the system. The ability of the knowledge base to be updated and the reasoner to develop new inferences in a reasonable period of time. In some ways this is the flip side of expressive adequacy. In general the more powerful a representation, the more it has expressive adequacy, the less efficient its [[automated reasoning]] engine will be. Efficiency was often an issue, especially for early applications of knowledge representation technology. They were usually implemented in interpreted environments such as Lisp which were slow compared to more traditional platforms of the time.\n\n== Ontology engineering ==\n{{main article|Ontology engineering|Ontology language}}\n\nIn the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic but medical diagnosis of certain kinds of diseases.\n\nAs knowledge-based technology scaled up the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common sense knowledge. In designing an artificial intelligence agent it was soon realized that representing common sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as [[CycL]].\n\nAfter CycL, a number of [[ontology language]]s have been developed.  Most are [[declarative language]]s, and are either [[frame language]]s, or are based on [[first-order logic]]. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated by Tom Gruber, "Every ontology is a treaty- a social agreement among people with common motive in sharing." There are always many competing and differing views that make any general purpose ontology impossible. A general purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.<ref>Russell, Stuart J.; Norvig, Peter (2010), Artificial Intelligence: A Modern Approach (3rd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-604259-7, p. 437-439</ref>\n\nThere is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,<ref>Hayes P, Naive physics I: Ontology for liquids. University of Essex report, 1978, Essex, UK.</ref> the lumped element model widely used in representing electronic circuits (e.g.,<ref>Davis R, Shrobe H E, Representing Structure and Behavior of Digital Hardware, IEEE Computer, Special Issue on Knowledge Representation, 16(10):75-82.</ref>), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.\nThe lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.\n\nOntologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.\n\nThe commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., [[MYCIN]]) looks substantially different from the same task viewed in terms of frames (e.g., [[INTERNIST]]). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.\n\n== See also ==\n* [[Chunking (psychology)]]\n* [[Commonsense knowledge base]]\n* [[Personal knowledge base]]\n* [[Valuation-based system]]\n* [[Conceptual Graph]]\n\n== References ==\n\n<references/>\n\n== Further reading ==\n* [[Ronald J. Brachman]]; [http://citeseer.nj.nec.com/context/177306/0 What IS-A is and isn\'t. An Analysis of Taxonomic Links in Semantic Networks]; IEEE Computer, 16 (10); October 1983\n* [[Ronald J. Brachman]], [[Hector J. Levesque]] \'\'Knowledge Representation and Reasoning\'\', Morgan Kaufmann, 2004 ISBN 978-1-55860-932-7\n* [[Ronald J. Brachman]], [[Hector J. Levesque]] (eds) \'\'Readings in Knowledge Representation\'\', Morgan Kaufmann, 1985, ISBN 0-934613-01-X\n* Chein, M., Mugnier, M.-L. (2009),\'\'[http://www.lirmm.fr/gbkrbook/ Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs]\'\', Springer, 2009,ISBN 978-1-84800-285-2.\n* Randall Davis, Howard Shrobe, and Peter Szolovits; [http://citeseer.ist.psu.edu/davis93what.html What Is a Knowledge Representation?] AI Magazine, 14(1):17-33,1993\n* [[Ronald Fagin]], [[Joseph Y. Halpern]], [[Yoram Moses]], [[Moshe Y. Vardi]] \'\'Reasoning About Knowledge\'\', MIT Press, 1995, ISBN 0-262-06162-7\n* Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: [http://www.informatik.uni-trier.de/~ley/db/conf/er/HainautHEHR96.html Understanding Implementations of IS-A Relations]. ER 1996: 42-57\n* Hermann Helbig: \'\'Knowledge Representation and the Semantics of Natural Language\'\', Springer, Berlin, Heidelberg, New York 2006\n* Arthur B. Markman: \'\'Knowledge Representation\'\'  Lawrence Erlbaum Associates, 1998\n* [[John F. Sowa]]: \'\'Knowledge Representation\'\': Logical, Philosophical, and Computational Foundations. Brooks/Cole: New York, 2000\n* Adrian Walker, Michael McCord, [[John F. Sowa]], and Walter G. Wilson: \'\'Knowledge Systems and Prolog\'\', Second Edition, Addison-Wesley, 1990\n\n== External links ==\n* [http://medg.lcs.mit.edu/ftp/psz/k-rep.html What is a Knowledge Representation?] by Randall Davis and others\n* [http://www.makhfi.com/KCM_intro.htm Introduction to Knowledge Modeling] by Pejman Makhfi\n* [http://www.inf.unibz.it/~franconi/dl/course/ Introduction to Description Logics course] by Enrico Franconi, Faculty of Computer Science, Free University of Bolzano, Italy\n* [http://www.ccl.kuleuven.ac.be/LKR/html/datr.html DATR Lexical knowledge representation language]\n* [http://www.isi.edu/isd/LOOM/LOOM-HOME.html Loom Project Home Page]\n* [http://www.research.att.com/sw/tools/classic/tm/ijcai-95-with-scenario.html Description Logic in Practice: A CLASSIC Application]\n* [http://www.dfki.uni-kl.de/ruleml/ The Rule Markup Initiative]\n* [http://nelements.org Nelements KOS] - a non-free 3d knowledge representation system\n\n{{Computer science}}\n{{computable knowledge}}\n{{Commons category|Knowledge representation}}\n\n{{DEFAULTSORT:Knowledge Representation And Reasoning}}\n[[Category:Knowledge representation| ]]\n[[Category:Scientific modeling]]\n[[Category:Programming paradigms]]\n[[Category:Reasoning]]']
['HiLog', '34697893', '\'\'\'HiLog\'\'\' is a programming [[logic]] with higher-order syntax, which allows arbitrary terms to appear in predicate and function positions. However, the [[model theory]] of HiLog is first-order. Although syntactically HiLog strictly extends [[first order logic]], HiLog can be embedded into this logic.\n\nHiLog is described in detail in\n<ref name="hilog-jlp">\nW. Chen, M. Kifer and D.S. Warren (1993), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.7860 \'\'HiLog: A Foundation for Higher-Order Logic Programming\'\']. Journal of Logic Programming, 1993.\n</ref>\n.<ref>\nW. Chen, M. Kifer and D.S. Warren (1989), [http://citeseerx.ist.psu.edu/showciting?cid=2016805 \'\'HiLog: A first order semantics for higher-order logic programming constructs\'\']. Proc. North American Logic Programming Conference, 1989.\n</ref> \nIt was later extended in the direction of [[many-sorted logic]] in.<ref>\nW. Chen and M. Kifer (1994), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.4332 \'\'Sorted HiLog: Sorts in Higher-Order Logic Data Languages\'\']. Int’l Conference on Database Theory, Springer Lecture Notes in Computer Science #893.\n</ref>\nOther contributions to the theory of HiLog include\n<ref>\nK.A. Ross (1994), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2148 \'\'On Negation in HiLog\'\']. Journal of Logic Programming, 1994.\n</ref>\n.<ref>\nJ. de Bruijn and S. Heymans (2008), [http://www.kr.tuwien.ac.at/staff/bruijn/priv/publications/frames-predicates-fi.pdf \'\'On the Relationship between Description Logic-based and F-Logic-based Ontologies\'\']. Fundamenta Informaticae 82:3, 2008, pp. 213-236.\n</ref>\n\nThe [[XSB|XSB System]] parses HiLog syntax, but the integration of HiLog into XSB is only partial. In particular, HiLog is not integrated with the XSB module system. A full implementation of HiLog is available in the [[Flora-2|Flora-2 system]].\n\nIn,<ref name="hilog-jlp"/> it has been shown that HiLog can be embedded into [[first-order logic]] through a fairly simple transformation. For instance, <tt>p(X)(Y,Z(V)(W))</tt> gets embedded as the following first-order term:\n\n  apply(p(X),Y,apply(apply(Z,V),W))\n\nDetails can be found in.<ref name="hilog-jlp"/>\n\nThe [[Rule Interchange Format#FLD|Framework for Logic-Based Dialects]] (RIF-FLD) of the [[Rule Interchange Format]] (RIF) is largely based on the ideas underlying HiLog and [[F-logic]].\n\n== Examples ==\n\nIn all the examples, below, capitalized symbols denote variables and the comma denotes [[logical conjunction]], as in most [[logic programming]] languages. The first and the second examples show that variables can appear in predicate positions. Predicates can even be complex terms, such as <tt>closure(P)</tt> or <tt>maplist(F)</tt> below. The third example shows that variables can also appear in place of atomic formulas, while the fourth example illustrates the use of variables in place of function symbols. The first example defines a generic transitive closure operator, which can be applied to an arbitrary binary predicate. The second example is similar. It defines a [[LISP]]-like mapping operator, which applies to an arbitrary binary predicate. The third example shows that the [[Prolog]] meta-predicate <tt>call/1</tt> can be expressed in HiLog in a natural way and without the use of extra-logical features. The last example defines a predicate that traverses arbitrary binary trees represented as [[Term (logic)|first-order term]]s.\n<source lang="prolog">\n  closure(P)(X,Y) <- P(X,Y).\n  closure(P)(X,Y) <- P(X,Z), closure(P)(Z,Y).\n\n  maplist(F)([],[]).\n  maplist(F)([X|R],[Y|Z]) <- F(X,Y), maplist(F)(R,Z).\n\n  call(X) <- X.\n\n  traverse(X(L,R)) <- traverse(L), traverse(R).\n</source>\n\n==References==\n{{reflist}}\n\n[[Category:Logic programming languages]]\n[[Category:Declarative programming languages]]\n[[Category:Knowledge representation]]']
['KL-ONE', '17188', '{{distinguish | KL1 }}\n\n\'\'\'KL-ONE\'\'\' (pronounced "kay ell won") is a well known [[knowledge representation]] system in the tradition of [[semantic networks]] and [[Frame (Artificial intelligence) | frames]]; that is, it is a [[frame language]]. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network.<ref>{{Cite journal | last1 = Woods | first1 = W. A. | authorlink1 = William Aaron Woods| last2 = Schmolze | first2 = J. G. | doi = 10.1016/0898-1221(92)90139-9 | title = The KL-ONE family | journal = Computers & Mathematics with Applications | volume = 23 | issue = 2–5 | pages = 133 | year = 1992 | pmid =  | pmc = }}</ref><ref>{{Cite journal | last1 = Brachman | first1 = R. J. | authorlink1 = Ronald J. Brachman| last2 = Schmolze | first2 = J. G. | doi = 10.1207/s15516709cog0902_1 | title = An Overview of the KL-ONE Knowledge Representation System | journal = Cognitive Science | volume = 9 | issue = 2 | pages = 171 | year = 1985 | pmid =  | pmc = }}</ref><ref>{{cite book | last = D.A. Duce | first = G.A. Ringland  | title = Approaches to Knowledge Representation, An Introduction | year = 1988 | publisher = Research Studies Press, Ltd. | isbn = 0-86380-064-5 }}</ref>\n\nThere is a whole family of KL-ONE-like systems. One of the innovations that KL-ONE initiated was the use of a [[deductive classifier]], an automated reasoning engine that can validate a frame ontology and deduce new information about the ontology based on the initial information provided by a domain expert. \n\nFrames in KL-ONE are called [[concepts]]. These form hierarchies using subsume-relations; in the KL-ONE terminology a [[superclass (computer science)|super class]] is said to subsume its [[Subclass (computer science) | subclasses]]. \n[[Multiple inheritance]] is allowed. Actually a concept is said to be well-formed only if it inherits from more than one other concept. All concepts, except the top concept (usually THING), must have at least one super class. \n\nIn KL-ONE descriptions are separated into two basic classes of concepts: primitive and defined. Primitives are domain concepts that are not fully defined. This means that given all the properties of a concept, this is not sufficient to classify it. They may also be viewed as incomplete definitions. Using the same view, defined concepts are complete definitions. Given the properties of a concept, these are [[necessary and sufficient]] conditions to classify the concept.\n\nThe slot-concept is called roles and the values of the roles are role-fillers. There are several different types of roles to be used in different situations. The most common and important role type is the generic RoleSet that captures the fact that the role may be filled with more than one filler.\n\n==See also==\n* [[Ontology language]]\n\n==References==\n{{reflist}}\n\n\n{{FOLDOC}}\n\n[[Category:Artificial intelligence]]\n[[Category:Knowledge representation]]\n[[Category:Ontology languages]]']
['Deductive classifier', '43342432', "A '''deductive classifier''' is a type of [[artificial intelligence]] [[inference engine]]. It takes as input a set of declarations in a [[frame language]] about a domain such as medical research or molecular biology. For example, the names of [[Class hierarchy|classes, sub-classes]], properties, and restrictions on allowable values.  The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to [[Automated theorem proving|theorem provers]] in that they take as input and produce output via [[First Order Logic]]. Classifiers originated with [[KL-ONE]] [[Frame language]]s. They are increasingly significant now that they form a part in the enabling technology of the [[Semantic Web]]. Modern classifiers leverage the [[Web Ontology Language]]. The models they analyze and generate are called [[Ontologies (computer science)|ontologies]].<ref>{{cite journal|last=Berners-Lee|first=Tim|first2=James|last2=Hendler|first3=Ora|last3=Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34–43}}</ref>\n\n== History ==\nA classic problem in [[knowledge representation]] for artificial intelligence is the trade off between the expressive power and the computational efficiency of the knowledge representation system. The most powerful form of knowledge representation is First Order Logic (FOL). However, it is not possible to implement knowledge representation that provides the complete expressive power of first order logic. Such a representation will include the capability to represent concepts such as the set of all integers which are impossible to iterate through. Implementing an assertion quantified for an infinite set by definition results in an undecidable non-terminating program. However, the problem is deeper than not being able to implement infinite sets. As Levesque demonstrated, the closer a knowledge representation mechanism comes to FOL, the more likely it is to result in expressions that require infinite or unacceptably large resources to compute.<ref>{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|page=49|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning|quote=The good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable.}}</ref>\n\nAs a result of this trade-off, a great deal of early work on knowledge representation for artificial intelligence involved experimenting with various compromises that provide a subset of FOL with acceptable computation speeds. One of the first and most successful compromises was to develop languages based predominately on [[modus ponens]], i.e. IF-THEN rules. [[Rule-based systems]] were the predominate knowledge representation mechanism for virtually all early [[expert systems]]. Rule-based systems provided acceptable computational efficiency while still providing powerful knowledge representation. Also, rules were highly intuitive to knowledge workers. Indeed, one of the data points that encouraged researchers to develop rule-based knowledge representation was psychological research that humans often represented complex logic via rules.<ref>{{cite book|last=Hayes-Roth|first=Frederick|pages=6–7|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|first2=Donald|last2=Waterman|first3=Douglas|last3=Lenat}}</ref>\n\nHowever, after the early success of rule-based systems there arose more pervasive use of frame languages instead of or more often combined with rules. Frames provided a more natural way to represent certain types of concepts, especially concepts in subpart or subclass hierarchies. This led to development of a new kind of inference engine known as a classifier. A classifier could analyze a class hierarchy (also known as an [[Ontology (computer science)|ontology]]) and determine if it was valid. If the hierarchy was invalid the classifier would highlight the inconsistent declarations. For a language to utilize a classifier it required a formal foundation. The first language to successfully demonstrate a classifier was the KL-ONE family of languages. The [[Loom (ontology)|LOOM language]] from ISI was heavily influenced by KL-ONE.  LOOM also was influenced by the rising popularity of object-oriented tools and environments. Loom provided a true object-oriented capability (e.g. message passing) in addition to Frame language capabilities. Classifiers play a significant role in the vision for the next generation Internet known as the Semantic Web. The Web Ontology Language provides a formalism that can be validated and reasoned on via classifiers such as Hermit and Fact++.<ref>{{cite journal|last1=MacGregor|first1=Robert|title=A Descriptive Classifier for the Predicate Calculus|journal=AAAI - 94 Proceedings|date=1994|url=http://www.aaai.org/Papers/AAAI/1994/AAAI94-033.pdf|accessdate=17 July 2014}}</ref>\n\n== Implementations ==\n[[File:Protégé 3.4.3.png|500px|thumbnail|right|Protege Ontology Editor]]\nThe earliest versions of classifiers were [[Automated theorem proving|logic theorem provers]]. The first classifier to work with a [[Frame language]] was the [[KL-ONE]] classifier.<ref>{{Cite journal | last1 = Woods | first1 = W. A. | authorlink1 = William Aaron Woods| last2 = Schmolze | first2 = J. G. | doi = 10.1016/0898-1221(92)90139-9 | title = The KL-ONE family | journal = Computers & Mathematics with Applications | volume = 23 | issue = 2–5 | pages = 133–177 | year = 1992 | pmid =  | pmc = }}</ref><ref>{{Cite journal | last1 = Brachman | first1 = R. J. | authorlink1 = Ronald J. Brachman| last2 = Schmolze | first2 = J. G. | doi = 10.1207/s15516709cog0902_1 | title = An Overview of the KL-ONE Knowledge Representation System | journal = Cognitive Science | volume = 9 | issue = 2 | pages = 171–216 | year = 1985 | pmid =  | pmc = }}</ref> A later system built on common lisp was LOOM from the Information Sciences Institute. LOOM provided true object-oriented capabilities leveraging the Common Lisp Object System, along with a frame language.<ref>{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=87683&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}</ref> In the Semantic Web the [[Protégé (software)|Protege]] tool from [[Stanford University|Stanford]] provides classifiers (also known as reasonsers) as part of the default environment.<ref>{{cite web|title=Protege Wiki: Reasoners that integrate with Protege|url=http://protegewiki.stanford.edu/wiki/Reasoning|publisher=Stanford University|accessdate=19 July 2014}}</ref>\n\n== External links ==\n* [http://owl.man.ac.uk/factplusplus/ Fact++ Reasoner]\n* [http://hermit-reasoner.com/ HermiT Reasoner]\n* [http://protege.stanford.edu/ Protege Ontology Editor]\n\n==References==\n{{Reflist}}\n\n[[Category:Artificial intelligence]]\n[[Category:Knowledge representation]]\n[[Category:Ontology languages]]\n[[Category:Classification algorithms]]"]
['Flint toolkit', '43763321', '{{COI|date=September 2014}}\nFlint is a software toolkit which supports various modes of uncertainty handling, namely, [[Fuzzy_logic| Fuzzy]], [[Bayesian_inference| Bayesian]] and [[Expert_system#Certainty_factors| Certainty Theory]].\n\nAlong with [[Flex_expert_system| Flex]], Flint was licensed to the Open University as part of T396: \'Artificial intelligence for technology\'.\n\nMuch of the material for this course is described by Prof Adrian Hopgood in his book: Intelligent Systems for Engineers and Scientists, Third Edition, and on his web-site.<ref name = "AI Toolkit">{{citation |url=http://www.adrianhopgood.com/aitoolkit/aitoolkit.shtml | title=AI toolkit: support site for Intelligent Systems for Engineers and Scientists}} by Adrian Hopgood </ref>\n\nFlint is produced by [[Logic Programming Associates|LPA]] and runs on PCs and web servers.\n\n==External links==\n*[http://www.generation5.org/content/2003/lpainference.asp The Shape of Inference] by Clive Spenser & Charles Langley, Generation5\n*[http://www.generation5.org/content/2003/probmodulation.asp Probability Modulation and Non-linearity in Bayesian Networks] by Clive Spenser & Charles Langley, Generation5\n*[http://www.generation5.org/content/2004/defuzz.asp Defuzzification Options in Flex] by Clive Spenser & Charles Langley, Generation5\n*[http://www.degruyter.com/view/j/pomr.2007.14.issue-3/v10012-007-0012-2/v10012-007-0012-2.xml Application of fuzzy inference to assessment of degree of hazard to ship power plant operator] by Tomasz Kowalewski, Antoni Podsiadło & Wiesław Tarełko\n*[http://www.slaai.lk/proc/2007/2007.pdf#page=42 Computational Modeling in Conceptual Models: Widening Scope of Artificial Life] by Mendis, Asoka. S. Karunananda, Samaratunga\n*[http://www.lpa.co.uk/fln.htm Flint Overview], LPA\n\n== References ==\n{{Reflist}}\n\n[[Category:Expert_systems]]\n[[Category:Knowledge engineering]]\n[[Category:Knowledge representation]]\n\n\n{{compu-ai-stub}}']
['Argument map', '6190251', '[[File:Argument Map.png|thumb|A schematic argument map showing a contention (or conclusion), supporting arguments and objections, and an inference objection.]]\nIn [[informal logic]] and [[philosophy]], an \'\'\'argument map\'\'\' or \'\'\'argument diagram\'\'\' is a visual representation of the structure of an [[argument]]. An argument map typically includes the key components of the argument, traditionally called the \'\'[[Logical consequence|conclusion]]\'\' and the \'\'[[premise]]s\'\', also called \'\'[[Main contention|contention]]\'\' and \'\'[[Reason (argument)|reason]]s\'\'.<ref>{{harvnb|Freeman|1991|pp=49–90}}</ref> Argument maps can also show [[co-premise]]s, [[Objection (argument)|objection]]s, [[counterargument]]s, [[rebuttal]]s, and [[Lemma (logic)|lemma]]s. There are different styles of argument map but they are often functionally equivalent and represent an argument\'s individual claims and the relationships between them.\n\nArgument maps are commonly used in the context of teaching and applying [[critical thinking]].<ref>For example: {{harvnb|Davies|2012}}; {{harvnb|Facione|2013|p=86}}; {{harvnb|Fisher|2004}}; {{harvnb|Kelley|2014|p=73}}; {{harvnb|Kunsch|Schnarr|van Tyle|2014}}; {{harvnb|Walton|2013|p=10}}</ref> The purpose of mapping is to uncover the logical structure of arguments, identify unstated assumptions, evaluate the support an argument offers for a conclusion, and aid understanding of debates. Argument maps are often designed to support deliberation of issues, ideas and arguments in [[wicked problem]]s.<ref>For example: {{harvnb|Culmsee|Awati|2013}}; {{harvnb|Hoffmann|Borenstein|2013}}; {{harvnb|Metcalfe|Sastrowardoyo|2013}}; Ricky Ohl, [//books.google.com/books?id=loa4BAAAQBAJ&pg=PA360 "Computer supported argument visualisation: modelling in consultative democracy around wicked problems"], in {{harvnb|Okada|Buckingham Shum|Sherborne|2014|pp=361–380}}</ref>\n\nAn argument map is not to be confused with a [[concept map]] or a [[mind map]], which are less strict in relating claims.\n\n==Key features of an argument map==\nA number of different kinds of argument map have been proposed but the most common, which Chris Reed and Glenn Rowe called the \'\'standard diagram\'\',<ref name="ReedRowe64">{{harvnb|Reed|Rowe|2007|p=64}}</ref> consists of a [[tree structure]] with each of the reasons leading to the conclusion. There is no consensus as to whether the conclusion should be at the top of the tree with the reasons leading up to it or whether it should be at the bottom with the reasons leading down to it.<ref name="ReedRowe64"/> Another variation diagrams an argument from left to right.<ref>For example: {{harvnb|Walton|2013|pp=18–20}}</ref>\n\nAccording to [[Doug Walton]] and colleagues, an argument map has two basic components: "One component is a set of circled numbers arrayed as points. Each number represents a proposition (premise or conclusion) in the argument being diagrammed. The other component is a set of lines or arrows joining the points. Each line (arrow) represents an inference. The whole network of points and lines represents a kind of overview of the reasoning in the given argument..."<ref>{{harvnb|Reed|Walton|Macagno|2007|p=2}}</ref> With the introduction of software for producing argument maps, it has become common for argument maps to consist of boxes containing the actual propositions rather than numbers referencing those propositions.\n\nThere is disagreement on the terminology to be used when describing argument maps,<ref>{{harvnb|Freeman|1991|pp=49–90}}; {{harvnb|Reed|Rowe|2007}}</ref> but the \'\'standard diagram\'\' contains the following structures:\n\n\'\'\'Dependent premises\'\'\' or \'\'\'co-premises\'\'\', where at least one of the joined premises requires another premise before it can give support to the conclusion: An argument with this structure has been called a \'\'linked\'\' argument.<ref>{{harvnb|Harrell|2010|p=19}}</ref>\n\n[[File:Dependent premises.jpg|thumb|centre|100px|Statements 1 and 2 are dependent premises or co-premises]]\n\n\'\'\'Independent premises\'\'\', where the premise can support the conclusion on its own: Although independent premises may jointly make the conclusion more convincing, this is to be distinguished from situations where a premise gives no support unless it is joined to another premise. Where several premises or groups of premises lead to a final conclusion the argument might be described as \'\'convergent\'\'. This is distinguished from a \'\'divergent\'\' argument where a single premise might be used to support two separate conclusions.<ref>{{harvnb|Freeman|1991|pp=91–110}}; {{harvnb|Harrell|2010|p=20}}</ref>\n\n[[File:Independent premises.jpg|thumb|centre|150px|Statements 2, 3, 4 are independent premises]]\n\n\'\'\'Intermediate conclusions\'\'\' or \'\'\'sub-conclusions\'\'\', where a claim is supported by another claim that is used in turn to support some further claim, i.e. the final conclusion or another intermediate conclusion: In the following diagram, statement \'\'\'4\'\'\' is an intermediate conclusion in that it is a conclusion in relation to statement \'\'\'5\'\'\' but is a premise in relation to the final conclusion, i.e. statement \'\'\'1\'\'\'. An argument with this structure is sometimes called a \'\'complex\'\' argument. If there is a single chain of claims containing at least one intermediate conclusion, the argument is sometimes described as a \'\'serial\'\' argument or a \'\'chain\'\' argument.<ref>{{harvnb|Beardsley|1950|pp=18–19}}; {{harvnb|Reed|Walton|Macagno|2007|pp=3–8}}; {{harvnb|Harrell|2010|pp=19–21}}</ref>\n\n[[File:Intermediate conclusion.jpg|thumb|centre|150px|Statement 4 is an intermediate conclusion or sub-conclusion]]\n\nEach of these structures can be represented by the equivalent "box and line" approach to argument maps. In the following diagram, the \'\'contention\'\' is shown at the top, and the boxes linked to it represent supporting \'\'reasons\'\', which comprise one or more \'\'premises\'\'. The green arrow indicates that the two \'\'reasons\'\' support the \'\'contention\'\':\n\n[[File:A box and line diagram.png|thumb|center|A box and line diagram]]\n\nArgument maps can also represent counterarguments. In the following diagram, the two \'\'objections\'\' weaken the \'\'contention\'\', while the \'\'reasons\'\' support the \'\'premise\'\' of the objection:\n\n[[File:A sample argument using objections.png|thumb|center|A sample argument using objections]]\n\n==Representing an argument as an argument map==\nA written text can be transformed into an argument map by following a sequence of steps. [[Monroe Beardsley]]\'s 1950 book \'\'Practical Logic\'\' recommended the following procedure:<ref name="Beardsley">{{harvnb|Beardsley|1950}}</ref>\n#Separate statements by brackets and number them.\n#Put circles around the logical indicators.\n#Supply, in parenthesis, any logical indicators that are left out.\n#Set out the statements in a diagram in which arrows show the relationships between statements.\n\n[[File:Diagram using Beardsley\'s procedure.jpg|thumb|right|100px|A diagram of the example from Beardsley\'s \'\'Practical Logic\'\']]\n\nBeardsley gave the first example of a text being analysed in this way:\n\n:Though <span style="color:red;">① [</span>people who talk about the "social significance" of the arts don’t like to admit it<span style="color:red;">]</span>, <span style="color:red;">② [</span>music and painting are bound to suffer when they are turned into mere vehicles for propaganda<span style="color:red;">]</span>. <span style="width:100%; border:solid 2px blue; padding-left:5px; padding-right:5px; border-radius:25px;">For</span> <span style="color:red;">③ [</span>propaganda appeals to the crudest and most vulgar feelings<span style="color:red;">]</span>: <span style="color:red;">(for)</span> <span style="color:red;">④ [</span>look at the academic monstrosities produced by the official Nazi painters<span style="color:red;">]</span>. What is more important, <span style="color:red;">⑤ [</span>art must be an end in itself for the artist<span style="color:red;">]</span>, <span style="width:100%; border:solid 2px blue; padding-left:5px; padding-right:5px; border-radius:25px;">because</span> <span style="color:red;">⑥ [</span>the artist can do the best work only in an atmosphere of complete freedom<span style="color:red;">]</span>.\n\nBeardsley said that the conclusion in this example is statement ②. Statement ④ needs to be rewritten as a declarative sentence, e.g. "Academic monstrosities [were] produced by the official Nazi painters." Statement ① points out that the conclusion isn\'t accepted by everyone, but statement ① is omitted from the diagram because it doesn\'t support the conclusion. Beardsley said that the logical relation between statement ③ and statement ④ is unclear, but he proposed to diagram statement ④ as supporting statement ③.\n\n[[File:Using Harrell\'s procedure.jpg|thumb|right|200px|A box and line diagram of Beardsley\'s example, produced using Harrell\'s procedure]]\n\nMore recently, philosophy professor Maralee Harrell recommended the following procedure:<ref>{{harvnb|Harrell|2010|p=28}}</ref>\n#Identify all the claims being made by the author.\n#Rewrite them as independent statements, eliminating non-essential words.\n#Identify which statements are premises, sub-conclusions, and the main conclusion.\n#Provide missing, implied conclusions and implied premises. (This is optional depending on the purpose of the argument map.)\n#Put the statements into boxes and draw a line between any boxes that are linked.\n#Indicate support from premise(s) to (sub)conclusion with arrows.\n\nArgument maps are useful not only for representing and analyzing existing writings, but also for thinking through issues as part of a [[Problem structuring methods|problem-structuring process]] or [[writing process]]. The use of such argument analysis for thinking through issues has been called "reflective argumentation".<ref>For example: {{harvnb|Hoffmann|Borenstein|2013}}; {{harvnb|Hoffmann|2015}}</ref>\n\n==History==\n\n===The philosophical origins and tradition of argument mapping===\n[[File:Whatley.png|thumb|From Whately\'s Elements of Logic p467, 1852 edition]]\nIn the \'\'Elements of Logic\'\', which was published in 1826 and issued in many subsequent editions,<ref>{{harvnb|Whately|1834}} (first published 1826)</ref> Archbishop [[Richard Whately]] gave probably the first form of an argument map, introducing it with the suggestion that "many students probably will find it a very clear and convenient mode of exhibiting the logical analysis of the course of argument, to draw it out in the form of a Tree, or Logical Division".\n\nHowever, the technique did not become widely used, possibly because for complex arguments, it involved much writing and rewriting of the premises.\n\n[[File:Wigmore chart.png|thumb|Wigmore evidence chart, from 1905]]\nLegal philosopher and theorist [[John Henry Wigmore]] produced maps of legal arguments using numbered premises in the early 20th century,<ref>{{harvnb|Wigmore|1913}}</ref> based in part on the ideas of 19th century philosopher [[Henry Sidgwick]] who used lines to indicate relations between terms.<ref>{{harvnb|Goodwin|2000}}</ref>\n\n===Anglophone argument diagramming in the 20th century===\nDealing with the failure of [[Formal system|formal]] reduction of informal argumentation, English speaking [[argumentation theory]] developed diagrammatic approaches to informal reasoning over a period of fifty years.\n\n[[Monroe Beardsley]] proposed a form of argument diagram in 1950.<ref name="Beardsley"/> His method of marking up an argument and representing its components with linked numbers became a standard and is still widely used. He also introduced terminology that is still current describing \'\'convergent\'\', \'\'divergent\'\' and \'\'serial\'\' arguments.\n\n[[File:Toulmindiag.png|thumb|A Toulmin argument diagram, redrawn from his 1959 \'\'Uses of Argument\'\']]\n[[File:Toulmingeneral.png|thumb|A generalised Toulmin diagram]]\n[[Stephen Toulmin]], in his groundbreaking and influential 1958 book \'\'The Uses of Argument\'\',<ref>{{harvnb|Toulmin|2003}} (first published 1958)</ref> identified several elements to an argument which have been generalized. The Toulmin diagram is widely used in educational critical teaching.<ref name="Simon06">{{harvnb|Simon|Erduran|Osborne|2006}}</ref><ref>{{harvnb|Böttcher|Meisert|2011}}; {{harvnb|Macagno|Konstantinidou|2013}}</ref> Whilst Toulmin eventually had a significant impact on the development of [[informal logic]] he had little initial impact and the Beardsley approach to diagramming arguments along with its later developments became the standard approach in this field. Toulmin introduced something that was missing from Beardsley\'s approach. In Beardsley, "arrows link reasons and conclusions (but) no support is given to the implication itself between them. There is no theory, in other words, of inference distinguished from logical deduction, the passage is always deemed not controversial and not subject to support and evaluation".<ref>{{harvnb|Reed|Walton|Macagno|2007|p=8}}</ref> Toulmin introduced the concept of \'\'warrant\'\' which "can be considered as representing the reasons behind the inference, the backing that authorizes the link".<ref>{{harvnb|Reed|Walton|Macagno|2007|p=9}}</ref>\n\nBeardsley\'s approach was refined by Stephen N. Thomas, whose 1973 book \'\'Practical Reasoning In Natural Language\'\'<ref>{{harvnb|Thomas|1997}} (first published 1973)</ref> introduced the term \'\'linked\'\' to describe arguments where the premises necessarily worked together to support the conclusion.<ref name="SnoeckHenkemans">{{harvnb|Snoeck Henkemans|2000|p=453}}</ref> However, the actual distinction between dependent and independent premises had been made prior to this.<ref name="SnoeckHenkemans"/> The introduction of the linked structure made it possible for argument maps to represent missing or "hidden" premises. In addition, Thomas suggested showing reasons both \'\'for\'\' and \'\'against\'\' a conclusion with the reasons \'\'against\'\' being represented by dotted arrows. Thomas introduced the term \'\'argument diagram\'\' and defined \'\'basic reasons\'\' as those that were not supported by any others in the argument and the \'\'final conclusion\'\' as that which was not used to support any further conclusion.\n\n[[File:Scrivendiag.png|thumb|Scriven\'s argument diagram. The explicit premise 1 is conjoined with additional unstated premises a and b to imply 2.]]\n[[Michael Scriven]] further developed the Beardsley-Thomas approach in his 1976 book \'\'Reasoning\'\'.<ref>{{harvnb|Scriven|1976}}</ref> Whereas Beardsley had said "At first, write out the statements...after a little practice, refer to the statements by number alone"<ref>{{harvnb|Beardsley|1950|p=21}}</ref> Scriven advocated clarifying the meaning of the statements, listing them and then using a tree diagram with numbers to display the structure. Missing premises (unstated assumptions) were to be included and indicated with an alphabetical letter instead of a number to mark them off from the explicit statements. Scriven introduced counterarguments in his diagrams, which Toulmin had defined as rebuttal.<ref>{{harvnb|Reed|Walton|Macagno|2007|pp=10–11}}</ref> This also enabled the diagramming of "balance of consideration" arguments.<ref>{{harvnb|van Eemeren|Grootendorst|Snoeck Henkemans|Blair|1996|p=175}}</ref>\n\nIn the 1990s, [[Tim van Gelder]] and colleagues developed a series of computer software applications that permitted the premises to be fully stated and edited in the diagram, rather than in a legend.<ref>{{harvnb|van Gelder|2007}}</ref> Van Gelder\'s first program, Reason!Able, was superseded by two subsequent programs, bCisive and Rationale.<ref>{{harvnb|Berg|van Gelder|Patterson|Teppema|2009}}</ref>\n\nThroughout the 1990s and 2000s, many other software applications were developed for argument visualization. By 2013, more than 60 such software systems existed.<ref>{{harvnb|Walton|2013|p=11}}</ref> One of the differences between these software systems is whether collaboration is supported.<ref name="Scheuer10">{{harvnb|Scheuer|Loll|Pinkwart|McLaren|2010}}</ref> Single-user argumentation systems include Convince Me, iLogos, LARGO, Athena, [[Araucaria (software)|Araucaria]], and Carneades; small group argumentation systems include Digalo, QuestMap, [[Compendium (software)|Compendium]], Belvedere, and AcademicTalk; community argumentation systems include [[Debategraph]] and [[Collaboratorium]].<ref name="Scheuer10"/> For more software examples, see: {{section link||External links}}.\n\nIn 1998 a series of large-scale argument maps released by [[Robert E. Horn]] stimulated widespread interest in argument mapping.<ref>{{harvnb|Holmes|1999}}; {{harvnb|Horn|1998}} and Robert E. Horn, [http://www.stanford.edu/~rhorn/a/topic/arg/artclCmptrSpArgmttn.pdf "Infrastructure for navigating interdisciplinary debates: critical decisions for representing argumentation"], in {{harvnb|Kirschner|Buckingham Shum|Carr|2003|pp=165–184}}</ref>\n\n==Applications==\nArgument maps have been applied in many areas, but foremost in educational, academic and business settings, including [[design rationale]].<ref name="Applications">{{harvnb|Kirschner|Buckingham Shum|Carr|2003}}; {{harvnb|Okada|Buckingham Shum|Sherborne|2014}}</ref> Argument maps are also used in [[forensic science]],<ref>For example: {{harvnb|Bex|2011}}</ref> [[law]], and [[artificial intelligence]].<ref>For example: {{harvnb|Verheij|2005}}; {{harvnb|Reed|Walton|Macagno|2007}}; {{harvnb|Walton|2013}}</ref> It has also been proposed that argument mapping has a great potential to improve how we understand and execute democracy, in reference to the ongoing evolution of [[e-democracy]].<ref>{{harvnb|Hilbert|2009}}</ref>\n\n===Difficulties with the philosophical tradition===\nIt has traditionally been hard to separate teaching critical thinking from the philosophical tradition of teaching [[logic]] and method, and most critical thinking textbooks have been written by philosophers. [[Informal logic]] textbooks are replete with philosophical examples, but it is unclear whether the approach in such textbooks transfers to non-philosophy students.<ref name="Simon06" /> There appears to be little statistical effect after such classes. Argument mapping, however, has a measurable effect according to many studies.<ref>{{harvnb|Twardy|2004}}; {{harvnb|Álvarez Ortiz|2007}}; {{harvnb|Harrell|2008}}; Yanna Rider and Neil Thomason, [//books.google.com/books?id=loa4BAAAQBAJ&pg=PA112 "Cognitive and pedagogical benefits of argument mapping: LAMP guides the way to better thinking"], in {{harvnb|Okada|Buckingham Shum|Sherborne|2014|pp=113–134}}; {{harvnb|Dwyer|2011}}; {{harvnb|Davies|2012}}</ref> For example, instruction in argument mapping has been shown to improve the critical thinking skills of business students.<ref>{{harvnb|Carrington|Chen|Davies|Kaur|2011}}; {{harvnb|Kunsch|Schnarr|van Tyle|2014}}</ref>\n\n===Evidence that argument mapping improves critical thinking ability===\nThere is empirical evidence that the skills developed in argument-mapping-based critical thinking courses substantially transfer to critical thinking done without argument maps. Alvarez\'s meta-analysis found that such critical thinking courses produced gains of around 0.70 SD, about twice as much as standard critical-thinking courses.<ref>{{harvnb|Álvarez Ortiz|2007|pp=69–70 \'\'et seq\'\'}}</ref> The tests used in the reviewed studies were standard critical-thinking tests.\n\n===How argument mapping helps with critical thinking===\nThe use of argument mapping has occurred within a number of disciplines, such as philosophy, management reporting, military and intelligence analysis, and public debates.<ref name="Applications"/>\n\n\'\'Logical structure\'\': Argument maps display an argument\'s logical structure more clearly than does the standard linear way of presenting arguments.\n\n\'\'Critical thinking concepts\'\': In learning to argument map, students master such key critical thinking concepts as "reason", "objection", "premise", "conclusion", "inference", "rebuttal", "unstated assumption", "co-premise", "strength of evidence", "logical structure", "independent evidence", etc. Mastering such concepts is not just a matter of memorizing their definitions or even being able to apply them correctly; it is also understanding why the distinctions these words mark are important and using that understanding to guide one\'s reasoning.\n\n\'\'Visualization\'\': Humans are highly visual and argument mapping may provide students with a basic set of visual schemas with which to understand argument structures.\n\n\'\'More careful reading and listening\'\': Learning to argument map teaches people to read and listen more carefully, and highlights for them the key questions "What is the logical structure of this argument?" and "How does this sentence fit into the larger structure?" In-depth cognitive processing is thus more likely.\n\n\'\'More careful writing and speaking\'\': Argument mapping helps people to state their reasoning and evidence more precisely, because the reasoning and evidence must fit explicitly into the map\'s logical structure.\n\n\'\'Literal and intended meaning\'\': Often, many statements in an argument do not precisely assert what the author meant. Learning to argument map enhances the complex skill of distinguishing literal from intended meaning.\n\n\'\'Externalization\'\': Writing something down and reviewing what one has written often helps reveal gaps and clarify one\'s thinking. Because the logical structure of argument maps is clearer than that of linear prose, the benefits of mapping will exceed those of ordinary writing.\n\n\'\'Anticipating replies\'\': Important to critical thinking is anticipating objections and considering the plausibility of different rebuttals. Mapping develops this anticipation skill, and so improves analysis.\n\n==Standards==\n\n===Argument Interchange Format===\nThe Argument Interchange Format, AIF, is an international effort to develop a representational mechanism for exchanging argument resources between research groups, tools, and domains using a semantically rich language.<ref>See the [http://www.arg.dundee.ac.uk/people/chris/publications/2006/aif_final.pdf AIF original draft description] (2006) and the [http://www.argdf.org/source/ArgDF_Ontology.rdfs full AIF-RDF ontology specifications] in [[RDFS]] format.</ref> AIF-RDF is the extended ontology represented in the [[Resource Description Framework]] Schema (RDFS) semantic language. Though AIF is still something of a moving target, it is settling down.<ref>{{harvnb|Bex|Modgil|Prakken|Reed|2013}}</ref>\n\n===Legal Knowledge Interchange Format===\nThe Legal Knowledge Interchange Format (LKIF),<ref>{{harvnb|Boer|Winkels|Vitali|2008}}</ref> developed in the European ESTRELLA project,<ref>{{cite web |title=Estrella project website |url=http://www.estrellaproject.org/ |website=estrellaproject.org |archiveurl=https://web.archive.org/web/20160212103522/http://www.estrellaproject.org/ |archivedate=2016-02-12 |accessdate=2016-02-24}}</ref> is an XML schema for rules and arguments, designed with the goal of becoming a standard for representing and interchanging policy, legislation and cases, including their justificatory arguments, in the legal domain. LKIF builds on and uses the [[Web Ontology Language]] (OWL) for representing concepts and includes a reusable basic ontology of legal concepts.\n\n== See also ==\n{{Commons category|Argument maps}}\n* [[Flow (policy debate)]]\n* [[Informal fallacy]]\n* [[Information graphics]]\n* [[Natural deduction]], a logical system with argument map-like notation\n\n== Notes ==\n{{Reflist|colwidth=20em}}\n\n== References ==\n* {{cite thesis |type=M.A. thesis |last=Álvarez Ortiz |first=Claudia María |title=Does philosophy improve critical thinking skills? |url=http://images.austhink.com/pdf/Claudia-Alvarez-thesis.pdf |year=2007 |publisher=Department of Philosophy, [[University of Melbourne]] |oclc=271475715 |ref=harv}}\n* {{cite book |last=Beardsley |first=Monroe C. |authorlink=Monroe Beardsley |date=1950 |title=Practical logic |location=New York |publisher=Prentice-Hall |oclc=4318971 |ref=harv}}\n* {{cite book |last1=Berg |first1=Timo ter |last2=van Gelder |first2=Tim |authorlink2=Tim van Gelder |last3=Patterson |first3=Fiona |last4=Teppema |first4=Sytske |year=2009 |title=Critical thinking: reasoning and communicating with Rationale |location=Amsterdam |publisher=Pearson Education Benelux |isbn=9043018015 |oclc=301884530 |ref=harv}}\n* {{cite book |last=Bex |first=Floris J. |date=2011 |title=Arguments, stories and criminal evidence: a formal hybrid theory |series=Law and philosophy library |volume=92 |location=Dordrecht; New York |publisher=Springer |isbn=9789400701397 |oclc=663950184 |doi=10.1007/978-94-007-0140-3 |ref=harv}}\n* {{cite journal |last1=Bex |first1=Floris J. |last2=Modgil |first2=Sanjay |last3=Prakken |first3=Henry |last4=Reed |first4=Chris |title=On logical specifications of the Argument Interchange Format |journal=[[Journal of Logic and Computation]] |volume=23 |issue=5 |pages=951–989 |year=2013 |doi=10.1093/logcom/exs033 |url=http://www.arg.dundee.ac.uk/people/chris/publications/2013/AIF-ASPIC-JLC-Final.pdf |ref=harv}}\n* {{cite book |last1=Boer |first1=Alexander |last2=Winkels |first2=Radboud |last3=Vitali |first3=Fabio |date=2008 |chapter=MetaLex XML and the Legal Knowledge Interchange Format |editor1-last=Casanovas |editor1-first=Pompeu |editor2-last=Sartor |editor2-first=Giovanni |editor3-last=Casellas |editor3-first=Núria |editor4-last=Rubino |editor4-first=Rossella |title=Computable models of the law: languages, dialogues, games, ontologies |series=Lecture notes in computer science |volume=4884 |location=Berlin; New York |publisher=Springer |pages=21–41 |isbn=9783540855682 |oclc=244765580 |doi=10.1007/978-3-540-85569-9_2 |chapterurl=https://www.researchgate.net/profile/Radboud_Winkels/publication/225708550_MetaLex_XML_and_the_Legal_Knowledge_Interchange_Format/links/02bfe510239cc79c8d000000.pdf |ref=harv |accessdate=2016-02-24}}\n* {{cite journal |last1=Böttcher |first1=Florian |last2=Meisert |first2=Anke |title=Argumentation in science education: a model-based framework |journal=Science & Education |volume=20 |issue=2 |pages=103–140 |date=February 2011 |doi=10.1007/s11191-010-9304-5 |ref=harv}}\n* {{cite journal |last1=Carrington |first1=Michal |last2=Chen |first2=Richard |last3=Davies |first3=Martin |last4=Kaur |first4=Jagjit |last5=Neville |first5=Benjamin |title=The effectiveness of a single intervention of computer‐aided argument mapping in a marketing and a financial accounting subject |journal=Higher Education Research & Development |volume=30 |issue=3 |pages=387–403 |date=June 2011 |doi=10.1080/07294360.2011.559197 |url=https://www.researchgate.net/profile/Martin_Davies/publication/232947753_The_effectiveness_of_a_single_intervention_of_computeraided_argument_mapping_in_a_marketing_and_a_financial_accounting_subject/links/0deec51d94fec63bdc000000.pdf |ref=harv |accessdate=2016-02-24}}\n* {{cite book |last1=Culmsee |first1=Paul |last2=Awati |first2=Kailash |date=2013 |origyear=2011 |chapter=Chapter 7: Visualising reasoning, and Chapter 8: Argumentation-based rationale |title=The heretic\'s guide to best practices: the reality of managing complex problems in organisations |location=Bloomington, IN |publisher=iUniverse, Inc. |pages=153–211 |isbn=9781462058549 |oclc=767703320 |chapterurl=https://books.google.com/books?id=Gb2uuT1zrAAC&pg=PA153 |ref=harv}}\n* {{cite journal |last=Davies |first=Martin |title=Computer-aided argument mapping as a tool for teaching critical thinking |journal=[[International Journal of Learning and Media]] |volume=4 |issue=3-4 |pages=79–84 |date=Summer 2012 |doi=10.1162/IJLM_a_00106 |url=http://www.mitpressjournals.org/doi/full/10.1162/IJLM_a_00106 |ref=harv}}\n* {{cite thesis |type=Ph.D. thesis |last=Dwyer |first=Christopher Peter |title=The evaluation of argument mapping as a learning tool |url=http://aran.library.nuigalway.ie/xmlui/bitstream/handle/10379/2617/C.Dwyer-PhD%20Thesis%20Psychology.pdf |year=2011 |publisher=School of Psychology, National University of Ireland, Galway |oclc=812818648 |ref=harv |accessdate=2016-02-24}}\n* {{cite book |last1=van Eemeren |first1=Frans H. |authorlink1=Frans H. van Eemeren |last2=Grootendorst |first2=Rob |authorlink2=Rob Grootendorst |last3=Snoeck Henkemans |first3=A. Francisca |last4=Blair |first4=J. Anthony |last5=Johnson |first5=Ralph H. |authorlink5=Ralph Johnson (philosopher) |last6=Krabbe |first6=Erik C. W. |last7=Plantin |first7=Christian |last8=Walton |first8=Douglas N. |authorlink8=Doug Walton |last9=Willard |first9=Charles A. |authorlink9=Charles Arthur Willard |last10=Woods |first10=John |authorlink10=John Woods (logician) |date=1996 |title=Fundamentals of argumentation theory: a handbook of historical backgrounds and contemporary developments |location=Mahwah, NJ |publisher=[[Lawrence Erlbaum Associates]] |isbn=0805818618 |oclc=33970847 |doi=10.4324/9780203811306 |ref=harv}}\n* {{cite book |last=Facione |first=Peter A. |title=THINK critically |year=2013 |origyear=2011 |edition=2nd |location=Boston |publisher=Pearson |isbn=0205490980 |oclc=770694200 |ref=harv}}\n* {{cite book |last=Fisher |first=Alec |title=The logic of real arguments |url=https://books.google.com/books?id=Q2a_RtDZUGgC |year=2004 |origyear=1988 |edition=2nd |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=0521654815 |oclc=54400059 |doi=10.1017/CBO9780511818455 |ref=harv |accessdate=2016-02-24}}\n* {{cite book |last=Freeman |first=James B. |title=Dialectics and the macrostructure of arguments: a theory of argument structure |url=https://books.google.com/books?id=ScvV9riTOGsC |year=1991 |location=Berlin; New York |publisher=Foris Publications |isbn=3110133903 |oclc=24429943 |ref=harv |accessdate=2016-02-24}}\n* {{cite journal |last=van Gelder |first=Tim |authorlink=Tim van Gelder |title=The rationale for Rationale |journal=Law, Probability and Risk |volume=6 |issue=1-4 |pages=23–42 |year=2007 |doi=10.1093/lpr/mgm032 |url=http://sites.google.com/site/timvangelder/publications-1/therationaleforrationale/TheRationaleforRationale.pdf |ref=harv}}\n* {{cite journal |last=Goodwin |first=Jean |title=Wigmore\'s chart method |journal=Informal Logic |volume=20 |issue=3 |pages=223–243 |year=2000 |url=http://windsor.scholarsportal.info/ojs/leddy/index.php/informal_logic/article/view/2278 |ref=harv}}\n* {{cite journal |last=Harrell |first=Maralee |title=No computer program required: even pencil-and-paper argument mapping improves critical-thinking skills |journal=[[Teaching Philosophy]] |volume=31 |issue=4 |pages=351–374 |date=December 2008 |doi=10.5840/teachphil200831437 |url=http://www.hss.cmu.edu/philosophy/harrell/HarrellTeachingPhilosophy2008.pdf |ref=harv}}\n* {{cite web |last=Harrell |first=Maralee |title=Creating argument diagrams |date=August 2010 |url=http://www.academia.edu/772321/Creating_Argument_Diagrams |website=[[academia.edu]] |ref=harv}}\n* {{cite journal |last=Hilbert |first=Martin |title=The maturing concept of e-democracy: from e-voting and online consultations to democratic value out of jumbled online chatter |journal=[[Journal of Information Technology and Politics]] |date=April 2009 |volume=6 |issue=2 |pages=87–110 |doi=10.1080/19331680802715242 |url=http://www.martinhilbert.net/e-democracyHilbertJITP.pdf |ref=harv}}\n* {{cite journal |last=Hoffmann |first=Michael H. G. |date=November 2015 |title=Reflective argumentation: a cognitive function of arguing |journal=Argumentation |doi=10.1007/s10503-015-9388-9 |ref=harv}}\n* {{cite journal |last1=Hoffmann |first1=Michael H. G. |last2=Borenstein |first2=Jason |date=February 2013 |title=Understanding ill-structured engineering ethics problems through a collaborative learning and argument visualization approach |journal=Science and Engineering Ethics |volume=20 |issue=1 |pages=261–276 |doi=10.1007/s11948-013-9430-y |pmid=23420467 |url=http://works.bepress.com/michael_hoffmann/39/ |ref=harv}}\n* {{cite news |last=Holmes |first=Bob |title=Beyond words |url=http://www.newscientist.com/article/mg16321944.700-beyond-words.html |date=10 July 1999 |newspaper=New Scientist |issue=2194 |archiveurl=https://web.archive.org/web/20080928062226/http://www.newscientist.com/article/mg16321944.700-beyond-words.html |archivedate=28 September 2008 |ref=harv}}\n* {{cite book |last=Horn |first=Robert E. |authorlink=Robert E. Horn |title=Visual language: global communication for the 21st century |year=1998 |location=Bainbridge Island, WA |publisher=MacroVU, Inc. |isbn=189263709X |oclc=41138655 |ref=harv}}\n* {{cite book |last=Kelley |first=David |authorlink=David Kelley |title=The art of reasoning: an introduction to logic and critical thinking |year=2014 |origyear=1988 |edition=4th |location=New York |publisher=W. W. Norton & Company |isbn=0393930785 |oclc=849801096 |ref=harv}}\n* {{cite book |editor1-last=Kirschner |editor1-first=Paul Arthur |editor2-last=Buckingham Shum |editor2-first=Simon J |editor3-last=Carr |editor3-first=Chad S |title=Visualizing argumentation: software tools for collaborative and educational sense-making |year=2003 |series=Computer supported cooperative work |location=New York |publisher=Springer |isbn=1852336641 |oclc=50676911 |doi=10.1007/978-1-4471-0037-9 |url=https://books.google.com/books?id=dNijwv-my_kC |ref=harv |accessdate=2016-02-24}}\n* {{cite journal |last1=Kunsch |first1=David W. |last2=Schnarr |first2=Karin |last3=van Tyle |first3=Russell |title=The use of argument mapping to enhance critical thinking skills in business education |journal=Journal of Education for Business |volume=89 |issue=8 |pages=403–410 |date=November 2014 |doi=10.1080/08832323.2014.925416 |url= |ref=harv}}\n* {{cite journal |last1=Macagno |first1=Fabrizio |last2=Konstantinidou |first2=Aikaterini |title=What students\' arguments can tell us: using argumentation schemes in science education |journal=Argumentation |volume=27 |issue=3 |pages=225–243 |date=August 2013 |doi=10.1007/s10503-012-9284-5 |url=http://ssrn.com/abstract=2185945 |ref=harv}}\n* {{cite journal |last1=Metcalfe |first1=Mike |last2=Sastrowardoyo |first2=Saras |date=November 2013 |title=Complex project conceptualisation and argument mapping |journal=International Journal of Project Management |volume=31 |issue=8 |pages=1129–1138 |doi=10.1016/j.ijproman.2013.01.004 |ref=harv}}\n* {{cite book |editor1-last=Okada |editor1-first=Alexandra |editor2-last=Buckingham Shum |editor2-first=Simon J |editor3-last=Sherborne |editor3-first=Tony |year=2014 |origyear=2008 |title=Knowledge cartography: software tools and mapping techniques |edition=2nd |series=Advanced information and knowledge processing |location=New York |publisher=Springer |isbn=9781447164692 |oclc=890438015 |doi=10.1007/978-1-4471-6470-8 |url=https://books.google.com/books?id=loa4BAAAQBAJ |ref=harv |accessdate=2016-02-24}}\n* {{cite journal |last1=Reed |first1=Chris |last2=Rowe |first2=Glenn |title=A pluralist approach to argument diagramming |journal=Law, Probability and Risk |volume=6 |issue=1-4 |pages=59–85 |year=2007 |doi=10.1093/lpr/mgm030 |url=http://lpr.oxfordjournals.org/content/6/1-4/59.full.pdf |ref=harv |accessdate=2016-02-24}}\n* {{cite journal |last1=Reed |first1=Chris |last2=Walton |first2=Douglas |authorlink2=Doug Walton |last3=Macagno |first3=Fabrizio |title=Argument diagramming in logic, law and artificial intelligence |journal=The Knowledge Engineering Review |volume=22 |issue=1 |pages=1–22 |date=March 2007 |doi=10.1017/S0269888907001051 |url=https://www.academia.edu/2992281/Argument_Diagramming_in_Logic_Artificial_Intelligence_and_Law |ref=harv}}\n* {{cite journal |last1=Scheuer |first1=Oliver |last2=Loll |first2=Frank |last3=Pinkwart |first3=Niels |last4=McLaren |first4=Bruce M. |title=Computer-supported argumentation: a review of the state of the art |journal=International Journal of Computer-Supported Collaborative Learning |volume=5 |issue=1 |pages=43–102 |year=2010 |doi=10.1007/s11412-009-9080-x |url=http://www.oliver-scheuer.info/publications/ScheuerEtAl-EdArgSystemsReview-IJCSCL-2010.pdf |ref=harv}}\n* {{cite book |last=Scriven |first=Michael |authorlink=Michael Scriven |title=Reasoning |year=1976 |location=New York |publisher=McGraw-Hill |isbn=0070558825 |oclc=2800373 |ref=harv}}\n* {{cite journal |last1=Simon |first1=Shirley |last2=Erduran |first2=Sibel |last3=Osborne |first3=Jonathan |title=Learning to teach argumentation: research and development in the science classroom |journal=International Journal of Science Education |volume=28 |issue=2-3 |pages=235–260 |year=2006 |doi=10.1080/09500690500336957 |url=http://cset.stanford.edu/sites/default/files/files/documents/publications/Simon-Learning%20to%20Teach%20Argumentation.pdf |ref=harv}}\n* {{cite journal |last=Snoeck Henkemans |first=A. Francisca |date=November 2000 |title=State-of-the-art: the structure of argumentation |journal=Argumentation |volume=14 |issue=4 |pages=447–473 |doi=10.1023/A:1007800305762 |ref=harv}}\n* {{cite book |last=Thomas |first=Stephen N. |date=1997 |origyear=1973 |title=Practical reasoning in natural language |edition=4th |location=Upper Saddle River, NJ |publisher=[[Prentice-Hall]] |isbn=0136782698 |oclc=34745923 |ref=harv}}\n* {{cite book |last=Toulmin |first=Stephen E. |authorlink=Stephen Toulmin |title=The uses of argument |url=https://books.google.com/books?id=8UYgegaB1S0C |year=2003 |origyear=1958 |edition=Updated |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=0521534836 |oclc=57253830 |doi=10.1017/CBO9780511840005 |accessdate=2016-02-24 }}\n* {{cite journal |last=Twardy |first=Charles R. |title=Argument maps improve critical thinking |journal=[[Teaching Philosophy]] |volume=27 |issue=2 |pages=95–116 |date=June 2004 |doi=10.5840/teachphil200427213 |url=http://cogprints.org/3008/1/reasonpaper.pdf |ref=harv}}\n* {{cite book |last=Verheij |first=Bart |title=Virtual arguments: on the design of argument assistants for lawyers and other arguers |year=2005 |location=The Hague |publisher=T.M.C. Asser Press |series=Information technology & law series |volume=6 |isbn=9789067041904 |oclc=59617214 |ref=harv}}\n* {{cite book |last=Walton |first=Douglas N. |authorlink=Doug Walton |title=Methods of argumentation |url=https://books.google.com/books?id=vaU0AAAAQBAJ |year=2013 |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=1107677335 |oclc=830523850 |doi=10.1017/CBO9781139600187 |ref=harv |accessdate=2016-02-24}}\n* {{cite book |last=Whately |first=Richard |authorlink=Richard Whately |title=Elements of logic: comprising the substance of the article in the Encyclopædia metropolitana: with additions, &c. |url=https://books.google.com/books?id=5mgAAAAAMAAJ |year=1834 |origyear=1826 |edition=5th |location=London |publisher=B. Fellowes |oclc=1739330 |ref=harv |accessdate=2016-02-24}}\n* {{cite book |last=Wigmore |first=John Henry |authorlink=John Henry Wigmore |title=The principles of judicial proof: as given by logic, psychology, and general experience, and illustrated in judicial trials |url=https://books.google.com/books?id=4ho-AAAAIAAJ |year=1913 |location=Boston |publisher=Little Brown |oclc=1938596 |ref=harv |accessdate=2016-02-24}}\n\n== Further reading ==\n* {{cite book |last1=van Eemeren |first1=Frans H. |authorlink1=Frans H. van Eemeren |last2=Garssen |first2=Bart |last3=Krabbe |first3=Erik C. W. |last4=Snoeck Henkemans |first4=A. Francisca |last5=Verheij |first5=Bart |last6=Wagemans |first6=Jean H. M. |date=2014 |title=Handbook of argumentation theory |location=New York |publisher=Springer |isbn=9789048194728 |oclc=871004444 |doi=10.1007/978-90-481-9473-5 |ref=harv}}\n* {{cite book|last1=Facione |first1=Peter A. |last2=Facione |first2=Noreen C. |title=Thinking and reasoning in human decision making: the method of argument and heuristic analysis |year=2007 |location=Milbrae, CA |publisher=California Academic Press |isbn=1891557580 |oclc=182039452 |ref=harv}}\n* {{cite web |last=van Gelder |first=Tim |authorlink=Tim van Gelder |url=http://timvangelder.com/2009/02/17/what-is-argument-mapping/ |title=What is argument mapping? |publisher=timvangelder.com |date=17 February 2009 |accessdate=12 January 2015 |ref=harv}}\n* {{cite book |last=van Gelder |first=Tim |authorlink=Tim van Gelder |date=2015 |chapter=Using argument mapping to improve critical thinking skills |editor1-last=Davies |editor1-first=Martin |editor2-last=Barnett |editor2-first=Ronald |title=The Palgrave handbook of critical thinking in higher education |location=New York |publisher=[[Palgrave Macmillan]] |pages=183–192 |isbn=9781137378033 |oclc=894935460 |doi=10.1057/9781137378057_12 |ref=harv}}\n* {{cite journal |last=Harrell |first=Maralee |title=Using argument diagramming software in the classroom |journal=[[Teaching Philosophy]] |volume=28 |issue=2 |pages=163–177 |date=June 2005 |doi=10.5840/teachphil200528222 |url=http://www.hss.cmu.edu/philosophy/harrell/ArgumentDiagramsInClassroom.pdf |ref=harv}}\n* {{cite journal |last1=Schneider |first1=Jodi |last2=Groza |first2=Tudor |last3=Passant |first3=Alexandre |date=April 2013 |title=A review of argumentation for the social semantic web |journal=Semantic Web |volume=4 |issue=2 |pages=159–218 |url=http://semantic-web-journal.org/sites/default/files/swj138_0.pdf |ref=harv}}\n\n== External links ==\n\n===Argument mapping software===\n*[http://araucaria.computing.dundee.ac.uk/ Araucaria] (open source, cross platform/Java)\n*[http://sourceforge.net/projects/argumentative/ Argumentative] (open source, Windows); supports single-user, graphical argumentation\n*[http://www.argunet.org/editor/ Argunet] (open source, cross platform)\n*[http://compendiuminstitute.net/ Compendium] (open source, cross platform/Java)\n*[http://www.phil.cmu.edu/projects/argument_mapping/ iLogos] (cross platform/Java)\n*[http://ova.arg-tech.org/ OVA] (Web based, Online Visualisation of Argument)\n*[http://www.cs.ie.niigata-u.ac.jp/Research/PIRIKA/PIRIKA.html PIRIKA (PIlot for the RIght Knowledge and Argument)] (open source, Linux, Windows)\n*[http://www.vangeldermonk.com/reasoningapp/ The Reasoning PowerPoint App] ([[PowerPoint]] add-in, Windows 2007 or later)\n\n===Online, collaborative software===\n*[http://agora.gatech.edu/ AGORA-net] (user interface in English, German, Spanish, Chinese, and Russian)\n*[http://arguman.org/ Arguman] (Open source, English, Turkish, and Chinese)\n*[http://www.bcisiveonline.com/ bCisiveOnline]\n*[http://carneades.github.io/ Carneades] (open source, argument (re)construction, evaluation, mapping and interchange)\n*[https://code.google.com/p/collam/ Collam] ([[JavaScript]] library for visualizing argument maps)\n*[http://www.debategraph.org/ Debategraph]\n*[http://www.truthmapping.com/ TruthMapping]\n\n[[Category:Argument mapping]]\n[[Category:Arguments]]\n[[Category:Critical thinking]]\n[[Category:Diagrams]]\n[[Category:Knowledge representation]]\n[[Category:Logic]]\n[[Category:Problem structuring methods]]']
['WordNet', '33955', '\'\'\'WordNet\'\'\' is a [[lexical database]] for the [[English language]].<ref>G. A. Miller, R. Beckwith, C. D. Fellbaum, D. Gross, K. Miller. 1990. WordNet: An online lexical database. Int. J. Lexicograph. 3, 4, pp. 235–244.</ref> It groups English [[word]]s into sets of [[synonyms]] called \'\'[[synsets]]\'\', provides short definitions and usage examples, and records a number of relations among  these synonym sets or their members. WordNet can thus be seen as a combination of [[dictionary]] and [[thesaurus]]. While it is  accessible to human users via a [[web browser]],<ref name="WordNet Search">{{cite web|url=http://wordnetweb.princeton.edu/perl/webwn|title=WordNet Search - 3.1}}</ref> its primary use is in automatic [[natural language processing|text analysis]] and [[artificial intelligence]] applications. The [[database]] and [[software]] tools have been released under a [[BSD License|BSD style license]] and are freely available for download from the WordNet website. Both the lexicographic data (\'\'lexicographer files\'\') and the compiler (\'\'called grind\'\') for producing the distributed database are available.\n[[File:WordNet.PNG|thumb|This is a snapshot of WordNet\'s definition of itself.]]\n\n== History and team members ==\nWordNet was created in the [[Cognitive Science]] Laboratory of [[Princeton University]] under the direction of  [[psychology]] [[professor]] [[George Armitage Miller]] starting in 1985 and has been directed in recent years by [[Christiane Fellbaum]]. The project received funding from government agencies including the [[National Science Foundation]], [[DARPA]], the [[Disruptive Technology Office]] (formerly the Advanced Research and Development Activity), and REFLEX. George Miller and Christiane Fellbaum were awarded the 2006 [[European Language Resources Association#Antonio Zampolli Prize|Antonio Zampolli Prize]] for their work with WordNet.\n\n== Database contents ==\n[[File:Hamburger WordNet.png|thumb|Example entry "Hamburger" in WordNet]]\n\nAs of November 2012 WordNet\'s latest Online-version is 3.1.<ref>{{cite web|url=http://wordnet.princeton.edu/wordnet/download/current-version/ |title=Current WordNet version |publisher=Wordnet.princeton.edu |date=2012-11-09 |accessdate=2014-03-11}}</ref> The  database contains 155,287 words organized in 117,659 [[synsets]] for a total of 206,941 word-sense pairs; in [[data compression|compressed]] form, it is about 12 [[megabyte]]s in size.<ref>{{cite web|url=http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html |title=WordNet Statistics |publisher=Wordnet.princeton.edu |date= |accessdate=2014-03-11}}</ref>\n\nWordNet includes the lexical categories [[noun]]s, [[verb]]s, [[adjective]]s and [[adverb]]s but ignores [[preposition]]s, [[determiner (linguistics)|determiner]]s and other function words.\n\nWords from the same lexical category that are roughly synonymous are grouped into [[synsets]]. Synsets include simplex words as well as [[collocation]]s like "eat out" and "car pool." The different senses of a [[polysemous]] word form are assigned to different synsets. The meaning of a synset is further clarified with a short defining \'\'gloss\'\' and one or more usage examples. An example adjective synset is:\n\n: good, right, ripe – (most suitable or right for a particular purpose; "a good time to plant tomatoes"; "the right time to act"; "the time is ripe for great sociological changes")\n\nAll synsets are connected to other synsets by means of semantic relations. These relations, which are not all shared by all lexical categories, include:\n* [[Noun]]s\n**\'\'[[hypernym]]s\'\': \'\'Y\'\' is a hypernym of \'\'X\'\' if every \'\'X\'\' is a (kind of) \'\'Y\'\' (\'\'canine\'\' is a hypernym of \'\'[[dog]]\'\')\n**\'\'[[hyponym]]s\'\': \'\'Y\'\' is a hyponym of \'\'X\'\' if every \'\'Y\'\' is a (kind of) \'\'X\'\' (\'\'dog\'\' is a hyponym of \'\'canine\'\')\n**\'\'coordinate terms\'\': \'\'Y\'\' is a coordinate term of \'\'X\'\' if \'\'X\'\' and \'\'Y\'\' share a hypernym (\'\'wolf\'\' is a coordinate term of \'\'dog\'\', and \'\'dog\'\' is a coordinate term of \'\'wolf\'\')\n**\'\'[[meronymy|meronym]]\'\': \'\'Y\'\' is a meronym of \'\'X\'\' if \'\'Y\'\' is a part of \'\'X\'\' (\'\'window\'\' is a meronym of \'\'building\'\')\n**\'\'[[holonymy|holonym]]\'\': \'\'Y\'\' is a holonym of \'\'X\'\' if \'\'X\'\' is a part of \'\'Y\'\' (\'\'building\'\' is a holonym of \'\'window\'\')\n* [[Verb]]s\n**\'\'hypernym\'\': the verb \'\'Y\'\' is a hypernym of the verb \'\'X\'\' if the activity \'\'X\'\' is a (kind of) \'\'Y\'\' (\'\'to perceive\'\' is an hypernym of \'\'to listen\'\')\n**\'\'[[troponym]]\'\': the verb \'\'Y\'\' is a troponym of the verb \'\'X\'\' if the activity \'\'Y\'\' is doing \'\'X\'\' in some manner (\'\'to lisp\'\' is a troponym of \'\'to talk\'\')\n**\'\'[[entailment]]\'\': the verb \'\'Y\'\' is entailed by \'\'X\'\' if by doing \'\'X\'\' you must be doing \'\'Y\'\' (\'\'to sleep\'\' is entailed by \'\'to snore\'\')\n**\'\'coordinate terms\'\': those verbs sharing a common hypernym (\'\'to lisp\'\' and \'\'to yell\'\')\n\nThese semantic relations hold among all members of the linked synsets. Individual synset members (words) can also be connected with  lexical relations. For example, (one sense of) the noun "director" is linked to (one sense of) the verb "direct" from which it is derived via a "morphosemantic" link.\n\nThe morphology functions of the software distributed with the database try to deduce the [[Lemma (morphology)|lemma]] or [[stem (linguistics)|stem]] form of a [[word]] from the user\'s input. Irregular forms are stored in a list, and looking up "ate" will return "eat," for example.\n\n== Knowledge structure ==\nBoth nouns and verbs are organized into hierarchies, defined by [[hypernym]] or \'\'[[is-a|IS A]]\'\' relationships. For instance, one sense of the word \'\'dog\'\' is found following hypernym hierarchy; the words at the same level represent synset members.  Each set of synonyms has a unique index.\n{{tree list}}\n* {{Tree list/final branch}}dog, domestic dog, Canis familiaris\n** {{Tree list/final branch}}canine, canid\n*** {{Tree list/final branch}}carnivore\n**** {{Tree list/final branch}}placental, placental mammal, eutherian, eutherian mammal\n***** {{Tree list/final branch}}mammal\n****** {{Tree list/final branch}}vertebrate, craniate\n******* {{Tree list/final branch}}chordate\n******** {{Tree list/final branch}}animal, animate being, beast, brute, creature, fauna\n********* {{Tree list/final branch}}...\n{{tree list/end}}\nAt the top level, these hierarchies are organized into 25 beginner "trees" for nouns and 15 for verbs (called \'\'lexicographic files\'\' at a maintenance level).  All are linked to a unique beginner synset, "entity."\nNoun hierarchies are far deeper than verb hierarchies\n\nAdjectives are not organized into hierarchical trees. Instead, two "central" antonyms such as "hot" and "cold" form binary poles, while \'satellite\' synonyms such as "steaming" and "chilly" connect to their respective poles via a "similarity" relations. The adjectives can be visualized in this way as "dumbbells" rather than as "trees."\n\n== Psycholinguistic aspects of WordNet ==\n\nThe initial goal of the WordNet project was to build a lexical database that would be consistent with theories of human semantic memory developed in the late 1960s.  Psychological experiments indicated that speakers organized their knowledge of concepts in an economic, hierarchical fashion. Retrieval time required to access conceptual knowledge seemed to be directly related to the number of hierarchies the speaker needed to "traverse" to access the knowledge. Thus, speakers could more quickly verify that \'\'canaries can sing\'\' because a canary is a songbird ("sing" is a property stored on the same level as "canary"), but required slightly more time to verify that \'\'canaries can fly\'\' (where they had to access the concept "bird" on the superordinate level) and even more time to verify \'\'canaries have skin\'\' (requiring look-up across multiple levels of hyponymy, up to "animal").<ref>Collins A., Quillian M. R. 1972. Experiments on Semantic Memory and Language Comprehension. In \'\'Cognition in Learning and Memory\'\'. Wiley, New York.</ref>\nWhile such experiments and the underlying theories have been subject to criticism, some of WordNet\'s organization is consistent with experimental evidence. For example, [[anomic aphasia]] selectively affects speakers\' ability to produce words from a specific semantic category, a WordNet hierarchy. Antonymous adjectives (WordNet\'s central adjectives in the dumbbell structure) are found to co-occur far more frequently than chance, a fact that has been found to hold for many languages.\n\n== WordNet as a lexical ontology ==\n\nWordNet is sometimes called an ontology, a persistent claim that its creators do not make. The hypernym/hyponym relationships among the noun synsets can be interpreted as specialization relations among conceptual categories. In other words, WordNet can be interpreted and used as a lexical [[ontology (computer science)|ontology]] in the [[computer science]] sense. However, such an ontology should normally be corrected before being used since it contains hundreds of basic semantic inconsistencies such as (i) the existence of common specializations for exclusive categories and (ii) redundancies in the specialization hierarchy. Furthermore, transforming WordNet into a lexical ontology usable for knowledge representation should normally also involve (i)&nbsp;distinguishing the specialization relations into \'\'subtypeOf\'\' and \'\'instanceOf\'\' relations, and (ii)&nbsp;associating intuitive unique identifiers to each category. Although such corrections and transformations have been performed and documented as part of the integration of WordNet&nbsp;1.7 into the cooperatively updatable knowledge base of WebKB-2,<ref>{{cite web|author=http://www.phmartin.info |url=http://www.webkb.org/doc/wn/ |title=Integration of WordNet 1.7 in WebKB-2|publisher=Webkb.org |date= |accessdate=2014-03-11}}</ref> most projects claiming to re-use WordNet for knowledge-based applications (typically, knowledge-oriented information retrieval) simply re-use it directly.\n\nWordNet has also been converted to a formal specification, by means of a hybrid bottom-up top-down methodology to automatically extract association relations from WordNet, and interpret these associations in terms of a set of conceptual relations, formally defined in the [[Upper ontology (computer science)#DOLCE and DnS|DOLCE foundational ontology]].<ref>{{cite book |first1=A. |last1=Gangemi |first2=R. |last2=Navigli |first3=P. |last3=Velardi |url=http://www.w3.org/2001/sw/BestPractices/WNET/ODBASE-OWN.pdf |format=PDF |title=The OntoWordNet Project: Extension and Axiomatization of Conceptual Relations in WordNet |work= Proc. of International Conference on Ontologies, Databases and Applications of SEmantics (ODBASE 2003) |location=Catania, Sicily (Italy) |year=2003 |pages= 820–838}}</ref>\n\nIn most works that claim to have integrated WordNet into ontologies, the content of WordNet has not simply been corrected when it seemed necessary; instead, WordNet has been heavily re-interpreted and updated whenever suitable. This was the case when, for example, the top-level ontology of WordNet was re-structured<ref>{{cite conference | first1 = A. | last1 = Oltramari | first2 = A. | last2 = Gangemi | first3 = N. | last3 = Guarino | first4 = C. | last4 = Masolo | date = 2002 | title = Restructuring WordNet\'s Top-Level: The OntoClean approach | citeseerx = 10.1.1.19.6574 | conference = OntoLex\'2 Workshop, Ontologies and Lexical Knowledge Bases (LREC 2002) | location = Las Palmas, Spain | pages = 17–26 }}</ref> according to the [[OntoClean]] based approach or when WordNet was used as a primary source for constructing the lower classes of the SENSUS ontology.\n\n== Limitations ==\n\nWordNet does not include information about the [[etymology]] or the pronunciation of words and it contains only limited information about usage.\nWordNet aims to cover most of everyday English and does not include much domain-specific terminology.\n\nWordNet is the most commonly used computational lexicon of English for [[word sense disambiguation]] (WSD), a task aimed to assigning the context-appropriate meanings (i.e. synset members) to words in a text.<ref>R. Navigli. [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], \'\'ACM Computing Surveys\'\', 41(2), 2009, pp. 1–69</ref> However, it has been argued that WordNet encodes sense distinctions that are too fine-grained. This issue prevents WSD systems from achieving a level of performance comparable to that of humans, who do not always agree when confronted with the task of selecting a sense from a dictionary that matches a word in a context. The granularity issue has been tackled by proposing [[cluster analysis|clustering]] methods that automatically group together similar senses of the same word.<ref>E. Agirre, O. Lopez. 2003.\nClustering WordNet Word Senses. In \'\'Proc. of the Conference on Recent Advances on Natural Language (RANLP’03)\'\', Borovetz, Bulgaria, pp. 121–130.</ref><ref>R. Navigli. [http://acl.ldc.upenn.edu/P/P06/P06-1014.pdf Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance], In \'\'Proc. of the 44th Annual Meeting of the Association for Computational Linguistics joint with the 21st International Conference on Computational Linguistics (COLING-ACL 2006)\'\', Sydney, Australia, July 17-21st, 2006, pp. 105–112.</ref><ref>R. Snow, S. Prakash, D. Jurafsky, A. Y. Ng. 2007. [http://www.aclweb.org/anthology/D/D07/D07-1107.pdf Learning to Merge Word Senses], \'\'In Proc. of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)\'\', Prague, Czech Republic, pp. 1005–1014.</ref>\n\n=== Licensed vs. Open WordNets ===\nSome wordnets were subsequently created for other languages. A 2012 survey lists the wordnets and their availability.<ref>Francis Bond and Kyonghee Paik 2012a. [http://web.mysites.ntu.edu.sg/fcbond/open/pubs/2012-gwc-wn-license.pdf A survey of wordnets and their licenses]. In Proceedings of the 6th Global WordNet Conference (GWC 2012). Matsue. 64–71</ref> In an effort to propagate the usage of WordNets, the Global WordNet community had been slowly re-licensing their WordNets to an open domain where researchers and developers can easily access and use WordNets as language resources to provide [[ontology|ontological]] and [[lexicon|lexical]] knowledge in [[Natural Language Processing]] tasks.\n\nThe Open Multilingual WordNet<ref>http://compling.hss.ntu.edu.sg/omw/</ref> provides access to [[Open-source license|open licensed]] wordnets in a variety of languages, all linked to the Princeton Wordnet of English (PWN). The goal is to make it easy to use wordnets in multiple languages.\n\n== Applications ==\n\nWordNet has been used for a number of different purposes in information systems, including [[word-sense disambiguation]], [[information retrieval]], [[Document classification|automatic text classification]], [[Automatic summarization|automatic text summarization]], [[machine translation]] and even automatic crossword puzzle generation.\n\nA common use of WordNet is to determine the [[semantic similarity|similarity]] between words. Various algorithms have been proposed, and these include measuring the distance among the words and synsets in WordNet\'s graph structure, such as by counting the number of edges among synsets. The intuition is that the closer two words or synsets are, the closer their meaning. A number of WordNet-based word similarity algorithms are implemented in a [[Perl]] package called WordNet::Similarity,<ref>{{cite web|url=http://www.d.umn.edu/~tpederse/similarity.html |title=Ted Pedersen - WordNet::Similarity |publisher=D.umn.edu |date=2008-06-16 |accessdate=2014-03-11}}</ref> and in a [[Python (programming language)|Python]] package called [[NLTK]].\nOther more sophisticated WordNet-based similarity techniques include ADW,<ref>M. T. Pilehvar, D. Jurgens and R. Navigli.  [http://wwwusers.di.uniroma1.it/~navigli/pubs/ACL_2013_Pilehvar_Jurgens_Navigli.pdf Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity.]. Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), Sofia, Bulgaria, August 4–9, 2013, pp. 1341-1351.</ref> whose implementation is available in [[Java (programming language)|Java]]. WordNet can also be used to inter-link other vocabularies.<ref>{{cite journal  |vauthors=Ballatore A, etal |volume=20|issue=2| arxiv=1404.5372| journal=Annals of GIS |title=Linking geographic vocabularies through WordNet |publisher= |date=2014}}</ref>\n\n== Interfaces ==\nPrinceton maintains a list of related projects<ref>{{cite web|url=http://wordnet.princeton.edu/wordnet/related-projects/ |title=Related projects - WordNet - Related projects |publisher=Wordnet.princeton.edu |date=2014-01-06 |accessdate=2014-03-11}}</ref> that includes links to some of the widely used [[application programming interface]]s available for accessing WordNet using various programming languages and environments.\n\n== Related projects and extensions ==\nWordNet is connected to several databases of the [[Semantic Web]]. WordNet is also commonly re-used via mappings between the WordNet synsets and the categories from ontologies. Most often, only the top-level categories of WordNet are mapped.\n\n===Global WordNet Association===\nThe Global WordNet Association (GWA)<ref>{{cite web|author=The Global WordNet Association |url=http://www.globalwordnet.org/ |title=globalwordnet.org |publisher=globalwordnet.org |date=2010-02-04 |accessdate=2014-03-11}}</ref> is a public and non-commercial organization that provides a platform for discussing, sharing and connecting wordnets for all languages in the world. The GWA also promotes the standardization of wordnets across different languages to ensure its uniformity in enumerating the different synsets in human languages. The GWA keeps a list of wordnets developed around the world.<ref>{{cite web|title=Wordnets in the World|url=http://www.globalwordnet.org/gwa/wordnet_table.html|archiveurl=https://web.archive.org/web/20111021114613/http://www.globalwordnet.org/gwa/wordnet_table.html |archivedate=2011-10-21}}</ref>\n\n===Other languages===\n* [[Arabic WordNet]]:<ref>Black W., Elkateb S., Rodriguez H., Alkhalifa M., Vossen P., Pease A., Bertran M., Fellbaum C., (2006) The Arabic WordNet Project, Proceedings of LREC 2006</ref><ref>Lahsen Abouenour, Karim Bouzoubaa, Paolo Rosso (2013) On the evaluation and improvement of Arabic WordNet coverage and usability, Language Resources and Evaluation 47(3) pp 891–917</ref> WordNet for Arabic language.\n* [[Malayalam WordNet]] , developed by [[Cochin University of Science and Technology|Cochin University Of Science and Technology]]\n* CWN (Chinese Wordnet or 中文詞彙網路) supported by [[National Taiwan University]].<ref>[http://lope.linguistics.ntu.edu.tw/cwn/ Chinese Wordnet (中文詞彙網路) official page] at National Taiwan University</ref>\n* WOLF (WordNet Libre du Français), a French version of WordNet.<ref>S. Benoît, F. Darja. 2008. [http://alpage.inria.fr/~sagot/pub/Ontolex08.pdf Building a free French wordnet from multilingual resources]. In \'\'Proc. of Ontolex 2008\'\', Marrakech, Maroc.</ref>\n* JAWS (Just Another WordNet Subset), another French version of WordNet<ref>C. Mouton, G. de Chalendar. 2010.[http://www.iro.umontreal.ca/~felipe/TALN2010/Xml/Papers/all/taln2010_submission_71.pdf JAWS : Just Another WordNet Subset]. In \'\'Proc. of TALN 2010\'\'.</ref> built using the Wiktionary and semantic spaces\n* The [[IndoWordNet]]<ref name="PushpakBhattacharyya">Pushpak Bhattacharyya, IndoWordNet, Lexical Resources Engineering Conference 2010 (LREC 2010), Malta, May, 2010.</ref> is a linked lexical knowledge base of wordnets of 18 scheduled languages of India.\n* The MultiWordNet project,<ref>E. Pianta, L. Bentivogli, C. Girardi. 2002. [http://multiwordnet.itc.it/paper/MWN-India-published.pdf MultiWordNet: Developing an aligned multilingual database]. In \'\'Proc. of the 1st International Conference on Global WordNet\'\', Mysore, India, pp. 21–25.</ref> a multilingual WordNet aimed at producing an Italian WordNet strongly aligned with the Princeton WordNet.\n* The [[EuroWordNet]] project<ref>P. Vossen, Ed. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer, Dordrecht, The Netherlands.</ref> has produced WordNets for several European languages and linked them together; these are not freely available however. The Global Wordnet project attempts to coordinate the production and linking of "wordnets" for all languages.<ref>{{cite web|url=http://www.globalwordnet.org/ |title=The Global WordNet Association |publisher=Globalwordnet.org |date=2010-02-04 |accessdate=2014-01-05}}</ref> [[Oxford University Press]], the publisher of the [[Oxford English Dictionary]], has voiced plans to produce their own online competitor to WordNet.{{Citation needed|date=May 2009}}\n* The BalkaNet project<ref>D. Tufis, D. Cristea, S. Stamou. 2004. [http://www.racai.ro/~tufis/papers/Tufis-CS-ROMJIST2004.pdf Balkanet: Aims, methods, results and perspectives. A general overview]. \'\'Romanian J. Sci. Tech. Inform. (Special Issue on Balkanet)\'\', 7(1-2), pp. 9–43.</ref> has produced WordNets for six European languages (Bulgarian, Czech, Greek, Romanian, Turkish and Serbian). For this project, a freely available XML-based WordNet editor was developed. This editor – VisDic – is not in active development anymore, but is still used for the creation of various WordNets. Its successor, DEBVisDic, is client-server application and is currently used for the editing of several WordNets (Dutch in Cornetto project, Polish, Hungarian, several African languages, Chinese).\n* UWN is an automatically constructed multilingual lexical knowledge base extending WordNet to cover over a million words in many different languages.<ref>{{cite web|url=http://www.mpi-inf.mpg.de/yago-naga/uwn |title=UWN: Towards a Universal Multilingual Wordnet - D5: Databases and Information Systems (Max-Planck-Institut für Informatik) |publisher=Mpi-inf.mpg.de |date=2011-08-14 |accessdate=2014-01-05}}</ref>\n* Such projects as BalkaNet and EuroWordNet made it feasible to create standalone wordnets linked to the original one. One of such projects is Russian WordNet patronized by [[Petersburg State University of Means of Communication]]<ref>{{cite web|url=http://www.pgups.ru/abitur/inostrancam/inter/ruwordnet/ |title=Русский WordNet |publisher=Pgups.ru |date= |accessdate=2014-01-05}}</ref> or Russnet<ref>{{cite web|url=http://project.phil.spbu.ru/RussNet/index_ru.shtml |title=RussNet: Главная страница |publisher=Project.phil.spbu.ru |date= |accessdate=2014-03-11}}</ref> by [[Saint Petersburg State University]]\n* FinnWordNet is a Finnish version of the WordNet where all entries of the original English WordNet were translated.<ref>{{cite web|url=http://www.ling.helsinki.fi/en/lt/research/finnwordnet/ |title=FinnWordNet – The Finnish WordNet - Department of General Linguistics |publisher=Ling.helsinki.fi |date= |accessdate=2014-01-05}}</ref>\n* [[GermaNet]] is a German version of the WordNet developed by the University of Tübingen.<ref>{{cite web|url=http://www.sfs.uni-tuebingen.de/lsd/index.shtml |title=GermaNet |publisher=Sfs.uni-tuebingen.de |date= |accessdate=2014-03-11}}</ref>\n* OpenWN-PT  is a Brazilian Portuguese version of the original WordNet freely available for download under CC-BY-SA license.<ref>{{cite web|url=https://github.com/arademaker/openWordnet-PT |title=arademaker/openWordnet-PT — GitHub |publisher=Github.com |date= |accessdate=2014-01-05}}</ref>\n* [[plWordNet]]<ref>http://plwordnet.pwr.wroc.pl/wordnet/ official webpage</ref> is a Polish-language version of WordNet developed by [[Wrocław University of Technology]].\n* PolNet<ref>http://www.ltc.amu.edu.pl/polnet/ official webpage</ref> is a Polish-language version of WordNet developed by [[Adam Mickiewicz University in Poznań]] (distributed under CC BY-NC-ND 3.0 license).\n* [[BulNet]] is a Bulgarian version of the WordNet developed at the Department of Computational Linguistics of the [[Institute for Bulgarian Language]], Bulgarian Academy of Sciences.<ref>{{cite web|url=http://dcl.bas.bg/BulNet/general_en.html |title=BulNet |publisher=dcl.bas.bg |date= |accessdate=2015-05-07}}</ref>\n\n===Linked data===\n\n* [[BabelNet]],<ref>R. Navigli, S. P. Ponzetto. [http://www.aclweb.org/anthology/P/P10/P10-1023.pdf BabelNet: Building a Very Large Multilingual Semantic Network]. Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July 11–16, 2010, pp. 216–225.</ref> a very large multilingual [[semantic network]] with millions of concepts obtained from an integration of WordNet and Wikipedia based on an automatic mapping algorithm.\n* The [[Suggested Upper Merged Ontology|SUMO]] ontology<ref>A. Pease, I. Niles, J. Li. 2002. [https://www.aaai.org/Papers/Workshops/2002/WS-02-11/WS02-11-011.pdf The suggested upper merged ontology: A large ontology for the Semantic Web and its applications]. In \'\'Proc. of the AAAI-2002 Workshop on Ontologies and the Semantic Web\'\', Edmonton, Canada.</ref> has produced a mapping between all of the WordNet synsets, (including nouns, verbs, adjectives and adverbs), and [[SUMO class]]es.  The most recent addition of the mappings provides links to all of the more specific terms in the MId-Level Ontology (MILO), which extends SUMO.\n* [[OpenCyc]],<ref>S. Reed and D. Lenat. 2002. [http://www.cyc.com/doc/white_papers/mapping-ontologies-into-cyc_v31.pdf Mapping Ontologies into Cyc]. In \'\'Proc. of AAAI 2002 Conference Workshop on Ontologies For The Semantic Web\'\', Edmonton, Canada, 2002</ref> an open [[ontology (information science)|ontology]] and [[knowledge base]] of everyday common sense knowledge, has 12,000 terms linked to WordNet synonym sets.\n* [[Descriptive Ontology for Linguistic and Cognitive Engineering|DOLCE]],<ref>Masolo, C., Borgo, S., Gangemi, A., Guarino, N., Oltramari, A., Schneider, L.S. 2002. [http://www.loa-cnr.it/Papers/WonderWebD17V2.0.pdf WonderWeb Deliverable D17. The WonderWeb Library of Foundational Ontologies and the DOLCE ontology]. Report (ver. 2.0, 15-08-2002)</ref> is the first module of the WonderWeb Foundational Ontologies Library (WFOL). This upper-ontology has been developed in light of rigorous ontological principles inspired by the philosophical tradition, with a clear orientation toward language and cognition. OntoWordNet<ref>Gangemi, A., Guarino, N., Masolo, C., Oltramari, A. 2003 [http://www.loa-cnr.it/Papers/AIMag24-03-003.pdf Sweetening WordNet with DOLCE]. In AI Magazine 24(3): Fall 2003, pp. 13–24</ref> is the result of an experimental effort to align WordNet\'s upper level with DOLCE. It is suggested that such alignment could lead to an "ontologically sweetened" WordNet, meant to be conceptually more rigorous, cognitively transparent, and efficiently exploitable in several applications.\n* [[DBpedia]],<ref>C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, S. Hellmann, [http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/Bizer-etal-DBpedia-CrystallizationPoint-JWS-Preprint.pdf DBpedia – A crystallization point for the Web of Data]. Web Semantics, 7(3), 2009, pp. 154–165</ref> a database of structured information, is also linked to WordNet.\n* The [[eXtended WordNet]]<ref>S. M. Harabagiu, G. A. Miller, D. I. Moldovan. 1999. [http://www.ldc.upenn.edu/acl/W/W99/W99-0501.pdf WordNet 2 – A Morphologically and Semantically Enhanced Resource]. In \'\'Proc. of the ACL SIGLEX Workshop: Standardizing Lexical Resources\'\', pp. 1–8.</ref> is a project at the [[University of Texas at Dallas]] which aims to improve WordNet by semantically parsing the glosses, thus making the information contained in these definitions available for automatic knowledge processing systems. It is also freely available under a license similar to WordNet\'s.\n* The [[GCIDE]] project produced a dictionary by combining a [[public domain]] \'\'[[Webster\'s Dictionary]]\'\' from 1913 with some WordNet definitions and material provided by volunteers. It was released under the [[copyleft]] license [[GNU General Public License|GPL]].\n* [[ImageNet]] is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.<ref>J. Deng, W. Dong, R. Socher, L. Li, K. Li, L. Fei-Fei. [https://nlpainter.googlecode.com/svn-history/r16/trunk/papers/ImageNet__cvpr09.pdf ImageNet: A Large-Scale Hierarchical Image Database]. In \'\'Proc. of 2009 IEEE Conference on Computer Vision and Pattern Recognition\'\'</ref> Currently it has an average of over five hundred images per node.\n* BioWordnet, a biomedical extension of wordnet was abandoned due to issues about stability over versions.<ref>M. Poprat, E. Beisswanger, U. Hahn. 2008. [http://www.aclweb.org/anthology/W/W08/W08-0507.pdf Building a BIOWORDNET by Using WORDNET’s Data Formats and WORDNET’s Software Infrastructure – A Failure Story]. In \'\'Proc. of the Software Engineering, Testing, and Quality Assurance for Natural Language Processing Workshop\'\', pp. 31–39.</ref>\n* WikiTax2WordNet, a mapping between WordNet synsets and [[Wikipedia:Categorization|Wikipedia categories]].<ref>S. Ponzetto, R. Navigli. [http://ijcai.org/papers09/Papers/IJCAI09-343.pdf Large-Scale Taxonomy Mapping for Restructuring and Integrating Wikipedia], In \'\'Proc. of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009)\'\', Pasadena, California, July 14-17th, 2009, pp. 2083–2088.</ref>\n* WordNet++, a resource including over millions of semantic edges harvested from Wikipedia and connecting pairs of WordNet synsets.<ref>S. P. Ponzetto, R. Navigli. [http://aclweb.org/anthology-new/P/P10/P10-1154.pdf Knowledge-rich Word Sense Disambiguation rivaling supervised systems]. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), 2010, pp. 1522–1531.</ref>\n* SentiWordNet, a resource for supporting opinion mining applications obtained by tagging all the WordNet 3.0 synsets according to their estimated degrees of positivity, negativity, and neutrality.<ref>S. Baccianella, A. Esuli and F. Sebastiani. [http://nemis.isti.cnr.it/sebastiani/Publications/LREC10.pdf SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining]. In Proceedings of the 7th Conference on Language Resources and Evaluation (LREC\'10), Valletta, MT, 2010, pp. 2200–2204.</ref>\n* ColorDict, is an Android application to mobiles phones that use Wordnet database and others, like Wikipedia.\n* [[UBY-LMF]] a database of 10 resources including WordNet.\n\n===Related projects===\n* [[FrameNet]] is a lexical database that shares some similarities with, and refers to, WordNet. \n* [[Lexical markup framework]] (LMF) is an ISO standard specified within [[ISO/TC37]] in order to define a common standardized framework for the construction of lexicons, including WordNet. The subset of LMF for Wordnet is called Wordnet-LMF. An instantiation has been made within the KYOTO project.<ref>Piek Vossen, Claudia Soria, Monica Monachini: Wordnet-LMF: a standard representation for multilingual wordnets, in \'\'LMF Lexical Markup Framework\'\', edited by Gil Francopoulo ISTE / Wiley 2013 (ISBN 978-1-84821-430-9)</ref>\n* [[Universal Networking Language|UNL Programme]] is a project under the auspices of [[United Nations|UNO]] aimed to consolidate lexicosemantic data of many languages to be used in machine translation and information extraction systems.\n\n==Distributions==\n* [[Babylon (software)|Babylon]]<ref>{{cite web|url=http://www.babylon.com/free-dictionaries/reference/encyclopedias/WordNet-2.0/42406.html |title=Babylon WordNet |publisher=Babylon.com |date= |accessdate=2014-03-11}}</ref>\nWordNet Database is distributed as a dictionary package (usually a single file) for the following software:\n* [[GoldenDict]]<ref>{{cite web|url=http://sourceforge.net/projects/goldendict/files/dictionaries |title=GoldenDict - Browse /dictionaries at Sourceforge.net |publisher=Sourceforge.net |date=2010-12-01 |accessdate=2014-01-05}}</ref>\n* [[Lingoes (program)|Lingoes]]<ref>{{cite web|url=http://www.lingoes.net/en/dictionary/dict_down.php?id=12D98EC3940843498672A92149455292 |title=Lingoes WordNet |publisher=Lingoes.net |date=2007-11-16 |accessdate=2014-03-11}}</ref>\n\n== See also ==\n* [[Lexical Markup Framework]]\n* [[Machine-readable dictionary]]\n* [[Synonym Ring]]\n* [[Taxonomy (general)|Taxonomy]]\n* [[ThoughtTreasure]]\n\n== References ==\n{{Reflist|2}}\n\n== External links ==\n* {{Official website|http://wordnet.princeton.edu/ }}\n{{Lexicography}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Wordnet}}\n[[Category:English dictionaries]]\n[[Category:Lexical databases]]\n[[Category:Knowledge representation]]\n[[Category:Computational linguistics]]\n[[Category:Online dictionaries]]\n[[Category:Open data]]\n[[Category:Thesauri]]']
['Mathematical model', '20590', '{{Distinguish2|the same term used in [[model theory]], a branch of [[mathematical logic]].}}\n{{Refimprove|date=May 2008}} \nA \'\'\'mathematical model\'\'\' is a description of a [[system]] using [[mathematics|mathematical]] concepts and [[Language of mathematics|language]]. The process of developing a mathematical model is termed \'\'\'mathematical modeling\'\'\'. Mathematical models are used in the [[natural science]]s (such as [[physics]], [[biology]], [[earth science]], [[meteorology]]) and [[engineering]] disciplines (such as [[computer science]], [[artificial intelligence]]), as well as in the [[social sciences]] (such as [[economics]], [[psychology]], [[sociology]], [[political science]]).  [[Physicist]]s, [[engineer]]s, [[statistician]]s, [[operations research]] analysts, and [[economist]]s use mathematical models most extensively. A model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.\n\n== Elements of a mathematical model ==\n\nMathematical models can take many forms, including [[dynamical systems]], [[statistical model]]s, [[differential equations]], or [[Game theory|game theoretic models]].  These and other types of models can overlap, with a given  model involving a variety of abstract structures. In general, mathematical models may include [[model theory|logical models]].  In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments.  Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.\n\nIn the [[physical sciences]], the traditional mathematical model contains four major elements. These are\n# [[Governing equation]]s\n# [[Defining equation (physics)|Defining equation]]s\n# [[Constitutive equation]]s\n# [[Constraint (mathematics)|Constraint]]s\n\n== Classifications ==\nMathematical models are usually composed of relationships and \'\'[[variable (mathematics)|variables]]\'\'. Relationships can be described by \'\'[[Operator (mathematics)|operators]]\'\', such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system [[parameter (computer programming)|parameters]] of interest, that can be [[Quantification (science)|quantified]]. Several classification criteria can be used for mathematical models according to their structure:\n* \'\'\'Linear vs. nonlinear:\'\'\' If all the operators in a mathematical model exhibit [[linear]]ity, the resulting mathematical model is defined as linear. A model is considered to be nonlinear otherwise. The definition of linearity and nonlinearity is dependent on context, and linear models may have nonlinear expressions in them.  For example, in a [[Linear model|statistical linear model]], it is assumed that a relationship is linear in the parameters, but it may be nonlinear in the predictor variables.  Similarly, a differential equation is said to be linear if it can be written with linear [[differential operator]]s, but it can still have nonlinear expressions in it.  In a [[Optimization (mathematics)|mathematical programming]] model, if the objective functions and constraints are represented entirely by [[linear equation]]s, then the model is regarded as a linear model.  If one or more of the objective functions or constraints are represented with a [[nonlinearity|nonlinear]] equation, then the model is known as a nonlinear model.<br>Nonlinearity, even in fairly simple systems, is often associated with phenomena such as [[Chaos theory|chaos]] and [[irreversibility]].  Although there are exceptions, nonlinear systems and models tend to be more difficult to study than linear ones.  A common approach to nonlinear problems is [[linearization]], but this can be problematic if one is trying to study aspects such as irreversibility, which are strongly tied to nonlinearity.\n* \'\'\'Static vs. dynamic:\'\'\' A \'\'dynamic\'\' model  accounts for time-dependent changes in the state of the system, while a \'\'static\'\' (or steady-state) model calculates the system in equilibrium, and thus is time-invariant.  Dynamic models typically are represented by [[differential equation]]s or [[difference equation]]s.\n* \'\'\'Explicit vs. implicit:\'\'\' If all of the input parameters of the overall model are known, and the output parameters can be calculated by a finite series of computations, the model is said to be \'\'explicit\'\'. But sometimes it is the \'\'output\'\' parameters which are known, and the corresponding inputs must be solved for by an iterative procedure, such as [[Newton\'s method]] (if the model is linear) or [[Broyden\'s method]] (if non-linear). In such a case the model is said to be \'\'implicit\'\'. For example, a [[jet engine]]\'s physical properties such as turbine and nozzle throat areas can be explicitly calculated given a design [[thermodynamic cycle]] (air and fuel flow rates, pressures, and temperatures) at a specific flight condition and power setting, but the engine\'s operating cycles at other flight conditions and power settings cannot be explicitly calculated from the constant physical properties.\n* \'\'\'Discrete vs. continuous:\'\'\' A [[discrete modeling|discrete model]] treats objects as discrete, such as the particles in a [[molecular model]] or the states in a [[statistical model]]; while a [[continuous model]] represents the objects in a continuous manner, such as the velocity field of fluid in pipe flows, temperatures and stresses in a solid, and electric field that applies continuously over the entire model due to a point charge.\n* \'\'\'Deterministic vs. probabilistic (stochastic):\'\'\' A [[deterministic system|deterministic]] model is one in which every set of variable states is uniquely determined by parameters in the model and by sets of previous states of these variables; therefore, a deterministic model always performs the same way for a given set of initial conditions. Conversely, in a stochastic model—usually called a "[[statistical model]]"—randomness is present, and variable states are not described by unique values, but rather by [[probability]] distributions.\n* \'\'\'Deductive, inductive, or floating:\'\'\' A deductive model is a logical structure based on a theory. An inductive model arises from empirical findings and generalization from them. The floating model rests on neither theory nor observation, but is merely the invocation of expected structure. Application of mathematics in social sciences outside of economics has been criticized for unfounded models.<ref>{{cite book |authorlink=Stanislav Andreski |first=Stanislav |last=Andreski |year=1972 |title=Social Sciences as Sorcery |publisher=[[St. Martin’s Press]] |isbn=0-14-021816-5 }}</ref> Application of [[catastrophe theory]] in science has been characterized as a floating model.<ref>{{cite book |authorlink=Clifford Truesdell |first=Clifford |last=Truesdell |year=1984 |title=An Idiot’s Fugitive Essays on Science |pages=121–7 |publisher=Springer |isbn=3-540-90703-3 }}</ref>\n\n== Significance in the natural sciences ==\nMathematical models are of great importance in the natural sciences, particularly in [[physics]]. Physical [[theory|theories]] are almost invariably expressed using mathematical models.\n\nThroughout history, more and more accurate mathematical models have been developed. [[Newton\'s laws of motion|Newton\'s laws]] accurately describe many everyday phenomena, but at certain limits [[relativity theory]] and [[quantum mechanics]] must be used; even these do not apply to all situations and need further refinement. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the [[speed of light]]. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the [[de Broglie wavelength]] of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.\n\nIt is common to use idealized models in physics to simplify things. Massless ropes, point particles, [[ideal gases]] and the [[particle in a box]] are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton\'s laws, [[Maxwell\'s equations]] and the [[Schrödinger equation]]. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by [[molecular orbital]] models that are approximate solutions to the Schrödinger equation. In [[engineering]], physics models are often made by mathematical methods such as [[finite element analysis]].\n\nDifferent mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. [[Euclidean geometry]] is much used in classical physics, while [[special relativity]] and [[general relativity]] are examples of theories that use [[geometry|geometries]] which are not Euclidean.\n\n== Some applications==<!--whatever this section is, it isn\'t background (original heading), and isn\'t general-->\nSince [[prehistory|prehistorical times]] simple models such as [[map]]s and [[Mathematical diagram|diagrams]] have been used.\n\nOften when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in [[simulation]]s.\n\nA mathematical model usually describes a system by a [[Set (mathematics)|set]] of variables and a set of equations that establish relationships between the variables. Variables may be of many types; [[Real number|real]] or [[integer]] numbers, [[Boolean data type|boolean]] values or [[String (computing)|strings]], for example. The variables represent some properties of the system, for example, measured system outputs often in the form of [[Signal (electronics)|signals]], [[Chronometry|timing data]], counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.\n\n== Building blocks ==\nIn [[business]] and [[engineering]], mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: [[decision theory|decision variables]], [[state variable]]s, [[Exogeny|exogenous]] variables, and [[random variable]]s.\n\nDecision variables are sometimes known as independent variables.  Exogenous variables are sometimes known as [[parameter]]s or [[constant (mathematics)|constant]]s.\nThe variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables.  Furthermore, the output variables are dependent on the state of the system (represented by the state variables).\n\n[[Goal|Objective]]s and [[constraint (mathematics)|constraint]]s of the system and its users can be represented as [[function (mathematics)|function]]s of the output variables or state variables.  The [[objective function]]s will depend on the perspective of the model\'s user.  Depending on the context, an objective function is also known as an \'\'index of performance\'\', as it is some measure of interest to the user.  Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.\n\nFor example, [[economist]]s often apply [[linear algebra]] when using [[input-output model]]s. Complicated mathematical models that have many variables may be consolidated by use of [[vector space|vectors]] where one symbol represents several variables.\n\n== A priori information ==\n[[File:Blackbox3D-withGraphs.png|thumb|480px|To analyse something with a typical "black box approach", only  the behavior of the stimulus/response will be accounted for, to infer the (unknown) \'\'box\'\'. The usual representation of this \'\'black box system\'\'  is a [[data flow diagram]] centered in the box.]]\n\nMathematical modeling problems are often classified into [[black box]] or [[White box (software engineering)|white box]] models, according to how much [[a priori (philosophy)|a priori]] information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.\n\nUsually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an [[exponential decay|exponentially decaying]] function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.\n\nIn black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are [[neural networks]] which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of [[nonlinear system identification]] <ref name="SAB1">Billings S.A. (2013), \'\'Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains\'\', Wiley.</ref> can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.\n\n=== Subjective information ===\nSometimes it is useful to incorporate subjective information into a mathematical model.  This can be done based on [[Intuition (knowledge)|intuition]], [[experience]], or [[expert opinion]], or based on convenience of mathematical form.  [[Bayesian statistics]] provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a [[prior probability distribution]] (which can be subjective), and then update this distribution based on empirical data.\n\nAn example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads.  After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use.  Incorporation of such subjective information might be important to get an accurate estimate of the probability.\n\n== Complexity ==\n[[File:Mathematical models for complex systems.jpg|300px|right|thumb|This is a schematic representation of three types of mathematical models of complex systems with the level of their mechanistic understanding.]]\nIn general, model complexity involves a trade-off between simplicity and accuracy of the model.  [[Occam\'s razor]] is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable.  While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including [[numerical instability]].  [[Thomas Kuhn]] argues that as science progresses, explanations tend to become more complex before a [[paradigm shift]] offers radical simplification.\n\nFor example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, [[Isaac Newton|Newton\'s]] [[classical mechanics]] is an approximated model of the real world. Still, Newton\'s model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the [[speed of light]], and we study macro-particles only.\n\n== Training ==\nAny model which is not pure white-box contains some [[parameter]]s that can be used to fit the model to the system it is intended to describe. If the modeling is done by a [[neural network]] or other [[machine learning]], the optimization of parameters is called \'\'training\'\', while the optimization of model hyperparameters is called \'\'tuning\'\' and often uses [[cross-validation]]. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by \'\'[[curve fitting]]\'\'.\n\n== Model evaluation ==\nA crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately.  This question can be difficult to answer as it involves several different types of evaluation.\n\n=== Fit to empirical data ===\nUsually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data.  In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters.  An accurate model will closely match the verification data even though these data were not used to set the model\'s parameters. This practice is referred to as [[cross-validation (statistics)|cross-validation]] in statistics.\n\nDefining a [[Metric (mathematics)|metric]] to measure distances between observed and predicted data is a useful tool of assessing model fit.  In statistics, decision theory, and some [[economic model]]s, a [[loss function]] plays a similar role.\n\nWhile it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model.  In general, more mathematical tools have been developed to test the fit of [[statistical model]]s than models involving [[differential equations]].  Tools from [[non-parametric statistics]] can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model\'s mathematical form.\n\n=== Scope of the model ===\nAssessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward.  If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a "typical" set of data.\n\nThe question of whether the model describes well the properties of the system between data points is called [[interpolation]], and the same question for events or data points outside the observed data is called [[extrapolation]].\n\nAs an example of the typical limitations of the scope of a model, in evaluating Newtonian [[classical mechanics]], we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light.  Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.\n\n=== Philosophical considerations ===\nMany types of modeling implicitly involve claims about [[causality]].  This is usually (but not always) true of models involving differential equations.  As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.\n\nAn example of such criticism is the argument that the mathematical models of [[Optimal foraging theory]] do not offer insight that goes beyond the common-sense conclusions of [[evolution]] and other basic principles of ecology.<ref>{{Cite journal | last1 = Pyke | first1 = G. H. | doi = 10.1146/annurev.es.15.110184.002515 | title = Optimal Foraging Theory: A Critical Review | journal = Annual Review of Ecology and Systematics | volume = 15 | pages = 523–575 | year = 1984 | pmid =  | pmc = }}</ref>\n\n== Examples ==\n* One of the popular examples in [[computer science]] is the mathematical models of various machines, an example is the [[deterministic finite automaton]] which is defined as an abstract mathematical concept, but due to the deterministic nature of a DFA, it is implementable in hardware and software for solving various specific problems. For example, the following is a DFA M with a binary alphabet, which requires that the input contains an even number of 0s.\n\n[[File:DFAexample.svg|right|thumb|250px|The [[state diagram]] for \'\'M\'\']]\n\'\'M\'\' = (\'\'Q\'\', Σ, δ, \'\'q\'\'<sub>0</sub>, \'\'F\'\') where\n*\'\'Q\'\' = {\'\'S\'\'<sub>1</sub>, \'\'S\'\'<sub>2</sub>},\n*Σ = {0, 1},\n*\'\'q<sub>0</sub>\'\' = \'\'S\'\'<sub>1</sub>,\n*\'\'F\'\' = {\'\'S\'\'<sub>1</sub>}, and\n*δ is defined by the following [[state transition table]]:\n:{| border="1" cell padding="1" cell spacing="0"\n| || <center>\'\'\'0\'\'\'</center> || <center>\'\'\'1\'\'\'</center>\n|-\n|\'\'\'\'\'S\'\'<sub>1</sub>\'\'\' || \'\'S\'\'<sub>2</sub> || \'\'S\'\'<sub>1</sub>\n|-\n|\'\'\'\'\'S\'\'<sub>2</sub>\'\'\' || \'\'S\'\'<sub>1</sub> || \'\'S\'\'<sub>2</sub>\n|}\n\nThe state \'\'S\'\'<sub>1</sub> represents that there has been an even number of 0s in the input so far, while \'\'S\'\'<sub>2</sub> signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, \'\'M\'\' will finish in state \'\'S\'\'<sub>1</sub>, an accepting state, so the input string will be accepted.\n\nThe language recognized by \'\'M\'\' is the [[regular language]] given by the [[regular expression]] 1*( 0 (1*) 0 (1*) )*, where "*" is the [[Kleene star]], e.g., 1* denotes any non-negative number (possibly zero) of symbols "1".\n\n* Many everyday activities carried out without a thought are uses of mathematical models. A geographical [[map projection]] of a region of the earth onto a small, plane surface  is a model<ref>[http://www.landinfo.com/resources_dictionaryMP.htm landinfo.com, definition of map projection]</ref> which can be used for many purposes such as planning travel.\n* Another simple activity is predicting the position of a vehicle from its initial position, direction and speed of travel, using the equation that distance traveled is the product of time and speed. This is known as [[dead reckoning]] when used more formally. Mathematical modeling in this way does not necessarily require formal mathematics; animals have been shown to use dead reckoning.<ref>{{cite book |last=Gallistel |first= |title=The Organization of Learning |location=Cambridge |publisher=The MIT Press |year=1990 |ISBN=0-262-07113-4 }}</ref><ref>{{Cite journal | last1 = Whishaw | first1 = I. Q. | last2 = Hines | first2 = D. J. | last3 = Wallace | first3 = D. G. | doi = 10.1016/S0166-4328(01)00359-X | title = Dead reckoning (path integration) requires the hippocampal formation: Evidence from spontaneous exploration and spatial learning tasks in light (allothetic) and dark (idiothetic) tests | journal = Behavioural Brain Research | volume = 127 | issue = 1–2 | pages = 49–69 | year = 2001 | pmid =  11718884| pmc = }}</ref>\n* \'\'[[Population]] Growth\'\'. A simple (though approximate) model of population growth is the [[Malthusian growth model]]. A slightly more realistic and largely used population growth model is the [[logistic function]], and its extensions. \n* \'\'Individual-based cellular automata models of [[population]] growth\'\'\n[[File:Logical deterministic individual-based cellular automata model of single species population growth.gif|left|thumb|150px]]\n<br /><br /><br /><br /><br /><br /><br /><br /><br />\n* \'\'Model of a particle  in a potential-field\'\'. In this model we consider a particle as being a point of mass which describes a trajectory in space which is modeled by a function giving its coordinates in space as a function of time. The potential field is given by a function <math>V\\!:\\mathbb{R}^3\\!\\rightarrow\\mathbb{R}</math> and the trajectory, that is a function <math>\\mathbf{r}\\!:\\mathbb{R}\\rightarrow\\mathbb{R}^3</math>, is the solution of the [[differential equation]]:\n\n::<math> -\\frac{\\mathrm{d}^2\\mathbf{r}(t)}{\\mathrm{d}t^2}m=\\frac{\\partial V[\\mathbf{r}(t)]}{\\partial x}\\mathbf{\\hat{x}}+\\frac{\\partial V[\\mathbf{r}(t)]}{\\partial y}\\mathbf{\\hat{y}}+\\frac{\\partial V[\\mathbf{r}(t)]}{\\partial z}\\mathbf{\\hat{z}}, </math>\n\nthat can be written also as:\n\n::<math> m\\frac{\\mathrm{d}^2\\mathbf{r}(t)}{\\mathrm{d}t^2}=-\\nabla V[\\mathbf{r}(t)]. </math>\n\n:Note this model assumes  the particle is a point mass, which is certainly known to be false in many cases in which we use this model; for example, as a model of planetary motion.\n\n* \'\'Model of rational behavior for a consumer\'\'.  In this model we assume a consumer faces a choice of \'\'n\'\' commodities labeled 1,2,...,\'\'n\'\' each with a market price \'\'p\'\'<sub>1</sub>, \'\'p\'\'<sub>2</sub>,..., \'\'p\'\'<sub>\'\'n\'\'</sub>. The consumer is assumed to have a \'\'cardinal\'\' utility function \'\'U\'\' (cardinal in the sense that it assigns numerical values to utilities), depending on the amounts of commodities \'\'x\'\'<sub>1</sub>, \'\'x\'\'<sub>2</sub>,..., \'\'x\'\'<sub>\'\'n\'\'</sub> consumed.  The model further assumes that the consumer has a budget \'\'M\'\' which is used to purchase a vector \'\'x\'\'<sub>1</sub>, \'\'x\'\'<sub>2</sub>,..., \'\'x\'\'<sub>\'\'n\'\'</sub> in such a way as to maximize \'\'U\'\'(\'\'x\'\'<sub>1</sub>, \'\'x\'\'<sub>2</sub>,..., \'\'x\'\'<sub>\'\'n\'\'</sub>).  The problem of rational behavior in this model then becomes an [[Optimization (mathematics)|optimization]] problem, that is:\n:: <math> \\max U(x_1,x_2,\\ldots, x_n) </math>\n:: subject to:\n:: <math> \\sum_{i=1}^n p_i x_i \\leq M.</math>\n:: <math> x_{i} \\geq 0   \\; \\; \\; \\forall i \\in \\{1, 2, \\ldots, n \\} </math>\n: This model has been used in [[general equilibrium theory]], particularly to show existence and [[Pareto efficiency]] of economic equilibria.  However, the fact that this particular formulation assigns \'\'numerical values\'\' to levels of satisfaction is the source of criticism (and even ridicule).  However, it is not an essential ingredient of the theory and again this is an idealization.\n* \'\'[[Neighbour-sensing model]]\'\' explains the [[mushroom]] formation from the initially chaotic [[fungus|fungal]] network.\n* \'\'[[Computer science]]\'\': models in Computer Networks, data models, surface model,...\n* \'\'[[Mechanics]]\'\': movement of rocket model,...\n\nModeling requires selecting and identifying relevant aspects of a situation in the real world.\n\n== See also ==\n{{Portal|Mathematics}}\n{{div col|2}}\n* [[Agent-based model]]\n* [[Cliodynamics]]\n* [[Computer simulation]]\n* [[Conceptual model]]\n* [[Decision engineering]]\n* [[Grey box model]]\n* [[Mathematical biology]]\n* [[Mathematical diagram]]\n* [[Mathematical psychology]]\n* [[Mathematical sociology]]\n* [[Model inversion]]\n* [[Microscale and macroscale models]]\n* [[Statistical Model]]\n* [[System identification]]\n* [[TK Solver]] - Rule Based Modeling\n{{div col end}}\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n\n===Books===\n* Aris, Rutherford [ 1978 ] ( 1994 ). \'\'Mathematical Modelling Techniques\'\', New York: Dover. ISBN 0-486-68131-9\n* Bender, E.A. [ 1978 ] ( 2000 ). \'\'An Introduction to Mathematical Modeling\'\', New York: Dover. ISBN 0-486-41180-X\n* Gershenfeld, N. (1998) \'\'The Nature of Mathematical Modeling\'\', [[Cambridge University Press]] ISBN 0-521-57095-6 .\n* Lin, C.C. & Segel, L.A. ( 1988 ). \'\'Mathematics Applied to Deterministic Problems in the Natural Sciences\'\', Philadelphia: SIAM. ISBN 0-89871-229-7\n\n===Specific applications===\n* [[Korotayev]] A., Malkov A., Khaltourina D. (2006). [http://cliodynamics.ru/index.php?option=com_content&task=view&id=124&Itemid=70 \'\'Introduction to Social Macrodynamics: Compact Macromodels of the World System Growth\'\']. Moscow: [http://urss.ru/cgi-bin/db.pl?cp=&lang=en&blang=en&list=14&page=Book&id=34250 Editorial URSS] ISBN 5-484-00414-4 .\n* {{Cite journal | last1 = Peierls | first1 = R. | doi = 10.1080/00107518008210938 | title = Model-making in physics | journal = Contemporary Physics | volume = 21 | pages = 3–17 | year = 1980 | pmid =  | pmc = |bibcode = 1980ConPh..21....3P }}\n* \'\'[http://anintroductiontoinfectiousdiseasemodelling.com/ An Introduction to Infectious Disease Modelling]\'\' by Emilia Vynnycky and Richard G White.\n\n== External links ==\n\n;General reference\n\n* Patrone, F. [http://www.fioravante.patrone.name/mat/u-u/en/differential_equations_intro.htm Introduction to modeling via differential equations], with critical remarks.\n* [http://plus.maths.org/issue44/package/index.html Plus teacher and student package: Mathematical Modelling.] Brings together all articles on mathematical modeling from \'\'[[Plus Magazine]]\'\', the online mathematics magazine produced by the Millennium Mathematics Project at the University of Cambridge.\n\n;Philosophical\n\n* Frigg, R. and S. Hartmann, [http://plato.stanford.edu/entries/models-science/ Models in Science], in: The Stanford Encyclopedia of Philosophy, (Spring 2006 Edition)\n* Griffiths, E. C. (2010) [https://sites.google.com/a/ncsu.edu/emily-griffiths/whatisamodel.pdf What is a model?]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Mathematical Model}}\n[[Category:Applied mathematics]]\n[[Category:Collective intelligence]]\n[[Category:Conceptual models]]\n[[Category:Knowledge representation]]\n[[Category:Mathematical modeling| ]]\n[[Category:Mathematical terminology]]']
['UMBEL', '48794339', '{{for|the part of a plant|Umbel}}\n{{Paid contributions|date=December 2015}}\n{{Infobox software\n |name = UMBEL\n |logo =\n |screenshot =\n |caption = Upper Mapping and Binding Exchange Layer\n |developer =  Structured Dynamics\n |released = 16 July 2008\n |latest_release_version =  UMBEL 1.50\n |latest_release_date = 10 May 2016<ref>{{cite web |url=http://umbel.org/resources/news/new-major-upgrade-of-umbel-released-version-1.50/\n |title=New, Major Upgrade of UMBEL Released: version 1.50 |publisher=UMBEL Web site |date=11 May 2016}}</ref>\n |operating_system =\n |genre = {{Flatlist|\n* [[Ontology (information science)|Ontology]]\n* [[Semantic Web]]\n* [[Linked Data]]\n* [[Artificial intelligence]]\n}}\n |programming language = {{Flatlist|\n* [[Web Ontology Language|OWL 2]]\n* [[SKOS]]\n}}\n |license = [[Creative Commons licenses|Creative Commons Attribution 3.0]]\n |website = {{URL|umbel.org}}\n}}\n\'\'\'UMBEL\'\'\' (\'\'\'U\'\'\'pper \'\'\'M\'\'\'apping and \'\'\'B\'\'\'inding \'\'\'E\'\'\'xchange \'\'\'L\'\'\'ayer) is a logically organized knowledge [[Graph (discrete mathematics)|graph]] of 34,000 concepts and entity types that can be used in [[information science]] for relating information from disparate sources to one another. The [[Symbol grounding problem|grounding]] of this information occurs by common reference to the permanent [[Uniform Resource Identifier|URIs]] for the UMBEL concepts; the connections within the UMBEL [[Upper ontology (information science)|upper ontology]] enable concepts from sources at different levels of abstraction or specificity to be logically related. Since UMBEL is an [[open source]] extract of the [[Cyc#OpenCyc|OpenCyc]] [[knowledge base]], it can also take advantage of the [[Inference engine|reasoning capabilities]] within [[Cyc]].\n\nUMBEL has two means to promote the [[semantic interoperability]] of information:.<ref>{{cite web\n | url = http://techwiki.umbel.org/index.php/UMBEL_Specification\n | title = UMBEL (Upper Mapping and Binding Exchange Layer)\n | website = UMBEL Web Site\n | access-date = 8 February 2016\n}}</ref> It is:\n\n* An [[ontology (information science)|ontology]] of about 35,000 reference concepts, designed to provide common [[Data mapping|mapping]] points for relating different ontologies or [[Conceptual schema|schema]] to one another, and\n* A [[Controlled vocabulary|vocabulary]] for aiding that ontology mapping, including expressions of likelihood relationships distinct from exact identity or equivalence. This vocabulary is also designed for interoperable [[Ontology (information science)#Domain ontologies and upper ontologies|domain ontologies]].\n\n[[File:LOD Cloud 2014.svg|thumb|400px|Diagram showing [[Linking Open Data]] datasets.  UMBEL is near the hub, below and to the right of the central DBpedia.]]\n\nUMBEL is written in the [[Semantic Web]] languages of [[SKOS]] and [[Web Ontology Language|OWL 2]]. It is a [[Class (set theory)|class]] structure used in [[Linked Data]], along with OpenCyc, [[Yago (database)|YAGO]], and the [[DBpedia]] ontology. Besides data integration, UMBEL has been used to aid concept search,<ref>{{cite book\n|last1= Sah\n|first1= M\n|last2= Wade\n|first2= V\n|year= 2012\n|chapter= A novel concept-based search for the web of data using UMBEL and a fuzzy retrieval model\n|chapter-url= https://www.researchgate.net/profile/Melike_Sah/publication/262218017_A_novel_concept-based_search_for_the_web_of_data_using_UMBEL_and_a_fuzzy_retrieval_model/links/555d9c5808ae8c0cab2ad795.pdf\n|title= The Semantic Web: Research and Applications\n|publisher= Springer\n|location= Berlin Heidelberg\n|publication-date= 27 May 2012\n|pages= 103–118\n}}</ref><ref>{{cite book\n|last1= Sah\n|first1= M\n|last2= Wade\n|first2= V\n|year= 2013\n|chapter= Personalized concept-based search and exploration on the web of data using results categorization\n|chapter-url= http://eswc-conferences.org/sites/default/files/papers2013/sah.pdf\n|title= The Semantic Web: Semantics and Big Data\n|publisher= Springer\n|location= Berlin Heidelberg\n|publication-date= 26 May 2013\n|pages= 532–547\n}}</ref> concept definitions,<ref>{{cite book\n|last1= Ballatore\n|first1= Andrea\n|editor1-first= Harlan\n|editor1-last= Onsrud\n|editor2-first= Werner\n|editor2-last= Kuhn\n|year= 2016\n|chapter= Prolegomena for an Ontology of Place\n|chapter-url= http://eprints.cdlib.org/uc/item/0rw1n045.pdf\n|title= Advancing Geographic Information Systems\n|publisher= GSDI Association Press\n|location= Needham, MA\n|pages= 91–103\n}}</ref> [[Ranking (information retrieval)|query ranking]],<ref>{{cite journal\n| last       = Stecher\n| first      = R\n| last2      = Costache\n| first2     = S\n| last3      = Niederée\n| first3     = C\n| last4      = Nejdl\n| first4     = W\n| date       = 7 Jun 2010\n| title      = Query ranking in information integration\n| url        = https://www.researchgate.net/profile/Stefania_Costache2/publication/220921101_Query_Ranking_in_Information_Integration/links/00b7d516d17814201b000000.pdf\n| journal    = Advanced Information Systems Engineering\n| publisher  = Springer Berlin Heidelberg\n| pages      = 230–235\n}}</ref> ontology integration,<ref>{{cite book\n|last1= Damova\n|first1= M\n|last2= et\n|first2= al.\n|year= 2012\n|editor1-first= Maria Teresa\n|editor1-last= Pazienza\n|chapter= Creation and Integration of Reference Ontologies for Efﬁcient LOD Management\n|url= http://www.igi-global.com/book/semi-automatic-ontology-development/58294\n|title= Semi-Automatic Ontology Development: Processes and Resources\n|publisher= IGI Global\n|pages= 162–199\n}}</ref> and ontology consistency checking.<ref>{{cite journal\n| last       = Sheng\n| first      = Z\n| last2      = Wang\n| first2     = X\n| last3      = Shi\n| first3     = H\n| last4      = Feng\n| first4     = Z\n| date       = 26 Oct 2012\n| title      = Checking and handling inconsistency of DBpedia\n| url        = http://link.springer.com/chapter/10.1007/978-3-642-33469-6_60\n| journal    = Web Information Systems and Mining\n| publisher  = Springer Berlin Heidelberg\n| pages      = 480–488\n}}</ref> It has also been used to build large ontologies <ref>{{cite journal\n| last       = Yablonsky\n| first      = S\n| date       = Jun 2009\n| title      = Semantic Web framework for development of very large ontologies\n| url        = http://www.scielo.org.mx/pdf/poli/n39/n39a4.pdf\n| journal    = Polibits\n| issue      = 39\n| pages      = 19–26\n}}</ref> and for online [[question answering]] systems.<ref>{{cite journal\n| last       = Bishop\n| first      = B\n| last2      = et\n| first2     = al.\n| date       = Jan 2011\n| title      = Factforge: A fast track to the web of data\n| url        = http://www.semantic-web-journal.net/sites/default/files/swj77_2.pdf\n| journal    = Semantic Web\n| volume     = 2\n| issue      = 2\n| pages      = 157–166\n}}</ref>\n\nIncluding OpenCyc, UMBEL has about 65,000 formal mappings to [[DBpedia]], PROTON, [[GeoNames]], and [[schema.org]], and provides linkages to more than 2 million [[Wikipedia]] pages (English version). All of its reference concepts and mappings are organized under a hierarchy of 31 different "super types",<ref>See [http://techwiki.umbel.org/index.php/UMBEL_-_Annex_G Annex G] in the UMBEL specifications.</ref> which are mostly disjoint from one another. Each of these "super types" has its own typology of entity classes to provide flexible tie-ins for external content. 90% of UMBEL is contained in these entity classes.\n\nUMBEL was first released in July 2008. Version 1.00 was released in February 2011.<ref>See the [http://umbel.org/resources/news/finally-umbel-v-100] release announcement.</ref> Its current release is version 1.50.<ref>See http://umbel.org/resources/news/new-major-upgrade-of-umbel-released-version-1.50/</ref>\n\n==See also==\n* [[Cyc]]\n* [[DBpedia]]\n\n==Notes==\n{{Reflist}}\n\n==External links==\n* [http://umbel.org/ Main page for UMBEL]\n* [http://techwiki.umbel.org/index.php/UMBEL_Specification UMBEL specification], and its accompanying [http://umbel.org/specifications/annexes Annexes A - L, Z]\n\n{{Semantic Web}}\n{{Use dmy dates|date=October 2012}}\n\n{{DEFAULTSORT:Umbel}}\n[[Category:Knowledge representation]]\n[[Category:Ontology (information science)]]\n[[Category:Semantic Web]]\n[[Category:Knowledge bases]]']
['Moovly', '50643936', '{{Orphan|date=May 2016}}\n{{Infobox company\n| name             = Moovly\n| type             = [[Public company|Public]]\n| key_people       = Brendon Grunewald (Co-Founder and CEO), Geert Coppens (Co-Founder and CTO)\n| industry         = [[Internet Marketing]]\n| products         = Moovly web-based animation software video creation and generation platform.\n| foundation       = {{Start date and age|2012}}\n| location_city    = [[Vancouver]]\n| location_country = [[Canada]]\n| homepage         = {{URL|www.moovly.com}}\n}}\n\n\'\'\'Moovly\'\'\' is a company that provides a cloud-based platform [[Software as a service|(SaaS)]] that enables users to create and generate multimedia content: animated videos, video presentations, animated info graphics and any other video content that includes a mix of animation and motion graphics.<ref>Flanders Today, [http://www.flanderstoday.eu/business/moovly-offers-new-online-tool-develop-multimedia-designs Flanders Today], September 13th, 2013, "Moovly offers new online tool to develop multimedia designs"</ref>\n\n== History ==\nMoovly was originally founded in Belgium in November 2012 by Brendon Grunewald and Geert Coppens with the vision of "becoming the number one platform for engaging customisable multimedia content creation". The company\'s mission is to "Enable everyone to create engaging multimedia content by making it affordable, Intuitive and Simple".  The company has seen steady subscriber growth from multinational corporations, small and medium business as well as educational institutions.. The company initially secured several rounds of external investment to fund its growth, and in July 2016, listed on the Toronto Venture Stock Exchange as Moovly Media Inc under the symbol MVY (TSX.V: MVY).<ref>Levak, Rachel, [https://www.crunchbase.com/organization/moovly/funding-rounds Crunchbase] , Apr 09, 2015, "Moovly - Funding Rounds |  CrunchBase"</ref>\n\n== Product ==\nMoovly is a cloud based digital media and content creation software platform. Content can be created via various interfaces, including the editor as well as simple, custom-made video generation interfaces.\n\nUsing a combination of uploaded images, videos and sounds, as well as a pre-defined library of objects, users are able to quickly assemble new animated content. The final videos or presentations can be downloaded as an [[MP4]] for example, or published on a variety of video platforms.\n\nMoovly provides a feature-rich free license allowing users to create animated videos<ref>Wilson, Liévano, [http://jsk.stanford.edu/news-notes/2014/10-things-you-need-to-know-when-producing-a-data-animation-for-a-newsroom/ John S. Knight Journalism Fellowships at Stanford], Sep 04th, 2014, "10 things you need to know when producing a data animation for a newsroom"</ref> that can be exported to [[Facebook]] and [[YouTube]], as well as premium licenses for advanced and professional use. The free videos include the Moovly branding. As an educational tool<ref>Hart, Jane, [http://c4lpt.co.uk/top100tools/moovly/ Centre for Learning & Performance Technologies], Sep 21, 2015, " Top 100 Tools for Learning | Centre for Learning & Performance Technologies"</ref> and for educational purposes,<ref>Janssens, Mieke, [http://www.klascement.eu/sites/60764/ KlasCement Educational Resources Network], Sep 22, 2015, "Moovly: Create animated videos and presentations | KlasCement Educational Resources Network"</ref> Moovly offers specific licenses.<ref>[https://www.moovly.com/education-solutions Moovly Website]</ref>\n\nCompanies and brands can use their own library of animated graphics, their own fonts and standard color set.<ref>Waldron, John, [http://www.markitwrite.com/moovly/ markITwrite] , Nov, 2014, "Moovly: Video Animation for Everyone |  markITwrite"</ref>\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{official website|https://www.moovly.com/}}\n* [http://web.tmxmoney.com/company.php?qm_symbol=MVY&locale=EN Toronto Venture Exchange Moovly Page]\n* [http://www.bloomberg.com/profiles/companies/1291746D:BB-moovly-nv Bloomberg overview]\n\n[[Category:Animation software]]\n[[Category:Companies established in 2012]]\n[[Category:Cloud applications]]\n[[Category:Computer animation]]\n[[Category:Presentation software]]\n[[Category:Knowledge representation]]']
['Library branch', '52141053', "[[File:New York Public Library May 2011.JPG|200 px|thumb|right|The [[New York Public Library Main Branch]] in [[Bryant Park]], [[Manhattan]]]]\n'''Library branches''' are libraries that form part of a [[library system]] but are not located in the same area, building or city, but use the same [[Library classification]] for their catalogs and are interconnected with all the branches of the system that form part of the systems and to library patrons through a [[integrated library system]].<ref>{{cite web|url=http://www.merriam-webster.com/dictionary/branch |title=Branch &#124; Definition of Branch by Merriam-Webster |website=Merriam-webster.com |date= |accessdate=2016-11-05}}</ref>\n\nMost of [[County|counties]] of every country have their own [[library system]] that usually have between to 20 libraries on every city of their counties, some of them are; London Public Library (on Canada) with 16 library branches, [[Helsinki Metropolitan Area Libraries]] with 63 libraries,<ref>{{cite web| url=http://www.iii.com/news/pr_display.php?id=559 | title=Helsinki Metropolitan Area Libraries (Finland) Upgrades to Sierra Services Platform | publisher=Innovative | type= Press release | date=5 February 2013 | accessdate=1 August 2014 }}</ref> [[National Library of Venezuela]] with 685 branches.\n\nSome popular library branches includ [[New York Public Library Main Branch]], part of [[New York Public Library|New York Public Library System]], and [[Martin Luther King Jr. Memorial Library]], a branch of [[District of Columbia Public Library|District of Columbia Public Library System]].\n\n==References==\n{{Reflist}}\n\n[[Category:Public libraries]]\n[[Category:Private libraries]]\n[[Category:Libraries]]\n[[Category:Culture]]\n[[Category:Knowledge representation]]"]
