['Category:Citation indices', '24447333', '{{Cat main|Citation index}}\n{{distinguish|Category:Bibliographic databases and indexes|Category:Citation metrics}}\n\n[[Category:Bibliometrics|Indices]]\n[[Category:Reference works]]\n[[Category:Indexes]]\n[[Category:Information retrieval]]\n[[Category:Bibliographic databases and indexes| ]]']
['Category:Information retrieval organizations', '46964829', '[[Category:Information retrieval]]\n[[Category:Organizations by subject]]']
['Category:Information retrieval researchers', '46967136', '[[Category:Computer scientists by field of research|Information retrieval]]\n[[Category:Information retrieval]]']
['Metadirectory', '943530', "A '''metadirectory''' system provides for the flow of data between one or more [[directory service]]s and [[database]]s, in order to maintain synchronization of that data, and is an important part of [[identity management]] systems. The data being synchronized typically are collections of entries that contain user profiles and possibly authentication or policy information. Most metadirectory deployments synchronize data into at least one [[Lightweight Directory Access Protocol|LDAP]]-based directory server, to ensure that LDAP-based applications such as [[single sign-on]] and portal servers have access to recent data, even if the data is mastered in a non-LDAP data source.\n\nMetadirectory products support filtering and transformation of data in transit.\n\nMost [[identity management]] suites from commercial vendors include a metadirectory product, or a [[provisioning#User provisioning|user provisioning]] product.\n\n== See also ==\n* [[Virtual directory]]\n* [[Identity correlation]]\n* [[Microsoft Identity Integration Server]]\n* [[Novell Identity Manager]]\n* [[Critical Path, Inc.|Critical Path Metadirectory]]\n\n[[Category:Directory services]]\n[[Category:Data management]]"]
['Category:Concurrency control', '1462863', '{{Cat main|Concurrency control}}\n\n[[Category:Data management]]\n[[Category:Synchronization]]\n[[Category:Operating system technology]]\n[[Category:Concurrency (computer science)]]\n[[Category:Distributed computing problems]]']
['Nested transaction', '1867103', "A '''nested transaction''' is a [[database transaction]] that is started by an instruction within the scope of an already started transaction.\n\nNested transactions are implemented differently in different databases. However, they have in common that the changes are not made visible to any unrelated transactions until the outermost transaction has committed. This means that a commit in an inner transaction does not necessarily persist updates to the database.\n\nIn some databases, changes made by the nested transaction are not seen by the 'host' transaction until the nested transaction is committed. According to some,{{Who|date=November 2009}} this follows from the isolation property of transactions.\n\nThe capability to handle nested transactions properly is a prerequisite for true component-based application architectures. In a component-based encapsulated architecture, nested transactions can occur without the programmer knowing it. A component function may or may not contain a database transaction (this is the encapsulated secret of the component. See [[Information hiding]]). If a call to such a component function is made inside a BEGIN - COMMIT bracket, nested transactions occur. Since popular databases like [[MySQL]]<ref>\n{{cite web\n |url=http://dev.mysql.com/doc/refman/4.1/en/implicit-commit.html\n |title=Statements That Cause an Implicit Commit\n |author=\n |work=MySQL 4.1 Reference Manual\n |publisher=Oracle\n |accessdate=5 December 2010\n}}\n</ref> do not allow nesting BEGIN - COMMIT brackets, a framework or a transaction monitor is needed to handle this. When we speak about nested transactions, it should be made clear that this feature is DBMS dependent and is not available for all databases.\n\nTheory for nested transactions is similar to the theory for flat transactions.<ref>{{Cite journal\n  | last = Resende | first = R.F. | last2 = El Abbadi | first2 = A.\n  | title = On the serializability theorem for nested transactions\n  | journal = Information Processing Letters | volume = 50 | issue = 4\n  | pages = 177–183 | date = 1994-05-25 \n  | doi = 10.1016/0020-0190(94)00033-6 }}</ref>\n\nThe banking industry usually processes financial transactions using ''open nested transactions'',{{Citation needed|date=August 2015}} which is a looser variant of the nested transaction model that provides higher performance while accepting the accompanying trade-offs of inconsistency.<ref>{{cite journal\n  | last = Weikum | first = Gerhard |author2=Hans-J. Schek\n  | title = Concepts and Applications of Multilevel Transactions and Open Nested Transactions\n  | journal = Database Transaction Models for Advanced Applications\n  | pages = 515–553 | publisher = Morgan Kaufmann | year = 1992\n  | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7962 | isbn = 1-55860-214-3\n  | accessdate = 2007-11-13 }}</ref>\n\n==Further reading==\n* Gerhard Weikum, Gottfried Vossen, ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, 2002, ISBN 1-55860-508-8\n\n==References==\n{{reflist}}\n\n[[Category:Data management]]\n[[Category:Transaction processing]]\n\n{{database-stub}}"]
['Data Transformation Services', '1605292', '\'\'\'Data Transformation Services\'\'\', or \'\'\'DTS\'\'\', is a set of objects and utilities to allow the automation of [[extract, transform, load|extract, transform and load]] operations to or from a database. The objects are DTS packages and their components, and the utilities are called DTS tools. DTS was included with earlier versions of [[Microsoft SQL Server]], and was almost always used with SQL Server databases, although it could be used independently with other databases.\n\nDTS allows data to be transformed and loaded from [[heterogeneous]] sources using [[OLE DB]], [[ODBC]], or text-only files, into any supported [[database]]. DTS can also allow automation of data import or transformation on a scheduled basis, and can perform additional functions such as [[File Transfer Protocol|FTPing]] files and executing external programs. In addition, DTS provides an alternative method of version control and backup for packages when used in conjunction with a version control system, such as [[Microsoft Visual SourceSafe]].\n[[Image:DTS Designer screenshot.PNG|right|thumb|300px|Here a DTS package is edited with DTS Designer in [[Windows XP]].]] DTS has been superseded by [[SQL Server Integration Services]] in later releases of Microsoft SQL Server though there was some backwards compatibility and ability to run DTS packages in the new SSIS for a time.\n\n__TOC__\n{{clear}}\n\n==History==\n\nIn SQL Server versions 6.5 and earlier, [[database administrators]] (DBAs) used [[SQL Server Transfer Manager]] and [[Bulk Copy Program]], included with SQL Server, to transfer data. These tools had significant shortcomings, and many{{quantify|date=May 2014}} DBAs used third-party tools such as [[Pervasive Data Integrator]] to transfer data more flexibly and easily. With the release of SQL Server 7 in 1998, "Data Transformation Services" was packaged with it to replace all these tools.\n\nSQL Server 2000 expanded DTS functionality in several ways. It introduced new types of tasks, including the ability to [[File Transfer Protocol|FTP]] files, move databases or database components, and add messages into [[Microsoft Message Queuing|Microsoft Message Queue]]. DTS packages can be saved as a Visual Basic file in SQL Server 2000, and this can be expanded to save into any COM-compliant language. Microsoft also integrated packages into [[Windows 2000 security]] and made DTS tools more user-friendly;  tasks can accept input and output parameters.\n\nDTS comes with all editions of SQL Server 7 and 2000, but was superseded by [[SQL Server Integration Services]] in the Microsoft SQL Server 2005 release in 2005.\n\n== DTS packages ==\nThe DTS package is the fundamental logical component of DTS; every DTS object is a [[Information hiding|child component]] of the package. Packages are used whenever one modifies data using DTS. All the [[metadata]] about the data transformation is contained within the package. Packages can be saved directly in a SQL Server, or can be saved in the [[Microsoft Repository]] or in [[Component Object Model|COM]] files. SQL Server 2000 also allows a programmer to save packages in a [[Visual Basic]] or other language file (when stored to a VB file, the package is actually scripted—that is, a VB script is executed to dynamically create the package objects and its component objects).\n\nA package can contain any number of [[ActiveX Data Objects|connection objects]], but does not have to contain any. These allow the package to read data from any [[OLE DB]]-compliant data source, and can be expanded to handle other sorts of data. The functionality of a package is organized into \'\'tasks\'\' and \'\'steps\'\'.\n\nA DTS Task is a discrete set of functionalities executed as a single step in a DTS package. Each task defines a work item to be performed as part of the data movement and data transformation process or as a job to be executed.\n\nData Transformation Services supplies a number of tasks that are part of the DTS [[object model]] and that can be accessed graphically through the DTS Designer or accessed programmatically. These tasks, which can be configured individually, cover a wide variety of data copying, data transformation and notification situations. For example, the following types of tasks represent some actions that you can perform by using [[Data transformation service|DTS]]: executing a single SQL statement, sending an email, and transferring a file with FTP.\n\nA step within a DTS package describes the order in which tasks are run and the precedence constraints that describe what to do in the case damage or of failure. These steps can be executed sequentially or in parallel.\n\nPackages can also contain [[global variable]]s which can be used throughout the package. SQL Server 2000 allows input and output parameters for tasks, greatly expanding the usefulness of global variables. DTS packages can be edited, password protected, scheduled for execution, and retrieved by version.\n\n==DTS tools==\nDTS tools packaged with SQL Server include the DTS wizards, DTS Designer, and DTS Programming Interfaces.\n\n===DTS wizards===\nThe DTS [[Wizard (software)|wizards]] can be used to perform simple or common DTS tasks. These include the \'\'Import/Export Wizard\'\' and the \'\'Copy of Database Wizard\'\'. They provide the simplest method of copying data between [[OLE DB]] data sources. There is a great deal of functionality that is not available by merely using a wizard. However, a package created with a wizard can be saved and later altered with one of the other DTS tools.\n\nA \'\'Create Publishing Wizard\'\' is also available to schedule packages to run at certain times. This only works if [[SQL Server Agent]] is running; otherwise the package will be scheduled, but will not be executed.\n\n===DTS Designer===\nThe DTS Designer is a [[graphical tool]] used to build complex DTS Packages with workflows and event-driven logic. DTS Designer can also be used to edit and customize DTS Packages created with the DTS wizard.\n\nEach connection and task in DTS Designer is shown with a specific [[Icon (computing)|icon]]. These icons are joined with precedence constraints, which specify the order and requirements for tasks to be run. One task may run, for instance, only if another task succeeds (or fails). Other tasks may run concurrently.\n\nThe DTS Designer has been criticized for having unusual quirks and limitations, such as the inability to visually [[copy and paste]] multiple tasks at one time. Many of these shortcomings have been overcome in [[SQL Server Integration Services]], DTS\'s successor.\n\n===DTS Query Designer===\nA graphical tool used to build [[Information retrieval|queries]] in DTS.\n\n===DTS Run Utility===\nDTS Packages can be run from the command line using the DTSRUN Utility.<BR/>\nThe utility is invoked using the following  syntax: \n<PRE>\ndtsrun /S server_name[\\instance_name]\n        { {/[~]U user_name [/[~]P password]} | /E }\n    ]\n    {    \n        {/[~]N package_name }\n        | {/[~]G package_guid_string}\n        | {/[~]V package_version_guid_string}\n    }\n    [/[~]M package_password]\n    [/[~]F filename]\n    [/[~]R repository_database_name]\n    [/A global_variable_name:typeid=value] \n    [/L log_file_name]\n    [/W NT_event_log_completion_status]\n    [/Z] [/!X] [/!D] [/!Y] [/!C]\n]\n</PRE>\n\nWhen passing in parameters which are mapped to Global Variables, you are required to include the typeid. This is rather difficult to find on the Microsoft site. Below are the TypeIds used in passing in these values.\n\n{| class="wikitable sortable"\n! Type !! typeid\n|-\n| Boolean\n| align="right" | 11\n|-\n| Currency\n| align="right" | 6\n|-\n| Date\n| align="right" | 7\n|-\n| Decimal\n| align="right" | 14\n|-\n| HRESULT\n| align="right" | 25\n|-\n| Int\n| align="right" | 22\n|-\n| Integer (1-byte)\n| align="right" | 16\n|-\n| Integer (8-byte)\n| align="right" | 20\n|-\n| Integer (small)\n| align="right" | 2\n|-\n| LPWSTR\n| align="right" | 31\n|-\n| Pointer\n| align="right" | 26\n|-\n| Real (4-byte)\n| align="right" | 4\n|-\n| Real (8-byte)\n| align="right" | 5\n|-\n| String\n| align="right" | 8\n|-\n| Unsigned int (1-byte)\n| align="right" | 17\n|-\n| Unsigned int (2-byte)\n| align="right" | 18\n|-\n| Unsigned int (4-byte)\n| align="right" | 19\n|-\n| Unsigned int (1-byte)\n| align="right" | 21\n|-\n| Unsigned int\n| align="right" | 23\n|}\n\n==See also==\n\n* [[OLAP]]\n* [[Data warehouse]]\n* [[Data mining]]\n* [[SQL Server Integration Services]]\n* [[Meta Data Services]]\n\n==References==\n* {{cite book |author1=Chaffin, Mark |author2=Knight, Brian |author3=Robinson, Todd | title=Professional SQL Server 2000 DTS | publisher=[[Wrox Press]] (Wiley Publishing, Inc.) | year=2003 | isbn=0-7645-4368-7}}\n\n== External links ==\n* [http://msdn2.microsoft.com/en-us/library/aa933484(SQL.80).aspx Microsoft SQL Server: Data Transformation Services (DTS)]\n* [http://www.sqldts.com/ SQL DTS unique DTS information resource]\n* [http://support.microsoft.com/kb/238912 Understanding Microsoft Repository]\n* [http://pragmaticworks.com/Resources/webinars/Default.aspx DTS Videos & Training]\n* [http://www.softrus.org/dts/ DTS Documenter]\n\n[[Category:Microsoft database software]]\n[[Category:Data management]]\n[[Category:Extract, transform, load tools]]\n[[Category:Microsoft server technology]]']
['Serializability', '4367801', '{{About|serializability of database transactions|serialization of objects in object-oriented languages|serialization}}\n\nIn [[concurrency control]] of [[database]]s,<ref name=Bernstein87>[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx \'\'Concurrency Control and Recovery in Database Systems\'\'] (free PDF download), Addison Wesley Publishing Company, ISBN 0-201-10715-5</ref><ref name=Weikum01>[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description \'\'Transactional Information Systems\'\'], Elsevier, ISBN 1-55860-508-8</ref> [[transaction processing]] (transaction management), and various [[Database transaction|transactional]] applications (e.g., [[transactional memory]]<ref name=Herlihy1993>[[Maurice Herlihy]] and J. Eliot B. Moss. \'\'Transactional memory: architectural support for lock-free data structures.\'\' Proceedings of the 20th annual international symposium on Computer architecture (ISCA \'93). Volume 21, Issue 2, May 1993.</ref> and [[software transactional memory]]), both centralized and [[Distributed computing|distributed]], a transaction [[Schedule (computer science)|schedule]] is \'\'\'serializable\'\'\' if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e., sequentially without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions\' executions. It is considered the highest level of [[isolation (computer science)|isolation]] between [[Database transaction|transactions]], and plays an essential role in [[concurrency control]]. As such it is supported in all general purpose database systems. \'\'[[Two-phase locking|Strong strict two-phase locking]]\'\' (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.\n\n\'\'\'Serializability theory\'\'\' provides the formal framework to reason about and analyze serializability and its techniques. Though it is [[Mathematics|mathematical]] in nature, its fundamentals are informally (without mathematics notation) introduced below.\n\n==Database transaction==\n{{main|Database transaction}}\n\nA \'\'\'database transaction\'\'\' is a specific intended run (with specific parameters, e.g., with transaction identification, at least) of a computer program (or programs) that accesses a database (or databases). Such a program is written with the assumption that it is running in \'\'isolation\'\' from other executing programs, i.e., when running, its accessed data (after the access) are not changed by other running programs. Without this assumption the transaction\'s results are unpredictable and can be wrong. The same transaction can be executed in different situations, e.g., in different times and locations, in parallel with different programs. A \'\'live\'\' transaction (i.e., exists in a computing environment with already allocated computing resources; to distinguish from a \'\'transaction request\'\', waiting to get execution resources) can be in one of three states, or phases:\n#\'\'Running\'\' - Its program(s) is (are) executing.\n#\'\'Ready\'\' - Its program\'s execution has ended, and it is waiting to be \'\'Ended (Completed)\'\'.\n#\'\'Ended\'\' (or \'\'Completed\'\') - It is either \'\'Committed\'\' or \'\'Aborted (Rolled-back)\'\', depending whether the execution is considered a success or not, respectively . When committed, all its \'\'recoverable\'\' (i.e., with states that can be controlled for this purpose), \'\'durable\'\' resources (typically \'\'database data\'\') are put in their \'\'final\'\' states, states after running. When aborted, all its recoverable resources are put back in their \'\'initial\'\' states, as before running.\n\nA failure in transaction\'s computing environment before ending typically results in its abort. However, a transaction may be aborted also for other reasons as well (e.g., see below).\n\nUpon being ended (completed), transaction\'s allocated computing resources are released and the transaction disappears from the computing environment. However, the effects of a committed transaction remain in the database, while the effects of an aborted (rolled-back) transaction disappear from the database. The concept of \'\'atomic transaction\'\' ("all or nothing" semantics) was designed to exactly achieve this behavior, in order to control correctness in complex faulty systems.\n\n==Correctness==\n\n===Serializability===\n\'\'\'Serializability\'\'\' is used to keep the data in the data item in a consistent state.  Serializability is a property of a transaction [[Schedule (computer science)|schedule]] (history). It relates to the \'\'[[Isolation (database systems)|isolation]]\'\' property of a [[database transaction]].\n:\'\'\'Serializability\'\'\' of a schedule means equivalence (in the outcome, the database state, data values) to a \'\'serial schedule\'\' (i.e., sequential with no transaction overlap in time) with the same transactions. It is the major criterion for the correctness of concurrent transactions\' schedule, and thus supported in all general purpose database systems.\n\n:\'\'\'The rationale behind serializability\'\'\' is the following:\n:If each transaction is correct by itself, i.e., meets certain integrity conditions, then a schedule that comprises any \'\'serial\'\' execution of these transactions is correct (its transactions still meet their conditions): "Serial" means that transactions do not overlap in time and cannot interfere with each other, i.e, complete \'\'isolation\'\' between each other exists. Any order of the transactions is legitimate, if no dependencies among them exist, which is assumed (see comment below). As a result, a schedule that comprises any execution (not necessarily serial) that is equivalent (in its outcome) to any serial execution of these transactions, is correct.\n\nSchedules that are not serializable are likely to generate erroneous outcomes. Well known examples are with transactions that debit and credit accounts with money: If the related schedules are not serializable, then the total sum of money may not be preserved. Money could disappear, or be generated from nowhere. This and violations of possibly needed other [[invariant (computer science)|invariant]] preservations are caused by one transaction writing, and "stepping on" and erasing what has been written by another transaction before it has become permanent in the database. It does not happen if serializability is maintained.\n\nIf any specific order between some transactions is requested by an application, then it is enforced independently of the underlying serializability mechanisms. These mechanisms are typically indifferent to any specific order, and generate some unpredictable [[partial order]] that is typically compatible with multiple serial orders of these transactions. This partial order results from the scheduling orders of concurrent transactions\' data access operations, which depend on many factors.\n\nA major characteristic of a database transaction is \'\'[[Atomicity (database systems)|atomicity]]\'\', which means that it either \'\'commits\'\', i.e., all its operations\' results take effect in the database, or \'\'aborts\'\' (rolled-back), all its operations\' results do not have any effect on the database ("all or nothing" semantics of a transaction). In all real systems transactions can abort for many reasons, and serializability by itself is not sufficient for correctness. Schedules also need to possess the \'\'[[Schedule (computer science)#Recoverable|recoverability]]\'\' (from abort) property. \'\'\'Recoverability\'\'\' means that committed transactions have not read data written by aborted transactions (whose effects do not exist in the resulting database states). While serializability is currently compromised on purpose in many applications for better performance (only in cases when application\'s correctness is not harmed), compromising recoverability would quickly violate the database\'s integrity, as well as that of transactions\' results external to the database. A schedule with the recoverability property (a \'\'recoverable\'\' schedule) "recovers" from aborts by itself, i.e., aborts do not harm the integrity of its committed transactions and resulting database. This is false without recoverability, where the likely integrity violations (resulting incorrect database data) need special, typically manual, corrective actions in the database.\n\nImplementing recoverability in its general form may result in \'\'cascading aborts\'\': Aborting one transaction may result in a need to abort a second transaction, and then a third, and so on. This results in a waste of already partially executed transactions, and may result also in a performance penalty. \'\'\'[[Schedule (computer science)#Avoids cascading aborts (rollbacks)|Avoiding cascading aborts]]\'\'\' (ACA, or Cascadelessness) is a special case of recoverability that exactly prevents such phenomenon. Often in practice a special case of ACA is utilized: \'\'\'[[Schedule (computer science)#Strict|Strictness]]\'\'\'. Strictness allows an efficient database recovery from failure.\n\nNote that the \'\'recoverability\'\' property is needed even if no database failure occurs and no database \'\'recovery\'\' from failure is needed. It is rather needed to correctly automatically handle aborts, which may be unrelated to database failure and recovery from failure.\n\n===Relaxing serializability===\n\nIn many applications, unlike with finances, absolute correctness is not needed. For example, when retrieving a list of products according to specification, in most cases it does not matter much if a product, whose data was updated a short time ago, does not appear in the list, even if it meets the specification. It will typically appear in such a list when tried again a short time later. Commercial databases provide concurrency control with a whole range of [[isolation (computer science)#Isolation levels|isolation levels]] which are in fact (controlled) serializability violations in order to achieve higher performance. Higher performance means better transaction execution rate and shorter average transaction response time (transaction duration). \'\'[[Snapshot isolation]]\'\' is an example of a popular, widely utilized efficient relaxed serializability method with many characteristics of full serializability, but still short of some, and unfit in many situations.\n\nAnother common reason nowadays for [[Serializability#Distributed serializability|distributed serializability]] relaxation (see below) is the requirement of [[availability]] of [[internet]] products and [[Internet service provider|services]]. This requirement is typically answered by large-scale data [[Replication (computer science)|replication]]. The straightforward solution for synchronizing replicas\' updates of a same database object is including all these updates in a single atomic [[distributed transaction]]. However, with many replicas such a transaction is very large, and may span several [[computer]]s and [[computer network|networks]] that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.<ref name=Gray1996>{{cite conference\n | author = [[Jim Gray (computer scientist)|Gray, J.]]\n | coauthors = Helland, P.; [[Patrick O\'Neil|O’Neil, P.]]; [[Dennis Shasha|Shasha, D.]]\n | year = 1996\n | title = The dangers of replication and a solution\n | conference = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]\n | pages = 173–182\n | url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf\n | doi = 10.1145/233269.233330\n }}</ref>\nConsequently, [[Optimistic replication]] (Lazy replication) is often utilized (e.g., in many products and services by [[Google]], [[Amazon.com|Amazon]], [[Yahoo]], and alike), while serializability is relaxed and compromised for [[eventual consistency]]. Again in this case, relaxation is done only for applications that are not expected to be harmed by this technique.\n\nClasses of schedules defined by \'\'relaxed serializability\'\' properties either contain the serializability class, or are incomparable with it.\n\n==View and conflict serializability ==\n\nMechanisms that enforce serializability need to execute in [[Real-time computing|real time]], or almost in real time, while transactions are running at high rates. In order to meet this requirement special cases of serializability, sufficient conditions for serializability which can be enforced effectively, are utilized.\n\nTwo major types of serializability exist: \'\'view-serializability\'\', and \'\'conflict-serializability\'\'. View-serializability matches the general definition of serializability given above. Conflict-serializability is a broad special case, i.e., any schedule that is conflict-serializable is also view-serializable, but not necessarily the opposite. Conflict-serializability is widely utilized because it is easier to determine and covers a substantial portion of the view-serializable schedules. Determining view-serializability of a schedule is an [[NP-complete]] problem (a class of problems with only difficult-to-compute, excessively time-consuming known solutions).\n\n:\'\'\'View-serializability\'\'\' of a schedule is defined by equivalence to a serial schedule (no overlapping transactions) with the same transactions, such that respective transactions in the two schedules read and write the same data values ("view" the same data values).\n\n:\'\'\'Conflict-serializability\'\'\' is defined by equivalence to a serial schedule (no overlapping transactions) with the same transactions, such that both schedules have the same sets of respective chronologically ordered pairs of conflicting operations (same precedence relations of respective conflicting operations).\n\nOperations upon data are \'\'read\'\' or \'\'write\'\' (a write: either \'\'insert\'\' or \'\'modify\'\' or \'\'delete\'\'). Two operations are \'\'conflicting\'\', if they are of different transactions, upon the same datum (data item), and at least one of them is \'\'write\'\'. Each such pair of conflicting operations has a \'\'conflict type\'\': It is either a \'\'read-write\'\', or \'\'write-read\'\', or a \'\'write-write\'\' conflict. The transaction of the second operation in the pair is said to be \'\'in conflict\'\' with the transaction of the first operation. A more general definition of conflicting operations (also for complex operations, which may consist each of several "simple" read/write operations) requires that they are [[noncommutative]] (changing their order also changes their combined result). Each such operation needs to be atomic by itself (by proper system support) in order to be considered an operation for a commutativity check. For example, read-read operations are commutative (unlike read-write and the other possibilities) and thus read-read is not a conflict. Another more complex example: the operations \'\'increment\'\' and \'\'decrement\'\' of a \'\'counter\'\' are both \'\'write\'\' operations (both modify the counter), but do not need to be considered conflicting (write-write conflict type) since they are commutative (thus increment-decrement is not a conflict; e.g., already has been supported in the old [[IBM Information Management System|IBM\'s IMS "fast path"]]). Only precedence (time order) in pairs of conflicting (non-commutative) operations is important when checking equivalence to a serial schedule, since different schedules consisting of the same transactions can be transformed from one to another by changing orders between different transactions\' operations (different transactions\' interleaving), and since changing orders of commutative operations (non-conflicting) does not change an overall operation sequence result, i.e., a schedule outcome (the outcome is preserved through order change between non-conflicting operations, but typically not when conflicting operations change order). This means that if a schedule can be transformed to any serial schedule without changing orders of conflicting operations (but changing orders of non-conflicting, while preserving operation order inside each transaction), then the outcome of both schedules is the same, and the schedule is conflict-serializable by definition.\n\nConflicts are the reason for blocking transactions and delays (non-materialized conflicts), or for aborting transactions due to serializability violations prevention. Both possibilities reduce performance. Thus reducing the number of conflicts, e.g., by commutativity (when possible), is a way to increase performance.\n\nA transaction can issue/request a conflicting operation and be \'\'in conflict\'\' with another transaction while its conflicting operation is delayed and not executed (e.g., blocked by a [[Lock (computer science)|lock]]). Only executed (\'\'materialized\'\') conflicting operations are relevant to \'\'conflict serializability\'\' (see more below).\n\n==Enforcing conflict serializability==\n\n===Testing conflict serializability===\n\nSchedule compliance with conflict serializability can be tested with the [[precedence graph]] (\'\'serializability graph\'\', \'\'serialization graph\'\', \'\'conflict graph\'\') for committed transactions of the schedule. It is the [[directed graph]] representing precedence of transactions in the schedule, as reflected by precedence of conflicting operations in the transactions.\n\n:In the \'\'\'[[precedence graph]]\'\'\'  transactions are nodes and precedence relations are directed edges. There exists an edge from a first transaction to a second transaction, if the second transaction is \'\'in conflict\'\' with the first (see Conflict serializability above), and the conflict is \'\'\'materialized\'\'\' (i.e., if the requested conflicting operation is actually executed: in many cases a requested/issued conflicting operation by a transaction is delayed and even never executed, typically by a [[Lock (computer science)|lock]] on the operation\'s object, held by another transaction, or when writing to a transaction\'s temporary private workspace and materializing, copying to the database itself, upon commit; as long as a requested/issued conflicting operation is not executed upon the database itself, the conflict is \'\'\'non-materialized\'\'\'; non-materialized conflicts are not represented by an edge in the precedence graph).\n\n:\'\'\'Comment:\'\'\' In many text books only \'\'committed transactions\'\' are included in the precedence graph. Here all transactions are included for convenience in later discussions.\n\nThe following observation is a \'\'\'key characterization of conflict serializability\'\'\':\n\n:A schedule is \'\'conflict-serializable\'\' [[if and only if]] its precedence graph of \'\'committed transactions\'\' (when only \'\'committed\'\' transactions are considered) is \'\'[[directed acyclic graph|acyclic]]\'\'. This means that a cycle consisting of committed transactions only is generated in the (general) precedence graph, if and only if conflict-serializability is violated.\n\nCycles of committed transactions can be prevented by aborting an \'\'undecided\'\' (neither committed, nor aborted) transaction on each cycle in the precedence graph of all the transactions, which can otherwise turn into a cycle of committed transactions (and a committed transaction cannot be aborted). One transaction aborted per cycle is both required and sufficient number to break and eliminate the cycle (more aborts are possible, and can happen in some mechanisms, but unnecessary for serializability). The probability of cycle generation is typically low, but nevertheless, such a situation is carefully handled, typically with a considerable overhead, since correctness is involved. Transactions aborted due to serializability violation prevention are \'\'restarted\'\' and executed again immediately.\n\nSerializability enforcing mechanisms typically do not maintain a precedence graph as a data structure, but rather prevent or break cycles implicitly (e.g., SS2PL below).\n\n===Common mechanism - SS2PL===\n{{main|Two-phase locking}}\n\n\'\'Strong strict two phase locking\'\' (SS2PL) is a common mechanism utilized in database systems since their early days in the 1970s (the "SS" in the name SS2PL is newer though) to enforce both conflict serializability and \'\'[[Schedule (computer science)#Strict|strictness]]\'\' (a special case of recoverability which allows effective database recovery from failure) of a schedule. In this mechanism each datum is locked by a transaction before accessing it (any read or write operation): The item is marked by, associated with a \'\'[[lock (computer science)|lock]]\'\' of a certain type, depending on operation (and the specific implementation; various models with different lock types exist; in some models locks may change type during the transaction\'s life). As a result, access by another transaction may be blocked, typically upon a conflict (the lock delays or completely prevents the conflict from being materialized and be reflected in the precedence graph by blocking the conflicting operation), depending on lock type and the other transaction\'s access operation type. Employing an SS2PL mechanism means that all locks on data on behalf of a transaction are released only after the transaction has ended (either committed or aborted).\n\nSS2PL is the name of the resulting schedule property as well, which is also called \'\'rigorousness\'\'. SS2PL is a special case ([[proper subset]]) of [[Two-phase locking]] (2PL)\n\nMutual blocking between transactions results in a \'\'deadlock\'\', where execution of these transactions is stalled, and no completion can be reached. Thus deadlocks need to be resolved to complete these transactions\' execution and release related computing resources. A deadlock is a reflection of a potential cycle in the precedence graph, that would occur without the blocking when conflicts are materialized. A deadlock is resolved by aborting a transaction involved with such potential cycle, and breaking the cycle. It is often detected using a \'\'[[wait-for graph]]\'\' (a graph of conflicts blocked by locks from being materialized; it can be also defined as the graph of non-materialized conflicts; conflicts not materialized are not reflected in the precedence graph and do not affect serializability), which indicates which transaction is "waiting for" lock release by which transaction, and a cycle means a deadlock. Aborting one transaction per cycle is sufficient to break the cycle. Transactions aborted due to deadlock resolution are \'\'restarted\'\' and executed again immediately.\n\n===Other enforcing techniques===\n\nOther known mechanisms include:\n* [[Precedence graph]] (or Serializability graph, Conflict graph) cycle elimination\n* [[Two-phase locking]] (2PL)\n* [[Timestamp-based concurrency control|Timestamp ordering]] (TO)\n* [[Snapshot isolation#Making Snapshot Isolation Serializable|Serializable snapshot isolation]]<ref name=Cahill08>Michael J. Cahill, Uwe Röhm, Alan D. Fekete (2008): [http://portal.acm.org/citation.cfm?id=1376690  "Serializable isolation for snapshot databases"], \'\'Proceedings of the 2008 ACM SIGMOD international conference on Management of data\'\', pp. 729-738, Vancouver, Canada, June 2008, ISBN 978-1-60558-102-6 (SIGMOD 2008 best paper award)</ref> (SerializableSI)\n\nThe above (conflict) serializability techniques in their general form do not provide recoverability. Special enhancements are needed for adding recoverability.\n\n====Optimistic versus pessimistic techniques====\n\nConcurrency control techniques are of three major types:\n# \'\'Pessimistic\'\': In Pessimistic concurrency control a transaction blocks data access operations of other transactions upon conflicts, and conflicts are \'\'non-materialized\'\' until blocking is removed. This is done to ensure that operations that may violate serializability (and in practice also recoverability) do not occur.\n# \'\'Optimistic\'\': In [[Optimistic concurrency control]] data access operations of other transactions are not blocked upon conflicts, and conflicts are immediately \'\'materialized\'\'. When the transaction reaches the \'\'ready\'\' state, i.e., its \'\'running\'\' state has been completed, possible serializability (and in practice also recoverability) violation by the transaction\'s operations (relatively to other running transactions) is checked: If violation has occurred, the transaction is typically \'\'aborted\'\' (sometimes aborting \'\'another\'\' transaction to handle serializability violation is preferred). Otherwise it is \'\'committed\'\'.\n# \'\'Semi-optimistic\'\': Mechanisms that mix blocking in certain situations with not blocking in other situations and employ both materialized and non-materialized conflicts\n\nThe main differences between the technique types is the conflict types that are generated by them. A pessimistic method blocks a transaction operation upon conflict and generates a non-materialized conflict, while an optimistic method does not block and generates a materialized conflict. A semi-optimistic method generates both conflict types. Both conflict types are generated by the chronological orders in which transaction operations are invoked, independently of the type of conflict. A cycle of committed transactions (with materialized conflicts) in the \'\'[[precedence graph]]\'\' (conflict graph) represents a serializability violation, and should be avoided for maintaining serializability. A cycle of (non-materialized) conflicts in the \'\'[[wait-for graph]]\'\' represents a deadlock situation, which should be resolved by breaking the cycle. Both cycle types result from conflicts, and should be broken. At any technique type conflicts should be detected and considered, with similar overhead for both materialized and non-materialized conflicts (typically by using mechanisms like locking, while either blocking for locks, or not blocking but recording conflict for materialized conflicts). In a blocking method typically a [[context switch]]ing occurs upon conflict, with (additional) incurred overhead. Otherwise blocked transactions\' related computing resources remain idle, unutilized, which may be a worse alternative. When conflicts do not occur frequently, optimistic methods typically have an advantage. With different transactions loads (mixes of transaction types) one technique type (i.e., either optimistic or pessimistic) may provide better performance than the other.\n\nUnless schedule classes are \'\'inherently blocking\'\' (i.e., they cannot be implemented without data-access operations blocking; e.g., 2PL, SS2PL and SCO above; see chart), they can be implemented also using optimistic techniques (e.g., Serializability, Recoverability).\n\n====Serializable multi-version concurrency control====\n\n:See also [[Multiversion concurrency control]] (partial coverage)\n:and [[Snapshot isolation#Serializable Snapshot Isolation|Serializable_Snapshot_Isolation]] in [[Snapshot isolation]]\n\n\'\'\'Multi-version concurrency control\'\'\' (MVCC) is a common way today to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions\' read operations of several last relevant versions (of each object), depending on scheduling method. MVCC can be combined with all the serializability techniques listed above (except SerializableSI which is originally MVCC based). It is utilized in most general-purpose DBMS products.\n\nMVCC is especially popular nowadays through the \'\'relaxed serializability\'\' (see above) method \'\'[[Snapshot isolation]]\'\' (SI) which provides better performance than most known serializability mechanisms (at the cost of possible serializability violation in certain cases). [[Snapshot isolation#Making Snapshot Isolation Serializable|SerializableSI]], which is an efficient enhancement of SI to make it serializable, is intended to provide an efficient serializable solution. [[Snapshot isolation#Making Snapshot Isolation Serializable|SerializableSI has been analyzed]]<ref name=Cahill08/><ref name=fekete2009>Alan Fekete (2009), [http://www.it.usyd.edu.au/~fekete/teaching/serializableSI-Fekete.pdf "Snapshot Isolation and Serializable Execution"], Presentation, Page 4, 2009, The university of Sydney (Australia). Retrieved 16 September 2009</ref> via a general theory of MVCC\n\n==Distributed serializability==\n\n===Overview===\n\n\'\'\'Distributed serializability\'\'\' is the serializability of a schedule of a transactional [[distributed system]] (e.g., a [[distributed database]] system). Such system is characterized by \'\'[[distributed transaction]]s\'\' (also called \'\'global transactions\'\'), i.e., transactions that span computer processes (a process abstraction in a general sense, depending on computing environment; e.g., [[operating system]]\'s [[Thread (computer science)|thread]]) and possibly network nodes. A distributed transaction comprises more than one \'\'local sub-transactions\'\' that each has states as described above for a [[Serializability#Database transaction|database transaction]]. A local sub-transaction comprises a single process, or more processes that typically fail together (e.g., in a single [[processor core]]). Distributed transactions imply a need in [[Atomic commit]] protocol to reach consensus among its local sub-transactions on whether to commit or abort. Such protocols can vary from a simple (one-phase) hand-shake among processes that fail together, to more sophisticated protocols, like [[Two-phase commit]], to handle more complicated cases of failure (e.g., process, node, communication, etc. failure). Distributed serializability is a major goal of [[distributed concurrency control]] for correctness. With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], and small, portable, powerful computing devices (e.g., [[smartphone]]s) the need for effective distributed serializability techniques to ensure correctness in and among distributed applications seems to increase.\n\nDistributed serializability is achieved by implementing distributed versions of the known centralized techniques.<ref name=Bernstein87 /><ref name=Weikum01 /> Typically all such distributed versions require utilizing conflict information (either of materialized or non-materialized conflicts, or equivalently, transaction precedence or blocking information; conflict serializability is usually utilized) that is not generated locally, but rather in different processes, and remote locations. Thus information distribution is needed (e.g., precedence relations, lock information, timestamps, or tickets). When the distributed system is of a relatively small scale, and message delays across the system are small, the centralized concurrency control methods can be used unchanged, while certain processes or nodes in the system manage the related algorithms. However, in a large-scale system (e.g., \'\'Grid\'\' and \'\'Cloud\'\'), due to the distribution of such information, substantial performance penalty is typically incurred, even when distributed versions of the methods (Vs. centralized) are used, primarily due to computer and communication [[latency (engineering)|latency]]. Also, when such information is distributed, related techniques typically do not scale well. A well-known example with scalability problems is a [[distributed lock manager]], which distributes lock (non-materialized conflict) information across the distributed system to implement locking techniques.\n\n==See also==\n\n*[[Two-phase locking|Strong strict two-phase locking]] (SS2PL or Rigorousness).\n*[[Snapshot isolation#Making Snapshot Isolation Serializable|Making snapshot isolation serializable]]<ref name=Cahill08 /> in [[Snapshot isolation]].\n*[[Global serializability]], where the \'\'Global serializability problem\'\' and its proposed solutions are described.\n* [[Linearizability]], a more general concept in [[concurrent computing]]\n\n==Notes==\n{{reflist}}\n\n==References==\n{{more footnotes|date=November 2011}}\n<!--Supposed sources for most of the material about centralized (vs. distributed) serializability:-->\n*[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx \'\'Concurrency Control and Recovery in Database Systems\'\'], Addison Wesley Publishing Company, ISBN 0-201-10715-5\n*[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description \'\'Transactional Information Systems\'\'], Elsevier, ISBN 1-55860-508-8\n\n[[Category:Data management]]\n[[Category:Databases]]\n[[Category:Concurrency control]]\n[[Category:Transaction processing]]\n[[Category:Distributed computing problems]]\n\n[[el:Σειριοποιησιμότητα Συγκρούσεων]]']
['IMS VDEX', '5446251', "{{Multiple issues|\n{{refimprove|date=December 2010}}\n{{primary sources|date=December 2010}}\n{{notability|date=December 2010}}\n{{context|date=December 2010}}\n}}\n\n'''IMS VDEX''', which stands for '''IMS Vocabulary Definition Exchange''', is a mark-up language – or grammar – for [[Controlled vocabulary|controlled vocabularies]] developed by IMS Global as an open specification, with the Final Specification being approved in February 2004.\n\nIMS VDEX allows the exchange and expression of simple machine-readable lists of human language terms, along with information that may assist a human in understanding the meaning of the various terms, i.e. a flat list of values, a hierarchical tree of values, a thesaurus, a taxonomy, a glossary or a dictionary.\n\nStructural a vocabulary has an identifier, title and a list of terms. Each term has a unique key, titles and (optional) descriptions. A term may have nested terms, thus a hierarchical structure can be created. It is possible to define relationships between terms and add custom metadata to terms.\n\nIMS VDEX support multilinguality. All values supposed to be read by a human, i.e. titles, can be defined in one or more languages.\n\n== Purposes ==\nVDEX was designed to supplement other IMS specifications and the IEEE LOM standard by giving additional semantic control to tool developers. IMS VDEX could be used for the following purposes. It is used in practice for other purposes as well.\n\n* ''Interfaces providing pre-defined choices'' – providing radio buttons and drop-down menus for interfaces such as metadata editors or a repository browse tool, based on the vocabulary allowed in the metadata profile used\n* ''Distributing vocabularies among many users'' – achieved by simple XML file sharing, or possibly a searchable [[Repository Open Service Interface Definition|repository]] or registry of vocabularies\n* ''XML stylesheets used to select and generate different views'' – selecting an overview of an entire vocabulary as an [[HTML]] or [[PDF]] file, for example; providing scope notes for catalogues; or storing a glossary of terms which are called upon by hyperlinks within a document\n* ''Validation of metadata instances'' – validated against an application profile, by comparison of the vocabulary terms used in certain metadata elements with those of the machine readable version of the vocabularies specified by the application profile.\n* ''Controlled terms for other IMS specifications and IEEE LOM'' – both may contain elements where controlled terms should be used. These elements are often specified as being of a vocabulary data type, and a definition of the permitted terms and their usage may be expressed using VDEX.\n\n== Technical details ==\n[[Image:VDEX model.PNG|350px|right|thumb|simplified VDEX data model]]\nThe VDEX Information Model is represented in the diagram. A VDEX file describing a vocabulary comprises a number of information elements, most of which are relatively simple, such as a string representation of the default (human) language or a [[URI]] identifying the value domain (or vocabulary). Some of the elements are ‘containers’ – such as a ''term'' – that contain additional elements.\n\nElements may be required or optional, and in some cases, repeatable. Within a term, for example, a ''description'' and ''caption'' may be defined. Multiple language definitions can be used inside a description, by using a ''langstring'' element, where the description is paired with the language to be used. Additional elements within a term include ''media descriptors'', which are one or more media files to supplement a term’s description; and ''metadata'', which is used to describe the vocabulary further.\n\nThe ''relationship'' container defines a relationship between terms by identifying the two terms and the specifying type or relationship, such as a term being broader or narrower than another. The term used to specify the type of relationship may conform to the ISO standards for thesauri.\n\n''Vocabulary identifiers'' are unique, persistent URIs, whereas term or relationship identifiers are locally unique strings. VDEX also allows for a ''default language'' and ''vocabulary name'' to be given, and for whether the ordering of terms within the vocabulary is significant (''order significance'') to be specified.\n\nA ''profile type'' is specified to describe the type of vocabulary being expressed; different features of the VDEX model are permitted depending on the profile type, providing a common grammar for several classes of vocabulary. For example, it is possible, in some profile types, for terms to be contained within one another and be nested, which is suited to the expression of hierarchical vocabularies. Five profile types exist: ''lax'', ''thesaurus'', ''hierarchicalTokenTerms'', ‘glossaryOrDictionary’ and ''flatTokenTerms''. The lax profile is the least restrictive and offers the full VDEX model, whereas the flatTokenTerms profile is the most restrictive and lightweight.\n\nVDEX also offers some scope for complex vocabularies, assuming the existence of a well-defined application profile (for exchange interoperability). Some examples are:\n* ''Faceted schemes'' – faceted vocabularies are possible with the definition of appropriate relationships\n* ''Multi-lingual thesauri'' – metadata could be used within a relationship to achieve multilingual thesauri\n* ''Polyhierarchical taxonomies'' – can be expressed using the source/target value pairs in the relationship.\n\nIdentifiers in VDEX data should be persistent, unique, resolvable, transportable and URI-compliant. Specifically, vocabulary identifiers should be unique URIs, whereas term and relationship identifiers should be locally unique strings.\n\n== Implementations ==\n* [http://aloha2.netera.ca/ ALOHA Metadata Tagging Tool] — Java-based software project that can read IMS VDEX files.\n* [http://www.ivimeds.org/news/demonstrator.html IVIMEDS 1G v1.0] – from The International Virtual Medical School – includes VDEX instances in curriculum maps. Partners can create their own maps in VDEX format and use these to help students search the repository.\n* [http://www.elframework.org/projects/spws/view Skills Profiling Web Service] — project implemented and demonstrated use of a skills profiling web service using open standards in a medical context. IMS VDEX files were used in the representation of the SPWS hierarchy skills framework.\n* [http://www.scottishdoctor.org/ Scottish Doctors] — project used VDEX as a format for expressing curricular outcome systems.\n* [http://prs.heacademy.ac.uk/technical/vdex_scripts.html VDEX XSLT scripts] — developed by The Higher Education Academy Centre for Philosophical and Religious Studies to convert VDEX to XHTML and PostgreSQL .\n* [http://www.icbl.hw.ac.uk/vdex VDEX Implementation Project] — carried out by the Institute for Computer Based Learning at Heriot-Watt University, with a primary objective of creating a tool for editing vocabularies in VDEX format. The project, which ended in January 2004, was based on the Public Draft (not the current Final Specification).\n* [http://sourceforge.net/projects/vdex-j/ VDEX Java Binding] — implementation neutral Java interface for VDEX, as well as providing a default implementation of that interface, and XML marshalling functionality.\n* [https://pypi.python.org/pypi/imsvdex imsvdex Python egg] —  API for VDEX XML-files. It is free software written in [[Python (programming language)|Python]].\n* [http://plone.org/products/atvocabularymanager ATVocabularyManager] — addon for [[Plone]] CMS uses VDEX as a possible format to define vocabularies.\n* [https://pypi.python.org/pypi/collective.vdexvocabulary collective.vdexvocabulary] — implements IMS VDEX as standard [[Zope]] vocabulary which can also be used in [[Plone]] CMS, written in [[Python (programming language)|Python]].\n* [https://pypi.python.org/pypi/vdexcsv/ vdexcsv] — offers a commandline converter from [[Comma-separated values|CSV]] to VDEX. It is written in [[Python (programming language)|Python]].\n\n== See also ==\n*IMS Global\n*[[Learning object metadata]]\n\n==References==\n#{{note|coillie}} Marc van Coillie [http://www.eife-l.org/publications/standards/interop/europasscv/europassCV-IMS-AP/usingvdex Using IMS VDEX for the EDS AP - EIfEL]\n#{{note|sarasa}} Antonio Sarasa, Jose Manuel Canabal, Juan Carlos Sacristan, Raquel Jimenez [http://online-journals.org/i-jet/article/view/806 Using IMS VDEX in Agrega]\n\n== External links ==\n* [http://www.imsglobal.org/vdex IMS VDEX] — official resources by IMS global\n* [http://wiki.cetis.ac.uk/What_is_IMS_VDEX What is IMS VDEX] — JISC CETIS\n* [http://metadata.cetis.ac.uk/ CETIS Metadata and Digital Repository Special Interest Group (SIG)] — mailing list for those in UK Higher and Further Education  interested in creating, storing and serving educational metadata.\n\n[[Category:Data management]]\n[[Category:Educational technology standards]]\n[[Category:Knowledge representation]]\n[[Category:Library science]]\n[[Category:Metadata]]\n[[Category:Standards]]\n[[Category:Standards organizations]]\n[[Category:Technical communication]]"]
['Single source publishing', '1227094', '{{redirect|Single source}}\n\'\'\'Single source publishing\'\'\', also known as \'\'\'single sourcing publishing\'\'\', is a [[content management]] method which allows the same source [[Content (media)|content]] to be used across different forms of [[Media (communication)|media]] and more than one time.<ref>Kay Ethier, \'\'XML and FrameMaker\'\', pg. 19. [[New York City|New York]]: [[Apress]], 2004. ISBN 9781430207191</ref><ref>Lucas Walsh, "The Application of Single-Source Publishing to E-Government." Taken from \'\'Encyclopedia of Digital Government\'\', pg. 64. Eds. Ari-Veikko Anttiroiko and Matti Mälkiä. [[Hershey, Pennsylvania|Hershey]]: IGI Global, 2007. ISBN 9781591407904</ref><ref>[http://www.stylusstudio.com/single_source_publishing.html Single Source Publishing] at [[Stylus Studio]]. Copyright © 2005-2013 [[Progress Software]]. Accessed June 11, 2013.</ref><ref name=petra>[http://www.writersua.com/articles/singlesource/ Single Source Publishing with Flare]. Copyright © 2010 WritersUA. Published November 16, 2010; accessed June 11, 2013.</ref> The labour-intensive and expensive work of [[Technical editing#Technical editing|editing]] need only be carried out once, on only one document;<ref name=cms>Barry Schaeffer, [http://www.cmswire.com/cms/information-management/single-source-publishing-creating-customized-output-015069.php Single Source Publishing: Creating Customized Output]. CMS Wire, 3 April 2012. Accessed 10 June 2013.</ref> that source document can then be stored in one place and reused.<ref>[[Ann Rockley]] and Charles Cooper, [https://books.google.com/books?id=82X6jGY_dHMC&pg=PT75&dq=single+source+publishing&hl=en&sa=X&ei=rRehU_TxFs2W0QXT6YCQAQ&ved=0CDoQ6AEwBg#v=onepage&q=single%20source%20publishing&f=false Managing Enterprise Content: A Unified Content Strategy], Chapter 5: Product content. 2nd ed. [[Berkeley, California|Berkeley]]: [[New Riders Press]], 2012. ISBN 9780132931649</ref> This reduces the potential for error, as corrections are only made one time in the source document.<ref>Janet Mackenzie, \'\'The Editor\'s Companion\'\', pg. 92. [[Cambridge]]: [[Cambridge University Press]], 2011. ISBN 9781107402188</ref>\n\nThe benefits of single source publishing primarily relate to the editor rather than the [[User (computing)|user]]. The user does benefit from consistent terminology and information, but this consistency is also a potential weakness of single source publishing if the content manager does not have an organized [[Conceptualization (information science)|conceptualization]].<ref name=petra/> Single-source publishing is sometimes used synonymously with \'\'\'multi-channel publishing\'\'\' though whether or not the two terms are synonymous is a matter of discussion.<ref name=mek>[http://www.mekon.com/index.php/pages/knowledge_zone/single-sourcing-multi-channel-publishing/technology_standards Single Source & Multi-Channel Publishing]. © 2013 Mekon, accessed 23 June 2013.</ref>\n\n==Definition==\nWhile there is a general definition of single source publishing, there is no single official delineation between single source publishing and multi-channel publishing, nor are there any official governing bodies to provide such a delinieation. Single source publishing is most often understood as the creation of one source document in [[Microsoft Word]] or [[Adobe FrameMaker]] and converting that document into different [[file format]]s or human [[language]]s (or both) multiple times with minimal effort. Multi-channel publishing can either be seen as synonymous with single source publishing, or similar in that there is one source document but the process itself results in more than a mere reproduction of that source.<ref name=mek/>\n\n==History==\nThe origins of single-source publishing lie, indirectly, with the release of [[Windows 3.0]] in 1990.<ref name=bob162>Bob Boiko, [https://books.google.com/books?id=p6nUDn3ZaBoC&pg=PA162&dq=Single+source+publishing&hl=en&sa=X&ei=CUiqU6LEDaLV0QXd44CQDw&ved=0CCoQ6AEwAzgK#v=onepage&q=Single%20source%20publishing&f=false Content Management Bible], pg. 162. [[Hoboken, New Jersey|Hoboken]]: [[John Wiley & Sons]], 2005. ISBN 9780764583643</ref> With the eclipsing of [[MS-DOS]] by [[graphical user interface]]s, help files went from being unreadable text along the bottom of the screen to hypertext systems such as [[WinHelp]]. On-screen help interfaces allowed software companies to cease the printing of large, expensive help manuals with their products, reducing costs for both producer and consumer. This system raised opportunities as well, and many developers fundamentally changed the way they thought about publishing. Writers of [[software documentation]] did not simply move from being writers of traditional bound books to writers of [[electronic publishing]], but rather they became authors of central documents which could be reused multiple times across multiple formats.<ref name=bob162/>\n\nThe first single-source publishing project was started in 1993 by Cornelia Hofmann at [[Schneider Electric]] in [[Seligenstadt]], using software based on [[Interleaf]] to automatically create paper documentation in multiple languages based on a single original source file.<ref>[https://books.google.com/books?id=inCeft4AkXcC&pg=PA65&dq=single+source+publishing&hl=en&sa=X&ei=rRehU_TxFs2W0QXT6YCQAQ&ved=0CCoQ6AEwAw#v=onepage&q=single%20source%20publishing&f=false Translating Into Success: Cutting-edge Strategies for Going Multilingual in a Global Age], pg. 227. Eds. Robert C. Sprung and Simone Jaroniec. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2000. ISBN 9789027231871</ref>\n\n[[XML]], developed during the mid- to late-1990s, was also significant to the development of single source publishing as a method. XML, a markup language, allows developers to separate their documentation into two layers: a shell-like layer based on presentation and a core-like layer based on the actual written content. This method allows developers to write the content only one time while switching it in and out of multiple different formats and delivery methods.<ref>Doug Wallace and Anthony Levinson, "The XML e-Learning Revolution: Is Your Production Model Holding You Back?" Taken from [https://books.google.com/books?id=4RK7tJ-OO3cC&pg=PA65&dq=Single+source+publishing&hl=en&sa=X&ei=CUiqU6LEDaLV0QXd44CQDw&ved=0CEgQ6AEwCDgK#v=onepage&q=Single%20source%20publishing&f=false Best of The eLearning Guild\'s Learning Solutions: Articles from the eMagazine\'s First Five Years], pg. 63. Ed. Bill Brandon. Hoboken: John Wiley & Sons, 2008. ISBN 9780470277157</ref>\n\nIn the mid-1990s, several firms began creating and using single source content for technical documentation (Boeing Helicopter, Sikorsky Aviation and Pratt & Whitney Canada) and user manuals (Ford owners manuals) based on tagged SGML and XML content generated using the Arbortext Epic editor with add-on functions developed by a contractor.  The concept behind this usage was that complex, hierarchical content that did not lend itself to discrete componentization could be used across a variety of requirements by tagging the differences within a single document using the capabilities built into SGML and XML.\nFord, for example, was able to tag its single owner\'s manual files so that 12 model years could be generated via a resolution script running on the single completed file.  Pratt & Whitney, likewise, was able to tag up to 20 subsets of its jet engine manuals in single source files, calling out the desired version at publication time.  World Book Encyclopedia also used the concept to tag its articles for American and British versions of English.\n\nStarting from the early 2000s, single source publishing was used with an increasing frequency in the field of [[technical translation]]. It is still regarded as the most efficient method of publishing the same material in different languages.<ref>Bert Esselink, "Localisation and translation." Taken from [https://books.google.com/books?id=a4W7lWgCqYoC&pg=PA73&dq=Single+source+publishing&hl=en&sa=X&ei=CUiqU6LEDaLV0QXd44CQDw&ved=0CCUQ6AEwAjgK#v=onepage&q=Single%20source%20publishing&f=false Computers and Translation: A Translator\'s Guide], pg. 73. Ed. H. L. Somers. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2003. ISBN 9789027216403</ref> Once a printed manual was translated, for example, the online help for the software program which the manual accompanies could be automatically generated using the method.<ref>Burt Esselink, \'\'A Practical Guide to Localization\'\', pg. 228. Volume 4 of Language international world directory. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2000. ISBN 9781588110060</ref> [[Metadata]] could be created for an entire manual and individual pages or files could then be translated from that metadata with only one step, removing the need to recreate information or even database structures.<ref>Cornelia Hofmann and Thorsten Mehnert, "Multilingual Information Management at Schneider Automation." Taken from \'\'Translating Into Success\'\', pg. 67.</ref>\n\nAlthough single source publishing is now decades old, its importance has increased urgently as of the 2010s. As consumption of information products rises and the number of target audiences expands, so does the work of developers and content creators. Within the industry of software and its documentation, there is a perception that the choice is to embrace single source publishing or render one\'s operations obsolete.<ref name=cms/>\n\n==Criticism==\nSingle-source publishing has been criticized due to the quality of work, being compared to as the "conveyor belt assembly" of content creation by its critics.<ref>Mick Hiatt, [http://mashstream.com/mashups/the-myth-of-single-source-authoring/ The Myth of Single-Source Authoring]. Mashstream, November 18, 2009.</ref> \n\nWhile heavily used in technical translation, there are risks of error in regard to [[Index (publishing)|indexing]]. While two words might be [[synonym]]s in English, they may not be synonyms in another language. In a document produced via single sourcing, however, the index will be translated automatically and the two words will be rendered as synonyms because they are in the [[Source language (translation)|source language]], while in the [[Target language (translation)|target language]] they are not.<ref>Nancy Mulvany, [https://books.google.com/books?id=G0Eqm8FbiTMC&pg=PA312&dq=single+source+publishing&hl=en&sa=X&ei=rRehU_TxFs2W0QXT6YCQAQ&ved=0CEcQ6AEwCA#v=onepage&q=single%20source%20publishing&f=false Indexing Books], pg. 154. 2nd ed. [[Chicago]]: [[University of Chicago Press]], 2009. ISBN 9780226550176</ref>\n\n==See also==\n* [[Content management]]\n* [[Darwin Information Typing Architecture]]\n* [[EPUB]]\n* [[Markup language]]\n\n===List of single-source publishing tools=== \n* [[Adobe FrameMaker]]<ref>Sarah S. O\'Keefe, Sheila A. Loring, Terry Smith and Lydia K. Wong, [https://books.google.com/books?id=b-yEKgcQmN8C&pg=PA6&dq=single+source+publishing&hl=en&sa=X&ei=rRehU_TxFs2W0QXT6YCQAQ&ved=0CE0Q6AEwCQ#v=onepage&q=single%20source%20publishing&f=false Publishing Fundamentals: Unstructured FrameMaker 8], pg. 6. Scriptorium Publishing, 2008. ISBN 9780970473349</ref>\n* [[Apache Cocoon]]\n* [[Apache Forrest]]\n* [[Altova]]\n* [[Booktype]]\n* [[DocBook XSL]]\n* [[DITA Open Toolkit]]\n* [[Help & Manual]]\n* [[MadCap Flare]]\n* [[Oxygen XML Editor|Oxygen XML editor]]\n* [[Scenari]]\n* [[Sphinx (documentation generator)|Sphinx]]<ref>{{cite web | url = http://pythonic.pocoo.org/2008/3/21/sphinx-is-released | title = Sphinx is released! &raquo; And now for something completely Pythonic... | publisher = Georg Brandl| work = And now for something completely Pythonic... | accessdate = 2011-04-03 }}</ref>\n* [[XPLM Publisher]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite book | last = Ament | first = Kurt | authorlink = | title = Single Sourcing: Building Modular Documentation | publisher = William Andrew | date = 2007-12-17 |  location = | pages = 245 | url = | doi = | id = | isbn = 0-8155-1491-3 }}\n* {{cite book | last = Hackos | first = JoAnn T. | authorlink = | title = Content Management for Dynamic Web Delivery | publisher = Wiley | date = 2002-02-14 | location = | pages = 432 | url = | doi = | id = | isbn = 0-471-08586-3 }}\n* {{cite book | last = Glushko| first = Robert J. | authorlink = |author2=Tim McGrath | title = Document Engineering: Analyzing and Designing Documents for Business Informatics and Web Services | publisher = MIT Press| year = 2005 | location = | pages = 728| url = | doi = | id = | isbn = 0-262-57245-1 }}\n* {{cite book | last = Maler | first = Eve | authorlink = |author2=Jeanne El Andaloussi  | title = Developing SGML DTDs: From Text to Model to Markup | publisher = Prentice Hall PTR | date = 1995-12-15 | location = | pages = 560 | url = | doi = | id = | isbn = 0-13-309881-8 }} (the "bible" for Data Modeling)\n\n==External links==\n* [http://www.elkera.com/cms/articles/seminars_and_presentations/planning_a_single_source_publishing_application_for_business_documents/ Planning a Single Source Publishing Application for Business Documents] (A paper presented by Peter Meyer at OpenPublish, Sydney, on 29 July 2005)\n* [https://www.tug.org/TUGboat/tb29-1/tb91sojka.pdf Single-source publishing in multiple formats for different output devices]\n* [http://www.agilemodeling.com/essays/singleSourceInformation.htm Single Sourcing Information - An Agile Practice for Effective Documentation]\n* [http://www.stcsig.org/ss Society for Technical Communication Single-sourcing Special Interest Group]\n* [http://www.wisegeek.com/what-is-single-source-publishing.htm What Is Single Source Publishing?] at WiseGeek\n* [http://www.technical-communication.org/topics/information-development.html tekom Europe] (Articles about Information Development and Single Source Publishing)\n\n[[Category:Technical communication]]\n[[Category:Computer file systems]]\n[[Category:Data management]]']
['DAMA', '1390371', '{{about|the association||Dama (disambiguation)}}\n\n== About DAMA ==\n\n\'\'\'DAMA\'\'\' (the [http://dama.org Data Management Association]) is a not-for-profit, vendor-independent, international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management (IRM) and [[data resource management]] (DRM).\n\nDAMA\'s primary purpose is to promote the understanding, development and practice of managing information and data as a key enterprise asset. The group is organized as a set of more than 40 chapters and members-at-large around the world, with an International Conference held every year. \n\n== Chapters, Chapter Structure, and Central Membership ==\n\nDAMA International is organised through a Chapter structure, with each Chapter being a separate legal entity that formally affiliates with DAMA International. There are over 40 chapters established in over 16 countries around the world. The United States is disproportionately represented in the number of Chapters due to the number of city-based chapters as opposed to country-level in other jurisdictions.\n\nIn 2015 DAMA introduced a "Central" membership to help support and develop member services in a more consistent manner internationally and to provide a clear rallying point for members world wide who may lack a local chapter structure.\n\nA full listing of Chapters can be found on the [http://www.dama.org/browse-chapters DAMA International website]. This list does not include "chapters in formation" which have yet to meet the criteria for recognition as Chapters and formal affiliation with DAMA-I.\n\n== The Data Management Body of Knowledge (DMBOK) ==\n\nThe DAMA Guide to the [[Data management|Data Management]] Body of Knowledge" (DAMA-DMBOK Guide) was first published in April 5th, 2009.  \n\nIt defines ten knowledge domains which are at the core of Information and Data Management. \n* Data Governance (the central knowledge domain that connects all the others) \n* Data Architecture Management \n* Data Development \n* Data Operations Management \n* Data Security Management \n* Reference and Master Data Management \n* Data Warehousing and Business Intelligence \n* Document and Content Management \n* Metadata Management \n* Data Quality Management \nThe DMBOK is copyright DAMA International.\n\n== CDMP ==\n\nDAMA International is the owner of the [[Certified Data Management Professional]] certification. This Certification is based on a range of learning objectives derived from the DMBOK.\n\nIn October 2015, DAMA International terminated its relationship with ICCP who had provided administrative services for the delivery of the CDMP certification. \n\n== DAMA Awards ==\n\nFrom its inception in 1989 through to 2002, The DAMA Individual Achievement Awards have recognized a data professional who has made significant, demonstrable contributions to the information resource management industry consistent with DAMA International\'s vision.  From 2003 to 2015, DAMA created additional categories to recognize more people who have made special contributions to the world of data management.  The awards categories are:\n* Academic Achievement Award: To a member from academia for outstanding research or theoretical contributions in the area of IRM/DRM\n* DAMA Community Award : To a member of the DAMA community who has gone beyond the call of volunteer service to enhance the efforts of providing exceptional benefits to the DAMA Membership.\n* Government Achievement Award : To a member of the leadership populace for instituting the inclusion and adherence to DRM/IRM principles.\n* Professional Achievement Award : To a member from the ‘industry’ (business, discipline, specialist) who has made significant, demonstrable contributions to the IRM/DRM.\n* Lifetime Achievement and Contribution Award : This special award has been presented to John Zachman in 2002, Michael Brackett in 2006 and Catherine Nolan in 2015. \n\nStarting in 2016, DAMA International created the DAMA International Award for Data Management Excellence.  This award will be presented to organizations or individuals who have made contributions to data management principles.  The first awards under this new structure will be given in April, 2016.\n\n=== List of Award Winners ===\n\nA list of award winners can be found on the [https://www.dama.org/content/award-results DAMA website].\n\n=== Speakers Bureau ===\nDAMA International provides a speakers bureau service to connect conference and event organisers with internationally regarded expert speakers.\n\nA full listing of speakers can be found [http://www.dama.org/speakers here].\n\n== References ==\n<references />\n\n==External links==\n* [http://www.dama.org/ DAMA International]\n\n{{prof-assoc-stub}}\n[[Category:Data management]]']
['Versomatic', '9372544', "'''Versomatic''' installs as a file system service where it tracks file changes and preemptively archives a copy of a file before it is modified. Archiving copies pre-emptively obviates the need to archive a reference copy of the files beforehand, as would be the case if the files were archived after being edited.\n\nStarting from the moment Versomatic is installed, the last version of a file remains where the user expects it to be, and prior versions, if any, reside in a separate archive. Without this capability, we would have to scan your entire hard drive beforehand and make a duplicate copy of every file in order to create a baseline for subsequent revisions.\n\nFiles can be moved, renamed and copied without losing connection to their revision histories. Depending on user preferences, files can be monitored on local, removable and network drives. File revision histories are stored in a central database. Thus, for example, a user may insert a USB drive into his computer and edit a file on the USB drive. The edited file remains on the USB drive but a copy of the original unedited version is copied to the archive.\n\nWhen a file is deleted, the deleted file is added to the repository together with any previous versions of the deleted file. This provides excellent level of protection against inadvertent file deletion.\n\nAccess to prior versions of a file is easy. A user merely selects a file and clicks the right mouse button to display a contextual pop-up menu listing the X most recent previous versions of the file, if any.\n\nUpon user selection of one of the entries the appropriate previous version is opened with read-only privileges using the same application that created the file. Versomatic can also retrieve a copy of a previous version and move it into the same directory where the current version resides. The previous version will have a date & time stamp added to its file name in order to distinguish it from the current version. If a previous version is exported from Versomatic’s archive and changes are made thereto, a new revision history is created for the new file.\n\n==History==\n\nVersomatic is the first product from the long time team of [[Joaquin de Soto]], Jorge Miranda and Manny Menendez released under their new company [[Acertant]]. The team has a long list of hit products to their credit: MacLightning, [[ACD Canvas|Canvas]], Spelling Coach, BigThesaurus, Comment, UltraPaint, Artworks, [[ACDSee]], DenebaCAD, etc.\n\n==External links==\n*[http://www.acertant.com Company Web Page]\n\n[[Category:Data management]]"]
['Content re-appropriation', '2579709', "{{Orphan|date=February 2009}}\nFundamental to modern [[information architecture]]s, and driven by [http://www.webreference.com/internet/semantic/ semantic Web] technologies, '''content re-appropriation''' is the act of searching, filtering, gathering, grouping, and aggregation which allows information to be related, classified and identified.  This is achieved by applying syntactic or semantic meaning though intelligent tagging or artificial interpretation of fragmented content (see [[Resource Description Framework]]).  Hence, all information becomes valuable and interpretable.\n\n==Domain==\nSince the domain of Content applies to areas of [[software applications]], [[document]]s, and [[Computer media|media]], these can be processed though a pipeline of generation, aggregation, transform-many, and serialization (see [http://www.w3.org/TR/xml-pipeline/ XML Pipeline]).  The output of this can viewed in a medium most effect for decision making.\n\nThe desired outcomes of content re-appropriation are:\n\n*Seamless, Integrated, and Shared User experiences\n*[[Software visualization|Visualization]]\n*Detection, Analysis & Investigation\n*[[Personalization]] unique to the User\n*Inbound or Outbound [[web syndication|Syndication]] of Information\n*[[Publish]] or [[Subscribe]] to Information\n*Dynamically adapted output to Users medium\n\nEssentially to make ''information'' disparities transparent to the [[user (computing)|user]] - getting to the bottom line … quickly.\n\n==Areas of Use==\nContent re-appropriation is effective across the [[Content-Tier]], that is places where Content exists:\n\n*Identity & Directory Management e.g. [[Lightweight Directory Access Protocol|LDAP]], [[Security Assertion Markup Language|SAML]] & [[JNDI]]\n*Content Management e.g. [http://jakarta.apache.org/slide/ Apache Slide]\n*Content Systems e.g. [[File System]]s, [[E-mail]], [[Network share]]s, [[Storage Area Network|SAN]] & [[Database]]\n*Business Systems e.g. [[Enterprise resource planning|ERP]] & [[Customer Relationship Management|CRM]]\n*Data Warehouse e.g. [[OLAP]]\n*Internet & Web Services e.g. [[HyperText Transfer Protocol|HTTP]] & [[Simple Object Access Protocol|SOAP]]\n*[[Instant messenger|Presence]] and [[peer-To-Peer]]\n\n== See also ==\n* [[Knowledge visualization]]\n* [[Web indexing]]\n* [[Taxonomy (general)|Taxonomy]]\n\n[[Category:Data management]]\n[[Category:Technical communication]]"]
['Global concurrency control', '12380968', "{{POV|Commitment ordering|date=November 2011}}\n'''Global concurrency control''' typically pertains to the [[concurrency control]] of a system comprising several components, each with its own concurrency control. The overall concurrency control of the whole system, the ''Global concurrency control'', is determined by the concurrency control of its components, [[Modular programming|module]]s. In this case also the term '''Modular concurrency control''' is used.\n\nIn many cases a system may be distributed over a communication network. In this case we deal with [[distributed concurrency control]] of the system, and the two terms sometimes overlap. However, distributed concurrency control typically relates to a case where the distributed system's components do not have each concurrency control of its own, but rather are involved with a concurrency control mechanism that spans several components in order to operate. For example, as typical in a [[distributed database]].\n\nIn ''[[database systems]]'' and ''[[transaction processing]]'' (''transaction management'') global concurrency control relates to the concurrency control of a ''multidatabase system'' (for example, a [[Federated database]]; other examples are [[Grid computing]] and [[Cloud computing]] environments). It deals with the properties of the ''global [[schedule (computer science)|schedule]]'', which is the unified schedule of the multidatabase system, comprising all the individual schedules of the [[database system]]s and possibly other [[transactional object]]s in the system. A major goal for global concurrency control is ''[[Global serializability]]'' (or ''Modular serializability''). The problem of achieving global serializability in a [[heterogeneous]] environment had been [[open problem|open]] for many years, until an effective solution based on [[Commitment ordering]] (CO) has been proposed (see [[Global serializability]]). Global concurrency control deals also with [[global serializability#Relaxing global serializability|relaxed]] forms of global serializability which compromise global serializability (and in many applications also correctness, and thus are avoided there). While local (to a database system) [[Serializability#Relaxing serializability|relaxed serializability methods]] compromise serializability for performance gain (utilized when the application allows), it is unclear that the various proposed relaxed global serializability methods provide any performance gain over CO, which guarantees global serializability.\n\n\n==See also==\n*[[Concurrency control]]\n*[[Global serializability]]\n*[[Commitment ordering]]\n*[[Distributed concurrency control]]\n\n[[Category:Data management]]\n[[Category:Distributed computing problems]]\n[[Category:Databases]]\n[[Category:Concurrency control]]\n[[Category:Transaction processing]]"]
['Data governance', '6222875', '{{Governance}}\n\n\'\'\'Data governance\'\'\' is a [[Control (management)|control]] that ensures that the [[data]] entry by an operations team member or by automated processes meets precise standards, such as a business rule, a data definition and data integrity constraints in the data model. The data governor uses data quality monitoring against production data to communicate errors in data back to operational team members, or to the technical support team, for corrective action.  Data governance is used by organizations to exercise control over processes and methods used by their [[data stewards]] and [[data custodian]]s in order to improve data quality.\n\nData governance is a set of processes that ensures that important data assets are formally managed throughout the enterprise. Data governance ensures that data can be trusted and that people can be made accountable for any adverse event that happens because of low data quality. It is about putting people in charge of fixing and preventing issues with data so that the enterprise can become more efficient. Data governance also describes an evolutionary process for a company, altering the company’s way of thinking and setting up the processes to handle information so that it may be utilized by the entire organization. It’s about using technology when necessary in many forms to help aid the process. When companies desire, or are required, to gain control of their data, they empower their people, set up processes and get help from technology to do it.<ref name="sarsfield">Sarsfield, Steve (2009). "The Data Governance Imperative", IT Governance.</ref>\n\nAccording to one vendor, data governance is a [[quality control]] discipline for assessing, managing, using, improving, monitoring, maintaining, and protecting organizational information. It is a system of decision rights and accountabilities for information-related processes, executed according to agreed-upon models which describe who can take what actions with what information, and when, under what circumstances, using what methods.<ref name="The DGI Data Governance Framework">{{cite web|url=http://www.datagovernance.com/wp-content/uploads/2014/11/dgi_framework.pdf|title=The DGI Data Governance Framework}}</ref>\n\n== Overview ==\nData governance encompasses the people, processes, and [[information technology]] required to create a consistent and proper handling of an organization\'s data across the business enterprise.  Goals may be defined at all levels of the enterprise and doing so may aid in acceptance of processes by those who will use them.  Some goals include\n\n* Increasing consistency and confidence in [[decision making]]\n* Decreasing the risk of regulatory fines\n* Improving [[information security|data security]], also defining and verifying the requirements for data distribution policies<ref>Gianni, D., (2015, Jan). Data Policy Definition and Verification for System of Systems Governance, in Modeling and Simulation Support for System of Systems Engineering [http://onlinelibrary.wiley.com/doi/10.1002/9781118501757.ch5/summary]</ref> \n* Maximizing the income generation potential of data\n* Designating accountability for information quality\n* Enable better planning by supervisory staff\n* Minimizing or eliminating re-work\n* Optimize staff effectiveness\n* Establish process performance baselines to enable improvement efforts\n* Acknowledge and hold all gain\n\nThese goals are realized by the implementation of Data governance programs, or initiatives using Change Management techniques\n\n==Data governance drivers==\nWhile data governance initiatives can be driven by a desire to improve data quality, they are more often driven by C-Level leaders responding to external regulations. Examples of these regulations include [[Sarbanes-Oxley]], [[Basel I]], [[Basel II]], [[HIPAA]], [[General Data Protection Regulation|GDPR]] and a number of data privacy regulations. To achieve compliance with these regulations, business processes and controls require formal management processes to govern the data subject to these regulations.<ref>[http://www.rimes.com/rimes-data-governance-handbook \'Rimes Data Governance Handbook\'] [[RIMES]]</ref> Successful programs identify drivers meaningful to both supervisory and executive leadership.\n\nCommon themes among the external regulations center on the need to manage risk. The risks can be financial misstatement, inadvertent release of sensitive data, or poor data quality for key decisions. Methods to manage these risks vary from industry to industry. Examples of commonly referenced best practices and guidelines include [[COBIT]], [[ISO/IEC 38500]], and others. The proliferation of regulations and standards creates challenges for data governance professionals, particularly when multiple regulations overlap the data being managed. Organizations often launch data governance initiatives to address these challenges.\n\n== Data governance initiatives (Dimensions)==\nData governance initiatives improve [[data quality]] by assigning a team responsible for data\'s accuracy, accessibility, consistency, and completeness, among other metrics.  This team usually consists of executive leadership, [[project management]], [[line function|line-of-business managers]], and [[data steward]]s. The team usually employs some form of methodology for tracking and improving enterprise data, such as [[Six Sigma]], and tools for [[data mapping]], [[data profiling|profiling]], cleansing, and monitoring data.\n\nData governance initiatives may be aimed at achieving a number of objectives including offering better visibility to internal and external customers (such as [[supply chain]] management), compliance with [[compliance (regulation)|regulatory law]], improving operations after rapid company growth or [[mergers and acquisitions|corporate mergers]], or to aid the efficiency of enterprise [[knowledge worker]]s by reducing confusion and error and increasing their scope of knowledge. Many data governance initiatives are also inspired by past attempts to fix information quality at the departmental level, leading to incongruent and redundant data quality processes. Most large companies have many applications and databases that can\'t easily share information. Therefore, knowledge workers within large organizations often don\'t have access to the information they need to best do their jobs. When they do have access to the data, the [[data quality]] may be poor. By setting up a data governance practice or [[corporate data|Corporate Data]] Authority, these problems can be mitigated.\n\nThe structure of a data governance initiative will vary not only with the size of the organization, but with the desired objectives or the \'focus areas\' <ref name="focus areas">{{cite web|url=http://datagovernance.com/fc_focus_areas_for_data_governance.html |title=Data Governance Focus Areas |deadurl=yes |archiveurl=https://web.archive.org/web/20081006152845/http://www.datagovernance.com/fc_focus_areas_for_data_governance.html |archivedate=2008-10-06 |df= }}</ref> of the effort.\n\n== Implementation ==\nImplementation of a Data Governance initiative may vary in scope as well as origin. Sometimes, an executive mandate will arise to initiate an enterprise wide effort, sometimes the mandate will be to create a pilot project or projects, limited in scope and objectives, aimed at either resolving existing issues or demonstrating value. Sometimes an initiative will originate lower down in the organization’s hierarchy, and will be deployed in a limited scope to demonstrate value to potential sponsors higher up in the organization.  The initial scope of an implementation can vary greatly as well, from review of a one-off IT system, to a cross-organization initiative.\n\n== Data governance tools ==\nLeaders of successful data governance programs declared in December 2006 at the Data Governance Conference in Orlando, Fl, that data governance is between 80 and 95 percent communication."<ref>{{cite web\n |url=http://www.dmreview.com/issues/2007_48/10001356-1.html \n |title=Data Governance: One Size Does Not Fit All \n |last=Hopwood \n |first=Peter \n |authorlink=Peter Hopwood \n |publisher=[[DM Review Magazine]] \n |date=June 2008 \n |accessdate=2008-10-02 \n |archiveurl=http://www.webcitation.org/5bGHaz1gA?url=http://www.dmreview.com/issues/2007_48/10001356-1.html \n |archivedate=2008-10-02 \n |quote=At the inaugural Data Governance Conference in Orlando, Florida, in December 2006, leaders of successful data governance programs declared that in their experience, data governance is between 80 and 95 percent communication. Clearly, data governance is not a typical IT project. \n |deadurl=yes \n |df= \n}}</ref> That stated, it is a given that many of the objectives of a Data Governance program must be accomplished with appropriate tools. Many vendors are now positioning their products as Data Governance tools; due to the different focus areas of various data governance initiatives, any given tool may or may not be appropriate, in addition, many tools that are not marketed as governance tools address governance needs.<ref>{{cite web\n |url=http://www.datagovernancesoftware.com \n |title=DataGovernanceSoftware.com \n |publisher=[[The Data Governance Institute]] \n |accessdate=2008-10-02 \n |archiveurl=http://www.webcitation.org/5bGI3dfHV?url=http://www.datagovernancesoftware.com/ \n |archivedate=2008-10-02 \n |quote= \n |deadurl=yes \n |df= \n}}</ref>\n\n== Data governance organizations ==\n;DAMA International<ref>[http://www.dama.org/i4a/pages/index.cfm?pageid=1 DAMA International]</ref>\n:[[DAMA]] (the Data Management Association) is a not-for-profit, vendor-independent, international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management (IRM) and data resource management (DRM).\n\n;Data Governance Professionals Organization (DGPO)<ref>\n[http://www.dgpo.org/ Data Governance Professionals Organization<!-- Bot generated title -->]</ref>\n:The Data Governance Professionals Organization (DGPO) is a non-profit, vendor neutral, association of business, IT and data professionals dedicated to advancing the discipline of data governance.  The objective of the DGPO is to provide a forum that fosters discussion and networking for members and to encourage, develop and advance the skills of members working in the data governance discipline.\n\n;The Data Governance Society <ref>[http://www.datagovernancesociety.org Data Governance Society]</ref>\n:The Data Governance Society, Inc. is dedicated to fostering a new paradigm for the effective use and protection of information in which Data is governed and leveraged as a unique corporate asset.\n\n;The Data Governance Council <ref>[https://www-935.ibm.com/services/uk/cio/pdf/leverage_wp_data_gov_council_maturity_model.pdf Data Governance Council]</ref> \n:The Data Governance Council is an organization formed by IBM consisting of companies, institutions and technology solution providers with the stated objective to build consistency and quality control in governance, which will help companies better protect critical data."\n\n;IQ International -- the International Association for Information and Data Quality<ref>[http://iaidq.org/ IQ International, the International Association for Information and Data Quality]</ref>\n:IQ International is a not-for-profit, vendor neutral, professional association formed in 2004, dedicated to building the information and data quality profession.\n\n== Data governance conferences ==\nA number of major conferences relevant to data governance are held annually:\n;Data Governance and Information Quality Conference<ref>[http://dgiq-conference.com/ Data Governance and Information Quality Conference<!-- Bot generated title -->]</ref> \n:Commercial conferences held each year in the USA\n\n;Data Governance Conference Europe,<ref>[http://www.irmuk.co.uk/ Data Governance Conference Europe<!-- Bot generated title -->]</ref>\n:Commercial conferences held annually in London, England .\n;Information and Data Quality Conference<ref>[http://idq-conference.com/ Information and Data Quality Conference]</ref>\n:Not for profit conference run by IQ International in the USA\n;Master Data Management & Data Governance Conferences<ref>[http://www.tcdii.com/events/cdimdmsummitseries.html MDM SUMMIT Conference<!-- Bot generated title -->]</ref> \n:Six major conferences are run annually by the MDM Institute in London, San Francisco, Sydney, Toronto, Madrid, Frankfurt, and New York City.\n;Financial Information Summit series of conferences<ref>[http://www.financialinformationsummit.com<!-- Bot generated title -->]</ref> \n;Hosted by Inside Reference Data magazine in New York, London, Hong Kong, Toronto, Chicago, Frankfurt, Paris and Tokyo.\n\n==See also==\n* [[Information Architecture]]\n* [[Information technology governance]]\n* [[Semantics of Business Vocabulary and Business Rules]]\n* [[Master data management]]\n* [[COBIT]]\n* [[ISO/IEC 38500]]\n* [[ISO/TC 215]]\n* [[Operational risk management]]\n* [[Basel II Accord]]\n* [[HIPAA]]\n* [[Sarbanes-Oxley Act]]\n* [[Information technology controls]]\n* [[Data Protection Directive]] (EU)\n* [[Universal Data Element Framework]]\n* [[Asset Description Metadata Schema]]\n\n==References==\n<!--to cite a web resource, use this template\n<ref>{{cite web\n  | url = MANDATORY\n  | title = MANDATORY\n  | last =\n  | first =\n  | authorlink =\n  | coauthors =\n  | work =\n  | publisher =\n  | date =\n  | format =\n  | language=\n  | doi =\n  | accessdate =  \n  | archiveurl = SHOULD BE USED ON PAGES ALLOWING ARCHIVING - USE A SERVICE LIKE webcitation.org or archive.org\n  | archivedate = MANDATORY IF archiveurl\n  | quote = \n }}</ref>\n-->\n{{reflist}}\n\n[[Category:Information technology governance]]\n[[Category:Data management]]']
['Microsoft Office PerformancePoint Server', '9562761', "{{multiple issues|\n{{more footnotes|date=August 2013}}\n{{refimprove|date=August 2013}}\n}}\n\n{{Infobox Software\n| name = Microsoft Office PerformancePoint Server\n| developer = [[Microsoft]]\n| released = {{Start date|2007|11}}\n| latest_release_version = 1.0 SP2\n| latest_release_date = 2008\n| operating_system = [[Microsoft Windows]]\n| genre = [[Enterprise Performance Management]]\n| license = [[Proprietary software|Proprietary]] [[EULA]]\n| website = [https://web.archive.org/web/20071016055516/http://www.microsoft.com/business/performancepoint/ www.microsoft.com/business/performancepoint]\n}}\n'''Microsoft Office PerformancePoint Server''' is a [[business intelligence]] [[Computer software|software]] product released in 2007 by [[Microsoft]]. Although discontinued in 2009, the dashboard, scorecard, and analytics capabilities of PerformancePoint Server were incorporated into [[Sharepoint 2010|SharePoint 2010]] and later versions.\n\nPerformancePoint Server also provided a planning and budgeting component directly integrated with Excel.\n\n==History==\n\nMicrosoft offered preview releases of PerformancePoint Server starting in mid-2006. Previews of the product were formed from [[Business Scorecard Manager 2005]] and the Planning Server component. Acquisitions [[ProClarity Corporation|ProClarity]] and [[Great Plains Software|Great Plains]] brought additional analytics and planning/reporting capabilities, as well as companion products ProClarity 6.3 and [[Microsoft FRx|FRx]].\n\nPerformancePoint Server was officially released in November 2007.\n\nMicrosoft discontinued PerformancePoint Server as an independent product in 2009 and folded its dashboard, scorecard and analytics capabilities into PerformancePoint Services in [[SharePoint Server 2010]].<ref>{{cite web |url=http://www.informationweek.com/news/business_intelligence/analytics/showArticle.jhtml?articleID=212902915&subSection=Business+Intelligence |title=Microsoft Makes Sweeping Changes To BI Software Strategy |date=January 27, 2009 |last=Weier |first=Mary Hayes |work=[[InformationWeek]]}}</ref>\n\n==Monitoring Server Component==\nBusiness monitoring capabilities, including dashboards, scorecards & key performance indicators, navigable reports for deeper analysis, strategy maps, and linked filtering, are provided by PerformancePoint's Monitoring Server component. A Dashboard Designer application that is distributed from Monitoring Server enables business analysts or IT Administrators to:\n\n* create & test data source connections\n* create views that use those data connections\n* assemble the views into a dashboard\n* deploy the dashboard as a [[SharePoint]] page\n\nDashboard Designer saved content and security information back to the Monitoring Server. Data source connections, such as OLAP cubes or relational tables, were also made through Monitoring Server.\n \nAfter a dashboard has been published to the Monitoring Server database, it would be deployed as a SharePoint page and shared with other users as such. When the pages were opened in a web browser, Monitoring Server updated the data in the views by connecting back to the original data sources.\n\n==Planning Server Component==\nPerformancePoint's Planning Server component supported maintenance of logical business models, budget & approval workflows, enterprise data sources, and it followed [[Generally Accepted Accounting Principles]].\n\nPlanning Server made use of Excel for input and line-of-business reporting, as well as SQL Server for storing and processing business models.\n\n==Management Reporter Component==\nThe Management Reporter component was designed to perform financial reporting and can read PerformancePoint Planning models directly. A development kit was also available to allow this component to read other models .{{which|date=August 2013}}\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://msdn2.microsoft.com/en-us/office/bb660518.aspx PerformancePoint Server 2007 Developer Portal]\n* [http://blogs.technet.com/datapuzzle Data Puzzle]\n* [http://performancepointinsider.com/blogs/default.aspx PerformancePoint Insider]\n* [http://alanwhitehouse.wordpress.com/2009/01/26/pps-planning-being-discontinued/ Performance Point Planning being discontinued]\n\n{{Microsoft Office}}\n\n[[Category:Microsoft Office servers]]\n[[Category:Business intelligence]]\n[[Category:Data management]]"]
['Category:Data-centric programming languages', '925067', '{{Cat main|Data-centric programming language}}\n\nThis [[Wikipedia:Category|category]] lists those [[programming languages]] that are data-centric, with significant built-in functionality for data storage and manipulation.\n\n[[Category:Data management|Programming languages]]\n[[Category:Persistent programming languages]]']
['Edge data integration', '14124901', "{{Refimprove|date=January 2008}}\nAn '''edge data integration''' is an implementation of [[data integration]] technology undertaken in an ad hoc or tactical fashion. This is also sometimes referred to as point-to-point integration because it connects two types of data directly to serve a narrow purpose. Many edge integrations, and actually the vast majority of all data integration, involves hand-coded scripts. Some may take the form of [[Business Mashups]] (web application hybrids), [[Rich Internet application]]s, or other browser-based models that take advantage of [[Web 2.0]] technologies to combine data in a Web browser.\n\nExamples of edge data integration projects might be:\n\n* extracting a list of customers from a host [[Sales Force Automation]] application and writing the results to an [[Microsoft Excel|Excel]] spreadsheet\n* creating a script-driven framework for managing [[RSS]] feeds\n* combining data from a weather Web site, a shipping company's Web site, and a company's internal logistics database to track shipments and estimated arrival times of packages\n\nIt has been claimed that edge data integration do not typically require large budgets and centrally managed technologies, which is in contrast to a [[core data integration]].\n\n== See also ==\n* [[core data integration]]\n* [[Business Mashups]]\n* [[Rich Internet application]]\n* [[Web 2.0]]\n* [[Yahoo! Pipes]]\n* [[Microsoft Popfly]]\n*[[IBM Mashup Center]]\n\n[[Category:Data management]]"]
['Paper data storage', '13756939', '{{refimprove|date=August 2012}}\n\'\'\'Paper data storage\'\'\' refers to the use of [[paper]] as a [[data storage device]]. This includes [[writing]], [[illustrating]], and the use of data that can be interpreted by a machine or is the result of the functioning of a machine.  A defining feature of paper data storage is the ability of humans to produce it with only simple tools and interpret it visually.\n\nThough this is now mostly obsolete, paper was once also an important form of [[computer data storage]].\n\n==History==\nBefore paper was used for storing data, it had been used in several applications for storing instructions to specify a machine\'s operation.  The earliest use of paper to store instructions for a machine was the work of [[Basile Bouchon]] who, in 1725, used punched paper rolls to control textile looms.  This technology was later developed into the wildly successful [[Jacquard loom]].  The 19th century saw several other uses of paper for controlling machines.  In 1846, telegrams could be prerecorded on [[punched tape]] and rapidly transmitted using [[Alexander Bain (inventor)|Alexander Bain]]\'s automatic telegraph.  Several inventors took the concept of a mechanical organ and used paper to represent the music.  \n\nIn the late 1880s [[Herman Hollerith]] invented the recording of data on a medium that could then be read by a machine.  Prior uses of machine readable media, above, had been for control ([[automaton]]s, [[piano roll]]s, [[Jacquard loom|looms]], ...), not data.  "After some initial trials with paper tape, he settled on [[punched card]]s..."<ref>[http://www.columbia.edu/acis/history/hollerith.html Columbia University Computing History - Herman Hollerith]</ref>  Hollerith\'s method was used in the 1890 census.<!-- The Census Bureau is not "an independent 3rd party" source - as required by Wikipedia - for Census Bureau performance claims. FOLLOWING CLAIM DELETED. --- and the completed results were "... finished months ahead of schedule and far under budget".<ref>[http://www.census.gov/history/www/technology/010873.html U.S. Census Bureau: Tabulation and Processing]</ref>-->  Hollerith\'s company eventually became the core of [[International Business Machines|IBM]]. \n\nOther technologies were also developed that allowed  machines to work with marks on paper instead of punched holes.  This technology was widely used for [[optical scan voting system|tabulating votes]] and grading [[scantron|standardized tests]].  [[Barcode]]s made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it. Banks used magnetic ink on checks, supporting MICR scanning.\n\nIn an early electronic computing device, the [[Atanasoff-Berry Computer]], electric sparks were used to singe small holes in paper cards to represent binary data.  The altered [[dielectric constant]] of the paper at the location of the holes could then be used to read the binary data back into the machine by means of electric sparks of lower voltage than the sparks used to create the holes.  This form of paper data storage was never made reliable and was not used in any subsequent machine.\n\nAs of 2014, [[Universal Product Code]] barcodes, first used in 1974, are ubiquitous.\n\nSome people recommend a width of at least 3 pixels for each minimum-width gap and each minimum-width bar for 1D barcodes;\nand a width of at least 4 pixels—e.g., a 4&nbsp;×&nbsp;4 pixel = 16 pixel module for [[2D barcode]]s.<ref>\nAccusoft.\n[http://www.accusoft.com/whitepapers/barcodes/BarcodesinDocuments-BestPractices.pdf "Using Barcodes in Documents – Best Practices"].\n2007.\nRetrieved 2014-04-25.\n</ref>\nFor a typical black-and-white barcode scanned by a typical 300 dpi [[image scanner]],\nand assuming roughly half the space is occupied by finder patterns, fiducial alignment patterns, and error detection and correction codes, that recommendation gives a maximum data density of roughly 50 bits per linear inch (about 2 bit/mm) for 1D barcodes, and roughly 2 800 bits per square inch (about 4.4 bit/mm<sup>2</sup>).\n\n==Limits==\nThe limits of data storage depend on the technology to write and read such data.  For example, an 8″&nbsp;×&nbsp;10″ (roughly A4 without margins) 300dpi 8-bit greyscale image map contains 7.2 megabytes of data—assuming a scanner can accurately reproduce the printed image to that resolution and [[color depth]], and a program can accurately interpret such an image.  A similarly sized image in 2400dpi 24-bit true color theoretically contains 1.38 gigabytes of information.\n\n==See also==\n{{div col|3}}\n*[[Banknote]] read by a [[vending machine]]\n*[[Book music]]\n*[[Edge-notched card]]\n*[[Index card]]\n*[[Kimball tag]]\n*[[Machine-readable medium]]\n*[[Magnetic ink character recognition]]\n*[[Mark sense]]\n*[[Music roll]]\n*[[Optical mark recognition]]\n*[[Paper disc]]\n*[[Perfin]]\n*[[Perforation]]\n*[[Punched tape]]\n*[[Spindle (stationery)]]\n*[[Stenotype]]\n*[[Ticker tape]]\n{{div col end}}\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.microglyphs.com/english/html/dataglyphs.shtml DataGlyphs]\n* [http://ollydbg.de/Paperbak/ PaperBack data storage]\n{{Paper data storage media}}\n\n[[Category:Data management]]\n[[Category:Storage media]]']
['Long-lived transaction', '17866900', '{{Multiple issues|\n{{Orphan|date=February 2009}}\n{{No sources|date=October 2015}}\n}}\n\nA \'\'\'long-lived transaction\'\'\' is a [[Database transaction|transaction]] that spans multiple database transactions. The transaction is considered "long-lived" because its boundaries must, by necessity of business logic, extend past a single database transaction. A long-lived transaction can be thought of as a sequence of database transactions grouped to achieve a single atomic result.\n\nA common example is a multi-step sequence of requests and responses of an interaction with a user through a web client.\n\nA long-lived transaction creates challenges of [[concurrency control]] and [[scalability]].\n\nA chief strategy in designing long-lived transactions is [[optimistic concurrency control]] with [[Version control|versioning]].\n\nSo much research work related to these long lived transactions was carried out by several professors from the Oxford University and Michigan State University and the Central University of Hyderabad. Dr. James from the Oxford University created several hypotheses for long-lived transactions. Dr Copperfield of the Michigan State University was regarded highly for his contributions in this field. Dr A B Sagar of Hyderabad Central University has also done very creative work in relating long-lived transactions with financial transactions in Microfinance.\n\nHowever the study is not complete and is still open to challenges and research issues.\n\n==See also==\n*[[Long-running transaction]]\n\n[[Category:Data management]]\n[[Category:Transaction processing]]\n\n\n{{software-eng-stub}}']
['Retention period', '3544624', 'The \'\'\'retention period\'\'\' of information is an aspect of [[records management|records and information management]] (RIM) and the [[records life cycle]].  It identifies the duration of time for which the information should be maintained or "retained", irrespective of format (paper, electronic, or other).  Retention periods vary on different types of information, based on content and a variety of other factors including: internal organizational need, regulatory requirements for inspection or audit, legal statutes of limitation, involvement in litigation, taxation and financial reporting needs, as well as other factors as defined by local, regional, state, national and/or international governing entities.  \n\nOnce an applicable retention period has elapsed for a given type or series of information, and all holds/moratoriums have been released, the information is typically destroyed using an approved and effective destruction method, which renders the information completely and irreversibly unusable via any means.  Information with historical value beyond its "usable value" may be accessioned to the custody of an archive organization for permanent or extended long-term preservation.  \n\n==Defensible retention==\n\'\'Defensible retention\'\' refers to the ability of an identified and applied retention period to effectively provide for the defense of the record, and its eventual destruction or accessioning when scrutinized within a court of law or by other review.\n\nIt is commonly advised by [[Records management|Records and Information Management]] (RIM) professionals that any and all retention periods applied to organizational information should be reviewed and approved for use by competent legal counsel, which represents the organization, and is familiar with the specific legal and regulatory requirements of the organization.\n\n==Guidance and education organizations==\n*[[ARMA International]]\n*[[Information and Records Management Society]]\n\n==See also==\n*[[Retention schedule]]\n\n==References==\n<references/>\n\n[[Category:Legal documents]]\n[[Category:Data management]]\n[[Category:Public records]]\n[[Category:Records management]]']
['Category:Document-oriented databases', '19642057', "A '''document-oriented database''' is a [[database management system]] designed for document-oriented applications\n{{Cat main|Document-oriented database}}\n{{see also|Document-oriented database#Implementations}}\n{{see also cat|Full text databases}}\n{{see also cat|Key-value databases}}\n\n[[Category:Data management]]\n[[Category:Database management systems]]\n[[Category:Types of databases]]"]
['Metadata controller', '21423528', "'''Metadata controller''' (or MDC) is a [[storage area network]] (SAN) technology for managing [[file locking]], space allocation and data access authorization.\nThis is needed when several clients are given block level access to the same disk volume, [[Computer data storage|data storage]] sharing.\n\nThe abstract for the patent describing this technology can be read [http://www.freepatentsonline.com/7448077.html here]\n\n[[Category:Data management]]\n[[Category:Telecommunications engineering]]\n[[Category:Storage area networks]]\n[[Category:Local area networks]]\n\n{{compu-storage-stub}}"]
['Consumer relationship system', '21395468', '\'\'\'Consumer relationship systems\'\'\' (\'\'\'CRS\'\'\') are specialized [[customer relationship management]] (CRM) [[software]] applications that are used to handle a company\'s dealings with its customers.<ref name ="Insight44-50">[http://www.nxtbook.com/nxtbooks/cmp/cmi_200709/index.php ICMI Customer Management Insight Magazine, September 2007, pp 44–50], Retrieved 11 January 2012</ref>\n\nCurrent consumer relationship systems integrate the [[software]] with telephone and call recording systems as well as with corporate systems for input and reporting. Customers can provide input from the company\'s website directly into the CRS. These systems are popular because they can deliver the \'voice of the consumer\' that contributes to product quality improvement and that ultimately increases corporate profits.<ref name ="Insight44-50" />\n\nConsumer relationship systems that provide automated support as well as advanced systems may have [[artificial intelligence]] (AI) interfaces that can extract and analyse [[data]] collected, or handle basic questions and complaints.<ref>{{cite web|last1=Smith| first1=S.E.|title= What is Consumer Relationship System? |date= |publisher= WiseGeek.net|url= http://www.wisegeek.net/what-is-consumer-relationship-system.htm|accessdate=1 February 2013}}</ref>\n\n==History==\nThe first CRS was developed in the 1980s. In 1981 Michael Wilke and Robert Thornton founded Wilke/Thornton, Inc in [[Columbus, Ohio]], to develop new CRS software.<ref>[http://www.wilke-thornton.com/WTI/Pages/products.html Wilke/Thornton, Inc Products] Retrieved 11 January 2012</ref>\n\n==See also==\n* [[ECRM]]\n* [[Business intelligence]]\n* [[Customer experience]]\n* [[Customer intelligence]]\n* [[Customer service]] – contains ISO standards\n* [[Customer value maximization]]\n* [[Enterprise relationship management]] (ERM)\n* [[Sales force management system]]\n* [[Sales intelligence]]\n* [[Sales process engineering]]\n\n== References ==\n{{reflist}}\n\n[[Category:Business intelligence]]\n[[Category:Data management]]']
['Two-phase commit protocol', '787850', '{{Redirect|2PC|the play in American and Canadian football|Two-point conversion|the cryptographic protocol|Commitment scheme}}\n\nIn [[transaction processing]], [[database]]s, and [[computer networking]], the \'\'\'two-phase commit protocol\'\'\' (\'\'\'2PC\'\'\') is a type of [[Atomic commit|atomic commitment protocol]] (ACP). It is a [[distributed algorithm]] that coordinates all the processes that participate in a [[Distributed transaction|distributed atomic transaction]] on whether to \'\'[[Commit (data management)|commit]]\'\' or \'\'abort\'\' (\'\'roll back\'\') the transaction (it is a specialized type of [[Consensus (computer science)|consensus]] protocol). The protocol achieves its goal even in many cases of temporary system failure (involving either process, network node, communication, etc. failures), and is thus widely used.<ref name="bernstein1987">[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  \'\'Concurrency Control and Recovery in Database Systems\'\'], Chapter 7, Addison Wesley Publishing Company, ISBN 0-201-10715-5</ref><ref name="weikum2001">[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  \'\'Transactional Information Systems\'\'], Chapter 19, Elsevier, ISBN 1-55860-508-8</ref><ref name=Bern2009>Philip A. Bernstein, Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 \'\'Principles of Transaction Processing\'\', 2nd Edition], Chapter 8, Morgan Kaufmann (Elsevier), ISBN 978-1-55860-623-4</ref>\nHowever, it is not resilient to all possible failure configurations, and in rare cases, user (e.g., a system\'s administrator) intervention is needed to remedy an outcome. To accommodate recovery from failure (automatic in most cases) the protocol\'s participants use [[Server log|logging]] of the protocol\'s states. Log records, which are typically slow to generate but survive failures, are used by the protocol\'s [[recovery procedure]]s. Many protocol variants exist that primarily differ in logging strategies and recovery mechanisms. Though usually intended to be used infrequently, recovery procedures compose a substantial portion of the protocol, due to many possible failure scenarios to be considered and supported by the protocol.\n\nIn a "normal execution" of any single [[distributed transaction]] ( i.e., when no failure occurs, which is typically the most frequent situation), the protocol consists of two phases:\n#The \'\'commit-request phase\'\' (or \'\'voting phase\'\'), in which a \'\'coordinator\'\' process attempts to prepare all the transaction\'s participating processes (named \'\'participants\'\', \'\'cohorts\'\', or \'\'workers\'\') to take the necessary steps for either committing or aborting the transaction and to \'\'vote\'\', either "Yes": commit (if the transaction participant\'s local portion execution has ended properly), or "No": abort (if a problem has been detected with the local portion), and\n#The \'\'commit phase\'\', in which, based on \'\'voting\'\' of the cohorts, the coordinator decides whether to commit (only if \'\'all\'\' have voted "Yes") or abort the transaction (otherwise), and notifies the result to all the cohorts. The cohorts then follow with the needed actions (commit or abort) with their local transactional resources (also called \'\'recoverable resources\'\'; e.g., database data) and their respective portions in the transaction\'s other output (if applicable).\n\nNote that the two-phase commit (2PC) protocol should not be confused with the [[two-phase locking]] (2PL) protocol, a [[concurrency control]] protocol.\n\n==Assumptions==\nThe protocol works in the following manner: one node is a designated \'\'\'coordinator\'\'\', which is the master site, and the rest of the nodes in the network are designated the \'\'\'cohorts\'\'\'. The protocol assumes that there is [[stable storage]] at each node with a [[Write ahead logging|write-ahead log]], that no node crashes forever, that the data in the write-ahead log is never lost or corrupted in a crash, and that any two nodes can communicate with each other. The last assumption is not too restrictive, as network communication can typically be rerouted. The first two assumptions are much stronger; if a node is totally destroyed then data can be lost.\n\nThe protocol is initiated by the coordinator after the last step of the transaction has been reached. The cohorts then respond with an \'\'\'agreement\'\'\' message or an \'\'\'abort\'\'\' message depending on whether the transaction has been processed successfully at the cohort.\n\n==Basic algorithm==\n\n===Commit request phase===\nor \'\'\'voting phase\'\'\'\n\n#The coordinator sends a \'\'\'query to commit\'\'\' message to all cohorts and waits until it has received a reply from all cohorts.\n#The cohorts execute the transaction up to the point where they will be asked to commit.  They each write an entry to their \'\'undo log\'\' and an entry to their \'\'[[redo log]]\'\'.\n#Each cohort replies with an \'\'\'agreement\'\'\' message (cohort votes \'\'\'Yes\'\'\' to commit), if the cohort\'s actions succeeded, or an \'\'\'abort\'\'\' message (cohort votes \'\'\'No\'\'\', not to commit), if the cohort experiences a failure that will make it impossible to commit.\n\n===Commit phase===\nor \'\'\'Completion phase\'\'\'\n\n====Success====\nIf the coordinator received an \'\'\'agreement\'\'\' message from \'\'all\'\' cohorts during the commit-request phase:\n#The coordinator sends a \'\'\'commit\'\'\' message to all the cohorts.\n#Each cohort completes the operation, and releases all the locks and resources held during the transaction.\n#Each cohort sends an \'\'\'acknowledgment\'\'\' to the coordinator.\n#The coordinator completes the transaction when all acknowledgments have been received.\n\n====Failure====\nIf \'\'any\'\' cohort votes \'\'\'No\'\'\' during the commit-request phase (or the coordinator\'s timeout \'\'\'expires\'\'\'):\n#The coordinator sends a \'\'\'rollback\'\'\' message to all the cohorts.\n#Each cohort undoes the transaction using the undo log, and releases the resources and locks held during the transaction.\n#Each cohort sends an \'\'\'acknowledgement\'\'\' to the coordinator.\n#The coordinator undoes the transaction when all acknowledgements have been received.\n\n====Message flow====\n<pre>\nCoordinator                                         Cohort\n                              QUERY TO COMMIT\n                -------------------------------->\n                              VOTE YES/NO           prepare*/abort*\n                <-------------------------------\ncommit*/abort*                COMMIT/ROLLBACK\n                -------------------------------->\n                              ACKNOWLEDGMENT        commit*/abort*\n                <--------------------------------  \nend\n</pre>\nAn * next to the record type means that the record is forced to stable storage.<ref name="mohan1986">[[C. Mohan]], Bruce Lindsay and R. Obermarck (1986): [http://dl.acm.org/citation.cfm?id=7266  "Transaction management in the R* distributed database management system"],\'\'ACM Transactions on Database Systems (TODS)\'\', Volume 11 Issue 4, Dec. 1986, Pages 378 - 396</ref>\n\n==Disadvantages==\nThe greatest disadvantage of the two-phase commit protocol is that it is a blocking protocol. If the coordinator fails permanently, some cohorts will never resolve their transactions: After a cohort has sent an \'\'\'agreement\'\'\' message to the coordinator, it will block until a \'\'\'commit\'\'\' or \'\'\'rollback\'\'\' is received.\n\n==Implementing the two-phase commit protocol==\n\n===Common architecture===\nIn many cases the 2PC protocol is distributed in a computer network. It is easily distributed by implementing multiple dedicated 2PC components similar to each other, typically named \'\'[[Transaction manager]]s\'\' (TMs; also referred to as \'\'2PC agents\'\' or Transaction Processing Monitors), that carry out the protocol\'s execution for each transaction (e.g., [[The Open Group]]\'s [[X/Open XA]]). The databases involved with a distributed transaction, the \'\'participants\'\', both the coordinator and cohorts, \'\'register\'\' to close TMs (typically residing on respective same network nodes as the participants) for terminating that transaction using 2PC. Each distributed transaction has an ad hoc set of TMs, the TMs to which the transaction participants register. A leader, the coordinator TM, exists for each transaction to coordinate 2PC for it, typically the TM of the coordinator database. However, the coordinator role can be transferred to another TM for performance or reliability reasons. Rather than exchanging 2PC messages among themselves, the participants exchange the messages with their respective TMs. The relevant TMs communicate among themselves to execute the 2PC protocol schema above, "representing" the respective participants, for terminating that transaction. With this architecture the protocol is fully distributed (does not need any central processing component or data structure), and scales up with number of network nodes (network size) effectively.\n\nThis common architecture is also effective for the distribution of other [[atomic commitment protocol]]s besides 2PC, since all such protocols use the same voting mechanism and outcome propagation to protocol participants.<ref name="bernstein1987" /><ref name="weikum2001" />\n\n===Protocol optimizations===\n[[Database]] research has been done on ways to get most of the benefits of the two-phase commit protocol while reducing costs by \'\'protocol optimizations\'\'<ref name="bernstein1987" /><ref name="weikum2001" /><ref name="Bern2009" /> and protocol operations saving under certain system\'s behavior assumptions.\n\n====Presumed Abort and Presumed Commit====\n\'\'Presumed abort\'\' or \'\'Presumed commit\'\' are common such optimizations.<ref name="weikum2001" /><ref name=Bern2009/><ref name="mohan1983">[[C. Mohan]], Bruce Lindsay (1985): [http://portal.acm.org/citation.cfm?id=850772  "Efficient commit protocols for the tree of processes model of distributed transactions"],\'\'ACM SIGOPS Operating Systems Review\'\',\n19(2),pp. 40-52 (April 1985)</ref> An assumption about the outcome of transactions, either commit, or abort, can save both messages and logging operations by the participants during the 2PC protocol\'s execution. For example, when presumed abort, if during system recovery from failure no logged evidence for commit of some transaction is found by the recovery procedure, then it assumes that the transaction has been aborted, and acts accordingly. This means that it does not matter if aborts are logged at all, and such logging can be saved under this assumption. Typically a penalty of additional operations is paid during recovery from failure, depending on optimization type. Thus the best variant of optimization, if any, is chosen according to failure and transaction outcome statistics.\n\n====Tree two-phase commit protocol====\nThe \'\'\'[[Tree (data structure)|Tree]] 2PC protocol\'\'\'<ref name="weikum2001" /> (also called \'\'Nested 2PC\'\', or \'\'Recursive 2PC\'\') is a common variant of 2PC in a [[computer network]], which better utilizes the underlying communication infrastructure. The participants in a distributed transaction are typically invoked in an order which defines a tree structure, the \'\'invocation tree\'\', where the participants are the nodes and the edges are the invocations (communication links). The same tree is commonly utilized to complete the transaction by a 2PC protocol, but also another communication tree can be utilized for this, in principle. In a tree 2PC the coordinator is considered the root ("top") of a communication tree (inverted tree), while the cohorts are the other nodes. The coordinator can be the node that originated the transaction (invoked recursively (transitively) the other participants), but also another node in the same tree can take the coordinator role instead. 2PC messages from the coordinator are propagated "down" the tree, while messages to the coordinator are "collected" by a cohort from all the cohorts below it, before it sends the appropriate message "up" the tree (except an \'\'\'abort\'\'\' message, which is propagated "up" immediately upon receiving it or if the current cohort initiates the abort).\n\nThe \'\'\'Dynamic two-phase commit\'\'\' (Dynamic two-phase commitment, D2PC) \'\'\'protocol\'\'\'<ref name="weikum2001" /><ref name="raz1995">[[Yoav Raz]] (1995): [http://www.springerlink.com/content/pv12p828kk616258/  "The Dynamic Two Phase Commitment (D2PC) protocol "],\'\'Database Theory — ICDT \'95\'\', \'\'Lecture Notes in Computer Science\'\', Volume 893/1995, pp. 162-176, Springer, ISBN 978-3-540-58907-5</ref> is a variant of Tree 2PC with no predetermined coordinator. It subsumes several optimizations that have been proposed earlier. \'\'\'Agreement\'\'\' messages (\'\'\'Yes\'\'\' votes) start to propagate from all the leaves, each leaf when completing its tasks on behalf of the transaction (becoming \'\'ready\'\'). An intermediate (non leaf) node sends when \'\'ready\'\' an \'\'\'agreement\'\'\' message to the last (single) neighboring node from which \'\'\'agreement\'\'\' message has not yet been received. The coordinator is determined dynamically by racing \'\'\'agreement\'\'\' messages over the transaction tree, at the place where they collide. They collide either at a transaction tree node, to be the coordinator, or on a tree edge. In the latter case one of the two edge\'s nodes is elected as a coordinator (any node). D2PC is time optimal (among all the instances of a specific transaction tree, and any specific Tree 2PC protocol implementation; all instances have the same tree; each instance has a different node as coordinator): By choosing an optimal coordinator D2PC commits both the coordinator and each cohort in minimum possible time, allowing the earliest possible release of locked resources in each transaction participant (tree node).\n\n==See also==\n*[[Atomic commit]]\n*[[Commit (data management)]]\n*[[Three-phase commit protocol]]\n*[[X/Open XA|XA]]\n*[[Paxos algorithm]]\n*[[Two Generals\' Problem]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://exploredatabase.blogspot.in/2014/07/two-phase-commit-protocol-in-pictures.html Two Phase Commit protocol explained in Pictures] by exploreDatabase\n\n{{DEFAULTSORT:Two-Phase Commit Protocol}}\n[[Category:Data management]]\n[[Category:Transaction processing]]']
['Enterprise information system', '1010494', 'An \'\'\'enterprise information system\'\'\' (\'\'\'EIS\'\'\') is any kind of [[information system]] which improves the functions of an enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of [[data]] and capable of supporting some large and possibly complex [[organization]] or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.<ref name="eidvtai" />\n\nThe word \'\'enterprise\'\' can have various connotations. Frequently the term is used only to refer to very large organizations such as multi-national companies or public-sector organizations. However, the term may be used to mean virtually anything, by virtue of it having become the latest corporate-speak [[buzzword]].{{Citation needed|date=June 2016}}\n\n==Purpose==\nEnterprise information systems provide a technology platform that enables organizations to [[Enterprise integration|integrate]] and coordinate their [[business processes]] on a robust foundation. An EIS is currently used in conjunction with [[customer relationship management]] and [[supply chain management]] to automate business processes.<ref name="eidvtai" />  An enterprise information system provides a single system that is central to the organization that ensures information can be shared across all functional levels and management [[hierarchies]].\n\nAn EIS can be used to increase business [[productivity]] and reduce service cycles, [[product development]] cycles and marketing life cycles.<ref name="eidvtai">{{cite book |title=Enterprise Information Systems: Contemporary Trends and Issues |last=Olson |first=David L. |author2=Subodh Kesharwani |year=2010 |publisher=World Scientific |isbn=9814273163 |pages=2, 13–16 |url=https://books.google.com/books?id=-AwDAp7Fe2UC |accessdate=20 August 2013}}</ref>  It may be used to amalgamate existing applications. Other outcomes include higher [[operational efficiency]] and cost savings.<ref name="eidvtai" />\n\nFinancial value is not usually a direct outcome from the implementation of an enterprise information system.<ref name="eiscmta">{{cite book |title=Enterprise Information Systems: Concepts, Methodologies, Tools and Applications |author=Information Resources Management Association |year=2010 |publisher=Idea Group Inc |isbn=1616928530 |pages=38, 43 |url=https://books.google.com/books?id=hpc6-SfS2scC |accessdate=20 August 2013}}</ref>\n\n==Design stage==\nAt the design stage the main characteristic of EIS efficiency evaluation is the probability of timely delivery of various messages such as command, service, etc.<ref>{{cite journal |title=ОЦЕНКА ХАРАКТЕРИСТИК ФУНКЦИОНИРОВАНИЯ КОРПОРАТИВНЫХ ИНФОРМАЦИОННЫХ СИСТЕМ С НЕОДНОРОДНОЙ НАГРУЗКОЙ |trans-title=Efficiency Evaluation of Enterprise Information Systems with Non-uniform Load |language=ru |url=http://ntv.ifmo.ru/en/article/13881/ocenka_harakteristik_funkcionirovaniya_korporativnyhinformacionnyh_sistem_s_neodnorodnoy_nagruzkoy.htm |author1=Kalinin I.V. |author2=Maharevs E. |author3=Muravyeva-Vitkovskaya L.A. |journal=Scientific and Technical Journal of Information Technologies, Mechanics and Optics |volume=15 |issue=5 |pages=863–868 |year=2015 |doi=10.17586/2226-1494-2015-15-5-863-868}}</ref>\n\n==Information systems==\n{{main|Information systems}}\nEnterprise systems create a standard [[data structure]] and are invaluable in eliminating the problem of information fragmentation caused by multiple information systems within an organization.  An EIS differentiates itself from [[legacy system]]s in that it self-transactional, self-helping and adaptable to general and specialist conditions.<ref name="eidvtai" />  Unlike an enterprise information system, legacy systems are limited to department wide communications.<ref name="eiscmta" />\n\nA typical enterprise information system would be housed in one or more [[data center]]s, would run [[enterprise software]], and could include applications that typically cross organizational borders such as [[content management systems]].\n\n==See also==\n{{Portal|Computing|Business}}\n*[[Executive information system]]\n*[[Management information system]]\n*[[Enterprise planning systems]]\n*[[Enterprise software]]\n\n==References==\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:Enterprise architecture]]\n[[Category:Enterprise modelling]]\n[[Category:Website management]]']
['Data integration', '4780372', '\'\'\'Data integration\'\'\' involves combining [[data]] residing in different sources and providing users with a unified view of these data.<ref name="refone">\n{{cite conference | author=Maurizio Lenzerini | title=Data Integration:  A Theoretical Perspective | booktitle=PODS 2002 | year=2002 | pages=233–246 | url=http://www.dis.uniroma1.it/~lenzerin/homepagine/talks/TutorialPODS02.pdf}}</ref> This process becomes significant in a variety of situations, which include both commercial (when two similar companies need to merge their [[database]]s) and scientific (combining research results from different [[bioinformatics]] repositories, for example) domains.  Data integration appears with increasing frequency as the volume and the need to share existing data [[Information explosion|explodes]].<ref name="DataExplode">{{cite news | author=Frederick Lane | title=IDC: World Created 161 Billion Gigs of Data in 2006 | year=2006 | url=http://www.toptechnews.com/story.xhtml?story_id=01300000E3D0&full_skip=1 }}</ref>  It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.\n\n==History==\n[[File:datawarehouse.png|thumb|right|Figure 1: Simple schematic for a data warehouse.  The [[Extract, transform, load|ETL]] process extracts information from the source databases, transforms it and then loads it into the data warehouse.]]\n\n[[File:dataintegration.png|thumb|right|Figure 2: Simple schematic for a data-integration solution.  A system designer constructs a mediated schema against which users can run queries.  The [[virtual database]] interfaces with the source databases via [[Wrapper pattern|wrapper]] code if required.]]\n\nIssues with combining [[heterogeneous]] data sources, often referred to as [[information silo]]s, under a single query interface have existed for some time. In the early 1980s, computer scientists began designing systems for interoperability of heterogeneous databases.<ref>{{cite news | author= John Miles Smith | title= Multibase: integrating heterogeneous distributed database systems | year=1982 | journal=AFIPS \'81 Proceedings of the May 4–7, 1981, national computer conference  | pages= 487–499 |url=http://dl.acm.org/citation.cfm?id=1500483|display-authors=etal}}</ref> The first data integration system driven by structured metadata was designed at the [[University of Minnesota]] in 1991, for the [[IPUMS|Integrated Public Use Microdata Series (IPUMS)]]. IPUMS used a [[data warehousing]] approach, which [[Extract, transform, load|extracts, transforms, and loads]] data from heterogeneous sources into a single view [[logical schema|schema]] so data from different sources become compatible.<ref>{{cite news | author= [[Steven Ruggles]], J. David Hacker, and Matthew Sobek | title= Order out of Chaos: The Integrated Public Use Microdata Series | year=1995 | journal=Historical Methods |volume=28 | pages= 33–39}}</ref> By making thousands of population databases interoperable, IPUMS demonstrated the feasibility of large-scale data integration. The data warehouse approach  offers a [[Coupling (computer science)|tightly coupled]] architecture because the data are already physically reconciled in a single queryable repository, so it usually takes little time to resolve queries.<ref>{{cite news | author= Jennifer Widom | title= Research problems in data warehousing | year=1995 | journal=CIKM \'95 Proceedings of the fourth international conference on information and knowledge management | pages= 25–30 | url=http://dl.acm.org/citation.cfm?id=221319}}</ref>\n\nThe data warehouse approach is less feasible for datasets that are frequently updated, requiring the [[Extract, transform, load|ETL]] process to be continuously re-executed for synchronization.  Difficulties also arise in constructing data warehouses when one has only a query interface to summary data sources and no access to the full data.  This problem frequently emerges when integrating several commercial query services like travel or classified advertisement web applications.\n\n{{As of | 2009}} the trend in data integration favored loosening the coupling between data{{Citation needed|date=June 2009}} and providing a unified query-interface to access real time data over a [[data mediation|mediated]] schema (see figure 2), which allows information to be retrieved directly from original databases. This is consistent with the [[Service-oriented architecture|SOA]] approach popular in that era. This approach relies on mappings between the mediated schema and the schema of original sources, and transform a query into specialized queries to match the schema of the original databases.  Such mappings can be specified in 2 ways : as a mapping from entities in the mediated schema to entities in the original sources (the "[[Global As View]]" (GAV) approach), or as a mapping from entities in the original sources to the mediated schema (the "[[Local As View]]" (LAV) approach).  The latter approach requires more sophisticated inferences to resolve a query on the mediated schema, but makes it easier to add new data sources to a (stable) mediated schema.\n\n{{As of | 2010}} some of the work in data integration research concerns the [[semantic integration]] problem.  This problem addresses not the structuring of the architecture of the integration, but how to resolve [[semantic]] conflicts between heterogeneous data sources.  For example, if two companies merge their databases, certain concepts and definitions in their respective schemas like "earnings" inevitably have different meanings.  In one database it may mean profits in dollars (a floating-point number), while in the other it might represent the number of sales (an integer).  A common strategy for the resolution of such problems involves the use of [[ontology (computer science)|ontologies]] which explicitly define schema terms and thus help to resolve semantic conflicts. This approach represents [[ontology-based data integration]]. On the other hand, the problem of combining research results from different bioinformatics repositories requires bench-marking of the similarities, computed from different data sources, on a single criterion such as positive predictive value. This enables the data sources to be directly comparable and can be integrated even when the natures of experiments are distinct.<ref>{{cite journal| url=http://shubhrasankar.tripod.com/cgi-bin/combiningMultisourceIEEE.pdf  | journal=IEEE Transactions on Biomedical Engineering | title=Combining Multi-Source Information through Functional Annotation based Weighting: Gene Function Prediction in Yeast| author=Shubhra S. Ray| volume = 56 | pages=229–236 | pmid=19272921 | year=2009| issue=2 | doi=10.1109/TBME.2008.2005955|display-authors=etal}}</ref>\n\n{{As of | 2011}} it was determined that current [[data modeling]] methods were imparting data isolation into every [[data architecture]] in the form of islands of disparate data and information silos. This data isolation is an unintended artifact of the data modeling methodology that results in the development of disparate data models. Disparate data models, when instantiated as databases, form disparate databases. Enhanced data model methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated data models.<ref>{{cite news | author= Michael Mireku Kwakye | title= A Practical Approach To Merging Multidimensional Data Models | year=2011 | url=http://hdl.handle.net/10393/20457 }}</ref><ref>{{cite web | url=http://www.iri.com/pdf/RapidAce-Brochure.pdf  | title=Rapid Architectural Consolidation Engine&nbsp;– The enterprise solution for disparate data models. | year=2011 }}</ref> One enhanced data modeling method recasts data models by augmenting them with structural [[metadata]] in the form of standardized data entities.  As a result of recasting multiple data models, the set of recast data models will now share one or more commonality relationships that relate the structural metadata now common to these data models.  Commonality relationships are a peer-to-peer type of entity relationships that relate the standardized data entities of multiple data models.  Multiple data models that contain the same standard data entity may participate in the same commonality relationship.  When integrated data models are instantiated as databases and are properly populated from a common set of master data, then these databases are integrated.\n\nSince 2011, [[data hub]] approaches have been of greater interest than fully structured (typically relational) Enterprise Data Warehouses. Since 2013, [[data lake]] approaches have risen to the level of Data Hubs. (See all three search terms popularity on Google Trends.<ref>{{cite web |title=Hub Lake and Warehouse search trends|url=https://www.google.com/trends/explore#q=enterprise%20data%20warehouse%2C%20%22data%20hub%22%2C%20%22data%20lake%22&cmpt=q&tz=Etc%2FGMT%2B5}}</ref>) These approaches combine unstructured or varied data into one location, but do not necessarily require an (often complex) master relational schema to structure and define all data in the Hub.\n\n==Example==\nConsider a [[web application]] where a user can query a variety of information about cities (such as crime statistics, weather, hotels, demographics, etc.).  Traditionally, the information must be stored in a single database with a single schema.  But any single enterprise would find information of this breadth somewhat difficult and expensive to collect.  Even if the resources exist to gather the data, it would likely duplicate data in existing crime databases, weather websites, and census data.\n\nA data-integration solution may address this problem by considering these external resources as [[materialized view]]s over a [[Virtual database|virtual mediated schema]], resulting in "virtual data integration".  This means application-developers construct a virtual schema—the \'\'mediated schema\'\'—to best model the kinds of answers their users want.  Next, they design "wrappers" or adapters for each data source, such as the crime database and weather website.  These adapters simply transform the local query results (those returned by the respective websites or databases) into an easily processed form for the data integration solution (see figure 2).  When an application-user queries the mediated schema, the data-integration solution transforms this query into appropriate queries over the respective data sources.  Finally, the virtual database combines the results of these queries into the answer to the user\'s query.\n\nThis solution offers the convenience of adding new sources by simply constructing an adapter or an application software blade for them.  It contrasts with [[Extract, transform, load|ETL]] systems or with a single database solution, which require manual integration of entire new dataset into the system. The virtual ETL solutions leverage [[Virtual database|virtual mediated schema]] to implement data harmonization; whereby the data are copied from the designated "master" source to the defined targets, field by field. Advanced [[data virtualization]] is also built on the concept of object-oriented modeling in order to construct virtual mediated schema or virtual metadata repository, using [[hub and spoke]] architecture.\n\nEach data source is disparate and as such is not designed to support reliable joins between data sources.  Therefore, data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets.  Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.\n\nOne solution is to recast disparate databases to integrate these databases without the need for [[Extract, transform, load|ETL]]. The recast databases support commonality constraints where referential integrity may be enforced between databases.  The recast databases provide designed data access paths with data value commonality across databases.\n\n==Theory==\nThe theory of data integration<ref name="refone" /> forms a subset of database theory and formalizes the underlying concepts of the problem in [[first-order logic]].  Applying the theories gives indications as to the feasibility and difficulty of data integration.  While its definitions may appear abstract, they have sufficient generality to accommodate all manner of integration systems,<ref>{{cite web|url=http://link.springer.com/chapter/10.1007/3-540-46093-4_14 |title=A Model Theory for Generic Schema Management}}</ref> including those that include nested relational / XML databases <ref>{{cite web|url=http://www.vldb.org/conf/2006/p67-fuxman.pdf |title=Nested Mappings: Schema Mapping Reloaded }}</ref> and those that treat databases as programs.<ref>{{cite web|url=http://homepages.inf.ed.ac.uk/dts/pub/psi.pdf |title=The Common Framework Initiative for algebraic specification and development of software}}</ref>  Connections to particular databases systems such as Oracle or DB2 are provided by implementation-level technologies such as [[JDBC]] and are not studied at the theoretical level.\n\n===Definitions===\nData integration systems are formally defined as a [[Triple (mathematics)|triple]] <math>\\left \\langle G,S,M\\right \\rangle</math> where <math>G</math> is the global (or mediated) schema, <math>S</math> is the heterogeneous set of source schemas, and <math>M</math> is the mapping that maps queries between the source and the global schemas.  Both <math>G</math> and <math>S</math> are expressed in [[formal language|languages]] over [[alphabet (computer science)|alphabets]] composed of symbols for each of their respective [[Relational database|relations]].  The [[Functional predicate|mapping]] <math>M</math> consists of assertions between queries over <math>G</math> and queries over <math>S</math>.  When users pose queries over the data integration system, they pose queries over <math>G</math> and the mapping then asserts connections between the elements in the global schema and the source schemas.\n\nA database over a schema is defined as a set of sets, one for each relation (in a relational database).  The database corresponding to the source schema <math>S</math> would comprise the set of sets of tuples for each of the heterogeneous data sources and is called the \'\'source database\'\'.  Note that this single source database may actually represent a collection of disconnected databases.  The database corresponding to the virtual mediated schema <math>G</math> is called the \'\'global database\'\'.  The global database must satisfy the mapping <math>M</math> with respect to the source database.  The legality of this mapping depends on the nature of the correspondence between <math>G</math> and <math>S</math>.  Two popular ways to model this correspondence exist: \'\'Global as View\'\' or GAV and \'\'Local as View\'\' or LAV.\n\n[[File:GAVLAV.png|thumb|right|Figure 3: Illustration of tuple space of the GAV and LAV mappings.<ref name="refseven">{{cite journal|author=Christoph Koch |title=Data Integration against Multiple Evolving Autonomous Schemata |year=2001 |url=http://www.csd.uoc.gr/~hy562/Papers/thesis_final.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20070926211342/http://www.csd.uoc.gr/~hy562/Papers/thesis_final.pdf |archivedate=2007-09-26 |df= }}</ref> In GAV, the system is constrained to the set of tuples mapped by the mediators while the set of tuples expressible over the sources may be much larger and richer. In LAV, the system is constrained to the set of tuples in the sources while the set of tuples expressible over the global schema can be much larger. Therefore, LAV systems must often deal with incomplete answers.]]\n\nGAV systems model the global database as a set of [[view (database)|views]] over <math>S</math>.  In this case <math>M</math> associates to each element of <math>G</math> a query over <math>S</math>.  [[Query optimizer|Query processing]] becomes a straightforward operation due to the well-defined associations between <math>G</math> and <math>S</math>.  The burden of complexity falls on implementing mediator code instructing the data integration system exactly how to retrieve elements from the source databases.  If any new sources join the system, considerable effort may be necessary to update the mediator, thus the GAV approach appears preferable when the sources seem unlikely to change.\n\nIn a GAV approach to the example data integration system above, the system designer would first develop mediators for each of the city information sources and then design the global schema around these mediators.  For example, consider if one of the sources served a weather website.  The designer would likely then add a corresponding element for weather to the global schema.  Then the bulk of effort concentrates on writing the proper mediator code that will transform predicates on weather into a query over the weather website.  This effort can become complex if some other source also relates to weather, because the designer may need to write code to properly combine the results from the two sources.\n\nOn the other hand, in LAV, the source database is modeled as a set of [[view (database)|views]] over <math>G</math>.  In this case <math>M</math> associates to each element of <math>S</math> a query over <math>G</math>.  Here the exact associations between <math>G</math> and <math>S</math> are no longer well-defined.  As is illustrated in the next section, the burden of determining how to retrieve elements from the sources is placed on the query processor.  The benefit of an LAV modeling is that new sources can be added with far less work than in a GAV system, thus the LAV approach should be favored in cases where the mediated schema is less stable or likely to change.<ref name="refone" />\n\nIn an LAV approach to the example data integration system above, the system designer designs the global schema first and then simply inputs the schemas of the respective city information sources.  Consider again if one of the sources serves a weather website.  The designer would add corresponding elements for weather to the global schema only if none existed already.  Then programmers write an adapter or wrapper for the website and add a schema description of the website\'s results to the source schemas.  The complexity of adding the new source moves from the designer to the query processor.\n\n===Query processing===\nThe theory of query processing in data integration systems is commonly expressed using conjunctive [[Database query language|queries]] and [[Datalog]], a purely declarative [[logic programming]] language.<ref name="reffive">{{cite conference | author=[[Jeffrey D. Ullman]] | title=Information Integration Using Logical Views | booktitle=ICDT 1997 | year=1997 | pages=19–40 | url=http://www-db.stanford.edu/pub/papers/integration-using-views.ps}}</ref>  One can loosely think of a [[conjunctive query]] as a logical function applied to the relations of a database such as "<math>f(A,B)</math> where <math>A < B</math>".  If a tuple or set of tuples is substituted into the rule and satisfies it (makes it true), then we consider that tuple as part of the set of answers in the query.  While formal languages like Datalog express these queries concisely and without ambiguity, common [[SQL]] queries count as conjunctive queries as well.\n\nIn terms of data integration, "query containment" represents an important property of conjunctive queries.  A query <math>A</math> contains another query <math>B</math> (denoted <math>A \\supset B</math>) if the results of applying <math>B</math> are a subset of the results of applying <math>A</math> for any database.  The two queries are said to be equivalent if the resulting sets are equal for any database.  This is important because in both GAV and LAV systems, a user poses conjunctive queries over a \'\'virtual\'\' schema represented by a set of [[view (database)|views]], or "materialized" conjunctive queries.  Integration seeks to rewrite the queries represented by the views to make their results equivalent or maximally contained by our user\'s query.  This corresponds to the problem of answering queries using views ([[AQUV]]).<ref name="refsix">{{cite conference | author=[[Alon Y. Halevy]] | title=Answering queries using views: A survey | booktitle=The VLDB Journal | year=2001 | pages=270–294 | url=http://www.cs.uwaterloo.ca/~david/cs740/answering-queries-using-views.pdf}}\n</ref>\n\nIn GAV systems, a system designer writes mediator code to define the query-rewriting.  Each element in the user\'s query corresponds to a substitution rule just as each element in the global schema corresponds to a query over the source.  Query processing simply expands the subgoals of the user\'s query according to the rule specified in the mediator and thus the resulting query is likely to be equivalent.  While the designer does the majority of the work beforehand, some GAV systems such as [http://www-db.stanford.edu/tsimmis/ Tsimmis] involve simplifying the mediator description process.\n\nIn LAV systems, queries undergo a more radical process of rewriting because no mediator exists to align the user\'s query with a simple expansion strategy.  The integration system must execute a search over the space of possible queries in order to find the best rewrite.  The resulting rewrite may not be an equivalent query but maximally contained, and the resulting tuples may be incomplete.  {{As of | 2009}} the MiniCon algorithm<ref name="refsix" /> is the leading query rewriting algorithm for LAV data integration systems.\n\nIn general, the complexity of query rewriting is [[NP-complete]].<ref name="refsix" />  If the space of rewrites is relatively small this does not pose a problem—even for integration systems with hundreds of sources.\n\n==Tools==\n* Alteryx\n* Analytics Canvas\n* [[Capsenta]]\'s Ultrawrap Platform\n*[[Cloud Elements]] API Integration\n* DataWatch\n* [[Denodo|Denodo Platform]]\n* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]<ref name="dca">M. Haghighat, M. Abdel-Mottaleb, &  W. Alhalabi (2016). [http://ieeexplore.ieee.org/document/7470527/ Discriminant Correlation Analysis: Real-Time Feature Level Fusion for Multimodal Biometric Recognition]. IEEE Transactions on Information Forensics and Security, 11(9), 1984-1996.</ref>\n* [[elastic.io]] Integration Platform\n* [http://www.hiperfabric.com HiperFabric]\n* [[Lavastorm Analytics|Lavastorm]]\n* [[Informatica]] Platform\n* Oracle Data Integration Services\n* ParseKit (enigma.io)\n* Paxata\n* [[RapidMiner]] Studio\n* [[Red Hat]] JBoss Data Virtualization. Community project: teiid.\n* [[Microsoft Azure|Azure]] Data Factory (ADF) \n* [[SQL Server Integration Services|SQL Server Integration Services (SSIS)]]\n* [http://www.tmmdata.com TMMData]\n* [http://www.dataladder.com Data Ladder]\n\n==In the life sciences==\nLarge-scale questions in science, such as [[global warming]], [[invasive species]] spread, and [[resource depletion]], are increasingly requiring the collection of disparate data sets for [[meta-analysis]]. This type of data integration is especially challenging for ecological and environmental data because [[metadata standards]] are not agreed upon and there are many different data types produced in these fields. [[National Science Foundation]] initiatives such as [[Datanet]] are intended to make data integration easier for scientists by providing [[cyberinfrastructure]] and setting standards. The five funded [[Datanet]] initiatives are [[DataONE]],<ref>{{cite web|author=William Michener |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by William Michener at the [[University of New Mexico]]; The Data Conservancy,<ref>{{cite web|author=Sayeed Choudhury |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by Sayeed Choudhury of [[Johns Hopkins University]]; SEAD: Sustainable Environment through Actionable Data,<ref>{{cite web|author=[[Margaret Hedstrom]] |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19|display-authors=etal}}</ref> led by [[Margaret Hedstrom]] of the [[University of Michigan]]; the DataNet Federation Consortium,<ref>{{cite web|author=[[Reagan Moore]] |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by Reagan Moore of the [[University of North Carolina]]; and \'\'Terra Populus\'\',<ref>{{cite web|author=[[Steven Ruggles]] |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by [[Steven Ruggles]] of the [[University of Minnesota]].  The [[Research Data Alliance]],<ref>{{cite web|author=[[Bill Nichols]] |url=http://rd-alliance.org/ |title=Research Data Alliance |publisher=rd-alliance.org | accessdate=2014-10-01}}</ref> has more recently explored creating global data integration frameworks. The [[OpenPHACTS]] project, funded through the [[European Union]] [[Innovative Medicines Initiative]], built a drug discovery platform by linking datasets from providers such as [[European Bioinformatics Institute]], [[Royal Society of Chemistry]], [[UniProt]], [[WikiPathways]] and [[DrugBank]].\n\n==See also==\n{{div col||20em}}\n* [[Business semantics management]]\n* [[Core data integration]]\n* [[Customer data integration]]\n* [[Data curation]]\n* [[Data fusion]]\n* [[Data mapping]]\n* [[Data wrangling]]\n* [[Database model]]\n* [[Dataspaces]]\n* [[Edge data integration]]\n* [[Enterprise application integration]]\n* [[Enterprise architecture framework]]\n* [[Enterprise information integration]] (EII)\n* [[Enterprise integration]]\n* [[Geodi]]: Geoscientific Data Integration\n* [[Information integration]]\n* [[Information server]]\n* [[Information silo]]\n* [[Integration Competency Center]]\n* [[Integration Consortium]]\n* [[JXTA]]\n* [[Master data management]]\n* [[Object-relational mapping]]\n* [[Open Text]]\n* [[Schema matching]]\n* [[Three schema approach]]\n* [[UDEF]]\n* [[Web service]]\n{{div col end}}\n\n==External links==\n* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]<ref name="dca"></ref>\n\n==References==\n{{reflist|30em}}\n\n{{data}}\n\n{{DEFAULTSORT:Data Integration}}\n[[Category:Data management]]']
['Disaster recovery', '640655', '{{About|1=[[business continuity planning]]|2=societal disaster recovery|3=emergency management}}\n{{Other uses|DR (disambiguation)}}\n\n{{merge to|business continuity|date=June 2015}}\n\n\'\'\'Disaster recovery\'\'\' (DR) involves a set of policies and procedures to enable the recovery or continuation of vital technology infrastructure and systems following a [[natural disaster|natural]] or [[man-made hazards|human-induced]] [[disaster]]. Disaster recovery focuses on the IT or [[technology systems]] supporting critical business functions,<ref>[http://continuity.georgetown.edu/dr/ \'\'Systems and Operations Continuity: Disaster Recovery.\'\'] Georgetown University. University Information Services. Retrieved 3 August 2012.</ref> as opposed to [[business continuity]], which involves keeping all essential aspects of a business functioning despite significant disruptive events.  Disaster recovery is therefore a subset of business continuity.<ref>[http://www-304.ibm.com/partnerworld/gsd/solutiondetails.do?solution=44832&expand=true&lc=en \'\'Disaster Recovery and Business Continuity, version 2011.\'\'] {{webarchive |url=https://web.archive.org/web/20130111203921/http://www-304.ibm.com/partnerworld/gsd/solutiondetails.do?solution=44832&expand=true&lc=en |date=January 11, 2013 }} IBM. Retrieved 3 August 2012.</ref>\n\n==History==\nDisaster recovery developed in the mid- to late 1970s as computer center managers began to recognize the dependence of their organizations on their computer systems. At that time, most systems were [[Batch processing|batch]]-oriented [[Mainframe computer|mainframe]]s which in many cases could be [[downtime|down]] for a number of days before significant damage would be done to the organization.\n\nAs awareness of the potential business disruption that would follow an IT-related disaster, the disaster recovery industry developed to provide backup computer centers, with Sun Information Systems (which later became Sungard Availability Services) becoming the first major US commercial hot site vendor, established in 1978 in Philadelphia.\n\nDuring the 1980s and 90s, customer awareness and industry both grew rapidly, driven by the advent of open systems and [[Real-time computing|real-time processing]] which increased the dependence of organizations on their IT systems. Regulations mandating business continuity and disaster recovery plans for organizations in various sectors of the economy, imposed by the authorities and by business partners, increased the demand and led to the availability of commercial disaster recovery services, including mobile data centers delivered to a suitable recovery location by truck.\n\nWith the rapid growth of the [[Internet]] through the late 1990s and into the 2000s, organizations of all sizes became further dependent on the continuous [[availability]] of their IT systems, with some organizations setting objectives of 2, 3, 4 or 5 nines (99.999%) availability of critical systems.{{citation needed|date=March 2016}} This increasing dependence on IT systems, as well as increased awareness from large-scale disasters such as tsunami, earthquake, flood, and volcanic eruption, spawned disaster recovery-related products and services, ranging from [[high-availability]] solutions to [[hot-site]] facilities.  Improved networking meant critical IT services could be served remotely, hence on-site recovery became less important.\n\nThe rise of cloud computing since 2010 continues that trend: nowadays, it matters even less where computing services are physically served, just so long as the network itself is sufficiently reliable (a separate issue, and less of a concern since modern networks are highly resilient by design).  \'Recovery as a Service\' (RaaS) is one of the security features or benefits of cloud computing being promoted by the Cloud Security Alliance.<ref>[https://cloudsecurityalliance.org/download/secaas-category-9-bcdr-implementation-guidance/ \'\'SecaaS Category 9 // BCDR Implementation Guidance\'\'] CSA, retrieved 14 July 2014.</ref>\n\n===Classification of disasters===\nDisasters can be classified into two broad categories. The first is natural disasters such as floods, hurricanes, tornadoes or earthquakes. While preventing a natural disaster is impossible, risk management measures such as avoiding disaster-prone situations and good planning can help. The second category is man made disasters, such as hazardous material spills, infrastructure failure, bio-terrorism, and disastrous IT bugs or failed change implementations. In these instances, surveillance, testing and mitigation planning are invaluable.\n\n==Importance of disaster recovery planning==\nRecent research supports the idea that implementing a more holistic pre-disaster planning approach is more cost-effective in the long run. Every $1 spent on hazard mitigation(such as a [[disaster recovery plan]]) saves society $4 in response and recovery costs.<ref>{{cite web|first=Partnership for Disaster Resilience|title=Post-Disaster Recovery Planning Forum: How-To Guide|url=http://nthmp.tsunami.gov/Minutes/oct-nov07/post-disaster_recovery_planning_forum_uo-csc-2.pdf|publisher=University of Oregon\'s Community Service Center|accessdate=2013-05-23}}</ref>\n\nAs [[Information technology|IT systems]] have become increasingly critical to the smooth operation of a company, and arguably the economy as a whole, the importance of ensuring the continued operation of those systems, and their rapid recovery, has increased. For example, of companies that had a major loss of business data, 43% never reopen and 29% close within two years. As a result, preparation for continuation or recovery of systems needs to be taken very seriously. This involves a significant investment of time and money with the aim of ensuring minimal losses in the event of a disruptive event.<ref>{{cite web|url=http://www.ready.gov/business/implementation/IT|title=IT Disaster Recovery Plan|date=25 October 2012|publisher=FEMA|accessdate=11 May 2013}}</ref>\n\n==Control measures==\nControl measures are steps or mechanisms that can reduce or eliminate various threats for organizations. Different types of measures can be included in disaster recovery plan (DRP).\n\nDisaster recovery planning is a subset of a larger process known as business continuity planning and includes planning for resumption of applications, data, hardware, electronic communications (such as networking) and other IT infrastructure. A business continuity plan (BCP) includes planning for non-IT related aspects such as key personnel, facilities, crisis communication and reputation protection, and should refer to the disaster recovery plan (DRP) for IT related infrastructure recovery / continuity.\n\nIT disaster recovery control measures can be classified into the following three types:\n# Preventive measures - Controls aimed at preventing an event from occurring.\n# Detective measures - Controls aimed at detecting or discovering unwanted events.\n# Corrective measures - Controls aimed at correcting or restoring the system after a disaster or an event.\n\nGood disaster recovery plan measures dictate that these three types of controls be documented and exercised regularly using so-called "DR tests".\n\n==Strategies==\nPrior to selecting a disaster recovery strategy, a disaster recovery planner first refers to their organization\'s business continuity plan which should indicate the key metrics of [[recovery point objective]] (RPO) and [[recovery time objective]] (RTO) for various business processes (such as the process to run payroll, generate an order, etc.). The metrics specified for the business processes are then mapped to the underlying IT systems and infrastructure that support those processes.<ref>Gregory, Peter. CISA Certified Information Systems Auditor All-in-One Exam Guide, 2009. ISBN 978-0-07-148755-9. Page 480.</ref>\n\nIncomplete RTOs and RPOs can quickly derail a disaster recovery plan. Every item in the DR plan requires a defined recovery point and time objective, as failure to create them may lead to significant problems that can extend the disaster’s impact.<ref>{{cite web|url=http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx |title=Five Mistakes That Can Kill a Disaster Recovery Plan |publisher=Dell.com |accessdate=2012-06-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20130116112225/http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx |archivedate=2013-01-16 |df= }}</ref> Once the RTO and RPO metrics have been mapped to IT infrastructure, the DR planner can determine the most suitable recovery strategy for each system.  The organization ultimately sets the IT budget and therefore the RTO and RPO metrics need to fit with the available budget. While most business unit heads would like zero data loss and zero time loss, the cost associated with that level of protection may make the desired high availability solutions impractical. A [[cost-benefit analysis]] often dictates which disaster recovery measures are implemented.\n\nTraditionally, a disaster recovery system involved cutover or switch-over recovery systems.{{citation needed|date=April 2016}} Such measures would allow an organization to preserve its technology and information, by having a remote disaster recovery location that produced backups on a regular basis. However, this strategy proved to be expensive and time-consuming. Therefore, more affordable and effective cloud-based systems were introduced.\n\nSome of the most common strategies for [[Data recovery|data protection]] include: \n* backups made to tape and sent off-site at regular intervals\n* backups made to disk on-site and automatically copied to off-site disk, or made directly to off-site disk\n* replication of data to an off-site location, which overcomes the need to restore the data (only the systems then need to be restored or synchronized), often making use of [[storage area network]] (SAN) technology\n* Private Cloud solutions which replicate the management data (VMs, Templates and disks) into the storage domains which are part of the private cloud setup. These management data are configured as an xml representation called OVF (Open Virtualization Format), and can be restored once a disaster occurs.\n* Hybrid Cloud solutions that replicate both on-site and to off-site data centers.  These solutions provide the ability to instantly fail-over to local on-site hardware, but in the event of a physical disaster, servers can be brought up in the cloud data centers as well.\n* the use of high availability systems which keep both the data and system replicated off-site, enabling continuous access to systems and data, even after a disaster (often associated with [[cloud storage]])<ref>{{cite web|url=http://www.inc.com/guides/201106/how-to-use-the-cloud-as-a-disaster-recovery-strategy.html|title=How to Use the Cloud as a Disaster Recovery Strategy|last=Brandon|first=John|date=23 June 2011|publisher=Inc. |accessdate=11 May 2013}}</ref>\n\nIn many cases, an organization may elect to use an outsourced disaster recovery provider to provide a stand-by site and systems rather than using their own remote facilities, increasingly via [[cloud computing]].\n\nIn addition to preparing for the need to recover systems, organizations also implement precautionary measures with the objective of preventing a disaster in the first place. These may include: \n* local mirrors of systems and/or data and use of disk protection technology such as [[RAID]]\n* surge protectors — to minimize the effect of power surges on delicate electronic equipment\n* use of an [[uninterruptible power supply]] (UPS) and/or backup generator to keep systems going in the event of a power failure\n* fire prevention/mitigation systems such as alarms and fire extinguishers\n* anti-virus software and other security measures\n\n==See also==\n* [[Backup site]]\n* [[High availability]]\n* [[Continuous data protection]]\n* [[Data recovery]]\n* [[Emergency management]]\n* [[IT service continuity]]\n* [[Remote backup service]]\n* [[Seven tiers of disaster recovery]]\n* [[Virtual tape library]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* ISO/IEC 22301:2012 (replacement of BS-25999:2007) Societal Security - Business Continuity Management Systems - Requirements\n* ISO/IEC 27001:2013 (replacement of ISO/IEC 27001:2005 [formerly BS 7799-2:2002]) Information Security Management System\n* ISO/IEC 27002:2013 (replacement of ISO/IEC 27002:2005 [renumbered ISO17799:2005]) Information Security Management - Code of Practice\n* ISO/IEC 22399:2007 Guideline for incident preparedness and operational continuity management\n* ISO/IEC 24762:2008 Guidelines for information and communications technology disaster recovery services\n* IWA 5:2006 Emergency Preparedness—British Standards Institution --\n* BS 25999-1:2006 Business Continuity Management Part 1: Code of practice\n* BS 25999-2:2007 Business Continuity Management Part 2: Specification\n* BS 25777:2008 Information and communications technology continuity management - Code of practice—Others --\n* "A Guide to Business Continuity Planning" by James C. Barnes\n* "Business Continuity Planning", A Step-by-Step Guide with Planning Forms on CDROM by Kenneth L Fulmer\n* "Disaster Survival Planning: A Practical Guide for Businesses" by Judy Bell\n* ICE Data Management (In Case of Emergency) made simple - by MyriadOptima.com\n* Harney, J.(2004). Business continuity and disaster recovery: Back up or shut down.\n* AIIM E-Doc Magazine, 18(4), 42-48.\n* Dimattia, S. (November 15, 2001).Planning for Continuity. Library Journal,32-34.\n\n==External links==\n* [https://www.ready.gov/business/implementation/IT IT Disaster Recovery Plan from Ready.gov]\n<!--========================({{No More Links}})============================\n | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA |\n | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |\n | |\n | Excessive or inappropriate links WILL BE DELETED. |\n | See [[Wikipedia:External links]] & [[Wikipedia:Spam]] for details. |\n | |\n | If there are already plentiful links, please propose additions or |\n | replacements on this article\'s discussion page, or submit your link |\n | to the relevant category at the Open Directory Project (dmoz.org) |\n | and link back to that category using the {{dmoz}} template. |\n =======================({{No More Links}})=============================-->\n\n{{Disasters}}\n\n{{DEFAULTSORT:Disaster Recovery}}\n[[Category:Disaster recovery]]\n[[Category:Data management]]\n[[Category:Backup]]\n[[Category:IT risk management]]']
['Database transaction', '233953', '{{Refimprove|date=August 2010}}\n\nA \'\'\'transaction\'\'\' symbolizes a unit of work performed within a [[database management system]] (or similar system) against a database, and treated in a coherent and reliable way independent of other transactions. A transaction generally represents any change in database. Transactions in a database environment have two main purposes:\n\n# To provide reliable units of work that allow correct recovery from failures and keep a database consistent even in cases of system failure, when execution stops (completely or partially) and many operations upon a database remain uncompleted, with unclear status.\n# To provide isolation between programs accessing a database concurrently. If this isolation is not provided, the programs\' outcomes are possibly erroneous.\n\nA database transaction, by definition, must be [[Atomicity (database systems)|atomic]], [[Consistency (database systems)|consistent]], [[Isolation (database systems)|isolated]] and [[Durability (database systems)|durable]].<ref>[http://msdn.microsoft.com/en-us/library/aa366402(VS.85).aspx A transaction is a group of operations that are atomic, consistent, isolated, and durable (ACID).]</ref> Database practitioners often refer to these properties of database transactions using the acronym [[ACID]].\n\nTransactions provide an "all-or-nothing" proposition, stating that each work-unit performed in a database must either complete in its entirety or have no effect whatsoever. Further, the system must isolate each transaction from other transactions, results must conform to existing constraints in the database, and transactions that complete successfully must get written to durable storage.\n\n==Purpose==\n[[Database]]s and other data stores which treat the [[data integrity|integrity]] of data as paramount often include the ability to handle transactions to maintain the integrity of data. A single transaction consists of one or more independent units of work, each reading and/or writing information to a database or other data store. When this happens it is often important to ensure that all such processing leaves the database or data store in a consistent state.\n\nExamples from [[Double-entry bookkeeping system|double-entry accounting systems]] often illustrate the concept of transactions. In double-entry accounting every debit requires the recording of an associated credit. If one writes a check for $100 to buy groceries, a transactional double-entry accounting system must record the following two entries to cover the single transaction:\n\n# Debit $100 to Groceries Expense Account\n# Credit $100 to Checking Account\n\nA transactional system would make both entries pass or both entries would fail. By treating the recording of multiple entries as an atomic transactional unit of work the system maintains the integrity of the data recorded. In other words, nobody ends up with a situation in which a debit is recorded but no associated credit is recorded, or vice versa.\n\n==Transactional databases==\nA \'\'\'transactional database\'\'\' is a [[DBMS]] where write transactions on the database are able to be rolled back if they are not completed properly (e.g. due to power or connectivity loss).\n\nMost {{As of|2008|alt=modern}}  [[relational database management system]]s fall into the category of databases that support transactions.\n\nIn a database system a transaction might consist of one or more data-manipulation statements and queries, each reading and/or writing information in the database. Users of [[database system]]s consider [[Data consistency|consistency]] and [[data integrity|integrity]] of data as highly important. A simple transaction is usually issued to the database system in a language like [[Structured Query Language|SQL]] wrapped in a transaction, using a pattern similar to the following:\n\n# Begin the transaction\n# Execute a set of data manipulations and/or queries\n# If no errors occur then commit the transaction and end it\n# If errors occur then rollback the transaction and end it\n\nIf no errors occurred during the execution of the transaction then the system commits the transaction. A transaction commit operation applies all data manipulations within the scope of the transaction and persists the results to the database. If an error occurs during the transaction, or if the user specifies a [[Rollback (data management)|rollback]] operation, the data manipulations within the transaction are not persisted to the database. In no case can a partial transaction be committed to the database since that would leave the database in an inconsistent state.\n\nInternally, multi-user databases store and process transactions, often by using a transaction [[identifier|ID]] or XID.\n\nThere are multiple varying ways for transactions to be implemented other than the simple way documented above. [[Nested transaction]]s, for example, are transactions which contain statements within them that start new transactions (i.e. sub-transactions). \'\'Multi-level transactions\'\' are a variant of nested transactions where the sub-transactions take place at different levels of a layered system architecture (e.g., with one operation at the database-engine level, one operation at the operating-system level) <ref>Beeri, C., Bernstein, P.A., and Goodman, N. A model for concurrency in nested transactions systems. Journal of the ACM, 36(1):230-269, 1989</ref> Another type of transaction is the [[compensating transaction]].\n\n===In SQL===\nTransactions are available in most SQL database implementations, though with varying levels of robustness. (MySQL, for example, does not support transactions in the [[MyISAM]] storage engine, which was its default storage engine before version 5.5.)\n\nA transaction is typically started using the command <code>BEGIN</code> (although the SQL standard specifies <code>START TRANSACTION</code>). When the system processes a <code>[[Commit (SQL)|COMMIT]]</code> statement, the transaction ends with successful completion.  A <code>[[Rollback (data management)|ROLLBACK]]</code> statement can also end the transaction, undoing any work performed since <code>BEGIN TRANSACTION</code>. If [[autocommit]] was disabled using <code>START TRANSACTION</code>, autocommit will also be re-enabled at the transaction\'s end.\n\nOne can set the [[Isolation (database systems)|isolation level]] for individual transactional operations as well as globally. At the READ COMMITTED level, the result of any work done after a transaction has commenced, but before it has ended, will remain invisible to other database-users until it has ended. At the lowest level (READ UNCOMMITTED), which may occasionally be used to ensure high concurrency, such changes will be visible.\n\n==Object databases==\nRelational databases traditionally comprise tables with fixed size fields and thus records. Object databases comprise variable sized [[Binary large object|blobs]] (possibly incorporating a [[mime-type]] or [[Serializable (databases)|serialized]]). The fundamental similarity though is the start and the [[Commit (data management)|commit]] or [[Rollback (data management)|rollback]].\n\nAfter starting a transaction, database records or objects are locked, either read-only or read-write. Actual reads and writes can then occur. Once the user (and application) is happy, any changes are committed or rolled-back [[Atomicity (database systems)|atomically]], such that at the end of the transaction there is no [[Consistency (database systems)|inconsistency]].\n\n==Distributed transactions==\nDatabase systems implement [[distributed transaction]]s as transactions against multiple applications or hosts. A distributed transaction enforces the ACID properties over multiple systems or data stores, and might include systems such as databases, file systems, messaging systems, and other applications. In a distributed transaction a coordinating service ensures that all parts of the transaction are applied to all relevant systems. As with database and other transactions, if any part of the transaction fails, the entire transaction is rolled back across all affected systems.\n\n==Transactional filesystems==\nThe [[Namesys]] [[Reiser4]] filesystem for [[Linux]]<ref>[http://namesys.com/v4/v4.html#committing namesys.com<!-- Bot generated title -->]</ref> supports transactions, and as of [[Microsoft]] [[Windows Vista]], the Microsoft [[NTFS]] filesystem<ref>{{cite web|url=http://msdn.microsoft.com/library/default.asp?url=/library/en-us/fileio/fs/portal.asp|title=MSDN Library|publisher=|accessdate=16 October 2014}} {{dead link|date=May 2014}}</ref> supports [[distributed transaction]]s across networks.\n\n==See also==\n* [[Concurrency control]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* <cite id=Bern2009>[[Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 \'\'Principles of Transaction Processing\'\', 2nd Edition],  Morgan Kaufmann (Elsevier), ISBN 978-1-55860-623-4 </cite>\n* Gerhard Weikum, Gottfried Vossen (2001), \'\'Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery\'\', Morgan Kaufmann, ISBN 1-55860-508-8\n\n==External links==\n* [[c2:TransactionProcessing]]\n* https://docs.oracle.com/database/121/CNCPT/transact.htm#CNCPT016\n* https://docs.oracle.com/cd/B28359_01/server.111/b28318/transact.htm\n\n{{Databases}}\n\n{{DEFAULTSORT:Database Transaction}}\n[[Category:Data management]]\n[[Category:Transaction processing]]']
['Enterprise Data Planning', '25209669', '{{Orphan|date=October 2010}}\n{| style="width: 80%; margin: 0 0 0 10%; border-collapse: collapse; background: #FBFBFB; border: 1px solid #aaa; border-left: 10px solid #f28500;"\n|-\n| style="width: 52px; padding: 2px 0px 2px 0.5em; text-align: center;" | [[Image:Newspaper nicu buculei 01.svg|50px]]\n| style="padding: 0.25em 0.5em;" | \'\'\'This article or section reads like an [[Wikipedia:What Wikipedia is not#Wikipedia is not a soapbox|advertisement]] for EDMworks.\'\'\'<br/>To meet Wikipedia\'s [[:Category:Wikipedia style guidelines|quality standards]] and comply with Wikipedia\'s [[Wikipedia:Neutral point of view|neutral point of view]] policy, it may require [[Wikipedia:Cleanup|cleanup]].\n|}\n\'\'\'Enterprise Data Planning\'\'\' is the starting point for enterprise wide change. It states the destination and describes how you will get there. It defines benefits, costs and potential risks.  It provides measures to be used along the way to judge progress and adjust the journey according to changing circumstances.\n\n[[Data]] is fundamental to investment enterprises. Effective, economic management of data underpins operations and enables transformations needed to satisfy customer demands, competition and regulation. Data warehouse(s) and other aspects of the overall [[data architecture]] are critical to the enterprise.\n\nEDMworks has created a strategic data planning approach for the Investment Sector.  It consists of a planning process, planning intranets, templates and training materials.\n\nEDMworks planning process is based on the belief that extensive domain knowledge significantly shortens planning iterations and enables progressively higher quality plans to be produced and implemented.<ref name=hull>Introduction to Futures and Options Markets (John Hull) 1995</ref><ref name=taylor>Mastering Derivatives Markets (Francesca Taylor) 2007</ref>  This approach drives the development of an effective and economic Enterprise Data Architecture.\n\nEnterprise Data Planning is based on proven business disciplines.<ref name=stutely>The Definitive Business Plan (Richard Stutely) 2002</ref> Key architectural layers for data and applications are then added in order to provide an enterprise wide understanding of the uses and interdependencies of data.<ref name=tozer>Planning for Effective Business Information Systems ([[Edwin E. Tozer]]) 1998</ref> This enables the definition of the core components of the [[Enterprise data management|EDM]] plan:\n\n* Industry structure and business objectives\n* Assessment of systems and services\n* Target architecture for applications, data and infrastructure\n* Target organization structures\n* Systems, database, infrastructure and organizational plans\n* Business case, costs, benefits, results and risks.\n\nEDMworks uses several components from the Open Systems Group [[TOGAF]] enterprise systems planning process. [[TOGAF]] acts as an extension to good business planning methods to provide a framework for the development of the systems and data architectural components.\n\n==History==\n\n[[James Martin (author)|James Martin]] was one of the pathfinders in data planning methodologies.  He was one of the first to identify data as being an enterprise wide asset that required management.  He developed a series of tools and methods to support that process.<ref name=Martin>Martin 1982</ref>\n \nMost of the large consulting firms developed their own methods to address the same basic issue.  Frequently, their approaches were incorporated into their own branded system development methodologies that encompassed the complete systems development life-cycle. \n \nOthers, such as [[Edwin E. Tozer|Ed Tozer]], developed more focused offerings that dealt with the complexities of extracting key business needs from senior management and then defining relevant architectural visions for the specific enterprise.<ref name=tozer/>\n \nFrom these various sources, the concepts of Business, Data, Applications and Technology Architectures emerged. \n \nThe Open Group Architectural Framework (TOGAF) has taken this work forward and has established a sound method in TOGAF version 9.\n \nEDMworks approach is to adopt these planning and architectural practices as a basis and then add two additional dimensions to the planning and implementation focus:\n* Domain knowledge of the Investments sector.  Investments is a complex global industry with a common set of characteristics about clients, information vendors, competition and regulation.  Domain knowledge significantly improves the quality of the planning and implementation processes\n* Development of people and teams.  Change is a major feature of in any Enterprise Data Management program and people and teams both need development in order to make EDM effective throughout an organization.\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [http://www.edmworks.com Enterprise Data Management works]\n\n[[Category:Data management]]']
['Network transparency', '1786529', '{{Multiple issues|\n{{Unreferenced|date=December 2009}}\n{{Lead rewrite|date=July 2015}}\n}}\n\n\'\'\'Network transparency\'\'\' in its most general sense refers to the ability of a protocol to transmit data over the [[computer network|network]] in a manner which is [[Transparency (human–computer interaction)|transparent]] (invisible) to those using the applications that are using the protocol.\n\n==X Window==\nThe term is often partially correctly applied in the context of the [[X Window System]], which is able to transmit graphical data over the network and integrate it seamlessly with applications running and displaying locally; however, certain extensions of the X Window System are not capable of working over the network.<ref>{{cite web |url=https://lwn.net/Articles/553415/ |title=The Wayland Situation: Facts About X vs. Wayland (Phoronix) |publisher=[[LWN.net]] |date=23 June 2013}}</ref>\n\n==Databases==\nIn a [[centralized database system]], the only available resource that needs to be shielded from the user is the data (that is, the [[storage system]]). In a [[Distributed Database Management System|distributed DBMS]], a second resource needs to be managed in much the same manner: the [[computer network|network]]. Preferably, the user should be protected from the network operational details. Then there would be no difference between database applications that would run on the centralized database and those that would run on a distributed one. This kind of transparency is referred to as \'\'\'network transparency\'\'\' or \'\'\'distribution transparency\'\'\'. From a [[database management system]] (DBMS) perspective, distribution transparency requires that users do not have to specify where data is located.\n\nSome have separated distribution transparency into location transparency and naming transparency.\n\nLocation transparency in commands used to perform a task is independent both of the locations of the data, and of the system on which an operation is carried out.\n\nNaming transparency means that a unique name is provided for each object in the database.\n\n==Firewalls==\n{{See also|Proxy server#Transparent proxy}}\n\nTransparency in firewall technology can be defined at the networking (IP or [[Internet Layer|Internet layer]]) or at the [[Internet Layer|application layer]].\n\nTransparency at the IP layer means the client targets the real IP address of the server. If a connection is non-transparent, then the client targets an intermediate host (address), which could be a proxy or a caching server. IP layer transparency could be also defined from the point of server\'s view. If the connection is transparent, the server sees the real client IP. If it is non-transparent, the server sees the IP of the intermediate host.\n\nTransparency at the application layer means the client application uses the protocol in a different way. An example of a transparent HTTP request for a server:\n\n<syntaxhighlight lang="text">\nGET / HTTP/1.1\nHost: example.org\nConnection: Keep-Alive\n</syntaxhighlight>\n\nAn example non-transparent HTTP request for a proxy (cache):\n\n<syntaxhighlight lang="text">\nGET http://foo.bar/ HTTP/1.1\nProxy-Connection: Keep-Alive\n</syntaxhighlight>\n\nApplication layer transparency is symmetric when the same working mode is used on both the sides. The transparency is asymmetric when the firewall (usually a proxy) converts server type requests to proxy type or vice versa. \n\nTransparency at the IP layer does not mean automatically application layer transparency.\n\n== See also ==\n{{Portal|Computer networking}}\n\n* [[Data independence]]\n* [[Replication transparency]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Network Transparency}}\n[[Category:Telecommunications]]\n[[Category:Data management]]']
['Nonlinear medium', '676568', "{{Distinguish|Non-linear media}}\n{{Unreferenced|date=December 2009}}\nA '''nonlinear medium''' is one which is intended to be accessed in a nonlinear fashion. It is the opposite of a [[Linear_medium|Linear Medium]]. \n\nExamples include:\n* a [[hard drive]]\n* a [[newspaper]]\n* a [[phone book]]\n* a [[dictionary]]\n\n==See also==\n*[[nonlinear]]\n*[[linear medium]]\n*[[Random access]]\n\n{{DEFAULTSORT:Nonlinear Medium}}\n[[Category:Data management]]\n\n\n{{Tech-stub}}"]
['Data virtualization', '26041421', '\'\'\'Data virtualization\'\'\' is any approach to data management that allows an application to retrieve and manipulate data without requiring technical details about the data, such as how it is formatted at source, or where it is physically located.<ref>[http://searchdatamanagement.techtarget.com/definition/data-virtualization "What is Data Virtualization?"], Margaret Rouse, TechTarget.com, retrieved 19 August 2013</ref>\n\nUnlike the traditional [[extract, transform, load]] ("ETL") process, the data remains in place, and real-time access is given to the source system for the data, thus reducing the risk of data errors and reducing the workload of moving data around that may never be used.\n\nUnlike a [[federated database system]], it does not attempt to impose a single data model on the data (heterogeneous data). The technology also supports the writing of transaction data updates back to the source systems.<ref name="morgan">[http://www.computerweekly.com/feature/Data-virtualisation-on-rise-as-ETL-alternative-for-data-integration "Data virtualisation on rise as ETL alternative for data integration"] Gareth Morgan, Computer Weekly, retrieved 19 August 2013</ref>\n\nTo resolve differences in source and consumer formats and semantics, various abstraction and transformation techniques are used.\nThis concept and software is a subset of [[data integration]] and is commonly used within [[business intelligence]], [[service-oriented architecture]] data services, [[cloud computing]], [[enterprise search]], and [[master data management]].\n\n==Examples ==\n* The Phone House—the trading name for the European operations of UK-based mobile phone retail chain [[Carphone Warehouse]]—implemented [[Denodo]]’s data virtualization technology between its Spanish subsidiary’s transactional systems and the Web-based systems of mobile operators.<ref name="morgan"/>\n* [[Novartis]], which implemented a data virtualization tool from [[Composite Software]] to enable its researchers to quickly combine data from both internal and external sources into a searchable virtual data store.<ref name="morgan"/>\n* The storage-agnostic [http://primarydata.com/ Primary Data] data virtualization platform enables applications, servers, and clients to transparently access data while it is intelligently migrated between direct-attached, network-attached, private and public cloud storage. Server flash memory pioneer [[Fusion-io]] co-founder David Flynn, now Primary Data CTO, saw the need to move data across storage types to maximize efficiency with data virtualization.\n* [[Linked Data]] can use a single hyperlink-based [[Data Source Name]] ([[Data Source Name|DSN]]) to provide a connection to a virtual database layer that is internally connected to a variety of back-end data sources using [[ODBC]], [[JDBC]], [[OLE DB]], [[ADO.NET]], [[Service-oriented architecture|SOA]]-style services, and/or [[REST]] patterns.\n* [[Database virtualization]] may use a single ODBC-based DSN to provide a connection to a similar virtual database layer.\n\n==Functionality==\n\nData Virtualization software provides some or all of the following capabilities:\n\n* \'\'\'Abstraction\'\'\' –  Abstract the technical aspects of stored data, such as location, storage structure, API, access language, and storage technology. \n* \'\'\'Virtualized Data Access\'\'\' – Connect to different data sources and make them accessible from a common logical data access point.\n* \'\'\'Transformation\'\'\' – Transform, improve quality, reformat, etc. source data for consumer use. \n* \'\'\'Data Federation\'\'\' – Combine result sets from across multiple source systems. \n* \'\'\'Data Delivery\'\'\' – Publish result sets as views and/or data services executed by client application or users when requested.\n\nData virtualization software may include functions for development, operation, and/or management.\n\nBenefits include:\n* Reduce risk of data errors\n* Reduce systems workload through not moving data around\n* Increase speed of access to data on a real-time basis\n* Significantly reduce development and support time\n* Increase governance and reduce risk through the use of policies<ref>[http://www.informatica.com/us/products/data-virtualization/data-services/ "Rapid Access to Disparate Data Across Projects Without Rework"] Informatica, retrieved 19 August 2013</ref>\n* Reduce data storage required<ref>[http://www.zdnet.com/blog/service-oriented/data-virtualization-6-best-practices-to-help-the-business-get-it/7897 Data virtualization: 6 best practices to help the business \'get it\'] Joe McKendrick, ZDNet, 27 October 2011</ref>\n\nDrawbacks include:\n* May impact Operational systems response time, particularly if under-scaled to cope with unanticipated user queries or not tuned early on<ref>[http://searchdatamanagement.techtarget.com/news/2240165242/IT-pros-reveal-the-benefits-drawbacks-of-data-virtualization-software|IT pros reveal benefits, drawbacks of data virtualization software"] Mark Brunelli, SearchDataManagement, 11 October 2012</ref>\n* Does not impose a heterogeneous data model, meaning the user has to interpret the data, unless combined with [[Federated database system|Data Federation]] and business understanding of the data<ref name="lawson">[http://www.itbusinessedge.com/cm/blogs/lawson/the-pros-and-cons-of-data-virtualization/?cs=48794 "The Pros and Cons of Data Virtualization"] Loraine Lawson, BusinessEdge, 7 October 2011</ref>\n* Requires a defined Governance approach to avoid budgeting issues with the shared services\n* Not suitable for recording the historic snapshots of data - data warehouse is better for this<ref name="lawson"/>\n* Change management "is a huge overhead, as any changes need to be accepted by all applications and users sharing the same virtualization kit"<ref name="lawson"/>\n\n== Technology ==\n\nSome data virtualization technologies include:\n\n*  [[Actifio]] Copy Data Virtualization\n*  [[Capsenta]]\'s Ultrawrap Platform <ref>https://capsenta.com/</ref>\n*  [[Cisco]] Data Virtualization (formerly [[Composite Software]])\n*  [[Denodo|Denodo Platform]]\n* DataVirtuality\n* Data Virtualization Platform\n*  HiperFabric Data Virtualization and Integration\n* Stonebond Enterprise Enabler Data Virtualization Platform\n* [[Red Hat]] [[JBoss Enterprise Application Platform]] Data Virtualization\n* [[XAware]] Data Services\n\n==History==\n[[Enterprise information integration]] (EII), first coined by Metamatrix, now known as Red Hat JBoss Data Virtualization, and [[federated database system]]s are terms used by some vendors to describe a core element of data virtualization: the capability to create relational JOINs in a federated VIEW.\n\n==See also==\n\n* [[Data integration]]\n* [[Enterprise information integration]] (EII)\n* [[Master data management]]\n* [[Database virtualization]]\n* [[Federated database system|Data Federation]]\n* [[Disparate system]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* \'\'\'Data Virtualization: Going Beyond Traditional Data Integration to Achieve Business Agility\'\'\', Judith R. Davis and Robert Eve\n* \'\'\'Data Virtualization for Business Intelligence Systems: Revolutionizing Data Integration for Data Warehouses\'\'\' Rick van der Lans\n* \'\'\'Data Integration Blueprint and Modeling: Techniques for a Scalable and Sustainable Architecture \'\'\' Anthony Giordano\n\n[[Category:Data management]]']
['BBC Archives', '23998233', '{{for|the Iron Maiden album|BBC Archives (album)}}\n{{distinguish|BBC Motion Gallery}}\n{{EngvarB|date=September 2013}}\n{{Use dmy dates|date=September 2013}}\n[[File:BBC Information and Archives Logo.svg|thumb|300px|BBC Information and Archives logo]]\n\n\'\'\'BBC Information and Archives\'\'\' (sometimes known just as \'\'\'BBC Archives\'\'\') are collections documenting the [[BBC]]\'s broadcasting history, including copies of [[BBC Television|television]] and [[BBC Radio|radio]] broadcasts, internal documents, photographs, [[BBC Online|online]] content, [[sheet music]], commercially available music, press cuttings and historic equipment.<ref name="BBCArchive TV 1"/> The original copies of these collections are permanently retained but are now in the process of being digitised, estimated to take until approximately 2015. Some collections are now being uploaded onto the BBC Archives website on [[BBC Online]] for viewers to see. The archive is one of the largest broadcast archives in the world with over 12 million items.<ref name=Perivale1>{{cite web|last=Hayes|first=Sarah|title=The new BBC Archive Centre in Perivale|url=http://www.bbc.co.uk/blogs/aboutthebbc/2011/10/the-new-bbc-archive-centre-at.shtml|work=About the BBC Blog|publisher=BBC Online|accessdate=17 January 2012}}</ref>\n\n==Overview==\nThe BBC Archives encompass numerous different archives containing different materials produced or acquired by the BBC. The earliest material dates back to 1890 and now consists of 1 million hours of playable material, in addition to documents, photographs and equipment.<ref name="gutechweekly">{{cite news|last=Kiss|first=Jemima|title=In The BBC Archive|url=https://www.theguardian.com/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road|work=Tech Weekly|publisher=Guardian News & Media Ltd|accessdate=21 August 2010 | location=London | date=18 August 2010| archiveurl= https://web.archive.org/web/20100821165828/http://www.guardian.co.uk/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road?| archivedate= 21 August 2010 <!--DASHBot-->| deadurl= no}}</ref> The archives contain 12 million items on 66 miles of shelving spread over several sites.<ref name=gutechweekly /> The stock is managed using a [[bar code]] system, which help to locate material on the shelves and also track material that has been lent out.<ref name=gutechweekly /> The BBC says that the budget for managing, protecting and digitising the archive accounts for only a small part of the BBC\'s overall spend.<ref name=gutechweekly />\n\nThe BBC is engaging in an ongoing project to [[Digital reformatting|digitise]] archived programme material, converting recordings made on older [[Analog recording|analogue formats]] such as audio tape, [[videotape]] and film to electronic formats which are compatible with modern computer systems. Much of the audio-visual material was originally recorded on formats which are now obsolete and incompatible with modern broadcast equipment due to the fact that the machines used to reproduce many formats are no longer being manufactured. Additionally, some film and audio formats are slowly disintegrating, and digitisation also serves as a [[digital preservation]] programme. As of summer 2010 BBC Archive staff have spent approximately ten years digitising half of the media content<ref name=gutechweekly /><ref name="BBC Archive BBCInternetblog"/> and due to improving work practices expect to complete the other half in five years. Current estimates suggest the digitised archive would comprise approximately 52 [[petabyte]]s of information,<ref name=gutechweekly /> with one programme minute of video requiring 1.4 [[gigabyte]]s of storage.<ref name=gutechweekly /> The BBC uses the [[Material Exchange Format]] (MXF)<ref name=gutechweekly /> which is an uncompressed, non-proprietary format which the BBC has been publicising to mitigate the threat of the format becoming obsolete (as digital formats can and do).<ref name=gutechweekly />\n\nThe Archive digitisation a key part of the BBC\'s programme to engineer a fully digital and [[tapeless|tapeless production workflow]] across the entire Corporation. It was closely tied in with the ill-fated [[Digital Media Initiative]] (DMI), a scheme which ran from 2008 to 2013 and attempted to create a unified online archive search and programme production system.<ref name=bbc-dmi>{{cite web|title=Digital Media Initiative|url=http://www.bbc.co.uk/careers/divisions/digital-media-initiative|publisher=BBC|accessdate=15 February 2012|archiveurl=https://web.archive.org/web/20120310041000/http://www.bbc.co.uk/careers/divisions/digital-media-initiative|archivedate=10 March 2012|deadurl=yes}}</ref> After spiralling development costs and project delays, the problems with DMI came to public attention during coverage of the [[death and funeral of Margaret Thatcher]] in April 2013, when it was reported that the lack of digital ingest facilities provided for [[BBC News]] staff meant that tapes had to be sent by taxi from the Perivale centre to be digitised by independent companies in central London.<ref>{{cite news|title=BBC\'s Thatcher coverage highlights problems with non-digital archives|url=https://www.theguardian.com/media/2013/apr/11/bbc-thatcher-coverage|accessdate=3 May 2013|newspaper=The Guardian|date=11 February 2012|location=London|first=Tara|last=Conlan}}</ref> DMI was cancelled in 2013.<ref name=bbc_abandons>{{cite news|title=BBC abandons £100m digital project|url=http://www.bbc.co.uk/news/entertainment-arts-22651126|accessdate=25 May 2013|newspaper=BBC News|date=24 May 2013}}</ref>\n\nThe BBC Archive website was relaunched online in 2008 and has provided newly released historical material regularly since then.<ref>{{cite web|last=Sangster|first=Jim|title=A new homepage for BBC Archive|url=http://www.bbc.co.uk/blogs/bbcinternet/2010/05/a_new_homepage_for_bbc_archive.html|work=BBC Internet Blog|publisher=BBC|accessdate=19 January 2012}}</ref> The BBC works in partnership with the [[British Film Institute]] (BFI), [[The National Archives]] and other partners in working with and using the materials.<ref name=gutechweekly /> A related project called "Genome" is expected to complete in 2011 and will make programme listings dating back to 1923, sourced from \'\'[[The Radio Times]]\'\', available to search online.<ref name=gutechweekly />\n\nIn July 2008, [[Roly Keating]] was appointed Director of Archive Content,<ref>{{cite web|url=http://www.bbc.co.uk/pressoffice/pressreleases/stories/2008/07_july/22/archive.shtml |title=Roly Keating appointed as Director of Archive Content |publisher=BBC Press Office|date=22 July 2008 |accessdate=1 July 2011}}</ref> with responsibility for increasing public access to the BBC’s archives. In October 2008, Keating appointed [[Tony Ageh]] Controller of Archive Development with "specific responsibility for developing ways of making the archive easily understandable and accessible to users".<ref>{{cite web|url=http://www.bbc.co.uk/pressoffice/pressreleases/stories/2008/10_october/10/ageh.shtml|publisher=BBC Press Office|date=10 October 2008|accessdate=1 July 2011|title=Tony Ageh appointed Controller of Archive Development}}</ref>\n\nIn 2012, BBC Archive Development produced a book - primarily aimed as BBC staff - titled \'BBC Archive Collections: What\'s In The Archive And How To Use Them\'.<ref>\'BBC Archive Collections: What\'s In The Archives, And How To Use Them\' Edited by Jake Berger https://www.dropbox.com/s/rz1o57nzlsf1v04/BBC%20Archive%20Collections%20Guide%202012.pdf?dl=0</ref>  This book describes the BBC\'s archive collections and offers guidance around on how items from the collections can be reused online.  The book\'s references to \'Fabric\', a system due to be delivered by the [[Digital Media Initiative]] are no longer accurate as the project was cancelled.\n\n==Buildings==\nFrom 1968 to 2010 the BBC Archive was housed at the Archive centre in Windmill Road, [[Brentford]], in [[W postcode area|west London]].<ref name=Perivale1/><ref name="BBC Archive BBCInternetblog"/><ref name="Perivale 2 AtBBCblog"/> The condition of the building deteriorated over the years and suffered occasional flooding incidents, and eventually the Archive was relocated to a new centre at Perivale Park, [[Perivale]], three miles north of the old site.<ref name="Perivale 2 AtBBCblog"/><ref name="Perivale Centre R4 Blog"/> The new BBC Archive Centre was opened in Summer 2010 and all material was successfully moved by March 2011.<ref name="Perivale 2 AtBBCblog"/><ref name="Perivale 3 S&Pblog"/> The cost of the refurbishment and of the move was approximately £16.6 million.<ref name=Perivale1/><ref name="Perivale 2 AtBBCblog">{{cite news|last=Skinner|first=Peter|title=A new home for the BBC Archive|url=http://www.bbc.co.uk/blogs/aboutthebbc/2010/08/a-warm-balmy-afternoon-in.shtml|accessdate=19 January 2012|newspaper=BBC About the BBC Blog|date=20 August 2010}}</ref><ref name="Perivale Centre R4 Blog">{{cite news|last=Bolton|first=Roger|title=Tears in Perivale – Feedback in the archives|url=http://www.bbc.co.uk/blogs/radio4/2011/09/tears_in_perivale_feedback_in_the_archives.html|accessdate=19 January 2012|newspaper=BBC Radio 4 and 4 Extra Blog|date=23 September 2011}}</ref><ref name="Perivale 3 S&Pblog">{{cite news|last=Kane|first=Chris|title=Preserving the past at Perivale|url=http://www.bbc.co.uk/blogs/spacesandplaces/2011/03/preserving_the_past_at_perival.shtml|accessdate=19 January 2012|newspaper=BBC Spaces & Places Blog|date=9 March 2011}}</ref><ref>{{cite web|url=http://downloads.bbc.co.uk/foi/classes/disclosure_logs/rfi20111170_cost_of_new_archive_centre.pdf|title=Freedom of Information Act 2000 – RFI20111170|last=Jupe|first=Steve|date=20 October 2011|work=Freedom of Information Request|publisher=BBC|accessdate=7 February 2012}}</ref><ref>{{cite web|title=The BBC Archive Centre has moved|url=http://www.bbc.co.uk/commissioning/news/the-bbc-archive-centre-has-moved.shtml|work=BBC Commissioning|publisher=BBC|accessdate=19 January 2012}}</ref>\n\nMaterial is stored in thirteen vaults,<ref name="Perivale 2 AtBBCblog"/> controlled to match the best climate for the material inside them,<ref name=Perivale1/><ref name=gutechweekly /><ref name="BBC Archive BBCInternetblog"/><ref name="Perivale 2 AtBBCblog"/> and named after a different BBC personality depending on the content contained in them.<ref name="Perivale 2 AtBBCblog"/> In addition to the vaults, new editing and workrooms have been added so that the material can easily be transferred between formats as well as viewed and restored.<ref name="Perivale 2 AtBBCblog"/> The building has also been fitted with fire suppression systems to protect the archive in the event of an incident at the centre, so the total loss of the archive is avoided.<ref name="Perivale Centre R4 Blog"/>\n\n==Television Archive==\nThe \'\'\'BBC Television Archive\'\'\' contains over 600,000 hours of television broadcast material<ref name="BBCArchive TV 1">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – What\'s in the BBC Archive|url=http://www.bbc.co.uk/archive/tv_archive.shtml|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> located on 600,000/650,000 film reels<ref name="Windmill Road">{{cite web|title=A Tour of the BBC Archive at Windmill Road|url=https://www.youtube.com/watch?v=S3Z2djrAW2M|publisher=[[BBC]]|accessdate=23 July 2015|date=13 Aug 2010}}</ref><ref name="BBC Archive BBCInternetblog">{{cite news|last=Williams|first=Adrian|title=Safeguarding the BBC\'s archive|url=http://www.bbc.co.uk/blogs/bbcinternet/2010/08/safeguarding_the_bbcs_archive.html|accessdate=19 January 2012|newspaper=BBC Internet Blog|date=18 August 2010}}</ref><ref name="BBCArchive PTV 4">{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive – Film|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> and 2.4/2.7 million videotapes.<ref name="BBC Archive BBCInternetblog"/><ref name="Windmill Road"/> The archive itself holds extensive material from approximately the mid-1970s onwards, when important recordings at the broadcaster were retained for the future.<ref name="BBCArchives TV 6">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – When did the BBC start to ensure that important broadcasts were not destroyed|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>\n\nRecordings from before this date are less comprehensively preserved; the process of [[Kinescope|telerecording]] was originally invented in 1947<ref name="BBCArchive TV 2">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – Why aren\'t there many recordings from the early days of television|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=2|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> while videotape recording was gradually introduced from the late 1950s onwards,<ref name="BBC Archive TV 4">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – When did the BBC start recording programmes regularly|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=4|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> but due to the expense of the tapes,<ref name="BBCArchives PTV 8">{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive – Why was videotape invented|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> recording was seen for production use only with recordings subsequently being [[wiping|wiped]].<ref name="BBC Archive TV 4"/> or telerecordings being junked. The exceptions in the early years were usually occasions of great importance, such as the [[coronation of Queen Elizabeth II]].<ref name="BBCArchive TV 2"/> In addition, numerous programmes at the time were broadcast \'live\' and so utilised no recording procedure in the production process.<ref name="BBCArchive TV 2"/> The earliest item in the collection is from 1936.<ref name="BBC Archive BBCInternetblog"/><ref name="Perivale 3 S&Pblog"/><ref name="BBCArchives PTV 6">{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive – The oldest BBC Television film clip|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>\n\nIn 2013 there were 340,000 D3 Tapes but the hardware they have could only transfer up to 130,000 D3 tapes.<ref name=Digitising>{{cite web|title=Digitising the BBC archive|url=http://www.bbc.co.uk/academy/technology/article/art20130704121742520|publisher=[[BBC]]|accessdate=23 July 2015|date=2013}}</ref> The BBC has had to be very selective of what they are transferring.<ref name=Digitising/>\n\nBefore anything is put into the archive a team of Digitisation Operators watch and listen to programs looking for problems with the tapes or transfers.<ref name="Sarah Bello">{{cite web|title=Sarah Bello, BBC Archive|url=https://www.youtube.com/watch?v=MyG1lSdsKQs|publisher=[[BBC]]|accessdate=23 July 2015|date=11 March 2013}}</ref> \n\nToday, the majority of programmes are kept, including news, entertainment, drama and a selection of other long-running programmes such as quiz shows.<ref name="BBCArchive TV 7">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – How does the BBC decide what to keep in its archive today|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=7|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The remaining material from the television archive is offered to the [[British Film Institute]] prior to being disposed of.<ref name="BBCArchive TV 8">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – Does the BBC offer recordings it\'s not keeping for the archive to anyone else|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>\n\n==Sound Archive==\n{{main|BBC Sound Archive}}\n\nThe \'\'\'BBC Sound Archive\'\'\' contains the archived output from the BBC\'s radio output. Widespread recordings exist in the archive from the mid-1930s, when recording of programmes and speeches were kept for rebroadcast; the catalyst for this was the launch of the [[BBC Empire Service]] in 1932 and the subsequent rebroadcast of speeches from political leaders at a time convenient in the different time zones.<ref name="BBCArchive - Radio 3">{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive – Why did the BBC start making recordings|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Prior to this, the broadcast of recordings was seen as being false to the listener and was avoided.<ref name="BBCArchives Radio 2">{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive – Why aren\'t there many recordings from the early days of radio|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=2|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Any recordings made were frequently disposed of and it was the efforts of [[Marie Slocombe]], who founded the Sound Archive in 1937 when she retained recordings of prominent figures in the country, that the archive became into being officially when she was appointed the Sounds Recording Librarian in 1941.<ref name="BBCArchives - PRadio 6">{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – How did the Sound Archive begin|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Today, all of the BBC\'s radio output is recorded for re-use,<ref name="BBCArchive - Radio 8">{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive – Does the BBC keep copies of all programmes today|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> with approximately 66% of output being preserved in the Archives;<ref name="BBCArchive - Radio 8"/> programmes involving guests or live performances from artists are kept<ref name="BBCArchive - Radio 8"/> whereas programmes in which the DJ plays commercially available music are only sampled and not kept entirely.<ref name="BBCArchive - Radio 8"/> Prior to any material being disposed of, the material is offered to the [[British Library Sound Archive]].<ref name="BBCArchive TV 8"/>\n\nThe archive consists of a number of different formats including 200 [[Phonograph cylinder|wax cylinders]],<ref name="BBCArchive - PRadio 3">{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – What are the earliest sound recordings|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> numerous [[gramophone record]]s made from both [[shellac]] and [[vinyl]]<ref name="BBCArchives - PRadio 4">{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – Discs|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> as well as numerous more recordings on [[Reel-to-reel audio tape recording|tape]], CD and on [[digital audio tape]] (DAT).<ref name="BBCArchive PRadio 5">{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – Tape|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=5|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The difficulty of these different formats is the availability of the machines required to play them; some of the vinyl records in the archive are 16 inches in size and require large [[phonograph]] units to play,<ref name="BBCArchives - PRadio 4"/> while the players for the wax cylinders and DATs are no longer in production.<ref name="BBCArchive PRadio 5"/> There are 700,00 vinyl records, 180,000 78\'s records, 400,000 [[LP record]] and 350,000 [[Compact disc|Cd\'s]] in the archive.<ref name="Windmill Road"/>\n\nThe oldest item is a wax cylinder containing a recording made by [[Florence Nightingale]], recorded on 30 July 1890.<ref name="BBCArchive - PRadio 3"/> Another unique item is the gramophone record from [[Mary of Teck|Queen Mary]]\'s [[doll house]], which is approximately an inch in size and had the [[God Save the Queen|national anthem]] on it.<ref name="BBCArchives - PRadio 4"/>\n\nThe Sound Archive is based at the new BBC Archive Centre in Perivale, along with the television archive,<ref name=Perivale1/><ref name="Perivale 3 S&Pblog"/> and was previously based at Windmill Road, Brentford.\n\n==Written Archives==\nThe \'\'\'BBC Written Archive\'\'\' contains all the internal written documents and communications from the corporation from the launch in 1922 to the present day.<ref name="Written Archives BBC Story">{{cite web|title=The Written Archives|url=http://www.bbc.co.uk/historyofthebbc/contacts/wac.shtml|work=The BBC Story|publisher=BBC|accessdate=19 January 2012}}</ref><ref name="BBCArchive Written 1">{{cite web|last=Kavanagh|first=Jacquie|title=BBC Written Archives – What are the BBC Written Archives|url=http://www.bbc.co.uk/archive/written.shtml|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Its collections shed light into the behind the scenes workings of the corporation and also elaborate on the difficulties of getting a television or radio programme to or off the air as the case may be.<ref name="BBC Archive Written 3">{{cite web|last=Kavanagh|first=Jacquie|title=BBC Written Archives – What do the documents reveal|url=http://www.bbc.co.uk/archive/written.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The archive guidelines state that access to files post-1980 is restricted due to the current nature of the files; the general exception to this rule are documents such as scripts and Programme as Broadcast records.\n\nThe Written Archives are located at the BBC Written Archive Centre in [[Caversham, Berkshire]], near [[Reading, Berkshire|Reading]].<ref name="Written Archives BBC Story"/> The centre houses the archive on four and a half miles of shelving along with reading rooms. The centre is different from the other BBC Archives in that the centre opens for writers and academic researchers in higher education.<ref name="Written Archives BBC Story"/>\n\n==Photographic Library==\nThe \'\'\'BBC Photographic Library\'\'\' is responsible for approximately 10 million images,<ref name="BBCArchive Photo 1">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – What\'s in the BBC Photo Library|url=http://www.bbc.co.uk/archive/photo_library.shtml|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> dating back to 1922,<ref>{{cite web|title=BBC Pictures|url=http://www.bbc.co.uk/mediacentre/pictures/index.html|work=BBC Media Centre|publisher=BBC|accessdate=19 January 2012}}</ref> created for publicity purposes and subsequently kept for future use.<ref name="BBCArchive Photo 2">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – Why does the BBC have photographs|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=2|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> In addition to programme promotion, a large number of images are of historic events which are often incorporate into the daily news bulletins; as a result, half the photographic library team work specifically with these images.<ref name="BBCArchive Photo 4">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – The Team|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The images themselves are kept as originals in the archive, with digitisation only utilised when a specific image is required for use, when the image is sent in a digital format.<ref name="BBCArchive Photo 5">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – What format are the images stored on|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=5|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Copies of images are also used in case any images are damaged, notable due to [[vinegar syndrome]].<ref name="BBCArchive Photo 6">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – Preservation|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The BBC Photographic library itself is based within [[BBC Television Centre]], London.\n\nThe most popular images from the Archive include [[Colin Firth]] in \'\'[[Pride and Prejudice (1995 TV series)|Pride and Prejudice]]\'\', [[Michael Parkinson]] interviewing [[Muhammad Ali]], [[Jimmy Savile]] presenting the first \'\'[[Top of the Pops]]\'\', [[Martin Bashir]] interviewing [[Diana, Princess of Wales]] and a picture of [[Delia Derbyshire]] at work in the Radiophonic workshop at the BBC.<ref name="BBCArchive Photo 8">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – Our Top 10|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=8|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>\n\n==Heritage Collection==\nThe \'\'\'BBC Heritage Collection\'\'\' is the newest of the BBC Archives and holds a variety of historic broadcast technology, art, props and merchandise.<ref name="BBCArchives Heritage 2">{{cite web|last=O\'Connell|first=Rory|title=BBC Heritage Collection – Where do the items come from|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=2|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The collection was created out of personal collections and bequeaths by former staff members, as the BBC had no formal policy on the heritage collection until c.2003.<ref name="BBCArchives Heritage 2"/>\n\nThe collection includes, amongst other items, the BBC One Noddy Globe and clock,<ref name="BBCArchive Heritage 3">{{cite web|last=O\'Connell|first=Rory|title=BBC Heritage Collection – Broadcast technology|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> a [[BBC-Marconi Type A]] microphone,<ref name="BBCArchive Heritage 3"/> an early [[crystal radio]] made by the [[British Broadcasting Company]],<ref name="BBCArchive Heritage 3"/> a [[405-line television system|Marconi/EMI camera]] used in the early [[BBC Television]] experiments,<ref name="BBCArchive Heritage 3"/> a [[BBC Micro]] computer<ref name="BBCArchive Heritage 3"/> and a selection of items used to create [[Foley (filmmaking)|Foley]].<ref name="BBCArchive Heritage 3"/> In addition to all the broadcast technology, art is also kept, namely the portraits of all the BBC [[Director-General of the BBC|Director General]]s,<ref name="BBCArchive Heritage 4">{{cite web|last=O\'Connell|first=Rory|title=BBC Heritage Collection – Art|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> as well as props including an original [[TARDIS]] from \'\'[[Doctor Who]]\'\'<ref name="BBCArchive Heritage 5">{{cite web|last=O\'Connell|first=Rory|title=BBC Heritage Collection – Costumes and Props|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=5|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> and the children\'s television puppet [[Gordon the Gopher]].<ref name="BBCArchive Heritage 5"/>\n\nThe heritage collection itself has no one permanent home, as the majority of objects are on display, either around BBC properties or on loan to museums or other collections; the most notable museum housing the collection is the [[National Media Museum]] in [[Bradford]].<ref name="BBCArchive Heritage 8">{{cite web|last=O\'Connell|first=Rory|title=BBC Heritage Collection – Where can I see items from the collection|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>\n\n==Archive Treasure Hunt==\nAt the turn of the millennium, the BBC launched the \'\'\'BBC Archive Treasure Hunt\'\'\', a public appeal to recover pre-1980s lost BBC radio and television productions.<ref>{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/about.shtml |title=BBC Online – Cult – Treasure Hunt – About the Campaign |publisher=Bbc.co.uk |date= |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100721235531/http://www.bbc.co.uk/cult/treasurehunt/about/about.shtml| archivedate= 21 July 2010 <!--DASHBot-->| deadurl= no}}</ref> Original material, featuring several popular programmes were lost due to the practice of [[wiping]], because of copyright issues and for technological reasons.<ref>{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/lost.shtml |title=BBC Online – Cult – Treasure Hunt – About the Campaign |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}</ref><ref>{{cite web|author=Stuart Douglas - www.thiswaydown.org |url=http://www.btinternet.com/~m.brown1/bbchunt.htm |title=missing episodes articles |publisher=Btinternet.com |date=7 July 1965 |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100814103420/http://www.btinternet.com/~m.brown1/bbchunt.htm| archivedate= 14 August 2010 <!--DASHBot-->| deadurl= no}}</ref>\n\nThe resolution of this appeal was that over one hundred productions were recovered<ref>{{cite web|url=http://fiatifta.org/aboutfiat/news/old/2001/2001-04/03.light.html |title=No 4 2001 – Missing Believed Wiped |publisher=Fiat/Ifta |date= |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100716032923/http://fiatifta.org/aboutfiat/news/old/2001/2001-04/03.light.html| archivedate= 16 July 2010 <!--DASHBot-->| deadurl= no}}</ref> including \'\'[[The Men from the Ministry]]\'\', \'\'[[Something to Shout About (film)|Something To Shout About]]\'\', \'\'[[Man and Superman]]\'\', \'\'[[The Doctor\'s Dilemma (play)|The Doctor\'s Dilemma]]\'\', \'\'[[I\'m Sorry, I\'ll Read That Again]]\'\', \'\'[[Hancock\'s Half Hour]]\'\', \'\'[[I\'m Sorry, I Haven\'t A Clue]]\'\' and \'\'[[The Ronnie Corbett Thing]]\'\' in addition to recording sessions with [[Elton John]], [[Ringo Starr]] and [[Paul Simon]].<ref name="BBCTH">{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/listoffinds.shtml |title=BBC Online – Cult – Treasure Hunt – List of Finds |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}</ref><ref>{{cite web|url=http://www.allbusiness.com/services/motion-pictures/4848337-1.html |title=\'hunt\' Unearths BBC Treasures From Radio, Tv &#124; Business solutions from |publisher=AllBusiness.com |date=9 November 2001 |accessdate=30 July 2010}}</ref> Also, the Peter Sellers Estate Collection donated numerous recordings featuring [[Peter Sellers]].<ref name="BBCTH"/>\n\n==Creative Archive Licence==\nThe BBC together with the [[British Film Institute]], the [[Open University]], [[Channel 4]] and [[Teachers\' TV]] formed a collaboration, named the Creative Archive Licence Group, to create a copyright licence for the re-release of archived material.<ref name=CAL>{{cite web|title=Creative Archive pilot|url=http://www.bbc.co.uk/creativearchive/|publisher=BBC|accessdate=31 March 2016}}</ref>\n\nThe Licence was a trial, launched in 2005, and notable for the re-release of part of the [[BBC News]]\' archive and programmes made by the [[BBC Natural History Unit]] for creative use by the public. While artists and teachers were encouraged to use the content to create works of their own, the terms of the licence were restrictive compared to [[copyleft]] licences. Use of Creative Archive content for commercial, "endorsement, campaigning, defamatory or derogatory purposes" was forbidden, any derivative works were to be released under the same licence, and content was only  to be used within the UK.<ref name=CAL/><ref>{{cite web|title=Creative Archive License|url=http://news.bbc.co.uk/1/hi/help/4527506.stm|publisher=BBC|accessdate=17 January 2012}}</ref> The trial ended in 2006 following a review by the [[BBC Trust]] and works released under the licence were withdrawn.<ref name=CAL/>\n\n==Voices from the archives==\nVoices from the Archives is a former [[BBC]] project, launched in partnership with [[BBC Four]] that provided free access to audio interviews with various notable people and professions from a variety of political, religious and social backgrounds. The website ceased to be updated in June 2005, and the concept was instead adopted by [[BBC Radio 4]] as a collection of film interviews from various programmes.\n\n==Programme catalog==\n{{main|BBC Programme Catalogue}}\nOver the years there the BBC has used various Programme catalog databases to keep a record of the programmes in the archives. Internal databases include [[Infax]] and [[BBC Fabric|Fabric]], and publicly accessible databases include [[BBC Genome]] and [http://www.bbc.co.uk/programmes BBC Programmes].\n\n==See also==\n{{portal|BBC}}\n* [[BBC Genome Project]]\n* [[Lost film]]\n* [[Film preservation]]\n* [[Missing Believed Wiped]]\n* [[Telerecording]]\n* [[Doctor Who missing episodes|\'\'Doctor Who\'\' missing episodes]]\n* [[Timeline of the BBC]]\n\n==References==\n{{reflist|2}}\n\n==External links==\n\n===BBC Archives===\n*{{bbc.co.uk|id=archive|title=BBC Archives}}\n*{{bbc.co.uk|id=bbcfour/collections|title=BBC Four – Collections}}\n*{{bbc.co.uk|id=archive/archive_pioneers|title=BBC Archive collection – Archive Pioneers: Saviours of sound at the BBC}}\n*{{bbc.co.uk|id=programmes|title=BBC Programmes}}\n*{{bbc.co.uk|id=informationandarchives|title=BBC Information and Archives}}\n* [https://www.theguardian.com/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road Tech Weekly podcast: In the BBC archives] from \'\'[[The Guardian]]\'\' website.\n* [https://www.dropbox.com/s/rz1o57nzlsf1v04/BBC%20Archive%20Collections%20Guide%202012.pdf?dl=0 BBC Archive Collections: What\'s In The Archives, And How To Use Them]\n\n===Wiped Material===\n* [http://www.missing-episodes.com/ British TV Missing Episodes Index]\n* [http://www.wipednews.com/ Wiped News.Com – A news and features website devoted to missing TV, Film & Radio]\n\n{{BBC}}\n\n[[Category:BBC]]\n[[Category:BBC New Media|Archives]]\n[[Category:Data management]]\n[[Category:Broadcasting websites]]\n[[Category:British websites]]\n[[Category:BBC offices, studios and buildings|Archives]]\n[[Category:Organisations based in Reading, Berkshire]]\n[[Category:History of television in the United Kingdom]]\n[[Category:History of radio]]\n[[Category:BBC history]]\n[[Category:Year of establishment missing]]\n[[Category:Archives in Berkshire]]\n[[Category:Television archives]]']
['Content inventory', '27255666', 'A \'\'\'content inventory\'\'\' is the process and the result of cataloging the entire contents of a [[website]]. An allied practice—a [[content audit]]—is the process of \'\'evaluating\'\' that content.<ref name="Halverson">{{cite web |url= http://www.peachpit.com/articles/article.aspx?p=1388961 |title= Content Strategy for the Web: Why You Must Do a Content Audit |first=Kristina |last=Halvorson |date=August 2009 |accessdate=6 May 2010}}</ref><ref name="Baldwin">{{cite web |url= http://nform.ca/blog/2010/01/doing-a-content-audit-or-inven |title= Doing a content audit or inventory |first=Scott |last=Baldwin |date=January 2010 |accessdate=29 April 2010}}</ref><ref name="Marsh">{{cite web |url=http://www.hilarymarsh.com/2012/03/12/how-to-do-a-content-audit/ |title=How to do a content audit |first=Hilary |last=Marsh |date=March 2012 |accessdate=2 May 2013}}</ref> A content inventory and a [[content audit]] are closely related concepts, and they are often conducted in tandem.\n\n==Description==\n\nA content inventory typically includes all information assets on a website, such as [[web page]]s (html), [[meta element]]s (e.g., keywords, description, page title), images, audio and video files, and document files (e.g., .pdf, .doc, .ppt).<ref name="Spencer2006">{{cite web |url=http://maadmob.net/donna/blog/2006/taking-a-content-inventory |title=Taking a content inventory |first=Donna |last=Spencer |date=January 2006 |accessdate=27 April 2010}}</ref><ref name="Doss2007">{{cite web |url=http://www.fatpurple.com/2010/02/26/content-inventory/ |title=Content Inventory: Sometimes referred to as Web Content Inventory or Web Audit |first=Glen |last=Doss |date=January 2007 |accessdate=27 April 2010}}</ref><ref name="Jones2009">{{cite web |url=http://www.uxmatters.com/mt/archives/2009/08/content-analysis-a-practical-approach.php |title=Content Analysis: A Practical Approach |first=Colleen |last=Jones |date=August 2009 |accessdate=27 April 2010}}</ref><ref name="Leise2007">{{cite web |url=http://boxesandarrows.com/view/content-analysis |title=Content Analysis Heuristics |first=Fred |last=Leise |date=March 2007 |accessdate=27 April 2010}}</ref><ref name="Baldwin2010">{{cite web |url=http://nform.ca/blog/2010/01/doing-a-content-audit-or-inven |title=Doing a content audit or inventory |first=Scott |last=Baldwin |date=January 2010 |accessdate=27 April 2010}}</ref><ref name="Krozser">{{cite web |url=http://www.alttags.org/content-management/the-content-inventory-roadmap-to-a-succesful-cms-implementation/ |title= The Content Inventory: Roadmap to a Successful CMS Implementation |first=Kassia |last=Krozser |date=April 2005 |accessdate=27 April 2010}}</ref> A content inventory is a [[Quantitative research|quantitative analysis]] of a website. It simply logs what is on a website. The content inventory will answer the question: “What is there?” and can be the start of a website review.<ref name="GOSS Interactive">{{cite web |url=http://www.gossinteractive.com/community/whitepapers/conducting-a-website-review-and-implementing-results-for-increased-customer-engagement-and-conversions |title= Conducting a website review and implementing results for increased customer engagement and conversions()|first=GOSS Interactive|date=October 2011 |accessdate=8 October 2011}}</ref> A related (and sometimes confused term) is a [[content audit]], a [[Qualitative research|qualitative analysis]] of information assets on a website. It is the assessment of that content and its place in relationship to surrounding Web pages and information assets. The content audit will answer the question: “Is it any good?”<ref name="Baldwin"/><ref name="Marsh"/>\n\nOver the years, techniques for creating and managing a content inventory have been developed and refined in the field of website [[content management]].<ref name="Halverson"/><ref name="Veen2002">{{cite web |url=http://www.adaptivepath.com/ideas/essays/archives/000040.php |title=Doing a Content Inventory (Or, A Mind-Numbingly Detailed Odyssey Through Your Web Site) |first=Jeffrey |last=Veen |date=June 2002 |accessdate=27 April 2010}}</ref><ref name="Bruns">{{cite web |url=http://donbruns.net/index.php/how-to-automatically-index-a-content-inventory/ |title= Automatically Index a Content Inventory with GetUXIndex() |first=Don |last=Bruns |date=March 2010 |accessdate=6 May 2010}}</ref>\n\nA [[spreadsheet]] application (e.g., [[Microsoft Excel]] or [[LibreOffice Calc]]) is the preferred tool for keeping a content inventory; the data can be easily configured and manipulated. Typical categories in a content inventory include the following:\n\n* Link — The [[URL]] for the page\n* Format — For example, .[[html]], [[.pdf]], [[Microsoft Word|.doc]], [[Microsoft PowerPoint|.ppt]]\n* Meta page title — Page title as it appears in the meta <title> tag\n* Meta keywords — Keywords as they appear in the [[Meta tag#The keywords attribute|meta name="keywords" tag element]]\n* Meta description — Text as it appears in the [[Meta tag#The description attribute|meta name="description" tag element]]\n* Content owner — Person responsible for maintaining page content\n* Date page last updated — Date of last page update\n* Audit Comments (or Notes) — Audit findings and notes\n\nThere are other descriptors that may need to be captured on the inventory sheet. Content management experts advise capturing information that might be useful for both short- and long-term purposes. Other information could include:\n\n* the overall topic or area to which the page belongs\n* a short description of the information on the page\n* when the page was created, date of last revision, and when the next page review is due\n* pages this page links to\n* pages that link to this page\n* page status – keep, delete, revise, in revision process, planned, being written, being edited, in review, ready for posting, or posted\n* rank of page on the website – is it a top 50 page? a bottom 50 page? Initial efforts might be more focused on those pages that visitors use the most and least.\n\nOther tabs in the inventory workbook can be created to track related information, such as meta keywords, new Web pages to develop, website tools and resources, or content inventories for sub-areas of the main website. Creating a single, shared location for information related to a website can be helpful for all website content managers, writers, editors, and publishers.\n\nPopulating the spreadsheet is a painstaking task, but some up-front work can be automated with software, and other tools and resources can assist the audit work.\n\n==Value==\n\nA content inventory and a content audit are performed to understand what is on a website and why it is there. The inventory sheet, once completed and revised as the site is updated with new content and information assets, can also become a resource for help in maintaining [[website governance]].\n\nFor an existing website, the information cataloged in a content inventory and content audit will be a resource to help manage all of the information assets on the website.<ref name="Usability">{{cite web |url=http://www.usability.gov/methods/design_site/inventory.html |title=Content Inventory |date=26 May 2009 |publisher=U.S. Department of Health & Human Services |accessdate=4 May 2010}}</ref> The information gathered in the inventory can also be used to plan a website re-design or site migration to a [[web content management system]].<ref name="Krozser"/> When planning a new website, a content inventory can be a useful [[project management]] tool: as a guide to map [[information architecture]] and to track new pages, page revision dates, content owners, and so on.\n\n==See also==\n\n* [[Content audit]]\n* [[Web content management system]]\n* [[Design methods]]\n* [[Information architecture]]\n* [[Web design]]\n* [[Website governance]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n\n* In his article [http://www.boxesandarrows.com/view/a-map-based-approach A Map-Based Approach to a Content Inventory], Patrick Walsh describes how to use [[Microsoft Access]] and Microsoft Excel to link a data attribute with a structural attribute to create “a tool that can be used throughout the lifetime of a website.”\n* In the article [http://www.louisrosenfeld.com/home/bloug_archive/000448.html The Rolling Content Inventory], author Louis Rosenfeld argues that “ongoing, partial content inventories” are more cost-effective and realistic to implement.\n* Colleen Jones writes from a UX design perspective in [http://www.uxmatters.com/mt/archives/2009/08/content-analysis-a-practical-approach.php Content Analysis: A Practical Approach].\n* [http://xmlpress.net/content-strategy/audits-and-inventories/ Content Audits and Inventories: A Handbook] is a practical guide to conducting content inventories and audits.\n\n==External links==\n* [http://home.snafu.de/tilman/xenulink.html Xenu\'s Link Sleuth]\n* [http://siteorbiter.com/ SiteOrbiter]\n* [http://www.webconfs.com/similar-page-checker.php Similar Page Checker]\n* [http://www.cryer.co.uk/resources/link_checkers.htm Link Checker Tools]\n* [http://www.kevinpnichols.com/downloads/kpn_content_audit.xls Kevin P Nichols\' Content Inventory and Audit Template]\n\n{{DEFAULTSORT:Content Inventory}}\n[[Category:Data management]]\n[[Category:Website management]]\n[[Category:Content management systems]]']
['Storage area network', '20444608', '{{Distinguish|Network-attached storage}}\n{{Use dmy dates|date=February 2013}}\n{{Area networks}}\n\nA \'\'\'storage area network\'\'\' (\'\'\'SAN\'\'\') <ref>{{cite web |url=http://cctvinstitute.co.uk/storage-area-network/|title=Storage Area Network by Noor Ul Mushtaq }}</ref> is a network which provides access to consolidated, [[Block device|block level data storage]]. SANs are primarily used to enhance storage devices, such as [[disk array]]s, [[tape library|tape libraries]], and [[optical jukebox]]es, accessible to [[Server (computing)|server]]s so that the devices appear to the [[operating system]] as [[Direct-attached storage|locally attached devices]]. A SAN typically has its own network of storage devices that are generally not accessible through the local area network (LAN) by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.\n\nA SAN does not provide file abstraction, only block-level operations. However, [[file systems]] built on top of SANs do provide file-level access, and are known as [[shared-disk file system]]s.\n\n== Storage ==\n{{Refimprove section|date=February 2014}}\nHistorically, [[data centers]] first created "islands" of [[SCSI]] [[disk array]]s as [[direct-attached storage]] (DAS), each dedicated to an application, often visible as a number of "virtual hard drives" addressed as [[Logical Unit Number]]s (LUNs).<ref>{{cite web |url=http://www.novell.com/documentation/oes/stor_ovw/?page=/documentation/oes/stor_ovw/data/ami6rr0.html |title=Novel Doc: OES 1 - Direct Attached Storage Solutions}}</ref> Essentially, a SAN consolidates such storage islands together using a high-speed network.\n\nOperating systems maintain their own [[file system]]s on their own dedicated, non-shared LUNs, as though they were local to themselves. If multiple systems were simply to attempt to share a LUN, these would interfere with each other and quickly corrupt the data. Any planned sharing of data on different computers within a LUN requires software, such as [[SAN file system]]s or [[clustered computing]].\n\nDespite such issues, SANs help to increase storage capacity utilization, since multiple servers consolidate their private storage space onto the disk arrays.\nCommon uses of a SAN include provision of transactionally accessed data that require high-speed [[block device|block-level access]] to the hard drives such as email servers, databases, and high usage file servers.\n\n===SAN compared to NAS===\n[[Network-attached storage]] (NAS) was designed independently of SAN systems. In both a NAS and SAN, the various computers in a network, such as individual users\' desktop computers and dedicated servers running applications ("[[application server]]s"), can share a more centralized collection of storage devices via a network connection such as a [[local area network]] (LAN).\n\nConcentrating the storage on one or more NAS servers or in a SAN instead of placing storage devices on each application server allows application server configurations to be optimized for running their applications instead of also storing all the related data and moves the storage management task to the NAS or SAN system. Both NAS and SAN have the potential to reduce the amount of excess storage that must be purchased and provisioned as spare space. In a DAS-only architecture, each computer must be provisioned with enough excess storage to ensure that the computer does not run out of space at an untimely moment. In a DAS architecture the spare storage on one computer cannot be utilized by another. With a NAS or SAN architecture, where storage is shared across the needs of multiple computers, one normally provisions a pool of shared spare storage that will serve the peak needs of the connected computers, which typically is less than the total amount of spare storage that would be needed if individual storage devices were dedicated to each computer.\n\nIn a NAS the storage devices are directly connected to a file server that makes the storage available at a file-level to the other computers. In a SAN, the storage is made available at a lower "block-level", leaving file system concerns to the "client" side. SAN protocols include [[Fibre Channel]], [[iSCSI]], [[ATA over Ethernet]] (AoE) and [[HyperSCSI]]. One way to loosely conceptualize the difference between a NAS and a SAN is that NAS appears to the client OS (operating system) as a file server (the client can map network drives to shares on that server) whereas a disk available through a SAN still appears to the client OS as a disk, visible in disk and volume management utilities (along with client\'s local disks), and available to be formatted with a file system and mounted.\n\nOne drawback to both the NAS and SAN architecture is that the connection between the various CPUs and the storage units are no longer dedicated high-speed busses tailored to the needs of storage access. Instead the CPUs use the LAN to communicate, potentially creating bandwidth as well as performance bottlenecks. Additional data security considerations are also required for NAS and SAN setups, as information is being transmitted via a network that potentially includes design flaws, security exploits and other vulnerabilities that may not exist in a DAS setup.\n\nWhile it is possible to use the NAS or SAN approach to eliminate all storage at user or application computers, typically those computers still have some local Direct Attached Storage for the operating system, various program files and related temporary files used for a variety of purposes, including [[cache (computing)|caching]] content locally.\n\nTo understand their differences, a comparison of SAN, DAS and NAS architectures may be helpful.<ref name="eval">{{cite web |title= Storage Architectures: DAS, SAN, NAS, iSCSI SAN |work= Marketing web site |url=  http://www.evaluatorgroup.com/document/storage-architectures/ |publisher= Evaluator Group |archivedate= September 17, 2016 |archiveurl= https://web.archive.org/web/20160917144751/http://www.evaluatorgroup.com/document/storage-architectures/ |accessdate= November 10, 2016 }}</ref>\n\n===SAN-NAS hybrid===\n[[Image:Compingles3.png|right|thumb|260px|Hybrid using SAN, [[Direct-attached storage|DAS]] and NAS technologies.]]\nDespite their differences, SAN and NAS are not mutually exclusive, and may be combined as a SAN-NAS hybrid, offering both file-level protocols (NAS) and block-level protocols (SAN) from the same system. An example of this is [[Openfiler]], a free software product running on Linux-based systems. A shared disk file system can also be run on top of a SAN to provide filesystem services.\n\n== Benefits ==\nSharing storage usually simplifies storage administration and adds flexibility since cables and storage devices do not have to be physically moved to shift storage from one server to another.\n\nOther benefits include the ability to allow servers to boot from the SAN itself. This allows for a quick and easy replacement of faulty servers since the SAN can be reconfigured so that a replacement server can use the [[Logical Unit Number|LUN]] of the faulty server. While this area of technology is still new, many view it as being the future of the enterprise datacenter.<ref>{{cite web | title=SAN vs DAS: A Cost Analysis of Storage in the Enterprise | url=http://capitalhead.com/articles/san-vs-das-a-cost-analysis-of-storage-in-the-enterprise.aspx | work=SAN vs DAS: A Cost Analysis of Storage in the Enterprise | date=31 October 2008 | accessdate=2010-01-28}}</ref>\n\nSANs also tend to enable more effective [[disaster recovery]] processes. A SAN could span a distant location containing a secondary storage array. This enables [[storage replication]] either implemented by [[disk array controller]]s, by server software, or by specialized SAN devices. Since IP [[Wide area network|WAN]]s are often the least costly method of long-distance transport, the [[Fibre Channel over IP]] (FCIP) and iSCSI protocols have been developed to allow SAN extension over IP networks. The traditional physical SCSI layer could support only a few meters of distance - not nearly enough to ensure business continuance in a disaster.\n\nThe economic consolidation of disk arrays has accelerated the advancement of several features including I/O caching, [[Snapshot (computer storage)|snapshotting]], and volume cloning ([[Business Continuance Volumes]] or BCVs).\n\n==Network types==\nMost storage networks use the [[SCSI]] protocol for communication between servers and disk drive devices. A mapping layer to other protocols is used to form a network:\n\n* [[ATA over Ethernet|ATA over Ethernet (AoE)]], mapping of [[AT Attachment|ATA]] over [[Ethernet]]\n* [[Fibre Channel Protocol]] (FCP), the most prominent one, is a mapping of SCSI over [[Fibre Channel]]\n* [[Fibre Channel over Ethernet]] (FCoE)\n* [[ESCON]] over Fibre Channel ([[FICON]]), used by [[mainframe computer]]s\n* [[HyperSCSI]], mapping of SCSI over Ethernet\n* [[iFCP]]<ref>{{cite web |url=http://www.techweb.com/encyclopedia/defineterm.jhtml?term=IPstorage |title=TechEncyclopedia: IP Storage |accessdate=2007-12-09}}</ref> or [[SANoIP]]<ref>{{cite web |url=http://www.techweb.com/encyclopedia/defineterm.jhtml?term=SANoIP |title=TechEncyclopedia: SANoIP |accessdate=2007-12-09}}</ref> mapping of FCP over IP\n* [[iSCSI]], mapping of SCSI over [[TCP/IP]]\n* [[iSCSI Extensions for RDMA]] (iSER), mapping of iSCSI over [[InfiniBand]]\n\nStorage networks may also be built using [[Serial attached SCSI|SAS]] and [[Serial ATA|SATA]] technologies. SAS evolved from SCSI direct-attached storage. SATA evolved from [[Parallel ATA|IDE]] direct-attached storage. SAS and SATA devices can be networked using [[Serial attached SCSI#SAS expanders|SAS Expanders]].\n\nExamples of stacked protocols using SCSI:\n\n{| class="wikitable" style="text-align:center"\n| colspan="5" | Applications\n|-\n| colspan="5" | [[SCSI]] Layer\n|-\n| rowspan="4" | [[Fibre Channel Protocol|FCP]]\n| rowspan="3" | [[Fibre Channel Protocol|FCP]]\n| [[Fibre Channel Protocol|FCP]]\n| [[Fibre Channel Protocol|FCP]]\n| rowspan="2" | [[iSCSI]]\n|-\n| [[Fibre Channel over IP|FCIP]]\n| [[Internet Fibre Channel Protocol|iFCP]]\n|-\n| colspan="3" | [[Internet Protocol|TCP]]\n|-\n| [[Fibre Channel over Ethernet|FCoE]]\n| colspan="3" | [[Internet Protocol|IP]]\n|-\n| [[Fibre Channel|FC]]\n| colspan="4" | [[Ethernet]]\n|}\n\n== SAN infrastructure ==\n[[Image:ML-QLOGICNFCCONN.JPG|thumb| [[Qlogic]] SAN-[[Fibre Channel switch|switch]] with optical [[Fibre Channel]] [[Electrical connector|connectors]] installed.]]\nSANs often use a [[Fibre Channel fabric]] topology, an infrastructure specially designed to handle storage communications. It provides faster and more reliable access than higher-level protocols used in [[Network-attached storage|NAS]]. A fabric is similar in concept to a [[network segment]] in a local area network. A typical Fibre Channel SAN fabric is made up of a number of [[Fibre Channel switch]]es.\n\nMany SAN equipment vendors also offer some form of Fibre Channel routing, and these can allow data to cross between different fabrics without merging them. These offerings use proprietary protocol elements, and the top-level architectures being promoted are radically different.\nFor example, they might map Fibre Channel traffic over IP or over [[Synchronous optical networking|SONET/SDH]].\n\n== Compatibility ==\nOne of the early problems with Fibre Channel SANs was that the switches and other hardware from different manufacturers were not compatible. Although the basic storage protocol FCP was standard, some of the higher-level functions did not interoperate well. Similarly, many host operating systems would react badly to other operating systems sharing the same fabric.{{Citation needed|date=October 2010}}.\n\n== In media and entertainment ==\n[[Video editing]] systems require very high data transfer rates and very low latency.\nSANs in media and entertainment are often referred to as serverless due to the nature of the configuration which places the video workflow (ingest, editing, playout) desktop clients directly on the SAN rather than attaching to servers. Control of data flow is managed by a distributed file system such as StorNext by Quantum.<ref>{{cite web|url=http://www.quantum.com/products/software/stornext/index.aspx |title=StorNext Storage Manager - High-speed file sharing, Data Management and Digital Archiving Software |publisher=Quantum.com |date= |accessdate=2013-07-08}}</ref>\n\nPer-node bandwidth usage control, sometimes referred to as [[quality of service]] (QoS), is especially important in video editing as it ensures fair and prioritized bandwidth usage across the network.\n\n==Storage virtualization==\n{{main|Storage virtualization}}\n[[Storage virtualization]] is the process of abstracting logical storage from physical storage. The physical storage resources are aggregated into storage pools, from which the logical storage is created. It presents to the user a logical space for data storage and transparently handles the process of mapping it to the physical location, a concept called [[location transparency]]. This is implemented in modern disk arrays, often using vendor proprietary technology. However, the goal of storage virtualization is to group multiple disk arrays from different vendors, scattered over a network, into a single storage device.  The single storage device can then be managed uniformly. {{Citation needed|date=September 2011}}\n\n==Quality of service==\nSAN Storage QoS enables the desired storage performance to be calculated and maintained for network customers accessing the device.\nSome factors that affect SAN QoS are:\n\n*[[Bandwidth (computing)|Bandwidth]] – The rate of data throughput available on the system.\n*[[Latency (engineering)|Latency]] – The time delay for a read/write operation to execute.\n*Queue depth – The number of outstanding operations waiting to execute to the underlying disks (traditional or [[solid-date drive]]s).\n\nQoS can be impacted in a SAN storage system by unexpected increase in data traffic (usage spike) from one network user that can cause performance to decrease for other users on the same network. This can be known as the “noisy neighbor effect.” When QoS services are enabled in a SAN storage system, the “noisy neighbor effect” can be prevented and network storage performance can be accurately predicted.\n\nUsing SAN storage QoS is in contrast to using disk over-provisioning in a SAN environment. Over-provisioning can be used to provide additional capacity to compensate for peak network traffic loads. However, where network loads are not predictable, over-provisioning can eventually cause all bandwidth to be fully consumed and latency to increase significantly resulting in SAN performance degradation.\n\n== See also ==\n* [[ATA over Ethernet]] (AoE)\n* [[Direct-attached storage]] (DAS)\n* [[Disk array]]\n* [[Fibre Channel]]\n* [[Fibre Channel over Ethernet]]\n* [[File Area Network]]\n* [[Host Bus Adapter]] (HBA)\n* [[iSCSI]]\n* [[iSCSI Extensions for RDMA]]\n* [[List of networked storage hardware platforms]]\n* [[List of storage area network management systems]]\n* [[Massive array of idle disks]] (MAID)\n* [[Network-attached storage]] (NAS)\n* [[Redundant array of independent disks]] (RAID)\n* [[SCSI RDMA Protocol]] (SRP)\n* [[Storage Management Initiative – Specification]] — (SMI-S)\n* [[Storage hypervisor]]\n* [[Storage Resource Management]] (SRM)\n* [[Storage virtualization]]\n* [[System area network]]\n\n==References==\n{{More footnotes|date=June 2008}}\n<references/>\n\n==External links==\n<!-- ATTENTION! Please do not add links without discussion and consensus on the talk page. Undiscussed links will be removed. -->\n* [https://www.redbooks.ibm.com/redbooks/pdfs/sg245470.pdf Introduction to Storage Area Networks Exhaustive Introduction into SAN, [[IBM Redbooks|IBM Redbook]]]\n* [http://capitalhead.com/articles/san-vs-das-a-cost-analysis-of-storage-in-the-enterprise.aspx SAN vs. DAS: A Cost Analysis of Storage in the Enterprise]\n* [http://searchstorage.techtarget.co.uk/generic/0,295582,sid181_gci1516893,00.html SAS and SATA, solid-state storage lower data center power consumption]\n* [https://www.youtube.com/playlist?list=PLivYD7W2z2HMGGRIwRoRcqLL4HMpR1dIe SAN NAS Videos]\n* [http://www.storageareanetworkinfo.blogspot.com.ar/ Storage Area Network Info]\n\n<!--Interwikies-->\n\n{{Authority control}}\n\n{{DEFAULTSORT:Storage Area Network}}\n[[Category:Data management]]\n[[Category:Telecommunications engineering]]\n[[Category:Storage area networks| ]]']
['ISO 8000', '20375252', '\'\'\'[[International Organization for Standardization|ISO]] 8000\'\'\', the global standard for \'\'[[Data Quality and Enterprise Master Data]]\'\', describes the features and defines the requirements for the Data Quality and Portability of Enterprise Master Data.  Master Data is typically "internal" business information about clients, products and operations.  The standard is currently under development, but is quickly being adopted by many Fortune 500 corporations and certain public agencies involved in the regulation and supervision of financial markets around the world. ISO 8000 is one of the emerging technology standards that large and complex organizations are turning to in order to improve business processes and control operational costs.  The standard will be published as a number of separate documents, which [[International Organization for Standardization|ISO]] calls "parts".\n\nISO 8000 is being developed by [[ISO TC 184/SC 4|ISO technical committee TC 184, \'\'Automation systems and integration\'\', sub-committee SC 4, \'\'Industrial data\'\']]. Like other [[International Organization for Standardization|ISO]] and [[International Electrotechnical Commission|IEC]] standards, ISO 8000 is copyrighted and is not freely available.<ref>[http://www.iso.org/iso/copyright.htm ISO copyright policy]</ref>\n\n== Published parts ==\n\nThe following part has already been published:\n\n* ISO/TS 8000-1:2011, \'\'Data quality &mdash; Part 1: Overview\'\'<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50798 ISO catalogue page for ISO/TS 8000-1:2011]</ref>\n* ISO 8000-2:2012, \'\'Data quality &mdash; Part 2: Vocabulary\'\'<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=57436 ISO catalogue page for ISO 8000-2:2012]</ref>\n* ISO 8000-61:2016, \'\'Data quality &mdash; Part 61: Data quality management: Process reference model\'\'<ref>[http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=63086 ISO catalogue page for ISO-61:2016]</ref>\n* ISO/TS 8000-100:2009, \'\'Data quality &mdash; Part 100: Master data: Exchange of characteristic data: Overview\'\'<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=52129 ISO catalogue page for ISO/TS 8000-100:2009]</ref>\n* ISO 8000-102:2009, \'\'Data quality &mdash; Part 102: Master data: Exchange of characteristic data: Vocabulary\'\'<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50799 ISO catalogue page for ISO 8000-102:2009]</ref>\n* ISO 8000-110:2009, \'\'Data quality — Part 110: Master data: Exchange of characteristic data: Syntax, semantic encoding, and conformance to data specification\'\'<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51653 ISO catalogue page for ISO 8000-110:2009]</ref>\n* ISO/TS 8000-120:2009, \'\'Data quality &mdash; Part 120: Master data: Exchange of characteristic data: Provenance\'\'<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50801 ISO catalogue page for ISO/TS 8000-120:2009]</ref>\n* ISO/TS 8000-130:2009, \'\'Data quality &mdash; Part 130: Master data: Exchange of characteristic data: Accuracy\'\'<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50802 ISO catalogue page for ISO/TS 8000-130:2009]</ref>\n* ISO/TS 8000-140:2009, \'\'Data quality &mdash; Part 140: Master data: Exchange of characteristic data: Completeness\'\'<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=53589 ISO catalogue page for ISO/TS 8000-140:2009]</ref>\n* ISO/TS 8000-150:2011, \'\'Data quality &mdash; Part 150: Master data: Quality management framework\'\'<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=53589 ISO catalogue page for ISO/TS 8000-150:2011]</ref>\n\n== Further reading ==\n\n{{Citation | last=Benson | first=Peter | journal=Real-World Decision Support (RWDS) Journal | year=2009 | volume=3 | issue=4 | title=ISO 8000 Data Quality — The Fundamentals, Part 1 | url=http://www.ewsolutions.com/resource-center/rwds_folder/rwds-archives/issue.2009-10-12.0790666855/document.2009-10-12.3367922336/view?searchterm=ISO%208000}}\n\n{{Citation | last=Benson | first=Peter | title=NATO Codification System as the foundation for ISO 8000, the International Standard for data quality (Oil IT Journal)| year=2008 | url=http://www.oilit.com/papers/Benson.pdf}}\n\n{{Citation | last=Benson | first=Peter | title=ISO 8000 &mdash; A new international standard for data quality | year=2009 | url=http://www.dataqualitypro.com/data-quality-home/iso-8000-a-new-international-standard-for-data-quality.html}}\n\n{{Citation | last=Benson | first=Peter | title=Peter Benson discusses the certification options of ISO 8000 (Live Recording) | year=2010 | url=http://www.dataqualitypro.com/data-quality-home/peter-benson-discusses-the-certification-options-of-iso-8000.html}}\n\n{{Citation | last=Grantner | first=Emily | title=ISO 8000 &mdash; A Standard for data quality | year=2007 | issue=Oct.-Dec. | journal=Logistics Spectrum | url=http://www.highbeam.com/doc/1P3-1518467381.html}}\n\n{{Citation | last=West | first=Matthew | title=ISO 8000 &mdash; the Emerging Standard for Data Quality | year=2009 | journal=IAIDQ\'s Information and Data Quality Newsletter | volume=5 | issue=3 | url=http://iaidq.org/publications/west-2009-07.shtml}} (full article requires no cost registration to access)\n\n== References ==\n<references/>\n\n{{ISO standards}}\n\n{{DEFAULTSORT:Iso 8000}}\n[[Category:ISO standards|#08000]]\n[[Category:Data management]]']
['Data binding', '15592339', "'''Data binding''' is a general technique that binds data sources from the provider and consumer together and [[data synchronization|synchronizes]] them. This is usually done with two data/information sources with different languages as in [[XML data binding]]. In [[UI data binding]], data and information objects of the same language but different logic function are bound together (e.g. [[Java (programming language)|Java]] [[user interface|UI]] elements to Java objects).<ref>{{cite web |url=https://www.techopedia.com/definition/15652/data-binding|title=What is Data Binding? |work=Techopedia.com |accessdate=30 December 2015}}</ref>\n\nIn a data binding process, each data change is reflected automatically by the elements that are bound to the data. The term data binding is also used in cases where an outer representation of data in an element changes, and the underlying data is automatically updated to reflect this change. As an example, a change in a [[text box|<code>TextBox</code>]] element could modify the underlying data value.<ref>{{cite web |url=https://msdn.microsoft.com/en-us/library/ms752347(v=vs.110).aspx |title=Data Binding Overview |work=Microsoft Developer Network |publisher=Microsoft |access-date=29 December 2016}}</ref>\n\n== Data binding frameworks and tools ==\n\n=== [[Embarcadero Delphi|Delphi]] ===\n* DSharp 3rd-party Data Binding tool\n* [[OpenWire (library)|OpenWire]] Visual Live Binding - 3rd-party Visual Data Binding tool\n* LiveBindings\n\n=== [[C Sharp (programming language)|C#]] ===\n* [[Windows Presentation Foundation]]\n\n=== [[JavaScript]] ===\n* [[AngularJS]]\n* [[Backbone.js]]\n* BindingJS\n* Datum.js<ref>{{cite web |url=http://datumjs.com|title=Datum.js|accessdate=7 November 2016}}</ref>\n* [[EmberJS]]\n* Generic Data Binder\n* [[KnockoutJS]]\n* [[React (JavaScript library)]]\n* SAP/OPEN UI5\n* [[Vue.js]]\n\n=== [[Java_(programming_language)|Java]] ===\n* [[Google Web Toolkit]]\n\n=== [[Objective-C]] ===\n* AKABeacon iOS Data Binding framework\n\n=== [[Scala (programming language)|Scala]] ===\n* Binding.scala<ref>{{cite web |url=https://github.com/ThoughtWorksInc/Binding.scala|title=Binding.scala|accessdate=30 December 2016}}</ref> Reactive data-binding for Scala\n\n==See also==\n* [[Windows Presentation Foundation]]\n* [[XML data binding]]\n* [[UI data binding]]\n* [[Bound property]]\n\n==References==\n<references/>\n\n==Further reading==\n*{{cite book |last=Noyes |first=Brian |title=Data Binding with Windows Forms 2.0: Programming Smart Client Data Applications with .NET |url=https://books.google.com/books?id=RxptHgJ5W2cC |date=12 January 2006 |publisher=Pearson Education |isbn=978-0-321-63010-0}}\n\n{{DEFAULTSORT:Data Binding}}\n<!--Categories-->\n[[Category:Data management]]"]
['Database schema', '345937', 'A \'\'\'database schema\'\'\' ({{IPAc-en|ˈ|s|k|i|.|m|ə}} {{respell|SKEE|mə}}) of a [[database system]] is its structure described in a [[formal language]] supported by the [[database management system]] (DBMS). The term "schema" refers to the organization of data as a blueprint of how the database is constructed (divided into database tables in the case of [[relational databases]]). The formal definition of a [[database]] schema is a set of formulas (sentences) called [[integrity constraints]] imposed on a database.{{citation needed|date=January 2016}} These integrity constraints ensure compatibility between parts of the schema. All constraints are expressible in the same language. A database can be considered a structure in realization of the [[database language]].<ref name="source1" /> The states of a created [[conceptual schema]] are transformed into an [[Explicit and implicit methods|explicit mapping]], the database schema. This describes how real-world entities are modeled in the database.\n\n"A database schema specifies, based on the [[database administrator]]\'s knowledge of possible applications, the facts that can enter the database, or those of interest to the possible [[end-user]]s."<ref name="source3"/> The notion of a database schema plays the same role as the notion of theory in [[predicate calculus]]. A model of this "theory" closely corresponds to a database, which can be seen at any instant of time as a [[mathematical object]]. Thus a schema can contain formulas representing [[Data integrity#Types of integrity constraints|integrity constraints]] specifically for an application and the constraints specifically for a type of database, all expressed in the same database language.<ref name="source1" /> In a [[relational database]], the schema defines the [[Table (database)|tables]], [[Field (computer science)|fields]], [[Relational model|relationship]]s, [[View (database)|view]]s, [[Index (database)|index]]es, [[Software package (installation)|package]]s, [[stored procedure|procedure]]s, [[subroutine|function]]s, [[Queue (data structure)|queue]]s, [[Database trigger|trigger]]s, [[Data type|type]]s, [[sequence]]s, [[materialized view]]s, [[Synonym (database)|synonym]]s, [[database link]]s, [[Directory (file systems)|directories]], [[XML schema]]s, and other elements.\n\nA database generally stores its schema in a [[data dictionary]]. Although a schema is defined in text database language, the term is often used to refer to a graphical depiction of the database structure. In other words, schema is the structure of the database that defines the objects in the database.\n\nIn an [[Oracle Database]] system, the term "schema" has a slightly different [[connotation]].\n\n==Ideal requirements for schema integration==\n{{See also|Database normalization}}\n\nThe requirements listed below influence the detailed structure of schemas that are produced. Certain applications will not require that all of these conditions are met, but these four requirements are the most ideal.\n\n; Overlap preservation\n: Each of the overlapping elements specified in the input mapping is also in a database schema relation.<ref name="source2" />\n\n; Extended overlap preservation\n: Source-specific elements that are associated with a source’s overlapping elements are passed through to the database schema.<ref name="source2" />\n\n; Normalization\n: Independent entities and relationships in the source data should not be grouped together in the same relation in the database schema. In particular, source specific schema elements should not be grouped with overlapping schema elements, if the grouping co-locates independent entities or relationships.<ref name="source2" />\n\n; Minimality\n: If any elements of the database schema are dropped then the database schema is not ideal.<ref name="source2" />\n\n==Example of two schema integrations==\nSuppose we want a mediated (database) schema to integrate two travel databases, Go-travel and Ok-travel.\n\n\'\'\'<code>Go-travel</code>\'\'\' has two relations:\n<syntaxhighlight lang="text">\nGo-flight(f-num, time, meal(yes/no))\nGo-price(f-num, date, price)\n</syntaxhighlight>\n(<code>f-num</code> being the flight number)\n\n\'\'\'<code>Ok-travel</code>\'\'\' has just one relation:\n<syntaxhighlight lang="text">\nOk-flight(f-num, date, time, price, nonstop(yes/no))\n</syntaxhighlight>\n\nThe overlapping information in Ok-travel’s and Go-travel’s schemas could be represented in a mediated schema:<ref name="source2" />\n<syntaxhighlight lang="text">\nFlight(f-num, date, time, price)\n</syntaxhighlight>\n\n== Oracle database specificity ==\nIn the context of [[Oracle database]]s, a \'\'\'schema object\'\'\' is a logical [[Database storage structures|data storage structure]].<ref>\n{{cite book\n|first1= Lance |last1= Ashdown\n|first2= Tom  |last2= Kyte\n|others= \'\'et al\'\'.\n|title= Oracle Database Concepts 11g Release 2 (11.2)\n|url= http://download.oracle.com/docs/cd/E11882_01/server.112/e10713/tablecls.htm#CNCPT111\n|accessdate= 2010-04-14 |date=February 2010\n|publisher= Oracle Corporation\n|quote= A database schema is a logical container for data structures, called schema objects. Examples of schema objects are tables and indexes. \n}}\n</ref>\n\nAn Oracle database associates a separate schema with each database \'\'\'user\'\'\'.<ref>\n{{cite book\n|title= Oracle Database Concepts 10g Release 2 (10.2)Part Number B14220-02 \n|url= http://docs.oracle.com/cd/B19306_01/server.102/b14220/schema.htm\n|accessdate= 2012-11-26\n|quote= A schema is a collection of logical structures of data, or schema objects. A schema is owned by a database user and has the same name as that user. Each user owns a single schema. Schema objects can be created and manipulated with SQL. \n}}</ref>\nA schema comprises a collection of schema objects. Examples of schema objects include:\n\n* [[Table (database)|tables]]\n* [[View (database)|views]]\n* [[sequence]]s\n* [[Synonym (database)|synonyms]]\n* [[Index (database)|indexes]]\n* clusters\n* database links\n* [[Snapshot (computer storage)|snapshot]]s\n* [[stored procedure|procedure]]s\n* functions\n* packages\n\nOn the other hand, non-schema objects may include:<ref>{{cite book\n|first1= Lance\n|last1= Ashdown\n|author1-link=\n|first2= Tom\n|last2= Kyte\n|author2-link=\n|others= et al.\n|title= Oracle Database Concepts 11g Release 2 (11.2)\n|url= http://download.oracle.com/docs/cd/E11882_01/server.112/e10713/tablecls.htm#CNCPT111\n|accessdate= 2010-04-14\n|date=February 2010\n|publisher= Oracle Corporation\n|location=\n|isbn=\n|quote= Other types of objects are also stored in the database and can be created and manipulated with SQL statements but are not contained in a schema. These objects include database users, roles, contexts, and directory objects.\n}}</ref>\n\n* users\n* roles\n* contexts\n* directory objects\n\nSchema objects do not have a one-to-one correspondence to physical files on disk that store their information. However, [[Oracle database]]s store schema objects logically within a [[tablespace]] of the database. The data of each object is physically contained in one or more of the tablespace\'s [[datafile]]s. For some objects (such as tables, indexes, and clusters) a [[database administrator]] can specify how much disk space the Oracle [[RDBMS]] allocates for the object within the tablespace\'s datafiles.\n\nThere is no necessary relationship between schemas and tablespaces: a tablespace can contain objects from different schemas, and the objects for a single schema can reside in different tablespaces.\n\n== See also ==\n{{too many see alsos|date=July 2013}}\n* [[Core architecture data model]] (CADM)\n* [[Data definition language]] (DDL)\n* [[Database design]]\n* [[Data dictionary]]\n* [[Data element]]\n* [[Data modeling]]\n* [[Data mapping]]\n* [[Database integrity]]\n* [[Entity–relationship model]]\n* [[Knowledge representation and reasoning]]\n* [[Object-role modeling]]\n* [[Relational algebra]]\n* [[Schema matching]]\n* [[Three schema approach]]\n\n==References==\n{{reflist|refs=\n<ref name="source1">{{cite journal |last=Rybinski |first=H. |year=1987|title=On First-Order-Logic Databases |journal=ACM Transactions on Database Systems |volume=12 |issue=3 |pages=325–349 |doi= 10.1145/27629.27630}}</ref>\n<ref name="source2">{{cite journal |last= Pottinger |first=P. |last2=Berstein |first2=P. |year=2008 |title= Schema merging and mapping creation for relational sources |journal= Proceedings of the 11th international conference on Extending database technology: Advances in database technology (EDBT \'08) |publisher=ACM |location= New York, NY |pages=73–84 |doi= 10.1145/1353343.1353357}}</ref>\n<ref name="source3">{{cite journal |last= Imielinski |first=T. |last2=Lipski |first2=W. |year=1982 |title=A systematic approach to relational database theory |journal= Proceedings of the 1982 ACM SIGMOD international conference on Management of data (SIGMOD \'82) |publisher=ACM |location= New York, NY |pages=8–14 |DOI= 10.1145/582353.582356}}</ref>\n}}\n\n== External links ==\n* [https://weblogs.asp.net/scottgu/Tip_2F00_Trick_3A00_-Online-Database-Schema-Samples-Library Tip/Trick: Online Database Schema Samples Library]\n* [http://web.archive.org/web/20081217074637/http://msdn.microsoft.com/en-us/library/bb187299%28SQL.80%29.aspx Database Schema Samples]\n* [http://web.archive.org/web/20080828210315/http://ciobriefings.com/Publications/WhitePapers/DesigningtheStarSchemaDatabase/tabid/101/Default.aspx Designing the Star Schema Database]\n\n{{DEFAULTSORT:Database Schema}}\n[[Category:Data management]]\n[[Category:Data modeling]]']
['Commitment ordering', '4379212', '{{multiple issues|\n{{expert subject|computer science|date=October 2012|reason=it is impossible to copy edit the article in its current state}}\n{{notability|date=December 2011}}\n{{more footnotes|date=November 2011}}\n{{technical|date=November 2011}}\n{{essay-like|date=November 2011}}\n}}\n\n\'\'\'Commitment ordering\'\'\' (\'\'\'CO\'\'\') is a class of interoperable \'\'[[serializability]]\'\' techniques in [[concurrency control]] of [[database]]s, [[transaction processing]], and related applications. It allows [[Serializability#Optimistic versus pessimistic techniques|optimistic]] (non-blocking) implementations. With the proliferation of [[multi-core processor]]s, CO has been also increasingly utilized in [[Concurrent computing|concurrent programming]], [[transactional memory]], and especially in [[software transactional memory]] (STM) for achieving serializability [[Optimistic concurrency control|optimistically]]. CO is also the name of the resulting transaction [[Schedule (computer science)|schedule]] (history) property, which was originally defined in 1988 with the name \'\'dynamic atomicity\'\'.<ref name=Fekete1988>Alan Fekete, [[Nancy Lynch]], Michael Merritt, William Weihl (1988): [http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA200980&Location=U2&doc=GetTRDoc.pdf \'\'Commutativity-based locking for nested transactions\'\' (PDF)] MIT, LCS lab, Technical report MIT/LCS/TM-370, August 1988.</ref> In a CO compliant schedule the chronological order of commitment events of transactions is compatible with the [[Serializability#Testing conflict serializability|precedence]] order of the respective transactions. CO is a broad special case of \'\'[[serializability#View and conflict serializability|conflict serializability]]\'\', and effective means ([[Reliability engineering|reliable]], high-performance, distributed, and [[Scalability|scalable]]) to achieve [[global serializability]] (modular serializability) across any collection of database systems that possibly use different concurrency control mechanisms (CO also makes each system serializability compliant, if not already).\n\nEach not-CO-compliant database system is augmented with a CO component (the commitment order coordinator—COCO) which orders the commitment events for CO compliance, with neither data-access nor any other transaction operation interference. As such CO provides a low overhead, general solution for global serializability (and distributed serializability), instrumental for [[global concurrency control]] (and [[distributed concurrency control]]) of multi database systems and other [[transactional object]]s, possibly highly distributed (e.g., within [[cloud computing]], [[grid computing]], and networks of [[smartphone]]s). An [[atomic commitment protocol]] (ACP; of any type) is a fundamental part of the solution, utilized to break global cycles in the conflict (precedence, serializability) graph. CO is the most general property (a [[necessary condition]]) that guarantees global serializability, if the database systems involved do not share concurrency control information beyond atomic commitment protocol (unmodified) messages, and have no knowledge whether transactions are global or local (the database systems are \'\'autonomous\'\'). Thus CO (with its variants) is the only general technique that does not require the typically costly distribution of local concurrency control information (e.g., local precedence relations, locks, timestamps, or tickets). It generalizes the popular \'\'[[Two-phase locking|strong strict two-phase locking]]\'\' (SS2PL) property, which in conjunction with the \'\'[[two-phase commit protocol]]\'\' (2PC) is the [[de facto standard]] to achieve global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join such SS2PL based solutions for global serializability.\n\nIn addition, locking based \'\'global deadlocks\'\' are resolved automatically in a CO based multi-database environment, an important side-benefit (including the special case of a completely SS2PL based environment; a previously unnoticed fact for SS2PL).\n\nFurthermore, \'\'\'strict commitment ordering\'\'\' (SCO; [[#Raz1991c|Raz 1991c]]), the intersection of \'\'[[Schedule (computer science)#Strict|Strictness]]\'\' and CO, provides better performance (shorter average transaction completion time and resulting better transaction [[throughput]]) than SS2PL whenever read-write conflicts are present (identical blocking behavior for write-read and write-write conflicts; comparable locking overhead). The advantage of SCO is especially significant during lock contention. Strictness allows both SS2PL and SCO to use the same effective \'\'database recovery\'\' mechanisms.\n\nTwo major generalizing variants of CO exist, \'\'\'extended CO\'\'\' (ECO; [[#Raz1993a|Raz 1993a]]) and \'\'\'multi-version CO\'\'\' (MVCO; [[#Raz1993b|Raz 1993b]]). They as well provide global serializability without local concurrency control information distribution, can be combined with any relevant concurrency control, and allow optimistic (non-blocking) implementations. Both use additional information for relaxing CO constraints and achieving better concurrency and performance. \'\'\'Vote ordering\'\'\' (VO or Generalized CO (GCO); [[#Raz2009|Raz 2009]]) is a container schedule set (property) and technique for CO and all its variants. Local VO is a necessary condition for guaranteeing global serializability, if the atomic commitment protocol (ACP) participants do not share concurrency control information (have the \'\'generalized autonomy\'\' property). CO and its variants inter-operate transparently, guaranteeing global serializability and automatic global deadlock resolution also together in a mixed, heterogeneous environment with different variants.\n\n==Overview==\n\nThe \'\'Commitment ordering\'\' (CO; [[#Raz1990|Raz 1990]], [[#Raz1992|1992]], [[#Raz1994|1994]], [[#Raz2009|2009]]) schedule property has been referred to also as \'\'Dynamic atomicity\'\' (since 1988<ref name=Fekete1988/>), \'\'commit ordering\'\', \'\'commit order serializability\'\',  and \'\'strong recoverability\'\' (since 1991). The latter is a misleading name since CO is incomparable with \'\'[[serializability#Correctness - recoverability|recoverability]]\'\', and the term "strong" implies a special case. This means that a schedule with a strong recoverability property does not necessarily have the CO property, and vice versa.\n\nIn 2009 CO has been characterized as a major concurrency control method, together with the previously known (since the 1980s) three major methods: \'\'Locking\'\', \'\'Time-stamp ordering\'\', and \'\'Serialization graph testing\'\', and as an enabler for the interoperability of systems using different concurrency control mechanisms.<ref name=Bern2009>[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 \'\'Principles of Transaction Processing\'\', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (pages 145, 360)</ref>\n\nIn a [[federated database system]] or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple and possibly [[Distributed database]]s. Enforcing [[global serializability]] in such system is problematic. Even if every local schedule of a single database is serializable, still, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach conflict serializability would lead to unacceptable performance, primarily due to computer and communication [[latency (engineering)|latency]]. The problem of achieving global serializability effectively had been characterized as [[open problem|open]] until the public disclosure of CO in 1991 by its [[Invention|inventor]] [[Yoav Raz]] ([[#Raz1991a|Raz 1991a]]; see also [[Global serializability]]).\n\nEnforcing CO is an effective way to enforce conflict serializability globally in a distributed system, since enforcing CO locally in each database (or other transactional object) also enforces it globally. Each database may use any, possibly different, type of concurrency control mechanism. With a local mechanism that already provides conflict serializability, enforcing CO locally does not cause any additional aborts, since enforcing CO locally does not affect the data access scheduling strategy of the mechanism (this scheduling determines the serializability related aborts; such a mechanism typically does not consider the commitment events or their order). The CO solution requires no communication overhead, since it uses (unmodified) \'\'[[atomic commitment]]\'\' protocol messages only, already needed by each distributed transaction to reach atomicity. An atomic commitment protocol plays a central role in the distributed CO algorithm, which enforces CO globally, by breaking global cycles (cycles that span two or more databases) in the global conflict graph.\nCO, its special cases, and its generalizations are interoperable, and achieve global serializability while transparently being utilized together in a single heterogeneous distributed environment comprising objects with possibly different concurrency control mechanisms. As such, \'\'Commitment ordering\'\', including its special cases, and together with its generalizations (see CO variants below), provides a general, high performance, fully distributed solution (no central processing component or central data structure are needed) for guaranteeing global serializability in heterogeneous environments of multidatabase systems and other multiple transactional objects (objects with states accessed and modified only by transactions; e.g., in the framework of [[transactional processes]], and within Cloud computing and Grid computing). The CO solution scales up with network size and the number of databases without any negative impact on performance (assuming the statistics of a single distributed transaction, e.g., the average number of databases involved with a single transaction, are unchanged).\n\nWith the proliferation of [[Multi-core processor]]s, Optimistic CO (OCO) has been also increasingly utilized to achieve serializability in software transactional memory, and numerous STM articles and patents utilizing "commit order" have already been published (e.g., Zhang et al. 2006<ref name=Zhang2006/>).\n\n==The commitment ordering solution for global serializability==\n\n===General characterization of CO===\n\n\'\'Commitment ordering\'\' (CO) is a special case of conflict serializability. CO can be enforced with \'\'non-blocking\'\' mechanisms (each transaction can complete its task without having its data-access blocked, which allows [[optimistic concurrency control]]; however, commitment could be blocked). In a CO schedule the commitment events\' ([[partial order|partial]]) precedence order of the transactions corresponds to the precedence (partial) order of the respective transactions in the ([[directed graph|directed]]) conflict graph (precedence graph, serializability graph), as induced by their conflicting access operations (usually read and write (insert/modify/delete) operations; CO also applies to higher level operations, where they are conflicting if [[noncommutative]], as well as to conflicts between operations upon multi-version data).\n\n;Definition{{colon}} commitment ordering: Let <math>T_{1}, T_{2}</math> be two \'\'committed\'\' transactions in a schedule, such that <math>T_{2}</math> is \'\'in a conflict\'\' with <math>T_{1}</math> (<math>T_{1}</math> \'\'precedes\'\' <math>T_{2}</math>). The schedule has the \'\'\'Commitment ordering\'\'\' (CO) property, if for every two such transactions <math>T_{1}</math> commits before <math>T_{2}</math> commits.\n\nThe commitment decision events are generated by either a local commitment mechanism, or an atomic commitment protocol, if different processes need to reach consensus on whether to commit or abort. The protocol may be distributed or centralized. Transactions may be committed concurrently, if the commit partial order allows (if they do not have conflicting operations). If different conflicting operations induce different partial orders of same transactions, then the conflict graph has [[cycle (graph theory)|cycles]], and the schedule will violate serializability when all the transactions on a cycle are committed. In this case no partial order for commitment events can be found. Thus, cycles in the conflict graph need to be broken by aborting transactions. However, any conflict serializable schedule can be made CO without aborting any transaction, by properly delaying commit events to comply with the transactions\' precedence partial order.\n\nCO enforcement by itself is not sufficient as a concurrency control mechanism, since CO lacks the recoverability property, which should be supported as well.\n\n===The distributed CO algorithm===\n\nA fully distributed \'\'Global commitment ordering\'\' enforcement algorithm exists, that uses local CO of each participating database, and needs only (unmodified) Atomic commitment protocol messages with no further communication. The distributed algorithm is the combination of local (to each database) CO algorithm processes, and an atomic commitment protocol (which can be fully distributed).\nAtomic commitment protocol is essential to enforce atomicity of each distributed transaction (to decide whether to commit or abort it; this procedure is always carried out for distributed transactions, independently of concurrency control and CO). A common example of an atomic commitment protocol is the \'\'[[two-phase commit protocol]]\'\', which is resilient to many types of system failure. In a reliable environment, or when processes usually fail together (e.g., in the same [[integrated circuit]]), a simpler protocol for atomic commitment may be used (e.g., a simple handshake of distributed transaction\'s participating processes with some arbitrary but known special participant, the transaction\'s coordinator, i.e., a type of \'\'one-phase commit\'\' protocol). An atomic commitment protocol reaches consensus among participants on whether to \'\'commit\'\' or \'\'abort\'\' a distributed (global) transaction that spans these participants. An essential stage in each such protocol is the \'\'\'YES vote\'\'\' (either explicit, or implicit) by each participant, which means an obligation of the voting participant to obey the decision of the protocol, either commit or abort. Otherwise a participant can unilaterally abort the transaction by an explicit NO vote. The protocol commits the transaction only if YES votes have been received from \'\'all\'\' participants, and thus typically a missing YES vote of a participant is considered a NO vote by this participant. Otherwise the protocol aborts the transaction. The various atomic commit protocols only differ in their abilities to handle different computing environment failure situations, and the amounts of work and other computing resources needed in different situations.\n\nThe entire CO solution for global serializability is based on the fact that in case of a missing vote for a distributed transaction, the atomic commitment protocol eventually aborts this transaction.\n\n====Enforcing global CO====\n\nIn each database system a local CO algorithm determines the needed commitment order for that database. By the characterization of CO above, this order depends on the local precedence order of transactions, which results from the local data access scheduling mechanisms. Accordingly, YES votes in the atomic commitment protocol are scheduled for each (unaborted) distributed transaction (in what follows "a vote" means a YES vote). If a precedence relation (conflict) exists between two transactions, then the second will not be voted on before the first is completed (either committed or aborted), to prevent possible commit order violation by the atomic commitment protocol. Such can happen since the commit order by the protocol is not necessarily the same as the voting order. If no precedence relation exists, both can be voted on concurrently. This \'\'vote ordering strategy\'\' ensures that also the atomic commitment protocol maintains commitment order, and it is a \'\'necessary condition\'\' for guaranteeing Global CO (and the local CO of a database; without it both Global CO and Local CO (a property meaning that each database is CO compliant) may be violated).\n\nHowever, since database systems schedule their transactions independently, it is possible that the transactions\' precedence orders in two databases or more are not compatible (no global partial order exists that can [[Embedding|embed]] the respective local partial orders together). With CO precedence orders are also the commitment orders. When participating databases in a same distributed transaction do not have compatible local precedence orders for that transaction (without "knowing" it; typically no coordination between database systems exists on conflicts, since the needed communication is massive and unacceptably degrades performance) it means that the transaction resides on a global cycle (involving two or more databases) in the global conflict graph. In this case the atomic commitment protocol will fail to collect all the votes needed to commit that transaction: By the \'\'vote ordering strategy\'\' above at least one database will delay its vote for that transaction indefinitely, to comply with its own commitment (precedence) order, since it will be waiting to the completion of another, preceding transaction on that global cycle, delayed indefinitely by another database with a different order. This means a \'\'\'\'\'voting-[[deadlock]]\'\'\'\'\' situation involving the databases on that cycle.\nAs a result, the protocol will eventually abort some deadlocked transaction on this global cycle, since each such transaction is missing at least one participant\'s vote. Selection of the specific transaction on the cycle to be aborted depends on the atomic commitment protocol\'s abort policies (a [[timeout (telecommunication)|timeout]] mechanism is common, but it may result in more than one needed abort per cycle; both preventing unnecessary aborts and abort time shortening can be achieved by a dedicated abort mechanism for CO). Such abort will break the global cycle involving that distributed transaction. Both deadlocked transactions and possibly other in conflict with the deadlocked (and thus blocked) will be free to be voted on. It is worthwhile noting that each database involved with the voting-deadlock continues to vote regularly on transactions that are not in conflict with its deadlocked transaction, typically almost all the outstanding transactions. Thus, in case of incompatible local (partial) commitment orders, no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause of incompatibility. This means that the above \'\'vote ordering strategy\'\' is also a \'\'sufficient condition\'\' for guaranteeing Global CO.\n\nThe following is concluded:\n\n*\'\'\'The Vote ordering strategy for Global CO Enforcing [[Theorem]]\'\'\'\n\n:Let <math>T_{1}, T_{2}</math> be undecided (neither committed nor aborted) transactions in a database system that enforces CO for local transactions, such that <math>T_{2}</math> is \'\'global\'\' and \'\'in conflict\'\' with <math>T_{1}</math> (<math>T_{1}</math> \'\'precedes\'\' <math>T_{2}</math>). Then, having <math>T_{1}</math> ended (either committed or aborted) before <math>T_{2}</math> is voted on to be committed (the \'\'vote ordering strategy\'\'), in each such database system in a multidatabase environment, is a [[necessary and sufficient condition]] for guaranteeing Global CO (the condition guarantees Global CO, which may be violated without it).\n\n:\'\'\'Comments:\'\'\'\n# The \'\'vote ordering strategy\'\' that enforces global CO is referred to as <math>CD^3C</math> in ([[#Raz1992|Raz 1992]]).\n#The Local CO property of a global schedule means that each database is CO compliant. From the necessity discussion part above it directly follows that the theorem is true also when replacing "Global CO" with "Local CO" when global transactions are present. Together it means that Global CO is guaranteed [[if and only if]] Local CO is guaranteed (which is untrue for Global conflict serializability and Local conflict serializability: Global implies Local, but not the opposite).\n\nGlobal CO implies Global serializability.\n\nThe \'\'\'Global CO algorithm\'\'\' comprises enforcing (local) CO in each participating database system by ordering commits of local transactions (see [[Commitment ordering#Enforcing CO locally|Enforcing CO locally]] below) and enforcing the \'\'vote ordering strategy\'\' in the theorem above (for global transactions).\n\n====Exact characterization of voting-deadlocks by global cycles====\n\nThe above global cycle elimination process by a \'\'\'voting deadlock\'\'\' can be explained in detail by the following observation:\n\nFirst it is assumed, for simplicity, that every transaction reaches the ready-to-commit state and is voted on by at least one database (this implies that no blocking by locks occurs).\nDefine a \'\'"wait for vote to commit" graph\'\' as a directed graph with transactions as nodes, and a directed edge from any first transaction to a second transaction if the first transaction blocks the vote to commit of the second transaction (opposite to conventional edge direction in a [[wait-for graph]]). Such blocking happens only if the second transaction is in a conflict with the first transaction (see above). Thus this "wait for vote to commit" graph is identical to the global conflict graph. A cycle in the "wait for vote to commit" graph means a deadlock in voting. Hence there is a deadlock in voting if and only if there is a cycle in the conflict graph. Local cycles (confined to a single database) are eliminated by the local serializability mechanisms. Consequently, only global cycles are left, which are then eliminated by the atomic commitment protocol when it aborts deadlocked transactions with missing (blocked) respective votes.\n\nSecondly, also local commits are dealt with: Note that when enforcing CO also waiting for a regular local commit of a local transaction can block local commits and votes of other transactions upon conflicts, and the situation for global transactions does not change also without the simplifying assumption above: The final result is the same also with local commitment for local transactions, without voting in atomic commitment for them.\n\nFinally, blocking by a lock (which has been excluded so far) needs to be considered: A lock blocks a conflicting operation and prevents a conflict from being materialized. If the lock is released only after transaction end, it may block indirectly either a vote or a local commit of another transaction (which now cannot get to ready state), with the same effect as of a direct blocking of a vote or a local commit. In this case a cycle is generated in the conflict graph only if such a blocking by a lock is also represented by an edge. With such added edges representing events of blocking-by-a-lock, the conflict graph is becoming an \'\'augmented conflict graph\'\'.\n\n*\'\'\'Definition: augmented conflict graph\'\'\'\n\n:An \'\'\'augmented conflict graph\'\'\' is a [[serializability#Testing conflict serializability|conflict graph]] with added edges: In addition to the original edges a directed edge exists from transaction <math>T_{1}</math> to transaction <math>T_{2}</math> if two conditions are met:\n# <math>T_{2}</math> is blocked by a data-access lock applied by <math>T_{1}</math> (the blocking prevents the conflict of <math>T_{2}</math> with <math>T_{1}</math> from being materialized and have an edge in the regular conflict graph), and\n# This blocking will not stop before <math>T_{1}</math> ends (commits or aborts; true for any locking-based CO)\n\n:The graph can also be defined as the [[Union (set theory)|union]] of the (regular) \'\'conflict graph\'\' with the (reversed edge, regular) \'\'wait-for graph\'\'\n\n:\'\'\'Comments:\'\'\'\n# Here, unlike the regular conflict graph, which has edges only for materialized conflicts, all conflicts, both materialized and non-materialized, are represented by edges.\n# Note that all the new edges are all the (reversed to the conventional) edges of the \'\'wait-for graph\'\'. The \'\'wait-for graph\'\' can be defined also as the graph of non-materialized conflicts. By the common conventions edge direction in a \'\'conflict graph\'\' defines time order between conflicting operations which is opposite to the time order defined by an edge in a \'\'wait-for graph\'\'.\n# Note that such global graph contains (has embedded) all the (reversed edge) regular local \'\'wait-for\'\' graphs, and also may include locking based global cycles (which cannot exist in the local graphs). For example, if all the databases on a global cycle are SS2PL based, then all the related vote blocking situations are caused by locks (this is the classical, and probably the only global deadlock situation dealt with in the database research literature). This is a global deadlock case where each related database creates a portion of the cycle, but the complete cycle does not reside in any local wait-for graph.\n\nIn the presence of CO the \'\'augmented conflict graph\'\' is in fact a (reversed edge) \'\'local-commit and voting wait-for graph\'\': An edge exists from a first transaction, either local or global, to a second, if the second is waiting for the first to end in order to be either voted on (if global), or locally committed (if local). All \'\'global cycles\'\' (across two or more databases) in this graph generate voting-deadlocks. The graph\'s global cycles provide complete characterization for voting deadlocks and may include any combination of materialized and non-materialized conflicts. Only cycles of (only) materialized conflicts are also cycles of the regular conflict graph and affect serializability. One or more (lock related) non-materialized conflicts on a cycle prevent it from being a cycle in the regular conflict graph, and make it a locking related deadlock. All the global cycles (voting-deadlocks) need to be broken (resolved) to both maintain global serializability and resolve global deadlocks involving data access locking, and indeed they are all broken by the atomic commitment protocol due to missing votes upon a voting deadlock.\n\n\'\'\'Comment:\'\'\' This observation also explains the correctness of \'\'[[Commitment ordering#Extended CO (ECO)|Extended CO (ECO)]]\'\' below: Global transactions\' voting order must follow the conflict graph order with vote blocking when order relation (graph path) exists between two global transactions. Local transactions are not voted on, and their (local) commits are not blocked upon conflicts. This results in same voting-deadlock situations and resulting global cycle elimination process for ECO.\n\nThe \'\'voting-deadlock\'\' situation can be summarized as follows:\n\n*\'\'\'The CO Voting-Deadlock Theorem\'\'\'\n\n:Let a multidatabase environment comprise CO compliant (which eliminates \'\'local cycles\'\') database systems that enforce, each, \'\'Global CO\'\' (using the condition in the theorem above). Then a \'\'voting-deadlock\'\' occurs if and only if a \'\'global cycle\'\' (spans two or more databases) exists in the \'\'Global augmented conflict graph\'\' (also blocking by a data-access lock is represented by an edge). If the cycle does not break by any abort, then all the \'\'global transactions\'\' on it are involved with the respective voting-deadlock, and eventually each has its vote blocked (either directly, or indirectly by a data-access lock); if a local transaction resides on the cycle, eventually it has its (local) commit blocked.\n\n:\'\'\'Comment:\'\'\' A rare situation of a voting deadlock (by missing blocked votes) can happen, with no voting for any transaction on the related cycle by any of the database systems involved with these transactions. This can occur when local sub-transactions are [[Thread (computer science)|multi-threaded]]. The highest probability instance of such rare event involves two transactions on two simultaneous opposite cycles. Such global cycles (deadlocks) overlap with local cycles which are resolved locally, and thus typically resolved by local mechanisms without involving atomic commitment. Formally it is also a global cycle, but practically it is local (portions of local cycles generate a global one; to see this, split each global transaction (node) to local sub-transactions (its portions confined each to a single database); a directed edge exists between transactions if an edge exists between any respective local sub-transactions; a cycle is local if all its edges originate from a cycle among sub-transactions of the same database, and global if not; global and local can overlap: a same cycle among transactions can result from several different cycles among sub-transactions, and be both local and global).\n\nAlso the following locking based special case is concluded:\n\n*\'\'\'The CO Locking-based Global-Deadlock Theorem\'\'\'\n\n:In a CO compliant multidatabase system a locking-based global-deadlock, involving at least one data-access lock (non-materialized conflict), and two or more database systems, is a reflection of a global cycle in the \'\'Global augmented conflict graph\'\', which results in a voting-deadlock. Such cycle is not a cycle in the (regular) \'\'Global conflict graph\'\' (which reflects only materialized conflicts, and thus such cycle does not affect \'\'[[serializability]]\'\').\n\n:\'\'\'Comments:\'\'\'\n# Any blocking (edge) in the cycle that is not by a data-access lock is a direct blocking of either voting or local commit. All voting-deadlocks are resolved (almost all by \'\'Atomic commitment\'\'; see comment above), including this locking-based type.\n# Locking-based global-deadlocks can be generated also in a completely SS2PL-based distributed environment (special case of CO based), where all the vote blocking (and voting-deadlocks) are caused by data-access locks. Many research articles have dealt for years with resolving such global deadlocks, but none (except the CO articles) is known (as of 2009) to notice that \'\'atomic commitment\'\' automatically resolves them. Such automatic resolutions are regularly occurring unnoticed in all existing SS2PL based multidatabase systems, often bypassing dedicated resolution mechanisms.\n\nVoting-deadlocks are the key for the operation of distributed CO.\n\nGlobal cycle elimination (here voting-deadlock resolution by \'\'atomic commitment\'\') and resulting aborted transactions\' re-executions are time consuming, regardless of concurrency control used. If databases schedule transactions independently, global cycles are unavoidable (in a complete analogy to cycles/deadlocks generated in local SS2PL; with distribution, any transaction or operation scheduling coordination results in autonomy violation, and typically also in substantial performance penalty). However, in many cases their likelihood can be made very low by implementing database and transaction design guidelines that reduce the number of conflicts involving a global transaction. This, primarily by properly handling hot spots (database objects with frequent access), and avoiding conflicts by using commutativity when possible (e.g., when extensively using counters, as in finances, and especially multi-transaction \'\'accumulation counters\'\', which are typically hot spots).\n\nAtomic commitment protocols are intended and designed to achieve atomicity without considering database concurrency control. They abort upon detecting or [[Heuristic algorithm|heuristically]] finding (e.g., by timeout; sometimes mistakenly, unnecessarily) missing votes, and typically unaware of global cycles. These protocols can be specially enhanced for CO (including CO\'s variants below) both to prevent unnecessary aborts, and to accelerate aborts used for breaking global cycles in the global augmented conflict graph (for better performance by earlier release upon transaction-end of computing resources and typically locked data). For example, existing locking based global deadlock detection methods, other than timeout, can be generalized to consider also local commit and vote direct blocking, besides data access blocking. A possible compromise in such mechanisms is effectively detecting and breaking the most frequent and relatively simple to handle length-2 global cycles, and using timeout for undetected, much less frequent, longer cycles.\n\n===Enforcing CO locally===\n\n\'\'Commitment ordering\'\' can be enforced locally (in a single database) by a dedicated CO algorithm, or by any algorithm/protocol that provides any special case of CO. An important such protocol, being utilized extensively in database systems, which generates a CO schedule, is the \'\'strong strict [[two phase locking]]\'\' protocol (SS2PL: "release transaction\'s locks only after the transaction has been either committed or aborted"; see below). SS2PL is a [[proper subset]] of the intersection of [[Two-phase locking|2PL]] and strictness.\n\n====A generic local CO algorithm====\n\nA \'\'\'generic local CO algorithm\'\'\' ([[#Raz1992|Raz 1992]]; Algorithm 4.1) is an algorithm independent of implementation details, that enforces exactly the CO property. It does not block data access (nonblocking), and consists of aborting a certain set of transactions (only if needed) upon committing a transaction. It aborts a (uniquely determined at any given time) minimal set of other undecided (neither committed, nor aborted) transactions that run locally and can cause serializability violation in the future (can later generate cycles of committed transactions in the conflict graph; this is the ABORT set of a committed transaction T; after committing T no transaction in ABORT at commit time can be committed, and all of them are doomed to be aborted). This set consists of all undecided transactions with directed edges in the conflict graph to the committed transaction. The size of this set cannot increase when that transaction is waiting to be committed (in ready state: processing has ended), and typically decreases in time as its transactions are being decided. Thus, unless [[Real-time computing|real-time]] constraints exist to complete that transaction, it is preferred to wait with committing that transaction and let this set decrease in size. If another serializability mechanism exists locally (which eliminates cycles in the local conflict graph), or if no cycle involving that transaction exists, the set will be empty eventually, and no abort of set member is needed. Otherwise the set will stabilize with transactions on local cycles, and aborting set members will have to occur to break the cycles. Since in the case of CO conflicts generate blocking on commit, local cycles in the \'\'augments conflict graph\'\' (see above) indicate local commit-deadlocks, and deadlock resolution techniques as in [[Serializability#Common mechanism - SS2PL|SS2PL]] can be used (e.g., like \'\'timeout\'\' and \'\'wait-for graph\'\'). A local cycle in the \'\'augmented conflict graph\'\' with at least one non-materialized conflict reflects a locking-based deadlock. The local algorithm above, applied to the local augmented conflict graph rather than the regular local conflict graph, comprises the \'\'\'generic enhanced local CO algorithm\'\'\', a single local cycle elimination mechanism, for both guaranteeing local serializability and handling locking based local deadlocks. Practically an additional concurrency control mechanism is always utilized, even solely to enforce recoverability. The generic CO algorithm does not affect local data access scheduling strategy, when it runs alongside of any other local concurrency control mechanism. It affects only the commit order, and for this reason it does not need to abort more transactions than those needed to be aborted for serializability violation prevention by any combined local concurrency control mechanism. The net effect of CO may be, at most, a delay of commit events (or voting in a distributed environment), to comply with the needed commit order (but not more delay than its special cases, for example, SS2PL, and on the average significantly less).\n\nThe following theorem is concluded:\n\n*\'\'\'The Generic Local CO Algorithm Theorem\'\'\'\n:When running alone or alongside any concurrency control mechanism in a database system then\n#The \'\'Generic local CO algorithm\'\' guarantees (local) CO (a CO compliant schedule).\n#The \'\'Generic enhanced local CO algorithm\'\' guarantees both (local) CO and (local) locking based deadlock resolution.\n: and (when not using \'\'timeout\'\', and no \'\'real-time\'\' transaction completion constraints are applied) neither algorithm aborts more transactions than the minimum needed (which is determined by the transactions\' operations scheduling, out of the scope of the algorithms).\n\n====Example: Concurrent programming and Transactional memory====\n:See also \'\'[[The History of Commitment Ordering#Concurrent programming and Transactional memory|Concurrent programming and Transactional memory]]\'\'\n\nWith the proliferation of Multi-core processors, variants of the Generic local CO algorithm have been also increasingly utilized in Concurrent programming, [[Transactional memory]], and especially in Software transactional memory for achieving serializability optimistically by "commit order" (e.g., Ramadan et al. 2009,<ref name=Ramadan2009>Hany E. Ramadan, Indrajit Roy, Maurice Herlihy, Emmett Witchel (2009): [http://portal.acm.org/citation.cfm?id=1504201 "Committing conflicting transactions in an STM"] ([http://www.cs.utexas.edu/~indrajit/pubs/ppopp121-ramadan.pdf PDF]) \'\'Proceedings of the 14th ACM SIGPLAN symposium on Principles and practice of parallel programming\'\' (PPoPP \'09), ISBN 978-1-60558-397-6</ref> Zhang et al. 2006,<ref name=Zhang2006>Lingli Zhang, Vinod K.Grover, Michael M. Magruder, David Detlefs, John Joseph Duffy, Goetz Graefe (2006): [http://www.freepatentsonline.com/7711678.html  Software transaction commit order and conflict management] United States Patent 7711678, Granted 05/04/2010.</ref> von Parun et al. 2007<ref name=vonParun2007>Christoph von Praun, Luis Ceze, Calin Cascaval (2007) [http://portal.acm.org/citation.cfm?id=1229443 "Implicit Parallelism with Ordered Transactions"] ([http://www.cs.washington.edu/homes/luisceze/publications/ipot_ppopp07.pdf PDF]), \'\'Proceedings of the 12th ACM SIGPLAN symposium on Principles and practice of parallel programming\'\' (PPoPP \'07), ACM New York ©2007, ISBN 978-1-59593-602-8 doi 10.1145/1229428.1229443</ref>). Numerous related articles and patents utilizing CO have already been published.\n\n====Implementation considerations: The Commitment Order Coordinator (COCO)====\n\nA database system in a multidatabase environment is assumed. From a [[software architecture]] point of view a CO component that implements the generic CO algorithm locally, the \'\'Commitment Order Coordinator\'\' (COCO), can be designed in a straightforward way as a [[mediator pattern|mediator]] between a (single) database system and an atomic commitment protocol component ([[#Raz1991b|Raz 1991b]]). However, the COCO is typically an integral part of the database system. The COCO\'s functions are to vote to commit on ready global transactions (processing has ended) according to the local commitment order, to vote to abort on transactions for which the database system has initiated an abort (the database system can initiate abort for any transaction, for many reasons), and to pass the atomic commitment decision to the database system. For local transactions (when can be identified) no voting is needed. For determining the commitment order the COCO maintains an updated representation of the local conflict graph (or local augmented conflict graph for capturing also locking deadlocks) of the undecided (neither committed nor aborted) transactions as a data structure (e.g., utilizing mechanisms similar to [[lock (computer science)|locking]] for capturing conflicts, but with no data-access blocking). The COCO component has an [[interface (computer science)|interface]] with its database system to receive "conflict," "ready" (processing has ended; readiness to vote on a global transaction or commit a local one), and "abort" notifications from the database system. It also interfaces with the atomic commitment protocol to vote and to receive the atomic commitment protocol\'s decision on each global transaction. The decisions are delivered from the COCO to the database system through their interface, as well as local transactions\' commit notifications, at a proper commit order. The COCO, including its interfaces, can be enhanced, if it implements another variant of CO (see below), or plays a role in the database\'s concurrency control mechanism beyond voting in atomic commitment.\n\nThe COCO also guarantees CO locally in a single, isolated database system with no interface with an atomic commitment protocol.\n\n===CO is a necessary condition for global serializability across autonomous database systems===\n\nIf the databases that participate in distributed transactions (i.e., transactions that span more than a single database) do not use any shared concurrency control information and use unmodified atomic commitment protocol messages (for reaching atomicity), then maintaining (local) \'\'commitment ordering\'\' or one of its generalizing variants (see below) is a [[necessary condition]] for guaranteeing global serializability (a proof technique can be found in ([[#Raz1992|Raz 1992]]), and a different proof method for this in ([[#Raz1993a|Raz 1993a]])); it is also a [[sufficient condition]]. This is a mathematical fact derived from the definitions of \'\'serializability\'\' and a \'\'[[Database transaction|transaction]]\'\'. It means that if not complying with CO, then global serializability cannot be guaranteed under this condition (the condition of no local concurrency control information sharing between databases beyond atomic commit protocol messages). Atomic commitment is a minimal requirement for a distributed transaction since it is always needed, which is implied by the definition of transaction.\n\n([[#Raz1992|Raz 1992]]) defines \'\'database autonomy\'\' and \'\'independence\'\' as complying with this requirement without using any additional local knowledge:\n*\'\'\'Definition:\'\'\' (concurrency control based) \'\'\'autonomous database system\'\'\'\n:A database system is \'\'\'Autonomous\'\'\', if it does not share with any other entity any concurrency control information beyond unmodified [[atomic commitment protocol]] messages. In addition it does not use for concurrency control any additional local information beyond conflicts (the last sentence does not appear explicitly but rather implied by further discussion in [[#Raz1992|Raz 1992]]).\n\nUsing this definition the following is concluded:\n\n*\'\'\'The CO and Global serializability Theorem\'\'\'\n\n#CO compliance of every \'\'autonomous\'\' database system (or transactional object) in a multidatabase environment is a \'\'necessary condition\'\' for guaranteeing Global serializability (without CO Global serializability may be violated).\n#CO compliance of every database system is a \'\'sufficient condition\'\' for guaranteeing Global serializability.\n\nHowever, the definition of autonomy above implies, for example, that transactions are scheduled in a way that local transactions (confined to a single database) cannot be identified as such by an autonomous database system. This is realistic for some transactional objects, but too restrictive and less realistic for general purpose database systems. If autonomy is augmented with the ability to identify local transactions, then compliance with a more general property, \'\'Extended commitment ordering\'\' (ECO, see below), makes ECO the necessary condition.\n\nOnly in ([[#Raz2009|Raz 2009]]) the notion of \'\'Generalized autonomy\'\' captures the intended notion of autonomy:\n*\'\'\'Definition: generalized autonomy\'\'\'\n:A database system has the \'\'Generalized autonomy\'\' property, if it does not share with any other database system any local concurrency information beyond (unmodified) atomic commit protocol messages (however any local information can be utilized).\n\nThis definition is probably the broadest such definition possible in the context of database concurrency control, and it makes CO together with any of its (useful: No concurrency control information distribution) generalizing variants (Vote ordering (VO); see CO variants below) the necessary condition for Global serializability (i.e., the union of CO and its generalizing variants is the necessary set VO, which may include also new unknown useful generalizing variants).\n\n===Summary===\n\nThe \'\'Commitment ordering\'\' (CO) solution (technique) for global serializability can be summarized as follows:\n\nIf each \'\'database\'\' (or any other \'\'transactional object\'\') in a multidatabase environment complies with CO, i.e., arranges its local transactions\' commitments and its votes on (global, distributed) transactions to the \'\'[[atomic commitment]]\'\' protocol according to the local (to the database) [[partial order]] induced by the local conflict graph (serializability graph) for the respective transactions, then \'\'Global CO\'\' and \'\'Global serializability\'\' are guaranteed. A database\'s CO compliance can be achieved effectively with any local [[Serializability#View serializability and conflict serializability|conflict serializability]] based concurrency control mechanism, with neither affecting any transaction\'s execution process or scheduling, nor aborting it. Also the database\'s autonomy is not violated. The only low overhead incurred is detecting conflicts (e.g., as with locking, but with no data-access blocking; if not already detected for other purposes), and ordering votes and local transactions\' commits according to the conflicts.\n\n[[Image:CO-ScheduleClasses.jpg|thumb|350px| \'\'\'Schedule classes containment:\'\'\' An arrow from class A to class B indicates that class A strictly contains B; a lack of a directed path between classes means that the classes are incomparable.\n\nA property is \'\'\'inherently blocking\'\'\', if it can be enforced only by blocking transaction’s data access operations until certain events occur in other transactions. ([[#Raz1992|Raz 1992]])]]\n\nIn case of incompatible partial orders of two or more databases (no global partial order can [[Embedding|embed]] the respective local partial orders together), a global cycle (spans two databases or more) in the global conflict graph is generated. This, together with CO, results in a cycle of blocked votes, and a \'\'voting-[[deadlock]]\'\' occurs for the databases on that cycle (however, allowed concurrent voting in each database, typically for almost all the outstanding votes, continue to execute). In this case the atomic commitment protocol fails to collect all the votes needed for the blocked transactions on that global cycle, and consequently the protocol aborts some transaction with a missing vote. This breaks the global cycle, the voting-deadlock is resolved, and the related blocked votes are free to be executed. Breaking the global cycle in the global conflict graph ensures that both global CO and global serializability are maintained. Thus, in case of incompatible local (partial) commitment orders no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause for the incompatibility. Furthermore, also global deadlocks due to locking (global cycles in the \'\'augmented conflict graph\'\' with at least one data access blocking) result in voting deadlocks and are resolved automatically by the same mechanism.\n\n\'\'Local CO\'\' is a necessary condition for guaranteeing \'\'Global serializability,\'\' if the databases involved do not share any concurrency control information beyond (unmodified) atomic commitment protocol messages, i.e., if the databases are \'\'autonomous\'\' in the context of concurrency control. This means that every global serializability solution for autonomous databases must comply with CO. Otherwise global serializability may be violated (and thus, is likely to be violated very quickly in a high-performance environment).\n\nThe CO solution [[Scalability|scales up]] with network size and the number of databases without performance penalty when it utilizes [[Two-phase commit protocol#Common architecture|common distributed atomic commitment architecture]].\n\n==Distributed serializability and CO==\n\n===Distributed CO===\n\nA distinguishing characteristic of the CO solution to distributed serializability from other techniques is the fact that it requires no conflict information distributed (e.g., local precedence relations, locks, [[Timestamp-based concurrency control|timestamps]], tickets), which makes it uniquely effective. It utilizes (unmodified) atomic commitment protocol messages (which are already used) instead.\n\nA common way to achieve distributed serializability in a [[Distributed system|(distributed) system]] is by a [[distributed lock manager]] (DLM). DLMs, which communicate lock (non-materialized conflict) information in a distributed environment, typically suffer from computer and communication [[Latency (engineering)|latency]], which reduces the performance of the system. CO allows to achieve distributed serializability under very general conditions, without a distributed lock manager, exhibiting the benefits already explored above for multidatabase environments; in particular: reliability, high performance, scalability, possibility of using \'\'optimistic concurrency control\'\' when desired, no conflict information related communications over the network (which have incurred overhead and delays), and automatic distributed deadlock resolution.\n\nAll \'\'distributed transactional systems\'\' rely on some atomic commitment protocol to coordinate atomicity (whether to commit or abort) among processes in a [[distributed transaction]]. Also, typically \'\'recoverable data\'\' (i.e., data under transactions\' control, e.g., database data; not to be confused with the \'\'recoverability\'\' property of a schedule) are directly accessed by a single \'\'transactional data manager\'\' component (also referred to as a \'\'resource manager\'\') that handles local sub-transactions (the distributed transaction\'s portion in a single location, e.g., network node), even if these data are accessed indirectly by other entities in the distributed system during a transaction (i.e., indirect access requires a direct access through a local sub-transaction). Thus recoverable data in a distributed transactional system are typically partitioned among transactional data managers. In such system these transactional data managers typically comprise the participants in the system\'s atomic commitment protocol. If each participant complies with CO (e.g., by using SS2PL, or COCOs, or a combination; see above), then the entire distributed system provides CO (by the theorems above; each participant can be considered a separate transactional object), and thus (distributed) serializability. Furthermore: When CO is utilized together with an atomic commitment protocol also \'\'distributed deadlocks\'\' (i.e., deadlocks that span two or more data managers) caused by data-access locking are resolved automatically. Thus the following corollary is concluded:\n\n*\'\'\'The CO Based Distributed Serializability Theorem\'\'\'\n\n:Let a \'\'distributed transactional system\'\' (e.g., a [[distributed database]] system) comprise \'\'transactional data managers\'\' (also called \'\'resource managers\'\') that manage all the system\'s \'\'recoverable data\'\'. The data managers meet three conditions:\n# \'\'\'Data partition:\'\'\' Recoverable data are partitioned among the data managers, i.e., each recoverable datum (data item) is controlled by a single data manager (e.g., as common in a [[Shared nothing architecture]]; even copies of a same datum under different data managers are physically distinct, \'\'replicated\'\').\n# \'\'\'Participants in atomic commitment protocol:\'\'\' These data managers are the participants in the system\'s atomic commitment protocol for coordinating distributed transactions\' atomicity.\n# \'\'\'CO compliance:\'\'\' Each such data manager is CO compliant (or some CO variant compliant; see below).\n:Then\n# The entire distributed system guarantees (distributed CO and) \'\'serializability\'\', and\n# Data-access based \'\'distributed deadlocks\'\' (deadlocks involving two or more data managers with at least one non-materialized conflict) are resolved automatically.\n\n:Furthermore: The data managers being CO compliant is a \'\'necessary condition\'\' for (distributed) serializability in a system meeting conditions 1, 2 above, when the data managers are \'\'autonomous\'\', i.e., do not share concurrency control information beyond unmodified messages of atomic commitment protocol.\n\nThis theorem also means that when SS2PL (or any other CO variant) is used locally in each transactional data manager, and each data manager has exclusive control of its data, no distributed lock manager (which is often utilized to enforce distributed SS2PL) is needed for distributed SS2PL and serializability. It is relevant to a wide range of distributed transactional applications, which can be easily designed to meet the theorem\'s conditions.\n\n===Distributed optimistic CO (DOCO)===\n\nFor implementing Distributed Optimistic CO (DOCO) the generic local CO algorithm is utilized in all the atomic commitment protocol participants in the system with no data access blocking and thus with no local deadlocks. The previous theorem has the following corollary:\n\n*\'\'\'The Distributed optimistic CO (DOCO) Theorem\'\'\'\n\n:If DOCO is utilized, then:\n:# No local deadlocks occur, and\n:# Global (voting) deadlocks are resolved automatically (and all are serializability related (with non-blocking conflicts) rather than locking related (with blocking and possibly also non-blocking conflicts)).\n\n:Thus, no deadlock handling is needed.\n\n===Examples===\n\n====Distributed SS2PL====\n\nA distributed database system that utilizes [[Two-phase locking#Strong strict two-phase locking|SS2PL]] resides on two remote nodes, A and B. The database system has two \'\'transactional data managers\'\' (\'\'resource managers\'\'), one on each node, and the database data are partitioned between the two data managers in a way that each has an exclusive control of its own (local to the node) portion of data: Each handles its own data and locks without any knowledge on the other manager\'s. For each distributed transaction such data managers need to execute the available atomic commitment protocol.\n\nTwo distributed transactions, <math>T_{1}</math> and <math>T_{2}</math>, are running concurrently, and both access data x and y. x is under the exclusive control of the data manager on A (B\'s manager cannot access x), and y under that on B.\n\n:<math>T_{1}</math> reads x on A and writes y on B, i.e., <math>T_{1} = R_{1A}(x)</math> <math>W_{1B}(y)</math> when using notation common for concurrency control.\n:<math>T_{2}</math> reads y on B and writes x on A, i.e., <math>T_{2} = R_{2B}(y)</math> <math>W_{2A}(x)</math>\n\nThe respective \'\'local sub-transactions\'\' on A and B (the portions of <math>T_{1}</math> and <math>T_{2}</math> on each of the nodes) are the following:\n\n:{| class="wikitable" style="text-align:center;"\n|+Local sub-transactions\n|-\n! Transaction \\ Node !! A  !! B\n|-\n! <math>T_{1}</math>\n|  <math>T_{1A}=R_{1A}(x)</math> || <math>T_{1B}=W_{1B}(y)</math>\n|-\n! <math>T_{2}</math>\n| <math>T_{2A}=W_{2A}(x)</math> || <math>T_{2B}=R_{2B}(y)</math>\n|}\n\nThe database system\'s [[Schedule (computer science)|schedule]] at a certain point in time is the following:\n\n:<math>R_{1A}(x)</math> <math>R_{2B}(y)</math>\n:(also <math>R_{2B}(y)</math> <math>R_{1A}(x)</math> is possible)\n\n<math>T_{1}</math> holds a read-lock on x and <math>T_{2}</math> holds read-locks on y. Thus <math>W_{1B}(y)</math> and <math>W_{2A}(x)</math> are blocked by the [[Two-phase locking#Data-access locks|lock compatibility]] rules of SS2PL and cannot be executed. This is a distributed deadlock situation, which is also a voting-deadlock (see below) with a distributed (global) cycle of length 2 (number of edges, conflicts; 2 is the most frequent length). The local sub-transactions are in the following states:\n\n:<math>T_{1A}</math> is \'\'ready\'\' (execution has ended) and \'\'voted\'\' (in atomic commitment)\n:<math>T_{1B}</math> is \'\'running\'\' and blocked (a non-materialized conflict situation; no vote on it can occur)\n:<math>T_{2B}</math> is \'\'ready\'\' and \'\'voted\'\'\n:<math>T_{2A}</math> is \'\'running\'\' and blocked (a non-materialized conflict; no vote).\n\nSince the atomic commitment protocol cannot receive votes for blocked sub-transactions (a voting-deadlock), it will eventually abort some transaction with a missing vote(s) by [[Timeout (computing)|timeout]], either <math>T_{1}</math>, or <math>T_{2}</math>, (or both, if the timeouts fall very close). This will resolve the global deadlock. The remaining transaction will complete running, be voted on, and committed. An aborted transaction is immediately \'\'restarted\'\' and re-executed.\n\n\'\'\'Comments:\'\'\'\n# The data partition (x on A; y on B) is important since without it, for example, x can be accessed directly from B. If a transaction <math>T_{3}</math> is running on B concurrently with <math>T_{1}</math> and <math>T_{2}</math> and directly writes x, then, without a distributed lock manager the read-lock for x held by <math>T_{1}</math> on A is not visible on B and cannot block the write of <math>T_{3}</math> (or signal a materialized conflict for a non-blocking CO variant; see below). Thus serializability can be violated.\n# Due to data partition, x cannot be accessed directly from B. However, functionality is not limited, and a transaction running on B still can issue a write or read request of x (not common). This request is communicated to the transaction\'s local sub-transaction on A (which is generated, if does not exist already) which issues this request to the local data manager on A.\n\n====Variations====\n\nIn the scenario above both conflicts are \'\'non-materialized\'\', and the global voting-deadlock is reflected as a cycle in the global \'\'wait-for graph\'\' (but not in the global \'\'conflict graph\'\'; see [[Commitment ordering#Exact characterization of voting-deadlocks by global cycles|Exact characterization of voting-deadlocks by global cycles]] above). However the database system can utilize any CO variant with exactly the same conflicts and voting-deadlock situation, and same resolution. Conflicts can be either \'\'materialized\'\' or \'\'non-materialized\'\', depending on CO variant used. For example, if [[Commitment ordering#Strict CO (SCO)|SCO]] (below) is used by the distributed database system instead of SS2PL, then the two conflicts in the example are \'\'materialized\'\', all local sub-transactions are in \'\'ready\'\' states, and vote blocking occurs in the two transactions, one on each node, because of the CO voting rule applied independently on both A and B: due to conflicts <math>T_{2A}=W_{2A}(x)</math> is not voted on before <math>T_{1A}=R_{1A}(x)</math> ends, and <math>T_{1B}=W_{1B}(y)</math> is not voted on before <math>T_{2B}=R_{2B}(y)</math> ends, which is a voting-deadlock. Now the \'\'conflict graph\'\' has the global cycle (all conflicts are materialized), and again it is resolved by the atomic commitment protocol, and distributed serializability is maintained. Unlikely for a distributed database system, but possible in principle (and occurs in a multi-database), A can employ SS2PL while B employs SCO. In this case the global cycle is neither in the wait-for graph nor in the serializability graph, but still in the \'\'augmented conflict graph\'\' (the union of the two). The various combinations are summarized in the following table:\n\n{| class="wikitable" style="text-align:center;"\n|+Voting-deadlock situations\n|-\n!Case!! Node<br>A  !! Node<br>B !!Possible schedule!!Materialized<br>conflicts<br>on cycle!!Non-<br>materialized<br>conflicts!!<math>T_{1A}</math> =<br><math>R_{1A}(x)</math>!!<math>T_{1B}</math> =<br><math>W_{1B}(y)</math>!!<math>T_{2A}</math> =<br><math>W_{2A}(x)</math>!!<math>T_{2B}</math> =<br><math>R_{2B}(y)</math>\n|-\n! 1\n|SS2PL||SS2PL||<math>R_{1A}(x)</math> <math>R_{2B}(y)</math>|| 0 || 2 ||Ready<br>Voted||Running<br>(Blocked)||Running<br>(Blocked)||Ready<br>Voted\n|-\n! 2\n|SS2PL|| SCO ||<math>R_{1A}(x)</math> <math>R_{2B}(y)</math> <math>W_{1B}(y)</math>|| 1 || 1 ||Ready<br>Voted ||Ready<br>Vote blocked||Running<br>(Blocked)||Ready<br>Voted\n|-\n! 3\n|SCO||SS2PL|| <math>R_{1A}(x)</math> <math>R_{2B}(y)</math> <math>W_{2A}(x)</math> || 1 || 1 ||Ready<br>Voted||Running<br>(Blocked)||Ready<br>Vote blocked||Ready<br>Voted\n|-\n! 4\n|SCO||SCO||<math>R_{1A}(x)</math> <math>R_{2B}(y)</math> <math>W_{1B}(y)</math> <math>W_{2A}(x)</math>|| 2 || 0 ||Ready<br>Voted||Ready<br>Vote blocked ||Ready<br>Vote blocked||Ready<br>Voted\n|}\n\n:\'\'\'Comments:\'\'\'\n# Conflicts and thus cycles in the \'\'augmented conflict graph\'\' are determined by the transactions and their initial scheduling only, independently of the concurrency control utilized. With any variant of CO, any \'\'global cycle\'\' (i.e., spans two databases or more) causes a \'\'voting deadlock\'\'. Different CO variants may differ on whether a certain conflict is \'\'materialized\'\' or \'\'non-materialized\'\'.\n# Some limited operation order changes in the schedules above are possible, constrained by the orders inside the transactions, but such changes do not change the rest of the table.\n# As noted above, only case 4 describes a cycle in the (regular) conflict graph which affects serializability. Cases 1-3 describe cycles of locking based global deadlocks (at least one lock blocking exists). All cycle types are equally resolved by the atomic commitment protocol. Case 1 is the common Distributed SS2PL, utilized since the 1980s. However, no research article, except the CO articles, is known to notice this automatic locking global deadlock resolution as of 2009. Such global deadlocks typically have been dealt with by dedicated mechanisms.\n# Case 4 above is also an example for a typical voting-deadlock when [[Commitment ordering#Distributed optimistic CO (DOCO)|Distributed optimistic CO (DOCO)]] is used (i.e., Case 4 is unchanged when Optimistic CO (OCO; see below) replaces SCO on both A and B): No data-access blocking occurs, and only materialized conflicts exist.\n\n====Hypothetical Multi Single-Threaded Core (MuSiC) environment====\n\n\'\'\'Comment:\'\'\' While the examples above describe real, recommended utilization of CO, this example is hypothetical, for demonstration only.\n\nCertain experimental distributed memory-resident databases advocate multi single-threaded core (MuSiC) transactional environments. "Single-threaded" refers to transaction [[Thread (computer science)|threads]] only, and to \'\'serial\'\' execution of transactions. The purpose is possible orders of magnitude gain in performance (e.g., [[Michael Stonebraker#H-Store and VoltDB|H-Store]]<ref name=Stone08>Robert Kallman, Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alex Rasin, [[Stanley Zdonik]], Evan Jones, Yang Zhang, Samuel Madden, [[Michael Stonebraker]], John Hugg, Daniel Abadi (2008): [http://portal.acm.org/citation.cfm?id=1454211  "H-Store: A High-Performance, Distributed Main Memory Transaction Processing System"], \'\'Proceedings of the 2008 VLDB\'\', pages 1496 - 1499, Auckland, New-Zealand, August 2008.</ref> and [[VoltDB]]) relatively to conventional transaction execution in multiple threads on a same core. In what described below MuSiC is independent of the way the cores are distributed. They may reside in one [[integrated circuit]] (chip), or in many chips, possibly distributed geographically in many computers. In such an environment, if recoverable (transactional) data are partitioned among threads (cores), and it is implemented in the conventional way for distributed CO, as described in previous sections, then DOCO and Strictness exist automatically. However, downsides exist with this straightforward implementation of such environment, and its practicality as a general-purpose solution is questionable. On the other hand, tremendous performance gain can be achieved in applications that can bypass these downsides in most situations.\n\n\'\'\'Comment:\'\'\' The MuSiC straightforward implementation described here (which uses, for example, as usual in distributed CO, voting (and transaction thread) blocking in atomic commitment protocol when needed) is for demonstration only, and has \'\'\'no connection\'\'\' to the implementation in H-Store or any other project.\n\nIn a MuSiC environment local schedules are \'\'serial\'\'. Thus both local Optimistic CO (OCO; see below) and the \'\'Global CO enforcement vote ordering strategy\'\' condition for the atomic commitment protocol are met automatically. This results in both distributed CO compliance (and thus distributed serializability) and automatic global (voting) deadlock resolution.\n\nFurthermore, also local \'\'Strictness\'\' follows automatically in a serial schedule. By Theorem 5.2 in ([[#Raz1992|Raz 1992]]; page  307), when the CO vote ordering strategy is applied, also Global Strictness is guaranteed. Note that \'\'serial\'\' locally is the only mode that allows strictness and "optimistic" (no data access blocking) together.\n\nThe following is concluded:\n\n* \'\'\'The MuSiC Theorem\'\'\'\n:In MuSiC environments, if recoverable (transactional) data are partitioned among cores (threads), then both\n:#\'\'OCO\'\' (and implied \'\'Serializability\'\'; i.e., DOCO and Distributed serializability)\n:#\'\'Strictness\'\' (allowing effective recovery; 1 and 2 implying Strict CO—see SCO below) and\n:#(voting) \'\'deadlock resolution\'\'\n:automatically exist globally with unbounded scalability in number of cores used.\n\n:\'\'\'Comment:\'\'\' However, two major downsides, which need special handling, may exist:\n#Local sub-transactions of a global transaction are blocked until commit, which makes the respective cores idle. This reduces core utilization substantially, even if scheduling of the local sub-transactions attempts to execute all of them in time proximity, almost together. It can be overcome by detaching execution from commit (with some atomic commitment protocol) for global transactions, at the cost of possible cascading aborts.\n#increasing the number of cores for a given amount of recoverable data (database size) decreases the average amount of (partitioned) data per core. This may make some cores idle, while others very busy, depending on data utilization distribution. Also a local (to a core) transaction may become global (multi-core) to reach its needed data, with additional incurred overhead. Thus, as the number of cores increases, the amount and type of data assigned to each core should be balanced according to data usage, so a core is neither overwhelmed to become a bottleneck, nor becoming idle too frequently and underutilized in a busy system. Another consideration is putting in a same core partition all the data that are usually accessed by a same transaction (if possible), to maximize the number of local transactions (and minimize the number of global, distributed transactions). This may be achieved by occasional data re-partition among cores based on load balancing (data access balancing) and patterns of data usage by transactions. Another way to considerably mitigate this downside is by proper physical data replication among some core partitions in a way that read-only global transactions are possibly (depending on usage patterns) completely avoided, and replication changes are synchronized by a dedicated commit mechanism.\n\n==CO variants: Interesting special cases and generalizations==\n\nSpecial case schedule property classes (e.g., SS2PL and SCO below) are strictly contained in the CO class. The generalizing classes (ECO and MVCO) strictly contain the CO class (i.e., include also schedules that are not CO compliant). The generalizing variants also guarantee global serializability without distributing local concurrency control information (each database has the \'\'generalized autonomy\'\' property: it uses only local information), while relaxing CO constraints and utilizing additional (local) information for better concurrency and performance: ECO uses knowledge about transactions being local (i.e., confined to a single database), and MVCO uses availability of data versions values. Like CO, both generalizing variants are \'\'non-blocking\'\', do not interfere with any transaction\'s operation scheduling, and can be seamlessly combined with any relevant concurrency control mechanism.\n\nThe term \'\'\'CO variant\'\'\' refers in general to CO, ECO, MVCO, or a combination of each of them with any relevant concurrency control mechanism or property (including Multi-version based ECO, MVECO). No other interesting generalizing variants (which guarantee global serializability with no local concurrency control information distribution) are known, but may be discovered.\n\n===Strong strict two phase locking (SS2PL)===\n{{main|Two-phase locking}}\n\n\'\'\'Strong Strict Two Phase Locking\'\'\' (SS2PL;  also referred to as \'\'Rigorousness\'\' or \'\'Rigorous scheduling\'\') means that both read and write locks of a transaction are released only after the transaction has ended (either committed or aborted). The set of SS2PL schedules is a [[proper subset]] of the set of CO schedules.\nThis property is widely utilized in database systems, and since it implies CO, databases that use it and participate in global transactions generate together a serializable global schedule (when using any atomic commitment protocol, which is needed for atomicity in a multi-database environment). No database modification or addition is needed in this case to participate in a CO distributed solution: The set of undecided transactions to be aborted before committing in the [[Commitment ordering#The algorithm|local generic CO algorithm]] above is empty because of the locks, and hence such an algorithm is unnecessary in this case. A transaction can be voted on by a database system immediately after entering a "ready" state, i.e., completing running its task locally. Its locks are released by the database system only after it is decided by the atomic commitment protocol, and thus the condition in the \'\'Global CO enforcing theorem\'\' above is kept automatically. Interestingly, if a local timeout mechanism is used by a database system to resolve (local) SS2PL deadlocks, then aborting blocked transactions breaks not only potential local cycles in the global conflict graph (real cycles in the augmented conflict graph), but also database system\'s potential global cycles as a side effect, if the [[atomic commitment]] protocol\'s abort mechanism is relatively slow. Such independent aborts by several entities typically may result in unnecessary aborts for more than one transaction per global cycle. The situation is different for a local \'\'wait-for graph\'\' based mechanisms: Such cannot identify global cycles, and the atomic commitment protocol will break the global cycle, if the resulting voting deadlock is not resolved earlier in another database.\n\nLocal SS2PL together with atomic commitment implying global serializability can also be deduced directly: All transactions, including distributed, obey the [[Two-phase locking|2PL]] (SS2PL) rules. The atomic commitment protocol mechanism is not needed here for consensus on commit, but rather for the end of phase-two synchronization point. Probably for this reason, without considering the atomic commitment voting mechanism, automatic global deadlock resolution has not been noticed before CO.\n\n===Strict CO (SCO)===\n\n[[Image:SCO-VS-SS2PL.jpg|thumb|450px|\'\'\'Read-write conflict: SCO Vs. SS2PL\'\'\'. Duration of transaction T2 is longer with SS2PL than with SCO.\n\nSS2PL delays write operation w2[x] of T2 until T1 commits, due to a lock on x by T1 following read operation r1[x]. If t time units are needed for transaction T2 after starting write operation w2[x] in order to reach ready state, than T2 commits t time units after T1 commits. However, SCO does not block w2[x], and T2 can commit immediately after T1 commits. ([[#Raz1991c|Raz 1991c]])]]\n\n\'\'\'Strict Commitment Ordering\'\'\' (SCO; ([[#Raz1991c|Raz 1991c]])) is the intersection of [[Schedule (computer science)#Strict|strictness]] (a special case of recoverability) and CO, and provides an upper bound for a schedule\'s concurrency when both properties exist. It can be implemented using blocking mechanisms (locking) similar to those used for the popular SS2PL with similar overheads.\n\nUnlike SS2PL, SCO does not block on a read-write conflict but possibly blocks on commit instead. SCO and SS2PL have identical blocking behavior for the other two conflict types: write-read, and write-write. As a result, SCO has shorter average blocking periods, and more concurrency (e.g., performance simulations of a single database for the most significant variant of \'\'[[locks with ordered sharing]],\'\' which is identical to SCO, clearly show this, with approximately 100% gain for some transaction loads; also for identical transaction loads SCO can reach higher transaction rates than SS2PL before \'\'lock [[Thrashing (computer science)|thrashing]]\'\' occurs). More concurrency means that with given computing resources more transactions are completed in time unit (higher transaction rate, [[throughput]]), and the average duration of a transaction is shorter (faster completion; see chart). The advantage of SCO is especially significant during lock contention.\n\n*\'\'\'The SCO Vs. SS2PL Performance Theorem\'\'\'\n:SCO provides shorter average transaction completion time than SS2PL, if read-write conflicts exist. SCO and SS2PL are identical otherwise (have identical blocking behavior with write-read and write-write conflicts).\n\nSCO is as practical as SS2PL since as SS2PL it provides besides serializability also strictness, which is widely utilized as a basis for efficient recovery of databases from failure. An SS2PL mechanism can be converted to an SCO one for better performance in a straightforward way without changing recovery methods. A description of a SCO implementation can be found in (Perrizo and Tatarinov 1998).<ref>{{cite conference | first1 = William | last1 = Perrizo | first2 = Igor | last2 = Tatarinov | title =  A Semi-Optimistic Database Scheduler Based on Commit Ordering | citeseerx = 10.1.1.53.7318 | conference = 1998 Int\'l Conference on Computer Applications in Industry and Engineering | pages = 75–79 | location = Las Vegas | date = November 11, 1998 }}</ref> See also  \'\'[[The History of Commitment Ordering#Semi-optimistic database scheduler|Semi-optimistic database scheduler]]\'\'.\n\nSS2PL is a proper subset of SCO (which is another explanation why SCO is less constraining and provides more concurrency than SS2PL).\n\n===Optimistic CO (OCO)===\n\nFor implementing \'\'\'Optimistic commitment ordering\'\'\' (OCO) the generic local CO algorithm is utilized without data access blocking, and thus without local deadlocks. OCO without transaction or operation scheduling constraints covers the entire CO class, and is not a special case of the CO class, but rather a useful CO variant and mechanism characterization.\n\n===Extended CO (ECO)===\n\n====General characterization of ECO====\n\n\'\'\'Extended Commitment Ordering\'\'\' (ECO; ([[#Raz1993a|Raz 1993a]])) generalizes CO. When local transactions (transactions confined to a single database) can be distinguished from global (distributed) transactions (transactions that span two databases or more), commitment order is applied to global transactions only. Thus, for a local (to a database) schedule to have the ECO property, the chronological (partial) order of commit events of global transactions only (unimportant for local transactions) is consistent with their order on the respective local conflict graph.\n\n*\'\'\'Definition: extended commitment ordering\'\'\'\n\n:Let <math>T_{1}, T_{2}</math> be two committed \'\'global\'\' transactions in a schedule, such that a \'\'directed path\'\' of unaborted transactions exists in the \'\'conflict graph\'\' ([[precedence graph]]) from <math>T_{1}</math> to <math>T_{2}</math> (<math>T_{1}</math> precedes <math>T_{2}</math>, possibly [[transitive relation|transitively]], indirectly). The schedule has the \'\'\'Extended commitment ordering\'\'\' (ECO) property, if for every two such transactions <math>T_{1}</math> commits before <math>T_{2}</math> commits.\n\nA distributed algorithm to guarantee global ECO exists. As for CO, the algorithm needs only (unmodified) atomic commitment protocol messages. In order to guarantee global serializability, each database needs to guarantee also the conflict serializability of its own transactions by any (local) concurrency control mechanism.\n\n* \'\'\'The ECO and Global Serializability Theorem\'\'\'\n\n#(Local, which implies global) ECO together with local conflict serializability, is a sufficient condition to guarantee global conflict serializability.\n#When no concurrency control information beyond atomic commitment messages is shared outside a database (autonomy), and local transactions can be identified, it is also a necessary condition.\n\n:See a necessity proof in ([[#Raz1993a|Raz 1993a]]).\n\nThis condition (ECO with local serializability) is weaker than CO, and allows more concurrency at the cost of a little more complicated local algorithm (however, no practical overhead difference with CO exists).\n\nWhen all the transactions are assumed to be global (e.g., if no information is available about transactions being local), ECO reduces to CO.\n\n====The ECO algorithm====\n\nBefore a global transaction is committed, a generic local (to a database) ECO algorithm aborts a minimal set of undecided transactions (neither committed, nor aborted; either local transactions, or global that run locally), that can cause later a cycle in the conflict graph. This set of aborted transactions (not unique, contrary to CO) can be optimized, if each transaction is assigned with a weight (that can be determined by transaction\'s importance and by the computing resources already invested in the running transaction; optimization can be carried out, for example, by a reduction from the \'\'[[Max flow in networks]]\'\' problem ([[#Raz1993a|Raz 1993a]])). Like for CO such a set is time dependent, and becomes empty eventually. Practically, almost in all needed implementations a transaction should be committed only when the set is empty (and no set optimization is applicable). The local (to the database) concurrency control mechanism (separate from the ECO algorithm) ensures that local cycles are eliminated (unlike with CO, which implies serializability by itself; however, practically also for CO a local concurrency mechanism is utilized, at least to ensure Recoverability). Local transactions can be always committed concurrently (even if a precedence relation exists, unlike CO). When the overall transactions\' local partial order (which is determined by the local conflict graph, now only with possible temporary local cycles, since cycles are eliminated by a local serializability mechanism) allows, also global transactions can be voted on to be committed concurrently (when all their transitively (indirect) preceding (via conflict) \'\'global\'\' transactions are committed, while transitively preceding local transactions can be at any state. This in analogy to the distributed CO algorithm\'s stronger concurrent voting condition, where all the transitively preceding transactions need to be committed).\n\nThe condition for guaranteeing \'\'Global ECO\'\' can be summarized similarly to CO:\n\n*\'\'\'The Global ECO Enforcing Vote ordering strategy Theorem\'\'\'\n\n:Let <math>T_{1}, T_{2}</math> be undecided (neither committed nor aborted) \'\'global transactions\'\' in a database system that ensures serializability locally, such that a \'\'directed path\'\' of unaborted transactions exists in the \'\'local conflict graph\'\' (that of the database itself) from <math>T_{1}</math> to <math>T_{2}</math>. Then, having <math>T_{1}</math> ended (either committed or aborted) before <math>T_{2}</math> is voted on to be committed, in every such database system in a multidatabase environment, is a [[necessary and sufficient condition]] for guaranteeing Global ECO (the condition guarantees Global ECO, which may be violated without it).\n\nGlobal ECO (all global cycles in the global conflict graph are eliminated by atomic commitment) together with Local serializability (i.e., each database system maintains serializability locally; all local cycles are eliminated) imply Global serializability (all cycles are eliminated). This means that if each database system in a multidatabase environment provides local serializability (by \'\'any\'\' mechanism) and enforces the \'\'vote ordering strategy\'\' in the theorem above (a generalization of CO\'s vote ordering strategy), then \'\'Global serializability\'\' is guaranteed (no local CO is needed anymore).\n\nSimilarly to CO as well, the ECO \'\'voting-deadlock\'\' situation can be summarized as follows:\n\n*\'\'\'The ECO Voting-Deadlock Theorem\'\'\'\n\n:Let a multidatabase environment comprise database systems that enforce, each, both \'\'Global ECO\'\' (using the condition in the theorem above) and \'\'local conflict serializability\'\' (which eliminates local cycles in the global conflict graph). Then, a \'\'voting-deadlock\'\' occurs if and only if a \'\'global cycle\'\' (spans two or more databases) exists in the \'\'Global augmented conflict graph\'\' (also blocking by a data-access lock is represented by an edge). If the cycle does not break by any abort, then all the \'\'global transactions\'\' on it are involved with the respective voting-deadlock, and eventually each has its vote blocked (either directly, or indirectly by a data-access lock). If a local transaction resides on the cycle, it may be in any unaborted state (running, ready, or committed; unlike CO no local commit blocking is needed).\n\nAs with CO this means that also global deadlocks due to data-access locking (with at least one lock blocking) are voting deadlocks, and are automatically resolved by atomic commitment.\n\n===Multi-version CO (MVCO)===\n\n\'\'\'Multi-version Commitment Ordering\'\'\' (MVCO; ([[#Raz1993b|Raz 1993b]])) is a generalization of CO for databases with [[Multiversion concurrency control|multi-version resources]]. With such resources \'\'read-only transactions\'\' do not block or being blocked for better performance. Utilizing such resources is a common way nowadays to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions\' read operations of several last relevant versions (of each object). MVCO implies \'\'One-copy-serializability\'\' (1SER or 1SR) which is the generalization of [[serializability]] for multi-version resources. Like CO, MVCO is non-blocking, and can be combined with any relevant multi-version concurrency control mechanism without interfering with it. In the introduced underlying theory for MVCO conflicts are generalized for different versions of a same resource (differently from earlier multi-version theories). For different versions conflict chronological order is replaced by version order, and possibly reversed, while keeping the usual definitions for conflicting operations. Results for the regular and augmented conflict graphs remain unchanged, and similarly to CO a distributed MVCO enforcing algorithm exists, now for a mixed environment with both single-version and multi-version resources (now single-version is a special case of multi-version). As for CO, the MVCO algorithm needs only (unmodified) [[atomic commitment]] protocol messages with no additional communication overhead. Locking-based global deadlocks translate to voting deadlocks and are resolved automatically. In analogy to CO the following holds:\n\n*\'\'\'The MVCO and Global one-copy-serializability Theorem\'\'\'\n\n#MVCO compliance of every \'\'autonomous\'\' database system (or transactional object) in a mixed multidatabase environment of single-version and multi-version databases is a \'\'necessary condition\'\' for guaranteeing Global one-copy-serializability (1SER).\n#MVCO compliance of every database system is a \'\'sufficient condition\'\' for guaranteeing Global 1SER.\n#Locking-based global deadlocks are resolved automatically.\n\n:\'\'\'Comment\'\'\': Now a CO compliant single-version database system is automatically also MVCO compliant.\n\nMVCO can be further generalized to employ the generalization of ECO (MVECO).\n\n====Example: CO based snapshot isolation (COSI)====\n\n\'\'\'CO based snapshot isolation\'\'\' (COSI) is the intersection of \'\'[[Snapshot isolation]]\'\' (SI) with MVCO. SI is a [[multiversion concurrency control]] method widely utilized due to good performance and similarity to serializability (1SER) in several aspects. The theory in (Raz 1993b) for MVCO described above is utilized later in (Fekete et al. 2005) and other articles on SI, e.g., (Cahill et al. 2008);<ref name=Cahill08>Michael J. Cahill, Uwe Röhm, Alan D. Fekete (2008): [http://portal.acm.org/citation.cfm?id=1376690  "Serializable isolation for snapshot databases"], \'\'Proceedings of the 2008 ACM SIGMOD international conference on Management of data\'\', pp. 729-738, Vancouver, Canada, June 2008, ISBN 978-1-60558-102-6 (SIGMOD 2008 best paper award</ref> see also [[Snapshot isolation#Making Snapshot Isolation Serializable|Making snapshot isolation serializable]] and the references there), for analyzing conflicts in SI in order to make it serializable. The method presented in (Cahill et al. 2008), \'\'Serializable snapshot isolation\'\' (SerializableSI), a low overhead modification of SI, provides good performance results versus SI, with only small penalty for enforcing serializability. A different method, by combining SI with MVCO (COSI), makes SI serializable as well, with a relatively low overhead, similarly to combining the generic CO algorithm with single-version mechanisms. Furthermore, the resulting combination, COSI, being MVCO compliant, allows COSI compliant database systems to inter-operate and transparently participate in a CO solution for distributed/global serializability (see below). Besides overheads also protocols\' behaviors need to be compared quantitatively. On one hand, all serializable SI schedules can be made MVCO by COSI (by possible commit delays when needed) without aborting transactions. On the other hand, SerializableSI is known to unnecessarily abort and restart certain percentages of transactions also in serializable SI schedules.\n\n===CO and its variants are transparently interoperable for global serializability===\n\nWith CO and its variants (e.g., SS2PL, SCO, OCO, ECO, and MVCO above) global serializability is achieved via \'\'atomic commitment\'\' protocol based distributed algorithms. For CO and all its variants atomic commitment protocol is the instrument to eliminate global cycles (cycles that span two or more databases) in the \'\'global augmented\'\' (and thus also regular) \'\'conflict graph\'\' (implicitly; no global data structure implementation is needed). In cases of either incompatible local commitment orders in two or more databases (when no global [[partial order]] can [[Embedding|embed]] the respective local partial orders together), or a data-access locking related voting deadlock, both implying a global cycle in the global augmented conflict graph and missing votes, the atomic commitment protocol breaks such cycle by aborting an undecided transaction on it (see [[commitment ordering#The distributed CO algorithm|The distributed CO algorithm]] above). Differences between the various variants exist at the local level only (within the participating database systems). Each local CO instance of any variant has the same role, to determine the position of every global transaction (a transaction that spans two or more databases) within the local commitment order, i.e., to determine when it is the transaction\'s turn to be voted on locally in the atomic commitment protocol. Thus, all the CO variants exhibit the same behavior in regard to atomic commitment. This means that they are all interoperable via atomic commitment (using the same software interfaces, typically provided as [[Service (systems architecture)|service]]s, some already [[international standard|standardized]] for atomic commitment, primarily for the [[two phase commit]] protocol, e.g., [[X/Open XA]]) and transparently can be utilized together in any distributed environment (while each CO variant instance is possibly associated with any relevant local concurrency control mechanism type).\n\nIn summary, any single global transaction can participate simultaneously in databases that may employ each any, possibly different, CO variant (while concurrently running processes in each such database, and running concurrently with local and other global transactions in each such database). The atomic commitment protocol is indifferent to CO, and does not distinguish between the various CO variants. Any \'\'global cycle\'\' generated in the augmented global conflict graph may span databases of different CO variants, and generate (if not broken by any local abort) a voting deadlock that is resolved by atomic commitment exactly the same way as in a single CO variant environment. \'\'local cycles\'\' (now possibly with mixed materialized and non-materialized conflicts, both serializability and data-access-locking deadlock related, e.g., SCO) are resolved locally (each by its respective variant instance\'s own local mechanisms).\n\n\'\'\'Vote ordering\'\'\' (VO or Generalized CO (GCO); [[#Raz2009|Raz 2009]]), the union of CO and all its above variants, is a useful concept and global serializability technique. To comply with VO, local serializability (in it most general form, commutativity based, and including multi-versioning) and the \'\'vote order strategy\'\' (voting by local precedence order) are needed.\n\nCombining results for CO and its variants, the following is concluded:\n\n*\'\'\'The CO Variants Interoperability Theorem\'\'\'\n#In a multi-database environment, where each database system (transactional object) is compliant with some CO variant property (VO compliant), any global transaction can participate simultaneously in databases of possibly different CO variants, and Global serializability is guaranteed (\'\'sufficient condition\'\' for Global serializability; and Global one-copy-serializability (1SER), for a case when a multi-version database exists).\n#If only local (to a database system) concurrency control information is utilized by every database system (each has the \'\'generalized autonomy\'\' property, a generalization of \'\'autonomy\'\'), then compliance of each with some (any) CO variant property (VO compliance) is a \'\'necessary condition\'\' for guaranteeing Global serializability (and Global 1SER; otherwise they may be violated).\n#Furthermore, in such environment data-access-locking related global deadlocks are resolved automatically (each such deadlock is generated by a global cycle in the \'\'augmented conflict graph\'\' (i.e., a \'\'voting deadlock\'\'; see above), involving at least one data-access lock (non-materialized conflict) and two database systems; thus, not a cycle in the regular conflict graph and does not affect serializability).\n\n==References==\n\n*{{citation|first=Yoav|last=Raz|url=http://www.vldb.org/conf/1992/P292.PDF|title=The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment|work=Proceedings of the Eighteenth International Conference on Very Large Data Bases|pages=292–312|place=Vancouver, Canada|date=August 1992}} (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990)\n*{{citation|first=Yoav|last=Raz|title=Serializability by Commitment Ordering|work=Information Processing Letters|volume=51|number=5|pages=257–264|date=September 1994|doi=10.1016/0020-0190(94)90005-1}}\n*{{citation|first=Yoav|last=Raz|url=http://sites.google.com/site/yoavraz2/home/theory-of-commitment-ordering|title=Theory of Commitment Ordering: Summary|date=June 2009|accessdate=November 11, 2011}}\n*{{citation|first=Yoav|last=Raz|url=http://yoavraz.googlepages.com/DEC-CO-MEMO-90-11-16.pdf|title=On the Significance of Commitment Ordering|publisher=Digital Equipment Corporation|date=November 1990}}\n*<cite id=Raz1991a>Yoav Raz (1991a): US patents [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=3&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,504,899 (ECO)] [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=2&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,504,900 (CO)]  [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=1&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,701,480 (MVCO)] </cite>\n*<cite id=Raz1991b>Yoav Raz (1991b): "The Commitment Order Coordinator (COCO) of a Resource Manager, or Architecture for Distributed Commitment Ordering Based Concurrency Control", DEC-TR 843, Digital Equipment Corporation, December 1991. </cite>\n*<cite id=Raz1991c>Yoav Raz (1991c): "Locking Based Strict Commitment Ordering, or How to improve Concurrency in Locking Based Resource Managers", DEC-TR 844, December 1991. </cite>\n*<cite id=Raz1993a>Yoav Raz (1993a): [http://portal.acm.org/citation.cfm?id=153858 "Extended Commitment Ordering or Guaranteeing Global Serializability by Applying Commitment Order Selectivity to Global Transactions."] \'\'Proceedings of the Twelfth ACM Symposium on Principles of Database Systems\'\' (PODS), Washington, DC, pp. 83-96, May 1993. (also DEC-TR 842, November 1991) </cite>\n*<cite id=Raz1993b>Yoav Raz (1993b): [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=281924  "Commitment Ordering Based Distributed Concurrency Control for Bridging Single and Multi Version Resources."] \'\'Proceedings of the Third IEEE International Workshop on Research Issues on Data Engineering: Interoperability in Multidatabase Systems\'\' (RIDE-IMS), Vienna, Austria, pp. 189-198, April 1993. (also DEC-TR 853, July 1992)  </cite>\n\n==Footnotes==\n{{reflist}}\n\n==External links==\n*[http://sites.google.com/site/yoavraz2/the_principle_of_co Yoav Raz\'s Commitment ordering page]\n\n{{DEFAULTSORT:Commitment Ordering}}\n[[Category:Data management]]\n[[Category:Databases]]\n[[Category:Transaction processing]]\n[[Category:Concurrency control]]\n[[Category:Distributed algorithms]]']
['Category:Database theory', '10221974', '{{Cat main|database theory}}\n\n[[Category:Areas of computer science]]\n[[Category:Databases|Theory]]\n[[Category:Data management|Theory]]']
['Enterprise bus matrix', '29723359', '{{Multiple issues|\n{{weasel|date=December 2010}}\n{{orphan|date=February 2012}}\n{{cleanup|date=December 2010}}\n}}\n\nThe \'\'\'Enterprise Bus Matrix\'\'\' is a [[data Warehouse]] planning tool and model created by [[Ralph Kimball]], and is part of the Data Warehouse Bus Architecture. The Matrix is the logical definition of one of the core concepts of Kimball’s approach to Dimensional Modeling – Conformed dimensions.<ref>{{cite web|url=http://www.kimballgroup.com/2003/09/15/design-tip-49-off-the-bench/ |title=Design Tip #49: Off The Bench |publisher=Kimball Group |date=2003-09-15 |accessdate=2015-05-22 }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\nThe Bus Matrix defines part of the Data Warehouse Bus Architecture and is an output of the Business Requirements phase in [[The Kimball Lifecycle]]. It is applied in the following phases of [[dimensional modeling]] and development of the Data Warehouse . The matrix can be categorized as a hybrid model, being part technical design tool, part project management tool and part communication tool<ref name="Kimball">Kimball, Ralph & Ross, Margy; The Data Warehouse Toolkit: The Complete Guide to Dimensional Modeling, 2nd Edition John Wiley & Sons, 2002</ref>\n\n==Background==\nThe Enterprise Bus Matrix stems from the issue of how one goes about creating the overall Data Warehouse environment.  Historically there has been the structure of the centralized and planned approach and the more loosely defined, department specific, solutions developed in a more independent matter. Autonomous projects can result in a range of isolated stove pipe data marts. Naturally each approach has its issues; the overall visionary approach often struggles with long delivery cycles and lack of reaction time as the formalities and scope issues is evident. On the other hand, the development of isolated data marts, leading to [[Stovepipe system]]s that lacks synergy in development. Over time this approach will lead to a so-called data-mart-in-a-box architecture<ref>[http://www.mimno.com/avoiding-mistakes3.html#6]  {{webarchive |url=https://web.archive.org/web/20100704220014/http://www.mimno.com/avoiding-mistakes3.html#6 |date=July 4, 2010 }}</ref> where [[interoperability]] and lack of cohesion is apparent, and can hinder the realization of an overall enterprise Data Warehouse. As an attempt to handle this matter [[Ralph Kimball]] introduced the enterprise bus.\n\n==Bus matrix==\nThe bus matrix purpose is one of high abstraction and visionary planning on the Data Warehouse architectural level. By dictating coherency in the development and implementation of an overall Data Warehouse the Bus Architecture approach enables an overall vision of the broader enterprise integration and consistency while at the same time dividing the problem into more manageable parts<ref name="Kimball" /> – all in a technology and software independent manner .<ref>{{cite web|url=http://www.b-eye-network.com/view/713 |title=Data Warehouse: Ralph Kimball’s Vision by Katherine Drewek |publisher=Beyenetwork |date=2005-03-16 |accessdate=2015-05-22}}</ref>\n\nThe bus matrix and architecture builds upon the concept of conformed dimensions -  creating a structure of common dimensions that ideally can be used across the enterprise by all business processes related to the DW and the corresponding fact tables from which they derive their context. According to Kimball and Margy Ross\'s article  “Differences of Opinion”<ref>{{cite web|url=http://intelligent-enterprise.informationweek.com/showArticle.jhtml;jsessionid=0OVJNEHMPRXGRQE1GHRSKH4ATMY32JVN?articleID=17800088 |title=Enterprise Software News, Analysis, & Advice - InformationWeek |publisher=Intelligent-enterprise.informationweek.com |date= |accessdate=2015-05-22}}</ref> "\'\'The Enterprise Data warehouse built on the bus architecture ”identifies and enforces the relationship between business process metrics (facts) and descriptive attributes (dimensions)\'\'”.\n\nThe concept of a [[Bus (computing)|bus]] is well known in the language of [[Information Technology]], and is what reflects the conformed dimension concept in the Data Warehouse, creating the skeletal structure where all parts of a system connect, ensuring [[interoperability]] and consistency of data, and at the same time considers future expansion. This makes the conformed dimensions act as the integration ‘glue’, creating a robust backbone of the enterprise Data Warehouse.<ref>{{cite web|url=http://intelligent-enterprise.informationweek.com/showArticle.jhtml;jsessionid=GMS3H4SOBFQBBQE1GHOSKH4ATMY32JVN?articleID=17800088&pgno=2 |title=Enterprise Software News, Analysis, & Advice - InformationWeek |publisher=Intelligent-enterprise.informationweek.com |date= |accessdate=2015-05-22}}</ref>\n\n==Establishment and applicability==\nFigure 1<ref>{{cite web|url=http://www.widama.us/Documents/Kimball-DimensionalModeling.PDF |format=PDF |title=Dimensional Modeling Overview |author=Bob Becker |publisher=Widama.is |accessdate=2015-05-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20130322224742/http://www.widama.us:80/Documents/Kimball-DimensionalModeling.PDF |archivedate=2013-03-22 |df= }}</ref> shows the base for a single document planning tool for the whole of the DW implementation - a graphical overview of the enterprises core business processes or events each correspond to a measurement table of facts, that typically is complemented by a major source system in the horizontal rows.  In the vertical columns the groups of contextual data is found as the common, conformed dimensions.\n\nIn this way the shared dimensions are defined, as each process indicates what dimensions it applies to through the cells figure 2.<ref name="Kimball" /> By this definition and coordination of conformed dimensions and processes the development of the overall data DW bus architecture is realized.<ref name="Kimball" /> The matrix identifies the shared dimensions related to processes and fact tables, and can be a tool for planning, prioritizing what needs to be approached, coordinating implementation and communicating the importance for conformed dimensions .\n\nKimball extends the matrix bus in detail as seen in figure 3<ref name="Kimball" />  by introducing the other steps of the Datawarehouse Methodology; The Fact tables, Granularity, and at last the description of the needed facts.  description of the fact tables, granularity and fact instances of each process, structuring and specifying what is needed across the enterprise in a more specific matter, further exemplifying how the matrix can be used as a planning tool.\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Enterprise bus matrix}}\n[[Category:Business intelligence]]\n[[Category:Data management]]\n[[Category:Data warehousing]]\n[[Category:Information technology management]]']
['Global serializability', '11861063', '{{Technical|date=January 2017}}\nIn [[concurrency control]] of \'\'[[database]]s\'\', \'\'[[transaction processing]]\'\' (\'\'transaction management\'\'), and other transactional [[Distributed computing|distributed applications]], \'\'\'Global serializability\'\'\' (or \'\'\'Modular serializability\'\'\') is a property of a \'\'global schedule\'\' of [[Database transaction|transactions]]. A global schedule is the unified [[schedule (computer science)|schedule]] of all the individual database (and other [[transactional object]]) schedules in a multidatabase environment (e.g., [[federated database]]). Complying with global serializability means that the global schedule is \'\'[[serializable (databases)|serializable]]\'\', has the \'\'[[serializability]]\'\' property, while each component database (module) has a serializable schedule as well. In other words, a collection of serializable components provides overall system serializability, which is usually incorrect. A need in correctness across databases in multidatabase systems makes global serializability a major goal for \'\'[[global concurrency control]]\'\' (or \'\'modular concurrency control\'\'). With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], and small, portable, powerful computing devices (e.g., [[smartphone]]s), as well as increase in [[systems management]] sophistication, the need for atomic distributed transactions and thus effective global serializability techniques, to ensure correctness in and among distributed transactional applications, seems to increase.\n\nIn a [[federated database system]] or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple (and possibly [[Distributed database|distributed]]) databases. Enforcing global serializability in such system, where different databases may use different types of [[concurrency control]], is problematic. Even if every local schedule of a single database is serializable, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach [[Serializability#View and conflict serializability|conflict serializability]] globally would lead to unacceptable performance, primarily due to computer and communication [[latency (engineering)|latency]]. Achieving global serializability effectively over different types of concurrency control has been [[Open problem|open]] for several years. \'\'[[Commitment ordering]]\'\' (or Commit ordering; CO), a serializability technique publicly introduced in 1991 by [[Yoav Raz]] from [[Digital Equipment Corporation]] (DEC), provides an effective general solution for global ([[Serializability#View and conflict serializability|conflict]]) serializability across any collection of database systems and other [[transactional object]]s, with possibly different concurrency control mechanisms. CO does not need the distribution of conflict information, but rather utilizes the already needed (unmodified) [[atomic commitment]] protocol messages without any further communication between databases. It also allows [[Optimistic concurrency control|optimistic]] (non-blocking) implementations. CO generalizes \'\'[[Two-phase locking|Strong strict two phase locking]]\'\' (SS2PL), which in conjunction with the \'\'[[Two-phase commit protocol|Two-phase commit]]\'\' (2PC) protocol is the [[de facto standard]] for achieving global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join existing SS2PL based solutions for global serializability. The same applies also to all other multiple (transactional) object systems that use atomic transactions and need global serializability for correctness (see examples above; nowadays such need is not smaller than with database systems, the origin of atomic transactions).\n\nThe most significant aspects of CO that make it a uniquely effective general solution for global serializability are the following:\n#Seamless, low overhead integration with any concurrency control mechanism, with neither changing any transaction\'s operation scheduling or blocking it, nor adding any new operation.\n#[[Heterogeneity]]: Global serializability is achieved across multiple [[transactional objects]] (e.g., [[database management system]]s) with different (any) concurrency control mechanisms, without interfering with the mechanisms\' operations.\n#[[Modularity]]: Transactional objects can be added and removed transparently.\n#[[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|Autonomy]] of transactional objects: No need of conflict or equivalent information distribution (e.g., local precedence relations, locks, timestamps, or tickets; no object needs other object\'s information).\n#[[Scalability]]: With "normal" global transactions, [[computer network]] size and number of transactional objects can increase unboundedly with no impact on performance, and\n#Automatic global deadlock resolution.\n\nAll these aspects, except the first two, are also possessed by the popular [[Two-phase locking|SS2PL]], which is a (constrained, blocking) special case of CO and inherits many of CO\'s qualities.\n\n==The global serializability problem==\n\n===Problem statement===\n\nThe difficulties described above translate into the following problem:\n:Find an efficient (high-performance and [[fault tolerant]]) method to enforce \'\'Global serializability\'\' (global conflict serializability) in a heterogeneous distributed environment of multiple autonomous database systems. The database systems may employ different [[concurrency control]] methods. No limitation should be imposed on the operations of either local transactions (confined to a single database system) or [[distributed transaction|global transactions]] (span two or more database systems).\n\n===Quotations===\nLack of an appropriate solution for the global serializability problem has driven researchers to look for alternatives to [[serializability]] as a correctness criterion in a multidatabase environment (e.g., see \'\'[[Global serializability#Relaxing global serializability|Relaxing global serializability]]\'\' below), and the problem has been characterized as difficult and \'\'[[open problem|open]]\'\'. The following two quotations demonstrate the mindset about it by the end of the year 1991, with similar quotations in numerous other articles:\n\n*"Without knowledge about local as well as global transactions, it is highly unlikely that efficient global concurrency control can be provided... Additional complications occur when different component DBMSs [Database Management Systems] and the FDBMSs [Federated Database Management Systems] support different concurrency mechanisms... It is unlikely that a theoretically elegant solution that provides conflict serializability without sacrificing performance (i.e., concurrency and/or response time) and [[availability]] exists."<ref>Amit Sheth, James Larson (1990): [http://www.informatik.uni-trier.de/~ley/db/journals/csur/ShethL90.html  "Federated Database Systems for Managing Distributed, Heterogeneous, and Autonomous Databases"], \'\'ACM Computing Surveys\'\', Vol. 22, No 3, pp. 183-236, September 1990 (quotation from page 227)</ref>\n\n[[Commitment ordering]],<ref name=Raz1992/><ref name=Raz1994/> publicly introduced in May 1991 (see below), provides an efficient [[Elegance|elegant]] general solution, from both practical<ref name=Raz1990/><ref name=Raz1991/> and [[Theory|theoretical]]<ref name=Raz2009/> points of view, to the global serializability problem across database systems with possibly different concurrency control mechanisms. It provides conflict serializability with no negative effect on availability, and with no worse performance than the [[de facto standard]] for global serializability, CO\'s special case [[Two-phase locking#Strong strict two-phase locking|Strong strict two-phase locking]] (SS2PL). It requires knowledge about neither local nor global transactions.\n\n*"Transaction management in a heterogeneous, distributed database system is a difficult  issue. The main problem is that each of the local database management systems may be using a different type of concurrency control scheme. Integrating this is a challenging problem, made worse if we wish to preserve the local autonomy of each of the local databases, and allow local and global transactions to execute in parallel. One simple solution is to restrict global transactions to retrieve-only access. However, the issue of reliable transaction management in the general case, where global and local transactions are allowed to both read and write data, is [[open problem|still open]]."<ref>[[Abraham Silberschatz]], [[Michael Stonebraker]], and [[Jeffrey Ullman]] (1991): [http://www.informatik.uni-trier.de/~ley/db/journals/cacm/SilberschatzSU91.html  "Database Systems: Achievements and Opportunities"], \'\'Communications of the ACM\'\', Vol. 34, No. 10, pp. 110-120, October 1991 (quotation from page 120)</ref>\n\nThe commitment ordering solution comprises effective integration of autonomous database management systems with possibly different concurrency control mechanisms. This while local and global transactions execute in parallel without restricting any read or write operation in either local or global transactions, and without compromising the systems\' autonomy.\n\nEven in later years, after the public introduction of the Commitment ordering general solution in 1991, the problem still has been considered by many unsolvable:\n\n*"We present a transaction model for multidatabase systems with autonomous component systems, coined heterogeneous 3-level transactions. It has become evident that in such a system the requirements of guaranteeing full [[ACID]] properties and full local autonomy can not be reconciled..."<ref>Peter Muth (1997): [http://portal.acm.org/citation.cfm?id=264226  "Application Specific Transaction Management in Multidatabase Systems"], \'\'Distributed and Parallel Databases\'\', Volume 5, Issue 4, pp. 357 - 403, October 1997, {{ISSN|0926-8782}} (quotation from the article\'s Abstract)</ref>\n\nThe quotation above is from a 1997 article proposing a relaxed global serializability solution (see \'\'[[Global serializability#Relaxing global serializability|Relaxing global serializability]]\'\' below), and referencing [[Commitment ordering]] (CO) articles. The CO solution supports effectively both full [[ACID]] properties and full local autonomy, as well as meeting the other requirements posed above in the \'\'[[Global serializability#Problem statement|Problem statement]]\'\' section, and apparently has been misunderstood.\n\nSimilar thinking we see also in the following quotation from a 1998 article:\n\n*"The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency."<ref name=Shar1998>Sharad Mehrotra, Rajeev Rastogi, Henry Korth, [[Abraham Silberschatz]] (1998):\n[http://portal.acm.org/citation.cfm?id=277629 "Ensuring Consistency in Multidatabases by Preserving Two-Level Serializability"], \'\'ACM Transactions on Database Systems\'\' (TODS), Vol. 23, No. 2, pp. 199-230, June 1998 (quotation from the article\'s Abstract)</ref>\n\nAlso the above quoted article proposes a relaxed global serializability solution, while referencing the CO work. The CO solution for global serializability both bridges between different concurrency control protocols with no substantial concurrency reduction (and typically minor, if at all), and maintains the autonomy of local DBMSs. Evidently also here CO has been misunderstood. This misunderstanding continues to 2010 in a textbook by some of the same authors, where the same relaxed global serializability technique, \'\'Two level serializability\'\', is emphasized and described in detail, and CO is not mentioned at all.<ref name=Silber2010>[[Abraham Silberschatz|Avi Silberschatz]], Henry F Korth, S. Sudarshan (2010): [http://highered.mcgraw-hill.com/sites/0073523321/  \'\'Database System Concepts\'\'], 6th Edition, McGraw-Hill, ISBN 0-07-295886-3</ref>\n\nOn the other hand, the following quotation on CO appears in a 2009 book:<ref name=Bern2009>[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 \'\'Principles of Transaction Processing\'\', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (quotation from page 145)</ref>\n\n*"Not all concurrency control algorithms use locks... Three other techniques are timestamp ordering, serialization graph testing, and commit ordering. \'\'\'Timestamp ordering\'\'\' assigns each transaction a timestamp and ensures that conflicting operations execute in timestamp order. \'\'\'Serialization graph testing\'\'\' tracks conflicts and ensures that the serialization graph is acyclic. \'\'\'Commit ordering\'\'\' ensures that conflicting operations are consistent with the relative order in which their transactions commit, which can enable interoperability of systems using different concurrency control mechanisms."\n\n:\'\'\'Comments:\'\'\'\n#Beyond the common locking based algorithm SS2PL, which is a CO variant itself, also additional variants of CO that use locks exist, (see below). However, generic, or "pure" CO does not use locks.\n#Since CO mechanisms order the commit events according to conflicts that already have occurred, it is better to describe CO as "\'\'\'Commit ordering\'\'\' ensures that the relative order in which transactions commit is consistent with the order of their respective conflicting operations."\n\nThe characteristics and properties of the CO solution are discussed below.\n\n===Proposed solutions===\nSeveral solutions, some partial, have been proposed for the global serializability problem. Among them:\n\n* \'\'Global [[serializability#Testing conflict serializability|conflict graph]]\'\' (serializability graph, [[precedence graph]]) \'\'checking\'\'\n* \'\'Distributed [[Two phase locking]]\'\' (Distributed 2PL)\n* \'\'Distributed [[Timestamp-based concurrency control|Timestamp ordering]]\'\'\n* \'\'Tickets\'\' (local logical timestamps which define local total orders, and are propagated to determine global partial order of transactions)\n* \'\'Commitment ordering\'\'\n\n===Technology perspective===\nThe problem of global serializability has been a quite intensively researched subject in the late 1980s and early 1990s. \'\'Commitment ordering\'\' (CO) has provided an effective general solution to the problem, insight into it, and understanding about possible generalizations of \'\'[[serializability#Common mechanism - SS2PL|strong strict two phase locking]]\'\' (SS2PL), which practically and almost exclusively has been utilized (in conjunction with the \'\'[[Two-phase commit protocol]]\'\' (2PC) ) since the 1980s to achieve global serializability across databases. An important side-benefit of CO is the automatic \'\'global deadlock\'\' resolution that it provides (this is applicable also to distributed SS2PL; though global deadlocks have been an important research subject for SS2PL, automatic resolution has been overlooked, except in the CO articles, until today (2009)). At that time quite many commercial database system types existed, many non-relational, and databases were relatively very small. Multi database systems were considered a key for database scalability by database systems interoperability, and global serializability was urgently needed. Since then the tremendous progress in computing power, storage, and communication networks, resulted in [[Order of magnitude|orders of magnitude]] increases in both centralized databases\' sizes, transaction rates, and remote access to database capabilities, as well as blurring the boundaries between centralized computing and distributed one over fast, low-latency local networks (e.g., [[Infiniband]]). These, together with progress in database vendors\' distributed solutions (primarily the popular SS2PL with 2PC based, a [[de facto standard]] that allows interoperability among different vendors\' (SS2PL-based) databases; both SS2PL and 2PC technologies have gained substantial expertise and efficiency), [[workflow]] management systems, and [[database replication]] technology, in most cases have provided satisfactory and sometimes better [[information technology]] solutions without multi database atomic [[distributed transaction]]s over databases with different concurrency control (bypassing the problem above). As a result, the sense of urgency that existed with the problem at that period, and in general with high-performance distributed atomic transactions over databases with different concurrency control  types, has reduced. However, the need in concurrent distributed atomic transactions as a fundamental element of reliability exists in distributed systems also beyond database systems, and so the need in global serializability as a fundamental correctness criterion for such transactional systems (see also [[Serializability#Distributed serializability|Distributed serializability]] in [[Serializability]]). With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], small, portable, powerful computing devices (e.g., [[smartphone]]s), and sophisticated [[systems management]] the need for effective global serializability techniques to ensure correctness in and among distributed transactional applications seems to increase, and thus also the need in Commitment ordering (including the popular for databases special case SS2PL; SS2PL, though, does not meet the requirements of many other transactional objects).\n\n==The commitment ordering solution==\n{{POV-section|Commitment ordering|date=November 2011}}\n{{main|Commitment ordering}}\n{{main|The History of Commitment Ordering}}\n\nCommitment ordering<ref name=Raz1992>[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment"] {{webarchive |url=https://web.archive.org/web/20070523182950/http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html |date=May 23, 2007 }}, \'\'Proc. of the Eighteenth Int. Conf. on Very Large Data Bases\'\' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) \n</ref><ref name=Raz1994>Yoav Raz (1994): [http://linkinghub.elsevier.com/retrieve/pii/0020019094900051 "Serializability by Commitment Ordering"], \'\'Information Processing Letters\'\', [http://www.informatik.uni-trier.de/~ley/db/journals/ipl/ipl51.html#Raz94 Volume 51, Number 5],  pp. 257-264, September 1994. (Received August 1991)</ref> (or Commit ordering; CO) is the only high-performance, [[fault tolerant]], [[Serializability#View and conflict serializability|conflict serializability]] providing solution that has been proposed as a fully distributed (no central computing component or data-structure are needed), general mechanism that can be combined seamlessly with any local (to a database) [[concurrency control]] mechanism (see [[Commitment ordering#Summary|technical summary]]). Since the CO property of a schedule is a [[necessary condition]] for global serializability of [[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|\'\'autonomous databases\'\']] (in the context of concurrency control), it provides the only general solution for autonomous databases (i.e., if autonomous databases do not comply with CO, then global serializability may be violated). Seemingly by sheer luck, the CO solution possesses many attractive properties: \n \n#does not interfere with any transaction\'s operation, particularly neither block, restrict nor delay any data-access operation (read or write) for either local or [[distributed transaction|global]] transactions (and thus does not cause any extra aborts); thus allows seamless integration with any concurrency control mechanism.\n#allows [[Optimistic concurrency control|optimistic]] implementations (\'\'non-blocking\'\', i.e., non data access blocking).\n#allows [[heterogeneity]]: Global serializability is achieved across multiple [[transactional objects]] with different (any) concurrency control mechanisms, without interfering with the mechanisms\' operations.\n#allows [[modularity]]: Transactional objects can be added and removed transparently.\n#allows full [[ACID]] transaction support.\n#maintains each database\'s [[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|autonomy]], and does not need any concurrency control information distribution (e.g., local precedence relations, locks, timestamps, or tickets).\n#does not need any knowledge about the transactions.\n#requires no communication overhead since it only uses already needed, unmodified \'\'[[atomic commitment]]\'\' protocol messages (any such protocol; using [[fault tolerant]] atomic commitment protocols and database systems makes the CO solution fault tolerant).\n#automatically resolves global [[deadlock]]s due to [[lock (computer science)|locking]].\n#[[Scalability|scales up]] effectively with [[computer network]] size and number of databases, almost without any negative impact on performance, since each global transaction is typically confined to certain relatively small numbers of databases and network nodes.\n#requires no additional, artificial transaction access operations (e.g., "take [[Timestamp-based concurrency control|timestamp]]" or "take ticket"), which typically result in  additional, artificial conflicts that reduce concurrency.\n#requires low overhead.\n\nThe only overhead incurred by the CO solution is locally detecting conflicts (which is already done by any known serializability mechanism, both pessimistic and optimistic) and locally ordering in each database system both the (local) commits of local transactions and the voting for atomic commitment of global transactions. Such overhead is low. The net effect of CO may be some delays of commit events (but never more delay than SS2PL, and on the average less). This makes CO instrumental for global concurrency control of multidatabase systems (e.g., [[federated database system]]s). The underlying \'\'Theory of Commitment ordering\'\',<ref name=Raz2009>Yoav Raz (2009): [http://sites.google.com/site/yoavraz2/home/theory-of-commitment-ordering Theory of Commitment Ordering - Summary] GoogleSites - Site of Yoav Raz. Retrieved 1 Feb, 2011.</ref> part of [[Serializability]] theory, is both sound and [[Scientific method#Hypothesis development|elegant]] (and even [[Mathematical beauty|"mathematically beautiful"]]; referring to structure and dynamics of conflicts, graph cycles, and deadlocks), with interesting implications for transactional [[Distributed computing|distributed applications]].\n\nAll the qualities of CO in the list above, except the first three, are also possessed by SS2PL, which is a special case of CO, but blocking and constraining. This partially explains the popularity of SS2PL as a solution (practically, the only solution, for many years) for achieving global serializability. However, property 9 above, automatic resolution of global deadlocks, has not been noticed for SS2PL in the database research literature until today (2009; except in the CO publications). This, since the phenomenon of voting-deadlocks in such environments and their automatic resolution by the [[atomic commitment]] protocol has been overlooked.\n\nMost existing database systems, including all major commercial database systems, are \'\'[[serializability#Common mechanism - SS2PL|strong strict two phase locking (SS2PL)]]\'\' based and already CO compliant. Thus they can participate in a [[commitment ordering#Summary|CO based solution for global serializability in multidatabase environments]] without any modification (except for the popular \'\'[[Multiversion concurrency control|multiversioning]]\'\', where additional CO aspects should be considered). Achieving global serializability across SS2PL based databases using atomic commitment (primarily using \'\'[[two phase commit]], 2PC\'\') has been employed for many years (i.e., using the same CO solution for a specific special case; however, no reference is known prior to CO, that notices this special case\'s automatic global deadlock resolution by the atomic commitment protocol\'s [[commitment ordering#Exact characterization of voting-deadlocks by global cycles|augmented-conflict-graph]] global cycle elimination process). Virtually all existing distributed transaction processing environments and supporting products rely on SS2PL and provide 2PC. As a matter of fact SS2PL together with 2PC have become a [[de facto standard]]. This solution is a homogeneous concurrency control one, suboptimal (when both Serializability and [[Schedule (computer science)#Strict|Strictness]] are needed; see [[Commitment ordering#Strict CO (SCO)|Strict commitment ordering]]; SCO) but still quite effective in most cases, sometimes at the cost of increased computing power needed relatively to the optimum. (However, for better performance [[Serializability#Relaxing serializability|relaxed serializability]] is used whenever applications allow). It allows inter-operation among SS2PL-compliant different database system types, i.e., allows heterogeneity in aspects other than concurrency control. SS2PL is a very constraining schedule property, and "takes over" when combined with any other property. For example, when combined with any [[Concurrency control#Concurrency control mechanisms|optimistic property]], the result is not optimistic anymore, but rather characteristically SS2PL. On the other hand, CO does not change data-access scheduling patterns at all, and \'\'any\'\' combined property\'s characteristics remain unchanged. Since also CO uses atomic commitment (e.g., 2PC) for achieving global serializability, as SS2PL does, any CO compliant database system or transactional object can transparently join existing SS2PL based environments, use 2PC, and maintain global serializability without any environment change. This makes CO a straightforward, natural generalization of SS2PL for any conflict serializability based database system, for all practical purposes.\n\nCommitment ordering has been quite widely known inside the \'\'[[transaction processing]]\'\' and \'\'[[database]]s\'\' communities at \'\'[[Digital Equipment Corporation]]\'\' (DEC) since 1990. It has been under \'\'company confidentiality\'\' due to [[patent]]ing<ref name=Raz1990>Yoav Raz (1990): [http://yoavraz.googlepages.com/DEC-CO-MEMO-90-11-16.pdf  \'\'On the Significance of Commitment Ordering\'\'] - Call for patenting, Memorandum, [[Digital Equipment Corporation]], November 1990.</ref>\n<ref name=Raz1991>\nYoav Raz: US patents [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=3&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,504,899]  [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=2&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,504,900]   [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=1&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,701,480]</ref> processes. CO was disclosed outside of DEC by lectures and technical reports\' distribution to database researches in May 1991, immediately after its first patent filing. It has been misunderstood by many database researchers years after its introduction, which is evident by the quotes above from articles in 1997-1998 referencing Commitment ordering articles. On the other hand, CO has been utilized extensively as a solution for global serializability in works on [[Transactional processes]],\n<ref>Heiko Schuldt, Hans-Jörg Schek, and Gustavo Alonso (1999): [http://portal.acm.org/citation.cfm?id=853907 "Transactional Coordination Agents for Composite Systems"], In \'\'Proceedings of the 3rd International Database Engineering and Applications Symposium\'\' (IDEAS’99), IEEE Computer Society Press, Montrteal, Canada, pp. 321–331.</ref>\n<ref>Klaus Haller, Heiko Schuldt, Can Türker (2005): [http://portal.acm.org/citation.cfm?doid=1099554.1099563 "Decentralized coordination of transactional processes in peer-to-peer environments",] \'\'Proceedings of the 2005 ACM CIKM, International Conference on Information and Knowledge Management\'\', pp. 28-35, Bremen, Germany, October 31 - November 5, 2005, ISBN 1-59593-140-6</ref> and more recently in the related \'\'\'Re:GRIDiT\'\'\', \n<ref>Laura Cristiana Voicu, Heiko Schuldt, Fuat Akal, Yuri Breitbart, Hans Jörg Schek (2009): [http://dbis.cs.unibas.ch/publications/2009/grid2009/dbis_publication_view  "Re:GRIDiT – Coordinating Distributed Update Transactions on Replicated Data in the Grid"], \'\'10th IEEE/ACM International Conference on Grid Computing (Grid 2009)\'\', Banff, Canada, 2009/10.</ref>\n<ref>Laura Cristiana Voicu and Heiko Schuldt (2009): [http://dbis.cs.unibas.ch/publications/2009/clouddb09/dbis_publication_view  "How Replicated Data Management in the Cloud can benefit from a Data Grid Protocol — the Re:GRIDiT Approach"], \'\'Proceedings of the 1st International Workshop on Cloud Data Management (CloudDB 2009)\'\', Hong Kong, China, 2009/11.</ref>\nwhich is an approach for transaction management in the converging [[Grid computing]] and [[Cloud computing]]. \nSee more in \'\'[[The History of Commitment Ordering]]\'\'.\n\n==Relaxing global serializability==\nSome techniques have been developed for \'\'\'relaxed global serializability\'\'\' (i.e., they do not guarantee global serializability; see also \'\'[[Serializability#Relaxing serializability|Relaxing serializability]]\'\'). Among them (with several publications each):\n\n* \'\'Quasi serializability\'\'<ref name=Du1989>Weimin Du and Ahmed K. Elmagarmid (1989): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/DuE89.html  "Quasi Serializability: a Correctness Criterion for Global Concurrency Control in InterBase"], \'\'Proceedings of the Fifteenth International Conference on Very Large Data Bases\'\' (VLDB), August 22–25, 1989, Amsterdam, The Netherlands, pp. 347-355, Morgan Kaufmann, ISBN 1-55860-101-5</ref>\n* \'\'Two-level serializability\'\'<ref name=Shar1998 />\n\nWhile local (to a database system) relaxed serializability methods compromise \'\'serializability\'\' for performance gain (and are utilized only when the application can tolerate possible resulting inaccuracies, or its integrity is unharmed), it is unclear that various proposed \'\'relaxed global serializability\'\' methods which compromise \'\'global serializability\'\', provide any performance gain over \'\'commitment ordering\'\' which guarantees global serializability. Typically, the declared intention of such methods has not been performance gain over effective global serializability methods (which apparently have been unknown to the inventors), but rather correctness criteria alternatives due to lack of a known effective global serializability method. Oddly, some of them were introduced years after CO had been introduced, and some even quote CO without realizing that it provides an effective global serializability solution, and thus without providing any performance comparison with CO to justify them as alternatives to global serializability for some applications (e.g., \'\'Two-level serializability\'\'<ref name=Shar1998 />). \'\'Two-level serializability\'\' is even presented as a major global concurrency control method in a 2010 edition of a text-book on databases<ref name=Silber2010/> (authored by two of the original authors of Two-level serializability, where one of them, [[Abraham Silberschatz|Avi Silberschatz]], is also an author of the original \'\'[[The History of Commitment Ordering#AESO is modified to Strong recoverability (CO)|Strong recoverability]]\'\' articles). This book neither mentions CO nor references it, and strangely, apparently does not consider CO a valid \'\'Global serializability\'\' solution.\n\nAnother common reason nowadays for Global serializability relaxation is the requirement of [[availability]] of [[internet]] products and [[Internet service provider|services]]. This requirement is typically answered by large scale data [[Replication (computer science)|replication]]. The straightforward solution for synchronizing replicas\' updates of a same database object is including all these updates in a single atomic [[distributed transaction]]. However, with many replicas such a transaction is very large, and may span several [[computer]]s and [[computer network|networks]] that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.<ref name=Gray1996>{{cite conference\n | author = [[Jim Gray (computer scientist)|Gray, J.]]\n |author2=Helland, P. |author3=[[Patrick O\'Neil|O’Neil, P.]] |author4=[[Dennis Shasha|Shasha, D.]]\n | year = 1996\n | title = The dangers of replication and a solution\n | conference = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]\n | pages = 173–182\n | url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf\n | doi = 10.1145/233269.233330\n }}</ref>\nConsequently, [[Optimistic replication]] (Lazy replication) is often utilized (e.g., in many products and services by [[Google]], [[Amazon.com|Amazon]], [[Yahoo]], and alike), while Global serializability is relaxed and compromised for [[Eventual consistency]]. In this case relaxation is done only for applications that are not expected to be harmed by it.\n\nClasses of schedules defined by \'\'relaxed global serializability\'\' properties either contain the global serializability class, or are incomparable with it. What differentiates techniques for \'\'relaxed global conflict serializability\'\' (RGCSR) properties from those of \'\'relaxed conflict serializability\'\' (RCSR) properties that are not RGCSR is typically the different way \'\'global cycles\'\' (span two or more databases) in the \'\'global conflict graph\'\' are handled. No distinction between global and local cycles exists for RCSR properties that are not RGCSR. RCSR contains RGCSR. Typically RGCSR techniques eliminate local cycles, i.e., provide \'\'local serializability\'\' (which can be achieved effectively by regular, known [[concurrency control]] methods), however, obviously they do not eliminate all global cycles (which would achieve global serializability).\n\n==References==\n{{reflist|33em}}\n\n{{DEFAULTSORT:Global Serializability}}\n[[Category:Data management]]\n[[Category:Databases]]\n[[Category:Transaction processing]]\n[[Category:Concurrency control]]']
['Log trigger', '31397529', 'In [[relational database]]s, the \'\'\'Log trigger\'\'\' or \'\'\'History trigger\'\'\' is a mechanism for automatic recording of information about changes inserting or/and updating or/and deleting [[Row (database)|rows]] in a [[Table (database)|database table]].\n\nIt is a particular technique for [[Change data capture|change data capturing]], and in [[data warehousing]] for dealing with [[slowly changing dimension]]s.\n\n== Definition ==\n\nSuppose there is a [[Table (database)|table]] which we want to audit. This [[Table (database)|table]] contains the following [[Column (database)|columns]]:\n\n<code>Column1, Column2, ..., Columnn</code>\n\nThe [[Column (database)|column]] <code>Column1</code> is assumed to be the [[primary key]].\n\nThese [[Column (database)|columns]] are defined to have the following types:\n\n<code>Type1, Type2, ..., Typen</code>\n\nThe \'\'\'Log Trigger\'\'\' works writing the changes ([[Insert (SQL)|INSERT]], [[Update (SQL)|UPDATE]] and [[Delete (SQL)|DELETE]] operations) on the [[Table (database)|table]] in another, \'\'\'history table\'\'\', defined as following:\n\n<syntaxhighlight lang="sql">\nCREATE TABLE HistoryTable (\n   Column1   Type1,\n   Column2   Type2,\n      :        :\n   Columnn   Typen,\n\n   StartDate DATETIME,\n   EndDate   DATETIME\n)\n</syntaxhighlight>\n\nAs shown above, this new [[Table (database)|table]] contains the same [[Column (database)|columns]] as the original [[Table (database)|table]], and additionally two new [[Column (database)|columns]] of type <code>DATETIME</code>: <code>StartDate</code> and <code>EndDate</code>. This is known as [[Tuple-versioning|tuple versioning]]. These two additional [[Column (database)|columns]] define a period of time of "validity" of the data associated with a specified entity (the entity of the [[primary key]]), or in other words, it stores how the data were in the period of time between the <code>StartDate</code> (included) and <code>EndDate</code> (not included).\n\nFor each entity (distinct [[primary key]]) on the original [[Table (database)|table]], the following structure is created in the history [[Table (database)|table]]. Data is shown as example.\n\n[[File:example log trigger.png|center|example]]\n\nNotice that if they are shown chronologically the <code>EndDate</code> [[Column (database)|column]] of any [[Row (database)|row]] is exactly the <code>StartDate</code> of its successor (if any). It does not mean that both [[Row (database)|rows]] are common to that point in time, since -by definition- the value of <code>EndDate</code> is not included.\n\nThere are two variants of the \'\'\'Log trigger\'\'\', depending how the old values (DELETE, UPDATE) and new values (INSERT, UPDATE) are exposed to the trigger (it is RDBMS dependent):\n\n\'\'\'Old and new values as fields of a record data structure\'\'\'\n\n<syntaxhighlight lang="sql">\nCREATE TRIGGER HistoryTable ON OriginalTable FOR INSERT, DELETE, UPDATE AS\nDECLARE @Now DATETIME\nSET @Now = GETDATE()\n\n/* deleting section */\n\nUPDATE HistoryTable\n   SET EndDate = @Now\n WHERE EndDate IS NULL\n   AND Column1 = OLD.Column1\n\n/* inserting section */\n\nINSERT INTO HistoryTable (Column1, Column2, ...,Columnn, StartDate, EndDate) \nVALUES (NEW.Column1, NEW.Column2, ..., NEW.Columnn, @Now, NULL)\n</syntaxhighlight>\n\n\'\'\'Old and new values as rows of virtual tables\'\'\'\n\n<syntaxhighlight lang="sql">\nCREATE TRIGGER HistoryTable ON OriginalTable FOR INSERT, DELETE, UPDATE AS\nDECLARE @Now DATETIME\nSET @Now = GETDATE()\n\n/* deleting section */\n\nUPDATE HistoryTable\n   SET EndDate = @Now\n  FROM HistoryTable, DELETED\n WHERE HistoryTable.Column1 = DELETED.Column1\n   AND HistoryTable.EndDate IS NULL\n\n/* inserting section */\n\nINSERT INTO HistoryTable\n       (Column1, Column2, ..., Columnn, StartDate, EndDate)\nSELECT (Column1, Column2, ..., Columnn, @Now, NULL)\n  FROM INSERTED\n</syntaxhighlight>\n\n=== Compatibility notes ===\n\n* The function <code>GetDate()</code> is used to get the system date and time, a specific [[Relational database management system|RDBMS]] could either use another function name, or get this information by another way.\n* Several [[Relational database management system|RDBMS]] (DB2, MySQL) do not support that the same trigger can be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]). In such a case a trigger must be created for each operation; For an [[Insert (SQL)|INSERT]] operation only the \'\'inserting section\'\' must be specified, for a [[Delete (SQL)|DELETE]] operation only the \'\'deleting section\'\' must be specified, and for an [[Update (SQL)|UPDATE]] operation both sections must be present, just as it is shown above (the \'\'deleting section\'\' first, then the \'\'inserting section\'\'), because an [[Update (SQL)|UPDATE]] operation is logically represented as a [[Delete (SQL)|DELETE]] operation followed by an [[Insert (SQL)|INSERT]] operation.\n* In the code shown, the record data structure containing the old and new values are called <code>OLD</code> and <code>NEW</code>. On a specific [[Relational database management system|RDBMS]] they could have different names.\n* In the code shown, the virtual tables are called <code>DELETED</code> and <code>INSERTED</code>. On a specific [[Relational database management system|RDBMS]] they could have different names. Another [[Relational database management system|RDBMS]] (DB2) even let the name of these logical tables be specified.\n* In the code shown, comments are in C/C++ style, they could not be supported by a specific [[Relational database management system|RDBMS]], or a different syntax should be used.\n* Several [[Relational database management system|RDBMS]] require that the body of the trigger is enclosed between <code>BEGIN</code> and <code>END</code> keywords.\n\n=== [[Data warehousing]] ===\n\nAccording with the [[slowly changing dimension]] management methodologies, The \'\'\'log trigger\'\'\' falls into the following:\n\n* [[Slowly changing dimension#Type 2|Type 2]] ([[Tuple-versioning|tuple versioning]] variant)\n* [[Slowly changing dimension#Type 4|Type 4]] (use of history tables)\n\n== Implementation in common [[RDBMS]] ==\n\n=== [[IBM DB2]]<ref>"Database Fundamentals" by Nareej Sharma et al. (First Edition, Copyright IBM Corp. 2010)</ref> ===\n\n* A trigger cannot be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]), so a trigger must be created for each operation.\n* The old and new values are exposed as fields of a record data structures. The names of these records can be defined, in this example they are named as <code>O</code> for old values and <code>N</code> for new values.\n\n<syntaxhighlight lang="sql">\n-- Trigger for INSERT\nCREATE TRIGGER Database.TableInsert AFTER INSERT ON Database.OriginalTable\nREFERENCING NEW AS N\nFOR EACH ROW MODE DB2SQL\nBEGIN\n   DECLARE Now TIMESTAMP;\n   SET NOW = CURRENT TIMESTAMP;\n\n   INSERT INTO Database.HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)\n   VALUES (N.Column1, N.Column2, ..., N.Columnn, Now, NULL);\nEND;\n\n-- Trigger for DELETE\nCREATE TRIGGER Database.TableDelete AFTER DELETE ON Database.OriginalTable\nREFERENCING OLD AS O\nFOR EACH ROW MODE DB2SQL\nBEGIN\n   DECLARE Now TIMESTAMP;\n   SET NOW = CURRENT TIMESTAMP;\n\n   UPDATE Database.HistoryTable\n      SET EndDate = Now\n    WHERE Column1 = O.Column1\n      AND EndDate IS NULL;\nEND;\n\n-- Trigger for UPDATE\nCREATE TRIGGER Database.TableUpdate AFTER UPDATE ON Database.OriginalTable\nREFERENCING NEW AS N OLD AS O\nFOR EACH ROW MODE DB2SQL\nBEGIN\n   DECLARE Now TIMESTAMP;\n   SET NOW = CURRENT TIMESTAMP;\n\n   UPDATE Database.HistoryTable\n      SET EndDate = Now\n    WHERE Column1 = O.Column1\n      AND EndDate IS NULL;\n\n   INSERT INTO Database.HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)\n   VALUES (N.Column1, N.Column2, ..., N.Columnn, Now, NULL);\nEND;\n</syntaxhighlight>\n\n=== [[Microsoft SQL Server]]<ref>"Microsoft SQL Server 2008 - Database Development" by Thobias Thernström et al. (Microsoft Press, 2009)</ref> ===\n\n* The same trigger can be attached to all the [[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], and [[Update (SQL)|UPDATE]] operations.\n* Old and new values as rows of virtual tables named <code>DELETED</code> and <code>INSERTED</code>.\n\n<syntaxhighlight lang="sql">\nCREATE TRIGGER TableTrigger ON OriginalTable FOR DELETE, INSERT, UPDATE AS\n\nDECLARE @NOW DATETIME\nSET @NOW = CURRENT_TIMESTAMP\n\nUPDATE HistoryTable\n   SET EndDate = @now\n  FROM HistoryTable, DELETED\n WHERE HistoryTable.ColumnID = DELETED.ColumnID\n   AND HistoryTable.EndDate IS NULL\n\nINSERT INTO HistoryTable (ColumnID, Column2, ..., Columnn, StartDate, EndDate)\nSELECT ColumnID, Column2, ..., Columnn, @NOW, NULL\n  FROM INSERTED\n</syntaxhighlight>\n\n=== [[MySQL]] ===\n\n* A trigger cannot be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]), so a trigger must be created for each operation.\n* The old and new values are exposed as fields of a record data structures called <code>Old</code> and <code>New</code>.\n\n<syntaxhighlight lang="sql">\nDELIMITER $$\n\n/* Trigger  for INSERT */\nCREATE TRIGGER HistoryTableInsert AFTER INSERT ON OriginalTable FOR EACH ROW BEGIN\n   DECLARE N DATETIME;\n   SET N = now();\n    \n   INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)\n   VALUES (New.Column1, New.Column2, ..., New.Columnn, N, NULL);\nEND;\n\n/* Trigger for DELETE */\nCREATE TRIGGER HistoryTableDelete AFTER DELETE ON OriginalTable FOR EACH ROW BEGIN\n   DECLARE N DATETIME;\n   SET N = now();\n    \n   UPDATE HistoryTable\n      SET EndDate = N\n    WHERE Column1 = OLD.Column1\n      AND EndDate IS NULL;\nEND;\n\n/* Trigger for UPDATE */\nCREATE TRIGGER HistoryTableUpdate AFTER UPDATE ON OriginalTable FOR EACH ROW BEGIN\n   DECLARE N DATETIME;\n   SET N = now();\n\n   UPDATE HistoryTable\n      SET EndDate = N\n    WHERE Column1 = OLD.Column1\n      AND EndDate IS NULL;\n\n   INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)\n   VALUES (New.Column1, New.Column2, ..., New.Columnn, N, NULL);\nEND;\n</syntaxhighlight>\n\n=== [[Oracle Database|Oracle]] ===\n\n* The same trigger can be attached to all the [[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], and [[Update (SQL)|UPDATE]] operations.\n* The old and new values are exposed as fields of a record data structures called <code>:OLD</code> and <code>:NEW</code>.\n* It is necessary to test the nullity of the fields of the <code>:NEW</code> record that define the [[primary key]] (when a [[Delete (SQL)|DELETE]] operation is performed), in order to avoid the insertion of a new row with null values in all columns.\n\n<syntaxhighlight lang="sql">\nCREATE OR REPLACE TRIGGER TableTrigger\nAFTER INSERT OR UPDATE OR DELETE ON OriginalTable\nFOR EACH ROW\nDECLARE Now TIMESTAMP;\nBEGIN\n   SELECT CURRENT_TIMESTAMP INTO Now FROM Dual;\n\n   UPDATE HistoryTable\n      SET EndDate = Now\n    WHERE EndDate IS NULL\n      AND Column1 = :OLD.Column1;\n\n   IF :NEW.Column1 IS NOT NULL THEN\n      INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate) \n      VALUES (:NEW.Column1, :NEW.Column2, ..., :NEW.Columnn, Now, NULL);\n   END IF;\nEND;\n</syntaxhighlight>\n\n== Historic information ==\n\nTypically, [[Database dump|database backups]] are used to store and retrieve historic information. A [[Database dump|database backup]] is a security mechanism, more than an effective way to retrieve ready-to-use historic information.\n\nA (full) [[Database dump|database backup]] is only a snapshot of the data in specific points of time, so we could know the information of each snapshot, but we can know nothing between them. Information in [[Database dump|database backups]] is discrete in time.\n\nUsing the \'\'\'log trigger\'\'\' the information we can know is not discrete but continuous, we can know the exact state of the information in any point of time, only limited to the granularity of time provided with the <code>DATETIME</code> data type of the [[Relational database management system|RDBMS]] used.\n\n== Advantages ==\n\n* It is simple.\n* It is not a commercial product, it works with available features in common [[Relational database management system|RDBMS]].\n* It is automatic, once it is created, it works with no further human intervention.\n* It is not required to have good knowledge about the tables of the database, or the data model.\n* Changes in current programming are not required.\n* Changes in the current [[Table (database)|tables]] are not required, because log data of any [[Table (database)|table]] is stored in a different one.\n* It works for both programmed and ad hoc statements.\n* Only changes ([[Insert (SQL)|INSERT]], [[Update (SQL)|UPDATE]] and [[Delete (SQL)|DELETE]] operations) are registered, so the growing rate of the history tables are proportional to the changes.\n* It is not necessary to apply the trigger to all the tables on database, it can be applied to certain [[Table (database)|tables]], or certain [[Column (database)|columns]] of a [[Table (database)|table]].\n\n== Disadvantages ==\n\n* It does not automatically store information about the user producing the changes (information system user, not database user). This information might be provided explicitly. It could be enforced in information systems, but not in ad hoc queries.\n\n== Examples of use ==\n\n=== Getting the current version of a table ===\n\n<syntaxhighlight lang="sql">\nSELECT Column1, Column2, ..., Columnn\n  FROM HistoryTable\n WHERE EndDate IS NULL\n</syntaxhighlight>\n\nIt should return the same resultset of the whole original [[Table (database)|table]].\n\n=== Getting the version of a table in a certain point of time ===\n\nSuppose the <code>@DATE</code> variable contains the point or time of interest.\n\n<syntaxhighlight lang="sql">\nSELECT  Column1, Column2, ..., Columnn\n  FROM  HistoryTable\n WHERE  @Date >= StartDate\n   AND (@Date < EndDate OR EndDate IS NULL)\n</syntaxhighlight>\n\n=== Getting the information of an entity in a certain point of time ===\n\nSuppose the <code>@DATE</code> variable contains the point or time of interest, and the <code>@KEY</code> variable contains the [[primary key]] of the entity of interest.\n\n<syntaxhighlight lang="sql">\nSELECT  Column1, Column2, ..., Columnn\n  FROM  HistoryTable\n WHERE  Column1 = @Key\n   AND  @Date >= StartDate\n   AND (@Date <  EndDate OR EndDate IS NULL)\n</syntaxhighlight>\n\n=== Getting the history of an entity ===\n\nSuppose the <code>@KEY</code> variable contains the [[primary key]] of the entity of interest.\n\n<syntaxhighlight lang="sql">\nSELECT Column1, Column2, ..., Columnn, StartDate, EndDate\n  FROM HistoryTable\n WHERE Column1 = @Key\n ORDER BY StartDate\n</syntaxhighlight>\n\n=== Getting when and how an entity was created ===\n\nSuppose the <code>@KEY</code> variable contains the [[primary key]] of the entity of interest.\n\n<syntaxhighlight lang="sql">\nSELECT H2.Column1, H2.Column2, ..., H2.Columnn, H2.StartDate\n  FROM HistoryTable AS H2 LEFT OUTER JOIN HistoryTable AS H1\n    ON H2.Column1 = H1.Column1\n   AND H2.Column1 = @Key\n   AND H2.StartDate = H1.EndDate\n WHERE H2.EndDate IS NULL\n</syntaxhighlight>\n\n== Immutability of [[primary key]]s ==\n\nSince the trigger requires that [[primary key]] being the same throughout time, it is desirable to either ensure or maximize its immutability, if a [[primary key]] changed its value, the entity it represents would break its own history.\n\nThere are several options to achieve or maximize the [[primary key]] immutability:\n\n* Use of a [[Surrogate Key|surrogate key]] as a [[primary key]]. Since there is no reason to change a value with no meaning other than identity and uniqueness, it would never change.\n* Use of an immutable [[natural key]] as a [[primary key]]. In a good database design, a [[natural key]] which can change should not be considered as a "real" [[primary key]].\n* Use of a mutable [[natural key]] as a [[primary key]] (it is widely discouraged) where changes are propagated in every place where it is a [[foreign key]]. In such a case, the history table should be also affected.\n\n=== Alternatives ===\n\nSometimes the [[Slowly changing dimension]] is used as a method, this diagram is an example:\n[[File:Scd model.png|frame|right|Scd model]]\n\n== See also ==\n\n* [[RDBMS|Relational database]]\n* [[Primary key]]\n* [[Natural key]]\n* [[Surrogate key]]\n* [[Change data capture]]\n* [[Slowly changing dimension]]\n* [[Tuple-versioning|Tuple versioning]]\n\n== Notes ==\n\nThe Log trigger was written by [[Laurence Ruiz Ugalde|Laurence R. Ugalde]] to automatically generate history of transactional databases.\n\n==References==\n<references />\n\n{{DEFAULTSORT:Log Trigger}}\n[[Category:Computer data]]\n[[Category:Data management]]\n[[Category:Data modeling]]\n[[Category:Data warehousing]]']
['Category:Object-oriented database management systems', '2595964', 'Articles in this category are pure [[object-oriented database management system]]s.\n\n[[Category:Data management]]\n[[Category:Database management systems]]']
['Data deduplication', '17174890', '{{multiple issues|\n{{original research|date=February 2011}}\n{{More footnotes|date=September 2009}}\n}}\n\nIn [[computing]], \'\'\'data deduplication\'\'\' is a specialized [[data compression]] technique for eliminating duplicate copies of repeating data. Related and somewhat synonymous terms are \'\'\'intelligent (data) compression\'\'\' and \'\'\'[[single-instance storage|single-instance (data) storage]]\'\'\'. This technique is used to improve storage utilization and can also be applied to network data transfers to reduce the number of bytes that must be sent. In the deduplication process, unique chunks of data, or byte patterns, are identified and stored during a process of analysis. As the analysis continues, other chunks are compared to the stored copy and whenever a match occurs, the redundant chunk is replaced with a small reference that points to the stored chunk. Given that the same byte pattern may occur dozens, hundreds, or even thousands of times (the match frequency is dependent on the chunk size), the amount of data that must be stored or transferred can be greatly reduced.<ref>"[http://www.druva.com/blog/2009/01/09/understanding-data-deduplication/ Understanding Data Deduplication]" Druva, 2009. Retrieved 2013-2-13</ref>\n\nThis type of deduplication is different from that performed by standard file-compression tools, such as [[LZ77 and LZ78]]. Whereas these tools identify short repeated substrings inside individual files, the intent of storage-based data deduplication is to inspect large volumes of data and identify large sections – such as entire files or large sections of files – that are identical, in order to store only one copy of it. This copy may be additionally compressed by single-file compression techniques. For example, a typical email system might contain 100 instances of the same 1 MB ([[megabyte]]) file attachment. Each time the [[email]] platform is backed up, all 100 instances of the attachment are saved, requiring 100 MB storage space. With data deduplication, only one instance of the attachment is actually stored; the subsequent instances are referenced back to the saved copy for deduplication ratio of roughly 100 to 1.\n\n==Benefits==\n* Storage-based data deduplication reduces the amount of storage needed for a given set of files. It is most effective in applications where many copies of very similar or even identical data are stored on a single disk—a surprisingly common scenario. In the case of data backups, which routinely are performed to protect against data loss, most data in a given backup remain unchanged from the previous backup. Common backup systems try to exploit this by omitting (or [[hard link]]ing) files that haven\'t changed or storing [[Data differencing|differences]] between files.  Neither approach captures all redundancies, however. Hard-linking does not help with large files that have only changed in small ways, such as an email database;  differences only find redundancies in adjacent versions of a single file (consider a section that was deleted and later added in again, or a logo image included in many documents).\n* Network data deduplication is used to reduce the number of bytes that must be transferred between endpoints, which can reduce the amount of bandwidth required. See [[WAN optimization]] for more information.\n* Virtual servers benefit from deduplication because it allows nominally separate system files for each virtual server to be coalesced into a single storage space. At the same time, if a given server customizes a file, deduplication will not change the files on the other servers—something that alternatives like hard links or shared disks do not offer.  Backing up or making duplicate copies of virtual environments is similarly improved.\n\n==Deduplication overview==\nDeduplication may occur "in-line", as data is flowing, or "post-process" after it has been written.\n\n===Post-process deduplication===\nWith post-process deduplication, new data is first stored on the storage device and then a process at a later time will [[analysis|analyze]] the data looking for duplication. The benefit is that there is no need to wait for the hash calculations and lookup to be completed before storing the data, thereby ensuring that store performance is not degraded. Implementations offering policy-based operation can give users the ability to defer optimization on "active" files, or to process files based on type and location. One potential drawback is that duplicate data may be unnecessarily stored for a short time, which can be problematic if the system is nearing full capacity.\n\n===In-line deduplication===\nAlternatively, deduplication hash calculations can be done in real-time as data enters the target device. If the storage system identifies a block which it has already stored, only a reference to the existing block is stored, rather than the whole new block.\n\nThe advantage of in-line deduplication over post-process deduplication is that it requires less storage, since duplicate data is never stored.  On the negative side, it is frequently argued{{by whom|date=August 2016}} that because hash calculations and lookups take so long, [[data ingestion]] can be slower, thereby reducing the backup throughput of the device.  However, certain vendors with in-line deduplication have demonstrated equipment with similar performance to their post-process deduplication counterparts{{according to whom|date=April 2015}}.\n\nData coming in is stored into "lining space" before it hits real storage blocks. On SSD disks lining space is provided using [[Non-volatile random-access memory|NVRAM]] which is not cost efficient{{according to whom|date=August 2016}}.\n\nPost-process and in-line deduplication methods are often heavily debated.<ref>{{cite web|url=http://www.backupcentral.com/content/view/134/47/ |title=In-line or post-process de-duplication? (updated 6-08) |publisher=Backup Central |date= |accessdate=2009-10-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20091206035054/http://www.backupcentral.com:80/content/view/134/47 |archivedate=2009-12-06 |df= }}</ref><ref>{{cite web|url=http://searchdatabackup.techtarget.com/tip/0,289483,sid187_gci1315295,00.html |title=Inline vs. post-processing deduplication appliances |publisher=Searchdatabackup.techtarget.com |date= |accessdate=2009-10-16}}</ref>\n\n===Data formats===\n[[SNIA Dictionary]] identifies two methods:\n* content-agnostic data deduplication - a data deduplication method that does not require awareness of specific application data formats. \n* content-aware data deduplication - a data deduplication method that leverages knowledge of specific application data formats.\n\n===Source versus target deduplication===\nAnother way to classify data deduplication methods is according to where they occur. Deduplication occurring close to where data is created, is often referred to{{according to whom|date=August 2016}} as "source deduplication". When it occurs near where the data is stored, it is commonly called "target deduplication".\n\n* Source deduplication ensures that data on the data source is deduplicated.  This generally takes place directly within a file system.<ref>{{cite web|url=http://www.microsoft.com/windowsserver2008/en/us/WSS08/SIS.aspx |title=Windows Server 2008: Windows Storage Server 2008 |publisher=Microsoft.com |date= |accessdate=2009-10-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20091004073508/http://www.microsoft.com:80/windowsserver2008/en/us/WSS08/SIS.aspx |archivedate=2009-10-04 |df= }}</ref><ref>{{cite web|url=http://www.netapp.com/us/products/platform-os/dedupe.html |title=Products - Platform OS |publisher=NetApp |date= |accessdate=2009-10-16}}</ref>  The file system will periodically scan new files creating hashes and compare them to hashes of existing files.   When files with same hashes are found then the file copy is removed and the new file points to the old file.  Unlike [[hard links]] however, duplicated files are considered to be separate entities and if one of the duplicated files is later modified, then using a system called [[copy-on-write]] a copy of that file or changed block is created.  The deduplication process is transparent to the users and backup applications.  Backing up a deduplicated file system will often cause duplication to occur resulting in the backups being bigger than the source data.\n* Target deduplication is the process of removing duplicates when the data was not generated at that location.  Example of this would be a server connected to a SAN/NAS, The SAN/NAS would be a target for the server (Target deduplication).  The server is not aware of any deduplication, the server is also the point of data generation.\n\nA second example would be backup. If you have a backup system with deduplication. Generally this will be a backup store such as a data repository or a [[virtual tape library]].\n\n===Deduplication methods===\nOne of the most common forms of data deduplication implementations works by comparing chunks of data to detect duplicates. For that to happen, each chunk of data is assigned an identification, calculated by the software, typically using cryptographic hash functions. In many implementations, the assumption is made that if the identification is identical, the data is identical, even though this cannot be true in all cases due to the [[pigeonhole principle]]; other implementations do not assume that two blocks of data with the same identifier are identical, but actually verify that data with the same identification is identical.<ref>An example of an implementation that checks for identity rather than assuming it is described in [http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=1&f=G&l=50&co1=AND&d=PG01&s1=shnelvar&OS=shnelvar&RS=shnelvar "US Patent application # 20090307251"].</ref> If the software either assumes that a given identification already exists in the deduplication namespace or actually verifies the identity of the two blocks of data, depending on the implementation, then it will replace that duplicate chunk with a link.\n\nOnce the data has been deduplicated, upon read back of the file, wherever a link is found, the system simply replaces that link with the referenced data chunk. The deduplication process is intended to be transparent to end users and applications.\n\nCommercial deduplication implementations differ by their chunking methods and architectures.\n\n* Chunking.  In some systems, chunks are defined by physical layer constraints (e.g. 4KB block size in [[Write Anywhere File Layout|WAFL]]). In some systems only complete files are compared, which is called [[single-instance storage]] or SIS. The most intelligent (but CPU intensive) method to chunking is generally considered to be sliding-block. In sliding block, a window is passed along the file stream to seek out more naturally occurring internal file boundaries.\n* Client backup deduplication. This is the process where the deduplication hash calculations are initially created on the source (client) machines.  Files that have identical hashes to files already in the target device are not sent, the target device just creates appropriate internal links to reference the duplicated data.  The benefit of this is that it avoids data being unnecessarily sent across the network thereby reducing traffic load.\n* Primary storage and secondary storage. By definition, primary storage systems are designed for optimal performance, rather than lowest possible cost.  The design criteria for these systems is to increase performance, at the expense of other considerations.  Moreover, primary storage systems are much less tolerant of any operation that can negatively impact performance.  Also by definition, secondary storage systems contain primarily duplicate, or secondary copies of data.  These copies of data are typically not used for actual production operations and as a result are more tolerant of some performance degradation, in exchange for increased efficiency.\n\nTo date, data deduplication has predominantly been used with secondary storage systems.  The reasons for this are two-fold.  First, data deduplication requires overhead to discover and remove the duplicate data.  In primary storage systems, this overhead may impact performance.  The second reason why deduplication is applied to secondary data, is that secondary data tends to have more duplicate data.  Backup application in particular commonly generate significant portions of duplicate data over time.\n\nData deduplication has been deployed successfully with primary storage in some cases where the system design does not require significant overhead, or impact performance.\n\n==Drawbacks and concerns==\nWhenever data is transformed, concerns arise about potential loss of data.  By definition, data deduplication systems store data differently from how it was written.  As a result, users are concerned with the integrity of their data.  The various methods of deduplicating data all employ slightly different techniques.  However, the integrity of the data will ultimately depend upon the design of the deduplicating system, and the quality used to implement the algorithms.  As the technology has matured over the past decade, the integrity of most of the major products has been well proven .{{citation needed|date=November 2012}}\n\nOne method for deduplicating data relies on the use of [[cryptographic hash function]]s to identify duplicate segments of data. If two different pieces of information generate the same hash value, this is known as a [[collision (computer science)|collision]].  The probability of a collision depends upon the hash function used, and although the probabilities are small, they are always non zero. Thus, the concern arises that [[data corruption]] can occur if a [[hash collision]] occurs, and additional means of verification are not used to verify whether there is a difference in data, or not. Both in-line and post-process architectures may offer bit-for-bit validation of original data for guaranteed data integrity.<ref>{{citation |url=http://www.evaluatorgroup.com/document/data-de-duplication-%E2%80%93why-when-where-and-how-infostor-article-by-russ-fellows/ |title=Data Deduplication - Why, When, Where and How |publisher=Evaluator Group |accessdate=2011-07-05}}</ref> The hash functions used include standards such as [[SHA-1]], [[SHA-256]] and others.\n\nThe computational resource intensity of the process can be a drawback of data deduplication.  However, this is rarely an issue for stand-alone devices or appliances, as the computation is completely offloaded from other systems.  This can be an issue when the deduplication is embedded within devices providing other services. To improve performance, many systems utilize both weak and strong hashes.  Weak hashes are much faster to calculate but there is a greater risk of a hash collision.  Systems that utilize weak hashes will subsequently calculate a strong hash and will use it as the determining factor to whether it is actually the same data or not. Note that the system overhead associated with calculating and looking up hash values is primarily a function of the deduplication workflow. The reconstitution of files does not require this processing and any incremental performance penalty associated with re-assembly of data chunks is unlikely to impact application performance.\n\nAnother area of concern with deduplication is the related effect on [[Snapshot (computer storage)|snapshots]], [[backup]], and [[archival]], especially where deduplication is applied against primary storage (for example inside a [[Network-attached storage|NAS]] filer).{{elucidate|date=December 2011}} Reading files out of a storage device causes full reconstitution of the files (also known as rehydration), so any secondary copy of the data set is likely to be larger than the primary copy. In terms of snapshots, if a file is snapshotted prior to deduplication, the post-deduplication snapshot will preserve the entire original file. This means that although storage capacity for primary file copies will shrink, capacity required for snapshots may expand dramatically.\n\nAnother concern is the effect of compression and encryption. Although deduplication is a version of compression, it works in tension with traditional compression. Deduplication achieves better efficiency against smaller data chunks, whereas compression achieves better efficiency against larger chunks. The goal of encryption is to eliminate any discernible patterns in the data. Thus encrypted data cannot be deduplicated, even though the underlying data may be redundant. Deduplication ultimately reduces redundancy.  If this was not expected and planned for, this may ruin the underlying reliability of the system.  (Compare this, for example, to the [[LOCKSS]] storage architecture that achieves reliability through multiple copies of data.)\n\nScaling has also been a challenge for deduplication systems because ideally, the scope of deduplication needs to be shared across storage devices. If there are multiple disk backup devices in an infrastructure with discrete deduplication, then space efficiency is adversely affected. A deduplication shared across devices preserves space efficiency, but is technically challenging from a reliability and performance perspective.{{citation needed|date=December 2011}}\n\nAlthough not a shortcoming of data deduplication, there have been data breaches{{citation needed|date=August 2016}} when insufficient security and access validation procedures are used with large repositories of deduplicated data.  In some systems, as typical with cloud storage{{citation needed|date=August 2016}}, an attacker can retrieve data owned by others by knowing or guessing the hash value of the desired data.<ref>{{cite journal |title=A Cloud You Can Trust |publisher=[[IEEE]] |work=[[IEEE Spectrum]] |accessdate=2011-12-21 |url=http://spectrum.ieee.org/computing/networks/a-cloud-you-can-trust |author1=CHRISTIAN CACHIN |author2=MATTHIAS SCHUNTER |date=December 2011}}</ref>\n\n==See also==\n* [[Capacity optimization]]\n* [[Cloud storage]]\n* [[Single-instance storage]]\n* [[Content-addressable storage]]\n* [[Delta encoding]]\n* [[Linked data]]\n* [[Pointer (computer programming)|Pointer]]\n* [[Record linkage]]\n* [[Identity resolution]]\n* [[Convergent encryption]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* Biggar, Heidi(2007.12.11). [http://wayback.archive.org/web/20120325005645/http://www.infostor.com/webcast/display_webcast.cfm?ID=540 WebCast: The Data Deduplication Effect]\n* Fellows, Russ(Evaluator Group, Inc.) [http://www.evaluatorgroup.com/document/data-de-duplication-%E2%80%93why-when-where-and-how-infostor-article-by-russ-fellows/ Data Deduplication, why when where and how?]\n* [http://wayback.archive.org/web/20120328022229/http://www.tacoma.washington.edu/tech/docs/research/gradresearch/MSpiz.pdf Using Latent Semantic Indexing for Data Deduplication].\n* [http://www.forbes.com/2009/08/08/exagrid-storage-data-technology-cio-network-tape.html A Better Way to Store Data].\n* [http://www.eweek.com/c/a/Database/What-Is-the-Difference-Between-Data-Deduplication-File-Deduplication-and-Data-Compression What Is the Difference Between Data Deduplication, File Deduplication, and Data Compression?] - Database from eWeek\n* [http://www.snia.org/forums/dmf/programs/data_protect_init/ddsrsig/ SNIA DDSR SIG] * * [http://wayback.archive.org/web/20120322084240/http://www.snia.org/forums/dmf/knowledge/white_papers_and_reports/Understanding_Data_Deduplication_Ratios-20080718.pdf Understanding Data Deduplication Ratios]\n* [http://public.dhe.ibm.com/common/ssi/ecm/en/tsu12345usen/TSU12345USEN.PDF Data Footprint Reduction Technology Whitepaper]\n* [http://www.itnext.in/content/doing-more-less.html Doing More with Less by Jatinder Singh]\n* [http://www.sersc.org/journals/IJSIA/vol7_no5_2013/38.pdf Byte Index Chunking Algorithm for Data Deduplication]\n\n{{DEFAULTSORT:Data Deduplication}}\n[[Category:Data management]]\n[[Category:Data compression]]']
['Category:Data mapping', '34275960', '{{catmain|Data mapping}}\n\n[[Category:Data management]]']
['Database server', '815760', '{{refimprove|date=September 2014}}\nA \'\'\'database server\'\'\' is a [[computer program]] that provides [[database]] services to other computer programs or to [[computer]]s, as defined by the [[client–server]] [[software modeling|model]].{{cn|date=January 2017}} The term may also refer to a computer dedicated to running such a program. [[Database management system]]s frequently provide database-server functionality, and some [[database management system]]s (DBMSs) (such as [[MySQL]]) rely exclusively on the client–server model for database access.\n\nUsers access a database server either through a "[[Front and back ends|front end]]" running on the user\'s computer - which displays requested data - or through the "[[Front and back ends|back end]]", which runs on the server and handles tasks such as data analysis and storage.\n\nIn a [[Master-slave (technology)|master-slave]] model, database master servers are central and primary locations of data while database slave servers are synchronized backups of the master acting as [[proxy server|proxies]].\n\nMost database servers respond to a [[query language]]. Each database understands its query language and converts each submitted [[query (disambiguation) | query]] to server-readable form and executes it to retrieve results.\n\nExamples of proprietary database servers include [[Oracle Database|Oracle]], [[IBM DB2|DB2]], [[Informix]], and [[Microsoft SQL Server]]. Examples of [[GNU General Public Licence]] database servers include [[Ingres (database)|Ingres]] and [[MySQL]].  Every server uses its own query logic and structure. The [[SQL]] (Structured Query Language) query language is more or less the same on all [[relational database]] servers.\n\n[[DB-Engines]] lists over 200 DBMSs in its ranking.<ref>\n{{cite web\n|url= http://db-engines.com/en/ranking \n|title= DB-Engines Ranking \n|publisher= DB-Engines.com \n|date= 2013-12-01 \n|accessdate= 2013-12-28\n}}\n</ref>\n\n==History ==\nThe foundations for modeling large sets of data were first introduced by [[Charles Bachman]] in 1969.<ref name="dbhist">[http://knol.google.com/k/databases-history-early-development# Databases - History & Early Development]</ref> Bachman introduced [[Data structure diagram|Data Structure Diagrams (DSDs)]] as a means to graphically represent data. DSDs provided a means to represent the relationships between different data entities. In 1970, [[Edgar F. Codd|Codd]] introduced the concept that users of a database should be ignorant of the "inner workings" of the database.<ref name="dbhist"/> Codd proposed the "relational view" of data which later evolved into the [[Relational Model]] which most databases use today. In 1971, the Database Task Report Group of [[CODASYL]] (the driving force behind the development of the programming language [[COBOL]]) first proposed a "data description language for describing a database, a data description language for describing that part of the data base known to a program, and a data manipulation language." <ref name="dbhist"/> Most of the research and development of databases focused on the relational model during the 1970s.\n\nIn 1975 Bachman demonstrated how the relational model and the data structure set were similar and "congruent" ways of structuring data while working for the [[Honeywell]].<ref name="dbhist"/> The [[Entity-relationship model]] was first proposed in its current form by [[Peter Chen]] in 1976 while he was conducting research at [[MIT]].<ref>[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.1085 The Entity-Relationship Model: Toward a Unified View of Data (1976)]</ref> This model became the most frequently used model to describe relational databases. Chen was able to propose a model that was superior to the navigational model and was more applicable to the "real world" than the relational model proposed by Codd.<ref name="dbhist"/>\n\n== References ==\n{{Reflist}}\n\n==See also==\n* [[Replication (computer science)#Database replication|Database replication]]\n\n{{Database}}\n\n[[Category:Data management]]\n[[Category:Servers (computing)]]\n[[Category:Databases]]']
['Data grid', '35951900', '[[File:High Level View Data Grid V1.jpg|200px|right|High Level View Data Grid Topology]]\n\nA \'\'\'data grid\'\'\' is an [[architecture]] or set of services that gives individuals or groups of users the ability to access, modify and transfer extremely large amounts of geographically distributed [[data]] for research purposes.<ref>Allcock, Bill; Chervenak, Ann; Foster, Ian; et al. Data Grid tools: enabling science on big distributed data</ref> Data grids make this possible through a host of [[middleware]] [[Application software|applications]] and [[Service (systems architecture)|services]] that pull together data and [[Resource (computer science)|resources]] from multiple [[administrative domain]]s and then present it to users upon request. The data in a data grid can be located at a single site or multiple sites where each site can be its own administrative domain governed by a set of security restrictions as to who may access the data.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.37</ref> Likewise, multiple [[replica]]s of the data may be distributed throughout the grid outside their original administrative domain and the security restrictions placed on the original data for who may access it must be equally applied to the replicas.<ref>Shorfuzzaman, Mohammad; Graham, Peter; Eskicioglu, Rasit. Adaptive replica placement in hierarchical data grids. p.15</ref> Specifically developed data grid middleware is what handles the integration between users and the data they request by controlling access while making it available as efficiently as possible. The diagram to the right depicts a high level view of a data grid.\n\n==Middleware==\nMiddleware provides all the services and applications necessary for efficient management of [[dataset]]s and [[Computer file|files]] within the data grid while providing users quick access to the datasets and files.<ref>Padala, Pradeep. A survey of data middleware for Grid systems p.1</ref> There are a number of concepts and tools that must be available to make a data grid operationally viable. However, at the same time not all data grids require the same capabilities and services because of differences in access requirements, security and location of resources in comparison to users. In any case, most data grids will have similar middleware services that provide for a universal [[namespace|name space]], data transport service, data access service, data replication and resource management service. When taken together, they are key to the data grids functional capabilities.\n\n===Universal namespace===\nSince sources of data within the data grid will consist of data from multiple separate systems and [[Computer network|networks]] using different file [[naming convention]]s, it would be difficult for a user to locate data within the data grid and know they retrieved what they needed based solely on existing physical file names (PFNs). A universal or unified name space makes it possible to create logical file names (LFNs) that can be referenced within the data grid that map to PFNs.<ref>Padala, Pradeep. A survey of data middleware for Grid systems</ref> When an LFN is requested or queried, all matching PFNs are returned to include possible replicas of the requested data. The end user can then choose from the returned results the most appropriate replica to use. This service is usually provided as part of a management system known as a [[Storage Resource Broker]] (SRB).<ref>Arcot, Rajasekar; Wan, Michael; Moore, Reagan; Schroeder, Wayne; Kremenek. Storage resource broker – managing distributed data in a grid</ref> Information about the locations of files and mappings between the LFNs and PFNs may be stored in a [[metadata]] or replica catalogue.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.11</ref> The replica catalogue would contain information about LFNs that map to multiple replica PFNs.\n\n===Data transport service===\nAnother middleware service is that of providing for data transport or data transfer. Data transport will encompass multiple functions that are not just limited to the transfer of bits, to include such items as fault tolerance and data access.<ref>Coetzee, Serena. Reference model for a data grid approach to address data in a dynamic SDI p.16</ref> Fault tolerance can be achieved in a data grid by providing mechanisms that ensures data transfer will resume after each interruption until all requested data is received.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.21</ref> There are multiple possible methods that might be used to include starting the entire transmission over from the beginning of the data to resuming from where the transfer was interrupted. As an example, [[GridFTP]] provides for fault tolerance by sending data from the last acknowledged byte without starting the entire transfer from the beginning.\n\nThe data transport service also provides for the low-level access and connections between [[Host (network)|hosts]] for file transfer.<ref>Allcock, Bill; Foster,Ian; Nefedova, Veronika; Chervenak, Ann; Deelman, Ewa; Kesselman, Carl. High-performance remote access to climate simulation data: A challenge problem for data grid technologies.</ref> The data transport service may use any number of modes to implement the transfer to include parallel data transfer where two or more data streams are used over the same [[Channel (communications)|channel]] or striped data transfer where two or more steams access different blocks of the file for simultaneous transfer to also using the underlying built-in capabilities of the network hardware or specifically developed [[Protocol (object-oriented programming)|protocols]] to support faster transfer speeds.<ref>Izmailov, Rauf; Ganguly, Samrat; Tu, Nan. Fast parallel file replication in data grid p.2</ref> The data transport service might optionally include a [[network overlay]] function to facilitate the routing and transfer of data as well as file [[I/O]] functions that allow users to see remote files as if they were local to their system. The data transport service hides the complexity of access and transfer between the different systems to the user so it appears as one unified data source.\n\n===Data access service===\nData access services work hand in hand with the data transfer service to provide security, access controls and management of any data transfers within the data grid.<ref>Raman, Vijayshankar; Narang, Inderpal; Crone, chris; Hass, Laura; Malaika, Susan. Services for data access and data processing on grids</ref> Security services provide mechanisms for authentication of users to ensure they are properly identified. Common forms of security for authentication can include the use of passwords or [[Kerberos (protocol)]]. Authorization services are the mechanisms that control what the user is able to access after being identified through authentication. Common forms of authorization mechanisms can be as simple as file permissions. However, need for more stringent controlled access to data is done using [[Access Control List]]s (ACLs), [[Role-Based Access Control]] (RBAC) and Tasked-Based Authorization Controls (TBAC).<ref>Thomas, R. K. and Sandhu R. S. Task-based authorization controls (tbac): a family of models for active and enterprise-oriented authorization management</ref> These types of controls can be used to provide granular access to files to include limits on access times, duration of access to granular controls that determine which files can be read or written to. The final data access service that might be present to protect the confidentiality of the data transport is encryption.<ref>Sreelatha, Malempati. Grid based approach for data confidentiality. p.1</ref> The most common form of encryption for this task has been the use of [[Transport Layer Security|SSL]] while in transport. While all of these access services operate within the data grid, access services within the various administrative domains that host the datasets will still stay in place to enforce access rules. The data grid access services must be in step with the administrative domains access services for this to work.\n\n===Data replication service===\nTo meet the needs for scalability, fast access and user collaboration, most data grids support replication of datasets to points within the distributed storage architecture.<ref>Chervenak, Ann; Schuler, Robert; Kesselman, Carl; Koranda, Scott; Moe, Brian. Wide area data replication for scientific collaborations</ref> The use of replicas allows multiple users faster access to datasets and the preservation of bandwidth since replicas can often be placed strategically close to or within sites where users need them. However, replication of datasets and creation of replicas is bound by the availability of storage within sites and bandwidth between sites. The replication and creation of replica datasets is controlled by a replica management system. The replica management system determines user needs for replicas based on input requests and creates them based on availability of storage and bandwidth.<ref>Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments</ref> All replicas are then cataloged or added to a directory based on the data grid as to their location for query by users. In order to perform the tasks undertaken by the replica management system, it needs to be able to manage the underlying storage infrastructure. The data management system will also ensure the timely updates of changes to replicas are propagated to all nodes.\n\n====Replication update strategy====\nThere are a number of ways the replication management system can handle the updates of replicas. The updates may be designed around a centralized model where a single master replica updates all others, or a decentralized model, where all peers update each other.<ref>Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments</ref> The topology of node placement may also influence the updates of replicas. If a hierarchy topology is used then updates would flow in a tree like structure through specific paths. In a flat topology it is entirely a matter of the peer relationships between nodes as to how updates take place. In a hybrid topology consisting of both flat and hierarchy topologies updates may take place through specific paths and between peers.\n\n====Replication placement strategy====\nThere are a number of ways the replication management system can handle the creation and placement of replicas to best serve the user community. If the storage architecture supports replica placement with sufficient site storage, then it becomes a matter of the needs of the users who access the datasets and a strategy for placement of replicas.<ref>Padala, Pradeep. A survey of data middleware for Grid systems</ref> There have been numerous strategies proposed and tested on how to best manage replica placement of datasets within the data grid to meet user requirements. There is not one universal strategy that fits every requirement the best. It is a matter of the type of data grid and user community requirements for access that will determine the best strategy to use. Replicas can even be created where the files are encrypted for confidentiality that would be useful in a research project dealing with medical files.<ref>Kranthi, G. and Rekha, D. Shashi. Protected data objects replication in data grid p.40</ref> The following section contains several strategies for replica placement.\n\n=====Dynamic replication=====\nDynamic replication is an approach to placement of replicas based on popularity of the data.<ref>Belalem, Ghalem and Meroufel, Bakhta. Management and placement of replicas in a hierarchical data grid</ref> The method has been designed around a hierarchical replication model. The data management system keeps track of available storage on all nodes. It also keeps track of requests (hits) for which data clients (users) in a site are requesting. When the number of hits for a specific dataset exceeds the replication threshold it triggers the creation of a replica on the server that directly services the user’s client. If the direct servicing server known as a father does not have sufficient space, then the father’s father in the hierarchy is then the target to receive a replica and so on up the chain until it is exhausted. The data management system algorithm also allows for the dynamic deletion of replicas that have a null access value or a value lower than the frequency of the data to be stored to free up space. This improves system performance in terms of response time, number of replicas and helps load balance across the data grid. This method can also use dynamic algorithms that determine whether the cost of creating the replica is truly worth the expected gains given the location.<ref>Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments</ref>\n\n=====Adaptive replication=====\nThis method of replication like the one for dynamic replication has been designed around a hierarchical replication model found in most data grids. It works on a similar algorithm to dynamic replication with file access requests being a prime factor in determining which files should be replicated. A key difference, however, is the number and frequency of replica creations is keyed to a dynamic threshold that is computed based on request arrival rates from clients over a period of time.<ref>Shorfuzzaman, Mohammad; Graham, Peter; Eskicioglu, Rasit. Adaptive replica placement in hierarchical data grids</ref> If the number of requests on average exceeds the previous threshold and shows an upward trend, and storage utilization rates indicate capacity to create more replicas, more replicas may be created. As with dynamic replication, the removal of replicas that have a lower threshold that were not created in the current replication interval can be removed to make space for the new replicas.\n\n=====Fair-share replication=====\nLike the adaptive and dynamic replication methods before, fair-share replication is based on a hierarchical replication model. Also, like the two before, the popularity of files play a key role in determining which files will be replicated. The difference with this method is the placement of the replicas is based on access load and storage load of candidate servers.<ref>Rasool, Qaisar; Li, Jianzhong; Oreku, George S.; Munir, Ehsan Ullah. Fair-share replication in data grid</ref> A candidate server may have sufficient storage space but be servicing many clients for access to stored files. Placing a replicate on this candidate could degrade performance for all clients accessing this candidate server. Therefore, placement of replicas with this method is done by evaluating each candidate node for access load to find a suitable node for the placement of the replica. If all candidate nodes are equivalently rated for access load, none or less accessed than the other, then the candidate node with the lowest storage load will be chosen to host the replicas. Similar methods to the other described replication methods are used to remove unused or lower requested replicates if needed. Replicas that are removed might be moved to a parent node for later reuse should they become popular again.\n\n=====Other replication=====\nThe above three replica strategies are but three of many possible replication strategies that may be used to place replicas within the data grid where they will improve performance and access. Below are some others that have been proposed and tested along with the previously described replication strategies.<ref>Ranganathan, Kavitha and Foster, Ian. Identifying dynamic replication strategies for a high performance data grid</ref> \n* \'\'\'Static\'\'\' – uses a fixed replica set of nodes with no dynamic changes to the files being replicated.\n* \'\'\'Best Client\'\'\' – Each node records number of requests per file received during a preset time interval; if the request number exceeds the set threshold for a file a replica is created on the best client, one that requested the file the most; stale replicas are removed based on another algorithm. \n* \'\'\'Cascading\'\'\' – Is used in a hierarchical node structure where requests per file received during a preset time interval is compared against a threshold. If the threshold is exceeded a replica is created at the first tier down from the root, if the threshold is exceeded again a replica is added to the next tier down and so on like a waterfall effect until a replica is placed at the client itself.\n* \'\'\'Plain Caching\'\'\' – If the client requests a file it is stored as a copy on the client.\n* \'\'\'Caching plus Cascading\'\'\' – Combines two strategies of caching and cascading.\n* \'\'\'Fast Spread\'\'\' – Also used in a hierarchical node structure this strategy automatically populates all nodes in the path of the client that requests a file.\n\n===Tasks scheduling and resource allocation===\nSuch characteristics of the data grid systems as large scale and heterogeneity require specific methods of tasks scheduling and resource allocation. To resolve the problem, majority of systems use extended classic methods of scheduling.<ref>Epimakhov, Igor; Hameurlain, Abdelkader ; Dillon, Tharam; Morvan, Franck. Resource Scheduling Methods for Query Optimization in Data Grid Systems</ref> Others invite fundamentally different methods based on incentives for autonomous nodes, like virtual money or reputation of a node.\nAnother specificity of data grids, dynamics, consists in the continuous process of connecting and disconnecting of nodes and local load imbalance during an execution of tasks. That can make obsolete or non-optimal results of initial resource allocation for a task. As a result, much of the data grids utilize execution-time adaptation techniques that permit the systems to reflect to the dynamic changes: balance the load, replace disconnecting nodes, use the profit of newly connected nodes, recover a task execution after faults.\n\n===Resource management system (RMS)===\nThe resource management system represents the core functionality of the data grid. It is the heart of the system that manages all actions related to storage resources. In some data grids it may be necessary to create a federated RMS architecture because of different administrative policies and a diversity of possibilities found within the data grid in place of using a single RMS. In such a case the RMSs in the federation will employ an architecture that allows for interoperability based on an agreed upon set of protocols for actions related to storage resources.<ref>Krauter, Klaus; Buyya, Rajkumar; Maheswaran, Muthucumaru. A taxonomy and survey of grid resource management systems for distributed computing</ref>\n\n====RMS functional capabilities====\n* Fulfillment of user and application requests for data resources based on type of request and policies; RMS will be able to support multiple policies and multiple requests concurrently\n* Scheduling, timing and creation of replicas\n* Policy and security enforcement within the data grid resources to include authentication, authorization and access \n* Support systems with different administrative policies to inter-operate while preserving site autonomy\n* Support quality of service (QoS) when requested if feature available\n* Enforce system fault tolerance and stability requirements\n* Manage resources, i.e. disk storage, network bandwidth and any other resources that interact directly or as part of the data grid \n* Manage trusts concerning resources in administrative domains, some domains may place additional restrictions on how they participate requiring adaptation of the RMS or federation.\n* Supports adaptability, extensibility, and scalability in relation to the data grid.\n\n==Topology==\n[[File:Data Grid Multiple Topologies 1.jpg|right|Possible Data Grid Topologies]]\nData grids have been designed with multiple topologies in mind to meet the needs of the scientific community. On the right are four diagrams of various topologies that have been used in data grids.<ref>Zhu, Lichun. Metadata management in grid database federation</ref> Each topology has a specific purpose in mind for where it will be best utilized. Each of these topologies is further explained below.\n\n\'\'\'Federation topology\'\'\' is the choice for institutions that wish to share data from already existing systems. It allows each institution control over their data. When an institution with proper authorization requests data from another institution it is up to the institution receiving the request to determine if the data will go to the requesting institution. The federation can be loosely integrated between institutions, tightly integrated or a combination of both.\n\n\'\'\'Monadic topology\'\'\' has a central repository that all collected data is fed into. The central repository then responds to all queries for data. There are no replicas in this topology as compared to others. Data is only accessed from the central repository which could be by way of a web portal. One project that uses this data grid topology is the [[Network for Earthquake Engineering Simulation| Network for Earthquake Engineering Simulation (NEES)]] in the United States.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.16</ref> This works well when all access to the data is local or within a single region with high speed connectivity.\n\n\'\'\'Hierarchical topology\'\'\' lends itself to collaboration where there is a single source for the data and it needs to be distributed to multiple locations around the world. One such project that will benefit from this topology would be [[CERN]] that runs the [[Large Hadron Collider]] that generates enormous amounts of data. This data is located at one source and needs to be distributed around the world to organizations that are collaborating in the project.\n\n\'\'\'Hybrid Topology\'\'\' is simply a configuration that contains an architecture consisting of any combination of the previous mentioned topologies. It is used mostly in situations where researchers working on projects want to share their results to further research by making it readily available for collaboration.\n\n==History==\nThe need for data grids was first recognized by the [[scientific community]] concerning [[climate modeling]], where [[terabyte]] and [[petabyte]] sized [[data set]]s were becoming the norm for transport between sites.<ref>Allcock, Bill; Foster,Ian; Nefedova, Veronika; Chervenak, Ann; Deelman, Ewa; Kesselman, Carl. High-performance remote access to climate simulation data: A challenge problem for data grid technologies.</ref> More recent research requirements for data grids have been driven by the [[Large Hadron Collider]] (LHC) at [[CERN]], the [[LIGO|Laser Interferometer Gravitational Wave Observatory (LIGO)]], and the [[Sloan Digital Sky Survey|Sloan Digital Sky Survey (SDSS)]]. These examples of scientific instruments produce large amounts of data that need to be accessible by large groups of geographically dispersed researchers.<ref>Allcock, Bill; Chervenak, Ann; Foster, Ian; et al. p.571</ref><ref>Tierney, Brian L. Data grids and data grid performance issues. p.7</ref> Other uses for data grids involve governments, hospitals, schools and businesses where efforts are taking place to improve services and reduce costs by providing access to dispersed and separate data systems through the use of data grids.<ref>Thibodeau, P. Governments plan data grid projects</ref>\n \nFrom its earliest beginnings, the concept of a Data Grid to support the scientific community was thought of as a specialized extension of the “grid” which itself was first envisioned as a way to link super computers into meta-computers.<ref>Heingartner, douglas. The grid: the next-gen internet</ref> However, that was short lived and the grid evolved into meaning the ability to connect computers anywhere on the web to get access to any desired files and resources, similar to the way electricity is delivered over a grid by simply plugging in a device. The device gets electricity through its connection and the connection is not limited to a specific outlet. From this the data grid was proposed as an integrating architecture that would be capable of delivering resources for distributed computations. It would also be able to service numerous to thousands of queries at the same time while delivering gigabytes to terabytes of data for each query. The data grid would include its own management infrastructure capable of managing all aspects of the data grids performance and operation across multiple wide area networks while working within the existing framework known as the web.<ref>Heingartner, douglas. The grid: the next-gen internet</ref> \n \nThe data grid has also been defined more recently in terms of usability; what must a data grid be able to do in order for it to be useful to the scientific community. Proponents of this theory arrived at several criteria.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.1</ref> One, users should be able to search and discover applicable resources within the data grid from amongst its many datasets. Two, users should be able to locate datasets within the data grid that are most suitable for their requirement from amongst numerous replicas. Three, users should be able to transfer and move large datasets between points in a short amount of time. Four, the data grid should provide a means to manage multiple copies of datasets within the data grid. And finally, the data grid should provide security with user access controls within the data grid, i.e. which users are allowed to access which data.\n\nThe data grid is an evolving technology that continues to change and grow to meet the needs of an expanding community. One of the earliest programs begun to make data grids a reality was funded by the [[DARPA|Defense Advanced Research Projects Agency (DARPA)]] in 1997 at the [[University of Chicago]].<ref>Globus. About the globus toolkit</ref> This research spawned by DARPA has continued down the path to creating open source tools that make data grids possible. As new requirements for data grids emerge projects like the [[Globus Toolkit]] will emerge or expand to meet the gap. Data grids along with the "Grid" will continue to evolve.\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n*{{cite journal\n|last1= Allcock\n|first1= Bill |last2= Chervenak |first2= Ann |last3= Foster |first3= Ian |last4= Kesselman |first4= Carl |last5= Livny |first5= Miron \n|year= 2005\n|title= Data Grid tools: enabling science on big distributed data\n|journal= Journal of Physics: Conference Series\n|volume= 16\n|pages= 571–575\n|publisher= Institute of Physics Publishing\n|doi= 10.1088/1742-6596/16/1/079\n|url= http://iopscience.iop.org/1742-6596/16/1/079\n|accessdate= April 15, 2012}}\n\n*{{cite journal\n|last1=Allcock\n|first1=Bill |last2=Foster |first2=Ian |last3=Nefedova  |first3= Veronika  l|last4= Chervenak |first4= Ann |last5= Deelman |first5= Ewa |last6= Kesselman |first6= Carl |last7= Lee |first7= Jason |last8= Sim |first8= Alex |last9= Shoshani |first9= Arie |last10= Drach |first10=Bob |last11= Williams |first11= Dean    \n|title= High-performance remote access to climate simulation data: A challenge problem for data grid technologies\n|work =\n|publisher =  [[ACM Press]]\n|year = 2001\n|citeseerx = 10.1.1.64.6603\n|format =\n|doi =\n|accessdate = <!-- April 20, 2012 --> }}\n\n*{{cite web\n |last1=Arcot \n |first1=Rajasekar \n |last2=Wan \n |first2=Michael \n |last3=Moore \n |first3=Reagan \n |last4=Schroeder \n |first4=Wayne \n |last5=Kremenek \n |first5=George \n |title=Storage resource broker – managing distributed data in a grid \n |work= \n |publisher= \n |date= \n |url=http://www.npaci.edu/DICE/Pubs/CSI-paper-sent.doc \n |format= \n |doi= \n |accessdate=April 28, 2012 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20060507193028/http://www.npaci.edu:80/DICE/Pubs/CSI-paper-sent.doc \n |archivedate=May 7, 2006 \n |df= \n}}\n\n*{{cite journal\n|last1= Belalem\n|first1= Ghalem |last2= Meroufel |first2= Bakhta \n|year= 2011\n|title= Management and placement of replicas in a hierarchical data grid \n|journal= International Journal of Distributed and Parallel Systems (IJDPS)\n|volume= 2\n|issue= 6\n|pages= 23–30\n|location =\n|publisher=\n|doi= 10.5121/ijdps.2011.2603\n|url= http://www.scribd.com/doc/75105419/Management-and-Placement-of-Replicas-in-a-Hierarchical-Data-Grid\n|accessdate= April 28, 2012}}\n\n*{{cite journal\n|last1= Chervenak\n|first1= A.|last2= Foster |first2= I. |last3= Kesselman |first3= C.|last4= Salisbury |first4= C. |last5= Tuecke |first5= S.\n|year= 2001\n|title= The data grid: towards an architecture for the distributed management and analysis of large scientific datasets \n|journal= Journal of Network and Computer Applications\n|volume= 23\n|issue= \n|pages= 187–200\n|location =\n|publisher=\n|doi= 10.1006/jnca.2000.0110\n|url= http://www.globus.org/alliance/publications/papers/JNCApaper.pdf\n|accessdate= April 11, 2012}}\n\n*{{cite web\n|last1= Chervenak\n|first1= Ann |last2= Schuler |first2= Robert |last3= Kesselman | first3= Carl |last4= Koranda |first4= Scott |last5= Moe |first5= Brian\n|title= Wide area data replication for scientific collaborations\n|work= |publisher = [[IEEE]]\n|date = November 14, 2005\n|url= http://www.globus.org/alliance/publications/papers/chervenakGrid2005.pdf\n|format=\n|doi=\n| accessdate = April 25, 2012 }}\n\n*{{cite journal\n |last1=Coetzee \n |first1=Serena \n |year=2012 \n |title=Reference model for a data grid approach to address data in a dynamic SDI \n |journal=Geoinformatica \n |volume=16 \n |issue=1 \n |pages=111–129 \n |location= \n |publisher= \n |doi=10.1007/s10707-011-0129-4 \n |url=http://web.up.ac.za/sitefiles/file/48/16053/Coetzee_2011_ReferenceModelForDataGridApproach(2).pdf \n |accessdate=April 28, 2012 \n}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n*{{cite conference\n|last1= Epimakhov\n|first1= Igor \n|last2= Hameurlain\n|first2= Abdelkader \n|last3= Dillon\n|first3= Tharam \n|last4= Morvan\n|first4= Franck \n|title= Resource Scheduling Methods for Query Optimization in Data Grid Systems \n|booktitle = Advances in Databases and Information Systems. 15th International Conference, ADBIS 2011\n|pages = 185–199\n|publisher = Springer Berlin Heidelberg\n|year = 2011\n|location = Vienna, Austria\n|url = http://link.springer.com/chapter/10.1007%2F978-3-642-23737-9_14\n|doi = 10.1007/978-3-642-23737-9_14\n|id = \n|accessdate = September 20, 2011 }}\n\n*{{cite web\n|last1= Globus\n|first1= \n|title= About the globus toolkit\n|work=\n|publisher= [[Globus Alliance|Globus]]\n|year= 2012\n|url= http://www.globus.org/toolkit/about.html\n|doi=\n|accessdate = May 27, 2012 }}\n\n*{{cite news\n |last1=Heingartner \n |first1=Douglas \n |title=The Grid: The Next-Gen Internet \n |work=Wired \n |publisher= \n |date=March 8, 2001 \n |url=http://www.wired.com/science/discoveries/news/2001/03/42230 \n |format= \n |doi= \n |accessdate=May 13, 2012 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20120504035536/http://www.wired.com:80/science/discoveries/news/2001/03/42230 \n |archivedate=May 4, 2012 \n |df= \n}}\n\n*{{cite web\n |last1=Izmailov \n |first1=Rauf \n |last2=Ganguly \n |first2=Samrat \n |last3=Tu \n |first3=Nan \n |title=Fast parallel file replication in data grid \n |work= \n |publisher= \n |year=2004 \n |url=http://www.cs.huji.ac.il/labs/danss/p2p/resources/fast-parallel-file-replication-on-data-grid.pdf \n |format= \n |doi= \n |accessdate=May 10, 2012 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20120421081052/http://www.cs.huji.ac.il/labs/danss/p2p/resources/fast-parallel-file-replication-on-data-grid.pdf \n |archivedate=April 21, 2012 \n |df= \n}}\n\n*{{cite journal\n |last1=Kranthi \n |first1=G. Aruna \n |last2=Rekha \n |first2=D. Shashi \n |year=2012 \n |title=Protected data objects replication in data grid \n |journal=International Journal of Network Security & Its Applications (IJNSA) \n |volume=4 \n |issue=1 \n |pages=29–41 \n |location= \n |publisher= \n |doi=10.5121/ijnsa.2012.4103 \n |url=http://journaldatabase.org/articles/protected_data_objects_replication.html \n |issn=0975-2307 \n |accessdate=April 1, 2012 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20131008004646/http://journaldatabase.org/articles/protected_data_objects_replication.html \n |archivedate=October 8, 2013 \n |df= \n}}\n\n*{{cite journal\n|last1= Krauter\n|first1= Klaus |last2= Buyya |first2= Rajkumar |last3= Maheswaran |first3= Muthucumaru\n|year= 2002\n|title= A taxonomy and survey of grid resource management systems for distributed computing\n|journal= Software Practice and Experience (SPE)\n|volume= 32\n|issue= 2\n|pages= 135–164 \n|location = \n|publisher= \n|doi=10.1002/spe.432\n|citeseerx = 10.1.1.38.2122\n|accessdate= <!-- April 17, 2012 -->}}\n\n\n*{{cite conference\n|last1= Lamehamedi\n|first1= Houda |last2= Szymanski |first2= Boleslaw |last3= Shentu |first3= Zujun |last4= Deelman |first4= Ewa  \n|title = Data replication strategies in grid environments\n|booktitle = Fifth International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP’02)\n|pages = 378–383\n|publisher = Press\n|year = 2002\n|location =\n|citeseerx = 10.1.1.11.5473\n|doi =\n|accessdate = <!-- April 5, 2012 --> }}\n\n*{{cite journal\n|last1= Padala\n|first1= Pradeep\n|title= A survey of data middleware for Grid systems\n|work =\n|publisher = \n|date = \n|citeseerx = 10.1.1.114.1901\n|format =\n|doi =\n|accessdate = <!-- April 28, 2012 --> }}\n\n*{{cite web\n|last1= Raman \n|first1= Vijayshankar |last2= Narang |first2= Inderpal |last3= Crone |first3= Chris |last4= Hass |first4= Laura |last5= Malaika |first5= Susan \n|title= Services for data access and data processing on grids\n|work =\n|publisher = \n|date = February 9, 2003\n|url = http://www.ogf.org/documents/GFD.14.pdf\n|format =\n|doi =\n|accessdate = May 10, 2012 }}\n\n*{{cite conference\n|last1= Ranganathan\n|first1= Kavitha | last2= Foster |first2= Ian\n|title= Identifying dynamic replication strategies for a high performance data grid\n|booktitle= In Proc. of the International Grid Computing Workshop\n|pages= 75–86\n|publisher = \n|year= 2001\n|location=\n|citeseerx = 10.1.1.20.6836\n|format=\n|doi= 10.1007/3-540-45644-9_8\n|accessdate = <!-- May 15, 2012 --> }}\n\n*{{cite journal\n|last1= Rasool\n|first1= Qaisar |last2= Li |first2= Jianzhong |last3= Oreku| first3= George S.|last4= Munir |first4= Ehsan Ullah\n|year= 2008\n|title= Fair-share replication in data grid\n|journal= Information Technology Journal\n|volume= 7\n|issue= 5\n|pages= 776–782\n|publisher=\n|doi= 10.3923/itj.2008.776.782\n|url= http://scialert.net/abstract/?doi=itj.2008.776.782\n|accessdate= April 27, 2012 }}\n\n*{{cite journal\n|last1= Shorfuzzaman\n|first1= Mohammad |last2= Graham |first2= Peter |last3= Eskicioglu |first3= Rasit \n|year= 2010\n|title= Adaptive replica placement in hierarchical data grids\n|journal= Journal of Physics: Conference Series\n|volume= 256\n|issue= 1\n|pages= 1–18\n|location =\n|publisher= [[IOP Publishing Ltd]]\n|doi= 10.1088/1742-6596/256/1/012020\n|url= http://iopscience.iop.org/1742-6596/256/1/012020\n|accessdate= April 15, 2012}}\n\n*{{cite journal\n|last1= Sreelatha\n|first1= Malempati\n|year= 2011\n|title= Grid based approach for data confidentiality \n|journal= International Journal of Computer Applications\n|volume= 25\n|issue= 9\n|pages= 1–5\n|location =\n|publisher=\n|doi= 10.5120/3063-4186\n|issn = 0975-8887\n|url= http://www.ijcaonline.org/volume25/number9/pxc3874186.pdf\n|accessdate= April 28, 2012}}\n\n*{{cite journal\n|last1= Thibodeau\n|first1=P.   \n|title= Governments plan data grid projects\n|journal= Computerworld\n|volume= 39\n|issue= 42\n|pages= 14\n|location= United States\n|publisher= Computerworld\n|date = May 30, 2005\n|url = http://www.computerworld.com/s/article/102119/Governments_Plan_Data_Grid_Projects\n|format =\n|doi=\n|issn= 0010-4841\n|accessdate = April 28, 2012 }}\n\n*{{cite web\n|last1= Thomas\n|first1= R. K. |last2= Sandhu |first2= R. S. \n|title= Task-based authorization controls (tbac): a family of models for active and enterprise-oriented authorization management\n|work =\n|publisher = \n|year = 1997\n|url = http://profsandhu.com/confrnc/ifip/i97tbac.pdf\n|format =\n|doi =\n|accessdate = April 28, 2012 }}\n\n*{{cite web\n|last1=Tierney\n|first1=Brian L.   \n|title= Data grids and data grid performance issues\n|work =\n|publisher = \n|year = 2000\n|url = http://www-didc.lbl.gov/presentations/CSC2000-tierney.pdf\n|format =\n|doi =\n|accessdate = April 28, 2012 }}\n\n*{{cite journal\n|last1= Venugopal\n|first1= Srikumar |last2= Buyya |first2= Rajkumar |last3= Ramamohanarao |first3= Kotagiri\n|year= 2006\n|title= A taxonomy of data grids for distributed data sharing, management and processing\n|journal= ACM Computing Surveys\n|volume= 38\n|issue= 1\n|pages= 1–60 \n|location = New York\n|publisher= [[Association for Computing Machinery]]\n|doi=\n|url= http://www.cloudbus.org/reports/DataGridTaxonomy.pdf\n|accessdate= April 10, 2012}}\n\n*{{cite web\n |last1=Zhu \n |first1=Lichun \n |title=Metadata management in grid database federation \n |work= \n |publisher= \n |date= \n |url=http://cs.uwindsor.ca/richard/cs510/lichun_zhu_survey.pdf \n |format= \n |doi= \n |accessdate=May 15, 2012 \n}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n==Further reading==\n\n*{{cite web\n|last1= Allcock\n|first1= W.\n|authorlink = W. Allcock\n|title= Gridftp: protocol extensions to ftp for the grid\n|work=\n|publisher = [[Argonne National Laboratory]]\n|date = April 2003\n|url = http://www.globus.org/alliance/publications/papers/GFD-R.0201.pdf\n|format = \n|doi =\n| accessdate = April 20, 2012 }}\n\n*{{cite web\n|last1= Allcock\n|first1= W.|last2= Bresnahan |first2= J. |last3= Kettimuthu |first3= R.|last4= Link |first4= M.|last5= Dumitrescu |first5= C.|last6= Raicu |first6= I.|last7= Foster |first7= I.\n|title= The globus striped gridftp framework and server\n|work=\n|publisher= [[ACM Press]]\n|date= November 2005\n|url= http://www.globus.org/alliance/publications/papers/gridftp_final.pdf\n|format=\n|doi=\n|accessdate = April 20, 2012 }}\n\n*{{cite journal\n|last1= Foster\n|first1= Ian |last2= Kesselman |first2= Carl |last3= Tuecke |first3= Steven\n|year= 2001\n|title= The anatomy of the grid enabling scalable virtual organizations\n|journal= [[International Journal of High Performance Computing Applications]]\n|volume= 15\n|issue= 3\n|pages= 200–222 \n|location = Thousand Oaks\n|publisher= [[Sage Publications]]\n|doi=10.1177/109434200101500302\n|url= http://www.globus.org/alliance/publications/papers/anatomy.pdf\n|accessdate= April 10, 2012}}\n\n*{{cite web\n |last1=Foster \n |first1=Ian \n |last2=Kesselman \n |first2=Carl \n |last3=Nick \n |first3=Jeffrey M. \n |last4=Tuecke \n |first4=Steven \n |title=The physiology of the grid: an open grid services architecture for distributed systems integration \n |work= \n |publisher= \n |date=June 22, 2002 \n |url=http://forge.gridforum.org/sf/go/doc13483?nav=1 \n |format= \n |doi= \n |accessdate=May 10, 2012 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20080322035911/http://forge.gridforum.org:80/sf/go/doc13483?nav=1 \n |archivedate=March 22, 2008 \n |df= \n}}\n\n*{{cite journal\n|last1= Hancock\n|first1= B.\n|year= 2009\n|title= A simple data grid using the inferno operating system\n|journal= Library Hi Tech\n|volume= 27\n|issue= 3\n|pages= 382–392 \n|location =\n|publisher= [[Emerald Group Publishing Limited]]\n|doi= 10.1108/07378830910988513\n|url= }}<!--|accessdate= April 10, 2012-->\n\n*{{cite web\n |last1=Hoschek \n |first1=W. \n |last2=McCance \n |first2=G. \n |title=Grid enabled relational database middleware \n |work= \n |publisher=[[Global Grid Forum]] \n |date=October 10, 2001 \n |url=http://ppewww.ph.gla.ac.uk/preprints/2001/11/GGF3Rome2001.pdf \n |format= \n |doi= \n |accessdate=April 22, 2012 \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20060128234459/http://ppewww.ph.gla.ac.uk:80/preprints/2001/11/GGF3Rome2001.pdf \n |archivedate=January 28, 2006 \n |df= \n}}\n\n*{{cite web\n|last1= Kunszt\n|first1= Peter Z.|last2= Guy |first2= Leanne P.\n|title= The open grid services architecture and data grids\n|work =\n|publisher = \n|date = July 7, 2002\n|url = http://www.computing.surrey.ac.uk/courses/csm23/Papers/data_grid.pdf\n|format =\n|doi =\n|accessdate = May 10, 2012 }}\n\n*{{cite web\n|last1= Moore\n|first1= Reagan W. \n|title= Evolution of data grid concepts\n|work =\n|publisher = \n|date = \n|url = http://www.nesc.ac.uk/events/GGF10-DA/programme/papers/06-Moore-Grid-evolution.pdf\n|format =\n|doi =\n|accessdate = May 10, 2012 }}\n\n*{{cite conference\n|last1= Rajkumar\n|first1= Kettimuthu |last2= Allcock |first2= William |last3= Liming |first3= Lee |last4= Navarro |first4= John-Paul |last5= Foster |first5= Ian\n| title = GridCopy moving data fast on the grid\n| booktitle = International parallel and distributed processing symposium (IPDPS 2007)\n| pages = 1–6\n| publisher = IEEE International\n| date = March 30, 2007\n| location =  Long Beach\n| url = http://www.globus.org/alliance/publications/papers/GridCopy.pdf\n| doi =\n| id =\n| accessdate = April 29, 2012 }}\n\n*{{cite journal\n|last1= Thenmozhi\n|first1= N. |last2= Madheswaran |first2= M. \n|year= 2011\n|title= Content based data transfer mechanism for efficient bulk data transfer in grid computing environment\n|journal= International Journal of Grid Computing & Applications (IJGCA)\n|volume= 2\n|issue= 4\n|pages= 49–62\n|location =\n|publisher=\n|doi= 10.5121/ijgca.2011.2405\n|issn= 2229-3949\n|url= http://www.scribd.com/doc/78611092/Content-Based-Data-Transfer-Mechanism-for-Efficient-Bulk-Data-Transfer-in-Grid-Computing-Environment\n|accessdate= April 28, 2012}}\n\n*{{cite journal\n |last1=Tu \n |first1=Manghui \n |last2=Li \n |first2=Peng \n |last3=I-Ling \n |first3=Yen \n |last4=Thuraisingham \n |first4=Bhavani \n |last5=Khan \n |first5=Latifur \n |year=2010 \n |title=Secure data objects replication in data grid \n |journal=IEEE Transactions on Dependable and Secure Computing \n |volume=7 \n |issue=1 \n |pages=50–64 \n |publisher=[[IEEE]] \n |doi=10.1109/tdsc.2008.19 \n |url=http://www.utdallas.edu/~lkhan/papers/Secure_Data_Objects_Replication_in_Data_Grid.pdf \n |accessdate=April 26, 2012 \n}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n[[Category:Data management]]']
['Category:NewSQL', '37256859', '{{Cat main|NewSQL}}\n\n[[Category:Databases]] \n[[Category:Data management]]']
['Data flow diagram', '1344164', '{{context|date=July 2014}}\n[[File:Data Flow Diagram Example.jpg|thumb|360px|Data flow diagram example.<ref>John Azzolini (2000). [http://ses.gsfc.nasa.gov/ses_data_2000/000712_Azzolini.ppt Introduction to Systems Engineering Practices]. July 2001.</ref>]]\n\nA \'\'\'data flow diagram\'\'\' (\'\'\'DFD\'\'\') is a graphical representation of the "flow" of data through an [[information system]], modelling its \'\'process\'\' aspects. A DFD is often used as a preliminary step to create an overview of the system, which can later be elaborated.<ref>Bruza, P. D., Van der Weide, Th. P., "The Semantics of Data Flow Diagrams", University of Nijmegen, 1993.</ref> DFDs can also be used for the [[Data visualization|visualization]] of [[data processing]] (structured design).\n\nA DFD shows what kind of information will be input to and output from the system, where the data will come from and go to, and where the data will be stored. It does not show information about the timing of process or information about whether processes will operate in sequence or in parallel (which is shown on a [[flowchart]]).\n\n==History==\n[[Larry Constantine]], the original developer of structured design,<ref>W. Stevens, G. Myers, L. Constantine, [http://domino.watson.ibm.com/tchjr/journalindex.nsf/d9f0a910ab8b637485256bc80066a393/a801ae3750be70ac85256bfa00685ded!OpenDocument "Structured Design"], IBM Systems Journal, 13 (2), 115-139, 1974.</ref> based on Martin and Estrin\'s "Data Flow Graph"  model of computation.\n\nStarting in the 1970s, data flow diagrams (DFD) became a popular way to visualize the major steps and data involved in software system processes. DFDs were usually used to show data flow in a computer system, although they could in theory be applied to [[business process modeling]]. DFD were useful to document the major data flows or to explore a new high-level design in terms of data flow.<ref>Craig Larman, "Applying UML and Patterns", Pearson Education, ISBN 978-81-7758-979-5</ref>\n\n==Theory==\n[[File:DataFlowDiagram Example.png|thumb|360px|Data flow diagram example]]\n[[File:Data-flow-diagram-notation.svg|thumb|160px|Data flow diagram - [[Edward Yourdon|Yourdon]]/[[Tom DeMarco|DeMarco]] notation]]\n\nData flow diagrams are also known as bubble charts.<ref>[http://www.orm.net/pdf/jcm13.pdf Introduced by Clive Finkelstein in Australia,  CACI in the UK, and later writers such as James Martin]</ref> DFD is a designing tool used in the top-down approach to Systems Design. This context-level DFD is next "exploded", to produce a Level 1 DFD that shows some of the detail of the system being modeled. The Level 1 DFD shows how the system is divided into sub-systems (processes), each of which deals with one or more of the data flows to or from an external agent, and which together provide all of the functionality of the system as a whole. It also identifies internal data stores that must be present in order for the system to do its job, and shows the flow of data between the various parts of the system.\n\nData flow diagrams are one of the three essential perspectives of the structured-systems analysis and design method [[SSADM]]. The sponsor of a project and the end users will need to be briefed and consulted throughout all stages of a system\'s evolution.  With a data flow diagram, users are able to visualize how the system will operate, what the system will accomplish, and how the system will be implemented.  The old system\'s dataflow diagrams can be drawn up and compared with the new system\'s data flow diagrams to draw comparisons to implement a more efficient system. Data flow diagrams can be used to provide the end user with a physical idea of where the data they input ultimately has an effect upon the structure of the whole system from order to dispatch to report. How any system is developed can be determined through a data flow diagram model. \n\nIn the course of developing a set of \'\'levelled\'\' data flow diagrams the analyst/designer is forced to address how the system may be decomposed into component sub-systems, and to identify the [[transaction data]] in the [[data model]].\n\nData flow diagrams can be used in both Analysis and Design phase of the [[Systems development life cycle|SDLC]].\n\nThere are  different notations to draw data flow diagrams (Yourdon & Coad and [[Chris Gane (computer scientist)|Gane]] & [[Trish Sarson|Sarson]]<ref>[[Chris Gane (computer scientist)|Chris Gane]] and [[Trish Sarson]]. \'\'Structured Systems Analysis: Tools and Techniques.\'\' McDonnell Douglas Systems Integration Company, 1977</ref>), defining different visual representations for processes, data stores, data flow, and external entities.<ref>[http://www.smartdraw.com/tutorials/software/dfd/tutorial_01.htm How to draw Data Flow Diagrams]</ref>\n\n===Physical vs. logical DFD=== \nA logical DFD captures the data flows that are necessary for a system to operate. It describes the processes that are undertaken, the data required and produced by each process, and the stores needed to hold the data. On the other hand, a physical DFD shows how the system is actually implemented, either at the moment (Current Physical DFD), or how the designer intends it to be in the future (Required Physical DFD). Thus, a Physical DFD may be used to describe the set of data items that appear on each piece of paper that move around an office, and the fact that a particular set of pieces of paper are stored together in a filing cabinet. It is quite possible that a Physical DFD will include references to data that are duplicated, or redundant, and that the data stores, if implemented as a set of [[database table]]s, would constitute an un-normalised (or de-normalised) relational database. In contrast, a Logical DFD attempts to capture the data flow aspects of a system in a form that has neither redundancy nor duplication.\n\n==See also==\n* [[Activity diagram]]\n* [[Business Process Model and Notation]]\n* [[Control flow diagram]]\n* [[Data island]]\n* [[Dataflow]]\n* [[Directed acyclic graph]]\n* [[DRAKON|Drakon-chart]]\n* [[Functional flow block diagram]]\n* [[Function model]]\n* [[IDEF0]]\n* [[Logical Data Flow]]\n* [[Pipeline (software)|Pipeline]]\n* [[Structured Analysis and Design Technique]]\n* [[Structure chart]]\n* [[System context diagram]]\n* [[Value stream mapping]]\n* [[Workflow]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*[[Scott W. Ambler]]. [http://www.agilemodeling.com/artifacts/dataFlowDiagram.htm The Object Primer 3rd Edition Agile Model Driven Development with UML 2]\n\n==External links==\n*{{Commons-inline}}\n\n{{Data model}}\n{{Authority control}}\n\n{{DEFAULTSORT:Data Flow Diagram1}}\n[[Category:Information systems]]\n[[Category:Data management]]\n[[Category:Diagrams]]\n[[Category:Visualization (graphic)]]\n[[Category:Systems analysis]]']
['EU Open Data Portal', '38138335', '{{Infobox Website\n| logo = |250px\n| url = {{URL|http://data.europa.eu/euodp/}}\n| commercial = No\n| type = [[public services|Public service]] [[Web portal|portal]] and <br />institutional information\n| registration = Not required\n| language = 24 official languages of the EU \n| owner = {{Flag|European Union}}\n| content license = Open\n| author = [[Publications Office (European Union)|EU Publications Office]]\n| launch date = December 2012\n}}\n\nThe \'\'\'EU Open Data Portal\'\'\' is the single point of access to a wide range of data held by EU institutions, agencies and other bodies. The portal is a key element of EU open data strategy.\n\n== Legal basis and launch date ==\n\nLaunched in December 2012 in beta mode, the portal was formally established by Commission Decision of 12 December 2011 (2011/833/EU) on the reuse of Commission documents to promote accessibility and reuse.<ref name="r1">{{cite news|url=http://eur-lex.europa.eu/legal-content/en/TXT/?uri=CELEX:32011D0833|title=Commission Decision of 12 December 2011 (2011/833/EU)}}</ref> \n\nWhile the operational management of the portal is the task of the [[Publications Office of the European Union]], implementation of EU open data policy is the responsibility of the [[Directorate General for Communications Networks, Content and Technology]] of the European Commission.\n\n== Features ==\n\nThe portal allows anyone to easily search, explore, link, download and reuse the data for commercial or non-commercial purposes, through a catalogue of common metadata. Through this catalogue, users access data stored on the websites of the EU institutions, agencies and other bodies.\n\nSemantic technologies offer new functionalities. The metadata catalogue can be searched via an interactive search engine (Data tab) and through [[SPARQL]] queries (Linked data tab). There is also a showcase of visualisation applications from various EU institutions, agencies and other bodies.\n\nUsers can suggest data they would like the portal to be linked to, give feedback on the quality of data obtainable and share information with other users about how they have used it.\n\nThe interface is in 24 EU official languages, while most [[metadata]] are currently available in a limited number of languages (English, French and German). Some of the metadata (e.g. names of the data providers, geographical coverage) are in 24 languages following the translation of [[controlled vocabulary]] lists that are used by the portal.<ref name="r3">{{cite news|url=http://publications.europa.eu/mdr/authority/index.html|title=EU controlled vocabularies}}</ref>\n\n== Terms of reuse ==\n\nMost data accessible via the EU Open Data Portal are covered by the Europa Legal Notice <ref name="r6">{{cite news|url=http://ec.europa.eu/geninfo/legal_notices_en.htm|title=Europa Legal Notice}}</ref> and can be reused free of charge, for commercial and non-commercial purposes, provided that the source is acknowledged. Specific conditions on reuse, related mostly to the protection of third-party intellectual property rights, apply for a very limited amount of data.\n\n== Data available ==\n\nThe portal contains a very wide variety of high-value open data across EU policy domains, as also more recently identified by the G8 Open Data Charter. These include the economy, employment, science, environment and education. The number of data providers — which include [[Eurostat]], the [[European Environment Agency]] and the [[Joint Research Centre]] — continues to grow.\n\nSo far, around 56 EU institutions, bodies or departments (e.g. Eurostat, the European Environment Agency, the Joint Research Centre and other European Commission Directorates General and EU Agencies) have made datasets available, making a total of over 7,800.\n\nIn addition to giving access to datasets, the portal also is an easy entry point to a whole range of visualisation applications using EU data. The applications are displayed as much for their information value as for giving examples of what applications can be made using the data.\n\n== Architecture of the portal ==\n\nThe portal is built using open source solutions such as the [[Drupal]] content management system and [[CKAN]], the data catalogue software developed by the [[Open Knowledge Foundation]]. It uses Virtuoso as an [[Resource Description Framework|RDF]] database and has a [[SPARQL]] endpoint.\n\nIts metadata catalogue is built on the basis of international standards such as [[Dublin Core]], the data catalogue vocabulary DCAT and the asset description metadata schema [[ADMS]].<ref name="r5">{{cite news|url=http://ec.europa.eu/digital-agenda/en/open-data-portals|title=Open Data Portals in Europe}}</ref>\n\n==See also==\n\n*[[Open data]]\n*[[Institutions of the European Union]] \n*[[Agencies of the European Union]]\n*[[Bodies of the European Union]]\n*[[European Data Portal]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://ec.europa.eu/europe2020/index_en.htm Europe 2020 – Official EU Site]\n* [http://ec.europa.eu/digital-agenda/ Digital Agenda for Europe]\n* [http://ec.europa.eu/digital-agenda/en/open-data-0 Open Data section of above site]\n* [https://joinup.ec.europa.eu/community/ods/description Joinup community on EU open data]\n* [http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52011DC0882 Communication ‘Open data — An engine for innovation, growth and transparent governance’]\n* [https://ec.europa.eu/digital-agenda/en/legislative-measures Legal rules on public services information]\n* [http://okfn.org/ Open Knowledge Foundation]\n* [http://dublincore.org/ Dublin Core]\n* [http://latc-project.eu/ Publication and usage of linked data on the Web]\n* [http://datacatalogs.org/group/eu-official Data catalogues]\n* [http://5stardata.info/ Open data classification by [[Tim Berners Lee]]] \n* [http://opendatachallenge.org/ Open Data Challenge (now over)]\n\n[[Category:European Commission]]\n[[Category:Open data]]\n[[Category:Transparency (behavior)]]\n[[Category:Open government]]\n[[Category:Semantic Web]]\n[[Category:Data management]]\n[[Category:Creative Commons]]']
['Read–write conflict', '217827', "{{Unreferenced|date=June 2008}}\nIn [[computer science]], in the field of [[database]]s, '''read–write conflict''', also known as '''unrepeatable reads''', is a computational anomaly associated with interleaved execution of transactions. \n\nGiven a schedule S\n\n:<math>S = \\begin{bmatrix}\nT1 & T2 \\\\\nR(A) &  \\\\\n & R(A) \\\\\n & W(A)\\\\\n & Com. \\\\\nR(A) & \\\\\nW(A) & \\\\\nCom. & \\end{bmatrix}</math>\n\nIn this example, T1 has read the original value of A, and is waiting for T2 to finish. T2 also reads the original value of A, overwrites A, and commits.\n\nHowever, when T1 reads to A, it discovers two different versions of A, and T1 would be forced to [[Abort (computing)|abort]], because T1 would not know what to do. This is an unrepeatable read. This could never occur in a serial schedule. [[Strict two-phase locking]] (Strict 2PL) prevents this conflict.\n\n== Real-world example==\n[[Alice and Bob]] are using a website to book tickets for a specific show. Only one ticket is left for the specific show. Alice signs on first to see that only one ticket is left, and finds it expensive. Alice takes time to decide. Bob signs on and also finds one ticket left, and orders it instantly. Bob purchases and logs off. Alice decides to buy a ticket, to find there are no tickets. This is a typical read-write conflict situation.\n\n== See also ==\n\n* [[Concurrency control]]\n* [[Write–read conflict]]\n* [[Write–write conflict]]\n\n{{DEFAULTSORT:Read-write conflict}}\n[[Category:Data management]]\n[[Category:Transaction processing]]"]
['Data room', '1216068', "{{Refimprove|date=March 2015}}\n'''Data rooms''' are spaces used for housing data, usually of a secure or privileged nature. They can be physical data rooms, [[virtual data room]]s, or [[data centers]].<ref>{{cite web|title=Data Room (entry)|url=http://financial-dictionary.thefreedictionary.com/Data+room|website=Financial Dictionary, The Free Dictionary by Farlex}}</ref><ref>{{cite web|title=Data Room (entry)|url=http://www.nasdaq.com/investing/glossary/d/data-room|website=Nasdaq}}</ref> They are used for a variety of purposes, including data storage, document exchange, file sharing, financial transactions, legal transactions, and more.\n\nIn mergers and acquisitions, the traditional data room will literally be a physically secure continually monitored room, normally in the vendor’s offices (or those of their lawyers), which the bidders and their advisers will visit in order to inspect and report on the various documents and other data made available. Often only one bidder at a time will be allowed to enter and if new documents or new versions of documents are required these will have to be brought in by [[courier]] as [[hardcopy]]. Teams involved in large [[due diligence]] processes will typically have to be flown in from many regions or countries and remain available throughout the process.  Such teams often comprise a number of experts in different fields and so the overall cost of keeping such groups on call near to the data room is often extremely high.  Combating the significant cost of physical datarooms is the [[virtual data room]], which provides for the secure, online dissemination of confidential information.\n\nA [[virtual data room]] (VDR) is essentially a website with limited controlled access (using a secure log-on supplied by the vendor/authority which can be disabled at any time by the vendor/authority if a bidder withdraws) to which the bidders and their advisers are given access. Much of the information released will be confidential and restrictions should be applied to the viewers' ability to release this to third parties by forwarding, copying or printing. [[Digital rights management]] is sometimes applied to control information.\n\nDetailed auditing must be provided for legal reasons so that a record is kept of who has seen which version of each document.\n\nData rooms are commonly used by [[legal]], [[accounting]], [[investment banking]] and [[private equity]] companies performing [[mergers and acquisitions]], [[fundraising]], [[insolvency]], [[corporate restructuring]], and joint ventures including bio-technology and tender processes.\n\n==References==\n{{Reflist}}\n* [http://www.imaa-institute.org/docs/kummer-sliskovic_do%20virtual%20data%20rooms%20add%20value%20to%20the%20mergers%20and%20acquisitions%20process.pdf A report about the advantages and disadvantages of virtual vs. physical data rooms]\n\n{{DEFAULTSORT:Data Room}}\n[[Category:Data management]]"]
['ISO/IEC JTC 1/SC 32', '41418778', '\'\'\'ISO/IEC JTC 1/SC 32 Data management and interchange\'\'\' is a [[standardization]] subcommittee of the Joint Technical Committee [[ISO/IEC JTC1|ISO/IEC JTC 1]] of the [[International Organization for Standardization]] (ISO) and the [[International Electrotechnical Commission]] (IEC), which develops and facilitates standards within the field of data management and interchange. The international [[Secretariat (administrative office)|secretariat]] of ISO/IEC JTC 1/SC 32 is the [[American National Standards Institute]] (ANSI) located in the United States.<ref name=countries>{{cite web| title=ISO/IEC JTC 1/SC 32 - Data management and interchange| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee_participation.htm?commid=45342| author=ISO| accessdate=2013-10-03}}</ref>\n\n==History==\nISO/IEC JTC 1/SC 32 was formed in 1997, as a combination of the following three ISO/IEC JTC 1 subgroups: ISO/IEC JTC 1/SC 21/WG 3, Database; ISO/IEC JTC 1/SC 14, Data elements; and ISO/IEC JTC 1/SC 30, Open-edi. The new subcommittee was established with the intention of developing, and facilitating the development of, standards for data management within local and distributed information system environments.<ref name=briefing>{{cite news| title=Information technology: ISO/IEC JTC 1/SC 32, Data Management and Interchange| type=Briefings| author1=Cannan, Stephen| author2=Melton, Jim| journal=ISO Bulletin| date=January 2000| url=http://jtc1sc32.org/doc/N0601-0650/32N0607.pdf| pages=3–4| volume=31| issue=1}}</ref> ISO/IEC JTC 1/SC 32 was originally made up of five working groups (WGs), though ISO/IEC JTC 1/SC 32/WG 5, Database access and interchange, was disbanded in March 2002.<ref>{{cite report| type=Business Plan Draft| title=Draft Business Plan for ISO/IEC JTC 1/SC32, Data Management and Interchange| author=Mann, Douglas| accessdate=2013-10-04| url=http://jtc1sc32.org/doc/N0751-0800/32N0783.pdf| date=2002-04-04|page=4}}</ref> The four other original working groups of the subcommittee are currently active, although the title of ISO/IEC JTC 1/SC 32/WG 1 was changed from Open-edi to its current title, e-Business.<ref name=briefing/>\n\n==Scope==\nThe scope of ISO/IEC JTC 1/SC 32 is “Standards for data management within and among local and distributed information systems environments. SC 32 provides enabling technologies to promote harmonization of data management facilities across sector-specific areas. Specifically, SC32 standards include:”<ref name=business2012>{{cite report| type=Business Plan| url=http://jtc1info.org/wp-content/uploads/2013/03/SC-32-Business-Plan-2012.pdf| accessdate=2013-10-03| author=Melton, Jim| date=2012-10-02| title=Business Plan for JTC1/SC32: 2012-2013}}</ref>\n* Reference models and frameworks for the coordination of existing and emerging standards\n* Definition of data domains, data types, and data structures, and their associated semantics\n* Languages, services, and protocols for persistent storage, concurrent access and concurrent update, and interchange of data\n* Methods, languages, services, and protocols to structure, organize, and register metadata and other information resources associated with sharing and interoperability, including electronic commerce\n\n==Structure==\nISO/IEC JTC 1/SC 32 is made up of four active working groups, each of which carries out specific tasks in standards development within the field of data management and interchange. As a response to changing standardization needs, working groups of ISO/IEC JTC 1/SC 32 can be disbanded if their area of work is no longer applicable, or established if new working areas arise. The focus of each working group is described in the group’s terms of reference. Active working groups of ISO/IEC JTC 1/SC 32 are:<ref name=business2012/><ref>{{cite web| title=ISO/IEC JTC 1/SC 32 Data management and interchange| author=ISO| accessdate=2013-10-03| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342}}</ref>\n{| class="wikitable" width="60%"\n! width="20%" | Working Group\n! width="40%" | Working Area\n|-\n| ISO/IEC JTC 1/SC 32/WG 1 || [[Electronic business|e-Business]]\n|-\n|ISO/IEC JTC 1/SC 32/WG 2\t|| [[Metadata]]\n|-\n|ISO/IEC JTC 1/SC 32/WG 3\t|| [[Database#Database languages|Database languages]]\n|-\n|ISO/IEC JTC 1/SC 32/WG 4 || [[SQL]] Multimedia and application packages\n|-\n|}\n\n==Collaborations==\nISO/IEC JTC 1/SC 32 works in close collaboration with a number of other organizations or subcommittees, both internal and external to ISO or IEC, in order to avoid conflicting or duplicative work. Organizations internal to ISO or IEC that collaborate with or are in liaison to ISO/IEC JTC 1/SC 32 include:<ref name=business2012/><ref>{{cite web| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342| title=ISO/IEC JTC 1/SC 32 Data management and interchange| accessdate=2013-10-03| author=ISO}}</ref><ref>{{cite web| title=SC32 Liaison Organizations| accessdate=2013-10-03| author=ISO/IEC JTC 1/SC 32| url=http://jtc1sc32.org/}}</ref>\n* [[ISO/IEC JTC 1/SC 7]], Software and systems engineering\n* [[ISO/IEC JTC 1/SC 25]], Interconnection of information technology equipment\n* [[ISO/IEC JTC 1/SC 38]], Cloud Computing and Distributed Platforms\n* ISO/TC 12, Quantities and units\n* [[ISO/TC 37]], Terminology and other language and content resources \n* ISO/TC 37/SC 2, Terminographical and lexicographical working methods\n* ISO/TC 37/SC 3, Systems to manage terminology, knowledge and content\n* ISO/TC 37/SC 4, Language resource management\n* ISO/TC 46/SC 4, Technical interoperability\n* ISO/TC 46/SC 11, Archives/records management\n* ISO/TC 68/SC 2, Financial Services, security\n* ISO/TC 127, Earth-moving machinery\n* ISO/TC 154, Processes, data elements and documents in commerce, industry and administration\n* ISO/TC 184, Automation systems and integration\n* [[ISO TC 184/SC 4|ISO/TC 184/SC 4]], Industrial data\n* ISO/TC 204, Intelligent transport systems\n* [[ISO/TC 211]], Geographic information/Geomatics\n* [[ISO/TC 215]], Health informatics\n* ISO/TC 232, Learning services outside formal education\n\nSome organizations external to ISO or IEC that collaborate with or are in liaison to ISO/IEC JTC 1/SC 32 include:\n* [[Confédération Internationale des Sociétés d´Auteurs et Compositeurs|International Confederation of Societies of Authors and Composers]] (CISAC)\n* [[Dublin Core Metadata Initiative]] (DCMI)\n* [[EUROSTAT]]\n* [[International Telecommunications Satellite Organization]] (ITSO)\n* [[International Telecommunication Union|ITU]]\n* [[Infoterm]]\n* [[Object Management Group]] (OMG)\n* [[Society for Worldwide Interbank Financial Telecommunication]] (SWIFT)\n* [[UN/CEFACT]]\n* [[United Nations Economic Commission for Europe]] (UNECE)\n* [[World Meteorological Organization]] (WMO)\n* [[W3C]]\n\n==Member countries==\nCountries pay a fee to ISO to be members of subcommittees.<ref>{{cite manual| url=http://www.iso.org/iso/iso_membership_manual_2012.pdf| pages=-18| chapter=III. What Help Can I Get from the ISO Central Secretariat?| title=ISO Membership Manual| author=ISO| date=June 2012| accessdate=2013-07-12| publisher=ISO}}</ref>\n\nThe 14 "P" (participating) members of ISO/IEC JTC 1/SC 32 are: Canada, China, Czech Republic, Côte d\'Ivoire, Egypt, Finland, Germany, India, Japan, Republic of Korea, Portugal, Russian Federation, United Kingdom, and United States.<ref name=countries/>\n\nThe 22 "O" (observing) members of ISO/IEC JTC 1/SC 32 are: Australia, Austria, Belgium, Bosnia and Herzegovina, France, Ghana, Hungary, Iceland, Indonesia, Islamic Republic of Iran, Ireland, Italy, Kazakhstan, Luxembourg, Netherlands, Norway, Poland, Romania, Serbia, Spain, Switzerland, and Turkey.\n\n==Published standards==\nISO/IEC JTC 1/SC 32 standards are meant to structure, organize, and register metadata and other information resources associated with sharing and interoperability, including electronic commerce.<ref name=briefing/> ISO/IEC JTC 1/SC 32 currently has 74 published standards within the field of data management and interchange, including:<ref>{{cite web| title=ISO/IEC JTC 1/SC 32| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_tc_browse.htm?commid=45342&published=on| accessdate=2013-10-03| author=ISO}}</ref><ref>{{cite web| publisher=ISO| url=http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html| title=Freely Available Standards| accessdate=2013-09-26}}</ref>\n{| class="wikitable sortable" width="100%"\n! data-sort-type="number" width="14%" | ISO/IEC Standard\n! width="29%" | Title\n! width="6%" | Status\n! width="49%" | Description\n! width= "2%" | WG\n|-\n|data-sort-value="14662"|ISO/IEC 14662 [http://standards.iso.org/ittf/licence.html free] || Information technology – Open-edi reference model || Published (2010) || Specifies the framework for coordinating the integration of existing International Standards and the development of future International Standards for the interworking of Open-edi parties through Open-edi<ref>{{cite journal| title=ISO/IEC 14662| date=2010-02-15| author=ISO| edition=3| page=1}}</ref><ref>{{cite web| title=ISO/IEC 14662:2010| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=55290| date=2010-02-02| author=ISO}}</ref> || 1\n|-\n|data-sort-value="15944"|ISO/IEC 15944-1 [http://standards.iso.org/ittf/licence.html free] || Information technology – Business Operational View – Part 1: Operational aspects of Open-edi for implementation || Published (2011) || Allows constraints, including legal requirements, commercial and/or international trade and contract terms, public policy, and laws and regulations, to be defined and integrated into Open-edi through the business operational view (BOV)<ref>{{cite journal| title=ISO/IEC 15944-1| date=2011-08-01| author=ISO| edition=2| page=1}}</ref><ref>{{cite web| title=ISO/IEC 15944-1:2011| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=55289| date=2011-07-21| author=ISO}}</ref> || 1\n|-\n|data-sort-value="11179"|[[ISO/IEC 11179]]-3 [http://standards.iso.org/ittf/licence.html free] || Information technology – [[metadata registry|Metadata registries]] (MDR) – Part 3: Registry metamodel and basic attributes || Published (2013) || Specifies the structure of a metadata registry in the form of a conceptual data model and specifies basic attributes which are required to describe metadata items<ref>{{cite journal| page=1| date=2003-02-15| title=ISO/IEC 11179-3| author=ISO| edition=2}}</ref><ref>{{cite web| title=ISO/IEC 11179-3:2013| accessdate=2013-10-03| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=50340| date=2013-02-12| author=ISO}}</ref> || 2\n|-\n|data-sort-value="20943"| ISO/IEC TR 20943-1 [http://standards.iso.org/ittf/licence.html free] || Information technology – Procedures for achieving metadata registry content consistency – Part 1: Data elements || Published (2003) || “Describes a set of procedures for the consistent registration of data elements and their attributes in a registry.”<ref>{{cite journal| title=ISO/IEC TR 20943-1| author=ISO| page=1| date=2003-08-01| edition=1}}</ref><ref>{{cite web| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=34343| date=2012-12-19| title=ISO/IEC TR 20943-1:2003| author=ISO}}</ref> || 2\n|-\n|data-sort-value="20944"| ISO/IEC 20944-1 || Information technology – Metadata Registries Interoperability and Bindings (MDR-IB) – Part 1: Framework, common vocabulary, and common provisions for conformance || Published (2013) || Contains the overview, framework, common vocabulary, and common provisions for conformance for the ISO/IEC 20944 series, which provides the bindings and their interoperability for MDRs<ref>{{cite web| author=ISO| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=51914| date=2013-01-08| accessdate=2013-10-04| title=ISO/IEC 20944-1:2013}}</ref> || 2\n|-\n|data-sort-value="19502"|ISO/IEC 19502 || Information technology – [[Meta-Object Facility|Meta Object Facility (MOF)]] || Published (2005) || Defines a metamodel using MOF, and a set of interfaces using Open Distributed Processing (ODP) that can be used to define and manipulate a set of interoperable metamodels and their corresponding models<ref>{{cite web| title=ISO/IEC 19502:2005| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=32621| date=2011-03-17| author=ISO}}</ref> || 2\n|-\n|data-sort-value="19773"|ISO/IEC 19773 || Information technology – Metadata Registries (MDR) modules || Published (2011) || Specifies small modules of data to be used or reused in applications<ref>{{cite web| url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=41769| date=2011-09-01| accessdate=2013-10-04| author=ISO| title=ISO/IEC 19773:2011}}</ref> || 2\n|-\n|data-sort-value="09075"|ISO/IEC 9075-1 [http://standards.iso.org/ittf/licence.html free] || Information technology – Database languages – [[SQL#Standardization|SQL]] – Part 1: Framework (SQL/Framework) || Published (2011) || Defines the conceptual framework to specify the grammar of SQL and the result of processing statements in that language by an SQL-implementation<ref>{{cite journal| title=ISO/IEC 9075-1| page=1| date=2008-07-15| edition=3| author=ISO}}</ref><ref>{{cite web| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=53681| date=2013-02-04| accessdate=2013-10-04| author=ISO| title=ISO/IEC 9075-1:2011}}</ref> || 3\n|-\n|data-sort-value="13249"|ISO/IEC 13249-3 || Information technology – Database languages – SQL multimedia and application packages – Part 3: Spatial || Published (2011) || “Defines spatial user-defined types, routines, and schemas for generic spatial data handling.”<ref>{{cite web| date=2011-08-22| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=53698| accessdate=2013-10-04| author=ISO| title=ISO/IEC 13249-3:2011}}</ref> || 4\n|-\n|}\n\n==See also==\n* [[ISO/IEC JTC1]]\n* [[List of International Organization for Standardization standards|List of ISO standards]]\n* [[American National Standards Institute]]\n* [[International Organization for Standardization]]\n* [[International Electrotechnical Commission]]\n\n==References==\n{{Reflist|2}}\n\n==External links==\n* [http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342 ISO/IEC JTC 1/SC 32 page at ISO]\n\n{{DEFAULTSORT:ISO IEC JTC1 SC32}}\n[[Category:ISO/IEC JTC1 subcommittees|#032]]\n[[Category:Standards organizations]]\n[[Category:Data management]]\n[[Category:Data interchange standards]]']
['Geospatial metadata', '7419799', '\'\'\'Geospatial metadata\'\'\' (also \'\'\'geographic metadata\'\'\', or simply  \'\'\'metadata\'\'\' when used in a geographic context) is a type of [[metadata]] that is applicable to objects that have an explicit or implicit [[Geography|geographic]] extent, i.e. are associated with some position on the surface of the [[globe]]. Such objects may be stored in a [[geographic information system]] (GIS) or may simply be documents, data-sets, images or other objects, services, or related items that exist in some other native environment but whose features may be appropriate to describe in a (geographic) metadata catalog (may also be known as a data directory, data inventory, etc.).\n\n==Definition==\nI\'\'\'SO 19115:2013 "Geographic Information - Metadata\'\'\'"<ref name=":0">{{Cite web|url=https://www.iso.org/obp/ui/#iso:std:iso:19115:-1:ed-1:v1:en|title=ISO 19115-1:2014(en)|last=International Organization for Standardization|first=|date=2014-04-01|website=ISO|publisher=|access-date=2016-04-01}}</ref> from [[ISO/TC 211]], the industry standard for geospatial metadata, describes its scope as follows:\n\n"\'\'[This standard] provides information about the identification, the extent, the quality, the spatial and temporal aspects, the content, the spatial reference, the portrayal, distribution, and other properties of digital geographic data and services"<ref name=":0" />\'\'\n\nISO 19115:2013 also provides for non-digital mediums: \'\'"[t]hough this part of [https://www.iso.org/obp/ui/#iso:std:iso:19115:en ISO 19115] is applicable to digital data and services, its principles can be extended to many other types of resources such as maps, charts, and textual documents as well as non-geographic data.\'\'" <ref name=":0" />\n\n\'\'\'The U.S. Federal Geographic Data Committee (FGDC)\'\'\' describes geospatial metadata as follows:\n\n"\'\'A metadata record is a file of information, usually presented as an XML document, which captures the basic characteristics of a data or information resource. It represents the who, what, when, where, why and how of the resource. Geospatial metadata commonly document geographic digital data such as Geographic Information System (GIS) files, geospatial databases, and earth imagery but can also be used to document geospatial resources including data catalogs, mapping applications, data models and related websites. Metadata records include core library catalog elements such as Title, Abstract, and Publication Data; geographic elements such as Geographic Extent and Projection Information; and database elements such as Attribute Label Definitions and Attribute Domain Values.\'\'" <ref>{{Cite web|url=http://www.fgdc.gov/metadata|title=Geospatial Metadata — Federal Geographic Data Committee|website=www.fgdc.gov|access-date=2016-04-01}}</ref>\n\n==History==\nThe growing appreciation of the value of geospatial metadata through the 1980s and 1990s led to the development of a number of initiatives to collect metadata according to a variety of formats either within agencies, communities of practice, or countries/groups of countries. For example, [[NASA]]\'s "DIF" metadata format was developed during an Earth Science and Applications Data Systems Workshop in 1987,<ref>[http://gcmd.nasa.gov/User/difguide/whatisadif.html Gene Major and Lola Olsen: "A short history of the DIF". On GCMD website, visited 16 October 2006]</ref> and formally approved for adoption in 1988. Similarly, the U.S. FGDC developed its geospatial metadata standard over the period 1992–1994.<ref>[http://libraries.mit.edu/guides/subjects/metadata/standards/fgdc.html MIT Libraries Guide: "Federal Geographic Data Committee (FGDC) Metadata". On MIT Libraries website, visited 16 October 2006]\n</ref> The Spatial Information Council of Australia and New Zealand (ANZLIC),<ref>\n{{cite web\n| url         = http://anzlic.gov.au/sites/default/files/files/ANZLICmetadataProfileGuidelines_v1-2.pdf\n| title       = ANZLIC Metadata Profile Guidelines version 1.2 July 2011\n| year        = 2011\n| publisher   = ANZLIC\n| accessdate  = 2011-04-11\n| quote       = ANZLIC[:] The Spatial Information Council of Australia and New Zealand (formerly known as the Australia New Zealand Land Information Council)\n}}\n</ref> a combined body representing spatial data interests in Australia and New Zealand, released version 1 of its "metadata guidelines" in 1996.<ref>[http://anzlic.gov.au/resources/anzlic-metadata-profile ANZLIC Metadata Guidelines: Core metadata elements for geographic data in Australia and New Zealand, Version 2 (February 2001)]</ref> [[ISO/TC 211]] undertook the task of harmonizing the range of formal and \'\'de facto\'\' standards over the approximate period 1999–2002, resulting in the release of \'\'\'ISO 19115\'\'\' "\'\'\'Geographic Information - Metadata\'\'\'" in 2003 and a subsequent revision in 2013. {{As of | 2011}} individual countries, communities of practice, agencies, etc. have started re-casting their previously used metadata standards as "profiles" or recommended subsets of ISO 19115, occasionally with the inclusion of additional metadata elements as formal extensions to the ISO standard. The growth in popularity of Internet technologies and data formats, such as [[Extensible Markup Language]] (XML) during the 1990s led to the development of mechanisms for exchanging geographic metadata on the [[World Wide Web|web]]. In 2004, the [[Open Geospatial Consortium]] released the current version (3.1) of [[Geography Markup Language]] (GML), an XML grammar for expressing geospatial features and corresponding metadata. With the growth of the [[Semantic Web]] in the 2000s, the geospatial community has begun to develop [[Ontology (computer science)|ontologies]] for representing semantic geospatial metadata. Some examples include the [http://www.ordnancesurvey.co.uk/oswebsite/ontology/ Hydrology and Administrative ontologies] developed by the [[Ordnance Survey]] in the [[United Kingdom]].\n\n==ISO 19115: Geographic information - Metadata==\nISO 19115 is a standard of the International Organization for Standardization (ISO).<ref>ISO 19115 Geographic Information - Metadata. International Organization for Standardization (ISO), Geneva, 2003</ref> The standard is part of the [[ISO/TC 211|ISO geographic information suite of standards]] (19100 series). ISO 19115 and its parts define how to describe geographical information and associated services, including contents, spatial-temporal purchases, data quality, access and rights to use.\n\nThe objective of this International Standard is to provide a clear procedure for the description of digital geographic data-sets so that users will be able to determine whether the data in a holding will be of use to them and how to access the data. By establishing a common set of metadata terminology, definitions and extension procedures, this standard promotes the proper use and effective retrieval of geographic data.<ref>{{cite web|title=ISO 19115 Metadata Factsheet|url=http://www.isotc211.org/Outreach/Overview/Factsheet_19115.pdf|publisher=AG Outreach|accessdate=2012-11-22}}</ref>\n\nISO 19115 was revised in 2013 to accommodate growing use of the internet for metadata management, as well as add many new categories of metadata elements (referred to as codelists) and the ability to limit the extent of metadata use temporally or by user.<ref>{{Cite web|url=https://wiki.earthdata.nasa.gov/display/NASAISO/NASA+Metadata+and+the+New+ISO+19115-1+Capabilities|title=NASA Metadata and the New ISO 19115-1 Capabilities - NASA ISO for EOSDIS - Earthdata Wiki|website=wiki.earthdata.nasa.gov|access-date=2016-04-01}}</ref>\n\n{{Expand section|date=June 2012}}\n\n==ISO 19139 Geographic information Metadata XML schema implementation==\nISO 19139:2012 <ref>{{Cite web|url=https://www.iso.org/obp/ui/#iso:std:iso:ts:19139:-2:ed-1:v1:en|title=ISO/TS 19139-2:2012(en)|last=International Organization for Standardization|first=|date=2012-12-15|website=ISO|publisher=|access-date=2016-04-01}}</ref> provides the XML implementation schema for ISO 19115 specifying the metadata record format and may be used to describe, validate, and exchange geospatial metadata prepared in XML.<ref>[http://marinemetadata.org/references/iso19139 "ISO 19139 Geographic information Metadata XML schema implementation"], Marine Metadata Interoperability Project</ref>\n\nThe standard is part of the [[ISO/TC 211|ISO geographic information suite of standards]] (19100 series), and provides a spatial metadata XML (spatial metadata eXtensible Mark-up Language (smXML)) encoding, an XML schema implementation derived from ISO 19115, Geographic information – Metadata. The metadata includes information about the identification, constraint, extent, quality, spatial and temporal reference, distribution, lineage, and maintenance of the digital geographic data-set.\n\n{{Expand section|date=June 2012}}\n\n==Metadata directories==\nAlso known as metadata catalogues or data directories.\n\n(need discussion of, and subsections on GCMD, FGDC metadata gateway, ASDD, European and Canadian initiatives, etc. etc.)\n* [http://gisinventory.net GIS Inventory] – National GIS Inventory System which is maintained by the US-based [[National States Geographic Information Council|National States Geographic Information Council (NSGIC)]] as a tool for the entire US GIS Community. Its primary purpose is to track data availability and the status of geographic information system (GIS) implementation in state and local governments to aid the planning and building of statewide spatial data infrastructures (SSDI). The Random Access Metadata for Online Nationwide Assessment (RAMONA) database is a critical component of the GIS Inventory. RAMONA moves its FGDC-compliant metadata (CSDGM Standard) for each data layer to a web folder and a Catalog Service for the Web (CSW) that can be harvested by Federal programs and others. This provides far greater opportunities for discovery of user information. The GIS Inventory website was originally created in 2006 by NSGIC under award NA04NOS4730011 from the Coastal Services Center, National Oceanic and Atmospheric Administration, U.S. Department of Commerce. The Department of Homeland Security has been the principal funding source since 2008 and they supported the development of the Version 5 during 2011/2012 under Order Number HSHQDC-11-P-00177. The Federal Emergency Management Agency and National Oceanic and Atmospheric Administration have provided additional resources to maintain and improve the GIS Inventory. Some US Federal programs require submission of CSDGM-Compliant Metadata for data created under grants and contracts that they issue. The GIS Inventory provides a very simple interface to create the required Metadata. \n* [http://gcmd.nasa.gov GCMD] - Global Change Master Directory\'s goal is to enable users to locate and obtain access to Earth science data sets and services relevant to global change and Earth science research. The GCMD database holds more than 20,000 descriptions of Earth science data sets and services covering all aspects of Earth and environmental sciences.\n* [http://earthdata.nasa.gov/echo ECHO] - The EOS Clearing House (ECHO) is a spatial and temporal metadata registry, service registry, and order broker. It allows users to more efficiently search and access data and services through the [http://reverb.earthdata.nasa.gov/echo Reverb Client] or Application Programmer Interfaces (APIs). ECHO stores metadata from a variety of science disciplines and domains, totalling over 3400 Earth science data sets and over 118&nbsp;million granule records.\n* [http://www.gogeo.ac.uk/gogeo/ GoGeo] - GoGeo is a service run by [[EDINA]] (University of Edinburgh) and is supported by [[Jisc]]. GoGeo allows users to conduct geographically targeted searches to discover geospatial datasets. GoGeo searches many data portals from the HE and FE community and beyond. GoGeo also allows users to create standards compliant metadata through its Geodoc metadata editor.\n\n==Geospatial metadata tools==\nThere are many commercial GIS or geospatial products that support metadata viewing and editing on GIS resources. For example, [[ESRI]]\'s [[ArcGIS]] Desktop, [[SOCET GXP]], [[Autodesk]]\'s AutoCAD Map 3D 2008, [[Arcitecta]]\'s [[Mediaflux]] and [[Intergraph]]\'s [[GeoMedia]] support geospatial metadata extensively.\n\n[http://gisinventory.net GIS Inventory] is a free web-based tool that provides a very simple interface to create geospatial metadata. Participants create a profile and document their data layers through a survey-style interface. The GIS Inventory produces metadata that is compliant with the Federal Content Standard for Digital Geospatial Metadata (CSDGM). The GIS Inventory is also capably of ingesting already completed metadata through document upload and web server connectivity. Through the GIS Inventory web services, metadata are automatically shared with US Federal agencies.\n\n[http://geonetwork-opensource.org GeoNetwork opensource] is a comprehensive [[Free and Open Source Software]] solution to manage and publish geospatial metadata and services based on international metadata and catalog standards. The software is part of the [[Open Source Geospatial Foundation]]\'s software stack.\n\n[http://geocat.net/bridge GeoCat Bridge] allows to edit, validate and directly publish metadata from [[ArcGIS]] Desktop to [http://geonetwork-opensource.org GeoNetwork] (and generic CSW catalogs) and publishes data as map services on [http://geoserver.org GeoServer]. Several metadata profiles are supported.\n\n[[pycsw]] is an OGC CSW server implementation written in Python. pycsw fully implements the OpenGIS Catalogue Service Implementation Specification ([[Catalog Service for the Web|Catalogue Service for the Web]]). The project is certified OGC Compliant, and is an OGC Reference Implementation.\n\n[http://catmdedit.sourceforge.net/ CATMDEdit]\nterraCatalog\nArcCatalog\nArcGIS Server Portal\n[http://geonetwork-opensource.org GeoNetwork opensource]\n[http://www.conterra.de/en/products/sdi/terracatalog/index.shtm IME]\n[http://www.intelec.ca/html/en/technologies/m3cat.html M3CAT MetaD]\n[http://www.gigateway.org.uk/metadata/metagenie.html MetaGenie]\nParcs Canada Metadata Editor\nMapit/CADit\nNOKIS Editor\n\n{{Expand section|date=June 2008}}\n\n==References==\n<references/>\n[http://anzlic.gov.au/sites/default/files/files/ANZLICmetadataProfileGuidelines_v1-2.pdf ANZLIC Metadata Profile Version 1.2 (viewed July 2011)]\n\n==External links==\n*[http://www.fgdc.gov/metadata FGDC metadata page]\n*[http://gcmd.nasa.gov/ Global Change Master Directory(GCMD)]\n*[http://wiki.milcord.com/wiki/Geospatial_Exploitation_of_Motion_Imagery Geospatial Exploitation of Motion Imagery] is a geospatially aware and integrated Intelligent Video Surveillance (IVS) software system targeted at real-time and forensic video analytic and mining applications that require low-resolution detection, tracking, and classification of moving objects (people and vehicles) in outdoor, wide-area scenes.\n*[http://www.iso.org/iso/en/CatalogueDetailPage.CatalogueDetail?CSNUMBER=26020 ISO 19115:2003 Geographic information -- Metadata]\n*[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=32557 Geographic information -- Metadata -- XML schema implementation ]\n*[http://www.earthdatamodels.org/designs/metadata_BGS.html EarthDataModels design for Metadata] is a logical data model and physical implementation of a Spatial Metadata Database, based on ISO19115 and is INSPIRE compliant.\n\n{{use dmy dates|date=January 2011}}\n\n[[Category:Data management]]\n[[Category:Metadata]]\n[[Category:Geographic data and information]]']
['ANSI 834 Enrollment Implementation Format', '42783850', '{{Orphan|date=June 2014}}\nEdi 834 files.\n[[American National Standards Institute|ANSI]] 834 EDI Enrollment Implementation [[File format|Format]] is a standard format for electronically exchanging health plan enrollment data between employers and [[health insurance]] carriers. An 834 file contains a string of data elements and each data element represents a fact, such as a subscriber’s name, hire date, etc. The entire string is called a data segment. [[Health Insurance Portability and Accountability Act|The Health Insurance Portability and Accountability Act (HIPAA)]] requires that all health plans or health insurance carriers accept a standard enrollment format, ANSI 834A Version 5010. The ANSI 834A is the national standard for electronic enrollment and maintenance health plan.\n\nThe 834 is used to transfer enrollment information from the sponsor of the [[insurance]] coverage, benefits, or policy to a payer. The intent of this implementation guide is to meet the [[health care]] industry\'s specific need for the initial enrollment and subsequent maintenance of individuals who are enrolled in insurance products. This implementation guide specifically addresses the enrollment and maintenance of health care products only. One or more separate guides may be developed for life, flexible spending, and retirement products.\n\nAn example layout of an ANSI 834A Version 5010 file is shown below.\n\n\'\'\'Sample File Output\'\'\'<br />\nINS*Y*18*030*XN*A*E**FT~<br />\nREF*OF*152239999~<br />\nREF*1L*Blue~<br />\nDTP*336*D8*20070101~<br />\nNM1*IL*1*BLUTH*LUCILLE****34*152239999~<br />\nN3*224 N DES PLAINES*6TH FLOOR~<br />\nN4*CHICAGO*IL*60661*USA~<br />\nDMG*D8*19720121*F*M~<br />\nHD*030**VIS**EMP~<br />\nDTP*348*D8*20111016~<br />\nINS*N*19*030*XN*A*E***N*N~<br />\nREF*OF*152239999~<br />\nREF*1L*Blue~<br />\nDTP*357*D8*20111015~<br />\nNM1*IL*1*BLUTH*BUSTER~<br />\nN3*224 N DES PLAINES*6TH FLOOR~<br />\nN4*CHICAGO*IL*60661*USA~<br />\nDMG*D**19911015*M-HD*030**VIS~<br />\nDTP*348*D8*20110101~<br />\nDTP*349*D8*20111015~\n\n==See also==\n* [[X12 Document List]]\n\n==References==\n{{reflist}}\n* [http://getworkforce.com/ansi-834-file-layout/ "ANSI 834 File Layout"]\n* [http://getworkforce.com/ansi-834-file-layout/ "Guardian Electronic User Guide 834 Enrollment and Maintenance"]\n* [http://www.1edisource.com/transaction-sets?tset=834 "EDI 834 Benefit Enrollment and Maintenance"]\n\n[[Category:Data management]]']
['Government Performance Management', '26105075', '{{Refimprove|date=October 2014}}\n\'\'\'Government Performance Management\'\'\' (GPM) consists of a set of processes that help government organizations optimize their business performance. It provides a framework for organizing, automating, and analyzing business methodologies, metrics, processes and systems that drive business performance.<ref>{{cite web|url=http://www.information-management.com/bissues/20070301/2600312-1.html|title=Performance Management for Government|author=Michael Owellen|date=28 February 2007|work=BI Review Magazine|accessdate=22 October 2014}}</ref> Some commentators{{who|date=October 2014}} see GPM as the next generation of [[business intelligence]] (BI) for governments. GPM helps governments to make use of their financial, human, material, and other resources.  In the past, owners have sought to drive strategy down and across their organizations; they have struggled to transform strategies into actionable metrics and they have grappled with meaningful analysis to expose the cause-and-effect relationships that, if understood, could give profitable insight to their operational decision-makers.  GPM software and methods allow a systematic, integrated approach that links government strategy to core processes and activities. "Running by the numbers" now means something: planning, budgeting, analysis, and reporting can give the measurements that empower management decisions.<ref>{{cite web|url=http://www.encyclopedia.com/doc/1O12-performancemanagement.html |accessdate=February 7, 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20090112223214/http://www.encyclopedia.com/doc/1O12-performancemanagement.html |archivedate=January 12, 2009 }}</ref>\n\n== Performance Management (PM) Market ==\nAccording to [[Gartner]]{{citation needed|date=October 2014}}, the Enterprise Performance Management (EPM) suite market continues to experience strong momentum, growing 19% during 2007. This is slightly in advance of their earlier market sizing and forecast analysis, which anticipated 2007 revenue to be $1.836 million, representing an 18% year-over-year growth. In the latest forecast, Gartner believe that the market for EPM will be more than $3 billion by 2011, representing a 14.4% compound annual growth rate. Several factors contributed to the continued significant growth in EPM revenue during 2007:\n* Many organizations replaced difficult-to-maintain, inflexible, or outmoded spreadsheets and homegrown financial applications.\n* Continued growth in large enterprises was fueled by desires to achieve greater transparency and adherence to governance and compliance legislation.\n* Increased demand for applications that support strategic plans and operational activities drove new momentum in the deployment of scorecards.\n* There was increased demand from mid-size enterprises, representing one of the largest untapped and dynamic areas of the business application software sector.\n* Advertising and PR from increasingly large vendors and system integrators are raising the EPM profile and generating greater demand.\n\nGartner also expects the Business Intelligence software market to reach $3 billion in 2009. "Companies around the world have purchased more than US $40 billion worth of enterprise applications, including ERP, CRM and HR, during the past few years," said Colleen Graham, principal research analyst at Gartner. "This has generated significant volumes of data in support of the operational processes they automate. By investing in BI, companies can further leverage their enterprise application investments and turn the torrent of data into meaningful insight to better measure performance, respond more quickly to market changes and opportunities and comply with an increasingly complex regulatory environment."<ref>{{cite web|url=http://www.gartner.com/press_releases/asset_144782_11.html|title=Gartner News Room|publisher=Gartner.com|accessdate=22 October 2014}}</ref>\n\n== ITWorx Government Performance Management (GPM) ==\n\n[[ITWorx]] GPM is a bilingual, [[Microsoft]]-based framework that gives governments the capability to cascade, share, track, and update strategies and plans organization-wide. It creates detailed views of multi-source [[Key Performance Indicator]]s (KPIs) using customized [[balance scorecard]]s, dashboards, strategy maps, statistical charts, and reports, as well as provides ad hoc analytical and reporting tools.\n\n== ITWorx GPM Features ==\n{{Merge to|section=yes|ITWorx|date=October 2014}}\n{{advert|date=October 2014}}\nITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> provides a top-down approach in recording government strategy. The strategy is cascaded and shared across government bodies to define objectives and balancing targets.\n\nIt also links strategy to execution. Operational plans are recorded, linked to strategies, assigned a time-range for implementation, broken down to initiatives and business activities, approved, and then propagated to all levels.\n\nITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> defines a time-range for implementing an initiative, its owners, cost drivers, budgets, KPIs, and targets; and links initiatives to strategic objectives. Business activities, contributing in strategy execution, are defined including their KPIs and targets. KPIs can be entered and configured manually, calculated using other KPIs, or extracted from external data sources.\n\nITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> enables the definition of government-specific business rules such as KPI calculation formulas; it also enables administration of system settings such as the configuration of the organization structure and definition of approval [[workflows]] for each organization unit. Furthermore, administrators can manage user roles and groups as well as archive plans and approvals.\n\nITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> calculates measurement formulas, compares actual values against targets, and performs analysis. Color-coded KPIs are represented through strategic and customized scorecards, dashboards, strategy maps, and statistical charts and graphs, in addition to ad hoc analytical and reporting tools.\n\nThe solution enables communication throughout the decision-making process by allowing users to post comments and discuss topics regarding a strategy, KPI, or report. Keeping a documented record of why and when decisions are made, ITWorx GPM retains the history of contributions.\n\nITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> provides a mechanism for policy-makers and strategy implementers to facilitate the strategic management process without compromising data. Government frontline officials are provided with a feedback channel to submit change requests and propositions to approved strategic plans, targets, and actual data while securing the validity and consistency of data.\n\nFrontline officials can monitor performance though consolidated views while detailed views are provided for department and executive levels. Based on privileges, users can view rolled-up KPIs and drill-down for [[root cause analysis]] or corrective actions.\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://gpm.itworx.com Gpm.itworx.com]\n*[http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=efdc60d3-2622-44f5-aa5d-2b79d10c93ab  Microsoft.com]\n*[http://www.pcmag-mideast.com/gitex/tag/itworx/ Pcmag-mideast.com]\n*[http://www.itp.net/578068-itworx-releases-new-gpm-suite Itp.net]\n\n[[Category:Business intelligence]]\n[[Category:Data management]]']
['Category:Information privacy', '44363114', '{{Cat main}}\n\n[[Category:Data management|Privacy]]\n[[Category:Computer law|Privacy]]\n[[Category:Privacy]]\n[[Category:Computer security|Privacy]]\n[[Category:Information|Privacy]]']
['Category:Object storage', '44628227', '[[Category:Data management]]']
['Open Compute Project', '31547791', '{{Infobox Organization\n|name           = Open Compute Project\n|image          = OpenCompute logo.jpg\n|mcaption       = \n|formation      = 2011\n|type           = Industry trade group\n|purpose        = Sharing designs of [[data center]] products\n|headquarters   = \n|membership     = \n|website        = {{URL|opencompute.org}}\n|remarks        =\n}}\n[[File:Open Compute Server Front.jpg|thumb|Open Compute V2 Server]]\n[[File:Open Compute 1U Drive Tray Bent.jpg|thumb|Open Compute V2 Drive Tray,<br />2nd lower tray extended]]\nThe \'\'\'Open Compute Project\'\'\' (\'\'\'OCP\'\'\') is an organization that shares designs of [[data center]] products among companies, including [[Facebook]], [[Intel]], [[Nokia]], [[Google]], [[Apple Inc.|Apple]], [[Microsoft]], [[Seagate Technology]], [[Dell]], [[Rackspace]], [[Ericsson]],  [[Cisco]], [[Juniper Networks]], [[Goldman Sachs]], [[Fidelity Investments|Fidelity]], [[Lenovo]] and [[Bank of America]].<ref>{{cite web|url=http://www.wired.com/2015/03/facebook-got-even-apple-back-open-source-hardware/|title=How Facebook Changed the Basic Tech That Runs the Internet|date=11 Apr 2015}}</ref><ref>{{Cite web|url=http://www.opencompute.org/about/ocp-incubation-committee/|title=Incubation Committee|website=Open Compute|access-date=2016-08-19}}</ref>\n\nThe Open Compute Project\'s mission is to design and enable the delivery of the most efficient server, storage and data center hardware designs for scalable computing. "We believe that openly sharing ideas, specifications and other intellectual property is the key to maximizing innovation and reducing operational complexity in the scalable computing space."<ref>{{cite web|url=http://www.opencompute.org/about/mission-and-principles/|title=Mission and Principles|website = Open Compute|accessdate = 2016-05-13}}</ref><br />\nAll Facebook Data Centers are 100% OCP: Prineville Data Center, Forest City Data Center, Altoona Data Center, Luleå Data Center (Sweden).\nFacebook Data Centers under construction: Fort Worth Data Center, Clonee Data Center (Ireland).<ref>{{cite web|url=http://uk.businessinsider.com/facebook-eu-data-center-open-compute-project-2016-1|first=Matt|last=Weinberger|title=Facebook\'s newest data center is going to make some big tech companies very nervous|website=Open Compute|date=January 25, 2016|accessdate = 2016-05-16}}</ref>\n\n==Details==\nThe initiative was announced in April 2011 by Jonathan Heiliger<ref>{{cite news|last1 = Heiliger|first1 = Jonathan|title = Why I Started the Open Compute Project|url = http://www.vertexventures.com/2015/06/why-i-started-the-open-compute-project/|accessdate = 18 June 2015|date = 2015-06-15}}</ref> at [[Facebook]] to openly share designs of [[data center]] products.<ref>{{cite web |url= http://www.datacenterknowledge.com/archives/2011/04/14/will-open-compute-alter-the-data-center-market/ |title=Will Open Compute Alter the Data Center Market? |date=April 14, 2011 |first= Rich|last= Miller |work= Data Center Knowledge |accessdate= July 9, 2013 }}</ref>\nThe effort came out of a redesign of [[Facebook]]\'s data center in [[Prineville, Oregon]].<ref>{{Cite web |url= http://www.facebook.com/notes/facebook-engineering/building-efficient-data-centers-with-the-open-compute-project/10150144039563920 |title= Building Efficient Data Centers with the Open Compute Project |first= Jonathan|last= Heiliger |date= April 7, 2011 |work= Facebook Engineering\'s notes |accessdate= July 9, 2013 }}</ref>\nAfter two years, with regards to a more module server design, it was admitted that "the new design is still a long way from live data centers".<ref>{{Cite news |title= Facebook Shatters the Computer Server Into Tiny Pieces |date= January 16, 2013 |first= Cade|last= Metz |work= Wired |url= http://www.wired.com/wiredenterprise/2013/01/facebook-server-pieces/ |accessdate= July 9, 2013 }}</ref>\nHowever, some aspects published were used in the Prineville center to improve the energy efficiency, as measured by the [[power usage effectiveness]] index defined by [[The Green Grid]].<ref name="Stanford">{{Cite web |title= Facebook\'s Open Compute Project |work= Stanford EE Computer Systems Colloquium |date= February 15, 2012  |url= http://www.stanford.edu/class/ee380/Abstracts/120215.html |first= Amir|last= Michael |publisher= [[Stanford University]]}}  ([http://ee380.stanford.edu/cgi-bin/videologger.php?target=120215-ee380-300.asx video archive])</ref>\n\nThe Open Compute Project Foundation is a 501(c)(6) non-profit incorporated in the state of Delaware. Corey Bell serves as the Foundation\'s CEO. Currently there are 7 members who serve on board of directors which is made up of two individual members and five organizational members.  Jason Taylor ([[Facebook]]) is the Foundation\'s president and chairman. Frank Frankovsky (formerly of Facebook and past president and chairman) and  [[Andy Bechtolsheim]] are the two individual members.  In addition to Jason Taylor who represents [[Facebook]], other organizations on the Open Compute board of directors include [[Intel]] (Jason Waxman), [[Goldman Sachs]] (Don Duet), [[Rackspace]] (Mark Roenick), and [[Microsoft]] (Bill Laing).<ref>{{Cite web|title = Organization and Board|url = http://www.opencompute.org/about/organization-and-board/|website = Open Compute|accessdate = 2015-09-12}}</ref>\n\nOn March 11, 2015 [[Apple Inc.|Apple]], [[Cisco]] and [[Juniper Networks]] joined the project.<ref>{{Cite web |title= Open Compute: Apple, Cisco Join While HP Expands |first= Charles|last= Babcock |date= March 11, 2015 |url=http://www.informationweek.com/cloud/infrastructure-as-a-service/open-compute-apple-cisco-join-while-hp-expands/d/d-id/1319421  |accessdate= March 11, 2015 }}</ref>\n\nOn November 16, 2015 [[Nokia]] joined the project.<ref>{{Cite web |title= Nokia Networks joins Open Compute Project to advance its AirFrame Data Center Solution|date= November 16, 2015 |url=http://company.nokia.com/en/news/press-releases/2015/11/16/nokia-networks-joins-open-compute-project-to-advance-its-airframe-data-center-solution}}</ref>\n\nOn February 23, 2016 [[Lenovo]] joined the project.<ref>{{Cite web |title= Lenovo joins Open Compute Project |date= February 23, 2016 |url=http://news.lenovo.com/blog/lenovo-joins-open-compute-projects.htm }}</ref>\n\nOn March 9, 2016 [[Google]] joined the project.<ref>{{Cite web |title= Google joins the Open Compute Project |date= March 9, 2016 |url=http://techcrunch.com/2016/03/09/google-joins-the-open-compute-project/ }}</ref>\n\nComponents of the Open Compute Project include:\n\n* Server compute nodes included one for [[Intel]] processors and one for [[Advanced Micro Devices|AMD]] processors. In 2013, [[Calxeda]] contributed a design with [[ARM architecture]] processors.<ref>{{Cite web |title= ARM Server Motherboard Design for Open Vault Chassis Hardware v0.3 MB-draco-hesperides-0.3 |first= Tom|last= Schnell |date= January 16, 2013 |url=http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_ARM_Server_Specification_v0.3.pdf  |accessdate= July 9, 2013 }}</ref><br />Several generations of server designs have been deployed. So far being: Freedom (Intel), Spitfire (AMD), Windmill (Intel E5-2600), Watermark (AMD), Winterfell (Intel E5-2600 v2) and Leopard (Intel E5-2600 v3)<ref>{{Cite web |title=Guide to Facebook’s Open Source Data Center Hardware\n|author=Data Center Knowledge|date=April 28, 2016|url=http://www.datacenterknowledge.com/archives/2016/04/28/guide-to-facebooks-open-source-data-center-hardware/|accessdate=May 13, 2016}}</ref><ref>{{Cite web |title=Facebook rolls out new web and database server designs|first=The|last=Register|date=January 17, 2013|url=http://www.theregister.co.uk/2013/01/17/open_compute_facebook_servers/|accessdate=May 13, 2016}}</ref>\n\n* Open Vault storage building blocks offer high disk densities, with 30 drives in a 2U Open Rack chassis designed for easy [[disk drive]] replacement. The 3.5 inch disks are stored in two drawers, five across and three deep in each drawer, with connections via [[serial attached SCSI]].<ref>{{Cite web |title= Open Vault Storage Hardware V0.7 OR-draco-bueana-0.7 |author= Mike Yan and Jon Ehlen |date= January 16, 2013 |url= http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_Open_Vault_Storage_Specification_v0.7.pdf |accessdate= July 9, 2013 }}</ref> This storage is also called Knox, there is also a cold storage variant where the disks power down if not used to save energy consumption.<ref>{{Cite web |title=Under the hood: Facebook’s cold storage system|date=May 4, 2015|url=https://code.facebook.com/posts/1433093613662262/-under-the-hood-facebook-s-cold-storage-system-/|accessdate=May 13, 2016}}</ref> Another design concept was contributed by Hyve Solutions, a division of [[Synnex]] in 2012.<ref>{{Cite web |title= Hyve Solutions Contributes Storage Design Concept to OCP Community |work= News release |date= January 17, 2013 |url= http://ir.synnex.com/releasedetail.cfm?ReleaseID=733922 |accessdate= July 9, 2013 }}</ref><ref>{{Cite web |title= Torpedo Design Concept Storage Server for Open Rack Hardware v0.3 ST-draco-chimera-0.3 |first= Conor|last= Malone |date= January 15, 2012 |url= http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_Storage_Server_for_Open_Rack_Specification_v0.3.pdf  |accessdate= July 9, 2013 }}</ref><br />At the OCP Summit 2016 Facebook together with Taiwanese ODM Wistron\'s spin-off Wiwynn introduced Lightning, a flexible NVMe JBOF (just a bunch of flash), based on the existing Open Vault (Knox) design.<ref>{{Cite web |title=Introducing Lightning: A flexible NVMe JBOF|first=Chris|last=Petersen|date=March 9, 2016|url=https://code.facebook.com/posts/989638804458007/introducing-lightning-a-flexible-nvme-jbof/|accessdate= May 13, 2016}}</ref><ref>{{Cite web |title=Wiwynn Showcases All-Flash Storage Product with Leading-edge NVMe Technology|date=March 9, 2016|url=http://www.wiwynn.com/english/company/newsinfo/23|accessdate= May 13, 2016}}</ref>\n* Mechanical mounting system: Open racks have the same outside width (600&nbsp;mm) and depth as standard [[19-inch rack]]s, but are designed to mount wider chassis with a 537&nbsp;mm width (about 21 inches). This allows more equipment to fit in the same volume and improves air flow. Compute chassis sizes are defined in multiples of an OpenU, which is 48&nbsp;mm, slightly larger than the typical [[rack unit]].\n* Data center designs for energy efficiency, include 277 VAC power distribution that eliminates one transformer stage in typical data centers. A single voltage (12.5 VDC) power supply designed to work with 277 VAC input and 48 VDC battery backup.<ref name="Stanford" />\n* On May 8, 2013, an effort to define an open [[network switch]] was announced.<ref>{{Cite web |title= Up next for the Open Compute Project: The Network |date= May 8, 2013 |author= Jay Hauser for Frank Frankovsky |work= Open Compute blog |url= http://www.opencompute.org/blog/up-next-for-the-open-compute-project-the-network/ |accessdate= June 20, 2014 }}</ref> The plan was to allow Facebook to load its own [[operating system]] software onto the switch. Press reports predicted that more expensive and higher-performance switches would continue to be popular, while less expensive products treated more like a [[commodity]] (using the [[buzzword]] "top-of-rack") might adopt the proposal.<ref>{{Cite news |title= Can Open Compute change network switching? |first= David|last= Chernicoff |work= ZDNet |date= May 9, 2013 |url= http://www.zdnet.com/can-open-compute-change-network-switching-7000015141/ |accessdate= July 9, 2013 }}</ref><br />A similar project for a custom switch for the [[Google platform]] had been rumored, and evolved to use the [[OpenFlow]] protocol.<ref>{{Cite news |title= Facebook Rattles Networking World With ‘Open Source’ Gear |date= May 8, 2013 |first= Cade|last= Metz |work= Wired |url= http://www.wired.com/wiredenterprise/2013/05/facebook_networking/ |accessdate= July 9, 2013 }}</ref><ref>{{Cite news |title= Going With the Flow: Google’s Secret Switch to the Next Wave of Networking |date= April 17, 2012 |first= Steven|last= Levy |work= Wired |url= http://www.wired.com/wiredenterprise/2012/04/going-with-the-flow-google/ |accessdate= July 9, 2013 }}</ref><br />The first switch Open Sourced by Facebook was designed together with Taiwanese ODM Accton using Broadcom Trident II chip and is called Wedge, the Linux OS that it runs is called FBOSS.<ref>{{cite web|url=https://code.facebook.com/posts/681382905244727/introducing-wedge-and-fboss-the-next-steps-toward-a-disaggregated-network/|title=Introducing "Wedge" and "FBOSS," the next steps toward a disaggregated network|website =Meet the engineers who code Facebook|date=June 18, 2014|accessdate = 2016-05-13}}</ref><ref>{{cite web|url=https://code.facebook.com/posts/843620439027582/facebook-open-switching-system-fboss-and-wedge-in-the-open/|title=Facebook Open Switching System ("FBOSS") and Wedge in the open|website=Meet the engineers who code Facebook|date=March 10, 2015|accessdate = 2016-05-13}}</ref> Later switch contributions include "6-pack" and Wedge-100, based on Broadcom Tomahawk chips.<ref>{{cite web|url=https://code.facebook.com/posts/203733993317833/opening-designs-for-6-pack-and-wedge-100/|title=Opening designs for 6-pack and Wedge 100|website=Meet the engineers who code Facebook|date=March 9, 2016|accessdate = 2016-05-13}}</ref> Similar switch hardware designs have been contributed by: Edge-Core Networks Corporation (Accton spin-off), Mellanox Technologies, Interface Masters Technologies, Agema Systems.<ref>{{cite web|url=http://www.opencompute.org/wiki/Networking/SpecsAndDesigns|title=Accepted or shared hardware specifications|website=Open Compute|accessdate = 2016-05-13}}</ref> Capable of running ONIE compatible Operating Systems such as Cumulus Linux, Big Switch or Pica8.<ref>{{cite web|url=http://www.opencompute.org/wiki/Networking/ONIE/NOS_Status|title=Current Network Operating System (NOS) List|website=Open Compute|accessdate = 2016-05-13}}</ref>\n\n== Providers ==\nThe promoted vendors include:<ref>[http://www.opencompute.org/about/open-compute-project-solution-providers/ open compute project solution providers]</ref>\n* [[AMAX Information Technologies]]\n* Circle B\n* [[Itochu Techno-Solutions]] (CTC)\n* [[Hewlett Packard Enterprise]]\n* Hyperscale IT\n* [[Synnex|Hyve Solutions]]\n* [[Penguin Computing]]\n* [[Nokia]]\n* [[Quanta Computer]]\n* Racklive\n* Stack Velocity\n* Wiwynn\n\n== See also ==\n* [[Novena (computing platform)]]\n* [[Open-source computing hardware]]\n* [[OpenPOWER Foundation]]\n* [[Telecom Infra Project]]  - [[Facebook]] sister project focusing on [[Optical networking|Optical]] [[broadband networks]] and open [[cellular network|cellular networks]]\n\n== References ==\n{{reflist|33em}}\n\n== External links ==\n{{Commons category|Data Centers}}\n* {{Official website|http://opencompute.org/}}\n* [https://www.facebook.com/PrinevilleDataCenter/ Prineville Data Center]\n* [https://www.facebook.com/ForestCityDataCenter/ Forest City Data Center]\n* [https://www.facebook.com/AltoonaDataCenter/ Altoona Data Center]\n* [https://www.facebook.com/LuleaDataCenter/ Luleå Data Center (Sweden)]\n* [https://www.facebook.com/FortWorthDataCenter/ Fort Worth Data Center]\n* [https://www.facebook.com/CloneeDataCenter/ Clonee Data Center (Ireland)]\n* Videos\n** {{youtube|2hTfzUmdAOw|HC23-T2: The Open Compute Project}}, Hot Chips 23, 2011 2.5 Hour Tutorial\n** {{youtube|QtTF9pDQxPc|Facebook Open Compute Server}}, Facebook V1 Open Compute Server\n** {{youtube|ckNzwqhDS60|Facebook V2 Windmill Server}}\n** {{youtube|GbzQe3jO4hc|Hyve: Adapting Facebook\'s Servers for Your Data Center}}, Open Compute starts at 5:40\n\n{{Facebook navbox|state=collapsed}}\n\n[[Category:Open-source hardware]]\n[[Category:Facebook]]\n[[Category:2011 software]]\n[[Category:Data centers]]\n[[Category:Data management]]\n[[Category:Servers (computing)]]\n[[Category:Distributed data storage]]\n[[Category:Distributed data storage systems]]\n[[Category:Applications of distributed computing]]\n[[Category:Cloud storage]]\n[[Category:Computer networking]]\n[[Category:Science and technology in the San Francisco Bay Area]]']
['COMMIT (SQL)', '46362717', '{{Unreferenced|date=April 2015}}\nA <code>COMMIT</code> statement in [[SQL]] ends a [[database transaction|transaction]] within a [[relational database management system]] (RDBMS) and makes all changes visible to other users. The general format is to issue a <code>[[Begin work (SQL)|BEGIN WORK]]</code> statement, one or more SQL statements, and then the <code>COMMIT</code> statement. Alternatively, a <code>[[Rollback (data management)|ROLLBACK]]</code> statement can be issued, which undoes all the work performed since <code>BEGIN WORK</code> was issued. A <code>COMMIT</code> statement will also release any existing [[savepoint]]s that may be in use.\n\nIn terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a [[rollback (data management)|rollback]].\n\n==See also==\n* [[Commit (data management)]]\n* [[Atomic commit]]\n* [[Two-phase commit protocol]]\n* [[Three-phase commit protocol]]\n\n{{databases}}\n\n{{DEFAULTSORT:Commit (Data Management)}}\n[[Category:Data management]]\n[[Category:SQL]]\n[[Category:Transaction processing]]\n\n{{comp-sci-stub}}']
['Data lake', '46626475', '{{Use dmy dates|date=May 2016}}\nA \'\'\'data lake\'\'\' is a method of storing [[data]] within a system or repository, in its natural format,<ref>[http://blogs.sas.com/content/datamanagement/2016/11/21/growing-import-big-data-quality/ The growing importance of big data quality]</ref> that facilitates the collocation of data in various schemata and structural forms, usually object blobs or files. \n\n== Invention ==\nJames Dixon, then chief technology officer at [[Pentaho]] coined the term<ref name="woods2011">{{cite news | title=Big data requires a big architecture |last=Woods |first=Dan |work=Forbes |date=21 July 2011 |department=Tech |url=http://www.forbes.com/sites/ciocentral/2011/07/21/big-data-requires-a-big-new-architecture/ }}</ref> to contrast it with [[data mart]], which is a smaller repository of interesting attributes extracted from raw data.<ref name="dixon2010">{{cite web | last=Dixon|first=James|title=Pentaho, Hadoop, and Data Lakes|url=https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/|website=James Dixon’s Blog|publisher=James|accessdate=7 November 2015 |quote=If you think of a datamart as a store of bottled water – cleansed and packaged and structured for easy consumption – the data lake is a large body of water in a more natural state. The contents of the data lake stream in from a source to fill the lake, and various users of the lake can come to examine, dive in, or take samples.}}</ref> He argued that data marts have several inherent problems, and that data lakes are the optimal solution. These problems are often referred to as [[information silo]]ing. [[PricewaterhouseCoopers]] said that data lakes could "put an end to data silos.<ref name="stein2014">{{cite report | url=http://www.pwc.com/en_US/us/technology-forecast/2014/cloud-computing/assets/pdf/pwc-technology-forecast-data-lakes.pdf |format=pdf |title=Data lakes and the promise of unsiloed data |last2=Morrison |first2=Alan |last=Stein |first=Brian |publisher=PricewaterhouseCooper |series=Technology Forecast: Rethinking integration |year=2014 }}</ref> In their study on data lakes they noted that enterprises were "starting to extract and place data for analytics into a single, Hadoop-based repository."\n\n==Characteristics==\nThe idea of data lake is to have a single store of all data in the enterprise ranging from raw data (which implies exact copy of source system data) to transformed data which is used for various tasks including [[Data reporting|reporting]], [[data visualization|visualization]], [[data analytics|analytics]] and [[machine learning]].\n\nThe data lake includes structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and even binary data (images, audio, video) thus creating a centralized data store accommodating all forms of data.\n\n== Examples ==\n\nOne example of a data lake is the distributed file system [[Apache Hadoop]].\n\nMany companies also use cloud storage services such as [[Amazon S3]].<ref name="tuulos2015">{{cite web | title=Petabyte-Scale Data Pipelines with Docker, Luigi and Elastic Spot Instances |last=Tuulos |first=Ville |date=22 September 2015 |url=http://tech.adroll.com/blog/data/2015/09/22/data-pipelines-docker.html}}</ref> There is a gradual academic interest in the concept of data lakes, for instance, [http://www.researchgate.net/publication/283053696_Personal_Data_Lake_With_Data_Gravity_Pull Personal DataLake]<ref>http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?reload=true&arnumber=7310733</ref> an ongoing project at Cardiff University to create a new type of data lake which aims at managing big data of individual users by providing a single point of collecting, organizing, and sharing personal data.<ref>http://www.researchgate.net/publication/283053696_Personal_Data_Lake_With_Data_Gravity_Pull</ref>\n\nThe earlier data lake (Hadoop 1.0) had limited capabilities with its batch oriented processing (Map Reduce) and was the only processing paradigm associated with it. Interacting with the data lake meant you had to have expertise in Java with map reduce and higher level tools like Pig & Hive (which by themselves were batch oriented). With the dawn of Hadoop 2.0 and separation of duties with Resource Management taken over by YARN (Yet another resource negotiator), new processing paradigms like Streaming, interactive, on-line have become available via Hadoop and the Data Lake.\n\n== Criticism ==\n{{criticism section|date=December 2015}}\n\nIn June 2015, David Needle characterized "so-called data lakes" as "one of the more controversial ways to manage [[big data]]".<ref name="needle2015">{{cite news |  last=Needle |first=David | title=Hadoop Summit: Wrangling Big Data Requires Novel Tools, Techniques |date=10 June 2015 | work=eWeek | url= http://www.eweek.com/enterprise-apps/hadoop-summit-wrangling-big-data-requires-novel-tools-techniques-2.html | department=Enterprise Apps | access-date = 1 November 2015 | quote = Walter Maguire, chief field technologist at HP\'s Big Data Business Unit, discussed one of the more controversial ways to manage big data, so-called data lakes. }}</ref> [[PricewaterhouseCoopers]] were also careful to note in their research that not all data lake initiatives are successful. They quote Sean Martin, CTO of [[Cambridge Semantics]],\n{{quote|sign=|source=|We see customers creating big data graveyards, dumping everything into HDFS [Hadoop Distributed File System] and hoping to do something with it down the road. But then they just lose track of what’s there.<ref name="stein2014"/>}}\nThey advise that "The main challenge is not creating a data lake, but taking advantage of the opportunities it presents."<ref name="stein2014"/> They describe companies that build successful data lakes as gradually maturing their lake as they figure out which data and metadata are important to the organization.\n\n== References ==\n<references/>\n\n[[Category:Data management]]']
['Embedded analytics', '47485347', '\'\'\'Embedded analytics\'\'\' is the technology designed to make [[data analysis]] and [[business intelligence]] more accessible by all kinds of application or user.\n\n==Definition==\n\nAccording to Gartner analysts Kurt Schlegel, traditional [[business intelligence]] were suffering in 2008 a lack of integration between the data and the business users.<ref>{{cite web\n| last = Kelly\n| first = Jeff\n| title = Gartner Business Intelligence Summit: Embed BI within business processes\n| publisher = TechTarget\n| accessdate = August 2015\n| url = http://searchbusinessanalytics.techtarget.com/news/1507180/Gartner-Business-Intelligence-Summit-Embed-BI-within-business-processes\n}}</ref> This technology intention is to be more pervasive by real-time autonomy and self-service of data visualization or customization, meanwhile decision makers, business users or even customers are doing their own daily workflow and tasks.\n\n==History==\n\nFirst mentions of the concept were made by Howard Dresner, consultant, author, former Gartner analyst and inventor of the term "business intelligence".<ref>{{cite web\n| last = Kelly\n| first = Jeff\n| title = Gartner Business Intelligence Summit: Embed BI within business processes\n| publisher = TechTarget\n| accessdate= August 2015\n| url = http://searchbusinessanalytics.techtarget.com/news/1507180/Gartner-Business-Intelligence-Summit-Embed-BI-within-business-processes\n}}</ref> Consolidation of [[business intelligence]] "doesn\'t mean the BI market has reached maturity" <ref>{{cite web\n| last = Dresner\n| first = Howard \n| title = Howard Dresner predicts the future of business intelligence\n| publisher = TechTarget\n| accessdate= August 2015\n| url = http://searchbusinessanalytics.techtarget.com/podcast/Howard-Dresner-predicts-the-future-of-business-intelligence\n}}</ref> said Howard Dresner while he was working for Hyperion Solutions, a company that Oracle bought in 2007. Oracle started then to use the term "embedded analytics" at their press release for Oracle® Rapid Planning on 2009.<ref>{{cite web\n| title = Oracle Announces Oracle® Rapid Planning\n| publisher = Oracle\n| accessdate= August 2015\n| url = http://www.oracle.com/us/corporate/press/040402\n}}</ref> Gartner Group, a company for which Howard Dresner has been working,  finally added the term to their IT Glossary on November 5, 2012. \n<ref>{{cite web\n| title = Gartner IT Glossary: Embedded Analytics \n| publisher = Gartner\n| accessdate= August 2015\n| url = http://www.gartner.com/it-glossary/embedded-analytics \n}}</ref>\n. It was clear this was a mainstream technology when Dresner Advisory Services published the 2014 Embedded Business Intelligence Market Study as part of the Wisdom of Crowds® Series of Research, including 24 vendors.<ref>{{cite web\n| title = 2014 Embedded Business Intelligence Market Study Now Available From Dresner Advisory Services \n| publisher = Market Wired\n| accessdate= August 2015\n| url = http://www.marketwired.com/press-release/2014-embedded-business-intelligence-market-study-now-available-from-dresner-advisory-1962227.htm\n}}</ref>\n\n==Tools==\n\n{{colbegin|2}}\n\n* [[Actuate Corporation|Actuate]]\n* [[Dundas Data Visualization]]\n* [[GoodData]]\n* [[IBM]]\n* [[icCube]]\n* [[Logi Analytics]]\n* [[Pentaho]]\n* [[Qlik]]\n* [[SAP_SE|SAP]]\n* [[SAS_(software)|SAS]]\n* [[ServiceNow]]\n* [[Tableau Software|Tableau]]\n* [[ThoughtSpot]]\n* [[TIBCO]]\n* [[Sisense]]\n\n{{colend}}\n\n==References==\n{{Reflist}}\n\n[[Category:Types of analytics]]\n[[Category:Big data|analytics]]\n[[Category:Business intelligence]]\n[[Category:Data management]]']
['Data processing system', '466099', '{{other uses2|Data processing}}\n{{refimprove|date=July 2013}}\n\nA \'\'\'data processing system\'\'\' is a combination of machines, <!-- "data processing" is specific to machines, there is no data processing in nature, see the OED --> people, and processes that for a set of inputs produces a defined set of outputs.<ref>The first machines used for data processing were [[Unit record equipment|punched card machines]], now [[Computer]]s are used.</ref>  The inputs and outputs are interpreted as data, facts, information, ... depending on the interpreter\'s relation to the system. A common synonymous term is "[[Information systems#Types of information systems|information system]]".<ref name=Ralston>{{cite book|title=Encyclopedia of Computer Science 4th ed.|author=Anthony Ralston et al (ed.)|year=2000|publisher=Nature Publishing Group|page=865}}</ref>\n\nA data processing system may involve some combination of:\n* [[Data conversion|Conversion]] converting data to another format.\n* [[Data validation|Validation]] &ndash; Ensuring that supplied data is "clean, correct and useful."\n* [[Sorting]] &ndash; "arranging items in some sequence and/or in different sets."\n* [[Summary statistic|Summarization]] &ndash; reducing detail data to its main points.\n* [[Aggregate data|Aggregation]] &ndash; combining multiple pieces of data.\n* [[Statistical analysis|Analysis]] &ndash; the "collection, organization, analysis, interpretation and presentation of data.".\n* Reporting &ndash; list detail or summary data or computed information.\n\n==Types of data processing systems==\n\n===By application area===\n\n====Scientific data processing====\nScientific data processing "usually involves a great deal of computation (arithmetic and comparison operations) upon a relatively small amount of input data, resulting in a small volume of output." <ref name=Reddy>{{cite book|last=Reddy|first=R.J.|title=Business Data Processing & Computer Applications|year=2004|publisher=A P H Publishing Corporation|location=New Dehli|isbn=8176486493|page=17|url=https://books.google.com/books?id=FLKoXCts9ssC&lpg=PA17&dq=%22scientific%20data%20processing%22&pg=PA17#v=onepage&q=%22scientific%20data%20processing%22&f=false}}</ref>\n\n====Commercial data processing====\nCommercial data processing "involves a large volume of input data, relatively few computational operations, and a large volume of output."<ref name=Reddy />  Accounting programs are the prototypical examples of data processing applications. [[Information systems|Information systems (IS)]] is the field that studies such organizational computer systems.\n\n====Data analysis====\n"[[Data analysis]] is a body of methods that help to describe facts, detect patterns,\ndevelop explanations, and test hypotheses."<ref>{{cite web|last=Dartmouth College|title=Introduction: What Is Data Analysis?|url=http://www.dartmouth.edu/~mss/data%20analysis/Volume%20I%20pdf%20/006%20Intro%20%28What%20is%20the%20weal.pdf|accessdate=July 5, 2013}}</ref>  For example, data analysis might be used to look at sales and customer data to "identify connections between products to allow for cross selling campaigns."<ref>{{cite book|last1=Berthold|first1=M.R.|last2=Borgelt|first2=C|last3=Hőppner|first3=F.|last4=Klawonn|first4=F|title=Guide to Intelligent Data Analysis|year=2010|publisher=Springer|isbn=978-1-84882-260-3|page=15}}</ref>\n\n===By service type<ref name=Ralston />=== \n\n* [[Transaction processing system|Transaction processing systems]]\n* [[Information retrieval|Information storage and retrieval systems]]\n* Command and control systems\n* Computing service systems\n* [[Control system|Process control systems]]\n* Message switching systems\n\n==Examples==\n===Simple example===\nA very simple example of a data processing system is the process of maintaining a check register.  Transactions&mdash; checks and deposits&mdash; are recorded as they occur and the transactions are summarized to determine a current balance.  Monthly the data recorded in the register is reconciled with a hopefully identical list of transactions processed by the bank.\n\nA more sophisticated record keeping system might further identify the transactions&mdash; for example deposits by source or checks by type, such as charitable contributions.  This information might be used to obtain information like the total of all contributions for the year.\n\nThe important thing about this example is that it is a \'\'system\'\', in which, all transactions are recorded consistently, and the same method of bank reconciliation is used each time.\n\n===Real-world example===\nThis is a [[flowchart]] of a data processing system combining manual and computerized processing to handle [[accounts receivable]], billing, and [[general ledger]]\n\n[[File:Stockbridge system flowchart example.jpg]]\n<ref>the highest acceleration of data processing the point of software</ref>\n\n==References==\n{{Reflist}}\n\n== See also ==\n* [[Data processing]]\n* [[Electronic data processing]]\n* [[Computational science|Scientific computing]]\n* [[Information processing system]] (broader term)\n\n== Further reading ==\n* Bourque, Linda B.; Clark, Virginia A. (1992) Processing Data: The Survey Example. (Quantitative Applications in the Social Sciences, no. 07-085). Sage Publications. ISBN 0-8039-4741-0\n\n\n[[Category:Data management]]\n[[Category:Data processing]]']
['National Data Repository', '30966530', 'A \'\'\'National Data Repository\'\'\' (\'\'\'NDR\'\'\') is a data bank that seeks to preserve and promote a country’s natural resources data, particularly data related to the petroleum [[exploration and production]] (E&P) sector.\n\nA National Data Repository is normally established by an entity that governs, controls and supports the exchange, capture, transference and distribution of E&P information, with the final target to provide the State with the tools and information to assure the growth, govern-ability, control, independence and sovereignty of the industry.\n\nThe two fundamental reasons for a country to establish an NDR are to \'\'\'preserve\'\'\' data generated inside the country by the industry, and to \'\'\'promote\'\'\' investments in the country by utilizing data to reduce the exploration, production, and transportation business risks.\n\nCountries take different approaches towards preserving and promoting their natural resources data. The approach varies according to a country’s natural resources policies, level of openness, and its attitude towards foreign investment.\n\n==Data types==\nNDRs store a vast array of data related to a country’s natural resources. This includes wells, [[Well logging|well log data]], well reports, [[core sample]]s, [[seismic]] surveys, [[Seismic inversion#Post-stack seismic resolution inversion|post-stack seismic]], field data/tapes, seismic (acquisition/processing) reports, [[Oil production|production]] data, [[geological map]]s and reports, license data and [[geologic modeling|geological models]].\n\n==Funding models==\nSome NDRs are financed entirely by a country’s government. Others are industry-funded. Still some are hybrid systems, funded in part by industry and government.\nNDRs typically charge fees for data requests and for data loading. The cost differs significantly between countries. In some cases an annual membership is charged to oil companies to store and access the data in the NDR.\n\n==Standards body==\n[[Energistics]] is the global energy standards resource center for the upstream oil and gas industry.\n\nEnergistics National Data Repository Work Group:\nThe standards body is Energistics.<ref>[http://energistics.org/energistics-standards-directory Energistics]</ref>\n\n===Energistics-standards-directory===\nGlobal regulators of upstream oil and natural gas information, including seismic, drilling, production and reservoir data, formed the National Data Repository (NDR) Work Group in 2008 to collaborate on the development of data management standards and to assist emerging nations with hydrocarbon reserves to better collect, maintain and deliver oil and gas data to the public and to the industry.\n\nTen countries, led by the [[Netherlands]], [[Norway]] and the [[United Kingdom]], formed NDR to share best practices and to formalize the development and deployment of data management standards for regulatory agencies. The other countries involved in the NDR Work Group’s formation are [[Australia]], [[Canada]], [[India]], [[Kenya]], [[New Zealand]], [[South Africa]] and the [[United States]].\n\nAnnual NDR Conference: Approximately every 18 months Energistics organizes a National Data Repository Conference. The purpose is to provide government and regulatory agencies from around the world an opportunity to attend a series of workshops dedicated to developing data exchange standards, improving communications with the oil and gas industry and learning data management techniques for natural resources information.<ref>[http://www.energistics.org/regulatory/national-data-repository-ndr-work-group/ndr-meetings NDR Conference page on the Energistics website]</ref>\n\n===Society of Exploration Geophysicists and The International Oil and Gas Producers Association===\nThe SEG is the custodian of the SEG standards which are used for the exchange, retention and release of seismic data.  They are commonly used by National Data Repositories with the SEGD and SEGY being the field and processed exchange standards respectively.\n\n==NDRs around the world==\n\n[https://www.google.com/maps/d/viewer?mid=1by9vDDoWwnZD0f8vNt_le2TThTU Click here to see a map of the NDRs around the world]\n\n{| class="wikitable sortable"\n|-\n! Country  !!  Name  !!  Agency  !!  Scope  !! Status !!   Purposes   !! Data types/volumes !! Standards used !! Funding !! Website\n|-\n| {{flagcountry|Algeria}} || Banque de Données Nationale "BDN" || Agence Nationale pour la Valorisation des Ressources en Hydrocarbures (ALNAFT) || Onshore and Offshore Algeria || Ongoing project - agency created by new law in 2005 || Custodian of all E&P data of the country || Cultural, Seismic 2D & 3D, Wells, Data Wells, Wells report, Production, Facilities, Economical and Fiscality, Interpretation, Physical assets index, Data drilling, Transcription, Vectorisation, digitalization || ASCII, SEGY, UKOOA, LAS, DLIS, LIS, PDS, BIT, RODE, PDF, TIF....etc || Government funding/Agency revenue || http://www.alnaft.gov.dz/\n|-\n| {{flagcountry|Colombia}} || EPIS || Agencia Nacional de Hidrocarburos (ANH) || Onshore and offshore Colombia || Created originally for Ecopetrol and transferred to ANH when it was established in 2003. New system launched December 2009 || Promote and preserve all the technical E&P information assets of the country || wells, surveys, licenses, seismic sections, well reports, maps || REST Web services || Government funding ||http://www.epis.com.co\n|-\n|  {{flagcountry|Canada}} || CNSOPB || Nova Scotia Offshore Petroleum Board – Geoscience Research Centre- Digital Data Management Centre (DMC) || Offshore Nova Scotia, Canada ||  Operational since 2007 || To provide an effective & efficient system for the management of digital petroleum data, assist explorers in easily obtaining access to large volumes of data via the web, Data Preservation and Data Distribution ||  Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports || LAS, DLIS, SEGY || Funded 50/50 by the Federal and Provincial Governments with some funds from industry through cost recovery  || http://www.cnsopb.ns.ca/\n|-\n| {{flagcountry|Australia}} || PIMS ||Geoscience Australia || ||Active  || Various online and web based systems exist for E &P, geosciences\n||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports ||  ||  ||\nhttp://dbforms.ga.gov.au/pls/www/npm.pims_web.search\n|-\n| {{flagcountry|Western Australia}} || WAPIMS ||Government of Western Australia   || || Active || WAPIMS is a petroleum, geothermal and minerals exploration database ||Contains data on titles, wells, geophysical surveys and other petroleum exploration and production data submitted to DMP by the petroleum industry.\n  ||  ||  || http://dmp.wa.gov.au\n|-\n| {{flagcountry|New South Wales}} ||  ||Government of New South Wales ||  || Active  || Various online geoscience databases to assist New South Wales including DIGS  ||  ||  ||  ||\nhttp://www.dpi.nsw.gov.au/minerals\nhttp://digsopen.minerals.nsw.gov.au/\n|-\n| {{flagcountry|Northern Territory}} ||  || Government of Northern Territory  ||  || Active  || Various online geoscience databases to assist Northern Territories ||  ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||\nhttp://www.nt.gov.au/d/Minerals_Energy/index.cfm?header=Petroleum\t\n|-\n| {{flagcountry|Queensland}} ||  || Government of Queensland ||  || Active  || Various online geoscience databases to assist Queensland including Q-DEX ||  ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||\nhttps://www.dnrm.qld.gov.au/\n|-\n| {{flagcountry|South Australia}} || SARIG || Government of South Australia  ||  || Active  || Various online geoscience databases to assist South Australia such as PEP-SA  || ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||\nhttp://petroleum.statedevelopment.sa.gov.au/data_and_publications/sarig\nhttps://sarig.pir.sa.gov.au/\n|-\n| {{flagcountry|Tasmania}} ||  ||  ||  ||   ||Various online geoscience databases to assist Tasmania   || || Active ||  ||\nhttp://www.mrt.tas.gov.au/portal/page?_pageid=35,1&_dad=portal&_schema=PORTAL\t\t\n|-\n| {{flagcountry|China }} || CNPC  || Chinese National Petroleum Corporation  ||  ||   || Various oil companies in China with CNPC the largest and parent of Petrochina ||  ||  ||  ||\nhttp://www.cnpc.com.cn/en/\nhttp://www.petrochina.com.cn/ptr/\nhttp://www.cnooc.com.cn/\nhttp://english.sinopec.com/index.shtml\n|-\n| {{flagcountry|Russia}} ||  ||  Sakhalin, DIGC RDC ||  || Various oil companies in Russia the largest being Rosneft which is state owned  ||  ||  ||  ||  ||\nhttp://www.rosneft.com\nhttp://www.lukoil.com\nhttp://www.tnk-bp.com/en/\nhttp://www.surgutneftegas.ru/\nhttp://www.gazprom-neft.com/\nhttp://www.tatneft.ru/wps/wcm/connect/tatneft/portal_rus/homepage/\n|-\n| {{flagcountry|Indonesia}} || Indonesia\'s National Data Centre (NDC) for petroleum, energy and minerals data || Agency for Research and Development in the Ministry of Energy and Mineral Resources of the Republic of Indonesia||Onshore & Offshore || In 1997 Indonesia established Migas Data Management (MDM) operated by PT. Patra Nusa Data (PND)  || PND manages and promotes petroleum investment opportunities by compiling and value adding available petroleum data and information. ||  ||  ||  || http://www.patranusa.com/\n|-\n|  {{flagcountry|New Zealand}} || New Zealand Online Exploration Database || New Zealand Petroleum & Minerals, Ministry of Business Innovation & Employment || New Zealand onshore and offshore out to the outer continental shelf.  || Opened to public in April 2007. || Data preservation, Investment facilitation, aid in monitoring regulatory compliance, maximise the return to the nation by informing public policy and business strategy. || Wells, well log curves, petroleum reports (includes wells and surveys), mineral reports, coal reports, cores and samples, seismic surveys, post-stack seismic, field data/tapes, seismic acquisition/processing reports, geophysical and geochemical data acquired in mineral and coal exploration (incorporated as enclosures to reports), VSP (incorporated as enclosures to reports), Seismic survey observer logs. GIS data and projects (minerals and coal). Estimated total NDR Size: 2.5 TB loaded, 3.0 TB staged for loading, 40 TB field data offline.  || Closely follow Australian digital reporting standards. No naming standards for wells and surveys.  || 50% Government funding, 50% third party permit (license) fees paid by exploration companies. || https://data.nzpam.govt.nz\n|-\n| {{flagcountry|Jordan}} ||NRA  || Jordan Natural Resources Authority (NRA) ||Onshore || Active    || Online data room allows users to browse and select large data set quickly in a controlled and secure environment ||Reserves land records, field data, maps, engineering, seismic data, geological studies and well files.  ||  ||  || http://www.jordan.gov.jo\n|-\n| {{flagcountry|Angola}} ||  || Sonangol || Offshore Angola || Active || Promotion, Organisation & Management of all Exploration & Production (E&P) Data of Angola || Wells, surveys, licenses, seismic sections, well reports, maps ||  || Norad/OfD and NPD assistance || http://www.sonangol.co.ao\n|-\n| {{flagcountry|France}} ||BEPH ||  || French Territory || || Interactive maps of French territory of oil data are available to Internet users which includes: Permits for petroleum exploration, seismic exploration, oil drilling  (data, documents available) || Wells, Surveys, Licenses, Seismic Sections, Well Reports, Maps ||  ||  || http://www.beph.net/\n|-\n| {{flagcountry|São Tomé and Príncipe }} || ANP-STP  || National Petroleum Agency of São Tomé & Principe (ANP-STP)  || Offshore || ||  ||  ||  ||Norad/OfD and NPD assistance  || http://www.anp-stp.gov.st\n|-\n| {{flagcountry|Tanzania}} || TPDC  || Tanzania Petroleum Development Corp  ||  || Began in the early 1990s with Norwegian assistance || An E & P data archive centre || Geophysical survey data, Geological studies, Well drilling and completion reports, Cores and drill steam data ||  || Norad/OfD and NPD assistance || http://www.tpdc-tz.com\n|-\n| {{flagcountry|Oman}} || OGDR || Department of Petroleum Concession, Ministry of Oil and Gas || Onshore & Offshore || Operational, tendering OGDR as a managed service (fully outsourced) June 2015 || Preservation of E&P data, support concession promotion. || Well-related Data: Header, deviation, tops, field and processed logs, well documents. Seismic-related Data: Field and processed 2D/3D, Gravimag, VSP. || OGDR Data Submission Standard that uses industry standards where possible i.e. DLIS, SEG, UKOOA. || Government & concession holders.||  http://www.mog.gov.om/english/tabid/309/Default.aspx\n|-\n| {{flagcountry|Netherlands}} || DINO || The Geological Survey of the Netherlands, a division of [[Netherlands Organisation for Applied Scientific Research|TNO]] || The Netherlands including offshore waters ||Started in 2004. Currently BRO is being planned to succeed DINO. || To archive subsurface data of the Netherlands in one repository and provide easy access to the data to encourage multiple use of data.||  || WMS web services. DINO uses own naming conventions || 100% Government funding || http://www.nlog.nl/en/home/NLOGPortal.html\n|-\n| {{flagcountry|India}} ||DGH  || Directorate General of Hydrocarbons (DGH) ||  || Active - scheduled operation by April 2015\n ||  Establishing national data archival, improving data quality and access for quality exploration covering large area under exploration and providing basis for long term energy policy formulation as well as support OALP  ||Wells, Well Logs, Cores, Scanned core images, Seismic, Reports, production, Technical Reports\n  ||  ||Government of India || http://www.dghindia.org/DataManagement.aspx#\n|-\n| {{flagcountry|Sri Lanka}} || PRDS || Ministry of Petroleum and Petroleum Resources Development ||  || Active since 2009 ||   The PRDS developed a website to disseminate petroleum data and information to public and to investors to assist promotion of offshore areas to attract investors for petroleum exploration ||Wells, surveys, licenses, seismic sections, well reports, maps. Data historic and current, archived on different media (paper, mylar, magnetic tape) || ||  || http://www.prds-srilanka.com/data/onlineData.faces\n|-\n| {{flagcountry|Argentina}} || ENARSA || Energia Argentina SA  || || Established in 2006  ||  ||  ||  ||  ||  http://www.enarsa.com.ar http://energia.mecon.gov.ar/upstream/US_Pterminados.asp\n|-\n| {{flagcountry|Peru}} || PeruPetro ||  ||  || Active ||  ||  || ||  || http://www.perupetro.com.pe\n|-\n| {{flagcountry|Kazakhstan}} ||  || Ministry of Energy and Mineral Resources of the Republic of Kazakhstan (MEMR) ||  || Active ||  ||  ||  ||  || http://www.petrodata.kz\n|-\n| {{flagcountry|Pakistan}} || PPEPDR || Directorate General Petroleum Concessions (DGPC)  ||  || Active since 2001 ||  || Repository contains more than 10 terabytes of secure petrotechnical data ||  ||  || http://www.ppepdr.net/\n|-\n| {{flagcountry|Nigeria}} ||Department of Petroleum Resources  ||  ||  || Active since December 2003.\n || Preserve, maintain the integrity and promote the National E&P data assets with improved quality, efficiency and accessibility in the most rapid, secure and reliable manner|| || International and PetroBank data management standards || Funded by Establishment Costs - one-off funding by Government and Running Costs - Subscription &  Transaction Fees by Operators ||http://ndr.dprnigeria.com/\n|-\n| {{flagcountry|Turkey}} || PetroBank MDS  || Turkish Petroleum Corporation (TPAO). It is NOC of Turkey. || 36˚-42˚ northern parallel and the 26-45˚ eastern meridian.  || Operational since 2007 || Data assets preservation, easy access to assets, assets access controlling and auditing, consolidation of assets, national archive, central management of all assets, standardization of assets according to international standards and naming conventions, working with the most convenient assets. || Wells, Well log curves, well reports, cores and samples, seismic surveys, post-stack seismic, field data/tapes and seismic acquisition/processing reports. || International and PetroBank data management standards || Funded fully by the Turkish Petroleum Corporation. Service usage is free of charge.  || http://www.tpao.gov.tr\n|-\n| {{flagcountry|Norway}} || DISKOS- Norwegian National Data Repository || Norwegian Petroleum Directorate (NPD) and DISKOS Group of oil companies || Norwegian continental shelf || Started in 1995 || To ensure compliance with NPD reporting regulations for digital E&P data. To reduce data redundancy. To ensure that data is made generally available to the oil and gas industry and to society as a whole Long term preservation of data.\n || Wells, Well Log Curves, Seismic Surveys, field, pre-stack &  post-stack seismic, seismic reports, production data (monthly allocated).Size of NDR estimated at more than 3 Petabytes.\n || SEG-D for seismic field data, SEG-Y for pre-stack and post-stack seismic data (currently only limited amounts of field and pre-stack data) All relevant well data standards such as LIS, DLIS, LAS, SPLA, SCAL etc. PDF and TIF are also used. || Costs are shared equally between all participating oil companies (around 50) in the Diskos consortium, including the NPD. In addition reporting companies pay to submit and download data. All Norwegian universities have free access to public data in Diskos. Non-oil companies can apply for Associated Membership, there are currently around 25 such members. ||http://www.diskos.no/ http://www.npd.no\n|-\n| {{flagcountry|United Kingdom}} || CDA || CDA Common Data Access Ltd  || UK Offshore Waters || Wells went live in 1995. Infrastructure started operations in 2000. Seismic went live in 2009. Estimated NDR size: 6 Terabytes||Save costs for licenses,Improve access to data,Comply with regulations || Well log curves, Well reports, Post-stack seismic, Seismic reports, VSP, deviation and test data. Estimated NDR size: 6 Terabytes || CDA has adopted DECC’s naming standards for wells and surveys and continues to work closely with DECC and industry to identify a range of standards (see the CDA and DECC websites for more on this) || Owned by the UK oil and gas industry ||\nhttp://www.ukoilandgasdata.com\nhttp://www.gov.uk/oil-and-gas-petroleum-operations-notices\nhttp://www.cdal.com\n|-\n| {{flagcountry|United Kingdom}} || UKOGL || UK Onshore Geophysical Library || UK onshore || In operation since 1994. Managed and operated by Lynx Information Systems Ltd on behalf of UKOGL. || Custodian of all UK onshore seismic data || Seismic, well tops, logs, cultural. Current archive size approx 6TB || SEGY, UKOOA, LAS, DLIS || Self-funded through data sales ||\nhttp://www.ukogl.org.uk\nhttp://maps.lynxinfo.co.uk/UKOGL_LIVE/map.html\n|-\n| {{flagcountry|Brazil}} || ANP || Agência Nacional do Petróleo (ANP)||  || BDEP formed in May 2000\n || ||Stores seismic, well log, post stack and pre-stack seismic data and potential field data(Grav/Mag)  || ANP standards in place || Funded by Members || http://www.bdep.gov.br\n|-\n| {{flagcountry|Mexico}} || Ditep || Pemex ||  ||Established in 2002  || || Promotes and preserve all the technical E&P information assets of the country ||  || ||http://www.pep.pemex.com/index.html\n|-\n|  {{flagcountry|Israel}} ||  || The Ministry of National Infrastructures  || || Exploratory ||  ||  ||  ||  || http://www.mni.gov.il/mni/en-US/NaturalResources/OilandgasExploration/OilMaps/\n|-\n| {{flagcountry|Cyprus}} || MCIT || Ministry of Commerce, Industry and Tourism-Energy Service || Offshore  || Promotional || Responsible for granting licences for prospecting, exploration and exploitation of hydrocarbons ||  ||  ||  || http://www.mcit.gov.cy/mcit/mcit.nsf/dmlhexploration_en/dmlhexploration_en?OpenDocument\n|-\n| {{flagcountry|South Africa}} ||   || Petroleum Agency of South Africa ||  || Active || Seismic data, Well data, Samples, reports and diagrams ||  ||Standards: Formats – SEGD, SEGY, LIS, LAS, PDF and TIFF, Media – 3480, 3590, DLT, 8mm Exabyte, DAT   || From 2010 funded by Government || http://www.petroleumagencysa.com\n|-\n| {{flagcountry|Kenya}} || National Data Center (NDC) || National Oil Corporation of Kenya || Offshore and Onshore || Began in 2007, system implemented in 2010.  || Digital data preservation, National archive, to implement integrated data management systems, provide easy access to quality-controlled data for internal and external customers, attract oil and gas exploration investment and to reduce data management costs. || Wells, well log curves, well reports, post-stack seismic, field data/tapes, seismic acquisition/processing reports, interpretive maps and reports. || Seismic data- SEGY. 3590 or 3592 data cartridges.  || 100% Government Funded || http://www.nockenya.co.ke/\n|-\n| {{flagcountry|United States}} || BOEMRE || [[Bureau of Ocean Energy Management, Regulation and Enforcement|Bureau of Ocean Energy, Management, Regulation and Enforcement (BOEMRE)]] || Gulf of Mexico || Has replaced the former Minerals Management Service (MMS) ||  ||  ||  ||  || http://www.gomr.boemre.gov/homepg/data_center.html\n|-\n| {{flagcountry|United States}} || NGRDS || National Geoscience Data Repository System (NGDRS) ||  ||  || NGRDS is a system of geoscience data repositories, providing information about their respective holdings accessible through a web-based supercatalog. ||  || geologic, geophysical, and engineering data, maps, well logs, and samples || DOE has provided funds for the NGDRS since 1993  ||\nhttp://www.agiweb.org/ngdrs/index.html\nhttp://www.energy.gov/\nhttp://www.agiweb.org/index.html\n\nList of Repositories in US listed also as directory\nhttp://www.agiweb.org/ngdrs/overview/datadirectory.html\n|-\n| {{flagcountry|Cambodia}} ||CNPA ||Cambodia National Petroleum Authority  || Onshore & Offshore  ||  ||Promotion and preservation of technical E&P information assets of the country  || ||  ||Norad/OfD and NPD assistance || http://www.cnpa-cambodia.com/\n|-\n| {{flagcountry|Afghanistan}} ||MOM ||Ministry of Mines of the Islamic Republic of Afghanistan (MoM)\n|| Onshore ||  ||Promotion and preservation of technical E&P information assets of the country  || ||  ||Norad/OfD and NPD assistance ||http://mom.gov.af/en/news/1637\n|-\n| {{flagcountry|Bangladesh}} || MOEMR  ||Hydrocarbon Unit, Ministry of Power, Energy and Mineral Resources (MOEMR)|| Onshore & Offshore || Active and ongoing via HCU unit since 2005\n|| A mini-data bank has established in the HCU to handle Production data, Resource data by using Database & GIS Software 2005 and promotion of technical E&P information assets of the country  || ||Funded assistance    ||Norad/OfD and NPD assistance ||\nhttp://www.hcu.org.bd/\nhttp://www.petrobangla.org.bd\nhttp://www.bapex.com.bd\n|-\n| {{flagcountry|Ethiopia}} || MOME  ||Ministry of Mines and Energy Ethiopia  ||  || Active and ongoing ||Promotion of technical E&P information assets of the country   || ||  || ||\nhttp://www.mome.gov.et/petroleum.html\n|-\n| {{flagcountry|Cameroon}} ||SNH ||SNH Cameroon   ||  || Active & Ongoing || Preservation and promotion of technical E&P information assets of the country  || ||  || || http://www.snh.cm\n|-\n| {{flagcountry|Malaysia}} ||PIRI  ||Petronas  ||  ||Yet to establish full NDR  || Promotion and preservation of technical E&P information assets of the country  || ||  || || http://www.petronas.com.my\n|-\n| {{flagcountry|Spain}} || ATH  ||  ||  ||  ||Online GIS databases to geophyscial information SIGEOF and ATH (Archivo de Hydrocarbures) || ||  ||  ||\nhttp://www.mityc.es/energia/petroleo/Exploracion/Paginas/Estadisticas.aspx\nhttp://hidrocarburos.mityc.es/ath/\nhttp://www.igme.es/internet/sigeof/INICIOsiGEOF.htm\n|-\n| {{flagcountry|Morocco}} ||ONHYM  || Office National des Hydrocarbures et des Mines ||  ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||  || http://www.onhym.com\n|-\n| {{flagcountry|Madagascar}} ||OMNIS  ||  ||  ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||Norad OfD and NPD assistance    ||\n|-\n| {{flagcountry|Sudan}} || PIC (Petroleum Information Center) || Ministry of Oil and Gas ||  || Active since 2000  || Preserve and promote E&P data,managing Oil Museum || Wells, well log, well reports, cores and samples,seismic (acquisition/processing) reports, production data,GIS ||  ||  ||\nhttp://www.spc.gov.sd\n|-\n| {{flagcountry|Morocco}} ||ONHYM  || Office National des Hydrocarbures et des Mines ||  ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||  || http://www.onhym.com\n|-\n| {{flagcountry|Nicaragua}} ||MEM || ||  ||  Active & ongoing ||  || ||  || Norad OfD and NPD assistance   ||\nhttp://www.ine.gob.ni\nhttp://www.mem.gob.ni\n|-\n| {{flagcountry|Iraq}} ||MoO ||  Ministry of Oil Republic of Iraq||  || Active and Ongoing since 2005  || MoO establishing a centralized data base and NDR for Iraqi petroleum data and to ensure that data & information from petroleum activities is made available and attract more investors by promoting the petroleum activities || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Norad/OfD and NPD assistance || http://www.oil.gov.iq\n|-\n| {{flagcountry|Latvia}} ||LEGMC  ||Latvian Environment, Geology and Meteorology Centre    || Offshore & Onshore || || An E & P data archive centre which provides data available for purchase  || Geological (well and seismic data, maps, reports etc.)  ||  ||  ||  http://mapx.map.vgd.gov.lv/geo3/VGD_OIL_PAGE/index.htm\n|-\n|  {{flagcountry|Albania}} || AKBN  || National Agency of Natural Resources ||  ||  || Generates and promotes exploration opportunities in Albania, maintains archive of E & P data. ||  ||  ||  || http://www.akbn.gov.al/index.php?ak=details&cid=5&lng=en\n|-\n| {{flagcountry|Uganda}} ||PEPD|| Petroleum Exploration & Production Dept (PEPD) || Onshore || || || Technical E & P data archive and information ||  ||Norad/Ofd assistance || http://www.statehouse.go.ug/government.php?catId=10 http://www.energyandminerals.go.ug\n|-\n| {{flagcountry|Zambia}} |||| Ministry of Mines and Minerals Development, Geological Survey Department (GSD)\n || Onshore ||Active and ongoing || || Technical E & P data archive and information - Technical Records Unit||  ||Norad/Ofd & NPD assistance ||\nhttp://www.zambiageosurvey.gov.zm/\n|-\n| {{flagcountry|Ivory Coast}} ||MME|| Ministry of Mines & Energy || Onshore & Offshore || || || ||  ||Norad/Ofd and NPD assistance ||\nhttp://www.cotedivoirepr.ci/\nhttp://www.petroci.ci/index.php?numlien=31\n|-\n| {{flagcountry|Romania}} || || National Agency for Mineral Resources  ||  || ||Promotion and preservation of technical E&P information assets of the country  || ||  ||  || http://www.namr.ro/main_en.htm\n|-\n| {{flagcountry|Fiji}} ||SOPAC ||Mineral Resources Dept Fiji     |||| Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia  || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || In part externally managed ||\nhttp://www.mrd.gov.fj/gfiji/\nhttp://www.mrd.gov.fj/gfiji/petroleum/petroleum.html\nhttp://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html\nhttp://www.sopac.org/index.php/member-countries/fiji-islands\n|-\n| {{flagcountry|Papua New Guinea}} ||SOPAC||Department of Petroleum and Energy ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia   || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Externally managed ||\nhttp://www.petroleum.gov.pg\nhttp://www.petrominpng.com.pg/about.html\nhttp://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html\nhttp://www.sopac.org/index.php/member-countries/papua-new-guinea\n|-\n| {{flagcountry|Solomon Islands}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia   || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations  || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Externally managed ||\nhttp://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html\nhttp://www.sopac.org/index.php/member-countries/solomon-islands\n|-\n| {{flagcountry|Tonga}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia    || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Externally managed ||\nhttp://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html\nhttp://www.sopac.org/index.php/member-countries/tonga\n|-\n| {{flagcountry|Vanuatu}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia  || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Externally managed ||\nhttp://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html\nhttp://www.sopac.org/index.php/member-countries/vanuatu\n|-\n| {{flagcountry|Guyana}} ||GGMC  ||Guyana Geology and Mines Commission  ||  ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||  || http://www.ggmc.gov.gy\n|-\n| {{flagcountry|Syria}} ||SPC  || Syrian Petroleum Company  ||  ||  || || ||  ||  || http://www.spc-sy.com/en/aboutus/aboutus1_en.php\n|-\n| {{flagcountry|Liberia}} ||NOCAL || National Oil Company of Liberia ||  ||  ||Promotion and preservation of technical E&P information assets of the country   || ||  ||  || http://www.nocal-lr.com/\n|-\n| {{flagcountry|Chile}} ||ENAP || National Oil Company of Chile ||  ||  ||Promotion and preservation of technical E&P information assets of the country   || ||  ||  || http://www.enap.cl\n|-\n| {{flagcountry|Thailand}} ||PTTEP ||PTT Exploration and Production Public Company Ltd  ||  ||  ||Promotion and preservation of technical E&P information assets of the country   || ||  ||  || http://www.pttep.com/\nhttp://www.pttep.com/en/index.aspx\n|-\n| {{flagcountry|Venezuela}} ||PDVSA ||Petroleos de Venezuela  || Onshore & Offshore ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||  || http://www.pdvsa.com/\n|-\n| {{flagcountry|Trinidad}} || || Trinidad Ministry of Energy and Energy Affairs ||  ||  ||Promotion and preservation of technical E&P information assets of the country   || ||  ||  || http://www.energy.gov.tt/energy_industry.php?mid=31\nhttp://www.petrotrin.com/Petrotrin2007/UpstreamBusiness.htm\n|-\n| {{flagcountry|Mozambique}} || NAPD ||  ||  || Established in 1999 under NORAD support  || To ensure that data & information from petroleum activities is made available and attract more investors by promoting the petroleum activities || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || National Budget and INP funds || http://www.inp.gov.mz\n|-\n| {{flagcountry|Denmark}} || || Danish Energy Agency  ||  ||   || Online GIS service for wells and license data  || ||  ||  ||\nhttp://www.ens.dk/EN-US/OILANDGAS/Sider/Oilandgas.aspx\n|-\n| {{flagcountry|Dominican Republic}} || || Directorate of Hydrocarbons  ||  ||   ||  ||  ||  ||  || http://www.dgm.gov.do/sdhidrocarburo/index.html\n|-\n| {{flagcountry|Equatorial Guinea}} || ||   ||  ||   ||   Exploration databank for Equatorial Guinea||  ||  || ||\nhttp://www.equatorialoil.com\nhttp://www.equatorialoil.com/database.html\n|-\n| {{flagcountry|Faroe Islands}} || Jardfeingi || Jardfeingi Faorese Earth and Energy Directorate  ||  ||   ||  Promotion of exploration and licensing rounds ||  ||  || || http://www.jardfeingi.fo\n|-\n| {{flagcountry|Philippines}} || PNOC || Philippine National Oil Company  ||  ||   ||  Promotion of exploration and licensing rounds ||  ||  || ||\nhttp://www.pnoc.com.ph\nhttp://www.pnoc-ec.com.ph/business.php?id=2\n|-\n| {{flagcountry|Greenland}} || GreenPetroData || MMR- Ministry of Mineral Resources  ||  ||  ||  Web and GIS system providing access to all Released Well and Geophysical data. ||  ||  || ||\nhttps://www.greenpetrodata.gl/\nhttp://govmin.gl/\n|-\n| {{flagcountry|Iceland}} || Iceland Continental Shelf Portal (ICSP)  || Orkustofnunn - National Energy Authority   ||Offshore ||   The Iceland Continental Shelf Portal (ICSP)|| Provides access to information about data pertaining to the Icelandic Continental Shelf, in particular initially to the northern Dreki Area to assist with licensing round promotion || ||  ||  ||\nhttp://www.os.is\nhttp://www.nea.is/oil-and-gas-exploration/\n|-\n| {{flagcountry|Myanmar}} || MOGE || Myanmar Oil & Gas Enterprise   ||  ||   ||    ||  ||  || ||\nhttp://www.energy.gov.mm/upstreampetroleumsubsector.htm\n|-\n| {{flagcountry|Yemen}} || PEPA || Petroleum Exploration and Production Authority (PEPA)\n   ||  ||   ||    ||  ||  || ||\nhttp://www.pepa.com.ye/\n|-\n| {{flagcountry|Tunisia}} || ETAP || Enterprise Tunisienne D’Activities Petrolieres   ||  ||   ||  Promotion and preservation of technical E&P information assets of the country    ||  ||  || ||\nhttp://www.etap.com.tn\n|-\n| {{flagcountry|Gabon}} || DGH || Direction Generale des Hydrocarbures (DGH)||  ||   ||  ||  ||  || ||\nhttp://www.gabon-industriel.com/les-actions/energie/petrole\n|-\n| {{flagcountry|Republic of the Congo  }} || SNPC || Société Nationale des pétroles du Congo ||  ||   ||    ||  ||  || ||\n|-\n| {{flagcountry|Mali}} || Aurep || {{Not a typo|Autorite}} pour la Promotion de la Recherce des Petroliere au Mali\n||  ||   ||   Databank service managing the geological and geophysical data relative to petroleum research. ||  ||  || ||\nhttp://www.aurep.org\nhttp://www.aurep.org/htmlpages/mali.html\n|-\n| {{flagcountry|Guatemala}} || MEM || Dirección General de Hidrocarbures||  ||   ||  Online maps and images of wells, seismic, licenses, protected areas, exploration and production ||  ||  || ||\nhttp://www.mem.gob.gt/Portal/home.aspx\nhttp://www.mem.gob.gt/Portal/Home.aspx?secid=25\n|-\n| {{flagcountry|Iran}} || NIOC || National Oil Company of Iran ||  ||   ||   ||  ||  || ||\nhttp://www.nioc.ir\n|-\n| {{flagcountry|Libya}} || NOC || NOC Libya ||  ||   ||  Virtual data room in place for promotion of exploration and exploitation of hydrocarbons ||  ||  || ||\n|-\n| {{flagcountry|United Arab Emirates}} || ADNOC || Abu Dhabi National Oil Company ||  ||   ||  ||  ||  ||  ||\nhttp://www.adnoc.ae\n|-\n| {{flagcountry|Qatar}} ||  || Qatar Petroleum ||  ||   ||  ||  ||  ||  ||\nhttp://www.qp.com.qa\n|-\n| {{flagcountry|South Korea}} || KNOC || Korea National Petroleum Corporation ||  ||   ||  ||  ||  ||  ||\nhttp://www.knoc.co.kr\n|-\n| {{flagcountry|Seychelles}} || SNOC || Seychelles National Oil Company ||  ||   ||  ||  ||  ||  ||\n|-\n| {{flagcountry|Saudi Arabia}} ||  || Saudi Aramco ||  ||   ||  ||  ||  ||  ||\nhttp://www.saudiaramco.com\n|-\n| {{flagcountry|Belarus}} || ||  ||  ||   ||  ||  ||  ||  ||\nhttp://geologiya.org/index.php?categoryid=14\nhttp://minpriroda.by/ru/napravlenia/minsyrbaza\n|-\n| {{flagcountry|East Timor}} || LAFAEK || Autoridade Nacional do Petróleo ||  || || Online GIS with wells and licences ||  ||  || Norad/OfD assistance  || http://www.anp-tl.org/webs/anptlweb.nsf/pgMaps\n|}\n\n==See also==\n*[[Norwegian Petroleum Directorate]]\n*[[Energistics]]\n*[[Professional Petroleum Data Management Association (PPDM)]]\n*[[Oil and gas industry in the United Kingdom]]\n*[[Petroleum exploration in Guyana]]\n\n==Notes==\n{{Reflist}}\n\n==External links==\n*[http://www.energistics.org/regulatory/national-data-repository-ndr-work-group Energistics: National Data Repository Work Group]\n*[http://www.kadme.com/wp-content/uploads/KADME-Oil-and-Gas-Technology-Jan2011.pdf National Data Repositories: the case for open data in the oil and gas industry]\n*[http://www.seg.org/ts Society of Exploration Geophysicists]\n\n[[Category:Data management]]\n[[Category:Open standards]]\n[[Category:Hydrocarbons]]\n[[Category:Geophysics organizations]]']
['Data philanthropy', '49882988', '{{Orphan|date=March 2016}}\n\n\'\'\'Data philanthropy\'\'\' describes a form of collaboration in which private sector companies share data for public benefit.<ref name="Pawelke">Pawelke, A. and Tatevossian, A. (2013, May 8) [http://www.unglobalpulse.org/data-philanthropy-where-are-we-now Data philanthropy: where are we now?] United Nations Global Pulse.</ref> There are multiple uses of data philanthropy being explored from humanitarian, corporate, human rights, and academic use. Since the introduction of this term the [[United Nations Global Pulse]] has began pushing for a global “data philanthropy movement.”<ref name= "Coren">Coren, M. (2011, December 9) [http://www.fastcoexist.com/1678963/data-philanthropy-open-data-for-world-changing-solutions Data Philanthropy: Open data for world-changing solutions] Fast Company.</ref>\n\n== Definition==\nA large amount of data collected from the Internet comes from [[user-generated content]]. This includes blogs, posts on social networks, and information submitted in forms. Besides user-generated data, corporations are also currently [[data mining]] data from consumers in order to understand customers, identify new markets, and make investment decisions. Kirkpatrick the Director at United Nations Global Pulse labels this data “massive passive data” or “data exhaust.”<ref name="Kirkpatrick">Kirkpatrick, R. (2011, September 20). [http://www.forbes.com/sites/oreillymedia/2011/09/20/data-philanthropy-is-good-for-business/ Data philanthropy is good for business] Forbes.</ref> Data philanthropy is the idea that something positive can come from this overload of data. Data philanthropy is defined as the private sector sharing this data in ways that the public can benefit.<ref name="Pawelke" /> The term philanthropy helps to emphasis that [[data sharing]] is a positive act and that the shared data is a public good.<ref name="Kirkpatrick" />\n\n== Challenges ==\nA challenge that comes with sharing data is the [[Internet privacy]] of the user whose data is being used. Mathematical techniques ([[differential privacy]] and space time boxes) have been introduced in order to make personal data accessible, while providing the users providing such data with anonymity. But even if these algorithms work there is always the possibility and fear of re-identification.<ref name="Pawelke" />\n \nThe other challenge is convincing corporations to share their data. The big data corporations collect provides them with market competitiveness. They are able to infer meaning regarding [[consumer behavior]]. The fear is that by sharing all their information, they may lose their competitive edge.<ref name="Pawelke" />\n\n== Sharing strategies ==\nThe goal of data philanthropy is to create a global data commons where companies, governments, and individuals can contribute anonymous, aggregated datasets.<ref name="Coren" /> The United Nations Global Pulse offers four different tactics that companies can use to share their data that preserve consumer anonymity.  These include:<ref name="Pawelke" />\n# Share aggregated and derived data sets for analysis under nondisclosure agreements (NDA)\n# Allow researchers to analyze data within the private company’s own network, under NDA\n# Real-Time Data Commons: data pooled and aggregated between multiple companies of the same industry to protect competitiveness\n# Public/Private Alerting Network: companies mine data behind their own firewalls and share indicators\n\nBy providing these four tactics United Nations Global Pulse hopes to provide initiative and options for companies to share their data with the public.\n\n== Digital disease detection ==\nData philanthropy has led to advancements in the field of health and wellness. By using data gathered from social media, cell phones, and other communication modes health researchers have been able to track the spread of diseases.<ref name="Schmidt">Schmidt, C. (2012). [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3261963/ Trending Now: Using Social Media to Predict and Track Disease Outbreaks.] Environ Health Perspect, 120(1), A30–a33-A30–a33.</ref>\n\nIn the United States [[HealthMap]], a freely available website and mobile app software is using data philanthropy related tactics to track the outbreak of diseases. HealthMap analyzes data from publicly available media sources such as news websites, government alerts, and social media sites like Twitter for outbreaks of various illnesses around the world.<ref name="Schmidt" /><ref name="Reddy">Reddy, E. (2015, July 14). [https://blog.twitter.com/2015/twitter-data-public-health Using Twitter data to study the world\'s health] Twitter.</ref>  The creators of HealthMap have another website, Flu Near You, which allows users to report their own health status on a weekly basis. Traditional flu surveillance can take up to 2 weeks to confirm outbreaks.<ref name= "Schmidt" /> Doctors must wait for virological test to confirm the outbreak before reporting it to the Centers for Disease Control. This form of data philanthropy allows for up to date information regarding various health concerns by using publicly available information gathered from news outlets, government alerts, and social media sites. It is the data gathered on social media sites, where users are not aware their data is being mined that leads to HealthMap and Flue Near You being considered data philanthropy.<ref name="Schmidt" /> \n \nThe [[Centers for Disease Control and Prevention]] collaborated with Google and launched [[Google Flu Trends]] in 2008, a website that tracks flu-related searches and user location to track the spread of the flu. Users can visit the website to compare the amount of flu-related search activity against the reported numbers of flu outbreaks on a graphic map. The difficulty with this method of tracking is that Google searched are sometimes performed due to curiosity rather than because an individual is suffering from the flu. According to Ashley Fowlkes, an epidemiologist in the CDC Influenza division, “the Google Flu Trends system tries to account for that type of media bias by modeling search terms over time to see which ones remain stable.”<ref name="Schmidt" /> Google Flu Trends is not longer publishing current flu estimates on the public website. Visitors to the site can still view and download previous estimates. Current data can be shared with verified researchers.<ref name="O\'Connor\'">O\'Connor, F. (2015, August 20). [http://www.pcworld.com/article/2974153/websites/google-flu-trends-calls-out-sick-indefinitely.html Google Flu Trends calls out sick, indefinitely] PC World.</ref>\n \nA study by Harvard School of Public Health (HSPH) released in the October 12, 2012 issues of the journal Science discussed how phone data helped curb the spread of malaria in Kenya. The researchers mapped phone calls and texts made by 14,816,521 Kenyan mobile phone subscribers.<ref name= "Datz">Datz, T. (2012, October 11). [http://www.hsph.harvard.edu/news/press-releases/cell-phone-data-malaria/ Using cell phone data to curb the spread of malaria.] Harvard Chan.</ref> When individuals left their primary living location the destination and length of journey was calculated. This data was then compared to a 2009 malaria prevalence map to estimate the disease’s commonness in each location.  Combining all this information the researchers can estimate the probability of an individual carrying malaria and map the movement of the disease. This research, a result of data philanthropy, can be used to track the spread of similar diseases.<ref name="Datz" />\n\n==Application in various fields==\nThrough data philanthropy ‘[[big data]]’ corporations such as [[social networking sites]], telecommunication companies, [[search engines]] amongst others, collect and make user generated information available to a data sharing system. This also permits institutions to give back to a beneficial cause. With the onset of [[technological]] advancements, sharing data on a global scale and an in-depth analysis of these data structures could alter the reaction towards certain occurrences, be it [[natural disaster]]s, [[epidemics]], worldwide [[economic]] problems and many other events. Some analyst have argued<ref name="Forbes">[http://www.forbes.com/sites/oreillymedia/2011/09/20/data-philanthropy-is-good-for-business Data Philanthropy is Good for Business], by Robert Kirkpatrick, Forbes, 2011-09-20</ref> that this aggregated Information is beneficial for the common good and can lead to developments in [[research]] and [[data]] production in a range of varied fields.<ref name="Forbes"/>\n\n===Humanitarian aid===\nCalling patterns of [[mobile phone]] users can determine the [[socioeconomic]] standings of the populace which can be used to deduce “its access to housing, education, healthcare, and basic services such as water and electricity”.<ref name="Forbes"/> Researchers from Columbia University and Karolinska Institute utilize information from [[mobile phone]] providers, in order to assist in the dispersal of resources by deducing the movement of those displaced by natural disasters. Big data can also provide information on looming disasters and can assist relief organizations in rapid response and locating displaced individuals. By analyzing certain patterns within this ‘big data’, could successively transform the response to destructive occurrences like natural disasters, [[outbreak]]s of diseases and global economic distress, by employing real-time information to achieve a comprehension of the welfare of individuals. Corporations utilize digital services, such as human sensor systems to detect and solve impending problems within [[communities]]. This is a strategy implemented by the private sector in order to protect its citizens by anonymously dispersing customer information to the public sector, whilst also ensuring the protection of their privacy.<ref name="Forbes"/>\n\n===Impoverished areas===\n[[Poverty]] still remains a worldwide issue with over 2.5 billion people<ref name="Smart Data Collective">[http://www.smartdatacollective.com/rick-delgado/200566/lifting-how-big-data-can-help-eliminate-poverty Lifting Up: How Big Data Can Help Eliminate Poverty], by Rick Delgado, Smart Data Collection , 2014-05-23</ref> currently impoverished. Accumulating accurate data has been a complex issue but developments in [[technology]] and utilising \'big data\',<ref name="Smart Data Collective" /> is one solution for improving this situation. Statistics indicate the widespread use of mobile phones, even within impoverished communities. This availability could prove vital in gathering data on populations living in poverty. Additional data can be collected through [[Internet access]], social media, utility payments and [[governmental]] statistics. Data-driven activities can lead to the cumulation of ‘big data’, which in turn can assist international non-governmental organization in documenting and evaluating the needs of underprivileged populations. Through data philanthropy, [[NGO]]’s can distribute information whilst cooperating with governments and private companies.<ref name="Smart Data Collective" />\n\n===Corporate===\nData philanthropy incorporates aspects of social philanthropy by permitting  [[corporations]] to create profound impacts through the act of giving back by dispersing proprietary datasets.<ref name="Irevolution">[http://irevolution.net/2012/06/04/big-data-philanthropy-for-humanitarian-response/Big Data Philanthropy for Humanitarian Response], by Irevolution, 2012-07-04</ref> The [[public sector]], is faced with an unequal and limited access to the frequency of data and they also produce, collect and preserve information, which has proven to be an essential asset. Company’s track and analyze users online activities, so as to gain more insight into their needs in relation to new products and services.<ref>[https://hbr.org/2014/07/sharing-data-is-a-form-of-corporate-philanthropy/Sharing Data Is a Form of Corporate Philanthropy], by Matt Stempeck,Harvard Business Review 2014-07-24</ref>\nThese companies view the welfare of the population as a vital key to the expansion and progression of businesses by using their data to place a spotlight on the plight of global citizens.<ref name="Forbes" />[[Expert]]s in the private sector contend the importance of merging various data streams such as retail, mobile phone and social media data to create necessary solutions to handle global issues. Despite the inevitable risk of sharing private information, it works in a beneficial manner and serves the interest of the public.<ref>[https://hbr.org/2013/03/a-new-type-of-philanthropy-don&cm_sp=Article-_-Links-_-Top%20of%20Page%20Recirculation A New Type of Philanthropy: Donating Data], by Robert Kirkpatrick,Harvard Business Review 2013-03-21</ref> The digital revolution causes an extensive production of ‘big data’ that is user-generated and available on the web. Corporations accumulate information on customer preferences through the digital services they utilize and products they purchase, in order to gain a clear insight on their clientele and future market opportunities.<ref name="Forbes" /> However the rights of individuals concerning privacy and ownership of data are a controversial issue as governments and other institutions can use this collective data for other unethical purposes. Companies monitor and probe consumer online activities in order to better comprehend and develop tailored needs for their clientele and in turn increase their profits.<ref name="Jim Fruchterman">[https://hbr.org/2013/03/big-data-means-more-than-big-p Big Data Means More Than Big Profits], by Jim Fruchterman, Harvard Business Review, 2013-03-19</ref>\n\n===Academia===\nData philanthropy plays an important role in [[academia]]. Researchers encounter countless obstacles whilst attempting to access data. This data is available to a limited number of researchers with sole access to restricted resources who are authorized to utilize this information; like social media streams enabling them to produce more [[knowledge]] and develop new studies. For example, Twitter markets access to its real-time APIs at exorbitant prices, which often surpasses the budgets of most researchers. \'Data Grants’<ref name="Jim Fruchterman" /> is a trial program created by Twitter that provides a selective number of academics and researchers with access to real-time databases in order to garner more knowledge. They apply to gain entry into vast data downloads, on specific topics.<ref name="Jim Fruchterman" />\n\n===Human rights===\nData philanthropy aids the human rights movement, by assisting in the dispersal of evidence for truth commissions and war crimes tribunals. Proponents of human rights accumulate data on abuse occurring within states, which is then used for scientific analysis and propels awareness and action. For example, non-profit organizations compile data from Human Rights monitors in war zones in order to assist the UN High Commissioner for Human Rights. It uncovers inconsistencies in the number of casualties of war, which in turn leads to international attention and exerts influence on discussions relating to global policy.<ref name="Jim Fruchterman" />\n\n==See also==\n* [[Big Data]]\n\n==References==\n<references />\n\n== External links ==\n* [http://www.unglobalpulse.org/data-philanthropy-where-are-we-now Data Philanthropy, where are we now?] in UN Global Pulse blog by Adreas Pawelke and Anoush Rima Tatevossian (2013-05-08).\n\n[[Category:Big data| ]]\n[[Category:Data management]]']
['Data discovery', '40008710', '\'\'\'Data discovery\'\'\' is a [[business intelligence]] architecture aimed at creating and using interactive reports and explorable [[data]] from multiple sources. According to the [[United States]] information technology research and advisory firm [[Gartner]] "Data discovery has become a mainstream architecture in 2012".<ref>Kern, J., (2013), [http://www.information-management.com/news/data-discovery-saas-lead-bi-market-review-10024484-1.html Data Discovery, SaaS Lead BI Market Review], \'\'Information Management\'\'/</ref>\n\n==Definition==\nData discovery is a user-driven process of searching for patterns or specific items in a data set.  Data discovery applications use visual tools such as geographical maps, pivot-tables, and heat-maps to make the process of finding patterns or specific items rapid and intuitive.  Data discovery may leverage statistical and [[data mining]] techniques to accomplish these goals.\n\n==Data discovery and business intelligence (BI)==\nData discovery is a type of [[business intelligence]] in that they both provide the end-user with an application that visualizes [[data]].  Traditional BI covered dashboards, static and parameterized reports, and pivot tables.  Visualization of data in traditional BI incorporated standard charting, KPIs, and limited graphical representation and interactivity. BI is undergoing transformation in capabilities it offers, with a focus on end-user data analysis and discovery, access to larger volumes of data and an ability to create high fidelity presentations of information. \n\n==Players==\nData Discovery vendors include: [[Tableau_Software|Tableau]], [[Qlik]],  [[TIBCO_Software|TIBCO Spotfire]], Microsoft Power BI, [[MicroStrategy]], SAP (Lumira), Platfora, Datameer, ClearStory Data, [[AnswerRocket]], and Datawatch.<ref>The Rise of Data Discovery Has Set the Stage for a Major Strategic Shift in the BI and Analytics Platform Market \n15 June 2015 G00277789 \nAnalyst(s): Josh Parenteau | Rita L. Sallam | Cindi Howson \n</ref>\n\n==See also==\n* [[Business intelligence]]\n* [[Business intelligence tools]]\n\n==References==\n{{Reflist|30em}}\n\n[[Category:Business intelligence]]\n[[Category:Data management]]']
['Bright Computing', '50639093', '{{Infobox company\n| name = Bright Computing\n| logo = \n| logo_size = \n| logo_alt = \n| logo_caption = \n| logo_padding = \n| image = \n| image_size = \n| image_alt = \n| image_caption = \n| type = [[Privately held company|Private]]\n| founded = 2009 <!-- if known: {{start date and age|YYYY|MM|DD}} in [[city]], [[state]], [[country]] -->\n| founder = {{Unbulleted list|Matthijs van Leeuwen|Alex Ninaber}}\n| hq_location = {{Unbulleted list|[[Amsterdam, The Netherlands]]|[[San Jose, California]]}}\n| hq_location_city = \n| hq_location_country = \n| area_served = Global <!-- or: | areas_served = -->\n| key_people = {{Unbulleted list|Bill Wagner ([[Chief executive officer|CEO]])|Martijn de Vries ([[Chief technology officer|CTO]])|Kristin Hansen ([[Chief marketing officer|CMO]])|Bill Griffin ([[Chief financial officer|CFO]])}}\n| industry = [[Enterprise software]]\n| products = {{Unbulleted list|Bright Cluster Manager for HPC|Bright Cluster Manager for Big Data|Bright OpenStack}}\n| brands = \n| services = \n| former_name = ClusterVision (spin-off) \n| website = {{URL|brightcomputing.com}} <!-- or: | homepage = --><!-- {{URL|example.com}} -->\n}}\n\'\'\'Bright Computing\'\'\', Inc. is a developer of [[software]] for deploying and managing [[Supercomputer|high-performance]] (HPC) clusters, [[big data]] clusters, and [[OpenStack]] in [[data center]]s and using [[cloud computing]].<ref>{{Cite web|url=http://www.hpcwire.com/2016/02/03/24601/|title=Create Mixed HPC/Big Data Clusters Today Says Bright Computing|date=2016-02-03|website=HPCwire|access-date=2016-05-24}}</ref>\n\n== History ==\nBright Computing was founded by Matthijs van Leeuwen in 2009, who spun the company out of ClusterVision, which he had co-founded with Alex Ninaber and Arijan Sauer. Alex and Matthijs had worked together at UK’s Compusys, which was one of the first companies to commercially build HPC clusters.<ref>{{Cite web|url=http://www.bloomberg.com/research/stocks/private/person.asp?personId=282144720&privcapId=115561075&previousCapId=115561075&previousTitle=Bright%2520Computing,%2520Inc.|title=Matthijs van Leeuwen: Executive Profile & Biography - Businessweek|website=www.bloomberg.com|access-date=2016-05-24}}</ref><ref>{{Cite web|url=http://www.hpcwire.com/2009/10/07/clustervision_spins_off_cluster_management_software_company/|title=ClusterVision Spins Off Cluster Management Software Company|date=2009-10-07|website=HPCwire|access-date=2016-05-24}}</ref> They left Compusys in 2002 to start ClusterVision in the [[Netherlands]], after determining there was a growing market for building and managing supercomputer clusters using off-the-shelf hardware components and open source software, tied together with their own customized scripts.<ref>{{Cite web|url=http://www.clustervision.com/content/management-team|title=ClusterVision - Europe\'s Leading HPC and Cloud Specialist|website=ClusterVision|language=en-GB|access-date=2016-05-24}}</ref> ClusterVision also provided delivery and installation support services for HPC clusters at universities and government entities.<ref>{{Cite web|url=http://clustervision.com/about-the-company/|title=About the Company - ClusterVision|website=ClusterVision|language=en-GB|access-date=2016-05-24}}</ref>\n\nIn 2004, Martijn de Vries joined ClusterVision and began development of cluster management software. The software was made available to customers in 2008, under the name ClusterVisionOS v4.<ref>{{Cite web|url=http://www.beowulf.org/pipermail/beowulf/2008-September/023265.html|title=Roll your own cluster management system with ClusterVisionOS v4 was: [Beowulf] What services do you run on your cluster nodes?|last=holway|first=andrew|date=2008-09-23|access-date=2016-05-24}}</ref>\n\nIn 2009, Bright Computing was spun out of ClusterVision. ClusterVisionOS was renamed Bright Cluster Manager, and van Leeuwen was named Bright Computing’s CEO.<ref>{{Cite web|url=http://www.hpcwire.com/2009/10/07/clustervision_spins_off_cluster_management_software_company/|title=ClusterVision Spins Off Cluster Management Software Company|date=2009-10-07|website=HPCwire|access-date=2016-05-24}}</ref>\n\nIn 2010, [[ING Group|ING Corporate Investments]] made a $2.5 million investment in Bright Computing. In 2014, [[Draper Fisher Jurvetson]] (DFJ) (US), [[DFJ Esprit]] (UK), Prime Ventures (NL), and [[ING Group|ING Corporate Investments]] invested $14.5 million in Bright Computing. At that time, Bright Computing and ClusterVision were completely separated.<ref>{{Cite web|url=https://www.crunchbase.com/organization/ing-corporate-investments#/entity|title=ING Corporate Investments {{!}} CrunchBase|website=www.crunchbase.com|access-date=2016-05-24}}</ref>\n\nIn February 2016, Bright appointed Bill Wagner as chief executive officer. Matthijs van Leeuwen became chief strategy officer and board member.<ref>{{Cite web|url=http://insidehpc.com/2016/02/bright-computing-names-bill-wagner-as-chief-executive-officer/|title=Bright Computing Names Bill Wagner as CEO - insideHPC|date=2016-02-16|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref>\n\n== Customers  ==\nEarly customers included [[Boeing]],<ref>{{Cite web|url=http://insidehpc.com/2011/12/boeing-consolidates-on-brite-cluster-manager/|title=Boeing Consolidates on Bright Cluster Manager - insideHPC|date=2011-12-06|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref> [[Sandia National Laboratories]],<ref>{{Cite web|url=http://www.brightcomputing.com/news/sandia-labs-adopts-bright-cluster-manager|title=Sandia National Laboratories Adopts Bright Cluster Manager to Manage Departmental Clusters|last=Staff|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref> [[Virginia Tech]],<ref>{{Cite web|url=https://finance.yahoo.com/news/virginia-bioinformatics-institute-selects-bright-151500615.html|title=Virginia Bioinformatics Institute Selects Bright Cluster Manager for Big Data to Test New Research Methods|website=Yahoo Finance|access-date=2016-05-24}}</ref> [[Hewlett Packard Enterprise Services|Hewlett Packard]],<ref>{{Cite web|url=http://www8.hp.com/h20195/v2/GetPDF.aspx/4AA5-2604ENW.pdf|title=HPC Accelerates SMBs|last=|first=|date=|website=|publisher=Hewlett-Packard Development Company, L.P.|access-date=2007-05-24}}</ref> [[National Security Agency|NSA]], and [[Drexel University]]. Many early customers were introduced through resellers, including SICORP,<ref>{{Cite web|url=http://insidehpc.com/2010/02/bright-computing-and-sicorp-sign-reseller-agreement/|title=Bright Computing And SICORP Sign Reseller Agreement - insideHPC|date=2010-02-22|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref> [[Cray]],<ref>{{Cite web|url=http://www.cray.com/company/collaboration/partners|title=Partner Relationships {{!}} Cray|website=www.cray.com|access-date=2016-05-24}}</ref> [[Dell]],<ref>{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2015/11/11/MN54237|title=Bright Computing Announces Integration with Dell PowerEdge Servers for HPC Environments - The Business Journals|website=The Business Journals|access-date=2016-05-24}}</ref> Appro, and Advanced HPC.<ref>{{Cite web|url=http://www.advancedhpc.com/high_performance_servers/gpu_computing/bright_computing.html|title=Advanced HPC - GPU Computing - GPU Software - Bright Computing|website=www.advancedhpc.com|access-date=2016-05-24}}</ref>\n\nBy 2014, the company estimated 400 customers, including more than 20 [[Fortune 500]] Companies.<ref>{{Cite web|url=http://www.primeventures.com/portfolio/bright-computing/|title=Prime Ventures - Bright Computing|website=www.primeventures.com|access-date=2016-05-25}}</ref>\n\n== Products and services ==\nBright Cluster Manager for HPC lets customers deploy and manage complete clusters. It provides management for the hardware, the operating system, the HPC software, and users.<ref>{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-roll-new-line-products/|title=Bright Computing to Roll Out New Line of Products|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>\n\nIn 2014, Bright Computing introduced Bright Cluster Manager for [[Apache Hadoop]] clusters.<ref>{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2014/04/03/MN96527|title=Bright Computing Announces Full Support for Apache Hadoop at the Hadoop Summit Europe in Amsterdam - The Business Journals|website=The Business Journals|access-date=2016-05-24}}</ref> It was later extended to support [[Apache Spark]] and other big data applications, and was renamed, Bright Cluster Manager for Big Data. It can be used as a complete solution or to manage big data software distributions from leading vendors, including [[Cloudera]] and [[Hortonworks]]. It also has several features that allow the combination of Big Data and HPC workloads on the same cluster.<ref>{{Cite web|url=http://www.deskeng.com/de/bright-computing-releases-bright-cluster-manager-7-2/|title=Bright Computing Releases Bright Cluster Manager 7.2|date=2016-01-22|website=Desktop Engineering|language=en-US|access-date=2016-05-24}}</ref>\n\nIn 2014, the company announced Bright OpenStack, software to deploy, provision, and manage [[OpenStack]]-based private cloud infrastructures.<ref>{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-announces-bright-openstack-integration-talligent-openbook-billing-software-openstack-clouds/|title=Bright Computing Announces Bright OpenStack Integration With Talligent Software|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>\n\nIn January 2016, version 7.2 was released. The updates supported containers using [[Docker (software)|Docker]], improved integration with [[Puppet (software)|Puppet]] and job-based metrics.<ref>{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-highlights-version-7-2-bright-cluster-manager-software-solution-enterprisehpc-2016/|title=Bright Computing Highlights Version 7.2 of Bright Cluster Manager Software Solutions at EnterpriseHPC 2016|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>\n\nBright Cluster Manager software is frequently sold through [[original equipment manufacturer]] (OEM) resellers, including Dell and Cray.<ref>{{Cite web|url=http://www.brightcomputing.com/usa|title=USA|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\nTechnology partners include:\n{{Div col|3}}\n* Adaptive Computing<ref>{{Cite web|url=http://www.adaptivecomputing.com/news/adaptive-computing-bright-computing-deepen-product-integration-enhance-provisioning-workflow-optimization-technical-computing-environments/|title=Adaptive Computing and Bright Computing Deepen Product Integration to Enhance Provisioning and Workflow Optimization in Technical Computing Environments - Adaptive Computing|date=2014-06-24|website=Adaptive Computing|language=en-US|access-date=2016-05-24}}</ref>\n* Allinea<ref>{{Cite web|url=http://insidehpc.com/2015/09/bright-computing-collaborates-on-openhpec-accelerator-suite/|title=Bright Computing Collaborates on OpenHPEC Accelerator Suite|date=2015-09-16|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref>\n* [[Altair Engineering|Altair]]<ref>{{Cite web|url=http://www.prweb.com/releases/2011/9/prweb8788932.htm|title=Bright Computing Now Resells Altair PBS Professional Workload Manager, Delivering HPC Operational Efficiencies and Cost Savings|website=PRWeb|access-date=2016-05-24}}</ref>\n* [[Amazon Web Services|Amazon]]<ref>{{Cite web|url=http://www.hpcwire.com/2011/11/08/bright_computing_bursts_into_cloud/|title=Bright Computing Bursts Into Cloud|date=2011-11-08|website=HPCwire|access-date=2016-05-24}}</ref>\n* [[Ansys]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-ansys|title=Technology Partner - Ansys|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* [[Cavium]]<ref>{{Cite web|url=http://www.cavium.com/newsevents_Cavium-Collaboration-with-BrightComputing.html|title=Cavium Announces Collaboration with Bright Computing to Support the ThunderX Processor Family|website=www.cavium.com|access-date=2016-05-24}}</ref>\n* [[Cisco Systems|Cisco]]<ref>{{Cite web|url=https://gigaom.com/2014/07/28/bright-computing-takes-in-14-5m-to-push-its-cluster-management-smarts/|title=Bright Computing takes in $14.5M to push its cluster management smarts|last=Darrow|first=Barb|date=2014-07-28|website=gigaom.com|access-date=2016-05-24}}</ref>\n* Cloudera<ref>{{Cite web|url=http://insidebigdata.com/2014/04/10/bright-computing-achieves-cloudera-certification-bright-cluster-manager/|title=Bright Computing Achieves Cloudera Certification for Bright Cluster Manager|last=Brueckner|first=Rich|date=2014-04-10|website=insideBIGDATA|access-date=2016-05-24}}</ref>\n* Cray<ref>{{Cite web|url=https://cug.org/proceedings/cug2015_proceedings/includes/files/pap176-file2.pdf|title=Cray User Group - Bright Cluster Manager Presentation|last=|first=|date=|website=|publisher=|access-date=}}</ref>\n* DDN Storage<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-ddn|title=Technology Partner - DDN|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* Dell<ref>{{Cite web|url=http://insidehpc.com/2015/12/dellbright/|title=Bright Cluster Manager Integrates with Dell PowerEdge Servers for HPC Environments - insideHPC|date=2015-12-09|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref>\n* [[Fujitsu]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-fujitsu|title=Technology Partner - Fujitsu|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* Hewlett Packard Enterprise<ref>{{Cite web|url=http://www.datacenterknowledge.com/archives/2014/07/29/bright-cluster-management-raises-dough/|title=Bright Networks Cluster Management Gets $14.5m Round|date=2014-07-29|website=Data Center Knowledge|language=en-US|access-date=2016-05-24}}</ref>\n* Hortonworks<ref>{{Cite web|url=http://hortonworks.com/partner/bright-computing/|title=Bright Computing partners with Hortonworks for Hadoop|website=Hortonworks|language=en-US|access-date=2016-05-24}}</ref>\n* [[Huawei]]<ref>{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2014/09/16/MN12537|title=Huawei signs global reseller agreement with Bright Computing - The Business Journals|website=The Business Journals|access-date=2016-05-24}}</ref>\n* [[IBM]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-ibm|title=Technology Partner - IBM|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* [[Inspur]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-inspur|title=Technology Partner - Inspur|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* [[Intel]]<ref>{{Cite web|url=https://ctovision.com/2015/08/bright-computing-integrated-intel-enterprise-edition-lustre/|title=Bright Computing Integrated with Intel Enterprise Edition for Lustre - CTOvision.com|date=2015-08-13|website=CTOvision.com|language=en-US|access-date=2016-05-24}}</ref>\n* [[Lenovo]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-lenovo|title=Technology Partner - Lenovo|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* Magma<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-magma|title=Technology Partner - Magma|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* [[Mellanox Technologies]]<ref>{{Cite web|url=http://www.openstack.org/news/view/214/bright-computing-announces-bright-openstack%25E2%2584%25A2-integration-with-mellanox-technologies-infiniband-switches-and-virtual-extensible-lan-(vxlan)-offloading|title=» OpenStack Open Source Cloud Computing Software|website=www.openstack.org|access-date=2016-05-24}}</ref>\n* [[Microsoft]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-microsoft|title=Technology Partner - Microsoft|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* NICE<ref>{{Cite web|url=https://www.nice-software.com/partners/bright-computing|title=Bright Computing - NICE|website=www.nice-software.com|access-date=2016-05-24}}</ref>\n* [[Nvidia|NVidia]]<ref>{{Cite web|url=https://developer.nvidia.com/bright-cluster-manager|title=Bright Cluster Manager|date=2012-01-12|website=NVIDIA Developer|access-date=2016-05-24}}</ref>\n* [[Red Hat|Redhat]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-redhat|title=Technology Partner - RedHat|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* Runtime Design Automation<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-runtime|title=Technology Partner - Runtime|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>\n* [[Silicon Graphics International|SGI]]<ref>{{Cite web|url=https://www.sgi.com/partners/technology/partners.html|title=SGI - Partners: Technology Partners: SGI Partnerships|website=www.sgi.com|access-date=2016-05-24}}</ref>\n* [[Supermicro]]<ref>{{Cite web|url=http://www.montana.edu/rci/HyaliteCluster.html|title=Hyalite Cluster - Research Computing Group {{!}} Montana State University|website=www.montana.edu|access-date=2016-05-24}}</ref>\n* [[SUSE]]<ref>{{Citation|last=SUSE|title=Bright Computing and SUSE share common values system|date=2015-10-29|url=https://www.youtube.com/watch?v=4WwAKlOBhBo|accessdate=2016-05-24}}</ref>\n* [[Taligent]]<ref>{{Cite web|url=https://finance.yahoo.com/news/bright-computing-announces-bright-openstack-073000503.html|title=Bright Computing Announces Bright OpenStack™ Integration with Talligent Openbook Billing Software for OpenStack Clouds|website=Yahoo Finance|access-date=2016-05-24}}</ref>\n* [[Univa]]<ref>{{Cite web|url=http://www.prweb.com/releases/2012/10/prweb9961985.htm|title=Bright Computing and Univa Offer Combined Cluster & Workload Management Solution|website=PRWeb|access-date=2016-05-24}}</ref>\n{{Div col end}}\n\nBright Computing was covered by [[Software Magazine]]<ref>{{Cite web|url=http://www.softwaremag.com/bright-computing-releases-version-7-2-of-bright-cluster-manager-for-hpc-bright-cluster-manager-for-big-data-and-bright-openstack/|title=Bright Computing Releases Version 7.2 of Bright Cluster Manager for HPC, Bright Cluster Manager for Big Data, and Bright OpenStack –|website=www.softwaremag.com|access-date=2016-05-24}}</ref> and [[Yahoo! Finance]],<ref>{{Cite web|url=https://finance.yahoo.com/news/bright-computing-showcase-bright-openstack-162500598.html|title=Bright Computing to Showcase Bright OpenStack™ and HPC Cluster-as-a-Service (CaaS) at 2016 OpenStack Summit in Austin|website=Yahoo Finance|access-date=2016-05-24}}</ref> among other publications.\n\n== Awards ==\nIn 2016, Bright Computing was awarded a €1.5M [[Horizon 2020]] SME Instrument grant from the [[European Commission]].<ref>{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-receives-horizon-2020-grant-european-commission/|title=Bright Computing Receives Horizon 2020 Grant From European Commission|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>\n\nBright Computing was one of only 33 grant recipients from 960 submitted proposals.<ref>{{Cite web|url=https://ec.europa.eu/easme/en/horizons-2020-sme-instrument|title=Horizon 2020\'s SME Instrument - EASME - European Commission|website=EASME|access-date=2016-05-24}}</ref> In its category only 5 out of 260 grants were awarded.<ref>{{Cite web|url=https://ec.europa.eu/digital-single-market/en/open-disruptive-innovation-0|title=Open Disruptive Innovation - Digital Single Market - European Commission|website=Digital Single Market|access-date=2016-05-24}}</ref>\n\n* 2015 HPCwire Editor’s Choice Award for “Best HPC Cluster Solution or Technology."<ref>{{Cite web|url=http://www.hpcwire.com/2015-hpcwire-readers-choice-awards/|title=2015 HPCwire Awards – Readers’ & Editors’ Choice - HPCwire|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>\n* Main Software 50 “Highest Growth” award winner, 2013.<ref>{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2013/11/14/MN16669|title=Bright Computing Wins Main Software 50 "Highest Growth" Award - The Business Journals|website=The Business Journals|access-date=2016-05-24}}</ref>\n* [[Deloitte]] Technology Fast50 “Rising Star 2013” Award Winner.<ref>{{Cite web|url=http://www2.deloitte.com/be/en/pages/about-deloitte/articles/ayden-wins-fast50.html|title=Adyen wins Deloitte Technology Fast50 {{!}} Deloitte Belgium {{!}} TMT {{!}} News, press release|website=Deloitte Belgium|access-date=2016-05-24}}</ref>\n* Bio-IT World Conference & Expo ‘13, Boston, MA, Winner of “IT Hardware & Infrastructure” category of the “Best of Show Award” program.<ref>{{Cite web|url=http://www.bio-itworld.com/2013/4/10/2013-bio-it-world-best-show-winners-named.html|title=2013 Bio-IT World Best of Show Winners Named - Bio-IT World|access-date=2016-05-24}}</ref>\n* [[Red Herring (magazine)|Red Herring]] Top 100 Global Award, 2013.<ref>{{Cite web|url=http://www.redherring.com/events/rhna/2013-rhnawinners/|title=2013 Top 100 North America: Winners — Red Herring|website=Red Herring|language=en-US|access-date=2016-05-24}}</ref>\n\n== References ==\n{{reflist|30em}}\n\n==Further reading==\n* Morgan, Timothy Prickett (June 20, 2011). [http://www.theregister.co.uk/2011/06/20/bright_cluster_manager_5_2/ "Bright Computing revs up cluster manager"]. \'\'[[The Register]]\'\'.\n* Morgan, Timothy Prickett (November 8, 2011). [http://www.theregister.co.uk/2011/11/08/bright_cluster_manager_amazon_cloud_bursting/ "Bright Computing bursts HPC to EC2 clouds"]. \'\'The Register\'\'.\n\n[[Category:Big data companies]]\n[[Category:Cloud computing]]\n[[Category:Cloud infrastructure]]\n[[Category:Cluster computing]]\n[[Category:Data management]]\n[[Category:Supercomputers]]']
['Digital obsolescence', '934683', '{{Outdated|date=January 2015}}[[File:VCF 2010 Domesday tray open.jpg|thumb|300px|A Domesday Project machine with its modified [[Laserdisc]]. The Domesday Project was published in 1986.]]\n\'\'\'Digital obsolescence\'\'\' is a situation where a digital resource is no longer readable because of its archaic format: the physical media, the reader (required to read the media), the hardware, or the software that runs on it is no longer available.<ref name=\'national-archives\'>{{cite web | last = | first = | authorlink = | title =Managing Digital Obsolescence Risks | work = | publisher =The National Archives | date =April 2009 | url = http://www.nationalarchives.gov.uk/documents/information-management/siro-guidance-on-the-risk-of-digital-obsolescence.pdf| format = pdf| doi = | accessdate = | archiveurl =http://webarchive.nationalarchives.gov.uk/+/http://www.nationalarchives.gov.uk/documents/information-management/siro-guidance-on-the-risk-of-digital-obsolescence.pdf | archivedate = 28 Jun 2011}}</ref> \n\nA prime example of this is the [[BBC Domesday Project]] from the 1980s, although its data was eventually recovered after a significant amount of effort. [[Cornell University]] Library’s [http://www.icpsr.umich.edu/dpm/dpm-eng/eng_index.html digital preservation tutorial] (now hosted by [[ICPSR]]) has a timeline of obsolete media formats, called the [http://www.icpsr.umich.edu/dpm/dpm-eng/oldmedia/index.html “Chamber of Horrors”], that shows how rapidly new technologies are created and cast aside.\n\n==Introduction==\nThe rapid evolution and proliferation of different kinds of [[computer hardware]], modes of digital encoding, [[operating systems]] and general or specialized [[software]] ensures that digital obsolescence will become a problem in the future.<ref>Rothenberg, J. (1998). [http://www.clir.org/pubs/reports/rothenberg/introduction.html#longevity Avoiding Technological Quicksand: Finding a Viable Technical Foundation for Digital Preservation]</ref> Many versions of word-processing programs, data-storage media, standards for encoding images and films are considered "standards" for some time, but in the end are always replaced by new versions of the software or completely new hardware. Files meant to be read or edited with a certain program (for example [[Microsoft Word]]) will be unreadable in other programs, and as operating systems and hardware move on, even old versions of programs developed by the same company become impossible to use on the new platform (for instance, older versions of [[Microsoft Works]], before Works 4.5, cannot be run under [[Windows 2000]] or later). \n\nEarly attention was brought to the challenges of preserving [[machine-readable data]] by the work of [[Charles M Dollar]] in the 1970s, but it was only during the 1990s that libraries and archives came to appreciate the significance of the problem<ref>Hedstrom, M. (1995). [http://www.uky.edu/%7Ekiernan/DL/hedstrom.html Digital Preservation: A Time Bomb for Digital Libraries]</ref> and has been discussed among professionals in those branches, though so far without any obvious solutions other than continual forward-migration of files and information to the latest data-storage standards. File formats should be widespread, backward compatible, often upgraded, and, ideally, open format. In 2002, the National Initiative for a Networked Cultural Heritage cited<ref>National Initiative for a Networked Cultural Heritage. (2002). [http://www.nyu.edu/its/humanities/ninchguide/V/ NINCH Guide to Good Practice in the Digital Representation and Management of Cultural Heritage Materials]</ref> the following as “de facto” formats that are unlikely to be rendered obsolete in the near future:  uncompressed [[TIFF]]  and [[ASCII]] and [[Rich Text Format|RTF]] (for text).\n\nIn order to prevent this from happening, it is important that an institution regularly evaluate and explore its current technologies and evaluate its long term business model.<ref name=\'national-archives\'/>\n\n== Types ==\nDigital objects are vulnerable to three types of obsolescence:<ref>{{cite web|publisher=National Archives of Australia|title=Obsolescence – a key challenge in the digital age|url=http://www.naa.gov.au/records-management/agency/preserve/e-preservation/obsolescence.aspx|accessdate=17 March 2014}}</ref>\n# \'\'\'Physical media\'\'\': the physical carrier of the digital file becomes obsolete; e.g. 8 inch floppy disks, which are no longer commercially available. \n# \'\'\'Hardware\'\'\': the hardware needed to access the digital file becomes obsolete; e.g. floppy disk drive, which computers are no longer manufactured with.\n# \'\'\'Software\'\'\': the software needed to access the digital file becomes obsolete; e.g. [[WordStar]], a word processor popular in the 1980s which used a [[Open data|closed data]] format and is no longer readily available.\n\n== Strategies ==\nAny organization that has digital records should assess its records to identify any potential risks for file format obsolescence. The Library of Congress maintains [http://www.digitalpreservation.gov/formats/intro/intro.shtml Sustainability of Digital Formats], which includes technical details about many different format types. The UK National Archives maintains an online registry of file formats called [http://www.nationalarchives.gov.uk/PRONOM/Default.aspx PRONOM].\n\nIn its 2014 agenda, the National Digital Stewardship Alliance recommended developing File Format Action Plans: "it is important to shift from more abstract considerations about file format obsolescence to develop actionable strategies for monitoring and mining information about the heterogeneous digital files the organizations are managing."<ref>{{cite web|publisher=National Digital Stewardship Alliance|title=National Agenda for Digital Stewardship 2014|url=http://www.digitalpreservation.gov/ndsa/documents/2014NationalAgenda.pdf|accessdate=17 March 2014|date=2014}}</ref> \n\nFile Format Action Plans are documents internal to an organization which list the type of digital files in its holdings and assess what actions should be taken to ensure its ongoing accessibility.<ref>{{cite web|last=Owens|first=Trevor|title=File Format Action Plans in Theory and Practice|url=http://blogs.loc.gov/digitalpreservation/2014/01/file-format-action-plans-in-theory-and-practice/|accessdate=17 March 2014|date=6 January 2014}}</ref>  Examples include the [http://fclaweb.fcla.edu/node/795 Florida Digital Archive Action Plan] and University of Michigan\'s [http://deepblue.lib.umich.edu/static/about/deepbluepreservation.html Deep Blue Preservation and Format Support Policy].\n\n== Copyright issues ==\nUntangling [[copyright]] issues also presented a significant challenge for projects attempting to overcome the obsolescence issues related to the BBC Domesday Project. In addition to copyright surrounding the many contributions made by the estimated 1 million people who took part in the project, there are also copyright issues that relate to the technologies employed. It is likely that the Domesday Project will not be completely free of copyright restrictions until at least 2090, unless copyright laws are revised for earlier [[Copyright term|expiration]] of software into [[Public domain software|public domain]].<ref>{{Cite web |url=http://www2.si.umich.edu/CAMILEON/reports/IPRreport.doc|title= The CAMiLEON Project: Legal issues arising from the work aiming to preserve elements of the interactive multimedia work entitled "The BBC Domesday Project."|first= Andrew|last= Charlesworth|date= 5 November 2002|publisher= Information Law and Technology Unit, University of Hull|location= Kingston upon Hull|format= Microsoft Word|accessdate= 23 March 2011}}</ref>\n\n==Intentional obsolescence==\nIn some cases, obsolete technologies are used in a deliberate attempt to avoid data intrusion in a strategy known as "[[security through obsolescence]]".<ref>{{cite news | url=http://www.linux.com/articles/23313 | title=Security through obsolescence | first=Robin | last=Miller | publisher=Linux.com | date=2002-06-06 | accessdate=2008-07-18}}</ref>\n\n==See also==\n* [[Obsolescence]]\n* [[Digital preservation]]\n* [[Digital Dark Age]]\n* [[CAMiLEON]]\n* [[Emulation (computing)]]\n* [[M-DISC]]\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [http://www.digitalpreservation.gov/formats/ The Library of Congress, Sustainability of Digital Formats]\n* [[Wired Magazine]]: [http://wired-vig.wired.com/wired/archive/6.09/saved.html What death can\'t destroy and how to digitize it]\n* [https://www.icpsr.umich.edu/icpsrweb/content/datamanagement/preservation/ Digital Preservation at ICPSR]\n\n{{DigitalPreservation}}\n{{DEFAULTSORT:Digital Obsolescence}}\n[[Category:Data management]]\n[[Category:Digital libraries]]\n[[Category:Digital preservation]]\n[[Category:Future problems]]\n[[Category:Obsolescence]]\n[[Category:Records management]]']
['Intelligence Engine', '51136320', '\'\'\'Intelligence Engines\'\'\' are a type of [[enterprise information management]] that combine [[Business rule management system|business rule management]], [[predictive analytics|predictive]] and [[prescriptive analytics]] to form a unified information access platform that provides real-time intelligence through [[Web search engine|search technologies]], [[Dashboard (management information systems)|dashboards]] and/or existing business infrastructure.  Intelligence Engines are process and/or business problem specific, resulting in industry and/or function-specific [[marketing]] [[trademark]]s associated with them.  They can be differentiated from [[enterprise resource planning]] (ERP) software in that intelligence engines include organization-level business rules and proactive [[decision management]] functionality.\n\n==History==\nThe first intelligence engine application appears to have been introduced in 2001 by [[Sonus Networks|Sonus Networks, Inc.]] in their patent US6961334 B1.<ref name="sonus">{{cite patent | country = US | number = 6961334 | status = patent | title = Intelligence engine | gdate = 2005-11-01 | invent1 = Kaczmarczyk, Casimer M}}</ref>  Applied to the field of telecommunications systems, the intelligence engine was composed of a database queried by a data distributor layer, received by a telephony management layer and acted upon by a facility management command & control layer.<ref name="sonus" />  This combined standalone business intelligence tools like a [[data warehouse]], reporting and querying software and a [[decision support system]].\n\nThe concept was reinforced in 2002 in patent application US20030236689 A1<ref name="2002patent">{{cite patent | country = US | number = 20030236689 | status = application | title = Analyzing decision points in  business processes | pubdate = 2003-12-25 | invent1 = Casati, Fabio | invent2 = Sayal, Mehmet | invent3 = Guadalupe Castellanos, Maria | invent4 = Gunopulos, Dimitrios | url = https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&NR=2003236689&KC=A1&FT=E&locale=en_EP}}</ref> which applied predictive quantitative models to data and used rules to correlate context data at different stages of the business process with business process outcomes to be presented to end users.<ref name="2002patent" />\n\n[[LogRhythm|LogRhythm Inc.]] advanced the concept in 2010 by adding event managers to the end of the intelligence engine\'s process to determine reporting, remediation and other outcomes.<ref name="LogRhythm">{{cite patent | country = US | number = 2012131185 | status = application | title = Advanced Intelligence Engine | pubdate = 2012-05-24 | invent1 = Petersen , Chris | invent2 = Villella, Phillip | invent3 = Aisa, Brad | url = https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&NR=2012131185A1&KC=A1&FT=D}}</ref>\n\nIn 2016, professional service company [[KPMG]] continued to advance the concept by commercializing intelligence engines with the introduction of Third Party Intelligence, which is differentiated from past intelligence engines in its increased use of embedded intellectual property, diversity of global data inputs and focus on predictive analytics to mitigate risk and yield cost savings.<ref name="CIOReview">{{cite web|url=http://www.cioreview.com/news/kmpg-launches-third-party-intelligence-intelligence-engine-to-anticipate-thirdparty-disruptions-nid-14407-cid-78.html |title=KMPG Launches Third Party Intelligence: Intelligence Engine to Anticipate Third-party Disruptions|accessdate=2016-07-22}}</ref>\n\n==Traits==\nAs a system that combines human intelligence, data inputs, automated decision-making and unified information access, intelligence engines are an advancement in business intelligence tools because they: \n* integrate structured data and unstructured content in a single index<ref name="itbiz">{{cite web|url=http://www.itbusinessedge.com/blogs/it-unmasked/attivio-applies-predictive-analytics-to-indexed-data.html |title=Attivio Applies Predictive Analytics to Indexed Data |accessdate=2016-07-22}}</ref>\n* provide advanced workflow automation that can trigger multiple business processes<ref name="salesforce">{{cite press release|url=http://www.salesforce.com/company/news-press/press-releases/2015/03/150309.jsp |title=Salesforce Unveils Service Cloud Intelligence Engine—Fueling Smarter Customer Service for the Connected World |publisher=Salesforce.com|accessdate=2016-07-22}}</ref>\n* project future impact of data<ref name="inboundlog">{{cite web|url=http://resources.inboundlogistics.com/digital/issues/il_digital_may2016.pdf |title=Inbound Logistics Magazine May 2016|accessdate=2016-07-22}}</ref> such as supply chain threats <ref name="3pie">{{cite web|url=https://www.kpmgspectrum.com/3pie/index.html |title=KPMG Spectrum |accessdate=2016-07-22}}</ref>\n* recommend best actions<ref name="custmatrix">{{cite web|url=http://www.customermatrix.com/news-and-press-releases/press-releases/145-customermatrix-unveils-first-ever-cognitive-intelligence-engine-for-crm-2 |title=CustomerMatrix Unveils First-Ever Cognitive Intelligence Engine for CRM |accessdate=2016-07-22}}</ref> / highlight opportunities for process improvement<ref name="parasoft">{{cite web|url=https://www.parasoft.com/capability/process-intelligence-engine/ |title=Process Intelligence Engine (PIE) |accessdate=2016-07-22}}</ref>\n* leverage business intelligence from a variety of experts<ref name="inboundlog" /> \n* combine human expertise with the power of technology to deliver actionable intelligence<ref name="CIOReview" />\n* scale data visualization capabilities with the number of users<ref name="armanta">{{cite web|url=http://www.armanta.com/product/technology/intelligence-engine/ |title=A Big Data User Experience |accessdate=2016-07-22}}</ref>\n\n==Applications==\n* Attivio Active Intelligence Engine®<ref name="attivo-pr">{{cite web|url=http://info.attivio.com/rs/attivio/images/Attivio-Customer-Succes-Story-General-Electric.pdf |title=Active Intelligence Engine&reg; (AIE&reg;) Case Study: General Electric |accessdate=2016-07-22}}</ref>\n* [[KPMG]] Spectrum Intelligence Engine(s)<ref name="KPMGIE">{{cite web|url=https://www.kpmgspectrum.com/3pie/about.html |title=KPMG Spectrum: Action through Intelligence |accessdate=2016-07-22}}</ref>\n* [[Salesforce.com|Salesforce]] Service Cloud Intelligence Engine<ref name="salesforce" /> \n* [[FireEye]] Threat Intelligence Engine<ref name="fireeye">{{cite web|url=https://www.fireeye.com/products/dynamic-threat-intelligence/threat-intelligence-engine.html |title=FIREEYE THREAT INTELLIGENCE ENGINE |accessdate=2016-07-22}}</ref>\n* [[Factiva]] Intelligence Engine<ref name="factiva">{{cite web|url=http://solutions.dowjones.com/collateral/files/dj-factivacom-brochure-F-3465.pdf |title=Factiva&reg; - The Intelligence Engine |accessdate=2016-07-22}}</ref>\n* [[Parasoft]] Process Intelligence Engine<ref name="parasoft" />\n\n==See also==\n* [[Business Intelligence]] (BI)\n* [[Business intelligence tools]] \n* [[Business Rule Management System]]\n* [[Enterprise Information Management]] \n* [[Predictive Analytics]] \n* [[Prescriptive analytics]]\n* [[Decision Management]] \n* [[Data Science]]\n* [[Data Mining]]\n\n==References==\n\n{{Reflist}}\n\n{{DEFAULTSORT:Intelligence Engine}}\n[[Category:Data management]]\n[[Category:Information management]]\n[[Category:Big data]]\n[[Category:Business terms]]\n[[Category:Business intelligence]]\n[[Category:Information systems]]\n[[Category:Supply chain management terms]]']
['Wiping', '89314', '{{About|the broadcasting practice||Wipe (disambiguation)}}\n{{Multiple issues|\n{{refimprove|date=February 2007}}\n{{original research|date=September 2009}}\n{{globalize|date=September 2009}}\n}}\n\n\'\'\'Wiping\'\'\', also known as \'\'\'junking\'\'\', is a colloquial term for action taken by [[radio]] and [[television]] production and broadcasting companies, in which old [[audiotape]]s, [[videotape]]s, and [[telerecording]]s ([[kinescope]]s), are [[List of lost television broadcasts|erased, reused, or destroyed]]. Although the practice was once very common, especially in the 1960s and 1970s, wiping is now practiced much less frequently. Older video and audio formats took up much more storage space than modern digital video or audio files, making their retention more costly, thus increasing the incentive of discarding existing broadcast material to recover storage space for newer programmes.\n\nThe advent of domestic audiovisual playback technology (e.g., videocassette and [[DVD]]) has made wiping less beneficial, with broadcasters and production houses realizing both the economic and cultural value of keeping archived material for both rebroadcast and potential profits through release on [[home video]].\n\n==Australia==\nAustralian broadcasters did not gain access to videotape-recording technology until the early 1960s, and as a result nearly all programmes prior to that were broadcast live-to-air. Very little programming survives from the earliest years of Australian TV (1956–1960), as [[kinescope]] recording to film was expensive and most of what was recorded in this way has since been lost or destroyed. Some early programmes have survived, however; for example, ATN-7, a Sydney station, prerecorded (via kinescopes) some of their 1950s output such as \'\'[[Autumn Affair]]\'\' (1958–1959), \'\'[[The Pressure Pak Show]]\'\' (1957–1958) and \'\'[[Leave It to the Girls (Australian TV series)|Leave it to the Girls]]\'\' (1957–1958); some of these kinescopes have survived and are now held by the [[National Film and Sound Archive]],<ref>{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=;group=;groupequals=;page=0;parentid=;query=autumn%20affair%20Media%3A%22TELEVISION%22;querytype=;resCount=200|title = NSFA, Autumn Affair}}</ref><ref>{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=;group=;groupequals=;page=0;parentid=;query=leave%20it%20to%20the%20girls%20Media%3A%22TELEVISION%22;querytype=;resCount=10|title = NSFA, Leave it to the Girls}}</ref><ref>{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=yes;group=;groupequals=;page=0;parentid=;query=pressure%20pak%20show%20Years%3A%3E%3D1957%20Years%3A%3C%3D1958%20Media%3A%22TELEVISION%22;querytype=;resCount=20|title = NSFA, Pressure Pak Show°}}</ref> with soap opera \'\'Autumn Affair\'\' surviving near-intact, likely one of the earliest Australian series for which this is the case.\n\n===ABC===\n\nThe [[Australian Broadcasting Corporation]] (ABC) erased much of its early output. Much of the videotaped ABC programme material from the 1960s and early 1970s was erased as part of an economy policy instituted in the late 1970s in which old programme tapes were surrendered for bulk erasure and reuse. This policy particularly targeted older programmes recorded in black-and-white, leading to the loss of many recordings made before 1975, when Australian television converted to colour. The ABC continued erasing older television output into the early 1980s.\n\nProgrammes known to have been lost include most studio segments from the 1960s current affairs shows \'\'[[This Day Tonight]]\'\' and \'\'Monday Conference\'\', hundreds of episodes of the long-running rural serial \'\'[[Bellbird (TV series)|Bellbird]]\'\', all but a handful of episodes of the early-1970s drama series \'\'[[Certain Women (television series)|Certain Women]]\'\', an early-1970s miniseries of dramatizations based on [[Norman Lindsay]]\'s novels, and nearly all of the first 18 months of the weekly pop-music show \'\'[[Countdown (Australian TV series)|Countdown]]\'\'.\n\n===Network Ten===\n\nMany episodes of popular Australian commercial TV series are also lost. In the 1970s, [[Network Ten]] had an official policy to reuse tapes; hence, many tapes of \'\'[[Young Talent Time]]\'\' and \'\'[[Number 96 (TV series)|Number 96]]\'\' were wiped. To this day, Network Ten still only keeps some of its programming.{{Citation needed|date=November 2008}} Other notable losses from the Ten archive include hundreds of episodes of the Melbourne-based pop music shows commissioned and broadcast by ATV-0 Melbourne in the 1960s and early 1970s—\'\'[[The Go!! Show]]\'\' (1964–1967), \'\'Kommotion\'\' (1964–1967), \'\'Uptight\'\' (1968–70), and the \'\'Happening 70s\'\' series (1970–1972).\n\n===Nine Network===\n\nThe [[Nine Network]] discarded copies of some of their programs, including the popular [[GTV-9]] series \'\'[[In Melbourne Tonight]]\'\' hosted by [[Graham Kennedy]]. Though it ran five nights a week from 1957 to 1970, fewer than 100 episodes are known to survive, and many of the surviving episodes are edited prints made for rebroadcast across Australia. Early episodes of \'\'[[Hey Hey It\'s Saturday]]\'\' do not exist because the programme was broadcast live and did not begin videotape recordings until a number of years later.\n\n==Brazil==\nFrom 1968–1969, [[Rede Tupi|TV Tupi]] produced new episodes of the soap opera \'\'[[Beto Rockfeller]]\'\' by recording over previous episodes; as a result, few episodes survive. After the closure of TV Tupi in 1980 the 536 tapes at its São Paulo studios were simply left to deteriorate until they were recovered in 1985 and subsequently restored by [[TV Cultura]] in 1989. Only two TV Tupi O&Os are known to have any preserved videotapes; TV Itacolomi\'s archives are now owned by the unrelated [[TV Alterosa]], affiliated with [[Sistema Brasileiro de Televisão|SBT]], whereas the few remaining tapes belonging to TV Piratini are stored privately in a museum in Porto Alegre, albeit in a deteriorated state.\n\n[[Rede Record]] also lost much footage from the 1960s due to wiping, fires, and deterioration; most of the [[Música popular brasileira|MPB]] music festivals no longer exist, and the sitcom \'\'[[:pt:Família Trapo|Família Trapo]]\'\' has only one surviving episode, featuring [[Pelé]]. Until 1997 Rede Record had no policy on archiving videotapes, since then at least 600 videotapes that were previously believed to be lost have been recovered.\n\n[[Rede Globo]] lost the first 35 broadcasts of both \'\'[[Fantástico]]\'\' and \'\'[[Jornal Nacional]]\'\', in addition to many segments of their other soap operas, as a result of wiping, and also due to three fires that occurred in 1969, 1971 and 1976, where an estimated 920 to 1500 tapes were destroyed.\n\nMost of [[Rede Excelsior]]\'s output was damaged in a fire in 1969; however, in the late 1990s about 100 tapes of Rede Excelsior programming were discovered and these tapes were subsequently donated to the [[Cinemateca Brasileira]] in 2001.\n\n==Canada==\nThe [[Canadian Broadcasting Corporation]] never practiced wiping, and maintains a complete archive of all programming that was recorded.<ref>{{cite web|url=http://archives.cbc.ca/info/archives/archives_en_04.asp?IDLan=1|title=CBC Archives|date=10 April 2013|publisher=}}</ref>\n\nThe [[CTV Television Network]] has admitted to wiping many programmes during the 1970s. Because of [[Canadian content]] requirements, the need for Canadian-produced programming led to more preservation of the shows they produced, and even very poorly received programmes (such as the infamous \'\'[[The Trouble with Tracy]]\'\') were saved and rerun for several years after their cancellation. Furthermore, Canadian rebroadcasts have been a source of some broadcasts that are otherwise lost in the United States and the United Kingdom.\n\n==Japan==\nSome TV stations in Japan practiced wiping, this example included the [[Doraemon (1973 anime)|first anime]] adaption of \'\'[[Doraemon]]\'\'.\n\n==Philippines==\nEpisodes from 1979 to 1982 of the longest running noontime show, \'\'[[Eat Bulaga!]]\'\', have been lost.\n\nAnother example of the wiping of TV archives in the Philippines was when martial law was declared, soldiers raided the [[ABS-CBN|ABS-CBN Broadcast Center]] and placed it under military control. As a result, ABS-CBN\'s pre-martial law archives, dating from 1953 to 1972, were lost.\n\n==Mexico==\nDue to its multiple studio facilities, namely its Chapultepec and [[Televisa San Angel|San Angel studios]], [[Televisa]] preserved most of its scripted series for broadcast years after the preserved programs had ended their original runs.  Some Televisa programs, however, were lost not due to wiping, but due to the [[1985 Mexico City earthquake]] that destroyed part of the network\'s archive. However, smaller channels, such as [[XEIPN-TV]] and [[XHDF-TV]], did not began to preserve their recorded broadcasts until the early 1980s. [[Monterrey]]\'s [[Multimedios Televisión]] keeps most of its programming, though some special historical programming dealing with [[XHAW-TDT|its flagship station]]\'s history clearly shows that some footage has been either donated by viewers recorded from its original broadcast, or uses footage of its programming recorded by fans and uploaded to [[YouTube]].\n\n==United Kingdom==\n\n===BBC===\nThe [[BBC]], the United Kingdom\'s first [[public service broadcaster]], had no policy on archiving until 1978.<ref>{{cite web|url=http://cuttingsarchive.org.uk/missing/mis_overv.htm |title=Cuttings Archive: The Missing Episodes - Overview |publisher=Cuttings Archive |accessdate=2008-11-23 |deadurl=yes |archiveurl=https://web.archive.org/web/20080725012437/http://www.cuttingsarchive.org.uk/missing/mis_overv.htm |archivedate=July 25, 2008 }}</ref> Much of the corporation\'s output between the 1930s and 1980s has been lost. Rationales behind this policy include:\n\n====Technological====\nThe BBC\'s [[television]] service dates back to 1936 and was originally a nearly live-only medium. The hours of transmission were very limited and the bulk of the programming was transmitted either live from the studio, or from [[Outside broadcasting|outside broadcast (OB)]] units; film was a minor contributor to the output. When the first television broadcasts were made, there were two competing systems in use. The EMI electronic system (using [[405 lines]]) competed with the Baird 240-line [[mechanical television]] system. Baird adopted an intermediate film technique where the live material was filmed using a standard film camera mounted on a large cabinet which contained a rapid processing unit and an early [[flying spot scanner]] to produce the video output for transmission. The pioneer broadcasts were not, however, preserved on this intermediate film as the nitrate (celluloid) stock was scanned while still wet from the fixer bath and never washed to remove the fixer chemicals. Consequently, the film decomposed very soon after transmission; nothing is known to have survived.\nNo studio or OB programmes from 1936 to 1939 or 1946 to 1947 have survived because there was no means of preserving them. Historical \'firsts\' from this era; the world\'s earliest television crime drama \'\'[[Telecrime]]\'\' (1938–39 and 1946) or \'\'[[Pinwright\'s Progress]]\'\' (1946–47, the world\'s first regular [[situation comedy]]), only remain visually as a handful of still photographs.\n\nThe earliest recording method for television was [[Kinescope|telerecording]], which involved recording the image from a special television monitor onto film with a modified film camera. Early examples made by this method include the first two episodes of \'\'[[The Quatermass Experiment]]\'\' (1953), transmitted live while simultaneously telerecorded. The visual quality of the second episode\'s recording was considered so poor—a fly entered the gap between the camera and monitor at one point—that the remainder of the series was not recorded.\n\nAlthough [[Quadruplex videotape]] recording technology was utilised in the UK from 1958, this system was expensive and complex; recorded programmes were often erased after broadcast. The vast majority of live programmes were never recorded at all. Videotape was not initially thought to be a permanent archivable medium – its high cost and the potential reuse of the tapes led to the transfer of programme material to film via [[Kinescope|telerecording]] whenever sales of overseas screening rights were possible or preservation deemed worthwhile. The recycling of videotapes, coupled with savings made on the storage of the bulky 2" tapes,<ref>By 1973, about 20,000 hours of recorded material was stored on videotape at the BBC weighing about 400,000 lbs in total. See \'\'BBC Engineering\'\', No.95, September 1973, London: BBC Publications, p.3</ref> enabled the BBC to keep costs down.\n\n====Cultural====\nDrama and entertainment output was studio-based and followed the tradition of live [[theatre]]. Conventional filmmaking was only gradually introduced from the 1960s. \'\'The Sunday Night Play\'\' (a major event in the 1950s) was performed live in the studio. On Thursday, because telerecording was of insufficient broadcast quality, another live performance followed, the artists returning to perform the play again.\n\nToday, most programmes are pre-recorded and it is relatively inexpensive to preserve programming for posterity; even so, the [[BBC Charter]] makes no mention of any obligation to retain all of them.\n\n====Rights====\nAll television programmes have copyright and other rights issues associated with them. For some genres of programmes—such as drama and entertainment—the actors, writers, and musicians involved in a production all have underlying rights. In the past, these rights were defended rigorously—permission could even be denied by a contributor for the repeat or re-use of a programme. Talent [[Trade union|unions]] were highly suspicious of the threat to new work if programmes were repeated; indeed, before 1955 [[Equity (trade union)|Equity]] insisted that any telerecording made (of a repeat performance) could only "be viewed privately" on BBC premises and not transmitted.\n\n====Colour television====\nThe introduction of colour television in the United Kingdom from 1967 meant that broadcasters felt there was even less value in retaining monochrome recordings. Such tapes could not be re-used for colour production, so they were disposed of to create space for the new colour tapes in the archives, which were quickly filling up. The increased cost of colour [[2 inch Quadruplex videotape]]—approximately £1000 per tape at today\'s prices—meant that companies still often re-used the tapes for efficiency. Negative attitudes to a programme\'s value also persisted. For these reasons, many programmes survive only as monochrome film recordings, if at all.\n\nSome colour productions were telerecorded onto monochrome film for export to countries which did not yet have colour television. In some cases, early colour programmes only survive in this form.\n\n====Significant wiped programmes====\nHigh-profile examples of programme losses include many early episodes of [[Doctor Who]] (97), \'\'[[The Wednesday Play]]\'\', most of the seminal comedy series \'\'[[Not Only But Also]]\'\', all of the 1950s televised [[Francis Durbridge]] serials (further, the first two serials were never recorded), the vast majority of the BBC\'s [[Apollo 11]] [[British television Apollo 11 coverage|Moon landing studio coverage]], all but one of the 39 episodes of \'\'[[The First Lady (TV series)|The First Lady]]\'\',<ref>{{cite web|url=http://www.lostshows.com/default.aspx?programme=ec5863f7-6843-4552-acda-07e19396fdae|title=Lost UK TV Shows Search Engine|author=Simon Coward, Invisible Technology Ltd|publisher=}}</ref> and all 147 episodes of the [[soap opera]] \'\'[[United!]]\'\'. There are many gaps in many long-running BBC series (\'\'[[Dixon of Dock Green]]\'\', \'\'[[Hancock\'s Half Hour]]\'\', \'\'[[Sykes]]\'\', \'\'[[Out of the Unknown]]\'\', and \'\'[[Z-Cars]]\'\'). [[The Beatles]]\' only live appearance on \'\'[[Top Of The Pops]]\'\' in 1966, performing the single "[[Paperback Writer]]" is believed to have been wiped clean in a clear-out in the 1970s.\n\nThe first acting appearance of musician [[Bob Dylan]], in a 1963 play entitled \'\'[[The Madhouse on Castle Street]]\'\', was erased in 1968.<ref>{{cite news|url = http://www.offthetelly.co.uk/reviews/2005/arenadylan.htm|title = Arena: Dylan in the Madhouse|date = 2005-09-28|last = Worthington|first = TJ|work = OFF THE TELLY}}</ref>\n\nThere is lost material in all genres &mdash; as late as 1993, a large number of videotaped children\'s programmes from the 1970s and 1980s were irretrievably wiped by Adam Lee of the [[BBC]] [[archives]] on the assumption that they were of "no use", without consulting the BBC children\'s department itself.<ref>{{cite news |url=http://www.offthetelly.co.uk/oldott/www.offthetelly.co.uk/index8e01.html?page_id=781 |title=Of Finger Mice and Mr. Men - The Story of Watch with Mother Part Eleven: Andy is Waving Goodbye |last=Worthington |first=TJ |date = November 2006|work=Off the Telly}}</ref>\n\n====Other lost material====\nVirtually the entire runs of the corporation\'s pre-1970s soap operas have been lost. In the 1950s and 1960s, the [[BBC]] soap operas \'\'[[The Appleyards]]\'\', \'\'[[The Grove Family]]\'\', \'\'[[Compact (soap opera)|Compact]]\'\', \'\'[[The Newcomers (TV series)|The Newcomers]]\'\', \'\'[[199 Park Lane]]\'\', and \'\'[[United!]]\'\' produced approximately 1200 episodes altogether.\n\nThere are no episodes of either  \'\'United!\'\' or \'\'199 Park Lane\'\' in the archives, while only one episode of \'\'The Appleyards\'\', three episodes of \'\'The Grove Family\'\', and four episodes each of \'\'Compact\'\' and \'\'The Newcomers\'\' are known to exist.\n\nAlso vulnerable to the corporation\'s wiping policy were programmes that only lasted for one season. \'\'[[Abigail and Roger]]\'\', \'\'[[The Airbase]]\'\', \'\'[[As Good Cooks Go]]\'\', the 1960 adaptation of \'\'[[The Citadel (novel)|The Citadel]]\'\', the 1956 adaptation of \'\'[[David Copperfield (novel)|David Copperfield]]\'\', \'\'[[The Dark Island]]\'\', \'\'[[The Gnomes of Dulwich]]\'\', \'\'[[Hurricane]]\'\', \'\'[[For Richer...For Poorer]]\'\', \'\'[[Hereward the Wake]]\'\', \'\'The Naked Lady\'\', \'\'Night Train To Surbiton\'\', \'\'Outbreak of Murder\'\', \'\'Where do I Sit?\'\', and \'\'Witch Hunt\'\' have all been wiped with no footage surviving while four out of seven episodes of the paranormal anthology series \'\'[[Dead of Night (TV series)|Dead of Night]]\'\' were wiped.\n\nAn edition of \'\'[[Hugh and I]]\'\' ("Chinese Crackers"), starring [[Hugh Lloyd]], [[Terry Scott]], [[John Le Mesurier]] and [[David Jason]] was located by [[Kaleidoscope Publishing]] in 2010 in the archives of [[UCLA]], and brought to general public attention in February 2011.\n\nEarly episodes of the pop music-chart show \'\'[[Top of the Pops]]\'\' were wiped or never recorded while they were being transmitted live, including the only in-studio appearance by [[The Beatles]]. Clips of [[the Beatles]] miming "[[Can\'t Buy Me Love]]" and "[[You Can\'t Do That]]" on an episode from 25 March 1964 were found online by missing episode hunter Ray Langstone in 2015. The last lost edition dates from 8 September 1977. There are only four complete \'\'TOTP\'\' episodes surviving from the 1960s, while many otherwise-missing episodes survive only as fragments. Only two episodes still exist of \'\'[[The Sandie Shaw Supplement]]\'\' (a music-variety show hosted by the singer), recorded in 1967.\n\n====Finding missing BBC programmes====\nSince the establishment of an archival policy for television in 1978, BBC archivists and others over the years have used various contacts in the UK and abroad to try to track down missing programmes. For example, all [[BBC Worldwide]] customers—broadcasters around the world—who had bought programmes from the corporation were contacted to see if they still had copies which could be returned; \'\'Doctor Who\'\' is a prime example of how this method recovered episodes that the corporation did not hold itself. At the turn of the 21st century, the BBC established its [[BBC Archives#Archive Treasure Hunt|Archive Treasure Hunt]], a public appeal to recover lost productions, which has had some successes.<ref name="BBCTH">{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/listoffinds.shtml |title=BBC Online - Cult - Treasure Hunt - List of Finds |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}</ref>\n\nThe BBC also has close contacts with the [[National Film and Television Archive]], which is part of the [[British Film Institute]] and its "Missing Believed Wiped" event which was first held in 1993 and is part of a campaign to locate lost items from British television\'s past. There is also a network of collectors who, if they find any programmes missing from the BBC archives, will contact the corporation with information—or sometimes even the actual footage. Some examples of programmes recovered for the archives are \'\'[[Doctor Who]]\'\', \'\'[[Steptoe and Son]]\'\', \'\'[[Dad\'s Army]]\'\', \'\'[[Letter from America]]\'\',<ref>[http://www.bbc.co.uk/informationandarchives/archivenews/2014/letters_from_america_rediscovered.html Letter from America rediscovered], bbc.co.uk, 28 March 2014</ref> \'\'[[The Likely Lads]]\'\', and \'\'[[Play for Today]]\'\'.\n\nFor many years the [[television pilot|pilot]] episode of \'\'[[Are You Being Served?]]\'\' survived only in black and white, appearing in this form on the 2003 DVD release of the show. In 2009, a colour version was [[colour recovery|reconstructed]] when it was realised that the black and white film reel had actually recorded sufficient colour information as a [[dot crawl]] pattern to allow [[colour recovery]].\n\n===ITV===\nThe BBC was not alone in this practice – the commercial companies that formed its main rival [[ITV (TV network)|ITV]] also wiped videotapes and destroyed [[telerecording]]s, leaving gaps in their archive holdings. The state of the archives varies greatly between the different companies; [[Granada Television]] holds a large number of its older black-and-white programmes, the company having an unofficial policy of retaining as much of its broadcast material (albeit by telerecording) as possible despite financial hardship in its early years. This includes the entirety of the soap opera \'\'[[Coronation Street]]\'\' which is now held at the [[Yorkshire Television]] archive, which itself possesses largely intact archives, although some early colour shows from the late 1960s and the early 1970s such as the entire output of the drama \'\'Castle Haven\'\', the first two series of \'\'[[Sez Les]]\'\' and the children\'s variety show \'\'[[Junior Showtime]]\'\' are missing and believed wiped. The former ITV company [[Thames Television]] also has a significant library.\n\nThese cases tend to be the exception, however; the former nature of the ITV network, in which private independent companies were awarded licences to serve geographical areas for a set period of time, meant that when companies lost their licences their archives were often sold to third parties and became fragmented—and/or risked being destroyed, as ownership and [[copyright]] remained with the production companies rather than with the network. The archive of networked programmes made by [[Southern Television]], for example, is now owned by the otherwise-unconnected Australian media company [[Southern Star Group]] but Southern\'s regional output is in the hands of [[ITV plc]]. The few surviving tapes of [[Associated-Rediffusion]] belong to many different organisations as the majority of Associated-Rediffusion\'s tapes were recorded in [[monochrome]] and therefore deemed of no use upon the arrival of colour broadcasting; as such they were disposed of by London successor [[Thames Television]]), although in recent years there have been occasional discoveries such as a 1959 episode of \'\'[[Double Your Money]]\'\' and the remaining missing episode of \'\'[[Around the World with Orson Welles]]\'\', found by Ray Langstone in 2011. Many master tapes belonging to [[Associated Television|ATV]] have since deteriorated due to bad storage and are unsuitable for broadcasting. In particular, the ATV version of the popular soap \'\'[[Crossroads (soap opera)|Crossroads]]\'\' is missing 2,850 episodes of its original 3,555. Also often largely lost are quiz shows; few editions exist of the 1970s version of \'\'Celebrity Squares\'\' with [[Bob Monkhouse]], or Southern\'s children\'s quiz \'\'[[Runaround (game show)|Runaround]]\'\'.{{cn|date=December 2016}}\n\nFurther, responsibility for archive preservation was left to individual companies. For example, ITV has no record of its live coverage of [[Apollo 11|the 1969 Moon landings]] after the station responsible for providing the coverage, [[London Weekend Television]], wiped the tapes. Of the 96 [[United Kingdom|British]] inserts to the 1980s franchised [[United Kingdom|Anglo]]-[[United States|American]]-[[Canada|Canadian]] children\'s show \'\'[[Fraggle Rock]]\'\', only 12 are known to exist as the library of the British producer ([[Television South|TVS]]) has been sold and subsequently split up.\n\nIn recent years, the trend of preserving material has started to change. The archives of [[Westward Television]] and [[Television South West]] are now held in trust for the public as the South West Film and Television Archive, whilst changes in legislation mean that ITV companies which lose their franchises must donate archives to the [[British Film Institute]]. However, the change of ITV from a federal structure to one centralised company means that changes of regional companies in the future seems highly unlikely.\n\nMost material from the 1960s also only survive as telerecordings. Some early episodes are also believed to be damaged or in poor quality, whereas much of the output of other broadcasters – such as many early episodes of \'\'[[The Avengers (TV series)|The Avengers]]\'\' which were shot in the electronic studio rather than on film, produced by [[Associated British Corporation]] – have been destroyed.\n\nNo copies of \'\'The Adventures of [[Francie and Josie|Francie & Josie]]\'\' exist, as most of [[Scottish Television]]\'s early shows were destroyed in a fire in late 1969 (although some sources state 1973). \'\'The Adventures Of Francie & Josie\'\' was made from 1961 to 1965 by STV.\n\n===Recovery of missing programmes===\nSince the BBC library was first audited in 1978, missing programmes or extracts on either film or tape are often found in unexpected places. An appeal to broadcasters in other countries who had shown missing programmes (notably [[Australia]], [[New Zealand]], [[Canada]], and [[Africa]]n nations such as [[Nigeria]]) produced "missing" episodes from the archives of those television companies. Episodes have also been returned to broadcasters by private film collectors who had acquired 16mm film copies from various sources.{{cn|date=December 2016}}\n* Two Series 1 episodes of \'\'[[The Avengers (TV series)|The Avengers]]\'\' (an [[Associated British Corporation]] production) which were thought to be missing were recovered from the [[UCLA]] Film & Television Archive in the United States.\n* It emerged in September 2010 that more than 60 recordings of BBC and ITV drama productions originally sent for broadcast in the United States by the [[Public Broadcasting Service|PBS]] station [[WNET]] (which serves [[New York City]] and [[New Jersey]]) had been found at the [[Library of Congress]].<ref>Vanessa Thorpe [https://www.theguardian.com/tv-and-radio/2010/sep/12/lost-tapes-classic-british-television "Lost tapes of classic British television found in the US"], \'\'The Observer\'\', 15 September 2010</ref>\n* The BBC [[sitcom]] \'\'[[Steptoe and Son]]\'\' is completely intact, although approximately half of the colour episodes only exist in monochrome; this was after copies of episodes thought to be lost were recovered in the late 1990s from early non-broadcast standard video recordings made for writers [[Ray Galton]] and [[Alan Simpson (scriptwriter)|Alan Simpson]] by BBC technicians.\n* A few audio recordings of \'\'[[Til Death Us Do Part]]\'\' have been recovered, as well as an extract of the pilot and two episodes from series three.\n\nCopies of several compilations from the British 1960s comedy \'\'[[At Last The 1948 Show]]\'\', held by many to be a forerunner of \'\'[[Monty Python\'s Flying Circus]]\'\', were discovered in the archives of the Swedish broadcaster [[Sveriges Television|SVT]], to whom the producers [[Associated Rediffusion|Rediffusion London]] had sold them upon the companies\' loss of its broadcasting licence. The master tapes, along with much of Rediffusion\'s programming, were wiped or disposed of by London successor Thames Television. Their recovery enabled the reconstruction of otherwise missing original editions of the programme, meaning most of the series exists in visual form.\n\nOff-air home audio recordings of various television programmes have also been recovered, at least preserving the soundtracks to otherwise missing shows, and some of these (particularly from \'\'[[Doctor Who]]\'\') have been released on [[CD]] by the BBC following restoration and the addition of narration to describe purely visual elements. [[Tele-snaps]], a commercial service of off-screen shots of programmes often purchased by [[actor]]s and [[television director]]s to keep a record of their work in the days before [[videocassette recorder]]s, have also been recovered for many lost programmes.\n\n===Preservation of the current archive===\nAdvances in technology have resulted in old programmes being transferred to new digital media, where they can be restored or (if they are damaged or otherwise cannot be restored) kept from decaying further. In the United Kingdom, the archives of both the BBC and those available of ITV, along with other channels, are being switched from cumbersome [[Quadruplex videotape|2-inch quadruplex videotape]] to digital format. This is an extensive and expensive process and one that will take many years to complete.\n\nLive broadcasts in Britain are still not necessarily kept, and wiping of material has not ceased. According to writer and broadcaster [[Matthew Sweet (writer)|Matthew Sweet]], there are "big gaps in the record of children\'s television of the Nineties."<ref>Matthew Sweet [http://www.telegraph.co.uk/culture/tvandradio/10492487/Searching-for-televisions-missing-gems-Doctor-Who-Woody-Allen-Ridley-Scott-and-Dennis-Potter.html "Searching for television\'s missing gems: Doctor Who, Woody Allen, Ridley Scott and Dennis Potter"], telegraph.co.uk, 4 December 2013</ref>\n\n==United States==\nIn the [[United States]], the major broadcast networks also engaged in the practice of wiping recordings until the late 1970s. Many episodes were erased, especially daytime and late-night programming such as daytime [[soap opera]]s and [[game show]]s. The daytime shows, almost all of them having been taped, were erased because it was believed at the time that nobody wanted to see them after their first broadcast. The success of [[cable television]] networks devoted to reruns of these genres proved that this was not the case, as the large number of episodes that were required for a daily program made even a short-run game show an ideal candidate for [[broadcast syndication|syndication]]. By this time, however, the damage had already been done.\n\n===Preservation by institutions such as museums===\n\nSome museums and other [[cultural institution]]s such as the Paley Center for Media have taken steps to discover and preserve (see, e.g., "[[Paley Center for Media#Archives]]") old recordings previously thought to have been wiped or discarded, lost, or misfiled.\n\n===Hosting sequences===\nHosting sequences on videotape, nearly always featuring celebrities, were sometimes made for telecasts of family films, notably for the first nine telecasts of MGM\'s \'\'[[The Wizard of Oz (1939 film)|The Wizard of Oz]]\'\'. It is not known if those made for \'\'Oz\'\' survived since they have not been seen since 1967. One hosting sequence from that era that does survive is the one [[Eddie Albert]]  made for the 1965 CBS telecast of \'\'[[The Nutcracker]]\'\', starring [[Edward Villella]], [[Patricia McBride]], and [[Melissa Hayden (dancer)|Melissa Hayden]]. It has even been included on the DVD release of the program.<ref>{{cite web|url=http://www.wbshop.com/product/nutcracker+the+1965+tv+sp+1000179869.do?from=Search|title=Nutcracker, The (1965 TV SP) (MOD)|work=www.WBShop.com}}</ref>\n\n===Ernie Kovacs===\nMany of [[Ernie Kovacs]]\'s videotaped network programs were also wiped. During different times as comedian, writer, and performer Kovacs had programs on all four major television networks ([[American Broadcasting Company|ABC]], [[CBS]], [[DuMont Television Network|DuMont]], and [[NBC]]). After Kovacs\'s death, the networks wiped many programs. Kovacs\'s widow [[Edie Adams]] obtained as many programs and episodes as she could find, donating them to [[UCLA]]\'s [[Special Collections]].\n\n===Soap operas===\nThough most soap operas transitioned from live broadcast to videotaping their shows during the 1960s, it was still common practice to wipe and reuse the tapes. This practice was due to the high cost of videotape at the time. While soap operas began routinely saving their episodes between 1976 and 1979, several soaps have saved recordings of most or all their episodes. \'\'[[Days of Our Lives]]\'\'  has recordings of all its episodes; its first two episodes exist on their original master tapes, and were aired by [[SOAPnet]] in 2005. \'\'[[The Young and the Restless]]\'\',  \'\'[[Dark Shadows]]\'\'  and \'\'[[Ryan\'s Hope]]\'\' saved most of their episodes, despite the fact that they debuted during the 1960s and 1970s, before retaining tapes became common practice. Episodes of \'\'[[The Doctors (soap opera)|The Doctors]]\'\' began to be saved no later than December 4, 1967; this is where reruns of the series began when picked up by [[Retro Television Network]] in September 2014. Episodes of other soaps broadcast during the 1950s to 1970s do exist in different forms and have been showcased in various places online.\n\n[[Procter and Gamble]] started saving their shows around 1979. Very few pre-1979 color episodes of the Procter and Gamble-sponsored shows survive, with most extant episodes preserved as monochrome kinescopes. Exceptions include two episodes of \'\'[[The Guiding Light]]\'\' (1973 and 1977), which have been released on DVD. \'\'[[As the World Turns]]\'\' and \'\'[[The Edge of Night]]\'\' aired live until 1975, the year \'\'The Edge of Night\'\' moved to [[American Broadcasting Company|ABC]] and \'\'As the World Turns\'\' expanded from a 30-minute broadcast to one hour. Both shows began taping episodes in preparation for the move of \'\'The Edge of Night\'\' to ABC. \'\'The Edge of Night\'\'<nowiki>\'s</nowiki> ABC debut is believed to have survived. Overall, the number of surviving monochrome episodes recorded on kinescope outnumber color episodes for these programs.\n\n[[Agnes Nixon]] initially produced her series \'\'[[One Life to Live]]\'\' and \'\'[[All My Children]]\'\' through her own production company, Creative Horizons, Inc., and kept a complete archive of monochrome kinescopes until ABC bought the shows from her in 1975. When the network wanted to expand \'\'All My Children\'\' from 30 minutes to a full hour in the late 1970s, Nixon agreed on the condition that the network would begin saving the episodes. ABC complied, and full hour broadcasts began on April 25, 1977. However, a fire destroyed the vast majority of the early-1970s kinescopes, leaving only a few random episodes from each season.\n\nVirtually all episodes of \'\'[[General Hospital]]\'\', from its premiere in April 1963 through August 1970, are archived at [[UCLA]]. The [[UCLA Film & Television Archive]] holds a large number of daytime television airings that were spared from the wiping practice.  Also archived there are handfuls of episodes of each soap opera that was on the air from 1971 and 1973, including \'\'[[A World Apart (TV series)|A World Apart]]\'\', \'\'[[Where the Heart Is (1969 TV series)|Where the Heart Is]]\'\', and \'\'[[Return to Peyton Place (TV series)|Return to Peyton Place]]\'\'.\n\n===DuMont programs===\nIt is believed that virtually the entire archive of the [[DuMont Television Network]], covering its whole history from 1946 to 1956, was disposed of during the 1970s by a "successor" broadcaster (presumably [[Metromedia]], the holder of DuMont\'s assets), who dumped all of the kinescopes/videotapes into the [[East River]] to make room for other tapes (believed to be ABC\'s) at a New York City warehouse.<ref name="LoC">{{cite web|last = Adams|first = Edie|authorlink = Edie Adams|title = Television/Video Preservation Study: Los Angeles Public Hearing|work = National Film Preservation Board| publisher = Library of Congress|date = March 1996|url = http://www.loc.gov/film/hrng96la.html|accessdate = 2008-05-09}}</ref> Further, a large number of DuMont\'s kinescopes were destroyed in about 1958 for their silver content.\n\nOf the over 20,000 shows carried by DuMont in its ten-year existence, [[List of surviving DuMont Television Network broadcasts|approximately 350 or so episodes of DuMont programming are known to exist today]], less than two percent of its total output. The remainder were either never recorded (e.g., \'\'[[NFL on DuMont]]\'\') or were dumped in the earlier purges.\n\n===\'\'The Tonight Show\'\'===\n{{See also|The Tonight Show Starring Johnny Carson}}\nAlmost all of \'\'[[The Tonight Show]]\'\' with [[Jack Paar]] and the first ten years hosted by his successor [[Johnny Carson]] were taped over by the network, with Carson\'s blessing, under the assumption that the broadcasts were of no real value.<ref>[http://www.washingtonpost.com/entertainment/tv/carson-on-tcm-shows-why-johnny-was-the-king/2013/07/03/3c25d1ce-e264-11e2-aef3-339619eab080_story.html Carson on TCM shows why Johnny was the king]. \'\'The Washington Post\'\'. Retrieved July 9, 2013.</ref> This is part of the reason why Carson\'s late 1960s shows had poorer picture quality{{Citation needed|date=November 2010}} compared to his competitor [[Dick Cavett]] on [[American Broadcasting Company|ABC]]; [[NBC]] was using the \'\'Tonight Show\'\' tapes repeatedly. Another reason for their poorer quality is that many of the 1960s \'\'Tonight Show\'\' episodes only survived in the kinescope format. (Cavett\'s ABC shows were also taped over by his network in favor of other shows produced at ABC\'s studios in New York.)\n\n===Early sporting events===\n{{See also|List of World Series broadcasters|List of Super Bowl broadcasters|NFL on CBS|NFL on NBC}}\n\nMany early sporting events, such as the [[World Series]] and the first two [[Super Bowl]]s, were also lost, though a nearly intact recording of the first Super Bowl was found in 2005.\n\n====[[National Football League]]====\n\'\'[[Super Bowl I]]\'\' was aired by both [[CBS]] and [[NBC]] (the only Super Bowl to be aired by two networks), but neither network felt the need to preserve the game long-term; CBS saved the telecast for a few months and reran it as filler programming at least once before wiping it. A color videotape containing the first, second and fourth quarters of the telecast from [[WYOU]] (the CBS affiliate for [[Scranton, Pennsylvania]]) was found in 2005 and is in the process of being restored.<ref>Fybush, Scott (2011-02-07). [http://www.fybush.com/NERW/2011/110207/nerw.html Will New York Outlaw Pirate Radio?]. \'\'NorthEast Radio Watch\'\'. Retrieved 2011-02-07.</ref> On January 15, 2016, the [[NFL Network]] reaired the first Super Bowl, featuring audio from [[NBC Radio]] and most of the TV network broadcast and newly discovered [[NFL Films]] footage of the game. \'\'[[Super Bowl II]]\'\' was aired exclusively by CBS and was believed to have been erased, but it was later found that the entire telecast fully exists and rests in the vaults of [[NFL Films]].<ref name="foo">{{cite web\n | url        = http://www.marketwatch.com/story/the-hunt-for-tvs-lost-baseball-treasures-2010-10-27?pagenumber=2\n | title      = The hunt for TV’s lost baseball treasures\n | author     = David B. Wilkerson\n | date       = October 27, 2010\n | work       =\n | publisher  = [[Wall Street Journal]] Marketwatch\n | accessdate = November 26, 2012\n}}</ref>  Though the telecast of \'\'[[Super Bowl III]]\'\' exists in full color, only half of the \'\'[[Super Bowl IV]]\'\' broadcast does (the rest was preserved by Canadian television in black-and-white). The first three quarters of \'\'[[Super Bowl V]]\'\' broadcast by NBC Los Angeles affiliate [[KNBC]] exist, but the fourth quarter is missing, though the [[Mike Curtis (American football)|Mike Curtis]] interception and [[Jim O\'Brien (American football)|Jim O\'Brien]] game-winning field goal were recovered via news highlights from [[CBC Television|CBC]]. \'\'[[Super Bowl VI]]\'\' also exists in its entirety. It was not until \'\'[[Super Bowl VII]]\'\' that a continuous archive was established.<ref name="foo" />\n\nSimilarly, all of the telecasts of the [[NFL Championship Game]]s prior to the Super Bowl are believed to have been lost, with all surviving footage of those games coming from separately produced film. The status of most regular season and playoff games from the early years of television up to the immediate years following the 1970 [[AFL–NFL merger]] are also unknown. Among the footage that has survived include at least some of NBC\'s coverage from the 1972 AFC Divisional Playoff game between the [[1972 Pittsburgh Steelers season|Pittsburgh Steelers]] and [[1972 Oakland Raiders season|Oakland Raiders]] that featured the [[Immaculate Reception]], as well as the inaugural telecast of \'\'[[Monday Night Football]]\'\' between the [[1970 Cleveland Browns season|Cleveland Browns]] and the [[1970 New York Jets season|New York Jets]], though several \'\'Monday Night Football\'\' games in the ensuing seasons were lost. A [[1974 NFL season|1974 game]] that featured [[John Lennon]] being interviewed by [[Howard Cosell]] in the booth only survived due to a [[home video]] recording of the game; the game itself was wiped by ABC. CBS kept coverage of a 1978 [[Eagles–Giants rivalry|matchup]] between the [[1978 New York Giants season|New York Giants]] and [[1978 Philadelphia Eagles season|Philadelphia Eagles]] that would feature the now-infamous [[Miracle at the Meadowlands]], although the existence of many 1978 games on CBS by private collectors shows that the networks by that point started keeping recordings of regular season games. There are rare exceptions of CBS games from [[1977 NFL season|1977]] back, but by [[1978 NFL season|1978]] the library of most teams is almost fully complete. NBC is another story.\n\nThe NFL had its own filmmakers, [[NFL Films]], filming the game with its own equipment. Thus, preserving the telecasts on tape was not seen as a priority by the networks when another source was available – though the sportscasters\' play-by-play comments, as a result, were lost.\n\n====World Series telecasts====\nAll telecasts of World Series games starting in [[1975 World Series|1975]] ([[1975 Cincinnati Reds season|Reds]]–[[1975 Boston Red Sox season|Red Sox]]) are known to exist in full.<ref name="Surviving World Series Telecasts">{{cite web|url=http://www.dbsforums.com/vbulletin/showthread.php?t=78232|title=www.dbsforums.com|publisher=}}</ref> Follows is the known footage of World Series telecasts prior to 1975:\n* [[1952 World Series|1952]] ([[1952 New York Yankees season|Yankees]]–[[1952 Brooklyn Dodgers season|Dodgers]]) – Games 6–7 are intact.\n* [[1955 World Series|1955]] ([[1955 New York Yankees season|Yankees]]–[[1955 Brooklyn Dodgers season|Dodgers]]) – Only the first half of Game 5 is known to exist.\n* [[1956 World Series|1956]] ([[1956 New York Yankees season|Yankees]]–[[1956 Brooklyn Dodgers season|Dodgers]]) – Only the last three innings of Game 2 are known to exist. Game 3 is intact minus the second and third inning. Game 5 ([[Don Larsen]]\'s [[perfect game]]) is intact minus the first inning, and was aired on January 1, 2009 during the [[MLB Network]]\'s first broadcast day.\n* [[1957 World Series|1957]] ([[1957 New York Yankees season|Yankees]]–[[1957 Milwaukee Braves season|Braves]]) – Game 1 is intact by way of a print from the United States [[American Forces Network|Armed Forces Radio and Television Service]].<ref>https://www.youtube.com/watch?v=72Eo0pIka4o</ref> Game 3 is intact, minus a snip of [[Tony Kubek]]\'s second home run in the top 7th inning. Games 6 (most of the first six innings) and 7 reportedly exist as well.\n* [[1960 World Series|1960]] ([[1960 New York Yankees season|Yankees]]–[[1960 Pittsburgh Pirates season|Pirates]]) – Game 7 (with [[Bill Mazeroski]]\'s series-clinching walk-off home run) was found intact on [[kinescope]] in December 2009 in the wine cellar of Pirates\' part-owner [[Bing Crosby]], who had the game recorded at his own expense. MLB Network aired it in December 2010.<ref>{{cite news|url = http://www.nytimes.com/2010/09/24/sports/baseball/24crosby.html?_r=1&src=mv|title = In Bing Crosby\'s Wine Cellar, Vintage Baseball|first = Richard|last = Sandomir|authorlink = Richard Sandomir|publisher = \'\'[[The New York Times]]\'\'|date = 2010-09-23|accessdate = 2010-09-25}}</ref>\n* [[1961 World Series|1961]] ([[1961 New York Yankees season|Yankees]]–[[1961 Cincinnati Reds season|Reds]]) – Half-hour segments of Games 3 (the first two innings), 4 (the 4th and 5th innings), and 5 (open and top of the 1st inning) are known to exist.\n* [[1963 World Series|1963]] ([[1963 New York Yankees season|Yankees]]–[[1963 Los Angeles Dodgers season|Dodgers]]) – Game 3 is intact.\n* [[1965 World Series|1965]] ([[1965 Minnesota Twins season|Twins]]–[[1965 Los Angeles Dodgers season|Dodgers]]) – All seven games were preserved by the [[CBC Television|CBC]] on [[kinescope]].\n* [[1968 World Series|1968]] ([[1968 Detroit Tigers season|Tigers]]–[[1968 St. Louis Cardinals season|Cardinals]]) – All seven games were preserved by the [[CBC Television|CBC]] on [[kinescope]].\n** It is likely the 1965 and 1968 Series were preserved by the CBC due to the Twins\' and Tigers\' proximity to Canada; the country would not get its own MLB team until the [[Montreal Expos]] began play in 1969.\n* [[1969 World Series|1969]] ([[1969 Baltimore Orioles season|Orioles]]–[[1969 New York Mets season|Mets]]) – Games 1–2 were preserved by the [[CBC Television|CBC]] on [[kinescope]], while Games 3–5 exist on their original color videotape from "truck feeds".\n* [[1970 World Series|1970]] ([[1970 Baltimore Orioles season|Orioles]]–[[1970 Cincinnati Reds season|Reds]]) – Games 1–4 were preserved by the [[CBC Television|CBC]] on [[kinescope]], while Game 5 exists on its original color videotape from the "truck feed".\n* [[1971 World Series|1971]] ([[1971 Baltimore Orioles season|Orioles]]–[[1971 Pittsburgh Pirates season|Pirates]]) – Games 1–2 and 6–7 are intact, while Games 3–5 only partially exist and Game 4 (the first World Series night game) is near-complete.\n* [[1972 World Series|1972]] ([[1972 Oakland Athletics season|A\'s]]–[[1972 Cincinnati Reds season|Reds]]) – Game 4 is intact, along with nearly all of Game 5 and a fair chunk of Game 2. Fragments exist for Games 1, 3, and 6, while Game 7 is missing.\n* [[1973 World Series|1973]] ([[1972 Oakland Athletics season|A\'s]]–[[1972 New York Mets season|Mets]]) – Game 1 is intact, Game 2 is missing the last inning and a half (including both [[Mike Andrews]] plays), Game 3 is complete minus the last inning, Game 4 is intact from the pregame show to the top of the 4th inning, and Game 5 only has the last two innings. About 30 minutes of excerpts from Game 6 survive, while Game 7 cuts off with one out at the top of the 9th inning.\n** While the last inning and a half of Game 2 is missing from the Major League Baseball/[[Major League Baseball on NBC|NBC]] copy, the Andrews plays (totaling about 60 seconds of coverage) survived because after the World Series, NBC put together a 20-minute presentation tape narrated by [[Curt Gowdy]] to submit to the [[Peabody Awards]] in order to get consideration for an award for their coverage by the committee; the tape includes the two Andrews plays with Gowdy and [[Tony Kubek]]\'s calls and analysis of them. The presentation tape is held by the Peabody vault, creating a case where "reconstructing" a game in an incomplete format would require going to two different outlets.\n* [[1974 World Series|1974]] ([[1974 Oakland Athletics season|A\'s]]–[[1974 Los Angeles Dodgers season|Dodgers]]) – Games 1–4 are complete. Game 5 is near intact, but the bottom of the 9th inning is missing and only exists on the original radio broadcast.\n\n====League Championship Series telecasts====\nFor the League Championship Series telecasts spanning from 1969 to 1975, only Game 2 of the [[1972 American League Championship Series]] ([[1972 Oakland Athletics season|Oakland]]–[[1972 Detroit Tigers season|Detroit]]) is known to exist;<ref name="Surviving World Series Telecasts"/> however, the copy on the trade circuit is missing the [[Bert Campaneris]]–[[Lerrin LaGrow]] brawl.\n\nThere are some instances where the only brief glimpse of telecast footage of an early LCS game can be seen in a surviving newscast from that night.\n* Clips of Game 5 of the [[1972 National League Championship Series]] featuring the then-[[Cincinnati Reds]] announcer [[Al Michaels]] calling the two crucial plays of the game, the game-tying home run by [[Johnny Bench]] and wild pitch bringing home [[George Foster (baseball)|George Foster]] with the series-clinching run, are available.\n* The last out of the [[1973 National League Championship Series]] as described by [[Jim Simpson (sportscaster)|Jim Simpson]] was played on that night\'s \'\'[[NBC Nightly News]]\'\', but other than that the entire game is gone.\n* On the day the [[1969 New York Mets season|New York Mets]] and [[1969 Baltimore Orioles season|Baltimore Orioles]] wrapped up their respective League Championship Series in 1969, a feature story on the \'\'[[CBS Evening News]]\'\' showed telecast clips of the [[1969 American League Championship Series|ALCS]] game (albeit with no original sound). This is all that likely remains of anything from that third game of the [[1969 Baltimore Orioles season|Orioles]]–[[1969 Minnesota Twins season|Twins]] series.\n\nWhile all telecasts of World Series games starting with [[1972 World Series|1975]] are accounted for and exist, the LCS is still a spotty situation through the late 1970s:\n* [[1976 American League Championship Series|1976 ALCS]] – Game 5 is intact, from the [[American Broadcasting Company|ABC]] vault.\n* [[1976 National League Championship Series|1976 NLCS]] – Game 3 is intact, albeit an off-air recording taped in the [[KATU|Portland market]]. Apparently, this copy is the only extant version because the ABC vault copy has no sound.\n* [[1977 National League Championship Series|1977 NLCS]] – Game 3 is intact, from the [[1977 Philadelphia Phillies season|Philadelphia Phillies]]\' local [[KYW-TV|NBC affiliate]]. A copy is held by Major League Baseball, who also appears to have Game 4 as well.\n* [[1977 American League Championship Series|1977 ALCS]] – Game 5 is intact, with both the [[WPIX-TV|WPIX]] and [[Major League Baseball on NBC|NBC]] versions existing through off-air recordings.\n** Clips of these games may be seen in highlight shows such as \'\'[[Yankeeography]]\'\'. It is believed that incomplete tapes of the ALCS exist. It is possible these games are not shown in part because the audio quality is poor. A common method of getting around such deficiencies would be to overlay a radio telecast or narration by a player or commentator where gaps exist.\n* [[1978 American League Championship Series|1978 ALCS]] – All four games ([[Major League Baseball on ABC|ABC]] version) are intact via off-air recordings.\n* [[1978 National League Championship Series|1978 NLCS]] – Game 4 is intact, again from off-air recordings.\n\n====NBA Finals====\n{{see also|List of NBA Finals broadcasters}}\n* [[1963 NBA Finals|1963]]: [[Boston Celtics|Celtics]]–[[Los Angeles Lakers|Lakers]] – Game 6 is intact.\n* [[1969 NBA Finals|1969]]: Celtics–Lakers – only the entire 4th quarter of Game 7 exists.\n* [[1970 NBA Finals|1970]]: Lakers–[[New York Knicks|Knicks]] – Game 7 is intact.\n* [[1971 NBA Finals|1971]]: [[Milwaukee Bucks|Bucks]]–[[Washington Bullets|Bullets]] – only nearly all of the second half of game 4 exists.\n* [[1972 NBA Finals|1972]]: Knicks–Lakers – Game 5 is intact with the exception of the last 3–4 minutes of the game\n* [[1973 NBA Finals|1973]]: Knicks–Lakers – Games 1–4 are missing, while the entire Game 5 wasn\'t found until 2013 and some of which was shown in the \'\'[[30 for 30]]\'\' documentary \'\'When The Garden Was Eden\'\'.\n* [[1974 NBA Finals|1974]]: Bucks–Celtics – only the 4th quarter and 2 overtime of Game 6 and the 4th quarter of Game 7 exist.*[1975 NBA Finals -.Bullets-Warriors game ..1,2,&3 intact\n* [[1976]]: Suns-Celtics - Games 5 & 6 are intact.\n\n===Wiped programs===\n\n====Early live shows====\nMany programs in the early days of television were live broadcasts that are lost because they were not recorded. Most prime-time programs that were preserved used the [[kinescope|kinescope recording]] process, which involved filming the live broadcast from a television screen using a motion-picture camera (videotape, for recording programs, was not perfected until the late 1950s and was not widely used until the late 1960s). This was also a common practice for broadcasting live TV shows to the [[West Coast of the United States|west coast]], as performers often performed a show back-to-back, but never back-to-back-to-back.\n\nDaytime programs, however, were generally not kinescoped for preservation (although many were temporarily kinescoped for later broadcast, episodes recorded in this way were often junked). Many local station and network newscasts were prone to wiping.\n\n====News====\nSome early news programs, such as \'\'[[Camel News Caravan]]\'\', are largely lost. Moving images of [[Walter Cronkite]] reading the news in his studio every night for six years are gone with the exception of his coverage of the [[Cuban Missile Crisis]] in 1962 and the [[JFK assassination]] in 1963. Studio shots of [[Peter Jennings]] inside his [[American Broadcasting Company|ABC]] studio during his first year there (1965) are also gone.\n\n[[Vanderbilt University]] has kept all evening national news telecasts since Monday, August 5, 1968.\n\nAs of 1997, CBS had saved 1,000,000 videotapes of news reports, broadcasts, stock footage, and outtakes according to a report that year from the [[National Film Preservation Board]]. The same report added, "Television stations still erase and recycle their video cassettes", referring to local news programs.<ref>{{cite web\n | url        = http://www.loc.gov/film/tvstudy.html\n | title      = Television/Video Preservation Study: Volume 1: Report\n | author     = [[Librarian of Congress]]\n | date       = October 1997\n | work       =\n | publisher  = [[Library of Congress]]\n | accessdate = 26 November 2012\n}}</ref> Many local stations contract with outside companies for archiving news coverage.\n\n====Situation comedy====\nLittle of the first [[sitcom]], \'\'[[Mary Kay and Johnny|The Mary Kay and Johnny Show]]\'\', remains today. It was initially live and not recorded, but later in its run kinescopes were made for rebroadcasting. Fragments of episodes and one complete installment are known to exist.\n\n====Game shows====\n[[Game show]]s, more than any other genre, were prone to wiping. Many games between 1941 and 1980 had insignificantly-short runs (some measured in a span of weeks or even days) that the networks felt it unnecessary to keep them for posterity, whereas recycling the tapes would be more profitable and less of an effort than attempting to sell the series in reruns, in an era before [[cable television]].\n\n[[Mark Goodson]]–[[Bill Todman]] Productions (and to a lesser extent, [[Barry & Enright Productions|Barry-Enright Productions]] and [[Chuck Barris Productions]]) and to an even lesser extent [[Heatter-Quigley Productions]] had the foresight to preserve many of their games for later reruns; for years, these shows dominated the [[Game Show Network]] (GSN) line-up and now make up a major portion of [[Buzzr TV]]\'s lineup.\n\nMost other game shows from that era were not so fortunate.  All of the [[Bob Stewart (television)|Bob Stewart]] (except \'\'[[Pyramid (game show)|Pyramid]]\'\'), [[Heatter-Quigley Productions|Heatter–Quigley]] except for \'\'[[PDQ (game show)|PDQ]]\'\' which aired in syndication as well as many episodes of \'\'[[Hollywood Squares]]\'\', [[Stefan Hatos-Monty Hall Productions|Hatos–Hall]] (except for a large portion of \'\'[[Let\'s Make a Deal]]\'\'), and pre-1980 [[Merv Griffin]] productions have been destroyed, with the exception of a few rare pilots and "cast aside" episodes. The few remaining episodes have therefore become collectors\' items, and an active trading circuit exists among collectors.\n\nNBC and ABC continued the wiping process well into the 1970s; while ABC ceased in early 1978, NBC continued to wipe some shows into 1980, leaving much of their daytime game show content lost forever. CBS abandoned the wiping process by September 1972, largely as a result of their collaboration with Goodson-Todman; as a result, even the network\'s shorter-lived games (such as \'\'[[Spin-Off (game show)|Spin-Off]]\'\') still exist in their entirety. Incidentally, all three networks ended their wiping practices during the time [[Fred Silverman]] led their respective networks.\n\nWhile it remained in business, DuMont wished to keep its programs as intact as possible. However, the network ceased to exist in 1956 and its archive was destroyed in the 1970s. The corporate successor to DuMont, [[Fox Broadcasting Company|Fox]], not only has never aired any daytime programming (other than its [[Fox Kids]] block from 1990 to 2001) but debuted in 1986, well beyond the wiping era.\n\n====Award shows====\nSeveral award shows from the 1950s and 1960s, such as the [[Academy Awards]] and the [[Emmy Awards]], only survive in kinescope format. From [[29th Academy Awards|1957]] to [[37th Academy Awards|1965]], the Academy Awards were taped in black and white, but only survive in kinescope format for overseas distribution, especially for the European TV audiences, which used another system ([[576i|625 lines]] as opposed to [[480i|525 lines]]), as the tapes used for late broadcasting were reused. All of the taped broadcasts of the Oscars from [[38th Academy Awards|1966]] (the first to be broadcast in color) remain intact.\n\n==See also==\n{{portal|Television}}\n* [[Doctor Who missing episodes|\'\'Doctor Who\'\' missing episodes]]\n* [[British television Apollo 11 coverage]]\n* [[Missing Believed Wiped]]\n* [[Kinescope]]\n* [[Lost film]]\n* [[List of lost television broadcasts]]\n* [[Film preservation]]\n* [[List of surviving DuMont Television Network broadcasts]]\n\n==Footnotes==\n{{Reflist|2}}\n\n==References==\n*{{cite book |last= Fiddy |first= Dick |authorlink= |coauthors= |title= Missing, Believed Wiped: Searching for the Lost Treasures of British Television |year= 2002 |publisher= [[British Film Institute]] |location= London |isbn= 978-0-85170-866-9 }}\n\n==External links==\n*[http://www.bbc.co.uk/cult/treasurehunt/ Full Details of the BBC\'s treasure Hunt]\n*[http://www.lostshows.com/default.aspx? Lost Shows (UK) search engine], Kaleidoscope website\n*[http://www.missing-episodes.com/ British TV Missing Episodes Index]\n*[http://www.wipednews.com/ Wiped News.Com - A news and features website devoted to missing TV, Film & Radio]\n*[http://www.marketwatch.com/story/story/print?guid=E880D4C8-E078-11DF-B7D4-002128049AD6 The hunt for TV’s lost baseball treasures]\n*[http://www.tvobscurities.com/lost/lostormissing/ Television Obscurities >> Television — Lost or Missing]\n\n{{Major League Baseball on national television}}\n{{National Basketball Association on television}}\n{{National Football League on television and radio}}\n\n[[Category:Television terminology]]\n[[Category:Data management]]\n[[Category:Television preservation]]']
['Rasdaman', '36377941', '{{Infobox software\n| name = rasdaman\n| logo = [[Image:Rasdaman logo.png|frame|center|x250px|alt=rasdaman logo (used with permission of copyright holder)|rasdaman logo (used with permission of copyright holder)]]\n| developer = rasdaman GmbH\n| latest_release_version = rasdaman v9.2.1\n| latest_release_date = {{release date |2016|02|17}}\n| status = Active\n| operating_system = most [[Unix-like]] operating systems\n| programming language = [[C++]]<ref>{{cite web |url=https://www.openhub.net/p/rasdaman |title=The rasdaman Open Source Project on Open Hub |work=Open Hub |publisher=Black Duck Software |accessdate=2016-08-01}}</ref>\n| genre = [[Array DBMS]]\n| license = [[GNU General Public License|GPL v3]]/[[GNU Lesser General Public License|LGPL v3]] or [[Proprietary software|proprietary]]<ref>{{cite web|url=http://rasdaman.org/wiki/License |title=Rasdaman License |publisher=rasdaman.org |date= |accessdate=2016-08-01}}</ref>\n| website = {{URL|http://rasdaman.org}}, {{URL|http://rasdaman.com}}\n}}\n\n\'\'\'Rasdaman\'\'\' ("raster data manager") is an [[Array DBMS]], that is: a [[Database Management System]] which adds capabilities for storage and retrieval of massive multi-dimensional [[array data structure|arrays]], such as sensor, image, and statistics data. A frequently used synonym to arrays is raster data, such as in 2-D [[raster graphics]]; this actually has motivated the name \'\'rasdaman\'\'. However, rasdaman has no limitation in the number of dimensions - it can serve, for example, 1-D measurement data, 2-D satellite imagery, 3-D x/y/t image time series and x/y/z exploration data, 4-D ocean and climate data, and even beyond spatio-temporal dimensions.\n\n== History ==\n\nIn 1989, [[Peter Baumann (computer scientist)|Peter Baumann]] started a research on database support for images, then at [[Fraunhofer Society|Fraunhofer Computer Graphics Institute]]. Following an in-depth investigation on raster data formalizations in imaging, in particular the AFATL Image Algebra, he established a database model for multi-dimensional arrays, including a data model and declarative query language.<ref>Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/journals/vldb/vldb3.html#Baumann94 On the Management of Multidimensional Discrete Data]. VLDB Journal 4(3)1994, Special Issue on Spatial Database Systems, pp. 401 - 444</ref>\n\nAt [[Technical University Munich|TU Munich]], in the EU funded basic research project \'\'RasDaMan\'\', a first prototype was established, on top of the O2 [[Object-oriented database|object-oriented DBMS]], and tested in Earth and Life science applications.<ref name="cordis.europa.eu/">http://cordis.europa.eu/result/rcn/20754_en.html</ref> Over further EU funded projects, this system was completed and extended to support relational DBMSs.\nA dedicated research spin-off, rasdaman GmbH,<ref name="Rasdaman.com">http://www.rasdaman.com</ref> was established to give commercial support in addition to the research which subsequently has been continued at [[Jacobs University Bremen|Jacobs University]].<ref name="Rasdaman.com/Archive">http://www.rasdaman.com/News/archive.php</ref> Since then, both entities collaborate on the further development and use of the rasdaman technology.\n\n== Concepts ==\n\n=== Data model ===\n\nBased on an array algebra<ref>Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/conf/ngits/ngits99.html#Baumann99 A Database Array Algebra for Spatio-Temporal Data and Beyond]. Proc. NGITS’99, LNCS 1649, Springer 1999, pp.76-93</ref> specifically developed for database purposes, rasdaman adds a new attribute type, array, to the relational model. As this array definition is parametrized it constitutes a [[Second-order logic|second-order]] construct or [[Template (C++)|template]]); this fact is reflected by the second-order functionals in the algebra and query language.\n\nFor historical reasons, [[Table (database)|tables]] are called \'\'collections\'\', as initial design emphasized an embedding into the object-oriented database standard, [[ODMG]]. Anticipating a full integration with SQL, rasdaman collections represent a binary relation with the first attribute being an [[object identifier]] and the second being the array. This allows to establish [[Foreign key|foreign key references]] between arrays and regular [[Tuple|relational tuples]].\n\n=== Raster Query Language ===\n\nThe rasdaman query language, rasql, embeds itself into standard SQL and its set-oriented processing.\nOn the new attribute type, multi-dimensional arrays, a set of extra operations is provided which all are based on a minimal set of algebraically defined core operators, an \'\'array constructor\'\' (which establishes a new array and fills it with values) and an \'\'array condenser\'\' (which, similarly to SQL aggregates, derives scalar summary information from an array). The query language is declarative (and, hence, optimizable) and safe in evaluation - that is: every query is guaranteed to return after a finite number of processing steps.\n\nThe rasql query guide<ref>n.n.: [http://rasdaman.org/browser/manuals_and_examples/manuals/doc-guides/ql-guide.pdf Rasdaman Query Language Guide]</ref> provides details, here some examples may illustrate its use:\n\n* "From all 4-D x/y/z/t climate simulation data cubes, a cutout which contains all in x, a y extract between 100 and 200, all available along z, and a slice at position 42 (effectively resulting in a 3-D x/y/z cube)":\n<source lang="sql">\nselect c[ *:*, 100:200, *:*, 42 ] \nfrom   ClimateSimulations as c \n</source>\n\n* "In all Landsat satellite images, suppress all non-green areas":\n<source lang="sql">\nselect img * (img.green > 130)\nfrom   LandsatArchive as img\n</source>\n\nNote: this is a \'\'very\'\' naive phrasing of vegetation search; in practice one would use the [[NDVI]] formula, use null values for cloud masking, and several more techniques.\n\n* "All MRI images where, in some region defined by the bit masks, intensity exceeds a threshold of 250":\n<source lang="sql">\nselect img\nfrom   MRI as img, Masks as m\nwhere  some_cells( img > 250 and m )\n</source>\n\n* "A 2-D x/y slice from all 4-D climate simulation data cubes, each one encoded in PNG format": \n<source lang="sql">\nselect png( c[ *:*, *:*, 100, 42 ] )\nfrom   ClimateSimulations as c \n</source>\n\n== Architecture ==\n\n=== Storage management ===\n\n[[Image:Sample tiling of an array for storage in rasdaman.png|frame|x110px|alt=Sample rasdaman tiling|Sample array tiling in rasdaman]]\n\nRaster objects are maintained in a standard relational database, based on the partitioning of an raster object into \'\'tiles\'\'.<ref>\nFurtado, P., Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/conf/icde/icde99.html#FurtadoB99 Storage of Multidimensional Arrays based on Arbitrary Tiling]. Proc. ICDE\'99, March 23–26, 1999, Sydney, Australia, pp. 328-336</ref> Aside from a regular subdivision, any user or system generated partitioning is possible. As tiles form the unit of disk access, it is of critical importance that the tiling pattern is adjusted to the query access patterns; several tiling strategies assist in establishing a well-performing tiling. A geo index is employed to quickly determine the tiles affected by a query. Optionally, tiles are compressed using one of various choices, including lossless and lossy (wavelet) algorithms; independently from that, query results can be comressed for transfer to the client. Both tiling strategy and compression comprise database tuning parameters.\n\nTiles and tile index are stored as [[Binary large object|BLOBs]] in a relational database which also holds the data dictionary needed by rasdaman’s dynamic type system. Adaptors are available for several relational systems, among them open-source [[Postgresql|PostgreSQL]].\nFor arrays larger than disk space, hierarchical storage management (HSM) support has been developed.\n\n=== Query processing ===\n\nQueries are parsed, optimised, and executed in the rasdaman server. The parser receives the query string and generates the operation tree. Further, it applies algebraic optimisation rules to the query tree where applicable; of the 150 algebraic rewriting rules, 110 are actually optimising while the other 40 serve to transform the query into canonical form. Parsing and optimization together take less than a millisecond on a laptop.\n\nExecution follows a \'\'tile streaming\'\' paradigm: whenever possible, array tiles addressed by a query are fetched sequentially, and each tile is discarded after processing. This leads to an architecture scalable to data volumes exceeding server main memory by orders of magnitude.\n   \nQuery execution is parallelised. First, rasdaman offers inter-query parallelism: A dispatcher schedules requests into a pool of server processes on a per-transaction basis. Intra-query parallelism transparently distributes query subtrees across available cores, GPUs, or cloud nodes.\n\n=== Client APIs ===\n\nThe primary interface to rasdaman is the query language. Embeddings into C++ and Java APIs allow invocation of queries, as well as client-side convenience functions for array handling. Arrays per se are delivered in the main memory format of the client language and processor architecture, ready for further processing. Data format codecs allow to retrieve arrays in common raster formats, such as [[Comma-separated values|CSV]], [[Portable Network Graphics|PNG]], and [[Netcdf|NetCDF]].\n\nA Web design toolkit, raswct, is provided which allows to establish Web query frontends easily, including graphical widgets for parametrized query handling, such as sliders for thresholds in queries.\n\n=== Geo Web Services ===\n\nA [[Java (programming language)|Java]] servlet, \'\'petascope\'\', running as a rasdaman client offers Web service interfaces specifically for geo data access, processing and filtering. \nThe following [[Open Geospatial Consortium|OGC]] standards are supported: [[Web Map Service|WMS]], [[Web Coverage Service|WCS]], [[Web Coverage Processing Service|WCPS]], and [[Web Processing Service|WPS]].\n\nFor [[Web Coverage Service|WCS]] and [[Web Coverage Processing Service|WCPS]], rasdaman is the [[reference implementation]].\n\n== Status and license model ==\n\nToday, rasdaman is a fully-fledged implementation offering select / insert / update / delete array query functionality. It is being used in both research and commercial installations.\n\nIn a collaboration of the original code owner, rasdaman GmbH<ref name="Rasdaman.com"/> and [[Jacobs University]], a code split was performed in 2008 - 2009 resulting in \'\'rasdaman community\'\',<ref>http://www.rasdaman.org</ref> an [[open source]] branch, and \'\'rasdaman enterprise\'\', the commercial branch. Since then, \'\'rasdaman community\'\' is being maintained by Jacobs University whereas \'\'rasdaman enterprise\'\' remains proprietary to rasdaman GmbH.\nThe difference between both variants mainly consists of performance boosters (such as specific optimization techniques) intended to support particularly large databases, user numbers, and complex queries; Details are available on the \'\'rasdaman community\'\' website.<ref>[http://rasdaman.eecs.jacobs-university.de/trac/rasdaman/wiki/License rasdaman license model]</ref>\n\nThe \'\'rasdaman community\'\' license releases the server in [[GPL]] and all client parts in [[LGPL]], thereby allowing to use the system in any kind of license environment.\n\n== Impact and Use ==\n\nBeing the first Array DBMS shipped (first prototype available in 1996), rasdaman has shaped this recent database research domain. Concepts of the data and query model (declarativeness, sometimes choice of operators) find themselves in more recent approaches.\n\nIn 2008, the [[Open Geospatial Consortium]] released the [[Web Coverage Processing Service]] standard which defines a raster query language based on the concept of a [[Coverage data|coverage]]. Operator semantics<ref>Baumann, P.: [http://www.springerlink.com/openurl.asp?genre=article&id=doi:10.1007/s10707-009-0087-2 The OGC Web Coverage Processing Service (WCPS) Standard]. Geoinformatica, 14(4)2010, pp. 447-479</ref> is influenced by the rasdaman array algebra.\n\nEarthLook<ref>http://standards.rasdaman.org/</ref> is a showcase for [[Open Geospatial Consortium|OGC]] [[Coverage data|coverage]] standards in action, offering 1-D through 4-D use cases of raster data access and ad-hoc processing. EarthLook is built on rasdaman.\n\nA sample large project in which rasdaman is being used for large-scale services in all [[Earth sciences]] is EarthServer:<ref>http://www.earthserver.eu</ref> six services with a volume of at least 100 Terabyte each are being set up for integrated data / metadata retrieval and distributed query processing.\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:rasdaman}}\n[[Category:Free database management systems]]\n[[Category:Proprietary database management systems]]\n[[Category:NoSQL]]\n[[Category:Data management]]\n[[Category:Query languages]]']
['Category:Statistical data agreements', '24105895', '[[Category:Statistical data|Agreements]]\n[[Category:Agreements]]\n[[Category:Data management]]']
['Data profiling', '794330', '{{multiple issues|\n{{Expert needed|Mathematics|ex2=Science|talk=Section title goes here|reason=it needs additional citations|date=August 2016}}\n{{refimprove article|date=August 2010}}\n{{copy edit|for=Incorrect citation formatting|date=August 2016}}\n}}\n\n\'\'\'Data profiling\'\'\' is the process of examining data available from an existing information source (e.g. a [[database]] or a [[computer file|file]]) and collecting [[descriptive statistics|statistics]] or informative summaries about that data.<ref name="Johnson2009">[Theodore Johnson (2009), "Data Profiling", in Encyclopedia of Database Systems, Springer, Heidelberg]</ref> The purpose of these statistics may be to:\n# Find out whether existing data can easily be used for other purposes\n# Improve the ability to search data by [[tag (metadata)|tagging]] it with [[Index term|keywords]], descriptions, or assigning it to a category\n# Give [[Software metric|metrics]] on [[data quality]], including whether the data conforms to particular standards or patterns\n# Assess the risk involved in [[data integration|integrating data]] in new applications, including the challenges of [[Join (SQL)|join]]s\n# Discover [[metadata]] of the source database, including value patterns and [[frequency distribution|distributions]], [[candidate key|key candidates]], [[inclusion dependency|foreign-key candidates]], and [[functional dependency|functional dependencies]]\n# Assess whether known metadata accurately describes the actual values in the source database\n# Understanding data challenges early in any data intensive project, so that late project surprises are avoided. Finding data problems late in the project can lead to delays and cost overruns.\n# Have an enterprise view of all data, for uses such as [[master data management]], where key data is needed, or [[data governance]] for improving data quality.\n\n== Introduction ==\n\nData profiling refers to the analysis of information for use in a [[data warehouse]] in order to clarify the structure, content, relationships, and derivation rules of the data.<ref name="Kimball2008">[Ralph Kimball et al. (2008), “The Data Warehouse Lifecycle Toolkit”, Second Edition, Wiley Publishing, Inc., ISBN 9780470149775], (p. 297) (p. 376)</ref> Profiling helps to not only understand anomalies and assess data quality, but also to discover, register, and assess enterprise metadata.<ref name="Loshin2009">[David Loshin (2009), “Master Data Management”, Morgan Kaufmann Publishers, ISBN 9780123742254], (pp. 94–96)</ref><ref name="Loshin2003">[David Loshin (2003), “Business Intelligence: The Savvy Manager’s Guide, Getting Onboard with Emerging IT”, Morgan Kaufmann Publishers, ISBN 9781558609167], (pp. 110–111)]</ref> The result of the analysis is used to determine the suitability of the candidate source systems, usually giving the basis for an early go/no-go decision, and also to identify problems for later solution design.<ref name="Kimball2008"/>\n\n== How Data Profiling is Conducted ==\n\nData profiling utilizes methods of descriptive statistics such as minimum, maximum, mean, mode, percentile, standard deviation, frequency, variation, aggregates such as count and sum, and additional metadata information obtained during data profiling such as data type, length, discrete values, uniqueness, occurrence of null values, typical string patterns, and abstract type recognition.<ref name="Loshin2009"/><ref name="Rahm2000">[Erhard Rahm and Hong Hai Do (2000), “Data Cleaning: Problems and Current Approaches” in “Bulletin of the Technical Committee on Data Engineering”, IEEE Computer Society, Vol. 23, No. 4, December 2000]</ref><ref name="Singh2010">[Ranjit Singh, Dr Kawaljeet Singh et al. (2010), “A Descriptive Classification of Causes of Data Quality Problems in Data Warehousing”, IJCSI International Journal of Computer Science Issue, Vol. 7, Issue 3, No. 2, May 2010]</ref>\nThe metadata can then be used to discover problems such as illegal values, misspellings, missing values, varying value representation, and duplicates.\n\nDifferent analyses are performed for different structural levels. E.g. single columns could be profiled individually to get an understanding of frequency distribution of different values, type, and use of each column. Embedded value dependencies can be exposed in a cross-columns analysis. Finally, overlapping value sets possibly representing foreign key relationships between entities can be explored in an inter-table analysis.<ref name="Loshin2009"/>\n\nNormally, purpose-built tools are used for data profiling to ease the process.<ref name="Kimball2008"/><ref name="Loshin2009"/><ref name="Rahm2000"/><ref name="Singh2010"/><ref name="Kimball2004">"[Ralph Kimball (2004), “Kimball Design Tip #59: Surprising Value of Data Profiling”, Kimball Group, Number 59, September 14, 2004, (www.rkimball.com/html/designtipsPDF/ KimballDT59 SurprisingValue.pdf)]</ref><ref name="Olson2003">[Jack E. Olson (2003), “Data Quality: The Accuracy dimension”, Morgan Kaufmann Publishers], (pp. 140–142)</ref> The computation complexity increases when going from single column, to single table, to cross-table structural profiling. Therefore, performance is an evaluation criterion for profiling tools.<ref name="Loshin2003"/>\n\n== When Data Profiling is Conducted ==\n\nAccording to Kimball,<ref name="Kimball2008"/> data profiling is performed several times and with varying intensity throughout the data warehouse developing process. A light profiling assessment should be undertaken immediately after candidate source systems have been identified and DW/BI business requirements have been satisfied. The purpose of this initial analysis is to clarify at an early stage if the correct data is available at the appropriate detail level and that anomalies can be handled subsequently. If this is not the case the project may be terminated.<ref name="Kimball2008"/>\n\nAddition, more in-depth profiling is done prior to the dimensional modeling process in order assess what is required to convert data into a dimensional model. Detailed profiling extends into the ETL system design process in order to determine the appropriate data to extract and which filters to apply to the data set.<ref name="Kimball2008"/>\n\nAdditionally, data may be conducted in the data warehouse development process after data has been loaded into staging, the data marts, etc. Conducting data at these stages helps ensure that data cleaning and transformations have been done correctly and in compliance of requirements.\n\n==Benefits==\n\nThe benefits of data profiling are to improve data quality, shorten the implementation cycle of major projects, and improve users\' understanding of data.<ref name="Olson2003"/> Discovering business knowledge embedded in data itself is one of the significant benefits derived from data profiling.<ref name="Loshin2003"/> Data profiling is one of the most effective technologies for improving data accuracy in corporate databases.<ref name="Olson2003"/>\n\n==See also==\n* [[Data quality]]\n* [[Data governance]]\n* [[Master data management]]\n* [[Database normalization]]\n* [[Data visualization]]\n* [[Analysis paralysis]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Data Profiling}}\n[[Category:Data management]]\n[[Category:Data quality]]\n[[Category:Data analysis]]']
['Compound document', '299663', "{{about|compound documents in general|the W3C standard|Compound Document Format}}\n{{refimprove|date=November 2015}}\nIn [[computing]], a '''compound document''' is a document type typically produced using [[word processor|word processing]] software, and is a regular text document intermingled with non-text elements such as [[spreadsheet]]s, [[picture]]s, [[digital video]]s, [[digital audio]], and other [[multimedia]] features. It can also be used to collect several documents into one.\n\nCompound document [[technology|technologies]] are commonly utilized on top of a [[software componentry]] framework, but the idea of software componentry includes several other concepts apart from compound documents, and software components alone do not enable compound documents. Well-known technologies for compound documents include:\n\n*[[ActiveX Document]]s\n*[[Bonobo (computing)|Bonobo]] by [[Ximian]] (primarily used by [[GNOME]])\n*[[KPart]]s in [[KDE]]\n*[[Multipurpose Internet Mail Extensions]]\n*[[Object linking and embedding]] (OLE) by [[Microsoft]]; see [[Compound File Binary Format]]\n*[[Open Document Architecture]] from [[ITU-T]] (not used)\n*[[OpenDoc]] by [[Apple Computer]] (now defunct)\n*[http://sourceforge.net/projects/verdantium Verdantium]\n*[[XML]] and [[Extensible Stylesheet Language|XSL]] are encapsulation formats used for compound documents of all kinds\n\nThe first public implementation was on the [[Xerox Star]] [[workstation]], released in 1981.<ref>http://www.digibarn.com/collections/systems/xerox-8010/index.html</ref>\n\n==See also==\n* [[COM Structured Storage]]\n* [[Transclusion]]\n* [[Electronic Notebook]]\n\n==References==\n{{Reflist}}\n\n[[Category:Electronic documents]]\n[[Category:Multimedia]]\n\n{{Multimedia-software-stub}}"]
['Electronic article', '1005736', "'''Electronic articles''' are [[Article (publishing)|article]]s in [[academic journal|scholarly journal]]s or [[magazine]]s  that can be accessed via electronic transmission. They are a specialized form of [[electronic document]], with a specialized content, purpose, format, [[metadata]], and availability&ndash;they consist of individual articles from scholarly journals or  magazines (and now sometimes popular magazines), they have the purpose of providing material for academic [[research]] and study, they are formatted approximately like printed journal articles, the metadata is entered into specialized databases, such as the [[Directory of Open Access Journals]] as well as the databases for the discipline, and they are predominantly available through [[academic library|academic libraries]] and special [[library|libraries]], generally at a fixed charge. \n\nElectronic articles can be found in [[online and offline|online]]-only journals (par excellence), but in the 21st century they have also become common as online versions of articles that also appear in printed journals. The practice of [[Electronic publishing|publishing of an electronic version]] of an article before it later appears in print is sometimes called '''epub ahead of print''', particularly in [[PubMed]].<ref>{{cite web |url=http://www.nlm.nih.gov/services/ldepubahead.html |title=FAQ: Loansome Doc Article Ordering Service - Epub Ahead of Print |work= |accessdate=2010-10-23}}</ref><ref>{{cite web |url=http://www.gwumc.edu/library/blog/client/index.cfm/2007/11/26/Epub-ahead-of-print-What-does-this-mean |title=Himmelfarb Library Blog: Epub ahead of print… What does this mean?? |format= |work= |archiveurl=https://web.archive.org/web/20100119081653/http://www.gwumc.edu/library/blog/client/index.cfm/2007/11/26/Epub-ahead-of-print-What-does-this-mean |archivedate=2010-01-19 |deadurl=yes }}</ref>\n\nThe term can also be used for the electronic versions of less formal publications, such as online archives, working paper archives from universities, government agencies, private and public think tanks and institutes and private websites. In many academic areas, specialized [[bibliographic database]]s are available to find their online content.\n\nMost commercial sites are [[subscription business model|subscription]]-based, or allow pay-per-view access. Many universities subscribe to electronic journals to provide access to their students and faculty, and it is generally also possible for individuals to subscribe. An increasing number of journals are now available with open access, requiring no subscription. Most working paper archives and articles on personal homepages are free, as are collections in [[institutional repository|institutional repositories]] and [[disciplinary repository|subject repositories]].\n\nThe most common formats of transmission are [[HTML]], [[Portable Document Format|PDF]] and, in specialized fields like mathematics and physics, [[TeX]] and [[PostScript]].\n\n==See also==\n* [[Academic publishing]]\n* [[Eprint]]\n* [[Electronic journal]]\n* [[Scholarly article]]\n\n== References ==\n{{reflist}}\n\n[[Category:Academic publishing]]\n[[Category:Electronic publishing]]\n[[Category:Electronic documents]]"]
['Category:Content management systems', '691651', "{{Category diffuse}}\n{{Cat main|Content management system}}\n{{Commons category}}\n\nA '''[[content management system]]''' ('''CMS''') is a system used to organize and facilitate collaborative content creation. Recently, the term has been used specifically to refer to programs on [[WWW]] [[Web server|servers]], but it can also refer to hardware devices that manage documents on a large network.\n\n[[Category:Web software]]\n[[Category:Internet Protocol based network software]]\n[[Category:Data management software]]\n[[Category:Office software]]\n[[Category:Electronic documents]]"]
['Email management', '21888954', '\'\'\'Email management\'\'\' is a specific field of [[communications management]] for managing high volumes of inbound electronic mail received by organizations. Today, email management is an essential component of customer service management.  Customer service call centers currently employ email response management agents along with telephone support agents, and typically use software solutions to manage emails.<ref>"Communications Management." Media and Organizational Biomimetics Initiative. mdg.mit.edu, 25 Oct 2011. Retrieved from web  [http://mdg.mit.edu/email-lab-interests.asp <http://mdg.mit.edu/email-lab-interests.asp>] on 15 Nov 2011</ref><ref>How to use e-mail to improve customer service. Inc.com, Guide E-mail Customer Service, Retrieved from web [http://www.inc.com/guides/cust_email/20909.html <http://www.inc.com/guides/cust_email/20909.html >] on 20 January 2012</ref>\n\n==Background==\nEmail management evolved from [[database management]] and [[customer relationship management]] (CRM).  Database management began in the 1960s. IBM provided one of the earliest solutions and established standards for database management.  Prominent database management platforms include Oracle, SQL Server etc.<ref>"Database Management - History Of Database Management." Free Encyclopedia of Ecommerce. Net Industries, n.d. Retrieved from Web. [http://ecommerce.hostip.info/pages/295/Database-Management-HISTORY-DATABASE-MANAGEMENT.html  <http://ecommerce.hostip.info/pages/295/Database-Management-HISTORY-DATABASE-MANAGEMENT.html>]. on 19 Dec 2011.</ref>  Vern Watts, inventor of [[IBM Information Management System|IBM\'s Information Management System]] (IMS), and [[Larry Ellison]], founder of [[Oracle Corporation]], are pioneers in database management systems.<ref>Luongo, C. et al. (2008). The tale of Vern Watts. [Web Video]. Retrieved from [www.youtube.com/watch?v=x98hgieE08o  <www.youtube.com/watch?v=x98hgieE08o>]. on 19 December 2011</ref><ref>"Larry Ellison Biography." Academy of Achievement. American Academy of Achievement, 16 Feb 2010. Web. 19 Dec 2011. <http://www.achievement.org/autodoc/page/ell0bio-1>.</ref>\n\nAs database management solutions became more sophisticated in functionality, marketing and customer service departments of large organizations started using information about customers for [[database marketing]].  Customer service managers soon realized that they could extend database marketing to store and retrieve all customer communications to improve visibility with key clients.  This led to the development of CRM systems which managed communication with customers and prospective customers using various media, including phone, direct mail, web site, and email.<ref>"The history of CRM -- evolving beyond the customer database." CRM Software Guide. crm-software-guide.com, n.d. Retrieved from Web. [http://www.crm-software-guide.com/history-of-crm.htm <http://www.crm-software-guide.com/history-of-crm.htm>]. on 19 Dec 2011.</ref>  Pioneers in CRM include [[David Duffield]], creator of [[PeopleSoft]], and [[Thomas Siebel|Tom Siebel]], founder of [[Siebel Systems]].<ref>"PeopleSoft Inc." International Directory of Company Histories. 2000. In Retrieved Encyclopedia.com Retrieved from web [http://www.encyclopedia.com/doc/1G2-2843700094.html  <http://www.encyclopedia.com/doc/1G2-2843700094.html>] on 19 December 2011</ref><ref>Thomas Siebel 1952- Biography - Early life and education, Oracle, Siebel systems. ND. Reference for Business Encyclopedia of Business, 2nd ed. Retrieved from web [http://www.referenceforbusiness.com/biography/S-Z/Siebel-Thomas-1952.html#b < http://www.referenceforbusiness.com/biography/S-Z/Siebel-Thomas-1952.html#b >] on 20 January 2012</ref>\n\nAs email became one of the most prevalent business-to-customer communication media in the 1990s, customer service departments needed specialized systems of tools and trained staff to manage email communication with their customers and prospective customers.\n\n==History==\nIn 1994, Information Cybernetics, a company in Cambridge, Massachusetts, developed tools for pattern analysis and categorization of emails and other electronic communication channels. The platform of tools was called [[EchoMail]].  The first company to adopt  EchoMail was [[AT&T]]. [[J. C. Penney|JC Penney]] adopted EchoMail in 1997.<ref>Callaway, Erin. "Return to Sender." PC Week Executive. 14 July 1997: 111, 114. Print.</ref><ref>O\'Brien, J. A. (2002). Introduction to information systems. (10 ed., p. 370). McGraw-Hill Irwin. Retrieved from Web. [http://www.mcm.edu/~palafoxt/sixth.htm <http://www.mcm.edu/~palafoxt/sixth.htm>]. On 8 Dec 2011</ref><ref>"The EchoMail Digital Refinery." www.echomail.com. EchoMail, Inc., n.d. Retrieved from Web [http://echomail.com/technology-for-email-management-detailed/ <http://echomail.com/technology-for-email-management-detailed/>]. on 8 Dec 2011</ref>\n\nAnother early company that developed email management software systems was FortÈ Internet Software, which produced Adante.<ref>Pavita, H. (1997, June 24). Forte introduces adante 1.0 server software for managing internet-based customer service and communications. Business Wire, Retrieved from Web [http://www.thefreelibrary.com/Forte introduces Adante 1.0 server software for managing..-a019535024 <http://www.thefreelibrary.com/Forte introduces Adante 1.0 server software for managing..-a019535024>] on 8 Dec 2011</ref>  By late 1999, companies such as KANA Software, Inc., also emerged to support this effort.<ref>"Email Response System - Intelligent Message Handling :: KANA." www.kana.com. KANA Software, n.d. Retrieved from Web. [http://www.kana.com/customer-service/email-response-system.php  <http://www.kana.com/customer-service/email-response-system.php>]. on 8 Dec 2011</ref>  Eventually, companies such as Siebel CRM Systems, Inc., incorporated components of email management into their CRM systems.<ref>"Bookshelf v7.5: Overview of Siebel eMail Response." docs.oracle.com. ORACLE Corporation, 21 April 2003. Retrieved from Web. [http://docs.oracle.com/cd/E05554_01/books/eMail/eMailOverview.html  <http://docs.oracle.com/cd/E05554_01/books/eMail/eMailOverview.html>]. on 8 Dec 2011</ref>\n\n==Typical system components==\nAn email management system consists of various components to handle different phases of the email management process.<ref>"EMAIL Management." Media and Organizational Biomimetics Initiative. mdg.mit.edu, 25 Oct 2011. Retrieved from web  [http://mdg.mit.edu/email-lab-interests.asp <http://mdg.mit.edu/email-lab-interests.asp>] on 15 Nov 2011</ref>  These components include: \n*Email ticketing system - One of the key tasks performed by email [[management system]]s is to allocate reference numbers to all incoming [[email]]s. This process is known as ticketing. All subsequent emails relating to one matter can then be grouped under the same reference. This allows users to track their correspondence in a more time effective and productive way.\n*Email receipt module - Receives emails, filters out spam and unwanted content to a separate queue (sometimes called [[email filtering]]), and assigns unique ticket numbers based on certain conditions.\n*Bayesian spam filters - Statistical technique of filtering spam that most current email management systems utilize.\n*Data enhancement module - Adds tags to each email for further processing and may include the ability to connect to remote databases and retrieve specific information about the email author and his/her transactions with the organization.\n*Intelligent Analysis module - Reads the subject, message, and attachments, and any tags added by the data enhancement module, analyzing its content in an attempt to understand the subject matter of the email.  This module may store this \'intelligence\' as additional tags.\n\n==References==\n{{reflist|30em}}\n\n{{DEFAULTSORT:E-Mail Ticketing System}}\n[[Category:Email]]\n[[Category:Communication software]]\n[[Category:Electronic documents]]\n[[Category:Records management]]']
['Aperture card', '8403499', "[[Image:Aperture card.JPG|400px|right]]\nAn '''aperture card''' is a type of '''[[punched card]]''' with a cut-out window into which a chip of '''[[microform|microfilm]]''' is mounted.  Such a card is used for [[archive|archiving]] or for making multiple inexpensive copies of a document for ease of distribution.  The card is typically punched with machine-readable [[metadata]] associated with the microfilm image, and printed across the top of the card for visual identification.  The microfilm chip is most commonly 35mm in height, and contains an [[optics|optically reduced]] image, usually of some type of reference document, such as an [[engineering drawing]], that is the focus of the archiving process.  Aperture cards have several advantages and disadvantages when compared to digital systems.  Machinery exists to automatically store, retrieve, sort, duplicate, create, and digitize cards with a high level of automation.  While many aperture cards still play an important role in archiving, their role is gradually being replaced by digital systems.\n\n== Usage ==\nAperture cards are used for engineering drawings from all engineering disciplines.  The [[U.S. Department of Defense]] once made extensive use of aperture cards, and some are still in use, but most data is now digital.<ref>[https://web.archive.org/web/20060530111716id_/http://federalvoice.dscc.dla.mil/federalvoice/030924/tech.html Federal use of aperture cards (Archived Copy)]</ref>\n\nInformation about the drawing, for example the drawing number, could be both punched and printed on the remainder of the card.  With the proper machinery, this allows for automated handling.  In the absence of such machinery, the cards can still be read by a human with a lens and a light source.\n\n=== Advantages ===\nAperture cards have, for archival purposes, some advantages over digital systems.  They have a 500-year lifetime, they are human readable, and there is no expense or risk in converting from one digital format to the next when computer systems become obsolete.<ref>{{cite journal|first=Ed |last=LoTurco |title=The Engineering Aperture Card: Still Active, Still Vital |publisher=EDM Consultants |date=January 2004 |url=http://www.aiimne.org/library/LoTurcoWhitePaper1.pdf |accessdate=October 10, 2007 |archiveurl=https://web.archive.org/web/20071128162738/http://www.aiimne.org/library/LoTurcoWhitePaper1.pdf |archivedate=November 28, 2007 |deadurl=no |df= }}</ref>\n\n=== Disadvantages ===\n{{unreferenced section|date=February 2015}}\nMost of the disadvantages are related to the well established differences in analog and digital technology. In particular, searching for given strings within content is considerably slower.  Handling physical cards requires proprietary machinery and processing optical film takes significant time.\n\nThe very nature of microfilm cameras and the high contrast properties of microfilm stock itself also impose limits on the amount of detail that can be resolved particularly at the higher reduction ratios (36x or greater) needed to film larger drawings. Faded drawings or those of low or uneven contrast do not reproduce well and significant detail or annotations may be lost.\n\nIn common with other forms of microfilm mis-filing cards after use, particularly in large archives, results in the card being for all intents and purposes lost forever unless it's later found by accident.\n\nAperture cards created from 35mm roll film mounted on to blank cards have to be treated with great care. Bending the card can cause the film to detach and excessive pressure to a stack of cards can cause the mounting glue to ooze creating clumps of cards which will feed through duplicators and other machinery either poorly or not at all. Feeding a de-laminated card through machinery not only risks destroying the image it also risks jamming or damaging the machinery.\n\n== Machinery ==\nA set of cards could be rapidly sorted by drawing number or other punched data using a [[IBM 80 series Card Sorters|card sorter]].  Machines are now available that [[Image scanner|scan]] aperture cards and produce a digital version.<ref>For example, this aperture card scanner from  [http://www.oceusa.com/main/product_detail.jsp?FOLDER%3C%3Efolder_id=1408474395186237&PRODUCT%3C%3Eprd_id=845524441761057 Oce']</ref>  Aperture card plotters are machines that use a laser to create the image on the film.<ref>For example, this aperture card plotter from [http://www.wwl.co.uk/apertureplotters.htm Wicks & Wilson] {{webarchive |url=https://web.archive.org/web/20060627060408/http://www.wwl.co.uk/apertureplotters.htm |date=June 27, 2006 }}</ref>\n\n== Conversion ==\nAperture cards can be converted to digital documents using scanning equipment and software. The scan software we use allows for significant image cleanup and enhancement. Often, the digital image produced is better than the visual quality available prescan. A variety of output image types can be generated, most notably, Group 4 TIFF and PDF.<ref>{{cite web|last1=Bryant|first1=Joe|title=Aperture Card Scanning|url=http://www.microcomseattle.com/solutions/document-scanning/aperture-card/|website=Micro Com Seattle|accessdate=17 March 2015}}</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://stinet.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=AD0232960 1959 Defense Technical Information Center report] on the technology and its use for submitting engineering plans to the military.\n* [http://www.wipo.int/export/sites/www/scit/en/standards/pdf/03-07-a.pdf Detailed description of a particular format]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }} of Aperture cards from [[WIPO]].\n* [https://web.archive.org/web/20090327011959/http://www.green-sheet.net/tutorial2.6.htm Detailed information regarding duplicating microforms and aperture cards] (select and highlight to read black on black text)\n\n[[Category:Archival science]]\n[[Category:History of computing]]\n[[Category:Infographics]]\n[[Category:Technical drawing]]\n[[Category:Electronic documents]]"]
['Category:Data security', '4842680', '{{Commons category|Computer security}}\n{{Portal|Computer security}}\n{{Cat main|Data security}}\n\n[[Category:Computer security]]\n[[Category:Computer data|Security]]\n[[Category:Electronic documents]]\n[[Category:National security]]\n[[Category:Information technology management]]\n\n[[fi:Luokka:Tietoturva]]']
['Lettrs', '39130775', '{{lowercase title}}\n{{Infobox software\n| name         = lettrs\n| logo         = [[File:Lettrs logo Square.jpg|200px]]\n| released     = {{Start date|2012}}\n| developer   = [[Drew Bartkiewicz]] (CEO)\n| status             = Active\n| operating system   = [[IOS|Apple iOS]], [[Android (operating system)|Android]]\n| genre              = [[Social Networking]]\n| website  =  {{URL|http://www.lettrs.com}}\n}}\n\n\'\'\'lettrs\'\'\' is a global [[mobile app|mobile application]] and [[social network]] that allows users to compose and send mobile messages privately or publicly.<ref name=crunch>{{cite web |url=https://www.crunchbase.com/organization/lett-rs |title= lettrs |author=<!--Staff writer(s); no by-line.--> |website=CrunchBase.com |publisher=AOL Inc |accessdate=26 January 2015}}</ref><ref name=bweek>{{cite web |url=http://www.businessweek.com/articles/2014-01-30/youve-sent-mail-a-letter-writing-app-forces-users-to-slow-down |title=You\'ve Sent Mail: A Letter-Writing App Forces Users to Slow Down |last1=Leonard |first1=Devin |date=30 January 2014 |website=BusinessWeek.com |publisher=Bloomberg LP |accessdate=26 January 2015}}</ref><ref name=think>{{cite web |url=http://customerthink.com/lettrs_launches_platform_to_organize_and_deliver_the_world_s_letters_in_the_cloud/ |title=lettrs Launches Platform to Organize and Deliver the World’s Letters in the Cloud |author=<!--Staff writer(s); no by-line.--> |date=21 December 2012 |website=CustomerThink.com |publisher=Customer Think Corporation |accessdate=26 January 2015}}</ref> The lettrs app converts mobile voice, data and pictures to digital personal and public messages via its text and mobile delivery inventions.<ref name=mash1>{{cite web |url=http://mashable.com/2013/04/23/lettrs-mobile-app/ |title=Lettrs App Lets You Send Snail Mail From Your iPhone |last1=Petronzio |first1=Matt |date=23 April 2013 |website=Mashable.com |publisher=Mashable Inc |accessdate=26 January 2015}}</ref>\n\nlettrs is headquartered in [[New York City]] and [[Drew Bartkiewicz]] is the company’s CEO and co-founder.<ref name=crunch></ref> In 2015, [[Mark Jung]] was named the company [[Chairman]].<ref name=trutower2>{{cite web |url=http://www.trutower.com/2015/05/06/lettrs-chat-app-chairman-woman-note-23902/ |title=lettrs Messaging App Announces New Chairman and Launch of "Woman Of Note" Collection |last1=Nay |first1=Josh Robert |date=6 May 2015 |website=trutower.com |publisher=TruTower |access-date=3 July 2015}}</ref><ref name=techco>{{cite web |url=http://tech.co/lettrs-women-of-note-2015-05 |title=lettrs: Bringing Hand Written Notes to the Digital World |last1=Schmidt |first1=Will |date=6 May 2015 |website=tech.co |publisher=TechCo |access-date=3 July 2015}}</ref> lettrs has a global user base in 174 companies,<ref name=mobile>{{cite web |url=http://www.adweek.com/socialtimes/messaging-app-lettrs-launches-socialstamps-advertisers-charities/554471?red=im |title=Messaging App Lettrs Launches SocialStamps for Advertisers, Charities |last1=Shaul |first1=Brandy |date=9 December 2014 |website=Social Times |publisher=Prometheus Global Media |accessdate=26 January 2015}}</ref> over 1 million downloads and has been featured in several media outlets, including [[The Wall Street Journal]], [[CBS]] and [[NPR]].<ref name=crunch/><ref name=postal>{{cite web |url=http://postalvision2020.com/postalvision-2020-3-0/speakers-3-0-conference/drew-bartkiewicz/ |title=Drew Bartkiewicz |author=<!--Staff writer(s); no by-line.--> |website=PostalVision2020.com |publisher=Ursa Major Associates, LLC |accessdate=26 January 2015}}</ref>\n\n==History==\nlettrs was established in 2012 by technology entrepreneur [[Drew Bartkiewicz]].<ref name=bweek/><ref name=think/><ref name=postal/> Bartkiewicz came up with the idea for the company in 2008<ref name=bweek/> after being inspired by his grandmother’s letter writing<ref name=yahoo>{{cite web |url=https://news.yahoo.com/lettrs-brings-snail-mail-back-future-230223676.html |title=Lettrs Brings Snail Mail Back to The Future |last1=Van Paris |first1=Calin |date=19 June 2012 |website=Yahoo News |publisher=Mashable Inc |accessdate=26 January 2015}}</ref><ref name=mash2>{{cite web |url=http://mashable.com/2012/06/19/lettrs/ |title=Lettrs Brings Snail Mail Back to The Future |last1=Van Paris |first1=Calin |date=19 June 2012 |website=Nashable |publisher=Mashable Inc |accessdate=26 January 2015}}</ref> and his own experiences during his service in the military.<ref name=bweek/><ref name=think/><ref name=parcel>{{cite web |url=http://postandparcel.info/54661/in-depth/innovation-in-depth/postalvision-sets-sights-on-congress-and-younger-americans/ |title=PostalVision sets sights on Congress and younger Americans |author=<!--Staff writer(s); no by-line.--> |date=27 March 2013 |website=PostandParcel.info |publisher=Post & Parcel |accessdate=26 January 2015}}</ref> lettrs was officially established the summer of 2012 with the help of his wife, Araceli Bartkiewicz, and children,<ref name=postal/> though it was not launched as a global platform from its [[Software release life cycle#Beta|beta]] phase until December 2012.<ref name=think/><ref name=nextweb>{{cite web |url=http://thenextweb.com/apps/2013/04/23/word-up-lettrs-launches-on-ios/ |title=TNW Pick of the Day: Lettrs turns your iPhone into a personal writing desk, transcriber and post office |last1=Sawers |first1=Paul |date=23 April 2013 |website=The Next Web |publisher=The Next Web, Inc |accessdate=26 January 2015}}</ref>\n\nBartkiewicz introduced the lettrs mobile application at the PostalVision 2020/3.0 conference in [[Washington, D.C.]] in 2013.<ref name=postal/><ref name=mash1/><ref name=nextweb/> The Android version was released in July 2014,<ref name=android>{{cite web |url=http://www.androidcentral.com/lettrs-app-comes-android-more-personal-messages |title=The lettrs app comes to Android for more personal messages |last1=Callaham |first1=John |date=14 June 2014 |website=AndroidCentral.com |publisher=Mobile Nations |accessdate=26 January 2015}}</ref> followed by a re-release of the iOS app in October.<ref name=apple>{{cite web |url=http://www.prweb.com/releases/2014/10/prweb12261924.htm |title=lettrs Raises $1.5M, Releases First Native iPad and Popular New iPhone App |author=<!--Staff writer(s); no by-line.--> |date=21 October 2014 |website=PRWeb.com |publisher=Vocus PRW Holdings, LLC |accessdate=26 January 2015}}</ref>\n\n==Features==\nlettrs provides a [[mobile phone|mobile]] platform for customers to create and deliver mobile letters in 80 translated languages with a selection of writing themes, proprietary “SocialStamps” and styles.<ref name=think/><ref name=mnn>{{cite web |url=http://www.mnn.com/lifestyle/responsible-living/blogs/miss-sending-letters-try-lettrs |title=Miss sending letters? Try lettrs! |last1=Vartan |first1=Starre |date=8 February 2013 |website=Mother Nature Network |publisher=MNN Holding Company, LLC |accessdate=26 January 2015}}</ref> It facilitates both private messaging and public posting of signed, translated and networked mobile-to-mobile letters.<ref name=crunch/>\n\nThe signature service of lettrs is the translation of letter messages in real time complete with original user signatures and selectable SocialStamps. The lettrs mobile network is able to translate an original digital letter in up to 80 languages. Users may also share open letters and the lettrs stamps across other major social networks.<ref name=bweek/><ref name=mash1/><ref name=yahoo/><ref name=parcel/>\n\nIn December 2014 the company introduced a feature named SocialStamps that allows users to add a customized stamp to a letter. At the feature’s launch, the company offered 47 different stamps with plans to issue new stamps monthly. As part of the release the lettrs 2014 Persons of Note stamps on the lettrs network featured [[Michelle Phan]], [[Narendra Modi]], [[Bob Woodruff]] of [[ABC News]] and [[Stanley A. McChrystal]].<ref name=mobile/><ref name=stamps>{{cite web |url=http://venturebeat.com/2014/12/09/lettrs-com-calls-postage-stamps-into-social-duty-for-its-old-style-letters/ |title=Lettrs calls postage stamps into social duty for its old-style letters |last1=Levine |first1=Barry |date=9 December 2014 |website=VentureBeat.com |publisher=VentureBeat |accessdate=26 January 2015}}</ref><ref name=marketer>{{cite web |url=http://www.mobilemarketer.com/cms/news/social-networks/19316.html |title=United Way of New York City leverages lettrs\' SocialStamps for fundraising |last1=Samuely |first1=Alex |date=9 December 2014 |website=MobileMarketer.com |publisher=Napean LLC |accessdate=26 January 2015}}</ref>\n\nUsers can share letters and the SocialStamps via [[Facebook]] and [[Twitter]].<ref name=think/><ref name=nextweb/> lettrs also integrates with [[Google+]] and [[Instagram]] so that users may broaden the distribution of their letters beyond the mobile app.<ref name=bweek/> Users can also pen open public letters or petitions for supporting causes, persons, or brands.<ref name=think/><ref name=nextweb/>\n\nlettrs conducted its first Hollywood movie integration in April 2015 with Relativity Media. The company released stamps featuring images from the movie \'\'[[Desert Dancer]]\'\'.<ref name=trutower/> In May 2015, lettrs released the "Women of Note" stamp collection. It featured 12 notable women including [[Michelle Obama]], [[Queen Rania of Jordan]], [[Shakira]], [[Michelle Bachelet]], [[Laura Bush]], [[Sonia Gandhi]], [[Ellen DeGeneres]] and [[Angelina Jolie]].<ref name=trutower2/><ref name=techco/>\n\n==Recognition and partnerships==\nIn 2014, Google selected lettrs as one of the Best Android Apps of the year.<ref name=time>{{cite web |url=http://time.com/3611709/best-android-google-play-apps-2014/ |title=Google Says These Are 2014\'s Best Android Apps |last1=Luckerson |first1=Victor |date=1 December 2014 |website=Time.com |publisher=Time Inc |accessdate=1 February 2015}}</ref><ref name=techco/>\n\nlettrs has worked with the [[United Service Organizations|USO]], [[Aspen Institute]], and the [[United Way]].<ref name=trutower2/> In 2014, the company published the first digitally sourced book of letters, \'\'Poetguese\'\'. The book contains a foreword by author [[Paulo Coelho]] with all proceeds donated to charity.<ref name=broadway>{{cite web |url=http://www.broadwayworld.com/bwwbooks/article/Lettrs-Announces-POETGUESE-20141008 |title=lettrs Announces Poetguese |author=<!--Staff writer(s); no by-line.--> |date=8 October 2014 |website=broadwayworld.com |publisher=Wisdom Digital Media |access-date=3 July 2015}}</ref>\n\nlettrs established lettrs Foundation, an organization that partners with schools and non-profits to improve literacy through social networking, including partnerships with the United Way and Aspen Institute.<ref name=trutower>{{cite web |url=http://www.trutower.com/2014/07/14/lettrs-letter-writing-messaging-app-launch-on-android/ |title=lettrs Platform Launches on Android, Bringing Handwritten Letters Back to the Mainstream |last1=Nay |first1=Josh Robert |date=14 July 2014 |website=TruTower.com |publisher=TruTower |accessdate=1 February 2015}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n*{{Official site|http://about.lettrs.com}}\n*[http://www.lettrsfoundation.org lettrs Foundation]\n\n[[Category:Mobile social software]]\n[[Category:Postal system]]\n[[Category:Postal services]]\n[[Category:Internet terminology]]\n[[Category:Electronic documents]]']
['DjVu', '610868', '{{About|a computer file format|a computer-assisted translation software tool|Déjà Vu (software)}}\n\n{{ infobox file format\n| icon = Djvu icon.svg\n| logo = [[File:DjVu-logo.svg|frameless]]\n| screenshot =\n| caption =\n| extension = .djvu, .djv\n| mime = image/vnd.djvu, image/x-djvu\n| type code = DJVU\n| uniform type =\n| magic =\n| owner = [[AT&T Labs|AT&T Labs – Research]]\n| released = {{start date and age|1998}}\n| latest release version = Version 26<ref name=djvuvers>[http://www.djvu.org/forum/phpbb/viewtopic.php?p=952&sid=33819a3f9de6fe1db7870159f47c4dd5 DjVu File Format Version], By Jim Rile, Posted: Fri Feb 23, 2007 1:08 am, PlanetDjVu</ref>\n| latest release date = {{start date and age|2006|06}}\n| genre = [[Image file formats]]\n| container for =\n| contained by =\n| extended from =\n| extended to =\n| standard =\n| free = GNU GPLv2 for DjVu Reference Library and DjVuLibre-3.5;<br>License grants under the GNU GPL for several patents that cover aspects of the library<ref>{{cite web |title=DjVu Licensing |work=DjVu Sourceforge page |publisher=Sourceforge.net |date=2011-08-17 |url=http://djvu.sourceforge.net/licensing.html |accessdate=2011-09-21}}</ref>\n| url = {{url|http://www.djvu.org/}}\n}}\n\'\'\'DjVu\'\'\' ({{IPAc-en|ˌ|d|eɪ|ʒ|ɑː|ˈ|v|uː}} {{respell|DAY|zhah|VOO|\'}},<ref name="DjVuOverview">{{Cite web|url=http://www.cuminas.jp/en/technology/?src=technology_djvu.aspx|title=DjVu Technology|publisher=Cuminas|accessdate=2014-02-12}}</ref> like {{lang-fr|[[déjà vu]]}} {{IPA-fr|deʒavy|}}) is a [[computer]] [[file format]] designed primarily to store [[image scanner|scanned documents]], especially those containing a combination of text, line drawings, indexed color images, and photographs. It uses technologies such as image layer separation of text and background/images, [[Interlacing (bitmaps)|progressive loading]], [[arithmetic coding]], and [[lossy compression]] for bitonal ([[monochrome]]) images. This allows high-quality, readable images to be stored in a minimum of space, so that they can be made available on the [[World Wide Web|web]].\n\nDjVu has been promoted as an alternative to [[Portable Document Format|PDF]], promising smaller files than PDF for most scanned documents.<ref name="DjVu">{{Cite web|url=http://djvu.org/resources/whatisdjvu.php|title=What is DjVu – DjVu.org|publisher=DjVu.org|accessdate=2009-03-05}}</ref> The DjVu developers report that color magazine pages compress to 40–70&nbsp;kB, black-and-white technical papers compress to 15–40&nbsp;kB, and ancient manuscripts compress to around 100&nbsp;kB; a satisfactory [[JPEG]] image typically requires 500&nbsp;kB.<ref name=djvupaper>{{cite journal |author1=Léon Bottou |author2=Patrick Haffner |author3=Paul G. Howard |author4=Patrice Simard |author5=Yoshua Bengio |author6=Yann Le Cun |title=High Quality Document Image Compression with DjVu, 7(3):410–425|publisher=Journal of Electronic Imaging |year=1998 |url=http://leon.bottou.org/publications/pdf/jei-1998.pdf}}</ref> Like PDF, DjVu can contain an [[Optical character recognition|OCR]] text layer, making it easy to perform [[copy and paste]] and text search operations.\n\nFree browser plug-ins and desktop viewers from different developers are available from the djvu.org website. DjVu is supported by a number of multi-format document viewers and e-book reader software on Linux ([[Okular]], [[Evince]]) and Windows ([[SumatraPDF]]).\n\n== History ==\nThe DjVu technology was originally developed<ref name=djvupaper/> by [[Yann LeCun]], [[Léon Bottou]], Patrick Haffner, and Paul G. Howard at [[AT&T Labs]] from 1996 to 2001.\n\nDue to its declared higher compression ratio (and thus smaller file size) and the ease of converting large volumes of text into DjVu format, and because it is an [[open file format]], some independent technologists (such as [[Brewster Kahle]]<ref name="Kahle2005">{{Cite web|url=http://itc.conversationsnetwork.org/shows/detail400.html|author=Brewster Kahle|title=Universal Access to All Knowledge|format=Audio; Speech at 1h:31m:20s|publisher=Conversations Network|date=December 16, 2004}}</ref>) have historically considered it superior to [[PDF]].\n\nThe DjVu library distributed as part of the open-source package \'\'DjVuLibre\'\' has become the reference implementation for the DjVu format. DjVuLibre has been maintained and updated by the original developers of DjVu since 2002.<ref>http://djvu.sourceforge.net/</ref>\n\nThe DjVu file format specification has gone through a number of revisions:\n\n{| class="wikitable sortable"\n|+ Revision history\n! Support status\n! Version\n! Release date\n! Notes\n|-\n| Unsupported\n| 1–19<ref name=djvuvers />\n| 1996–1999\n| Developmental versions by AT&T labs preceding the sale of the format to [[LizardTech]].\n|-\n| Unsupported\n| Version 20<ref name=djvuvers /><!--keep the word \'\'version\'\' as it is part of the name-->\n| April 1999\n| DjVu version 3. DjVu changed from a single-page format to a multipage format.\n|-\n| Older, still supported\n| Version 21<ref name=djvuvers />\n| September 1999\n| Indirect storage format replaced. The searchable text layer was added.\n|-\n| Older, still supported\n| Version 22<ref name=djvuvers />\n| April 2001\n| Page orientation, color JB2\n|-\n| Unsupported\n| Version 23<ref name=djvuvers />\n| July 2002\n| CID chunk\n|-\n| Unsupported\n| Version 24<ref name=djvuvers />\n| February 2003\n| LTAnno chunk\n|-\n| Older, still supported\n| Version 25<ref name=djvuvers />\n| May 2003\n| NAVM chunk. Support for DjVu bookmarks (outlines) was added. Changes made by Versions 23 and 24 were made obsolete.\n|-\n| Current\n| Version 26<ref name=djvuvers />\n| April 2005\n| Text/line annotations\n|-\n|}\n\n== Technical overview ==\n\n=== File structure ===\nThe DjVu file format is based on the [[Interchange File Format]] and is composed of hierarchically organized chunks. The IFF structure is preceded by a 4-byte <code>AT&T</code> [[magic number (programming)|magic number]]. Following is a single <code>FORM</code> chunk with a secondary identifier of either <code>DJVU</code> or <code>DJVM</code> for a single-page or a multi-page document, respectively.\n\n==== Chunk types ====\n{| class="wikitable"\n|-\n! Chunk identifier !! Contained by !! Description\n|-\n| FORM:DJVU || FORM:DJVM || Describes a single page. Can either be at the root of a document and be a single-page document or referred to from a <code>DIRM</code> chunk. \n|-\n| FORM:DJVM || {{n/a}} || Describes a multi-page document. Is the document\'s root chunk.\n|-\n| FORM:DJVI || FORM:DJVM || Contains data shared by multiple pages.\n|-\n| FORM:THUM || FORM:DJVM || Contains thumbnails.\n|-\n| INFO || FORM:DJVU || Must be the first chunk. Describes the page width, height, format version, DPI, gamma, and rotation.\n|-\n| DIRM || FORM:DJVM || Must be the first chunk. References other <code>FORM</code> chunks. These chunks can either follow this chunk inside the <code>FORM:DJVM</code> chunk or be contained in external files. These types of documents are referred to as \'\'bundled\'\' or \'\'indirect\'\', respectively.\n|-\n| NAVM || FORM:DJVM || If present, must immediately follow the <code>DIRM</code> chunk. Contains a BZZ-compressed outline of the document.\n|}\n\n=== Compression ===\nDjVu divides a single image into many different images, then compresses them separately. To create a DjVu file, the initial image is first separated into three images: a background image, a foreground image, and a mask image. The background and foreground images are typically lower-resolution color images (e.g., 100 dpi); the mask image is a high-resolution bilevel image (e.g., 300 dpi) and is typically where the text is stored. The background and foreground images are then compressed using a [[Wavelet compression#Wavelet compression|wavelet-based compression]] algorithm named IW44.<ref name=djvupaper/> The mask image is compressed using a method called JB2 (similar to [[JBIG2]]). The JB2 encoding method identifies nearly identical shapes on the page, such as multiple occurrences of a particular character in a given font, style, and size. It compresses the bitmap of each unique shape separately, and then encodes the locations where each shape appears on the page. Thus, instead of compressing a letter "e" in a given font multiple times, it compresses the letter "e" once (as a compressed bit image) and then records every place on the page it occurs.\n\nOptionally, these shapes may be mapped to [[UTF-8]] codes (either by hand or potentially by a [[Text recognition|text recognition system]]), and stored in the DjVu file. If this mapping exists, it is possible to select and copy text.\n\nSince JBIG2 was based on JB2, both compression methods have the same problems when performing lossy compression.  Numbers may be substituted with similar looking numbers (such as replacing 6 with 8) if the text was scanned at a low DPI prior to lossy compression.\n\n== Format licensing ==\nDjVu is an [[open file format]] with patents.<ref name="DjVu"/> The file format specification is published, as well as source code for the reference library.<ref name="DjVu"/> The original authors distribute an [[Open-source software|open-source]] implementation named "\'\'DjVuLibre\'\'" under the [[GNU General Public License]]. The rights to the commercial development of the encoding software have been transferred to different companies over the years, including [[AT&T Corporation]], [[LizardTech]], \'\'Celartem\'\' and \'\'Cuminas\'\'.\n\n==Support==\n[[SumatraPDF]] (Windows) among others can manipulate DjVu files.\n\nIn 2002, the DjVu file format was chosen by the [[Internet Archive]] as a format in which its \'\'[[Million Book Project]]\'\' provides scanned [[public domain]] books online (along with [[TIFF]] and PDF).<ref>{{Cite web|url=http://wiki.laptop.org/go/DJVU |title=Image file formats – OLPC |publisher=Wiki.laptop.org |date= |accessdate=2008-09-09}}</ref>\n\n[[Wikimedia Commons]], a media repository used by [[Wikipedia]] among others, conditionally permits PDF and DjVu media files.<ref>[[commons:Commons:Scope#PDF and DjVu formats|PDF and DjVu]]</ref>\n\n== See also ==\n* [[JBIG2]]\n* [[Comparison of e-book formats]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n{{Commons category|DjVu}}\n* [http://djvu.org/ "The premier menu for DjVu resources"] (status of the site, which is maintained by an anonymous webmaster, is unclear)\n* [http://djvu.sourceforge.net/ DjVuLibre site]\n* [http://jwilk.net/software/ Jakub Wilk\'s pdf2djvu and other DjVu tools]\n* [https://bitbucket.org/jsbien/ndt/wiki/wyniki Poliqarp for DjVu search engine and other DjVu tools]\n* [http://djvu.org/forum/phpbb/viewtopic.php?t=146 Why won\'t Google index DjVu files after all this time?] – topic on  PlanetDjVu\n* [http://any2djvu.djvuzone.org Any2Djvu Server - online document converter]\n* [https://www.cuminas.jp/en/downloads Cuminas Software Downloads]\n* [http://www.djvu-soft.narod.ru/soft/ Table of Djvu Programmes (Russian)]\n\n{{Office document file formats}}\n{{Graphics file formats}}\n\n[[Category:1998 introductions]]\n[[Category:Computer file formats]]\n[[Category:Electronic documents]]\n[[Category:Electronic publishing]]\n[[Category:Filename extensions]]\n[[Category:Graphics file formats]]\n[[Category:Office document file formats]]\n[[Category:Open formats]]']
['Category:Electronic lab notebook', '49921430', '{{catmain}}\n\n[[Category:Electronic documents]]\n[[Category:Data management software]]\n[[Category:Science software]]\n[[Category:Scientific documents]]']
['E-receipt', '40545818', "An '''E-receipt''' is an electronic [[receipt]] of any goods/services that have been purchased, opposed to a paper receipt. They are usually sent via [[email]] to avoid wasting [[paper]] and for marketing purposes.<ref>{{cite news |last=Perring |first=Rebecca |url=http://www.express.co.uk/news/uk/639219/Ereceipts-British-shops-shopping-electronic-Internet |title=Retailers are now monitoring YOUR shopping habits and transactions with the eReceipt... |work=[[Daily Express#Sunday Express]] |date=2016-01-29 |accessdate=2016-12-11 }}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Electronic documents]]\n[[Category:Accounting source documents]]\n\n{{retailing-stub}}"]
['Science Citation Index', '6852678', '{{incomplete|date=January 2014}}\n{{ infobox bibliographic database\n| title = Science Citation Index\n| image = \n| caption = \n| producer = [[Thomson Reuters]]\n| country = United States\n| history = 2000-present\n| languages = \n| providers = \n| cost = \n| disciplines = Science, medicine, and technology\n| depth = \n| formats = \n| temporal = \n| geospatial = \n| number = \n\n| updates = \n| p_title = \n| p_dates = \n| ISSN = 0036-827X\n| web = http://thomsonreuters.com/science-citation-index-expanded/\n| titles = \n}}\nThe \'\'\'Science Citation Index\'\'\' (\'\'\'SCI\'\'\') is a [[citation index]] originally produced by the [[Institute for Scientific Information]] (ISI) and created by [[Eugene Garfield]]. It was officially launched in 1964. It is now owned by [[Clarivate Analytics]] (previously the Intellectual Property and Science business of [[Thomson Reuters]]).<ref name=dimension>\n{{cite journal\n|doi=10.1126/science.122.3159.108\n|title=Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas\n|url=http://ije.oxfordjournals.org/content/35/5/1123.full\n|format=Free web article download\n|year=1955\n|last1=Garfield\n|first1=E.\n|journal=Science\n|volume=122\n|issue=3159\n|pages=108–11\n|pmid=14385826|bibcode=1955Sci...122..108G\n}}</ref><ref name=evolve>\n{{cite journal\n |last = Garfield \n |first = Eugene\n |doi=10.2436/20.1501.01.10\n |url=http://garfield.library.upenn.edu/papers/barcelona2007a.pdf\n |format=Free PDF download\n |title=The evolution of the Science Citation Index|doi-broken-date = 2017-01-16\n }} International microbiology \'\'\'10.\'\'\'1 (2010): 65-69.</ref><ref name=gOverview>\n{{cite web\n | last = Garfield \n | first = Eugene\n | authorlink =\n | coauthors =\n | title = Science Citation Index\n | work = Science Citation Index 1961\n | publisher = Garfield Library - UPenn\n | date = 1963\n | url = http://garfield.library.upenn.edu/papers/80.pdf\n | format = Free PDF download\n | doi =\n | accessdate = 2013-05-27}} \n* Originally published by the Institute of Scientific Information in 1964\n* Other titles in this document are: What is a Citation Index? , How is the Citation Index Prepared? , How is the Citation Index Used? , Applications of the Science Citation Index , Source Coverage and Statistics , and a Glossary.</ref><ref name=history-cite-indexing>\n{{cite web\n | title =History of Citation Indexing \n | work =Needs of researchers create demand for citation indexing \n | publisher =Thomson Ruters \n | date =November 2010 \n | url =http://thomsonreuters.com/products_services/science/free/essays/history_of_citation_indexing/ \n | format =Free HTML download \n | accessdate =2010-11-04}}</ref> The larger version (\'\'\'Science Citation Index Expanded\'\'\') covers more than 8,500 notable and significant [[Scientific journal|journals]], across 150 disciplines, from 1900 to the present. These are alternatively described as the world\'s leading journals of [[science]] and [[technology]], because of a rigorous selection process.<ref name=Expanded>\n{{cite web \n|url=https://www.thomsonreuters.com/en/products-services/scholarly-scientific-research/scholarly-search-and-discovery/science-citation-index-expanded.html \n|title=Science Citation Index Expanded \n|work= |accessdate=2017-01-17}}</ref><ref name=wetland>{{cite journal| doi= 10.1007/s12665-012-2193-y|title= The Top-cited Wetland Articles in Science Citation Index Expanded: characteristics and hotspots|url=http://dns2.asia.edu.tw/~ysho/YSHO-English/Publications/PDF/Env%20Ear%20Sci-Ma.pdf|date= December 2012| last1= Ma| first1= Jiupeng| last2= Fu| first2= Hui-Zhen| last3= Ho| first3= Yuh-Shan| journal= Environmental Earth Sciences|volume= 70|issue= 3|pages= 1039}} (Springer-Verlag)</ref><ref name=shan>\n{{cite journal \n| doi= 10.1007/s11192-012-0837-z \n|title= The top-cited research works in the Science Citation Index Expanded \n|url= http://trend.asia.edu.tw/Publications/PDF/Scientometrics94,%201297.pdf \n| year= 2012 \n| last1= Ho \n| first1= Yuh-Shan \n| journal= Scientometrics \n| volume= 94 \n| issue= 3 \n| page= 1297}}</ref>\n\nThe index is made available online through different platforms, such as the [[Web of Science]]<ref name=AtoZ>{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2010 |url=http://wokinfo.com/products_tools/products/ |format=Choose databases on method of discovery and analysis |accessdate=2010-06-24}}</ref><ref>[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]</ref> and SciSearch.<ref>{{cite web |url=http://library.dialog.com/bluesheets/html/bl0034.html |title=SCISEARCH - A CITED REFERENCE SCIENCE DATABASE |publisher=Library.dialog.com |date= |accessdate=2014-04-17}}</ref> (There are also CD and printed editions, covering a smaller number of journals). This database allows a researcher to identify which later articles have cited any particular earlier article, or have cited the articles of any particular author, or have been cited most frequently. Thomson Reuters also markets several subsets of this database, termed "Specialty Citation Indexes",<ref name=SpCI>\n{{cite web \n|url=http://thomsonreuters.com/products_services/science/science_products/a-z/specialty_citation_indexes/ \n|title=Specialty Citation Indexes \n|work= |accessdate=2009-08-30}}</ref> \nsuch as the \'\'\'Neuroscience Citation Index\'\'\'<ref name=NCI>\n{{cite web \n|url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MD \n|title=Journal Search - Science - |work= |accessdate=2009-08-30}}</ref> and the \'\'\'Chemistry Citation Index\'\'\'.<ref>{{cite web |url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jloptions.cgi?PC=CD \n|title=Journal Search - Science - Thomson Reuters |accessdate=14 January 2011}}</ref>\n\n==Chemistry Citation Index==\nThe Chemistry Citation Index was first introduced by Eugene Garfield, a chemist by training. His original "search examples were based on [his] experience as a chemist".<ref name=Garcci/> In 1992 an electronic and print form of the index was derived from a core of 330 chemistry journals, within which all areas were covered. Additional information was provided from articles selected from 4,000 other journals. All chemistry subdisciplines were covered: organic, inorganic, analytical, physical chemistry, polymer, computational, organometallic, materials chemistry, and electrochemistry.<ref name=Garcci>Garfield, Eugene. "[http://garfield.library.upenn.edu/essays/v15p007y1992-93.pdf New Chemistry Citation Index On CD-ROM Comes With Abstracts, Related Records, and Key-Words-Plus]." Current Contents 3 (1992): 5-9.</ref>\n\nBy 2002 the core journal coverage increased to 500 and related article coverage increased to 8,000 other journals.<ref>\n[http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Institute of Process Engineering of the Chinese Academy of Sciences. 2003.</ref> \n\nOne 1980 study reported the overall citation indexing benefits for chemistry, examining the use of citations as a tool for the study of the sociology of chemistry and illustrating the use of citation data to "observe" chemistry subfields over time.<ref>\n{{cite journal\n|doi=10.1007/BF02016348\n|title=Science citation index and chemistry\n|year=1980\n|last1=Dewitt\n|first1=T. W.\n|last2=Nicholson\n|first2=R. S.\n|last3=Wilson\n|first3=M. K.\n|journal=Scientometrics\n|volume=2\n|issue=4\n|page=265}}</ref>\n\n==See also==\n* [[Arts and Humanities Citation Index]], which covers 1130 journals, beginning with 1975.\n* [[Impact factor]]\n* [[List of academic databases and search engines]]\n* [[Social Sciences Citation Index]], which covers 1700 journals, beginning with 1956.\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n*{{cite journal\n|doi= 10.1002/aris.1440360102\n|url= http://polaris.gseis.ucla.edu/cborgman/pubs/borgmanfurnerarist2002.pdf\n|title=Scholarly Communication and Bibliometrics\n|year= 2005\n|last1= Borgman\n|first1= Christine L.\n|last2= Furner\n|first2= Jonathan\n|journal= Annual Review of Information Science and Technology\n|volume= 36\n|issue= 1 \n|pages=3–72}}\n\n*{{cite journal\n|doi= 10.1002/asi.20677\n|url= http://staff.aub.edu.lb/~lmeho/meho-yang-impact-of-data-sources.pdf\n|format= Free PDF download\n|title= Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar\n|year= 2007\n|last1= Meho\n|first1= Lokman I.\n|last2= Yang\n|first2= Kiduk\n|journal= Journal of the American Society for Information Science and Technology\n|volume= 58\n|issue= 13\n|page= 2105}}\n\n*{{cite journal\n|doi= 10.1002/asi.5090140304\n|url= http://www.garfield.library.upenn.edu/essays/v6p492y1983.pdf\n|format= Free PDF download\n|title= New factors in the evaluation of scientific literature through citation indexing\n|year= 1963\n|last1= Garfield\n|first1= E.\n|last2= Sher\n|first2= I. H.\n|journal= American Documentation\n|volume= 14\n|issue= 3\n|page= 195}}\n\n*{{cite journal\n|doi= 10.1038/227669a0\n|url= http://www.garfield.library.upenn.edu/essays/V1p133y1962-73.pdf\n|format= Free PDF download\n|title= Citation Indexing for Studying Science\n|year= 1970\n|last1= Garfield\n|first1= E.\n|journal= Nature\n|volume= 227\n|issue= 5259\n|pages= 669–71\n|pmid= 4914589|bibcode= 1970Natur.227..669G\n}}\n\n*{{cite book\n | last =Garfield\n | first =Eugene \n | authorlink =\n | title =Citation Indexing: Its Theory and Application in Science, Technology, and Humanities\n | publisher =Wiley-Interscience\n | series = Information Sciences Series\n | edition = 1st\n | origyear = 1979| year = 1983\n | location = New York\n | isbn =9780894950247}}\n\n==External links==\n* [http://scientific.thomson.com/products/wos/ Introduction to SCI]\n* [http://science.thomsonreuters.com/mjl/ Master journal list]\n* [https://en.wikibooks.org/wiki/Chemical_Information_Sources/Author_and_Citation_Searches Chemical Information Sources/ Author and Citation Searches]. on WikiBooks. \n* [http://scientific.thomson.com/tutorials/citedreference/crs1.htm Cited Reference Searching: An Introduction]. Thomson Reuters. \n* [http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Chinweb.\n\n{{Thomson Reuters}}\n\n[[Category:Citation indices]]\n[[Category:Online databases]]\n[[Category:Thomson Reuters]]']
['Russian Science Citation Index', '35108736', '{{primary sources|date=March 2012}}\n\'\'\'Russian Science Citation Index\'\'\' is a [[bibliographic database]] of [[scientific publication]]s in Russian. It accumulates more than 2 million publications of Russian authors, as well as information about citing these publications from more than 2000 Russian journals. The Russian Science Citation Index has been developed since 2009 by the Scientific Electronic Library. The information-analytical system Science Index is a search engine of this database; it offers a wide range of services for authors, research institutions and scientific publishers. It is designed not only for operational search for relevant bibliographic information, but is also as a powerful tool to assess the impact and effectiveness of research organizations, scientists, and the level of scientific journals, etc.\n\n== Purpose ==\nFrom 3000 Russian scientific journals only about 150 are presented in foreign databases (i.e. not more than 5%). Those are mainly translated journals. So far, the vast majority of Russian scientific publications remain "invisible" and not available online.  Russian Science Citation Index makes it real to objectively compare Russian journals with  the best international journals and brings them closer to researchers all over the world.\n\n== Functionality ==\nIn Russia, this database is one of the main sources of information for evaluating the effectiveness of organizations involved in research. It allows to appraise: \n* Scientific capacity and effectiveness of research, and\n* Publication activity\nthrough the following indicators:\n* The number of publications (including foreign scientific and technical journals, and local publications from the list of [[Higher Attestation Commission]]) of researchers from a particular scientific organization, divided by the number of researchers,\n* The number of publications (registered in the Russian Science Citation Index) of researchers from a particular scientific organization, divided by the number of researchers, and\n* Citation of researchers (registered in the Russian Science Citation Index) from a particular scientific organization, divided by the number of researchers.\n\n== See also ==\n*[[List of academic databases and search engines]]\n*[[Science Citation Index]]\n*[[Scopus]]\n\n==External links==\n* [http://elibrary.ru/ Scientific Electronic Library]\n\n\n[[Category:Citation indices]]\n[[Category:Russian-language journals| ]]\n[[Category:Science and technology in Russia]]']
['Scientific Information Database', '40601299', "{{Refimprove|date=February 2014}}\n{{Notability|Web|date=December 2013}}\n{{Infobox company\n| name             = Scientific Information Database, Iran\n| type             = [Journal Citation Database]\n| location         = Tehran, Iran\n| homepage         = [http://www.sid.ir]\n| footnotes        = پایگاہ اطلاعات علمی و پژوھشی\n}}\n\n'''Scientific Information Database''' (or '''SID''') is the Iranian database for the calculation of Persian and English articles citation. It is the like the [[Institute for Scientific Information]] '''ISI''', a local citation counting manager.<ref>{{cite web|url=http://sid.ir |title=Scientific Information Database |publisher=Sid.ir |date= |accessdate=2014-02-03}}</ref>\n\n==Categories==\nThis database, does not include just the journal citation reports, it has different categories:\n* English Journals, English Journals Database of Iran\n* Persian Journals, بانک نشریات فارسی ایران\n* Research Projects, طرح ھای پژوھشی\n* English Scientific Community\n* Persian Scientific Community, بانک مجامع علمی فارسی ایران \n* Science Centers, بانک مراکز علمی ایران\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://sid.ir Homepage of SID]\n\n[[Category:Science and technology in Iran]]\n[[Category:Citation indices]]"]
['CBD Media', '4859787', '{{Infobox company |\n name = CBD Media LLC|\n logo = [[Image:CBD Media logo.jpg]]|\n type = Subsidiary of [[The Berry Company]]|\n slogan = |\n foundation = |\n location = |\n industry = [[Telephone directory]]|\n parent = [[Spectrum Equity]], etc. (2002-2007)<br>[[The Berry Company|Local Insight Media/Berry]] (2007-present)|\n products = Print Yellow Pages, Online Yellow Pages ads|\n}}\n\'\'\'CBD Media LLC\'\'\' (formerly \'\'\'Cincinnati Bell Directory\'\'\') is a division of Local Insight Media that publishes telephone directories under the [[Cincinnati Bell]] name. The company was created in 2002 following the sale of Cincinnati Bell Directory to a consortium led by [[Spectrum Equity]].\n\nCBD Media publishes the \'\'\'Cincinnati Bell Yellow Pages\'\'\', which consists of 15 directories, published under the "Real Pages" name. CBD Media also operates [http://www.cincinnatibellyellowpages.com CincinnatiBellYellowPages.com], the [[electronic yellow pages]] directory for [[Cincinnati Bell]].\n\nThe company was acquired by Local Insight Media Holdings in 2007.<ref>[http://www.spectrumequity.com/investments/investment?Id=1289 Spectrum Equity | Investments]</ref> Local Insight Media owned [[Local Insight Yellow Pages]], the former directory division of [[Windstream]]. In 2009, Local Insight acquired The Berry Company from [[AT&T]], and changed its own name to \'\'\'The Berry Company LLC\'\'\'.\n\n==See also==\n*[[Engels Maps]]\n\n==External links==\n*[http://www.cincinnatibellyellowpages.com CincinnatiBellYellowPages.com]\n\n==References==\n{{reflist}}\n\n{{Telephone directory publishers in the United States}}\n{{Cincinnati Bell}}\n\n[[Category:Advertising agencies of the United States]]\n[[Category:Directories]]\n[[Category:Media in Cincinnati]]\n[[Category:Publishing companies established in 2002]]\n[[Category:Cincinnati Bell]]\n[[Category:Publishing companies of the United States]]\n[[Category:Companies based in Cincinnati]]\n[[Category:2002 establishments in Ohio]]']
['Oregon Blue Book', '3214053', '{{Infobox book \n| name          = Oregon Blue Book\n| image         = OrBlueBookCover.png\n| caption       = Cover of the 2005 edition\n| editor        = [[Oregon Secretary of State]]\n| country       = United States\n| language      = English\n| subject       = Oregon history, government\n| genre         = Reference\n| published     = Biennially, 1911–present\n| media_type    = Print, online\n| isbn          = \n| external_url  = http://bluebook.state.or.us/\n}}\nThe \'\'\'\'\'Oregon Blue Book\'\'\'\'\' is the official directory and fact book for the U.S. state of [[Oregon]] prepared by the [[Oregon Secretary of State]]<ref name="ORS">{{cite web|url = https://www.oregonlegislature.gov/bills_laws/ors/ors177.html|title = ORS 177.120|publisher = [[Oregon Legislative Counsel]]|accessdate = February 16, 2015}}</ref> and published by the Office of the Secretary\'s [[Oregon State Archives|Archives Division]].\n\nThe \'\'Blue Book\'\' comes in both print and online editions. The [[Oregon Revised Statutes]] require the Secretary of State to publish the print edition "biennially on or about February 15 of the same year as the regular sessions of the [[Oregon Legislative Assembly|Legislative Assembly]],"<ref name=ORS/> which are during odd-numbered years; it has been so published since 1911.  The online edition is updated regularly.<ref name=About>{{cite web |url= http://bluebook.state.or.us/misc/about/about.htm |title= About the Oregon Blue Book |publisher= Oregon Secretary of State |accessdate= February 16, 2015}}</ref>\n\n==Contents==\nThe book contains information on the state, city, county, and federal governments in Oregon, educational institutions, finances, the economy, resources, population figures and demographics.<ref name=ERG83>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19830410&id=jP5VAAAAIBAJ&sjid=UeIDAAAAIBAJ&pg=3537,2331038 |title= New Oregon Blue Book Published |author= [[United Press International]] |date= April 10, 1983 |newspaper= [[The Register-Guard]] |accessdate= February 16, 2015}}</ref>\n\nThe 1919 edition contained a "statement of registered motor vehicles, chauffeurs, and dealers from 1905 to 1919", and "a general summary of in the taxable property in Oregon from 1858 to 1918".<ref name=Received>{{cite news |url= https://news.google.com/newspapers?nid=1243&dat=19190911&id=0NgsAAAAIBAJ&sjid=HCAEAAAAIBAJ&pg=3731,5589153 |date= September 11, 1919 |title= Blue Book is Received Here |newspaper= [[The Bulletin (Bend)|The Bulletin]] |accessdate= February 17, 2015}}</ref>\n\n==History==\nSecretary of State [[Ben Olcott]] published the first edition in 1911 in response to an "increased demand for information of a general character concerning Oregon".<ref name=Indispensable>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19950525&id=4ERWAAAAIBAJ&sjid=7OoDAAAAIBAJ&pg=3777,5960904 |title= Blue Book Indispensable |newspaper= The Register-Guard |date= May 25, 1995 |accessdate= February 16, 2015}}</ref>\n\nEarly editions of the book were available free from the State.<ref name=Received/> By 1937, copies cost 25; in 1981 the book cost $4.<ref>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19370921&id=dMpYAAAAIBAJ&sjid=Q-gDAAAAIBAJ&pg=2452,1169200 |title= Oregon Blue Book Being Distributed |author= United Press International |date= September 21, 1937 |newspaper= The Register-Guard |accessdate= February 17, 2015}}</ref><ref>{{cite news |url= https://news.google.com/newspapers?nid=1243&dat=19810427&id=tl0zAAAAIBAJ&sjid=EPcDAAAAIBAJ&pg=3755,5317981 |title= Oregon Blue Book Makes Biennial Appearance |author= United Press International |date= April 27, 1981 |newspaper= The Bulletin |accessdate= February 17, 2015}}</ref>\n\nIn 1953, a legislative ways and means subcommittee, headed by Representative [[Francis Ziegler]], was going to confer with Secretary of State [[Earl T. Newbry]] about how to improve the \'\'Blue Book\'\'.<ref name=Revision>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19530323&id=URZWAAAAIBAJ&sjid=veIDAAAAIBAJ&pg=5600,1181962 |title= Legislative Group to Study Revision of \'Blue Book\' |date= March 23, 1953 |newspaper= The Register-Guard |author= United Press International |accessdate= February 16, 2015}}</ref> This was following complaints by Representative [[Monroe Sweetland]] that the book was "obsolete, carelessly edited, and only of limited use."<ref name=Revision/> Calling the book "an inferior job", Sweetland criticized the timing of book\'s publication long after elections, as well as the map in the back.<ref name=Revision/> As a result, the [[47th Oregon Legislative Assembly|1953 Legislative Assembly]] passed a law requiring the book be published soon after the legislature convenes.<ref>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19550125&id=nPlVAAAAIBAJ&sjid=p-IDAAAAIBAJ&pg=6704,3150227 |title= Oregon Blue Book Printed But It Isn\'t Blue Any More |author= [[Associated Press]] |date= January 25, 1955 |newspaper= The Register-Guard |accessdate= February 17, 2015}}</ref>\n\nThe 1993–94 edition of the book contained a four-page [[errata]].<ref name=Indispensable/> When [[Norma Paulus]] was Secretary of State, she would send a free copy of the book to the first person to find a mistake in each new edition.<ref name=Indispensable/> The 1995–96 edition was reduced in size from its predecessors.<ref name=Indispensable/>\n\n==Reviews==\nA 1995 \'\'[[Register-Guard]]\'\' editorial called the book "indispensable".<ref name=Indispensable/>\n\n==See also==\n*\'\'[[The Oregon Encyclopedia]]\'\'\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://bluebook.state.or.us/ Oregon Blue Book] (official website)\n\n[[Category:1911 books]]\n[[Category:Government of Oregon|Blue Book]]\n[[Category:Directories]]\n[[Category:1911 establishments in Oregon]]']
['MiM', '10926747', "{{Other uses|MIM (disambiguation)}}\n{{Orphan|date=February 2009}}\n'''MIM''' stands for Music Industry Manual. It was founded in 1996 by James Robertson and in its first year was called The Promoter's Handbook. The Promoters Handbook was a reference manual for the [[dance music]] industry including [[DJ]]s agents, [[nightclub]]s and unusual venues, promoters, flyer designers. The following year its title was changed to give it a broader appeal.\n\nIt still caters for the dance music industry, but is now fully international with over 100,000 contacts from countries as remote as [[Azerbaijan]] to developed nations. Whilst the focus is still on DJ and club culture it has over 100 categories including bar designer, music lawyers, event management.\n\n==External links==\n* [http://www.mim.dj Official web site]\n\n{{primary sources|date=August 2007}}\n\n[[Category:Directories]]\n[[Category:Electronic dance music]]"]
["Writer's Market", '18980436', '{{italic title}}\n{{Advert|date=March 2011}}\n\'\'\'\'\'Writer\'s Market\'\' (\'\'WM\'\')\'\'\' is an annual resource book for writers who wish to sell their work. The publication is released by \'\'[[Writer\'s Digest]]\'\' Books (an imprint of [[F+W Media]]) and usually hits bookstores around June of each year. \'\'Writer\'s Market\'\' was first published in 1921, and is often called "The Bible for writers" or "the freelancer\'s Bible."<ref>http://search.barnesandnoble.com/Writers-Market-2008/Robert-Lee-Brewer/e/9781582974965</ref><ref>http://www.epinions.com/review/Book_Writers_Market_2007/content_298510028420</ref><ref>http://www.thegoodwebguide.co.uk/index.php?rid=000467</ref>\n\nThe most current edition is the 2016 edition; the current editor is Robert Lee Brewer.\n\n== Listings ==\nFor 89 years, the book has listed thousands of markets for writers who wish to sell their work. Said markets include magazines, newspapers, theaters (for stage plays), production companies, contests of all types, greeting card companies, literary agents, and more. Each listing has detailed instructions on how to submit work, relevant contact information, as well as what work each listing seeks.\n\n== Articles ==\nThe upfront section of \'\'WM\'\' has more than a dozen articles on writing topics, such as starting a freelancing business, syndication, freelancing for magazines, and a chart filled with typical payment rates concerning various writing assignments.\n\n== "Market Books" ==\n\'\'Writer\'s Market\'\' is one of nine "[[Market (economics)|market]] books" published each year by [[Writer\'s Digest Books]]. Others include: \'\'Guide to Literary Agents\'\', \'\'Photographer\'s Market\'\', \'\'Children\'s Writer\'s & Illustrator\'s Market\'\', \'\'Novel & Short Story Writer\'s Market\'\', \'\'Artist and Graphic Designer\'s Market\'\', \'\'Poet\'s Market\'\', \'\'Screenwriter\'s & Playwright\'s Market\'\' and \'\'Songwriter\'s Market\'\'. Each book is designed to give writers instructions on how to submit freelance work to markets.\n\n== See also ==\n* [[Publishing]]\n* \'\'[[Writer\'s Digest]]\'\'\n* [[literary agent]]\n* [[Literary agent#Querying|query]]\n* [[screenplay]]\n* [[royalties]]\n* [[Authors Guild]]\n* [[poetry]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{Official website|http://www.writersmarket.com|The book\'s official website}}\n*[http://www.writersdigest.com \'\'Writer\'s Digest\'\' magazine official site]\n\n[[Category:Directories]]\n[[Category:Literary agencies|.]]\n[[Category:Literary agents|.]]\n[[Category:American literary agencies]]']
['Reverse telephone directory', '5279920', 'A \'\'\'reverse telephone directory\'\'\' (also known as a \'\'\'gray pages\'\'\' directory, criss-cross directory or \'\'\'reverse phone lookup\'\'\') is a collection of telephone numbers and associated customer details. However, unlike a standard [[telephone directory]], where the user uses customer\'s details (such as name and address) in order to retrieve the telephone number of that person or business, a reverse telephone directory allows users to search by a telephone service number in order to retrieve the customer details for that service.\n\nReverse telephone directories are used by law enforcement and other emergency services in order to determine the origin of any request for assistance, however these systems include both publicly accessible (listed) and private (unlisted) services. As such, these directories are restricted to internal use only. Some forms of [[city directory|city directories]] provide this form of lookup for listed services by phone number, along with address cross-referencing.\n\nPublicly accessible reverse telephone directories may be provided as part of the standard directory services from the telecommunications carrier in some countries. In other countries these directories are often created by [[phreaking|phone phreaker]]s by collecting the information available via the publicly accessible directories and then providing a search function which allows users to search by the telephone service details.\n\n==History==\nPrinted reverse phone directories have been produced by the telephone companies (in the United States) for decades, and were distributed to the phone companies, law enforcement, and [[public library|public libraries]].<ref>{{cite news | url=https://news.google.com/newspapers?nid=1454&dat=19720102&id=87osAAAAIBAJ&sjid=vgkEAAAAIBAJ&pg=3122,379459 | title=Clinton Directory Issued | date=Jan 2, 1972 | accessdate=9 February 2014 | location=Page 16}}</ref> In the early 1990s, businesses started offering reverse telephone lookups for fees, and by the early 2000s advertising-based reverse directories were available online, prompting occasional alarms about privacy concerns.\n\n==Australia==\nIn 2001, a legal case \'\'[[Telstra|Telstra Corporation Ltd]] v Desktop Marketing Systems Pty Ltd\'\' was heard in the Australian Federal Court.<ref>{{cite web|url=http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html|title=Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd (2001) FCA 612 (25 May 2001)|author=[[Federal Court of Australia]]|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref><ref name=austliiPP>{{cite web|url=http://www.austlii.edu.au/au/journals/PLPR/2001/25.html|title=Private parts - PLPR 25; (2001) 8 PLPR 24|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref> gave Telstra, the predominant carrier within Australia and the maintainer of the publicly accessible [[White Pages]] (residential) and [[Yellow Pages]] (commercial) directories, [[copyright]] over the content of these directories.\n\nIn February 2010 a Federal Court of Australia case \'\'[[Telstra|Telstra Corporation Ltd]] v Phone Directories Company Pty Ltd\'\' determined that Telstra does not hold copyright in the White Pages or the Yellow Pages.<ref>{{cite news|url=http://www.smh.com.au/business/copyright-to-enter-a-new-dimension-20101215-18y9o.html|title=Copyright to enter a new dimension|newspaper=[[The Sydney Morning Herald]]| first=Malcolm|last=Maiden|date=16 December 2010|accessdate=20 December 2012}}</ref>\n\nAs it currently{{when|date=October 2014}} stands there is no legal way to ensure a particular number is not listed in the directories currently available.\n\n==United States==\n\nIn United States, landline phone subscribers can pay a small fee to exclude their number from the directory. This service is usually called "Your Listing Not Published" and the cost ranges between $0.80 and $1.50 for residential customers.\n\nAs [[cellular phones]] become more popular, there has been debate about releasing cell phone numbers into public [[4-1-1|411]] and reverse number directories. (S. 1963, the "Wireless 411 Privacy Act" 9/2004). However, opposition led by leading consumer-protection organization [[Consumers Union]] presented several privacy concerns in their congressional [http://www.consumersunion.org/pub/wireless%20411%20senate%20testimony%20final.pdf testimony]. Right now,{{when|date=October 2014}} cell phone numbers are not available in any public 411 or reverse-number directories. However, several information companies provide reverse cell phone lookups that are obtained from utility resources, and are available online. Because there is no central database of cell phone numbers, reverse phone directories that claim to be free cannot return information on those numbers.<ref>{{cite journal | title=Evaluating the utility and accuracy of a reverse telephone directory to identify the location of survey respondents. | work=2005 Feb |vauthors=Schootman M, Jeffe D, Kinman E, Higgs G, Jackson-Thompson J | pmid=15652722 | doi=10.1016/j.annepidem.2004.06.005 | volume=15 | pages=160-6}}</ref>\n\nIn recent years{{when|date=October 2014}} community web based services offer a reverse telephone directory of known telemarketers, debt collectors, fund raisers, and other solicitors which contact consumers by telephone.  Users of these services can perform a search of the telephone number which showed up on their caller ID and read through user comments to find the identity of the calling company or individual.\n\n==United Kingdom==\nIn the United Kingdom proper, reverse directory information is not publicly available.<ref>{{cite web | url=http://ico.org.uk/for_organisations/privacy_and_electronic_communications/the_guide/directories_of_subscribers | title=Directories of subscribers | publisher=Information Commissioner\'s Office | accessdate=9 February 2014}}</ref> However, in the [[Channel Islands]] it is provided in the printed telephone directories.\n\nAlthough the information is, of necessity, available to emergency services, for other agencies it is treated as \'communication data\' in the [[RIPA]] regime and subject to the same controls as requests for lists of and content of calls.\n\n==References==\n{{reflist}}\n\n==External links==\n<!-- Do not delete these comments. -->\n<!-- Do not put commercial links into this list. Doing so can get you blocked with no further warning. --> \n*[https://web.archive.org/web/20010721175437/http://blackpages.2600.org.au/ Wayback Machine (21 July 2001) archive of http://blackpages.2600.org.au]\n*[http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html Federal Court of Australia Case 612 (25 May 2001): Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd]\n\n[[Category:Telephone numbers]]\n[[Category:Directories]]\n[[Category:Information retrieval systems]]']
['Blue pages', '5507437', '{{Unreferenced|date=December 2009}}\n\'\'\'Blue pages\'\'\' are a [[telephone directory]] listing of American and Canadian state agencies,  [[government]] agencies, federal government and other official entities, along with specific offices, departments, or bureaus located wherein.\n\n==Canada==\nCanadian yellow-page listings currently indicate "Government Of Canada-See Government Listings In The Blue Pages"; in markets where the local telephone directory is a single volume, the blue pages and community information normally appear after the alphabetical white-page listings but before the yellow pages advertising. The blue page listings include both provincial and federal entities.{{cn|date=April 2013}}\n\n==United States==\nIn the [[United States]], the blue pages included state, federal, and local offices, including [[service district]]s such as school districts, port authorities, public utility providers, parks districts, fire districts, and the like. The blue pages also provided information about government services, in addition to officials\' names, addresses, telephone numbers, and other contact information. The color blue is likely derived from so-called government blue books, official publications printed by a government (such as that of a state) describing its organization, and providing a list of contact information. (The blue pages published in a printed telephone directory is usually quite abridged, compared to official blue books).\n\n==Other==\nThe name "blue pages" has been used for various specialised directories by private-sector entities such as the internal IBM Staff directory. \n\n{{DEFAULTSORT:Blue Pages}}\n[[Category:Telephone numbers]]\n[[Category:Directories]]\n\n{{telephony-stub}}']
['Mobile directory', '27409630', "{{multiple issues|\n{{Unreferenced|date=October 2010}}\n{{orphan|date=October 2010}}\n}}\n\nA '''mobile directory''' is a collection of subscriber details of a [[mobile phone]] [[Mobile phone companies|operators]]. Generally, the mobile telephony operators do not publish a mobile [[directory (databases)|directory]]. Some third party [[websites]] offer mobile directory facility through [[Reverse telephone directory|reverse search]].\n\n[[Category:Telephone numbers]]\n[[Category:Directories]]\n\n\n{{telephonenumber-stub}}\n{{mobile-stub}}"]
['Army List', '31790826', "The '''''Army List''''' is a list (or more accurately seven series of lists) of serving regular, [[militia]] or territorial [[British Army]] officers, kept in one form or another, since 1702.\n\nManuscript lists of army officers were kept from 1702–1752, the first official list being published in 1740.\n\n==Regular army==\n* Annual Army Lists, 1754–1879 (WO 65)\n* Quarterly Army Lists (First Series), 1879–1922\n* Half-Yearly Army Lists, 1923 - Feb 1950 (From 1947, annual, despite the name)\n* Modern Army Lists, 1951-Ongoing\n** Part 1; serving officers.\n** Part 2; retired officers, as of 2011 four-yearly\n** Part 3; the Gradation List, a short biography of officers, a restricted publication  not generally available.\n\n==Other lists==\n* Monthly Army Lists, 1798-June 1940. Officers of colonial, militia and territorial units are included.\n* Quarterly Army Lists (Second Series), July 1940-December 1950. These superseded the Monthly Army Lists, and, for the remainder of [[World War II]] were not published but  produced as confidential documents, monthly or bi-monthly until December 1943 and quarterly until April 1947, then three times a year, April, August and December.\n* [[British Home Guard|Home Guard]] List, 1939–1945\n* Militia Lists - various militia lists pertaining to the eighteenth and nineteenth centuries are extant.\n* ''[[Hart's Army List]]'', an unofficial list, produced between 1839 and 1915, containing details of war service which the official lists started covering only in 1881.\n\n==Further reading and bibliography==\n* ''The army lists of the [[Roundheads and Cavaliers]]: containing the names of the officers in the Royal and Parliamentary armies of 1642'', [[Edward Peacock (antiquary)|Edward Peacock]] (ed) (1874)\n* ''English army lists and commission registers, 1661–1714'', [[Charles Dalton]] (ed) (1892–1904)\n* [[Henry George Hart]], ''Hart's army list : the new army list exhibiting the rank, standing, and various services of every officer in the Army on full pay'' (1839-)\n* William Spencer, ''Army service records of the First World War'' (seventh edition, 2006)\n\n==See also==\n* [[Navy List]]\n* [[Crockford's Clerical Directory]]\n*\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://books.google.com/books?id=p_BfsBzDzWYC The 1740 Army List] at [[google books]]\n* [http://discovery.nationalarchives.gov.uk/SearchUI/Collection/Display?iaid=C14273&parentiaid=C543 War Office: Printed Annual Army Lists 1754-1879 (WO 65) - download for free]\n* Digitised copies of 'Quarterly army lists' from [http://digital.nls.uk/97136046 1913 to 1919] and from [http://digital.nls.uk/97136048 1940 to 1946] at [[National Library of Scotland]]\n* Digitised copies of 'Half-yearly army lists' from [http://digital.nls.uk/97136047 1938 to 1941] at National Library of Scotland\n* [http://www.nationalarchives.gov.uk/records/research-guides/british-army-lists.htm British Army Lists] ([[National Archives]]' Research Guide)\n* [https://archive.org/details/nlsarmylists Hart's Army List] at the [[Internet Archive]]\n\n[[Category:Directories]]\n[[Category:British Army]]"]
['Navy List', '47170', '{{for|a list of countries with navies|List of navies}}\n{{multiple issues|\n{{more footnotes|date = March 2013}}\n{{Globalize|date=May 2009}}\n}}\n\nA \'\'\'Navy List\'\'\' or \'\'\'Naval Register\'\'\' is an official list of [[navy|naval]] officers, their ranks and seniority, the ships which they command or to which they are appointed, etc., that is published by the government or naval authorities of a country.\n\n==Background==\nThe Navy List fulfills an important function in [[international law]] in that warships are required by article 29 of the [[United Nations Convention on the Law of the Sea]] to be commanded by a [[commissioned officer]] whose name appears in the appropriate service list.{{why|date=June 2016}}\n\nPast copies of the Navy List are also important sources of information for historians and genealogists.\n\nThe Navy List for the Royal Navy is no longer published in hard-copy.\n\nThe [[Royal Navy]] (United Kingdom) publishes annual lists of active and reserve officers, and biennial lists of retired officers. The equivalent in the [[United States Navy]] is the Naval Register, which is updated online on a continuous basis.  When a ship is removed from the [[Naval Vessel Register]] in the United States, or from a Naval List of any other country, the ship is said to be "[[:wikt:stricken|stricken]]".<ref>Edwards, Paul.  \'\'[https://books.google.com/books?id=OydzBgAAQBAJ&pg=PA37 Small United States and United Nations Warships in the Korean War]\'\', p. 37 (McFarland, 2008).</ref>\n\n== Resources ==\nGood sources of historical data on UK\'s Navy Lists are\n*The Naval Historical Branch, Portsmouth Naval Base.\n*The Central Library Portsmouth, Guildhall Square.\n*[[The National Archives (United Kingdom)|The National Archives]], Kew, that has an almost complete set including unpublished editions produced during the Second World War for internal use by the Admiralty.\n*The Caird Library of the [[National Maritime Museum]] has in its collection bound monthly lists published by the Admiralty, and the concurrently published Steel\'s lists\n\nThe current editor of the Navy List is Cliona Willis\n\n== Bibliography ==\n\n* \'\'The 1766 Navy List\'\', Edited by E. C. Coleman, Published by Ancholme Publishing, ISBN 0-9541443-0-9\n\n==See also==\n* [[Army List]]\n* [[Naval Vessel Register]]\n\n==References==\n<references />\n\n== External links ==\n* [http://www.royalnavy.mod.uk/~/media/royal%20navy%20responsive/documents/useful%20resources/navy%20list.pdf Navy List 2013]\n* [https://navalregister.bol.navy.mil US Naval Register] (US Navy)\n* [http://www.NavyListResearch.co.uk Navy List Research] (Royal Navy)\n\n[[Category:Royal Navy]]\n[[Category:United States Navy]]\n[[Category:Directories]]']
['List of yellow pages', '12767772', '{{multiple issues|\n{{Advert|date=May 2010}}\n{{Refimprove|date=May 2010}}\n}}\n\n[[Yellow pages]] [[telephone directories]] of businesses:\n\n{{compact ToC|side=yes|top=yes|num=yes}}\n\n==A==\n* \'\'\'Afghanistan\'\'\': In [[Afghanistan]], the Canadian INGO [[Peace Dividend Trust]] launched a free online directory with over 2700 verified and registered Afghan enterprises in late 2006.\n* \'\'\'Africa\'\'\': In [[Africa]], a business directory is YelloPagesAfrica published by \'\'Yellopages Development Company Limited\'\'. It is an online business directory. It ia an interactive online business directory with a mission to integrate Africa businesses. It covers the entire Africa continent and operates on a do-it-yourself basis.\n* \'\'\'Albania\'\'\': In [[Albania]], the directory is called Flete te Verdha - Albanian Yellow Pages which is a registered trademark belonging to Maxidisk SH.P.K Group and Fleteteverdha sh.p.k from Tirana.\n* \'\'\'Algeria\'\'\': In [[Algeria]], the Yellow Pages business directory is published in French as \'\'\'Les Pages Jaunes\'\'\'. It is also available online in English and in French.\n* \'\'\'Anguilla\'\'\': In [[Anguilla]], the directory is published by [[Global Directories Limited]] and titled \'\'Anguilla Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online at www.anguillayp.com.\n* \'\'\'Armenia\'\'\': In [[Armenia]], "Spyur" Information Center introduces "Armenia Yellow Pages". Directory is printed in English, [[Armenian language|Armenian]] and [[Russian language|Russian]](17500 copies a year). Other directory Armenian Business Pages was launched in 2015 by Comfy LLC and represents only digital version of yellow pages of Armenia. The directory is in the process of Electronic Armenia® trademark registration.<ref>http://www.pages.am</ref>\n* \'\'\'Aruba\'\'\': In [[Aruba]], the official telephone directory of Setar is published by [[Global Directories Limited]] and titled \'\'Aruba Yellow Pages\'\'. 85.000 Print copies are distributed free to households and companies and is also available online\n* \'\'\'Austria\'\'\': The "Yellow Pages" and [[Yellow Pages#Internet yellow pages|IYP]] services are provided by: HEROLD Business Data GmbH, a [[European Directories]] group company.\n* \'\'\'Australia\'\'\': In Australia, the most comprehensive business directory is the Yellow Pages published by [[Telstra#Directories and advertising (Sensis)|Sensis]]. The directory is also available online, on mobile and via smartphone app.\n\n==B==\n* \'\'\'Bangladesh\'\'\': In [[Bangladesh]], the business directory is published by \'\'\'\'\'Ad Yellowpages\'\'\'\'\' \'\'\'\'\'Pages\'\'\'\'\' and titled \'\'\'\'\'Ad Yellowpages Yellow Pages\'\'\'\'\'. AdYP, a sister conern of Ad Yellowpages.com is an new concept brought forward by the founders of the site. The site provide a variety of information about local places and businesses in Bangladesh.<ref>https://localyaar.com</ref>\n* \'\'\'Bahrain\'\'\': In [[Bahrain]], the business directory is published by \'\'\'\'\'Primedia International BSC (c)\'\'\'\'\' and titled \'\'\'\'\'Bahrain Yellow Pages\'\'\'\'\'. Primedia International signifies a fundamental move away from the traditional business directories to new print & online media.\n* \'\'\'Barbados\'\'\': In [[Barbados]], the directory is published by [[Global Directories Limited]] and titled \'\'Barbados Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online at www.barbadosyp.com and on mobile devices at yp2go.bb.\n* \'\'\'Belarus\'\'\': In [[Belarus]], the directory is titled \'\'Business-Belarus\'\' ([[Russian language|Russian]]), it is also available online. There is an alternative directory, called \'\'Belarus XXI vek\'\' (Belarus 21st century), which is analogue to Yellow Pages; it is also available online.\n* \'\'\'Belgium\'\'\': In Belgium, the directory is titled \'\'Pages d\'Or\'\' (golden pages) (French) or \'\'Gouden Gids\'\' (golden guide) ([[Dutch (language)|Dutch]]), and is distributed free to each telephone subscriber, it is also available online.\n* \'\'\'Bonaire\'\'\': In [[Bonaire]], the directory is published by [[Global Directories Limited]] and titled \'\'Bonaire Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber.\n* \'\'\'Bolivia\'\'\': In [[Bolivia]], yellowpages exist online under the URL \'\'Yellow Pages.com.bo\'\'.\n* \'\'\'Bosnia\'\'\': In [[Bosnia and Herzegovina|Bosnia]], Yellow Pages exist online under YellowPages.ba.\n* \'\'\'Brazil\'\'\': In [[Brazil]], the directory is titled \'\'Páginas Amarelas\'\' and is distributed free to each telephone subscriber. Available online by DYK Internet S/A.\n* \'\'\'British Virgin Islands\'\'\': In the [[British Virgin Islands]], the directory is published by [[Global Directories Limited]] and titled \'\'British Virgin Islands Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online at www.britishvirginislandsyp.com.\n\n==C==\n* \'\'\'Cambodia\'\'\': In [[Cambodia]], the official Yellow Pages directory is called [[Cambodia Yellow Pages]] and published under contract to local Ministry of Posts and Telecommunications by [[CAMYP Co., Ltd]].\n* \'\'\'Canada\'\'\': In Canada, the company [[Yellow Pages Group]] owns the trademarks \'\'Yellow Pages\'\' and \'\'Pages Jaunes\'\'. It produces and distributes directories in both English and French. Yellow Pages Group is the market leader in print and online commercial directories and one of the largest media companies in Canada, producing the official directories of [[Bell Canada]], [[Telus]], [[Aliant]], [[Manitoba Telecom Services|MTS]], and others. [[Saskatchewan]]\'s [[SaskTel]], through subsidiary [[DirectWest]], is believed to be the last major [[incumbent local exchange carrier]] to publish its own directories. Competitive local directory publishers, such as PhoneGuide or DirectWest\'s operations in Manitoba and Alberta, usually include commercial directories on yellow paper.\n* \'\'\'Cayman Islands\'\'\': In the [[Cayman Islands]], the directory is published by [[Global Directories Limited]] and titled \'\'Cayman Islands Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online and on mobile devices.\n* \'\'\'Chile\'\'\':\n* \'\'\'China\'\'\': In China, the modern yellow pages industry was started in the late 1990s with the formation of two international joint ventures between US yellow pages publishers and China’s telecom operators, namely: a joint venture started in Shenzhen between [[RHDonnelley]] and [[China Unicom]] (later including Hong Kong’s PCCW and InfoSpace); and a joint venture between [[China Telecom Shanghai]] and what later came to be known as the yellow pages operations of [[Verizon]] {{Citation needed|date=May 2010}}. Later, another mainly state-owned telecom operator, [[China Netcom]] began to produce, either directly or on a sub-contracted basis, yellow pages in selected cities around the country. By early 2005, there were a number of independent local and international yellow pages operators in numerous cities including [[Yilong Huangbaoshu]], based in Hangzhou, Zhejiang Province with operations in Hangzhou and Ningbo {{Citation needed|date=May 2010}}. However, there is no nationwide Yellow pages in any format and only some international-trade related businesses including INBIT (USA), CHINAPAGES.COM and ALIBABA.COM (Chinese) are running some kind of national online databases based on business lists not from telephone companies. [[China Yellow Pages]] is also a common-place for finding manufacturers and exporters from China.\n* \'\'\'Colombia\'\'\': In [[Colombia]], the standard yellow and [[White Pages]] are published and distributed every year free of charge by [[Publicar]], a Colombian subsidiary company of [[Carvajal Group|Carvajal]], which also publishes and distributes yellow and white pages in other Latin American countries.\n* \'\'\'Croatia\'\'\': In [[Croatia]], the directory is called \'\'Žute stranice\'\' (yellow pages), published by [[MTI Telefonski imenik/Zute stranice]]. Another directory is \'\'CroPages Business Directory/Poslovni Adresar\'\', published by [[Masmedia]].\n* \'\'\'Cuba\'\'\': In [[Cuba]], the equivalent online directory is titled [[Paginas Amarillas]], with information on the whole of Cuba.\n* \'\'\'Cyprus\'\'\': In [[Cyprus]], the Yellow Pages is edited by ID Yellowpages Ltd [[Cyprus Yellow Pages Directory]].\n* \'\'\'Cyprus (North)\'\'\'; In [[Turkish Republic of Northern Cyprus]] [[CYPYP North Cyprus Yellow Pages]]\n* \'\'\'Czech Republic\'\'\': In the [[Czech Republic]] and [[Slovakia]], the directory is titled \'\'Zlaté stránky\'\' (golden pages), published by [[Mediatel]], Prague (a [[European Directories]] group company) and is distributed free to each telephone subscriber, usually in exchange for its previous version.\n\n==D==\n* \'\'\'Denmark\'\'\': In Denmark, a full online directory including most phone numbers is provided by \'\'De Gule Sider\'\' (a brand of Eniro, a Nordic search engine and directories company), with paper versions of yellow and white pages distributed to subscribers throughout the country; it was formerly a part of [[TDC Forlag]], a subsidiary of the national telecoms operator.\n* \'\'\'Dominica\'\'\': In [[Dominica]], the directory is published by [[Global Directories Limited]] and titled \'\'Dominica Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online at www.dominicayp.com.\n* \'\'\'Dominican Republic\'\'\': In [[Dominican Republic]], published by [[Caribe Media]]. Publishing of printed and / or digital directories in the Dominican Republic.\n\n==E==\n* \'\'\'Egypt\'\'\': [[Egypt Yellow Pages Ltd]] is the official publisher of Yellow Pages branded products in Egypt. Egypt Yellow Pages Ltd, founded in 1991, is the owner of the Yellow Pages trademark in Egypt.\n* \'\'\'Europe\'\'\': For whole Europe, the [[European Yellow Pages]] apply. The European Yellow Pages is an effort of providing harmonized data to different language environments through keeping the character of having localized search capabilities on a regional level. Harmonizing data in this context means providing information to global users mainly in English and to local users in their native language.\n* \'\'\'Europe\'\'\': For [[Europe]] the directory [[Yellobook.eu]] is providing information about many branches and companies all around 33 major European countries.\n\n==F==\n* \'\'\'Finland\'\'\': In [[Finland]], the directories are called \'\'Keltaiset sivut\'\', Eniro.fi and [[Teloos.fi]]\n* \'\'\'France\'\'\': In France, Yellow Pages are referred to as \'\'Pages Jaunes\'\'. They are distributed free by Pagesjaunes\'\'\'.fr\'\'\', a company affiliated with [[Orange S.A.|France Télécom]]. Pagesjaunes\'\'\'.com\'\'\', the .com version of \'\'Pages Jaunes\'\', was the issue of a major court case at [[World Intellectual Property Organization|WIPO]]; the original registrant, an individual from Los Angeles, won against France Télécom. This court decision defended by the Parisian Lawyer, Andre Bertrand, was path-setting for the whole European Yellow Pages industry, as it decided that the phrase "Yellow Pages" cannot be considered the property of a single company. Previously, many former state monopoly telecom companies outside the US had tried to ban competition by claiming the term "yellow pages", or the translation of "yellow pages" into the vernacular, as their exclusive trademark. [[Vivendi|Vivendi Universal]] moved to enter the French Yellow Pages market in 2001 with scoot.fr, but the attempt was a killed by a reorganisation of the struggling company. Another French editor of Yellow Pages is [[Bottin]]. More competition is expected in November 2005 from the liberalisation of "12", the former unique "[[4-1-1]]" number of [[Renseignements Telephoniques]], French for Directory Inquiry. In November 2006 [[Orange S.A.|France Télécom]] sold its majority share in pagesjaunes.fr to Mediannuaire. In August 2007 pagesjaunes\'\'\'.com\'\'\' finally became active, giving France two different \'\'Pages Jaunes\'\'; thus creating agitation at pagesjaunes\'\'\'.fr\'\'\', which reshaped their site and started a massive advertisement campaign all over France.\n\n==G==\n* \'\'\'Georgia\'\'\': In Georgia, the directory is called ყვითელი ფურცლები and published by "Yellow Pages Tbilisi" Ltd.\n* \'\'\'Germany\'\'\': In Germany, a directory titled \'\'Die Gelben Seiten\'\' is distributed free to each subscriber, by the [[Deutsche Telekom]], owner of [[T-Mobile]]. Other Yellow pages are edited by \'\'Go Yellow.de\'\', \'\'Klicktel.de\'\' and [[Gelbex.de]]. In 2006 a lawsuit with the [[Deutsches Patentamt]] denied the validity of the German Trademark "Gelbe Seiten" which in fact is the German translation of the universal expression "Yellow Pages". Klaus Harisch, an Internet Pioneer from Munich and founder of Go Yellow.de had spent over 7 Million Euros on Lawyer Fees to fight for the cancellation of the German "Gelbe Seiten" trademark. Deutsche Telekom had also registered "Yellow Pages" as a German trademark which they lost at the same time. On a European Level Deutsche Telekom had failed to register "Gelbe Seiten Deutschland" or "Yellow Pages Germany" as a Euro Trademark with [[OMPI]].\n* \'\'\'Gibraltar\'\'\': A combined White and Yellow Pages directory, along with an [[Yellow Pages#Internet yellow pages|IYP]] service, are provided by: gibyellow.gi, a [[European Directories]] group company.\n* \'\'\'Greece\'\'\': In Greece, Yellow Pages are called \'\'"Chrysos Odigos"\'\' that can be translated as "The Golden Guide".\n* \'\'\'Grenada\'\'\': In [[Grenada]], the directory is published by [[Global Directories Limited]] and titled \'\'Grenada Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online at www.grenadayp.com.\n* \'\'\'Guyana\'\'\': In [[Guyana]], the directory service is provided by "[[GT&T]]" in printed format and in online services, there are quite a few, some of them are "[[YellowPagesGuyana]]", "[[YellowGuyana]]" and "[[GT&T]]\'s" own online yellowpages directory--"[[yellowpages.com.gy]]".\n\n==H==\n* \'\'\'Hong Kong\'\'\': In [[Hong Kong]], the phone directory is titled \'\'Hong Kong Yellow Pages\'\', published by [[PCCW|PCCW Media Limited]].\n* \'\'\'Hungary\'\'\': In Hungary, the directory is called \'\'Arany Oldalak\'\' (gold pages); are published and distributed by [[MTT Magyar Telefonkönyvkiadó]] Kft, Budaörs.\n\n==I==\n* \'\'\'India\'\'\': [[India]] is a very large country in terms of population, business activities and economy. There are multiple Yellow Pages being published by private sector companies. Some of them focus the whole nation and some are regional.\n* \'\'\'Indonesia\'\'\': In [[Indonesia]], the telecommunication company [[Telkom (Indonesia)|Telkom]] with [[PT. Infomedia Nusantara]] (one of its subsidiaries), regularly publishes phone books. The company provides directory, call centre, and content services since 1984. The phone books consist of white pages and yellow pages, which are published in hard and soft copies.\n* \'\'\'Iran\'\'\': In the Islamic Republic of [[Iran]], the directory is called \'\'The first book\'\' or in [[Persian language|Persian]] \'\'Keta:b e Avval\'\'. This directory divides into different sections such as Directory of Businesses, jobs and maps and city guides. There is an official YellowPages in Iran owned and published by Iranian Yellow Page company. It has been developed in Persian and English languages, and contains different categories and locations of Iran. There is also an unofficial company that runs \'\'The Iran Yellow Pages\'\'. This directory is published by Moballeghan Publishing and Advertising Company (1986) with the cooperation of The Trade Promotion Organization of Iran. By 2010 a new updated comprehensive directory called \'\'"The First Portal"\'\', or \'\'"First Eurasia E-commerce"\'\' or in [[Persian language|Persian]] \'\'"تجارت الكترونيك اول"\'\' comes to the [[Iran]] high potential markets.\n* \'\'\'Iraq\'\'\': In Iraq, the directory is called \'\'Yellow Pages\'\' or in Arabic (Al Safahat al Safraa). This directory divides into different sections such as directory of businesses, jobs and maps and city guides and contains thousands of businesses in many categories. The directory is published by Alam Al-Rooya Publishing and Advertising Company.\n* \'\'\'Ireland\'\'\': In the [[Republic of Ireland]], the directory is called \'\'Golden Pages\'\' and is published by FCR Media. Ireland\'s free Yellow pages is called BusinessPages.i.e.\n* \'\'\'Israel\'\'\': In Israel, the yellow pages Hebrew edition is called \'\'Dapei Zahav\'\' (Golden Pages) and the English edition is \'\'Golden Pages\'\'. The print directories come out in separate issues based on Israel\'s different telephone area codes, published by Golden Pages Publications Ltd. Five million copies of the yellow pages are distributed annually.\n* \'\'\'Italy\'\'\': in Italy, the directory is titled \'\'Pagine Gialle\'\' (Yellow Pages). The printed versions come out in separate issues for [[province]] as [[White pages]]. Some years ago, an alternative directory, called \'\'Pagine utili\'\' (Useful Page) was proposed.\n\n==J==\n* \'\'\'Jamaica\'\'\': In [[Jamaica]], the directory is published by [[Global Directories Limited]] and titled \'\'Jamaica Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online at www.jamaicayp.com and to mobile subscribers at yp2go.com.jm.\n* \'\'\'Japan\'\'\': In Japan, the Yellow Pages directory, are known as [[:ja:タウンページ|Town Page]], and published by [[Nippon Telegraph and Telephone|NTT]].\n* \'\'\'Jordan\'\'\': In [[Jordan]], the directory is titled \'\' Yellow Pages - Jordan\'\', Yellow Pages Jordan is operated since 2001 by PAGESJAUNES LIBAN, a subsidiary of the European PagesJaunes Group - France in 1997.\n\n==K==\n* \'\'\'Kazakhstan:\'\'\' In [[Kazakhstan]], the directory is \'\'Yellow Pages of Kazakhstan\'\', published by [[Yellow Pages Kazakhstan]] Management Group.\n* \'\'\'South Korea:\'\'\' In [[South Korea]], the directory is published and distributed by many publishers:\n** \'\'BiG Yellow Pages. Korean National Directory\'\', by [[Yellow Pages Korea]];\n** \'\'Korea Yellow Pages\'\', by [[Korea Yellow Pages]];\n** \'\'Korea English Yellow Pages\'\', by [[Korea Telecom Directory]].\n* \'\'\'Kosovo:\'\'\' In [[Kosovo]], [[Faqe te Verdha]] is a trademark belonging to [[KOSOFT]], [[Pristina]].\n* \'\'\'Kyrgyzstan:\'\'\' In [[Kyrgyzstan]], Yellowpages can be found under the URL "yellowpages.kg".\n\n==L==\n* \'\'\'Lebanon:\'\'\' In [[Lebanon]], the Yellow Pages business directory is published in [[Arabic language|Arabic]] and French by PAGESJAUNES LIBAN.\n\n==M==\n* \'\'\'Macau:\'\'\' In [[Macau]], the phone directory is titled \'\'Macau Yellow Pages/Páginas Amarelas\'\', publ. by Directel [[Macau Listas Telefonicas]] Lda.\n* \'\'\'Madagascar:\'\'\' In [[Madagascar]] yellow pages can be found via the site Madagascar Yellow Pages.\n* \'\'\'Malaysia:\'\'\' In [[Malaysia]], there are 4 large directories Malaysia Yellow Pages, Malaysia Super Pages, Malaysia Business Directory and BCZ.com\n* \'\'\'Maldives:\'\'\' In [[Maldives]], the commercial phone directory is called Yell.\n* \'\'\'Mali:\'\'\' In [[Mali]], the equivalent online directory is titled [[Malipages.com]].\n* \'\'\'Malta:\'\'\' In [[Malta]], the Yellow Pages Directory is published by [[Ark Publishing Group]]. It has been publishing the Yellow Pages since 1997 and each year distributes 200,000 directories free of charge to the general public.\n* \'\'\'Mauritius:\'\'\' In [[Mauritius]], the Yellow Pages Directory is published by [[Teleservices Ltd]] and is known as MT yellow pages \n* \'\'\'Mexico\'\'\': In Mexico, there are several commercial phone directories. The incumbent is called Seccion Amarilla.com.mx (Yellow Section) is published by Anuncios en Directorios, S.A. de C.V., a subsidiary of Telmex, the local Telco. Others are Paginas Amarillas.com.mx (Yellow Pages) published by Phonebook of the World, Paginas Amarillas.com published by Publicar, Mexico Data Online.com published by the Mexico Business Directory and Paginas Utiles.com.mx published by Ideas Intercativas, S.A.\n* \'\'\'Moldova:\'\'\' In [[Moldova]] yellow pages can be found via the site [[www.yp.md]].\n* \'\'\'Mongolia:\'\'\' In [[Mongolia]], the directory is called \'\'Mongolia Yellow Pages\'\' (yellow pages) and can be found via [[www.yp.mn]].\n* \'\'\'Montserrat\'\'\': In [[Montserrat]], the directory is published by [[Global Directories Limited]] and titled \'\'Montserrat Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online.\n* \'\'\'Morocco:\'\'\' In [[Morocco]], the directory is called \'\'Pages Jaunes\'\' (yellow pages).\n* \'\'\'Myanmar:\'\'\' In [[Myanmar]], the directory is called \'\'Yangon Directory\'\' (ရန်ကုန်လမ်းညွန်).\n\n==N==\n* \'\'\'Netherlands:\'\'\' In [[Netherlands]], the directory is called \'\'Gouden Gids\'\' (literally "Golden Guide"), and within the district concerned, it is distributed free to each telephone subscriber, by De Telefoongids BV (a [[European Directories]] group company).\n* \'\'\'New Zealand:\'\'\' In New Zealand, the yellow.co.nz directory is printed in 18 regional editions by Yellow Pages Group (YPG). YPG also publishes 18 regional editions of \'[[White Pages]]\' (combined government, residential and business listings), and a \'Local Directory\' for some urban areas and sub-regions.\n* \'\'\'Nigeria:\'\'\' In [[Nigeria]], Yellow Pages companies are privately owned. [[NigerianYellowPages.com]] is the official trademark owner of the walking finger logo with six (6) edition of its yellow pages in different formats. YellowPages.net.ng claimed to be the first Yellow Pages Directory in the world to emulate social media network concept.  \n** \'\'\'Nigerian Yellow Pages\'\'\': Content of [[Nigerian Yellow Pages]] or commonly known as NigerianYellowPages.com is available in six formats (editions): \'\'CD-ROM directory\'\'; \'\'MS Windows Desktop directory\'\'; \'\'Internet directory\'\'; \'\'Mobile Phone Internet directory\'\'  \'\'Mobile Phone SMS text directory\'\' and \'\'Print directory\'\'. There is also a mobile edition, which is accessible on mobile phones and other mobile devices such as PDA. There is a dedicated classifieds section in their yellow pages for jobs, properties, homes, rentals and announcements. It has its own toolbar, the [[Nigerian Yellow Pages Toolbar]].\n** \'\'\'Africaonline business directory\'\'\': The other Yellowpage business directory in Nigeria is the Africaonline business directory, [[yellopages.com]]. This is an interactive online business directory that enables businesses to upload their profiles and place their adverts. Yellopages.com includes Nigerian content and serves to integrate Nigerian businesses.\n* \'\'\'Norway:\'\'\' In Norway, the directory is called \'\'Gule Sider\'\' (Yellow Pages). The two second largest directories are [[Opplysningen 1881]] and [[Nettkatalogen]]. [[Gul.no]], [[180.no]], [[Avanti Media AS]], [[Bedriftssøket AS]], [[Gul Index]] and [[Finnfirma.no]] are som of the other directories in growth. The searchengine Sesam.no provides a business directory branded [[Sesam Katalog]].\n\n==P==\n* \'\'\'Pakistan:\'\'\'\n** \'\'\'Jamal\'s Yellow Pages of Pakistan:\'\'\' is a B2B [[Trade Directory]] published by US Publishers (Pvt) Ltd. since 1983. The directory is published in printed form (3 volumes per set), CDROM version and online.\n** \'\'\'Time\'s Trade Directory of Pakistan:\'\'\' Time Publishers (Pvt) Ltd. published "Time\'s Trade Directory of Pakistan - National Yellow Pages" since 2002. B2B Version also launch similarly as Time\'s e-Directory. The online version also provide comprehensive information about Pakistan Businesses to the web user worldwide.\n** \'\'\'PhoneBook.com.pk:\'\'\' The [[Pakistan Telecommunication Company]] maintains a yellow pages and white pages directory. JS Enterprises Private Limited is publishing this directory, which is also associated with [[Jang Group of Newspapers]] and the GEO Television Network.\n** \'\'\'Ebizpk.com: \'\'\'Online Green & Yellow Pages of Pakistan. Launched in January 2010. Initially Listing 10 companies of each sector.\n** \'\'\'Dmoz Pakistan:\'\'\' Database of [[Pakistani]] companies, [[Government]] departments and business organizations in categorized format.\n** \'\'\'[http://ypages.pk Ypages.pk]:\'\'\'  Launched in June 2012, Online Yellow Pages of [[Pakistan]] provides all local business contact details. Ypages associated with ALM Advertising Agency.\n** \'\'\'[yellowpagespk.com]: \'\'\'Marshall Online Yellow Pages  in Islamabad Pakistan & Online Business Directory in Islamabad Rawalpindi Lahore Karachi Pakistan.\n* \'\'\'Palestine:\'\'\' [[Palestine Yellow Pages Ltd]] is the official publisher of Yellow Pages branded products in Palestine. [[Palestine Yellow Pages]] is the exclusive owner of the Yellow Pages, Walking Fingers & Design, and YellowPages.com.ps trademarks in Palestine. [[Palestine Yellow Pages]] is part of the [[Al Wahda-Express Group of Companies]]. Founded in 1986, [[Al Wahda-Express Group of Companies]] employs nearly 1000 employees publishing print, online and mobile Yellow Pages directories throughout 5 countries including Palestine.\n* \'\'\'Panama:\'\'\' In [[Panama]], [[Yellow Pages Panama]]\n* \'\'\'Peru:\'\'\' In United States, [[Peruvian Yellow Pages]], since 1993, the printed edition, is the first and oldest publication for Peruvians living in the USA. Now with the online version covering coast to coast the American territory. The online version of the Peruvian yellow pages is available at [[Peruvian Yellow Pages]].\n* \'\'\'Philippines:\'\'\' In [[Philippines]], Directories Philippines Corporation (DPC), regularly publishes phone books of more than a dozen telecom companies in the country.\n* \'\'\'Poland:\'\'\' In [[Poland]], it is called \'\'żółte strony\'\' and is distributed by [[Polskie Książki Telefoniczne]] (a [[European Directories]] group company) as a part of their phone books. The second largest directory, published by [[Eniro]], is called "Panorama Firm" (panorama of companies). [[YellowPages.pl]]. It is the biggest online directory in Poland. Polish Yellow Pages has existed on the market since 1998. Yellow Pages enables them to search companies and products and services, it is a business platform, which helps to promote a company and to establish trade relations. In April 2007 started [[Zumi.pl]] - first local search web which connects maps and information about companies in Poland. Several historical directories from Poland are available online as scans, and can be searched via [[kalter.org]].\n* \'\'\'Portugal:\'\'\' In [[Portugal]], the \'\'Páginas Amarelas\'\' are controlled by [[Portugal Telecom]] and the website is [[pai.pt]]. The printed version is distributed for free to all land line users. There is also available a residential listing called Páginas Brancas.\n\n==Q==\n* \'\'\'Qatar\'\'\': In [[Qatar]], the official Yellow Pages directory is called Qatar Yellow Pages and published by \'\'\'\'Primedia Qatar W.L.L\'.\'\'\' The Qatar yellowpages features comprehensive business listings for industrial and commercial establishments across the region markets. This Directory is one of the most economical media for business to showcase their products and services. The user has a choice to reference print or source online or mobile wap.\n\n==R==\n* \'\'\'Romania:\'\'\' In [[Romania]], the directory is called \'Pagini Aurii\' (Golden Pages) [[paginiaurii.ro]].\n* \'\'\'Russia:\'\'\' In Russia, KONTAKT EAST HOLDINGS (KEH.ST) established in 2006, is a Swedish holding company that owns Russian Company OOO \'\'Желтые страницы\'\' ("JOLTI STRANITSI") (Russian translation of Yellow Pages). OOO "JOLTI STRANITSI" is the result of the successful merger in 2007 of YPI YELLOW PAGES, established in 1993, a leading publisher of Yellow Pages directories in the St. Petersburg and Perm markets and Eniro RUS-M, a publisher of leading Yellow Pages directories in Moscow, Samara and 7 other Russian cities in the Urals and Volga region.\n\nOther directories in Russia include:\n\n:*\'\'Адрес Mосква\'\' (Moscow address), by ZAO [[Verlag Euro Address]];\n:*\'\'Большая Телефонная Книга Москвы\'\' (Big Phone Book of Moscow), by [[Extra M Media]];\n:*\'\'Вся Деловая Москва\'\' (all business Moscow), by [[Biznes-Karta Business Information Agency]];\n:*\'\'Московский Бизнес - Moscow Business Telephone Guide\'\' by [[Московский Бизнес - Moscow Business Telephone Guide]].\n\n==S==\n* \'\'\'Saint Kitts and Nevis\'\'\': In [[St Kitts & Nevis]], the directory is published by [[Global Directories Limited]] and titled \'\'St Kitts and Nevis Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online at www.stkittsandnevisyp.com.\n* \'\'\'Saint Lucia\'\'\': In [[Saint Lucia]], the directory is published by [[Global Directories Limited]] and titled \'\'St Lucia Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber.\n* \'\'\'Saint Vincent\'\'\': In [[Saint Vincent and the Grenadines]], the directory is published by [[Global Directories Limited]] and titled \'\'St Vincent Yellow Pages\'\'. Print copies are distributed free to each telephone subscriber and is also available online at www.stvincentyp.com.\n* \'\'\'Saudi Arabia\'\'\': In [[Saudi Arabia]], the directory is Saudianyellowpages.com\' \'Saudiarabyellowpages.com\'\',. Established in 2001, is the LARGEST yellow pages of Saudi Arabia. Yellow Pages Saudi Arabia.\n* \'\'\'Saudi Arabia\'\'\': Daleeli.com is an online business directory in [[Saudi Arabia]] to locate addresses, Phone numbers, maps, websites & locations of Business Places and offices in Saudi Arabia.\n* \'\'\'Serbia:\'\'\' In [[Serbia]], the directory is called \'\'\'Zute Strane - Yellow Pages\'\'\' (Serbian Business Directory) which is a registered trademark belonging to Yellow Pages Co. from Belgrade.\n* \'\'\'Sierra Leone:\'\'\' In [[Sierra Leone]], the online yellow pages directory, [[LeoneDirect]] powered by [[Denza, LLC.]] provides contact information for local companies.\n* \'\'\'Singapore:\'\'\' In [[Singapore]], it is known as "Yellow Pages" and is registered as a [[Public company]] under the name [[Yellow Pages Singapore|Yellow Pages (Singapore) Limited (Reg. no.:200304719G)]]. It is [[Public company|listed]] on the Singapore [[Singapore Exchange|SGX]] mainboard on 9 Dec 2004. It includes the Singapore Phone Book, the Chinese Yellow Pages and the Yellow Pages Buying and Commercial/Industrial Guides and advertisement sales. Yellow Pages Singapore also publishes and distributes niche directories and guides.\n* \'\'\'Slovakia:\'\'\' In [[Slovakia]], it is called "Zlaté stránky" (which means Golden Pages), published by [[Mediatel]] (a [[European Directories]] group company), Bratislava and is distributed free to each telephone subscriber, usually in exchange for its previous version. The online version is available at [[zlatestranky.sk]].\n* \'\'\'Slovenia:\'\'\' In [[Slovenia]], the directory is called \'\'[[Rumene strani]]\'\' (Yellow Pages) which is a registered trademark belonging to Inter Marketing.\n* \'\'\'South Africa:\'\'\' In [[South Africa]], the directory is called \'the Yellow Pages\' which is distributed by Trudon [[yellowpages.co.za]], a subsidiary of World Directories which also publishes books in Belgium, Ireland, the Netherlands, Portugal and Romania. There are 19 regional editions covering the nine provinces. Each of the four metropolitan areas has a separate white and yellow pages book. The remaining 15 areas have both sections in one book. They also have a mobile version [[pocketbook.co.za]]\n* \'\'\'Spain:\'\'\' In Spain, it is called \'\'Páginas Amarillas\'\', it was distributed by [[Telefónica|Telefónica Publicidad e Información]] S.A. Yellowpages - now known as Yell Publicidad - can also be found via the Internet Address [[www.paginasamarillas.es]]. Since July 2006 the company is owned by Yell Group from the UK. A competitor is [[www.qdq.com]], a directory edited by Pages Jaunes Group from France. Another competitor is [[citiservi]], a different yellow pages service where Professionals search for Customers requesting services. Also there is an English-speaking expat directory of businesses along the south east of Spain called [[www.littleyellowpages.com]]. This site is aimed mainly at English speaking expatriates living in Spain.\n* \'\'\'Sri Lanka:\'\'\' In [[Sri Lanka]], the official \'Yellow Pages\' publisher is produced by [[Sri Lanka Telecom]]. However, competing publishers also use the term \'Rainbow Pages\' though not the walking fingers logo.\n* \'\'\'Sweden:\'\'\' In Sweden, it is called \'\'Gula Sidorna\'\', distributed by [[Eniro]]. yellowpages.se is a portal to different Yellowpages from Sweden. Gulex.se is an alternative Swedish directory, distributed by the Norwegian company Advista. Lokaldelen i Sverige AB (a [[European Directories]] group company) provide over 250 local directories in Sweden. Also hitta.se, an Online business directory by Norwegian company [[Schibsted]].\n* \'\'\'Switzerland:\'\'\' In Switzerland the brand local.ch produces and distributes directories in several forms (printed, online and on mobile) including yellow and white pages - online available in [[German language|German]], French, [[Italian language|Italian]] and English.\n* \'\'\'Syria:\'\'\' In [[Syria]], the directory is called الصفحات الصفراء (Yellow Pages).\n\n==T==\n* \'\'\'Thailand:\'\'\' In [[Thailand]] it is called \'\'Samood Nar Leung\'\' and also called \'\'Thailand YellowPages\'\'. The company [[Teleinfo Media Public Company Limited]] produce and distribute Yellow pages nationwide. Thailand YellowPages is generated in several forms e.g. paper, Call Center no. 1188. Thailand YellowPages is produced both in Thai and English.\n* \'\'\'Tunisia:\'\'\' In [[Tunisia]], it is called "الصفحات الصفراء" (Pages Jaunes) and it is owned by "Les Editions Techniques Spécialisées", a Tunisian private company. The online version, available at [[pagesjaunes.com.tn]] for free was one of the first online directories in Arabic.\n* \'\'\'Turkey:\'\'\' Yellow Medya Istanbul based Yellow Medya.\n* \'\'\'Turkish Republic of Northern Cyprus\'\'\' In [Turkish Republic of Northern Cyprus]. Known as CYPYP it is found at [[cypyp.com]]\n* \'\'\'Turks and Caicos Islands\'\'\': In the [[Turks and Caicos Islands]] there are two telephone directories. One is published by Olympia Publishing Company, a Turks & Caicos Islands company, and carries listings from the two major telecommunications companies on the Island and the other is published by a subsidiary of Global Directories Limited, a Caymanian-based company, which carries the listings from one of the two major telecommunications companies on the Islands. Both publications are titled the \'\'Turks and Caicos Islands Yellow Pages\'\' and refer to themselves as "Local" but the Olympia Publishing Company directory is the larger and more definitive and it is the only local directory publisher.\n\n==U==\n* \'\'\'Ukraine\'\'\': In [[Ukraine]], the free business directory is titled \'\'PromUA\'\' ([[Russian language|Russian]]), it is available online at [[prom.ua]]. Other directories are: ukrindustrial.com, ukrbiznes.com, [[ukrpartner.com]].\n* \'\'\'United Arab Emirates:\'\'\' Dubai-based Local Search UAE is an Online Business Directory UAE where all businesses across Abu Dhabi, Al Ain, Dubai, Fujairah, Sharjah, Ras Al Khaimah & Umm Al Quwain are listed and can be searched.\n* \'\'\'United Arab Emirates:\'\'\' Dubai-based [[Express Print (Publishers) L.L.C.]] is the official publisher of \'\'\'Etisalat Yellow Pages\'\'\' branded products in the UAE. Express Print (Publishers) L.L.C. publishes the Yellow Pages in both print and electronic formats. Etisalat Yellow Pages print edition consists of 3 regional directories for the areas of Abu Dhabi, Dubai and the Northern Emirates. Directories are published annually and distributed towards the end of the first quarter of each year. Express Print (Publishers) L.L.C. also publishes the Etisalat Yellow Pages on 2 electronic platforms -Online & Mobile.\n* \'\'\'United Arab Emirates:\'\'\' As of late 2016, Dubai-based [[ZOSER MEA]] is the official publisher of [[du Yellow Pages]] branded products in the UAE. ZOSER MEA publishes the Yellow Pages in both print and electronic formats. Directories are published annually and distributed in the month of January each year.\n* \'\'\'United Arab Emirates:\'\'\' In [[United Arab Emirates]], the directory is titled \'\'Yellow Page Gulf UAE\'\',. Established in January 2011, is the LARGEST yellow pages of GULF. Yellow Pages Gulf.\n* \'\'\'United Kingdom:\'\'\' The first Yellow Pages directory in the UK was produced by the [[Kingston upon Hull|Hull]] Corporation\'s telephone department (now [[Kingston Communications]]) in 1954. This was distributed with the alphabetical phone directory rather than as a stand-alone publication. The company now produces [[Hull Colour Pages|The Hull Colour Pages]].\n\n:With the encouragement of [[The Thomson Corporation]], at the time an advertising sales agent for the nationalised [[General Post Office (United Kingdom)|General Post Office]]\'s [[telephone directory]], a business telephone number directory named the Yellow Pages was first produced in 1966 by the GPO for the [[Brighton]] area, and was rolled out nationwide in 1973. The Thomson Corporation formed Thomson Yellow Pages in 1966 to publish and to distribute the directory to telephone subscribers for the GPO, and later for [[Royal Mail|The Post Office]].\n\n:Thomson Yellow Pages was sold by The Thomson Corporation in 1980, at the same time as Post Office Telecommunications became the (then) state-owned [[British Telecom]] (BT). The Yellow Pages directory continued to be distributed to all telephone subscribers by BT. At the same time, The Thomson Corporation formed Thomson Directories Ltd, and began to publish the [[Thomson Local]] directory, which would remain the Yellow Pages\' main, and often sole, competitor in the UK for more than the next two decades, and would be the competitive driving force behind such changes to Yellow Pages as the adoption (in 1999) of colour printing and "white knock out" listings.\n\n:In 1984 the year that BT was privatised, the department producing the directory became a stand alone subsidiary of BT, named Yellow Pages. In the mid-1990s the Yellow Pages business was re-branded as [[Yell Group|Yell]], although the directory itself continued to be known as the Yellow Pages.\n\n:Yell was bought by venture capitalists in 2001, and in 2003 was floated on the Stock Exchange. After the one year "no competition" clause expired BT too went into competition with the Yellow Pages, re-entering the market by adding similar content to their existing directory, "The Phone Book", adding a classified section to the traditional alphabetical domestic and business listings.\n\n:Yellow Pages, [[Thomson Local]] and BT\'s [[The Phone Book]] display advertising and can be booked directly with advertising sales representatives.\n\n:Nowadays the KC Yellow Pages is referred to as [[Hull Colour Pages]], and is separate from the White Pages. Yell now also publishes an East Yorkshire edition of Yellow Pages in competition.\n\n[[Image:Bsyps.png|right|Bell System Yellow Pages Logo]]\n* \'\'\'United States:\'\'\' In the past, AT&T, Verizon, and Qwest, the three largest phone companies in the U.S., dominated the U.S. yellow pages industry; however, the term "yellow pages" and the \'\'Walking Fingers\'\' logo was heavily marketed by AT&T pre [[Bell System Divestiture|divestiture]]. However, AT&T never filed a trademark registration application for the current and most recognized version of the \'\'Walking Fingers\'\' logo, so it is in the public domain. AT&T allowed the "independent yellow pages" industry to use the logo freely.<ref>[http://www.ll.georgetown.edu/federal/judicial/fed/opinions/9_opinions/91-1461.html Bellsouth v. Datanational]</ref> The "independents" are unrelated to the incumbent phone company and are either pure advertising operations with no phone infrastructure or telephone companies who provide local telephone service elsewhere. Such independents include operators who typically focus on industry or business segments, or local market directories.\n\n:Yellow pages publishers or their agents sell the right to place advertisements within the same category, next to the basic listings.\n\n:For example, [[AT&T]] is the dominant local telephone service provider in [[California]], but since Bell Atlantic and [[GTE]] merged to become [[Verizon]], it now provides service in many pockets such as [[West Los Angeles (region)|West Los Angeles]]. [[Los Angeles]] telephone users can select from telephone directories published by AT&T, Verizon (published by [[Idearc Media|Idearc]]), Yellow Book USA, PDC Pages (Phone Directories Company) [[PDC Pages.com]] and other independent publishing companies. [[R. H. Donnelley]] is also in local markets across country with Dex Printed Directories and [[DexKnows.com]]. In Northern California, Valley Yellow Pages [[MyYP.com]] is a large regional independent publisher. Additionally, in the smaller markets, many yellow pages publishers are beginning to offer directories catering to specific niche business or industry segments, such as automotive, manufacturing, environmental/green products, imports, exports, and the like. One such example is the [[Export Yellow Pages]] (a yellow page directory published in partnership with the US Department of Commerce that focuses on U.S. exporters) and vertical directories offered by Yellow Pages Nationwide, Inc. Media an Online Digital Yellow Pages company, Consolidation and M&A activity in the directory publishing market continues to remain very high in the U.S. and there is an increasing move toward internet based directories as internet usage for search increases and concerns over the possible negative environmental effects of the books becomes more evident.\n: [[Yellowpages.com]] LLC is a subsidiary of AT&T.\n\n* \'\'\'Uzbekistan:\'\'\' In [[Uzbekistan]], the directory is called \'\'Yellow Pages of Uzbekistan\'\', published by Yellow Pages Ltd.\n\n==V==\n* \'\'\'Vietnam:\'\'\' In [[Vietnam]], the official title "Telephone Directory & Yellow Pages\'\' in English and \'\'Nien Giam Dien Thoai Nhung Trang Vang va Nhung Trang Trang "in Vietnamese are produced and distributed nationwide by [http://yp.vn/ VietNam Yellow Pages Media JSC].\n\nVietnam Business Yellow Pages in Vietnamese and English is directory of Vietnam Business.\n\n==Notes and references==\n{{reflist}}\n\n==See also==\n* [[Telecommunications service]]\n* [[Yellow pages]]\n\n[[Category:Directories|*]]\n[[Category:Yellow pages|*]]']
['Spotlight (Casting Services Company)', '5813520', "{{Use dmy dates|date=September 2015}}\n{{Use British English|date=September 2015}}\n{{multiple issues|\n{{no footnotes|date=January 2014}}\n{{Primary sources|date=April 2013}}\n}}\n[[File:Cinema Museum, London object 15.JPG|thumb|Spotlight volumes preserved at the Cinema Museum, London.]]\n\t\n'''Spotlight''' was founded in 1927 and is the [[United Kingdom|UK's]] largest casting resource. Over 60,000 performers appear in Spotlight, including [[actor]]s and actresses, [[child actor|child artist]]s, [[presenter]]s, [[dancer]]s, and [[stuntman|stunt artists]]. Thousands of [[production company|production companies]], [[broadcasting|broadcasters]], [[advertising agency|ad agencies]], and independent casting directors use Spotlight as a casting resource. Their clients range from large organisations such as the [[BBC]], [[ITV (TV network)|ITV]], and [[Channel 4]], to small production companies and individual casting [[Television director|directors]].\n\nSpotlight also publishes the [[handbook]] Contacts both in hard copy and as an [[e-book]]. It includes listings for over 5000 [[company|companies]], services and individuals across all branches of [[television]], [[theatre|stage]], [[film industry|film]], and [[radio]].\n\n==External links==\n* [http://www.spotlight.com Spotlight website]\n* [http://www.contactshandbook.com Contacts website]\n\n{{UK-stub}}\n{{DEFAULTSORT:Spotlight}}\n[[Category:Directories]]\n[[Category:1927 establishments in the United Kingdom]]\n[[Category:Casting companies]]\n\n{{advertising-stub}}"]
["White's Directories", '40453381', "{{italic title}}\n'''''White's Directories''''' were a series of directory publications issued by William White of [[Sheffield]], England, beginning in the 1820s.<ref>{{citation |chapter=White's Directories (advert) |quote=Established 1822 |url= https://books.google.com/books?id=dMwUAAAAQAAJ&pg=PA83 |title=White's general and commercial directory of Hull |year=1882}}</ref><ref>{{Citation |url = http://openlibrary.org/books/ia:pigotcosnational1837dire/Pigot_and_Co.'s_national_commercial_directory_for_the_whole_of_Scotland_and_of_the_Isle_of_Man_..._t |title = Pigot and Co.'s National Commercial Directory for the Whole of Scotland and of the Isle of Man, ... Manchester, Liverpool, Leeds, Hull, Birmingham, Sheffiled, Carlisle, and Newcastle-upon-Tyne |publication-date = 1837 |location = London |publisher =[[James Pigot|J. Pigot & Co.]] }}</ref> White began his career in publishing by working for [[Edward Baines (1774–1848)|Edward Baines]].<ref>{{cite journal |title=Locational Behaviour of Urban Retailing during the Nineteenth Century: The Example of Kingston upon Hull |author= M. T. Wild and G. Shaw |journal=Transactions of the Institute of British Geographers |number= 61 |year=1974 |jstor=621602 }}</ref>{{refn|group=nb|By the 1850s Sheffield had two professional directory publishers: William White (34 Collegiate Crescent, Broomhall Park) and Francis White (Broomhall Terrace, 104 Ecclesial New Road)<ref>{{cite book |title=Post office directory of Sheffield |year=1854 |publisher=Kelley and Co. |url=https://books.google.com/books?id=bO4NAAAAQAAJ }}</ref><ref>{{Citation |publisher = W. Satchell |publication-place = London |title = Book of British Topography: a Classified Catalogue of the Topographical Works in the Library of the British Museum Relating to Great Britain and Ireland |author = John Parker Anderson |publication-date = 1881 |chapter=Yorkshire: Sheffield |chapterurl=https://archive.org/stream/bookofbritishtop00andeuoft#page/327/mode/1up }}</ref>}}\n\n==Notes==\n{{reflist|group=nb}}\n\n==References==\n{{reflist}}\n\n==Further reading==\n\n=== 1820s-1830s ===\n* {{cite book |title=History, directory, and gazetteer, of the counties of Durham and Northumberland, and the towns and counties of Newcastle-upon-Tyne and Berwick-upon-Tweed |location=Newcastle |publisher= Printed for W. White & Co. by E. Baines and Son |year= 1827–1828 |url= http://catalog.hathitrust.org/Record/009725890 }}\n* {{cite book |title=Directory of the Borough of Leeds, the City of York, and the Clothing District of Yorkshire |location=Leeds |publisher= Printed for Wm. Parson & Wm. White by Edward Baines and Son |year= 1830 |url=http://catalog.hathitrust.org/Record/007973427 }}\n* {{Citation |publisher = Printed for the author by R. Leader |title = History, Gazetteer, and Directory of Norfolk, and the City and County of the City of Norwich |url = http://openlibrary.org/books/OL20613547M/History_Gazetteer_and_Directory_of_Norfolk_and_the_City_and_County_of_the_City_of_Norwich_... |author = William White |publication-date = 1836 |oclc = 25166377 }}\n** [https://archive.org/stream/historygazettee01whitgoog#page/n3/mode/2up 1845 ed.]\n** [https://archive.org/stream/historygazetteer00whit#page/n5/mode/2up 1864 ed.]\n* {{Citation |publisher = W. White |publication-place = Sheffield |author =William White |title = History, Gazetteer, and Directory, of the West-Riding of Yorkshire, with the City of York and Port of Hull |publication-date = 1837 |url=https://archive.org/stream/historygazetteer01whit#page/n5/mode/2up }}\n** {{cite book |title=History, gazetteer and directory of the East and North Ridings of Yorkshire |author=William White |location= Sheffield |publisher= Robert Leader for the author |year= 1840 |url=http://catalog.hathitrust.org/Record/011724851 }}\n\n=== 1840s ===\n* {{cite book |title=History, gazetteer, and directory of Suffolk, and the towns near its borders |location=Sheffield |publisher= Printed for the author by R. Leader and sold by W. White |year=1844 |url= http://catalog.hathitrust.org/Record/000194916 }}\n** [http://catalog.hathitrust.org/Record/011595374 1874 ed.]\n* {{Citation |url = http://openlibrary.org/books/ia:generaldirectory00whit/General_directory_of_the_town_and_borough_of_Sheffield_with_Rotherham_Chesterfield_and_all_the_paris |title = General directory of the town and borough of Sheffield |publication-date = 1845 |publisher = William White |location=Sheffield }}\n* {{Citation |publisher = Printed for the author, by R. Leader |publication-place = Sheffield |author = William White |url = http://openlibrary.org/books/OL14012344M/History_gazetteer_and_directory_of_Leicestershire_and_the_small_county_of_Rutland |title = History, gazetteer, and directory of Leicestershire, and the small county of Rutland |publication-date = 1846 }}\n\n=== 1870s ===\n* {{cite book |title=History, gazetteer and directory of Lincolnshire, and the city and diocese of Lincoln |location=Sheffield |publisher= W. White |year= 1872 |url=http://catalog.hathitrust.org/Record/008912723 }}\n* {{Citation |publisher = W. White |publication-place = Sheffield |title = History, gazetteer and directory of the county of Hampshire, including the Isle of Wight |url = http://catalog.hathitrust.org/Record/009009769 |publication-date = 1878 }}\n* {{Citation |publisher = William White |publication-place = Sheffield |title = History, Gazetteer and Directory of the County of Devon including the City of Exeter |url = http://openlibrary.org/books/OL14012345M/History_gazetteer_and_directory_of_the_County_of_Devon_including_the_City_of_Exeter_and_comprising_a |publication-date = 1878 |edition=2nd }}\n\n==External links==\n* {{citation |title=Historical Directories |publisher=[[University of Leicester]] |location=UK |url=http://www.historicaldirectories.org/hd/findbykeyword.asp }}. Includes digitized White's directories, various dates\n* {{citation |work=WorldCat |url=http://www.worldcat.org/wcidentities/lccn-n50-38455 |title=William White of Sheffield }}\n\n[[Category:Directories]]\n[[Category:Publications established in the 1820s]]\n\n\n{{ref-book-stub}}"]
["Writers' & Artists' Yearbook", '37966541', '[[File:Writers\' & Artists\' Yearbook cover 2003.jpg|thumb|150px|2003 edition of \'\'Writers\' & Artists\' Yearbook\'\']]\n{{Italic title}}\'\'\'\'\'Writers\' & Artists\' Yearbook\'\'\'\'\' is an annual directory for writers, designers, illustrators and photographers. It is published in the UK each July, with a separate version for children\'s writers and artists published in August. The yearbook contains some 4,500 named industry contacts updated for each edition and includes articles about getting work published.<ref Name=BBC>[http://news.bbc.co.uk/dna/place-lancashire/plain/A16932017 "The Writers\' and Artists\' Yearbook", BBC]</ref><ref name="Irish Times">{{cite news|title=Essential Reading for Writers|newspaper=Irish Times|date=13 September 2003}}</ref> In 2007, an associated website, known as Writers&Artists, was launched.<ref name="website launch">{{cite web|title=New website with free resources for writers and artists|url=http://www.publishers.org.uk/index.php?option=com_content&view=article&id=554:new-website-with-free-resources-for-writers-and-artists&catid=80:general-news&Itemid=1617|publisher=Publishers Association|accessdate=2 March 2014}}</ref>\n\n== History ==\n\nFirst published in 1906, by [[A & C Black|Adam & Charles Black]], the original \'\'Writers’ & Artists’ Yearbook\'\' was an 80-page booklet, costing one [[shilling]]. It gave details of seven literary agents and 89 publishers.<ref Name=BBC/> It has been published on an annual basis since, expanding over time to include information for illustrators and photographers.<ref Name=BBC/> A & C Black became part of [[Bloomsbury Publishing]] in 2000, and other titles in its reference division include \'\'[[Who\'s Who (UK)|Who\'s Who]]\'\', \'\'[[Wisden Cricketers\' Almanack|Wisden]]\'\' and \'\'[[Black\'s Medical Dictionary]]\'\'.<ref name="A & C Black">{{cite news|last=Neill|first=Graeme|title=Coleman to leave A & C Black for Magi|url=http://www.thebookseller.com/news/coleman-leave-c-black-magi.html|accessdate=2 March 2014|newspaper=The Bookseller|date=2 February 2011}}</ref>\nArticles offering advice first appeared in the 1914 yearbook.<ref Name=BBC/> Forewords have been written by, among others, [[William Boyd (writer)|William Boyd]] and [[Kate Mosse]].<ref name=A&U>{{cite web|title=Writers\' and Artists\' Yearbook 2013|url=https://www.allenandunwin.com/default.aspx?page=305&book=9781408157497|publisher=Allen & Unwin|accessdate=2 March 2014}}</ref><ref name=Bibliography>{{cite web|last=Mosse|first=Kate|title=Complete Bibliography|url=http://www.katemosse.co.uk/index.php/kates-books/|publisher=Kate Mosse|accessdate=2 March 2014}}</ref> Following the success of \'\'[[Fifty Shades of Grey]]\'\', a new section on writing erotic fiction – by an anonymous author – appeared in the 2014 edition.<ref name="Fifty Shades">{{cite news|last=Wyatt|first=Daisy|title=Fifty Shades of Grey inspires new chapter on erotic fiction in Bloomsbury Writers\' and Artists\' Yearbook|url=http://www.independent.co.uk/arts-entertainment/books/news/fifty-shades-of-grey-inspires-new-chapter-on-erotic-fiction-in-bloomsbury-writers-and-artists-yearbook-8685703.html|accessdate=2 March 2014|newspaper=The Independent|date=3 July 2013}}</ref>\n\n=== Website and competitions ===\n\nIn 2007, \'\'Writers\' & Artists\' Yearbook\'\' launched an associated website. Initially this was only accessible to anyone purchasing the print edition.<ref name="website launch"/> In 2009, the website was relaunched and now includes blogs from guest authors and a social networking feature that enables authors and artists to add a public profile.<ref name=Bookseller>{{cite news|last=Gallagher|first=Victoria|title=Writers and Artists Yearbook launches social networking|url=http://www.thebookseller.com/news/writers-and-artists-yearbook-launches-social-networking.html|accessdate=2 March 2014|date=7 August 2009}}</ref> From 2013, the website featured a section focusing on [[self-publishing]], also hosting a conference on the subject in November of that year in association with [[National Novel Writing Month]].<ref name=self-publish>{{cite news|title=Self-published writers get online resource|url=http://www.thebookseller.com/news/self-published-writers-get-online-resource.html|accessdate=2 March 2014|newspaper=The Bookseller|date=27 September 2013}}</ref>\n\'\'Writers\' & Artists\' Yearbook\'\' runs an annual short story competition and has also collaborated with Bloomsbury to run a competition for aspiring crime writers.<ref>{{cite web|title=Writers’ & Artists’ Yearbook 2014 Short Story Competition|url=http://www.commonwealthwriters.org/writers-and-artists-short-story-competition-2014/|publisher=Commonwealth Writers|accessdate=2 March 2014}}</ref><ref name="Book Trust">{{cite web|title=Prizes|url=http://www.booktrust.org.uk/books/adults/short-stories/prizes/|publisher=Book Trust|accessdate=2 March 2014}}</ref><ref name="crime competition">{{cite news|last=Williams|first=Charlotte|title=Bloomsbury launches crime story competition|url=http://www.thebookseller.com/news/bloomsbury-launches-crime-story-competition.html|accessdate=2 March 2014|newspaper=The Bookseller|date=1 March 2012}}</ref>\n\n== Sections and listings ==\n\nThe yearbook is divided into the following sections:<ref Name=BBC/>\n* Newspapers and magazines – regional, national and overseas, [[Print syndication|syndicates]] and [[News agency|news agencies]]\n* Books – regional, national and overseas, audio publishers, [[Book packaging|book packagers]] and [[Book sales club|book clubs]]\n* Poetry organisations\n* Television, film and radio broadcasters\n* Theatre – producers\n* [[Literary agent]]s \n* Art and illustration – agents, commercial studios and card and stationery publishers \n* Societies, prizes and festivals – associations and clubs, prizes and awards and [[literary festival]]s\n* Digital and self-publishing\n* Resources for writers – courses, libraries and writers\' retreats\n* Copyright and libel information\n* Finance for writers and artists.\n\n== See also ==\n\n* \'\'[[Writer\'s Digest]]\'\'\n* \'\'[[Novel & Short Story Writer\'s Market]]\'\'\n\n== References ==\n\n{{reflist|2}}\n\n== External links ==\n*[http://www.writersandartists.co.uk/ Writers&Artists website]\n\n{{DEFAULTSORT:Writers\' and Artists\' Yearbook}}\n[[Category:Directories]]\n[[Category:1906 establishments in the United Kingdom]]\n[[Category:Handbooks and manuals]]\n[[Category:Yearbooks]]\n[[Category:Publishing]]\n[[Category:A & C Black books]]']
['Category:Domain name system', '5737409', '{{Cat main|Domain Name System}}\n{{Commonscat|Domain name system}}\n\n[[Category:Internet governance]]\n[[Category:Internet Standards]]\n[[Category:Internet architecture]]\n[[Category:Network addressing]]\n[[Category:Application layer protocols]]\n[[Category:Directories]]\n\n[[ms:Kategori:Sistem nama domain]]']
['Category:Lists', '691070', '{{Commons category|Information lists}}\n{{Category see also|Timelines}}\n{{Category diffuse}}\n\n[[Category:Wikipedia navigation]]\n[[Category:Directories]]']
['Index Herbariorum', '44490466', "{{Italic title}}\nThe '''Index Herbariorum''' provides a global directory of [[herbaria]] and their associated staff. This searchable online index allows scientists rapid access to data related to 3,400 locations where a total of 350&nbsp;million botanical [[Biological specimen|specimens]] are permanently housed (singular, [[herbarium]]; plural, herbaria). The Index Herbariorum has its own staff and website. Overtime, six editions of the Index were published from 1952 to 1974. The Index became available on-line in 1997.<ref name=IH>{{cite web|url=http://sciweb.nybg.org/science2/IndexHerbariorum.asp|title=Index Herbariorum|publisher=sciweb.nybg.org|accessdate=2014-11-23}}</ref>\n\nThe index was originally published by the [[International Association for Plant Taxonomy]], which sponsored the first six editions (1952–1974); subsequently the [[New York Botanical Garden]] took over the responsibility for the index. The Index provides the supporting institution's name (often a university, botanical garden, or not-for-profit organization) its city and state, each herbarium's acronym, along with contact information for staff members along with their research specialties and the important holdings of each herbarium's collection.\n\n==Editors==\n*6th edition (1974)  was co-edited by [[Patricia Holmgren]], Director of the  New York Botanical Garden, and\n*7th printed edition ed. by  Patricia Holmgren. \n*8th printed editions, ed. by  Patricia Holmgren.\n*Online edition, prepared by Noel Holmgren of the New York Botanical Garden\n*2006+, ed. by Barbara M. Thiers, Director of the New York Botanical Garden  Herbarium <ref name=IH />\n<ref name=IH />\n\n==References==\n{{Reflist}}\n\n[[Category:Directories]]\n[[Category:Herbaria]]\n\n\n{{botany-stub}}"]
['Western Australia Post Office Directory', '6013482', "{{Use Australian English|date=March 2015}}\n{{Use dmy dates|date=March 2015}}\n\nThe '''''Western Australia Post Office Directory''''', also known as ''Wise Directories'' or ''Wise Street Directories'' were published in Perth from 1893-1949.\n\nThey were published by H. Pierssené<ref>{{Citation | author1=Pierssené, Herbert | title=The Western Australian directory | publication-date=1893 | publisher=H. Pierssene | url=http://trove.nla.gov.au/work/28621466 | accessdate=6 March 2015 }}</ref> and later by H. Wise & Co.<ref>{{Citation | author1=Wise & Co | title=Wise's Western Australia post office directory | publication-date=1938 | publisher=H. Wise & Co. Pty Ltd | url=http://trove.nla.gov.au/work/19293522 | accessdate=6 March 2015 }}</ref>  They listed household, business, society, and Government contacts in [[Perth]], [[Freemantle]], [[Kalgoorlie, Western Australia|Kalgoorlie]], [[Boulder, Western Australia|Boulder]] and [[Coolgardie, Western Australia|Coolgardie]] including some rural areas of [[Western Australia]].\n\n==Publishers==\nThe ''Western Australian Directory'' was published by H. Pierssene between 1893-1895. Herbert Pierssene was a merchant and importer of English Continental and Ceylonese goods. He was an agent for McCulluch Carrying Company and a bottler of West Australian wines.<ref>{{cite web|title=Thomas Herbert Pierssené|url=http://www.territorystories.nt.gov.au/handle/10070/244383|website=Territory Stories|publisher=Northern Territory Department of Arts and Museums|accessdate=6 March 2015}}</ref>\n\nThe ''Western Australia Post Office Directory'' was published by Wise & Co. between the years 1895-1949 with the exception of 1943 and 1948.\n\n==Wise Directories== \t\nThe directories provide information by locality, individual surname, government service, and by trade or profession. The addresses of householders and businesses throughout Western Australia are included.<ref>{{cite news |url=http://nla.gov.au/nla.news-article77356821 |title=POST-OFFICE DIRECTORY. |newspaper=[[Daily_News_(Perth,_Western_Australia)|The Daily News (Perth, WA : 1882 - 1950)]] |location=Perth, WA |date=27 April 1909 |accessdate=6 March 2015 |page=2 Edition: THIRD EDITION |publisher=National Library of Australia}}</ref>  Maps were sometimes published with an edition of the directory.<ref> {{cite news |url=http://nla.gov.au/nla.news-article77329156 |title=WESTERN AUSTRALIA DIRECTORY. |newspaper=[[Daily_News_(Perth,_Western_Australia)|The Daily News (Perth, WA : 1882 - 1950)]] |location=Perth, WA |date=5 March 1908 |accessdate=6 March 2015 |page=6 Edition: THIRD EDITION |publisher=National Library of Australia}}</ref>  The towns section of the directories normally contained separate street directories of Perth and suburbs, Fremantle and Suburbs, Kalgoorlie, Boulder and Coolgardie.<ref>{{Citation | author1=Wise's Directories | author2=Archive CD Books Australia | title=Western Australia Post Office directory (Wise's) 1905 | publication-date=2004 | publisher=Archive CD Books Australia | isbn=978-1-920978-23-5 }}</ref>\n\nKnown colloquially to users and  book collectors as 'Wise Directories' or 'Wise Street Directories' the red covered directories were published between 1893 and 1949.  Due to the annual changes, the directories are valuable historical documents for Western Australian History.  They are scarce in the Australian rare book market.  \n\nThe directories have been invaluable referent points for such projects as the [[Dictionary of Western Australians]] and others where the street lists in the directory provide details of inhabitants and houses in some streets in the more built-up residential areas.  Country towns in the directory have name lists only. \n\nThey have been available in microfilm form in [[J S Battye Library]], and more recently have become online (see link below) in one of the J S Battye Library digitization projects.\n\n==References==\n{{reflist}}\n\n==External links==\n* http://www.slwa.wa.gov.au/find/wa_resources/post_office_directories\n\n==See also==\n* [[Australia Post]]\n*[[Australian Dictionary of Biography]]\n*[[Cyclopedia of Western Australia]]\n*[[Dictionary of Australian Biography]]\n*[[J S Battye Library]]\n*[[State Records Office of Western Australia]]\n\n[[Category:Books about Western Australia]]\n[[Category:History of Western Australia|Western Australia Post Office Directory]]\n[[Category:Australian directories]]\n[[Category:Directories]]\n[[Category:Gazetteers]]\n[[Category:Postal system of Australia]]"]
['Category:Internet search engines', '699876', '{{Commons category|Internet search engines}}\nGeneral [[search engine (computing)|search engine]]s that search for information on the [[Internet]]. \n\n[[Category:Websites|Search engines]]\n[[Category:Internet search]]\n[[Category:Online databases]]\n[[Category:Indexes]]\n[[Category:Aggregation websites]]\n[[Category:Search engine software]]']
['Category:Real-time web', '23686083', '{{Cat main|Real-time web}}\n\n[[Category:Internet search]]\n[[Category:Real-time computing|web]]']
['Hyper Search', '11853249', '\'\'\'Hyper Search\'\'\' has been the first{{cn|date=May 2013}} published technique to introduce [[link analysis]] for search engines. It was created by Italian researcher [[Massimo Marchiori]].\n\n==Bibliography==\n\n* [[Massimo Marchiori]], [http://www.w3.org/People/Massimo/papers/WWW6/ "The Quest for Correct Information on the Web: Hyper Search Engines"], \'\'Proceedings of the Sixth International World Wide Web Conference (WWW6)\'\', 1997.\n* [[Sergey Brin]] and [[Lawrence Page]], [http://www-db.stanford.edu/~backrub/google.html "The anatomy of a large-scale hypertextual Web search engine"], \'\'Proceedings of the Seventh International World Wide Web Conference (WWW7)\'\', 1998. \n\n== See also ==\n* [[PageRank]]\n* [[Spamdexing]]\n\n[[Category:Internet search]]\n\n{{web-stub}}']
['Instant indexing', '6111052', "{{Orphan|date=February 2009}}\n\n'''Instant indexing''' is a feature offered by [[Internet]] [[search engine]]s that enables users to submit content for immediate inclusion into the [[search engine indexing|index]].\n\n==Delayed inclusion==\nCertain search engine services may require an extended period of time for inclusion, which is seen as a delay and a frustration by [[website]] administrators who wish to have their websites appear in [[search engine results page|search engine results]].<ref>{{cite web|url=https://www.youtube.com/watch?v=HBDC35Vgj34|title=How to index your domains|accessdate=2015-12-24}}</ref>\n\nDelayed inclusion may due to the size of the index that the service must maintain or due to corporate, political or social policies{{Citation needed|date=February 2007}}. Some services, such as [[Ask.com]] only index content collected by a [[web crawling|crawler program]] which does not allow for manual adding of content to index.<ref>{{cite web|url=https://www.smartz.com/web-marketing/search-engine-optimization/submit-site-to-search-engines/|title=How to Submit Your Site to Search Engines|accessdate=2015-12-24}}</ref>\n\n==Criticisms==\nA criticism of instant indexing is that certain services filter results manually or via algorithms that prevent instant inclusion to avoid inclusion of content that violates the service's policies.{{Citation needed|date=February 2007}}\n\nInstant indexing impacts the timeliness of the content included in the index. Given the manner in which many [[web crawling|crawlers]] operate in the case of Internet search engines, websites are only visited if a some other website links to them. Unlinked web sites are never visited (see [[invisible web]]) by the crawler because it cannot reach the website during its traversal. It is assumed that unlinked websites are less authoritative and less popular, and therefore of less quality. Over time, if a website is popular or authoritative, it is assumed that other websites will eventually link to it. If a search engine service provides instant indexing, it bypasses this quality control mechanism by not requiring incoming links. This infers that the search engine's service produces lower quality results.\n\nSelect search services that offer such a service typically also offer [[paid inclusion]], also referred to as [[pay per click|inorganic search]]. This may reduce the quality of search results.\n\n==External links==\n* {{cite web | url = http://www.web-cite.com/search_marketing/000078.html | title = Don't Blink: Instant Indexing? | publisher = Web-Cite Exposure | date = 2003-03-26 | accessdate = 2006-09-23 |archiveurl = http://web.archive.org/web/20060427184004/http://www.web-cite.com/search_marketing/000078.html <!-- Bot retrieved archive --> |archivedate = 2006-04-27}}\n* {{cite web | url = http://www.earthstation9.com/index.html?us_searc.htm | title = The Wonderful World of Search Engines and Web Directories — A Search Engine Guide | author = Stan Daniloski | publisher = Earth Station 9 | date = 2004-09-17 | accessdate = 2006-09-23}}\n\n== See also ==\n* [[Search engine]]\n* [[Search engine indexing]]\n* [[Web crawling]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Internet terminology]]\n[[Category:Internet search]]\n\n\n{{website-stub}}"]
['Figaro Systems', '17910258', '{{Infobox company\n|name = Figaro Systems, Inc.\n|logo = [[Image:Figaro-logo.png|Figaro logo]]\n|type = [[Privately held company|Private]]\n|foundation = 1993\n|location_city = [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]]\n|location_country =[[United States]]\n|key_people = Patrick Markle, [[president]] and [[CEO]], [[Geoff Webb]], [[vice president|VP]]\n|homepage = [http://www.figarosystems.com figarosystems.com]\n}}\n\n\'\'\'Figaro Systems, Inc.\'\'\' is an American company that provides  seatback and [[wireless]] titling [[software]] and system installations to [[opera houses]] and other music performance venues worldwide. The company is based in [[Santa Fe, New Mexico|Santa Fe]], New Mexico. It was established in 1993 <ref>Andrew Webb, “Opera Subtitle Firm Eyes New Game,” \'\'New Mexico Business Weekly\'\', Nov. 21, 2003 [http://www.bizjournals.com/albuquerque/stories/2003/11/24/story2.html]</ref>\nby Patrick Markle, [[Geoff Webb]], and Ron Erkman  <ref name="figaro-systems.com"/> and was the first company to provide [[assistive technology]] that enables individualized, simultaneous, multi-lingual [[dialogue]] and [[libretto]]-reading for audiences.\n<ref>[http://www.highbeam.com/DocPrint.aspx?DocID=1P2:115622912 David Belcher, “Nothing Lost in Translation: [[Video]] system allows patrons to read words on chair backs,”] \'\'Albuquerque Journal\'\', June 4, 2006</ref>\n\n==History==\nFigaro Systems grew out of a conversation in 1992 among three opera colleagues: Patrick Markle, at that time Production Director of The [[Santa Fe Opera]], Geoffrey Webb, Design Engineer for the [[Metropolitan Opera House (Lincoln Center)|Metropolitan Opera House]] in New York, and Ronald Erkman, then a technician for the Met. At that time, opera houses had two options for the display of libretto and dialogue subtitles: projection onto a large screen above the stage or onto smaller screens throughout the theatre. Typically, the translation was in a single language.<ref>[http://www.bizjournals.com/albuquerque/stories/2005/04/11/story5.html?q=Figaro%20Systems Dennis Domrzalski, "Figaro: Eyes translate when ears don\'t get it",] \'\'New Mexico Business Weekly\'\', April 8, 2005</ref>\n\nThe [[Americans with Disabilities Act of 1990]] had recently been enacted; Markle was trying to solve the problem of venues which lacked accessibility to patrons with disabilities, including the profoundly [[deaf]].  Markle, Webb, and Erkman devised the first [[prototype]] of a personal seatback titling device and [[John Crosby (conductor)|John Crosby]], then General Director of The [[Santa Fe Opera]], saw its potential for opera patrons.<ref name="figaro-systems.com">[http://www.figaro-systems.com/about.php  Figaro Systems Official Website]</ref> Markle, Webb, and Erkman were further reinforced by their understanding of technology’s role in remediating the physical barriers people encounter, worldwide, which frustrate or prevent their access to the visual performing arts.<ref>[http://figarosystems.com/linkdownloads/052007_figaro_auditoria_article.pdf “[[User-friendly]] art: In-seat text displays that subtitle and translate”, \'\'Auditoria\'\', May 2007]</ref> Markle, Webb, and Erkman applied for and were granted [[patent]]s for their invention.\n<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=11&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 5,739,869, "Electronic libretto display apparatus and method," issued April 14, 1998. [[United States Patent and Trademark Office]] ]</ref><ref>[http://www.lanl.gov/orgs/pa/News/050701.html  Los Alamos Laboratory, \'\'Daily News Bulletin\'\', May 7, 2001]</ref>\n\nPhilanthropist and investor [[Alberto Vilar]] counted Figaro Systems among the companies in which he was a majority shareholder.<ref>[http://nymag.com/nymetro/arts/music/features/5616/ [[Robert Hilferty]], "A Knight at the Opera," \'\'[[New York Magazine]]\'\', January 14, 2002]</ref><ref>[http://biography.jrank.org/pages/3490/Vilar-Alberto-1940-Investor-Philanthropist-Privileges-Wealth.html  "Alberto Vilar: The Privileges of Wealth," \'\'The Free Encyclopedia\'\']</ref>  He donated the company\'s [[electronic libretto]] system to European venues including the [[Royal Opera House]] in [[London]], La Scala\'s [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]]. As a consequence of his failures to pay promised donations, most of these companies lost money.\n\nIn 2005 the Met charged the New Mexico company with unlawfully using its name in advertising promoting its "Simultext, system which defendant claims can display a simultaneous translation of an opera as it occurs on a stage and that defendant represented that its system is installed at the Met." <ref>[http://classactionlitigation.com/library/consumerlaw2006update.html#_edn173#_edn173 Timothy E. Eble, \'\'Class Action Litigation Information\'\'] on classactionlitigation.com</ref>\n\n==Products and technology==\nThe company’s products are known variously as seat back titles, [[surtitles]],\n<ref>[http://app1.kuhf.org/houston_public_radio-news-display.php?articles_id=20614 Eric Skelly, "Surtitles at the Opera," \'\'Public Radio News and Information in Houston, Texas\'\', KUHF 88.7 FM Houston Public Radio] on app1.kuhf.org/</ref> [[electronic libretto]] systems, opera supertitles, projected titles, and libretto translations.\n\nOpera venues have utilized the system to display librettos in [[English language|English]], [[French language|French]], [[German language|German]], [[Italian language|Italian]], [[Japanese language|Japanese]], [[Mandarin Chinese|Mandarin]], [[Russian language|Russian]], and [[Spanish language|Spanish]]\n<ref>[http://www.sandia.gov/news-center/news-releases/2005/tech-trans/smbusiness.html "Sandia helps 278 state businesses in 2004 through New Mexico Small Business Assistance Program," Sandia National Laboratories, Sandia Corporation, March 22, 2005] on sandia.gov</ref> although the software enables the reading of the libretto in any [[written language]].\n<ref name="entertanmentengineering.com">[http://www.entertanmentengineering.com/v4.issue04/page.06.html  “Giving the Opera a New Voice,”] \'\'Entertainment Engineering," Volume 4, Issue 2, p. 6</ref> Translation is provided by one screen and delivery system per person.<ref>[http://www.figarosystems.com  Figaro Systems Official Website]</ref>\n\nTypically, but not in all cases, the system is permanently installed along the backs of rows of seats. Each screen is positioned so that the text is clearly visible to each user. The displays were initially available in [[vacuum fluorescent display]], ([[Vacuum fluorescent display|VFD]]) and, in 2000, [[liquid crystal display]], ([[LCD]]) was used. In 2004 the displays became available with [[organic light-emitting diode]], ([[OLED]]) screens.  Each type of display provides the same text information and program annotation on eight channels simultaneously, may be turned off by the user, and is user-operated with a single button. The software is capable of supporting venues’ existing systems as well as Figaro Systems\' "Simultext" system. The software enables cueing of each line as it is sung, and it appears instantly on the screen.<ref name="entertanmentengineering.com"/>\n\nThe company builds fully [[modular]] systems including its [[wireless]] [[handheld]] screens \n<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=3&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 6,760,010. "Wireless electronic libretto display apparatus and method," issued July 6, 2004:] United States Patent and Trademark Office Patent Full-Text and Image Database</ref> for users who cannot use seatback systems, for example people in [[wheelchair]]s, who may be viewing the opera in areas lacking seatback viewing, or people with compromised eyesight.\n\n==Venues==\nIn the US, the company’s systems are in use in the [[Ellie Caulkins Opera House]] \n<ref>[http://www.highbeam.com/doc/1G1-135788390.html Marc Shulgold, "Opera dialogue shows on seat in front of you,"] \'\'Rocky Mountain News\'\' (Denver, Colorado), September 3, 2005 on highbeam.com,</ref> in [[Denver, Colorado|Denver]], Colorado, The Santa Fe Opera in Santa Fe,<ref>[https://web.archive.org/web/20080512022822/http://www.santafeopera.org/yournite/operatitles.php  Santa Fe Opera, Santa Fe, NM. Cached webpage],</ref> the [[Brooklyn Academy of Music]]<ref>[http://www.appliancemagazine.com/editorial.php?article=1768&zone=210&first=1  “An Operatic Performance,” \'\'Appliance Magazine\'\', June 2007],</ref> the [[Metropolitan Opera]], New York, where it is called "MetTitles"),<ref>[http://www.figaro-systems.com/installations.php  Figaro Systems Official Website. Installations],</ref> the [[Roy E. Disney]] Theatre in [[Albuquerque]]\'s [[National Hispanic Cultural Center]], [[McCaw Hall]] in [[Seattle Washington]], the [[Opera Theatre of St. Louis]] in St. Louis, Missouri, the [[Des Moines Metro Opera]] in [[Des Moines, Iowa|Des Moines]], Iowa and the Lyric Opera of Kansas City,  Missouri.<ref name="figaro-systems.com"/>\n\nIn the UK and Europe, the systems have been installed in venues including the [[Royal Opera House]] in London, the [[Teatro alla Scala]] and La Scala\'s [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, the [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]].\n<ref>[http://www.entertainmentengineering.com/v4.issue04/page.06.html “Giving the Opera a New Voice,” \'\'Entertainment Engineering.\'\', Volume 4, Issue 2, p. 6], on entertainmentengineering.com</ref>\n\n==Awards==\nIn 2001, the company won the [[Los Alamos, New Mexico|Los Alamos]] Laboratories’ Technology Commercialization Award for its Simultext system.<ref>[http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/1170 Todd Hanson, "Los Alamos announces technology commercialization awards," \'\'Los Alamos National Laboratory News\'\'], Los Alamos National Security, LLC, US Department of Energy\'s NNSA, May 7, 2001 on lanl.gov/news.</ref>\nIn 2008, the company’s software was one of four finalists for the Excellence Award for Commercial Software awarded by the New Mexico Information Technology and Software Association.\n\n==References==\n{{Reflist}}\n\n[[Category:Assistive technology]]\n[[Category:Companies based in Santa Fe, New Mexico]]\n[[Category:Companies established in 1993]]\n[[Category:Educational technology companies]]\n[[Category:Information retrieval organizations]]\n[[Category:Privately held companies based in New Mexico]]\n[[Category:Software companies based in New Mexico]]']
['International Society for Music Information Retrieval', '30882491', '{{Infobox non-profit\n| Non-profit_name   = The International Society for Music Information Retrieval\n| Non-profit_logo   = [[Image:LogoInternationalSocietyMIR.png|250px]]\n| Non-profit_type   = Non-profit organization\n| founded_date      = 2008\n| founder           = \n| location          = [[Canada]]\n| origins           = International Symposium for Music Information Retrieval\n| key_people        = \n| area_served       = Worldwide\n| focus             = [[Music information retrieval|Music Information Retrieval (MIR)]]\n| method            = Conferences, publications\n| revenue           = \n| endowment         = \n| num_volunteers    = \n| num_employees     = \n| num_members       = \n| owner             = \n| Non-profit_slogan = The world\'s leading research forum on processing, searching, organising and accessing music-related data\n| homepage          = {{URL|http://www.ismir.net/}}\n| tax_exempt        = \n| dissolved         = \n| footnotes         = \n}}\n\nThe \'\'\'[http://www.ismir.net International Society for Music Information Retrieval (ISMIR)]\'\'\' is an international forum for research on the organization of music-related data. It started as an informal group steered by an \'\'ad hoc\'\' committee in 2000<ref>[http://www.ismir.net/texts/Byrd02.html Donald Byrd and Michael Fingerhut: \'\'The History of ISMIR - A Short Happy Tale\'\'. D-Lib Magazine, Vol. 8 No. 11], {{ISSN|1082-9873}}.</ref> which established a yearly symposium - whence "ISMIR", which meant \'\'\'International Symposium on Music Information Retrieval\'\'\'. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.<ref>[http://www.ismir.net/ISMIR-Letters-Patent.pdf ISMIR Letters Patent. Canada, July 4, 2008.]</ref>\n\n==Purpose==\nGiven the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.\n\nAs the term [[Music Information Retrieval|Music Information Retrieval (MIR)]]  indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science, electrical engineering and many others.\n\n==Annual conferences==\nSince its inception in 2000, ISMIR has been the world’s leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.\n\n* ISMIR 2019, Delft (The Netherlands)\n* ISMIR 2018, Paris (France)\n* ISMIR 2017, Suzhou (China)\n* [http://ismir2016.ismir.net ISMIR 2016], 8–12 August 2016, New York City (USA) [http://dblp.uni-trier.de/db/conf/ismir/ismir2016.html proceedings]\n* [http://ismir2015.ismir.net ISMIR 2015], 26–30 October 2015, Malaga (Spain) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2015.html proceedings]\n* [http://ismir2014.ismir.net ISMIR 2014], 27–31 October 2014, Taipei (Taiwan) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2014.html proceedings]\n* [http://ismir2013.ismir.net ISMIR 2013], 4–8 November 2013, Curitiba (Brazil) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2013.html proceedings]\n* [http://ismir2012.ismir.net ISMIR 2012], 8–12 October 2012, Porto (Portugal) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2012\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2011.ismir.net ISMIR 2011], 24–28 October 2011, Miami (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2011\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2010.ismir.net ISMIR 2010], 9–13 August 2010, Utrecht (The Netherlands) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2010\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2009.ismir.net ISMIR 2009], 26–30 October 2009, Kobe (Japan) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2009\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2008.ismir.net ISMIR 2008], 14–18 September 2008, Philadelphia (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2008\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2007.ismir.net ISMIR 2007], 23–30 September 2007, Vienna (Austria) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2007\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2006.ismir.net ISMIR 2006], 8–12 October 2006, Victoria, BC (Canada) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2006\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2005.ismir.net ISMIR 2005], 11–15 September 2005, London (UK) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2005\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2004.ismir.net ISMIR 2004], 10–15 October 2004, Barcelona (Spain) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2004\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2003.ismir.net ISMIR 2003], 26–30 October 2003, Baltimore, Maryland (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2003\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2002.ismir.net ISMIR 2002], 13–17 October 2002, Paris (France) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2002\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2001.ismir.net ISMIR 2001], 15–17 October 2001, Bloomington, Indiana (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'2001\'&page=0&order=Authors&order_type=ASC proceedings]\n* [http://ismir2000.ismir.net ISMIR 2000], 23–25 October 2000, Plymouth, Massachusetts (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`=\'200\'&page=0&order=Authors&order_type=ASC proceedings]\n\nThe [http://www.ismir.net/ official webpage] provides a more up-to-date information on past and future conferences and provides access to all past websites and to the [http://www.ismir.net/proceedings cumulative database] of all papers, posters and tutorials presented at these conferences. An overview of all papers published at ISMIR can be found at [http://dblp.uni-trier.de/db/conf/ismir/index.html DBLP].\n\n==Research areas and topics==\nThe following list gives an overview of the main research areas and topics that are within the scope of \n[[Music Information Retrieval]].\n\n===MIR data and fundamentals===\n*    music signal processing\n*    symbolic music processing\n*    metadata, linked data and semantic web\n*    social tags and user generated data\n*    natural language processing, text and web mining\n*    multi-modal approaches to MIR\n\n===Methodology===\n*    methodological issues and philosophical foundations\n*    evaluation methodology\n*    corpus creation\n*    legal, social and ethical issues\n\n===Domain knowledge===\n*    representation of musical knowledge and meaning\n*    music perception and cognition\n*    computational music theory\n*    computational musicology and ethnomusicology\n\n===Musical features and properties===\n*    melody and motives\n*    harmony, chords and tonality\n*    rhythm, beat, tempo\n*    structure, segmentation and form\n*    timbre, instrumentation and voice\n*    musical style and genre\n*    musical affect, emotion and mood\n*    expression and performative aspects of music\n\n===Music processing===\n*    sound source separation\n*    music transcription and annotation\n*    optical music recognition\n*    alignment, synchronization and score following\n*    music summarization\n*    music synthesis and transformation\n*    fingerprinting\n*    automatic classification\n*    indexing and querying\n*    pattern matching and detection\n*    similarity metrics\n\n===Application===\n*    user behavior and modelling\n*    user interfaces and interaction\n*    digital libraries and archives\n*    music retrieval systems\n*    music recommendation and playlist generation\n*    music and health, well-being and therapy\n*    music training and education\n*    MIR applications in music composition, performance and production\n*    music and gaming\n*    MIR in business and marketing\n\n==MIREX==\nThe \'\'Music Information Retrieval Evaluation eXchange\'\' (MIREX) is an annual evaluation campaign for MIR algorithms, coupled to the ISMIR conference. Since it started in 2005, MIREX has fostered advancements both in specific areas of MIR and in the general understanding of how MIR systems and algorithms are to be evaluated.<ref name=DownieEBJ10>\n{{citation \n|author1=J. Stephen Downie |author2=Andreas F. Ehmann |author3=Mert Bay |author4=M. Cameron Jones |title=The Music Information Retrieval Evaluation eXchange: Some Observations and Insights\n|journal=Advances in Music Information Retrieval, Springer\n|year=2010\n|pages=93–115\n|doi=10.1007/978-3-642-11674-2_5}}\n</ref><ref name=DownieEEV05_ISMIR>\n{{cite journal\n|last1=Downie\n|first1=J. Stephen\n|last2=West\n|first2=Kris \n|last3=Ehmann\n|first3=Andreas F.\n|last4=Vincent\n|first4=Emmanuel\n|title=The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview\n|journal=Proceedings of the International Conference on Music Information Retrieval\n|year=2005\n|pages=320–323}}\n</ref> MIREX is to the MIR community what the [[Text Retrieval Conference]] (TREC) is to the text information retrieval community: A set of community-defined formal evaluations through which a wide variety of state-of-the-art systems, algorithms and techniques are evaluated under controlled conditions. MIREX is managed by the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) at the University of Illinois at Urbana-Champaign (UIUC).<ref name="DownieIMIRSEL">{{cite web|last1=Downie|first1=J. Stephen|title=The International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) Project|url=http://www.music-ir.org/evaluation/|publisher=University of Illinois|accessdate=22 April 2016}}</ref>\n\n==Related conferences==\n* [[ACM Multimedia]]\n* [[International Computer Music Conference|International Computer Music Conference (ICMC)]]\n* [[International Conference on Acoustics, Speech, and Signal Processing|International Conference on Acoustics, Speech, and Signal Processing (ICASSP)]]\n* [[International Conference on Digital Audio Effects|International Conference on Digital Audio Effects (DAFx)]]\n* [[New Interfaces for Musical Expression|International Conference on New Interfaces for Musical Expression (NIME)]]\n* International Symposium on Computer Music Modeling and Retrieval (CMMR)\n* [[Sound and Music Computing Conference|Sound and Music Computing Conference (SMC)]]\n\n==Related journals==\n* [[Computer Music Journal|Computer Music Journal (CMJ)]]\n* [http://asmp.eurasipjournals.springeropen.com/ EURASIP Journal on Audio, Speech, and Music Processing]\n* [http://www.signalprocessingsociety.org/publications/periodicals/taslp/ IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)]\n* [http://www.signalprocessingsociety.org/tmm/ IEEE Transactions on Multimedia (TMM)]\n* [http://mp.ucpress.edu/ Music Perception]\n* [[Journal of New Music Research|Journal of New Music Research (JNMR)]]\n\n==Further links==\n* [[Audio Engineering Society]]\n* [http://www.signalprocessingsociety.org/technical-committees/list/audio-tc/ Audio and Acoustic Signal Processing]\n* [[Music Technology]]\n* [http://www.ismir.net International Society for Music Information Retrieval (ISMIR)]\n* [[Sound and music computing|Sound and Music Computing]]\n\n==References==\n{{Reflist}}\n\n[[Category:Music information retrieval]]\n[[Category:Computer science conferences]]\n[[Category:Music technology]]\n[[Category:Multimedia]]\n[[Category:Information retrieval organizations]]\n[[Category:Music search engines]]']
['Clearinghouse for Networked Information Discovery and Retrieval', '45638306', 'The \'\'\'Clearinghouse for Networked Information Discovery and Retrieval\'\'\' or \'\'\'CNIDR\'\'\' was an organization funded by the U.S. [[National Science Foundation]] from 1993 to 1997 and based at the Microelectronics Center of North Carolina (MCNC) in [[Research Triangle Park]].<ref>National Science Foundation, [http://www.nsf.gov/awardsearch/showAward?AWD_ID=9216963 Award Abstract #9216963: Clearinghouse for Network Information Discovery Retrieval]</ref><ref>Brett, George. [http://grantome.com/grant/NSF/CNS-9315306 Clearinghouse for Networked Information Discovery and Retrieval (CNIDR)]</ref>  CNIDR was active in the research and development of open source software and open standards, centered on information discovery and retrieval, in the emerging Internet.\n\nAmong the software developed at CNIDR were Isite, an open source [[Z39.50]] implementation and successor to the free version of [[Wide area information server|WAIS]],<ref>Gamiel, Kevin and Nassar, Nassib.  1995.  Structural components of the Isite information system.  In \'\'Z39.50 Implementation Experiences\'\', P. Over, R. Denenberg, W. E. Moen, and L. Stovel, Eds.  National Institute of Standards and Technology Special Publication 500-229, US Department of Commerce, Gaithersburg, MD, 71-74.</ref><ref>Nebert, Douglas D. and Fullton, James.  [http://www.csdl.tamu.edu/DL95/papers/nebert/nebert.html Use of the ISite Z39.50 software to search and retrieve spatially-referenced data]</ref><ref>[http://inkdroid.org/tmp/www-talk/8133.html CNIDR Announces Isite v1.00 Integrated Information System]</ref><ref>[http://isite.awcubed.com/Isite.html The Isite Information System]</ref><ref>[http://www.loc.gov/z3950/mums.html Library of Congress Search Form]</ref> and [[Isearch]], an open source text retrieval system.  CNIDR staff were involved in the development of open standards in the [[Internet Engineering Task Force]], the Z39.50 Implementors Group and [[Dublin Core]].<ref>[https://www.ietf.org/meeting/past.html IETF Past Meetings]</ref><ref>[http://www.loc.gov/z3950/agency/zig/meetings/output.html ZIG Meeting Output]</ref><ref>[http://dublincore.org/workshops/dc1/ DC1: OCLC/NCSA Metadata Workshop: The Essential Elements of Network Object Description]</ref>\n\nCNIDR collaborated with the [[United States Patent and Trademark Office|U.S. Patent and Trademark Office (USPTO)]] to develop the USPTO\'s first Internet-based patent search systems.  One of these provided full text searching and images of medical patents related to the research and treatment of HIV/AIDS and issued by the US, Japanese and European patent offices.  Another system, known as the US Patent Bibliographic Database, provided searching of "front page" bibliographic information for all US patents since 1976.<ref>Miller, Annetta.  1994.  [http://www.newsweek.com/new-online-aids-database-186740 "A New Online Aids Database."]  In \'\'Newsweek\'\', November 13, 1994.</ref><ref>[http://www.pubzpro.com/Pubz/#!search/a/6105 MCNC and U.S. Patent Office Launch Internet AIDS Library]</ref><ref>Kawakami, Alice K.  [http://www.istl.org/98-summer/article5.html "Patents and Patent Searching."]</ref><ref>[http://www2.iastate.edu/~cyberstacks/hyb_t_7.htm Patents and Trademarks]</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Information retrieval organizations]]\n[[Category:Internet Standards]]\n[[Category:Internet protocols]]\n[[Category:Internet search engines]]\n[[Category:Organizations established in 1992]]\n[[Category:Computer-related organizations]]']
['Category:Google', '853521', '{{Commons category|Google}}\n{{Cat main|Google}}\n\n[[Category:Internet search engines]]\n[[Category:Software companies based in the San Francisco Bay Area]]\n[[Category:Internet companies of the United States]]\n[[Category:Technology companies based in the San Francisco Bay Area]]\n[[Category:Companies based in Mountain View, California]]\n[[Category:Web portals]]\n[[Category:Information retrieval organizations]]\n[[Category:Alphabet Inc.]]\n[[Category:Wikipedia categories named after information technology companies of the United States]]\n[[Category:Wikipedia categories named after websites]]']
['Artificial Solutions', '40218456', "{{Infobox company\n|name= Artificial Solutions\n|logo=[[Image:Artificial Solutions Logo.png]]\n|type=[[Private company]]\n|foundation=(2001)\n|founder=Johan Åhlund, Johan Gustavsson and Michael Söderström \n|location=[[Barcelona]], [[Spain]]\n|locations=Offices worldwide with R&D centers in [[Barcelona]], [[Hamburg]], [[London]], [[Mountain View, California|Mountain View]], [[Milan]], [[Utrecht]] and [[Stockholm]] \n|industry=[[Computer Software]], [[Natural language]], [[Intelligent software assistant]], \n|products= Teneo platform\n|homepage=[http://www.artificial-solutions.com/ www.artificial-solutions.com]\n}}\n\n'''Artificial Solutions''' is a multinational [[software company]] that develops and sells natural language interaction products for enterprise and consumer use.<ref>{{cite web|last=Ion |first=Florence |url=http://arstechnica.com/gadgets/2013/06/review-indigo-brings-siri-like-conversation-to-the-android-platform/ |title=Review: Indigo wants to bring Siri-like conversation to the Android platform |publisher=Ars Technica |date=2013-06-05 |accessdate=2013-09-08}}</ref> The company's natural language solutions have been deployed in a wide range of industries including finance,<ref>{{cite web|last=Thompson|first=Scott|title=Agria working with Artificial Solutions|url=http://www.fstech.co.uk/fst/AgriaDjurf%C3%B6rs%C3%A4kring_ArtificialSolutions.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Savvas|first=Antony|title=Co-operative Bank uses Mia to speed up contact centre calls|url=http://www.computerworlduk.com/news/it-business/3316914/co-operative-bank-uses-mia-to-speed-up-contact-centre-calls/|work=Computerworld UK|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Thompson|first=Scott|title=2012 FStech Awards: winners announced|url=http://www.fstech.co.uk/fst/2012_FStechAwards_Winners.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref> telecoms,<ref>{{cite web|last=Westerholm|first=Joel|title=Telenors elektroniska kundtjänst pressar kostnaderna|url=http://computersweden.idg.se/2.2683/1.143425|work=ComputerSweden|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|title=Artificial Solutions Powers Online IVA for Vodafone|url=http://langtechnews.hivefire.com/articles/262940/artificial-solutions-powers-online-iva-for-vodafon/|work=LangTechNews|accessdate=12 September 2013}}</ref> the public sector,<ref>{{cite web|last=Brax|first=Sofia|title=Digitala kolleger alltid till tjänst|url=http://www.publikt.se/artikel/digitala-kolleger-alltid-till-tjanst-38087|work=Publik|publisher=Fackforbundet ST|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Nilsson|first=Orjan|title=Cyber-damene husker deg|url=http://www.nettavisen.no/innenriks/ibergen/article1609734.ece|work=Nettavisen|publisher=iBergen}}</ref> retail<ref>{{cite web|author=Aaron Travis |url=http://techcrunch.com/2013/01/05/in-defense-of-the-humble-walkthrough/ |title=In Defense Of The Humble App Walkthrough |publisher=TechCrunch |date=2013-01-05 |accessdate=2013-09-08}}</ref> and travel.<ref>{{cite web|last=Fox|first=Linda|title=CWT brings virtual face to mobile service|url=http://www.tnooz.com/2013/04/16/news/cwt-brings-virtual-face-to-mobile-service/|work=Tnooz|accessdate=12 September 2013}}</ref>\n\n==History==\nArtificial Solutions was founded in Stockholm in 2001 by friends Johan Åhlund, Johan Gustavsson and Michael Söderström to create interactive web assistants using a combination of artificial intelligence and natural language processing. Though Åhlund initially took some persuading, he thought it sounded ridiculous to be talking to a virtual agent on the internet.<ref>{{cite web|url=http://it24.idg.se/2.2275/1.143922 |title=Löjlig affärside vinstlott för Artificial Solutions |publisher=IT24 |date= |accessdate=2013-09-08}}</ref>\n\nThe company expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants.<ref>{{cite web|url=http://www.elnuevolunes.es/historico/2008/1294/1294%20al%20grano.html |title=Al grano |publisher=Elnuevolunes.es |date= |accessdate=2013-09-08}}</ref> Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.<ref>{{cite web|author=Barry Levine |url=http://www.cmswire.com/cms/customer-experience/gartner-mq-for-crm-web-customer-service-kana-moxie-software-oraclerightnow-among-leaders-019626.php |title=Gartner MQ for CRM Web Customer Service: Kana, Moxie Software, Oracle-RightNow Among Leaders |publisher=Cmswire.com |date= |accessdate=2013-09-08}}</ref>\n\nIn 2006 Artificial Solutions acquired Kiwilogic, a German software house creating its own virtual assistants.<ref>{{cite web|url=http://www.earlybird.com/en/companies/tech/exited/kiwilogic.html |title=Venture Capital: KIWILOGIC.COM AG |publisher=Earlybird |date= |accessdate=2013-09-08}}</ref>\n[[Elbot]], Artificial Solutions’ test-bed to explore the psychology of human-machine communication, won the [[Loebner Prize]] in 2008 and is the closest contestant of the annual competition based on the [[Turing Test]] to reach the 30% threshold by fooling 25% of the human judges.<ref>[[Loebner Prize]]</ref><ref>{{cite web|url=http://news.bbc.co.uk/2/hi/uk_news/england/berkshire/7666246.stm |title=UK &#124; England &#124; Berkshire &#124; Test explores if robots can think |publisher=BBC News |date=2008-10-13 |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Robson|first=David|title=Almost human: Interview with a chatbot|url=http://www.newscientist.com/article/dn14925-almost-human-interview-with-a-chatbot.html#.UjHKzTdBuM9|work=New Scientist|publisher=Reed Business Information Ltd}}</ref>\n\nWith a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.<ref>{{cite web|author=[[Mike Elgan]] |url=http://www.computerworld.com/s/article/9237448/Smart_apps_think_so_you_don_t_have_to_ |title=Smart apps think (so you don't have to) |publisher=Computerworld |date=2013-03-09 |accessdate=2013-09-08}}</ref><ref>{{cite web|url=http://www.speechtechmag.com/Articles/News/Industry-News/Artificial-Solutions-Unveils-a-Software-Toolkit-for-Adding-Speech-to-Mobile-Apps-80015.aspx |title=Artificial Solutions Unveils a Software Toolkit for Adding Speech to Mobile Apps |publisher=SpeechTechMag.com |date=2012-01-17 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://www.computerworld.dk/art/220859/saa-effektiv-er-ikeas-chat-robot-har-vaeret-paa-efteruddannelse |title=Så effektiv er Ikeas chat-robot: Har været på 'efteruddannelse' - Computerworld |publisher=Computerworld.dk |date= |accessdate=2013-09-08}}</ref>\nIn 2013 Artificial Solutions launched [[Indigo (virtual assistant)|Indigo]], a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.<ref>{{cite web|last=Hoyle |first=Andrew |url=http://reviews.cnet.com/8301-13970_7-57570960-78/indigo-brings-siri-like-assistance-to-android-for-free-hands-on/ |title=Indigo brings Siri-like assistance to Android for free (hands-on) &#124; Mobile World Congress - CNET Reviews |publisher=Reviews.cnet.com |date=2013-02-24 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://lifehacker.com/indigo-wants-to-be-your-personal-assistant-across-devic-484924277 |title=Indigo Wants to Be Your Personal Assistant Across Devices |publisher=Lifehacker.com |date= |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Wollman |first=Dana |url=http://www.engadget.com/2013/02/26/indigo-personal-assistant-hands-on/ |title=Indigo is a cloud-based, cross-platform personal assistant for Android and Windows Phone 8 (hands-on) |publisher=Engadget.com |date=2013-02-26 |accessdate=2013-09-08}}</ref>\nA new round of funding was announced in June 2013. The $9.4m will be used to support expansion in the US market.<ref>{{cite web|url=http://www.altassets.net/private-equity-news/by-news-type/deal-news/artificial-solutions-raises-9-4m-in-scope-led-round-for-us-expansion.html |title=Artificial Solutions raises $9.4m in Scope-led round for US expansion &#124; AltAssets Private Equity News |publisher=Altassets.net |date=2013-06-25 |accessdate=2013-09-08}}</ref>\n\nIn February 2014 Artificial Solutions announced the Teneo Network of Knowledge, a patented intelligent framework that enables users to interact using natural language with private, shared and public ecosystem of devices, also known as the [[Internet of Things]].<ref>{{cite web|last1=Trenholm|first1=Rich|title=Next generation of personal assistant takes a step towards 'Her'-style super-Siri|url=http://www.cnet.com/news/next-generation-of-personal-assistant-takes-a-step-towards-her-style-super-siri/|website=Cnet|publisher=CBS Interactive}}</ref>\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n*[http://www.hello-indigo.com Indigo]\n*[http://www.elbot.com Elbot]\n\n[[Category:Natural language processing software]]\n[[Category:Intelligent software assistants]]\n[[Category:User interfaces]]\n[[Category:Artificial intelligence applications]]\n[[Category:Natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Information retrieval organizations]]"]
['Category:Alphabet Inc.', '47562417', "{{Commons category|Alphabet Inc.}}\n{{Portal|Alphabet|Google}}\n*'''[[Alphabet Inc.]]''' — an {{C|Multinational companies headquartered in the United States|American multinational}} {{C|Conglomerate companies of the United States|conglomerate company}} based in {{C|Mountain View, California|Mountain View}}, {{C|San Francisco Bay Area}}, {{C|California}}.\n:::::*It is the parent corporation of {{C|Google}}; and other [[information technology]], investment, life sciences, and research companies.\n\n\n{{clr}}\n::{{Cat main|Alphabet Inc.}}\n\n{{Alphabet Inc.|state=collapsed}}\n{{Google Inc.}}\n\n[[Category:Conglomerate companies of the United States]]\n[[Category:Holding companies of the United States]]\n[[Category:Multinational companies headquartered in the United States]]\n[[Category:Technology companies of the United States]]\n[[Category:Technology companies based in the San Francisco Bay Area]]\n[[Category:Companies based in Mountain View, California]]\n[[Category:Wikipedia categories named after conglomerate companies of the United States]]\n[[Category:Wikipedia categories named after information technology companies of the United States]]\n\n\n\n\n\n\n\n\n\n\n\n[[Category:Software companies based in the San Francisco Bay Area]]\n[[Category:Information retrieval organizations]]\n[[Category:Internet companies of the United States]]"]
['Category:Search engine software', '6521632', '[[Category:Information retrieval systems]]\n[[Category:Utility software by type]]\n[[Category:Marketing software]]\n[[Category:Web software]]']
['Ptx (Unix)', '1442890', "{{Unreferenced stub|auto=yes|date=December 2009}}\n{{Lowercase|title=ptx}}\n'''ptx''' is a [[Unix utility]], named for the ''[[permuted index]]'' which can perform the function of the [[Keyword in Context]] (KWIC) search mode. There is a corresponding [[IBM mainframe]] utility which performs the same function. permuted indexes are often used in such places as bibliographic or medical databases, [[thesaurus]]es, or web sites to aid in locating entries of interest.\n\n==See also==\n* [[Concordancer]]\n\n[[Category:Information retrieval systems]]\n[[Category:Unix text processing utilities]]\n\n\n{{Unix-stub}}"]
['Indexing Service', '4047242', '{{distinguish|Indexing and abstracting service}}\n{{Use dmy dates|date=February 2011}}\n{{Infobox Windows component\n| name                = Indexing Service\n| screenshot          = Indexing Service Query Form.PNG\n| screenshot_size     = 300px\n| caption             = The Indexing Service Query Form, used to query Indexing Service catalogs, hosted in [[Microsoft Management Console]].\n| type                = [[Desktop search]]\n| service_name        = Indexing Service\n| service_description = Indexes contents and properties of files on local and remote computers; provides rapid access to files through flexible querying language.\n| replaced_by         = [[Windows Search]]\n| included_with       = [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /><br/>[[Windows 2000]]<ref name="MIS-v3" /><br/>[[Windows XP]]<ref name="TnC-144" /><br/>[[Windows Server 2003]]<ref name="TnC-144" /><br/>[[Windows Server 2008]]<ref name="WIS-Install2008" />\n}}\n\n\'\'\'Indexing Service\'\'\' (originally called \'\'\'Index Server\'\'\') was a [[Windows service]] that maintained an index of most of the [[Computer file|files]] on a computer to improve searching performance on PCs and corporate [[computer network]]s. It updated indexes without user intervention. In [[Windows 7]], it has been replaced by a newer [[Windows Search]] indexer. The [[IFilter]] plugins to extend the indexing capabilities to more file formats and protocols are compatible between the legacy Indexing Service and the newer Windows Search indexer.\n\n== History ==\nIndexing Service was a [[desktop search]] service included with [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /> as well as [[Windows 2000]] and later.<ref name="MIS-v3" /><ref name="TnC-144" /><ref name="WIS-What" /> The first incarnation of the indexing service was shipped in August 1996<ref name="MIS-Intro" /> as a content search system for Microsoft\'s web server software, [[Internet Information Services]].{{Citation needed|date=February 2011}} Its origins, however, date further back to Microsoft\'s [[Cairo (operating system)|Cairo operating system]] project, with the component serving as the Content Indexer for the [[Object File System]]. Cairo was eventually shelved, but the content indexing capabilities would go on to be included as a standard component of later Windows desktop and server operating systems, starting with [[Windows 2000]], which includes Indexing Service 3.0.{{Citation needed|date=February 2011}}\n\nIn [[Windows Vista]], the content indexer was replaced with the [[Windows Search]] indexer which was enabled by default. Indexing Service is still included with Windows Server 2008 but is not installed or running by default.<ref name="WIS-Install2008" />\n\nIndexing Service has been deprecated in Windows 7 and Windows Server 2008 R2.<ref>{{cite web|title=Deprecated Features for Windows 7 and Windows Server 2008 R2|url=http://technet.microsoft.com/en-us/library/ee681698%28WS.10%29.aspx|work=Windows 7 Technical Library|publisher=Microsoft Corporation|accessdate=8 November 2011|location=Indexing Service|date=October 16, 2009}}</ref> It has been removed from [[Windows 8]].\n\n== Search interfaces ==\n\nComprehensive searching is available after initial building of the index, which can take up to hours or days, depending on the size of the specified directories, the speed of the hard drive, user activity, indexer settings and other factors. Searching using Indexing service works also on [[Uniform Naming Convention|UNC]] paths and/or mapped network drives if the sharing server indexes appropriate directory and is aware of its sharing.\n\nOnce the indexing service has been turned on and has built its index it can be searched in three ways. The search option available from the [[Start Menu]] on the [[Microsoft windows|Windows]] [[Taskbar]] will use the indexing service if it is enabled and will even accept complex queries. Queries can also be performed using either the \'\'Indexing Service Query Form\'\' in the [[Microsoft Management Console#Common snap-ins|Computer Management snap-in]] of Microsoft Management Console, or, alternatively, using third-party applications such as \'Aim at File\' or \'Grokker Desktop\'.\n\nMicrosoft Index Server 2.0 does not detect changes to a catalog if the data is located on a [[Volume Mount Point|mounted partition]]. It does not support mounted volumes because of technical limitations in the file system.<ref>{{cite web\n | url = http://support.microsoft.com/kb/319506\n | title = INFO: Index Server Does Not Support Mounted Volumes (Revision: 1.0)\n | work = Microsoft Support\n | publisher = 10 May 2002\n | accessdate = 1 February 2011\n}}</ref>\n\n== References ==\n{{Reflist|refs=\n<ref name = "MIS-Intro">{{Cite web\n  |url = http://msdn.microsoft.com/en-us/library/ms951563.aspx\n  |title = Introduction to Microsoft Index Server\n  |work = [[Microsoft Developer Network]]\n  |publisher = Microsoft Corporation\n  |date = 15 October 1997\n  |accessdate = 1 February 2011\n  |first1 = Krishna\n  |last1 = Nareddy\n  }}</ref>\n<ref name = "MIS-v3">{{Cite web\n  |url = http://msdn.microsoft.com/en-us/library/ms689644.aspx\n  |title = Indexing Service Version 3.0\n  |work = [[Microsoft Developer Network]]\n  |publisher = Microsoft Corporation\n  |date =\n  |accessdate = 1 February 2011\n  |first1 =\n  |last1 =\n  }}</ref>\n<ref name = "WIS-What">{{Cite web\n  |url = http://msdn.microsoft.com/en-us/library/ms689718.aspx\n  |title = What is Indexing Service?\n  |work = [[Microsoft Developer Network]]\n  |publisher = Microsoft Corporation\n  |date =\n  |accessdate = 1 February 2011\n  |first1 =\n  |last1 =\n  }}</ref>\n<ref name="WIS-Install2008">{{Cite web\n  |url = http://support.microsoft.com/kb/954822\n  |title = How to install and configure the Indexing Service on a Windows Server 2008-based computer (Revision: 3.0)\n  |work = Microsoft Support\n  |publisher = Microsoft Corporation\n  |date = 3 May 2010\n  |accessdate = 1 February 2011\n  }}</ref>\n<ref name="TnC-144">{{Cite book\n  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyId=1B6ACF93-147A-4481-9346-F93A4081EEA8&displaylang=en\n  |format = Microsoft Word\n  |title = Threats and Countermeasures: Security Settings in Windows Server 2003 and Windows XP\n  |edition = 2.0\n  |publisher = Microsoft Corporation\n  |page = 144\n  |date=December 2005\n  |first1 = Mike\n  |last1  = Danseglio\n  |first2 = Kurt\n  |last2  = Dillard\n  |first3 = José\n  |last3  = Maldonado\n  |first4 = Paul\n  |last4  = Robichaux\n  |editor1-first = Reid\n  |editor1-last  = Bannecker\n  |editor2-first = John\n  |editor2-last  = Cobb\n  |editor3-first = Jon\n  |editor3-last  = Tobey\n  |editor4-first = Steve\n  |display-editors = 3 |editor4-last  = Wacker\n  }}</ref>\n}}\n\n{{Microsoft Windows components}}\n{{DEFAULTSORT:Indexing Service}}\n[[Category:Windows communication and services]]\n[[Category:Desktop search engines|Desktop search engines]]\n[[Category:Information retrieval systems]]\n[[Category:Windows components]]']
['Outline of search engines', '34320324', '<!--... Attention:  THIS IS AN OUTLINE\n\n        part of the set of 700+ outlines listed at\n             [[Portal:Contents/Outlines]].\n\n                 Wikipedia outlines are\n              a special type of list article.\n              They make up one of Wikipedia\'s\n                content navigation systems\n\n                See [[Wikipedia:Outlines]]\n                      for more details.\n                   Further improvements\n              to this outline are on the way\n...-->\nThe following [[Outline (list)|outline]] is provided as an overview of and topical guide to search engines. \n\n\'\'\'[[Search engine (computing)|Search engine]]\'\'\' &ndash; [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented as a list, and are commonly called \'\'hits\'\'.\n\n{{TOC limit|limit=2}}\n\n== What \'\'type\'\' of thing is a search engine? ==\n\nA search engine can be described as all of the following:\n\n* [[Software]] &ndash;\n** [[Computer program]] &ndash;\n*** [[Application software]] &ndash; computer software designed to help the user to perform specific tasks. Also known as an application or an "app".\n\n== Types of search engines ==\n\n* [[Database search engine]] &ndash;\n* [[Desktop search engine]] &ndash;\n* [[Distributed search engine]] &ndash; search engine where there is no central server. Unlike traditional centralized search engines, work such as crawling, data mining, indexing, and query processing is distributed among several peers in decentralized manner where there is no single point of control.\n* [[Enterprise search engine]] &ndash; search engine employed on and for access to the information on an organization\'s computer network.\n* [[Human search engine]] &ndash; uses human participation to filter the search results and assist users in clarifying their search request. The goal is to provide users with a limited number of relevant results, as opposed to traditional search engines that often return a large number of results that may or may not be relevant.\n* [[Hybrid search engine]] &ndash; uses different types of data with or without ontologies to produce the algorithmically generated results based on web crawling. Previous types of search engines only use text to generate their results.\n* [[Intelligent medical search engine]]\n* [[Metasearch engine]] &ndash; search tool[1] that sends user requests to several other search engines and/or databases and aggregates the results into a single list or displays them according to their source. Metasearch engines enable users to enter search criteria once and access several search engines simultaneously.\n** [[Search aggregator]]\n* [[Organic search engine]] &ndash; manually operated search service which uses a combination of computer algorithms and human researchers to look up a search query. A search query submitted to an organic search engine is analysed by a human operator who researches the query then formats the response to the user.\n* [[Web search engine]] &ndash; designed to search for information on the World Wide Web and FTP servers. The search results are generally presented in a list of results often referred to as SERPS, or "search engine results pages".\n** [[Audio search engine]] &ndash; web-based search engine which crawls the web for audio content.\n** [[Collaborative search engine]] &ndash; emerging trend for Web search and Enterprise search within company intranets. CSEs let users concert their efforts in information retrieval (IR) activities, share information resources collaboratively using knowledge tags, and allow experts to guide less experienced people through their searches.\n** [[Social search engine]] &ndash; type of web search that takes into account the Social Graph of the person initiating the search query.\n** [[Video search engine]] &ndash; web-based search engine which crawls the web for video content. Some video search engines parse externally hosted content while others allow content to be uploaded and hosted on their own servers.\n* [[Visual search engine]] &ndash; designed to search for information on the World Wide Web through the input of an image or a search engine with a visual display of the search results. Information may consist of web pages, locations, other images and other types of documents. This type of search engines is mostly used to search on the mobile Internet through an image of an unknown object (unknown search query).\n\n== Specific search engines ==\n{{Main|List of search engines}}\n\n== Search engine software ==\n\n* [[List of search engine software]]\n\n== Search-based applications ==\n\n[[Search-based application]] &ndash;\n* [[Bibliographic database]]\n* [[Online database]]\n** [[List of online databases]]\n*** [[List of academic databases and search engines]]\n* [[Digital library]]\n** [[List of digital library projects]]\n*** [[List of online magazines]]\n*** [[Wikipedia:List of online newspaper archives]]\n* [[Electronic journal]]\n** [[Lists of academic journals]]\n*** [[List of open-access journals]]\n* Digital encyclopedia\n** [[Internet encyclopedia]]*\n*** [[List of online encyclopedias]]\n* [[Wiki]]\n** [[List of wikis]]\n* Digital dictionary\n** [[Online dictionary]]\n*** [[List of online dictionaries]]\n\n== Search engine technology ==\n\n[[Search engine technology]]\n* [[Search algorithm]]\n* [[Search engine image protection]]\n* [[Search engine indexing]]\n* [[Search engine optimization]]\n* [[Search engine results page]]\n* [[List of search engine software|Search engine software]]\n* [[Search engine submission]]\n** [[Search engine optimization copywriting]]\n* [[Web crawler]]\n\n== Search engine marketing ==\n[[Search engine marketing]]\n* [[Pay per click]]\n* [[Cost per impression]]\n* [[Search analytics]]\n* [[Web analytics]]\n\n== Persons influential in search engines ==\n* [[Sergey Brin]]\n* [[Larry Page]]\n* [[Eric Schmidt]]\n\n== See also ==\n* [[Outline of the Internet]]\n** [[Outline of Google]]\n* [[Human flesh search engine]]\n{{Clear}}\n\n== References ==\n{{Reflist}}\n\n== External links ==\n{{Sisterlinks|Search engine}}\n\n* [http://wikimindmap.com/viewmap.php?wiki=en.wikipedia.org&topic=Outline+of+search+engines&Submit=Search This outline displayed as a mindmap], at \'\'wikimindmap.com\'\'\n* {{Dmoz|Computers/Internet/Searching/Search_Engines/|Search Engines}}\n\n{{Outline footer}}\n\n[[Category:Information retrieval systems]]\n[[Category:Wikipedia outlines|Search engines]]\n[[Category:Articles created via the Article Wizard|Search engines]]']
['TREX search engine', '13179109', '\'\'\'TREX\'\'\' is a search engine in the [[NetWeaver|SAP NetWeaver]] integrated technology platform produced by [[SAP AG]] using [[columnar storage]].<ref>{{cite journal|url=http://db.csail.mit.edu/pubs/abadi-column-stores.pdf|doi=10.1561/1900000024|title=The Design and Implementation of Modern Column-Oriented Database Systems|author1=Daniel Abadi|author2=Peter Boncz|author3=Stavros Harizopoulos|author4=Stratos Idreos|author5=Samuel Madden|journal=Foundations and Trends in Databases|volume=5|issue=3|year=2012|pages=197–280}}</ref> The TREX engine is a standalone component that can be used in a range of system environments but is used primarily as an integral part of such SAP products as Enterprise Portal, Knowledge Warehouse, and \'\'\'Business Intelligence (BI, formerly [[SAP Business Information Warehouse]]).\'\'\' In SAP NetWeaver BI, the TREX engine powers the BI Accelerator, which is a plug-in appliance for enhancing the performance of [[online analytical processing]]. The name "TREX" stands for \'\'\'Text Retrieval and information EXtraction\'\'\', but it is not a registered trade mark of SAP and is not used in marketing collateral.\n\n==Search functions==\n\nTREX supports various kinds of text search, including exact search, boolean search, wildcard search, linguistic search (grammatical variants are normalized for the index search) and fuzzy search (input strings that differ by a few letters from an index term are normalized for the index search). Result sets are ranked using term frequency-inverse document frequency ([[tf-idf]]) weighting, and results can include snippets with the search terms highlighted.\n\nTREX supports text mining and classification using a [[vector space model]]. Groups of documents can be classified using query based classification, example based classification, or a combination of these plus keyword management.\n\nTREX supports structured data search not only for document metadata but also for mass business data and data in SAP [[Business Objects]]. Indexes for structured data are implemented compactly using [[data compression]] and the data can be aggregated in linear time, to enable large volumes of data to be processed entirely in memory.\n\nRecent developments include:\n* A join engine to join structured data from different fields in business objects\n* A fast update capability to write a delta index beside a main index and to merge them offline while a second delta index takes updates\n* A [[data mining]] feature pack for advanced mathematical analysis\n\n==History==\n\nThe first code for the engine was written in 1998 and TREX became an SAP component in 2000. The SAP NetWeaver BI Accelerator was first rolled out in 2005. As of Q1 2013, the current release of TREX is SAP NW 7.1.\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.sap.com/platform/netweaver/index.epx SAP NetWeaver]\n* [http://www.sap.com/platform/netweaver/components/bi/index.epx SAP NetWeaver Business Intelligence]\n* [http://www.sap.com/platform/netweaver/businessinformation.epx SAP NetWeaver Business Information Management]\n* [http://scn.sap.com/docs/DOC-8489 Search and Classification (TREX) on SAP Community Network]\n\n[[Category:SAP NetWeaver]]\n[[Category:Information retrieval systems]]\n[[Category:Business intelligence]]']
['Collaborative search engine', '22101925', '{{Recommender systems}}\n\'\'\'Collaborative search engines\'\'\' (CSE) are [[Web search engine]]s and [[enterprise search]]es within company intranets that let users combine their efforts in [[information retrieval]] (IR) activities, share information resources collaboratively using [[knowledge tags]], and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related [[information need]].\n\n== Models of collaboration ==\n\nCollaborative search engines can be classified along several dimensions: intent (explicit and implicit) and synchronization\n<ref name=Golo2007>{{citation\n | title = Collaborative Exploratory Search\n | year = 2007\n | author = Golovchinsky Gene\n | author2 = Pickens Jeremy\n | journal = Proceedings of HCIR 2007 workshop\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = http://projects.csail.mit.edu/hcir/web/hcir07.pdf\n}}</ref> and depth of mediation \n,<ref name=Pickens2008>{{citation\n | title = Collaborative Exploratory Search\n | year = 2008\n | author = Pickens Jeremy\n | author2 = Golovchinsky Gene\n | author3 = Shah Chirag\n | author4 = Qvarfordt Pernilla\n | author5 = Back Maribeth\n | booktitle = SIGIR \'08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval\n | pages = 315–322\n | volume = \n | issue = \n | doi = 10.1145/1390334.1390389\n | isbn = \n 9781605581644| url = http://portal.acm.org/citation.cfm?id=1390389\n| chapter = Algorithmic mediation for collaborative exploratory search\n }}</ref> task vs. trait,<ref name=Morris2008>{{citation\n | contribution = Understanding Groups’ Properties as a Means of Improving Collaborative Search Systems\n | year = 2008\n | author = Morris Meredith\n | author2 = Teevan Jaime\n | title = 1st International Workshop on Collaborative Information Retrieval, held in conjunction with [[Joint Confrence on Digital Libraries|JCDL]] 2008\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | contribution-url = http://workshops.fxpal.com/jcdl2008/submissions/tmpDF.pdf\n}}</ref> and division of labor and sharing of knowledge.<ref name=Foley2008>{{citation\n | title = Division of Labour and Sharing of Knowledge for Synchronous Collaborative Information Retrieval\n | year = 2008\n | author = Foley Colum\n | booktitle = PhD Thesis, Dublin City University\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = http://www.computing.dcu.ie/~cfoley/cfoley-PhD_thesis.pdf\n}}</ref>\n\n=== Explicit vs. implicit collaboration ===\n\nImplicit collaboration characterizes [[Collaborative filtering]] and [[recommendation systems]] in which the system infers similar information needs. I-Spy,<ref name=Smith2003>{{citation\n | title = Collaborative Web Search\n | year = 2003\n | author = Barry Smyth\n | author2 = Evelyn Balfe\n | author3 = Peter Briggs\n | author4 = Maurice Coyle\n | author5 = Jill Freyne\n | journal = IJCAI\n | pages = 1417–1419\n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref> [[Jumper 2.0]], [[Seeks]], the Community Search Assistant,<ref name=Glance2001>{{citation\n | title = Community search assistant\n | year = 2001\n | author = Natalie S. Glance\n | journal = Workshop on AI for Web Search AAAI\'02\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref> the CSE of Burghardt et al.,<ref name=BurghardtWI2008>{{citation\n | title = Discovering the Scope of Privacy Needs in Collaborative Search\n | year = 2008\n | author = Thorben Burghardt\n | author2 = Erik Buchmann\n | author3 = Klemens Böhm\n | journal = Web Intelligence (WI)\n | pages = \n 910| volume = \n | issue = \n | doi = 10.1109/WIIAT.2008.165\n | isbn = \n 978-0-7695-3496-1}}</ref> and the works of Longo et al.\n<ref name=Longo2009a>{{citation\n | title = Toward Social Search - From Explicit to Implicit Collaboration to Predict Users\' Interests\n | year = 2009\n | author = Longo Luca\n | author2 = Barrett Stephen\n | author3 = Dondio Pierpaolo\n | journal = \'\'[[Webist]]\'\' 2009 - Proceedings of the Fifth International Conference                on Web Information Systems and Technologies, Lisbon, Portugal,                March 23–26, 2009\n | pages = 693–696\n | volume = 1\n | issue = \n | doi = \n | isbn = 978-989-8111-81-4\n | url = \n}}</ref> \n<ref name=Longo2010>{{citation\n | title = Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time\n | year = 2010\n | author = Longo Luca\n | author2 = Barrett Stephen\n | author3 = Dondio Pierpaolo\n | journal = Transaction Computational Collective Intelligence II\n | pages = 46–69\n | volume = 2\n | issue = \n | doi = 10.1007/978-3-642-17155-0_3\n | isbn = \n 978-3-642-17154-3| url = http://www.springerlink.com/content/e12233858017h042/\n| series = Lecture Notes in Computer Science\n }}</ref> \n<ref name=Longo2009b>{{citation\n | title = Information Foraging Theory as a Form of Collective Intelligence                for Social Search\n | year = 2009\n | author = Longo Luca\n | author2 = Barrett Stephen\n | author3 = Dondio Pierpaolo\n | journal = Computational Collective Intelligence. Semantic Web, Social                Networks and Multiagent Systems, First International Conference,                ICCCI 2009, Wroclaw, Poland, October 5–7, 2009. Proceedings\n | pages = 63–74\n | volume = 1\n | issue = \n | doi = \n | isbn = 978-3-642-04440-3\n | url = http://dl.acm.org/citation.cfm?id=1692026\n}}</ref> \nall represent examples of implicit collaboration. Systems that fall under this category identify similar users, queries and links clicked automatically, and recommend related queries and links to the searchers.\n\nExplicit collaboration means that users share an agreed-upon information need and work together toward that goal. For example, in a chat-like application, query terms and links clicked are automatically exchanged. The most prominent example of this class is SearchTogether<ref name=Morris2007>{{citation\n | title = SearchTogether: An Interface for Collaborative Web Search\n | year = 2007\n | author = Meredith Ringel Morris\n | author2 = Eric Horvitz\n | journal = UIST\n| url = http://portal.acm.org/citation.cfm?id=1294211.1294215\n}}</ref> published in 2007. SearchTogether offers an interface that combines search results from standard search engines and a chat to exchange queries and links. Reddy et al.<ref name=Redy2008>{{citation\n | title = The Role of Communication in Collaborative Information Searching\n | year = 2008\n | author = Madhu C. Reddy\n | author2 = Bernhard J. Jansen\n | author3 = Rashmi Krishnappa\n | journal = ASTIS\n}}</ref> (2008) follow a similar approach and compares two implementations of their CSE called MUSE and MUST. Reddy et al. focuses on the role of communication required for efficient CSEs. Representatives for the class of implicit collaboration are I-Spy,<ref name="Smith2003"/> the Community Search Assistant,<ref name="Glance2001"/> and the CSE of Burghardt et al.<ref name="BurghardtWI2008" /> Cerciamo <ref name=Pickens2008 /> supports explicit collaboration by allowing one person to concentrate on finding promising groups of documents, while having the other person make in-depth judgments of relevance on documents found by the first person.\n\nHowever, in Papagelis et al.<ref name=Papagelis2007>{{citation| title = Searchius: A Collaborative Search Engine| year = 2007| author = Athanasios Papagelis| author2 = Christos Zaroliagis| journal = ENC \'07: Proceedings of the Eighth Mexican International Conference on Current Trends in Computer Science| pages = 88–98| doi = 10.1109/ENC.2007.34| url = http://portal.acm.org/citation.cfm?id=1302894| isbn = 0-7695-2899-6}}</ref> terms are used differently: they combine explicitly shared links and implicitly collected browsing histories of users to a hybrid CSE.\n\n=== Community of practice  ===\n\nRecent work in collaborative filtering and information retrieval has shown that sharing of search experiences among users having similar interests, typically called a [[community of practice]] or [[community of interest]], reduces the effort put in by a given user in retrieving the exact information of interest.<ref name=Rohini&Ambati>{{citation\n | title = A Collaborative Filtering based Re-ranking Strategy for Search in Digital Libraries\n | year = 2002\n | author = Rohini U\n | author2 = Vamshi Ambati\n | journal = ICADL2005: the 8th International Conference on Asian Digital Libraries\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-004.pdf }}</ref>\n\nCollaborative search deployed within a community of practice deploys novel techniques for exploiting context during search by indexing and ranking search results based on the learned preferences of a community of users.<ref name=Coyle2008>{{citation\n | title = Social Aspects of a Collaborative, Community-Based Search Network\n | editor4-first = Eelco\n | editor3-first = Pearl\n | editor2-first = Judy\n | editor1-first = Wolfgang\n | year = 2008\n | editor1-last = Nejdl\n | author = Maurice Coyle\n | author2 = Barry Smyth\n | last-author-amp = yes\n | journal = Adaptive Hypermedia and Adaptive Web-Based Systems\n | pages =  103–112  \n | volume = 5149/2008\n | issue = \n | series = | doi = 10.1007/978-3-540-70987-9\n | isbn = 978-3-540-70984-8\n | url = http://portal.acm.org/citation.cfm?id=1485050\n | editor2-last = Kay\n | editor4-last = Herder\n | editor3-last = Pu| display-editors = 3}}</ref> The users benefit by sharing information, experiences and awareness to personalize result-lists to reflect the preferences of the community as a whole. The community representing a group of users who share common interests, similar professions.  The best known example is the open-source project [[ApexKB]] (previously known as Jumper 2.0).<ref name=Jumper2010>{{citation\n | title = Jumper Networks Releases Jumper 2.0.1.5 Platform with New Community Search Features\n | year = 2010\n | author = Jumper Networks Inc.\n | journal = Press release\n | pages = \n | volume =\n | issue = \n | doi =\n | isbn =\n | url = http://www.trilexnet.com/labs/jumper}}</ref>\n\n=== Depth of mediation ===\n\nThis refers to the degree that the CSE mediates search.<ref name=Pickens2008 /> SearchTogether<ref name=Morris2007 /> is an example of UI-level mediation: users exchange query results and judgments of relevance, but the system does not distinguish among users when they run queries. Cerchiamo<ref name=Pickens2008 /> and recommendation systems such as I-Spy<ref name=Smith2003 /> keep track of each person\'s search activity independently, and use that information to affect their search results. These are examples of deeper algorithmic mediation.\n\n=== Task vs. trait ===\n\nThis model classifies people\'s membership in groups based on the task at hand vs. long-term interests; these may be correlated with explicit and implicit collaboration.<ref name=Morris2008 />\n\n== Privacy-aware collaborative search engines ==\n\nSearch terms and links clicked that are shared among users reveal their interests, habits, social\nrelations and intentions.<ref name=EUArticle29>{{citation\n | title = Article 29 EU Data Protection Working Party\n | year = 2008\n | author = Data Protection Working Party\n | journal = EU\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref> In other words, CSEs put the privacy of the users at risk. Studies have shown that CSEs increase efficiency. \n<ref name="Morris2007"/><ref name=Smith2005>{{citation\n | title = A Live-User Evaluation of Collaborative Web Search\n | year = 2005\n | author = Barry Smyth\n | author2 = Evelyn Balfe\n | author3 = Oisin Boydell\n | author4 = Keith Bradley\n | author5 = Peter Briggs\n | author6 = Maurice Coyle\n | author7 = Jill Freyne\n | journal = IJCAI\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref>\n<ref name=Smith2006>{{citation\n | title = Anonymous personalization in collaborative web search\n | year = 2005\n | author = Smyth, Barry\n | author2 = Balfe, Evelyn\n | last-author-amp = yes\n | journal = Inf. Retr.\n | pages = 165–190\n | volume = 9\n | issue = 2| doi = 10.1007/s10791-006-7148-z| isbn = \n | url = \n}}</ref>\n<ref name=Jung2004>{{citation\n | title = Applying Collaborative Filtering for Efficient Document Search\n | year = 2004\n | author = Seikyung Jung\n | author2 = Juntae Kim\n | author3 = Herlocker, JL\n | journal = Inf. Retr.\n | pages = 640–643\n | volume = \n | issue = \n | doi = \n | isbn = \n | url = \n}}</ref> Unfortunately, by the lack of privacy enhancing technologies, a privacy aware user who wants to benefit from a CSE has to disclose his entire search log. (Note, even when explicitly sharing queries and links clicked, the whole (former) log is disclosed to any user that joins a search session).  Thus, sophisticated mechanisms that allow on a more fine grained level which information is disclosed to whom are desirable.\n\nAs CSEs are a new technology just entering the market, identifying user privacy preferences and integrating [[Privacy enhancing technologies]] (PETs) into collaborative search are in conflict. On one hand, PETs have to meet user preferences, on the other hand one cannot identify these preferences without using a CSE, i.e., implementing PETs into CSEs. Today, the only work addressing this problem comes from Burghardt et al.<ref name=BurghardtCC2008>{{citation\n | title = Collaborative Search And User Privacy: How Can They Be Reconciled?\n | year = 2008\n | author = Thorben Burghardt\n | author2 = Erik Buchmann\n | author3 = Klemens Böhm\n | author4 = Chris Clifton\n | journal = CollaborateCom\n | pages = \n | volume = \n | issue = \n | doi = \n | isbn = \n | url = http://dbis.ipd.uni-karlsruhe.de/1184.php\n}}</ref> They implemented a CSE with experts from the information system domain and derived the scope of possible privacy preferences in a user study with these experts. Results show that users define preferences referring to (i) their current context (e.g., being at work), (ii) the query content (e.g., users exclude topics from sharing), (iii) time constraints (e.g., do not publish the query X hours after the query has been issued, do not store longer than X days, do not share between working time), and that users intensively use the option to (iv) distinguish between different social groups when sharing information. Further, users require (v) anonymization and (vi) define reciprocal constraints, i.e., they refer to the behavior of other users, e.g., if a user would have shared the same query in turn.\n\n== References ==\n{{reflist|2}}\n{{Internet search}}\n\n[[Category:Information retrieval systems]]']
['Sørensen–Dice coefficient', '9701718', 'The \'\'\'Sørensen–Dice index\'\'\', also known by other names (see [[Sørensen–Dice_coefficient#Name|Name]], below), is a [[statistic]] used for comparing the similarity of two [[Sample (statistics)|samples]]. It was independently developed by the [[botanist]]s [[Thorvald Sørensen]]<ref>{{cite journal |last=Sørensen |first=T. |year=1948 |title=A method of establishing groups of equal amplitude in [[plant sociology]] based on similarity of species and its application to analyses of the vegetation on Danish commons |journal=[[Kongelige Danske Videnskabernes Selskab]] |volume=5 |issue=4 |pages=1–34 |doi= }}</ref> and [[Lee Raymond Dice]],<ref>{{cite journal |last=Dice |first=Lee R. |title=Measures of the Amount of Ecologic Association Between Species |jstor=1932409 |journal=Ecology |volume=26 |issue=3 |year=1945 |pages=297–302 |doi=10.2307/1932409 }}</ref> who published in 1948 and 1945 respectively.\nThe Sørensen–Dice is also known as [[F1 score]] or Dice similarity coefficient (DSC).\n\n==Name==\nThe index is known by several other names, usually \'\'\'Sørensen index\'\'\' or \'\'\'Dice\'s coefficient\'\'\'. Both names also see "similarity coefficient", "index", and other such variations. Common alternate spellings for Sørensen are Sorenson, Soerenson index and Sörenson index, and all three can also be seen with the –sen ending.\n\nOther names include:\n*[[Jan Czekanowski|Czekanowski]]\'s binary (non-quantitative) index<ref name ="gallagher"/>\n\n==Formula==\nSørensen\'s original formula was intended to be applied to presence/absence data, and is\n\n:<math> QS =  \\frac{2 |X \\cap Y|}{|X|+ |Y|}</math>\n\nwhere |\'\'X\'\'| and |\'\'Y\'\'| are the numbers of elements in the two samples. Based on what is written here,\n\n:<math> DSC = \\frac{2 TP}{2 TP + FP + FN}</math>,\n\nas compared with the Jaccard index, which omits true negatives from both the numerator and the denominator. QS is the quotient of similarity and ranges between 0 and&nbsp;1.<ref>http://www.sekj.org/PDF/anbf40/anbf40-415.pdf</ref> It can be viewed as a similarity measure over sets.\n\nSimilarly to the [[Jaccard index]], the set operations can be expressed in terms of vector operations over binary vectors \'\'A\'\' and \'\'B\'\':\n\n:<math>s_v = \\frac{2 | A \\cdot B |}{| A |^2 + | B |^2} </math>\n\nwhich gives the same outcome over binary vectors and also gives a more general similarity metric over vectors in general terms.\n\nFor sets \'\'X\'\' and \'\'Y\'\' of keywords used in [[information retrieval]], the coefficient may be defined as twice the shared information (intersection) over the sum of cardinalities :<ref>{{cite book |last=van Rijsbergen |first=Cornelis Joost |year=1979\n|title=Information Retrieval\n|url=http://www.dcs.gla.ac.uk/Keith/Preface.html |publisher=Butterworths |location=London |isbn=3-642-12274-4 }}</ref>\n\nWhen taken as a string similarity measure, the coefficient may be calculated for two strings, \'\'x\'\' and \'\'y\'\' using [[bigram]]s as follows:<ref>{{cite conference |last=Kondrak |first=Grzegorz |author2=Marcu, Daniel |author3= Knight, Kevin  |year=2003\n|title=Cognates Can Improve Statistical Translation Models\n|booktitle=Proceedings of HLT-NAACL 2003: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics\n|pages=46–48 |url=http://aclweb.org/anthology/N/N03/N03-2016.pdf}}</ref>\n\n:<math>s = \\frac{2 n_t}{n_x + n_y}</math>\n\nwhere \'\'n\'\'<sub>\'\'t\'\'</sub> is the number of character bigrams found in both strings, \'\'n\'\'<sub>\'\'x\'\'</sub> is the number of bigrams in string \'\'x\'\' and \'\'n\'\'<sub>\'\'y\'\'</sub> is the number of bigrams in string \'\'y\'\'. For example, to calculate the similarity between:\n\n:<code>night</code>\n:<code>nacht</code>\n\nWe would find the set of bigrams in each word:\n:{<code>ni</code>,<code>ig</code>,<code>gh</code>,<code>ht</code>}\n:{<code>na</code>,<code>ac</code>,<code>ch</code>,<code>ht</code>}\n\nEach set has four elements, and the intersection of these two sets has only one element: <code>ht</code>.\n\nInserting these numbers into the formula, we calculate, \'\'s\'\'&nbsp;=&nbsp;(2&nbsp;·&nbsp;1)&nbsp;/&nbsp;(4&nbsp;+&nbsp;4)&nbsp;=&nbsp;0.25.\n\n==Difference from Jaccard ==\nThis coefficient is not very different in form from the [[Jaccard index]].  However, since it doesn\'t satisfy the triangle inequality, it can be considered a [[Metric (mathematics)#Generalized metrics|semimetric]] version of the Jaccard index.<ref name ="gallagher"/>\n\nThe function ranges between zero and one, like Jaccard. Unlike Jaccard, the corresponding difference function\n\n:<math>d = 1 -  \\frac{2 | X \\cap Y |}{| X | + | Y |} </math>\n\nis not a proper distance metric as it does not possess the property of [[triangle inequality]].<ref name ="gallagher">Gallagher, E.D., 1999. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.1334&rep=rep1&type=pdf COMPAH Documentation], University of Massachusetts, Boston</ref> The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of \'\'any\'\' two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.\n\n==Applications==\nThe Sørensen–Dice coefficient is useful for ecological community data (e.g. Looman & Campbell, 1960<ref>[http://links.jstor.org/sici?sici=0012-9658%28196007%2941%3A3%3C409%3AAOSK%28F%3E2.0.CO%3B2-1 Looman, J. and Campbell, J.B. (1960) Adaptation of Sorensen\'s K (1948) for estimating unit affinities in prairie vegetation. Ecology 41 (3): 409–416.]</ref>). Justification for its use is primarily  empirical rather than theoretical (although it can be justified  theoretically as the intersection of two [[fuzzy set]]s<ref>[http://dx.doi.org/10.1007/BF00039905 Roberts, D.W. (1986) Ordination on the basis of fuzzy set theory. Vegetatio 66 (3): 123–131.]</ref>). As compared to [[Euclidean distance]], Sørensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.<ref>McCune, Bruce & Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6.</ref> Recently the Dice score (and its variations, e.g. logDice taking a logarithm of it) has become popular in computer [[lexicography]] for measuring the lexical association score of two given words.<ref>[http://nlp.fi.muni.cz/raslan/2008/raslan08.pdf#page=14 Rychlý, P. (2008) A lexicographer-friendly association score. Proceedings of the Second Workshop on Recent Advances in Slavonic Natural Language Processing RASLAN 2008: 6–9]</ref> It is also commonly used in [[Image segmentation]], in particular for comparing algorithm output against reference masks in medical applications{{Citation needed|reason=Some seminal works need to be cited to show how Dice coefficient is used|date=December 2016}}.\n\n==Abundance version==\nThe expression is easily extended to [[Abundance (ecology)|abundance]] instead of presence/absence of species. This quantitative version is known by several names:\n* Quantitative Sørensen–Dice index<ref name ="gallagher"/>\n* Quantitative Sørensen index<ref name ="gallagher"/>\n* Quantitative Dice index<ref name ="gallagher"/>\n* [[Bray–Curtis dissimilarity|Bray–Curtis similarity]] (1 minus the \'\'Bray-Curtis dissimilarity\'\')<ref name ="gallagher"/>\n* [[Jan Czekanowski|Czekanowski]]\'s quantitative index<ref name ="gallagher"/>\n* Steinhaus index<ref name ="gallagher"/>\n* [[E. C. Pielou|Pielou]]\'s percentage similarity<ref name ="gallagher"/>\n* 1 minus the [[Hellinger distance]]<ref>{{cite journal |first=J. Roger |last=Bray |first2=J. T. |last2=Curtis |year=1957 |title=An Ordination of the Upland Forest Communities of Southern Wisconsin |journal=Ecological Monographs |volume=27 |issue=4 |pages=326–349 |doi=10.2307/1942268 }}</ref>\n\n==See also==\n* [[Correlation]]\n* [[Jaccard index]]\n* [[Hamming distance]]\n* [[Mantel test]]\n* [[Morisita\'s overlap index]]\n* [[Most frequent k characters]]\n* [[Overlap coefficient]]\n* [[Renkonen similarity index]] (due to [[Olavi Renkonen]])\n* [[Tversky index]]\n* [[Universal adaptive strategy theory (UAST)]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{Wikibooks|Algorithm implementation|Strings/Dice\'s coefficient|Dice\'s coefficient}}\n\n{{DEFAULTSORT:Sorensen-Dice coefficient}}\n[[Category:Information retrieval evaluation]]\n[[Category:String similarity measures]]\n[[Category:Measure theory]]']
['Mean reciprocal rank', '11184711', '{{Refimprove|date=June 2007}}\nThe \'\'\'mean reciprocal rank\'\'\' is a [[statistic]] measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the [[multiplicative inverse]] of the rank of the first correct answer. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:<ref>{{cite conference | title=Proceedings of the 8th Text Retrieval Conference | booktitle=TREC-8 Question Answering Track Report | author=E.M. Voorhees |year=1999 | pages=77&ndash;82}}</ref>\n\n:<math> \\text{MRR} = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{\\text{rank}_i}. \\!</math>\n\nwhere <math> \\text{rank}_i</math> refers to the rank position of the \'\'first\'\' relevant document for the \'\'i\'\'-th query.\n\nThe reciprocal value of the mean reciprocal rank corresponds to the [[harmonic mean]] of the ranks.\n\n== Example ==\nFor example, suppose we have the following three sample queries for a system that tries to translate English words to their plurals.  In each case, the system makes three guesses, with the first one being the one it thinks is most likely correct:\n\n{| class="wikitable"\n|-\n! Query\n! Results\n! Correct response\n! Rank\n! Reciprocal rank\n|-\n| cat\n| catten, cati, \'\'\'cats\'\'\'\n| cats\n| 3\n| 1/3\n|-\n|tori\n| torii, \'\'\'tori\'\'\', toruses\n| tori\n| 2\n| 1/2\n|-\n| virus\n| \'\'\'viruses\'\'\', virii, viri\n| viruses\n| 1\n| 1\n|}\n\nGiven those three samples, we could calculate the mean reciprocal rank as (1/3&nbsp;+&nbsp;1/2&nbsp;+&nbsp;1)/3 = 11/18 or about 0.61.\n\nThis basic definition does not specify what to do if none of the proposed results are correct, though reciprocal rank 0 could be used in this situation.  It also does not specify what do to if there are multiple correct answers in the list. In this case, [[Information retrieval#Mean average precision|mean average precision]] is a potential alternative metric.\n\n==See also==\n* [[Information retrieval]]\n* [[Question answering]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{cite conference | title=Evaluating web-based question answering systems | booktitle=Proceedings of LREC |author1=D. R. Radev |author2=H. Qi |author3=H. Wu |author4=W. Fan |year=2002 }}\n\n[[Category:Summary statistics]]\n[[Category:Information retrieval evaluation]]']
["Spearman's rank correlation coefficient", '235623', '[[File:spearman fig1.svg|300px|thumb|A Spearman correlation of 1 results when the two variables being compared are monotonically related, even if their relationship is not linear. This means that all data-points with greater x-values than that of a given data-point will have greater y-values as well. In contrast, this does not give a perfect Pearson correlation.]][[File:spearman fig2.svg|300px|thumb|When the data are roughly elliptically distributed and there are no prominent outliers, the Spearman correlation and Pearson correlation give similar values.]]\n[[File:spearman fig3.svg|300px|thumb|The Spearman correlation is less sensitive than the Pearson correlation to strong outliers that are in the tails of both samples. That is because Spearman\'s rho limits the outlier to the value of its rank.]] In [[statistics]], \'\'\'Spearman\'s rank correlation coefficient\'\'\' or \'\'\'Spearman\'s rho\'\'\', named after [[Charles Spearman]] and often denoted by the Greek letter [[rho (letter)|<math>\\rho</math>]] (rho) or as <math>r_s</math>, is a [[non-parametric statistics|nonparametric]] measure of [[rank correlation]] ([[correlation and dependence|statistical dependence]] between the [[ranking]] of two [[Variable (mathematics)#Applied statistics|variables]]). It assesses how well the relationship between two variables can be described using a [[monotonic]] function.\n\nThe \'\'\'Spearman correlation\'\'\' between two variables is equal to the [[Pearson product-moment correlation coefficient|Pearson correlation]] between the rank values of those two variables; while Pearson\'s correlation assesses linear relationships, Spearman\'s correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other.\n\nIntuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) [[Ranking|rank]] (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of -1) rank between the two variables.\n\nSpearman\'s coefficient is appropriate for both [[continuous variable|continuous]] and [[discrete variable]]s, including [[Level of measurement#Ordinal scale|ordinal]] variables.<ref>[[Level of measurement#Typology|Scale types]]</ref><ref>{{cite book|title=Jmp For Basic Univariate And Multivariate Statistics: A Step-by-step Guide|last=Lehman|first=Ann|publisher=SAS Press|year=2005|isbn=1-59047-576-3|location=Cary, NC|page=123}}</ref> Both Spearman\'s <math>\\rho</math> and [[Kendall tau rank correlation coefficient|Kendall\'s <math>\\tau</math>]] can be formulated as special cases of a more [[general correlation coefficient]].\n\n==Definition and calculation==\nThe Spearman correlation coefficient is defined as the [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]] between the [[Ranking|ranked variables]].<ref name="myers2003">{{Cite book | last1=Myers | first1=Jerome L. | first2=Arnold D.  |last2= Well | title=Research Design and Statistical Analysis | publisher=Lawrence Erlbaum | year=2003 | edition=2nd | isbn=0-8058-4037-0 | pages=508}}</ref>\n\nFor a sample of size \'\'n\'\', the \'\'n\'\' [[raw score]]s <math>X_i, Y_i</math> are converted to ranks <math>\\operatorname{rg} X_i, \\operatorname{rg} Y_i</math>, and <math>r_s</math> is computed from:\n\n:<math>r_s = \\rho_{\\operatorname{rg}_X,\\operatorname{rg}_Y} = \\frac {\\operatorname{cov}(\\operatorname{rg}_X,\\operatorname{rg}_Y)} { \\sigma_{\\operatorname{rg}_X} \\sigma_{\\operatorname{rg}_Y} }</math>\n:: where\n::* <math>\\rho</math> denotes the usual [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]], but applied to the rank variables.\n::* <math>\\operatorname{cov}(\\operatorname{rg}_X, \\operatorname{rg}_Y)</math> is the [[covariance]] of the rank variables.\n::* <math>\\sigma_{\\operatorname{rg}_X}</math> and <math>\\sigma_{\\operatorname{rg}_Y}</math> are the [[standard deviation]]s of the rank variables.\n\nOnly if all \'\'n\'\' ranks are \'\'distinct integers\'\', it can be computed using the popular formula\n\n:<math> r_s = {1- \\frac {6 \\sum d_i^2}{n(n^2 - 1)}}.</math>\n:: where\n::* <math>d_i = \\operatorname{rg}(X_i) - \\operatorname{rg}(Y_i)</math>, is the difference between the two ranks of each observation.\n::* \'\'n\'\' is the number of observations\n\nIdentical values are usually{{citation needed|date=May 2016}} each assigned [[Ranking#Fractional ranking .28.221 2.5 2.5 4.22 ranking.29|fractional ranks]] equal to the average of their positions in the ascending order of the values, which is equivalent to averaging over all possible permutations.\n\nIf ties are present in the data set, this equation yields incorrect results: Only if in both variables all ranks are distinct, then <math>\\sigma_{\\operatorname{rg}_X} \\sigma_{\\operatorname{rg}_Y} = \\operatorname{Var}{\\operatorname{rg}_X} = \\operatorname{Var}{\\operatorname{rg}_Y} = n(n^2 - 1)/6</math> (cf. [[tetrahedral number]] <math>T_{n-1}</math>).\nThe first equation—normalizing by the standard deviation—may even be used even when ranks are normalized to [0;1] ("relative ranks") because it is insensitive both to translation and linear scaling.\n<!-- For example, if [1,2,3,4,5] vs. [1,3,3,3,5] has r_s=0.894, but the simplified formula yields 0.877. -->\n\nThis method should also not be used in cases where the data set is truncated; that is, when the Spearman correlation coefficient is desired for the top X records (whether by pre-change rank or post-change rank, or both), the user should use the Pearson correlation coefficient formula given above.{{citation needed|date=September 2015}}\n\nThe standard error of the coefficient (\'\'σ\'\') was determined by Pearson in 1907 and Gosset in 1920. It is\n\n: <math> \\sigma_{r_s} = \\frac{ 0.6325 }{ \\sqrt{n-1} } </math>\n\n==Related quantities==\n{{Main article|Correlation and dependence}}\n\nThere are several other numerical measures that quantify the extent of [[statistical dependence]] between pairs of observations. The most common of these is the [[Pearson product-moment correlation coefficient]], which is a similar correlation method to Spearman\'s rank, that measures the “linear” relationships between the raw numbers rather than between their ranks.\n\nAn alternative name for the Spearman [[rank correlation]] is the “grade correlation”;<ref name="Yule and Kendall">{{cite book |last=Yule |first=G. \u2009U. |last2=Kendall |first2=M. \u2009G. |orig-year=1950 |title=An Introduction to the Theory of Statistics |edition=14th |year=1968 |publisher=Charles Griffin & Co. |page=268 }}</ref> in this, the “rank” of an observation is replaced by the “grade”. In continuous distributions, the grade of an observation is, by convention, always one half less than the rank, and hence the grade and rank correlations are the same in this case. More generally, the “grade” of an observation is proportional to an estimate of the fraction of a population less than a given value, with the half-observation adjustment at observed values. Thus this corresponds to one possible treatment of tied ranks. While unusual, the term “grade correlation” is still in use.<ref>{{cite journal |last=Piantadosi |first=J. |last2=Howlett |first2=P. |last3=Boland |first3=J. |year=2007 |title=Matching the grade correlation coefficient using a copula with maximum disorder |journal=Journal of Industrial and Management Optimization |volume=3 |issue=2 |pages=305–312 |doi= |url=http://aimsciences.org/journals/pdfs.jsp?paperID=2265&mode=abstract }}</ref>\n\n==Interpretation==\n{| style="float: right;"\n|+ \'\'\'Positive and negative Spearman rank correlations\'\'\'\n|- \n| [[File:spearman fig5.svg|300px|left|thumb|A positive Spearman correlation coefficient corresponds to an increasing monotonic trend between \'\'X\'\' and \'\'Y\'\'.]]\n| [[File:spearman fig4.svg|300px|thumb|A negative Spearman correlation coefficient corresponds to a decreasing monotonic trend between \'\'X\'\' and \'\'Y\'\'.]]\n|}\n\nThe sign of the Spearman correlation indicates the direction of association between \'\'X\'\' (the independent variable) and \'\'Y\'\' (the dependent variable).  If  \'\'Y\'\' tends to increase when \'\'X\'\' increases, the Spearman correlation coefficient is positive.  If \'\'Y\'\' tends to decrease when \'\'X\'\' increases, the Spearman correlation coefficient is negative.  A Spearman correlation of zero indicates that there is no tendency for \'\'Y\'\' to either increase or decrease when \'\'X\'\' increases.  The Spearman correlation increases in magnitude as \'\'X\'\' and \'\'Y\'\' become closer to being perfect monotone functions of each other.  When \'\'X\'\' and \'\'Y\'\' are perfectly monotonically related, the Spearman correlation coefficient becomes 1.  A perfect monotone increasing relationship implies that for any two pairs of data values {{math|\'\'X\'\'<sub>\'\'i\'\'</sub>, \'\'Y\'\'<sub>\'\'i\'\'</sub>}} and {{math|\'\'X\'\'<sub>\'\'j\'\'</sub>, \'\'Y\'\'<sub>\'\'j\'\'</sub>}}, that {{math|\'\'X\'\'<sub>\'\'i\'\'</sub> − \'\'X\'\'<sub>\'\'j\'\'</sub>}} and {{math|\'\'Y\'\'<sub>\'\'i\'\'</sub> − \'\'Y\'\'<sub>\'\'j\'\'</sub>}} always have the same sign.  A perfect monotone decreasing relationship implies that these differences always have opposite signs.\n\nThe Spearman correlation coefficient is often described as being "nonparametric".  This can have two meanings:  First, a perfect Spearman correlation results when \'\'X\'\' and \'\'Y\'\' are related by any [[monotonic function]]. Contrast this with the Pearson correlation, which only gives a perfect value when \'\'X\'\' and \'\'Y\'\' are related by a \'\'linear\'\' function. The other sense in which the Spearman correlation is nonparametric in that its exact sampling distribution can be obtained without requiring knowledge (\'\'i.e.\'\', knowing the parameters) of the joint [[probability distribution]] of \'\'X\'\' and \'\'Y\'\'.\n\n==Example==\nIn this example, the raw data in the table below is used to calculate the correlation between the [[IQ]] of a person with the number of hours spent in front of [[TV]] per week.\n{| class="wikitable sortable" style="text-align:right;"\n|-\n![[IQ]], <math>X_i</math>\n!Hours of [[TV]] per week, <math>Y_i</math>\n|-\n|106\n|7\n|-\n|86\n|0\n|-\n|100\n|27\n|-\n|101\n|50\n|-\n|99\n|28\n|-\n|103\n|29\n|-12\n|97\n|20\n|-\n|113\n|12\n|-\n|112\n|6\n|-\n|110\n|17\n|}\n\nFirstly, evaluate <math>d^2_i</math>. To do so use the following steps, reflected in the table below.\n# Sort the data by the first column (<math>X_i</math>). Create a new column <math>x_i</math> and assign it the ranked values 1,2,3,...\'\'n\'\'.\n# Next, sort the data by the second column (<math>Y_i</math>). Create a fourth column <math>y_i</math> and similarly assign it the ranked values 1,2,3,...\'\'n\'\'.\n# Create a fifth column <math>d_i</math> to hold the differences between the two rank columns (<math>x_i</math> and <math>y_i</math>).\n# Create one final column <math>d^2_i</math> to hold the value of column <math>d_i</math> squared.\n\n{| class="wikitable sortable" style="text-align:right;"\n|-\n![[IQ]], <math>X_i</math>\n!Hours of [[TV]] per week, <math>Y_i</math>\n!rank <math>x_i</math>\n!rank <math>y_i</math>\n!<math>d_i</math>\n!<math>d^2_i</math>\n|-\n|86\n|0\n|1\n|1\n|0\n|0\n|-\n|97\n|20\n|2\n|6\n| −4\n|16\n|-\n|99\n|28\n|3\n|8\n| −5\n|25\n|-\n|100\n|27\n|4\n|7\n| −3\n|9\n|-\n|101\n|50\n|5\n|10\n| −5\n|25\n|-\n|103\n|29\n|6\n|9\n| −3\n|9\n|-\n|106\n|7\n|7\n|3\n|4\n|16\n|-\n|110\n|17\n|8\n|5\n|3\n|9\n|-\n|112\n|6\n|9\n|2\n|7\n|49\n|-\n|113\n|12\n|10\n|4\n|6\n|36\n|}\n\nWith <math>d^2_i</math> found, add them to find <math>\\sum d_i^2 = 194</math>. The value of \'\'n\'\' is 10. These values can now be substituted back into the equation: <math> \\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}}.</math> to give\n\n:<math> \\rho = 1- {\\frac {6\\times194}{10(10^2 - 1)}}</math>\n\nwhich evaluates to {{math|1=\'\'ρ\'\' = -29/165 = −0.175757575...}}\nwith a [[P-value]] = 0.627188 (using the [[Student\'s t-distribution|t distribution]])\n\n[[File:Spearman\'s Rank chart.png|thumb|Chart of the data presented. It can be seen that there might be a negative correlation, but that the relationship does not appear definitive.]]\nThis low value shows that the correlation between IQ and hours spent watching TV is very low, although the negative value suggests that the longer the time spent watching television the lower the IQ. In the case of ties in the original values, this formula should not be used; instead, the Pearson correlation coefficient should be calculated on the ranks (where ties are given ranks, as described above).\n\n==Determining significance==\nOne approach to test whether an observed value of ρ is significantly different from zero (\'\'r\'\' will always maintain −1 ≤ \'\'r\'\' ≤ 1) is to calculate the probability that it would be greater than or equal to the observed \'\'r\'\', given the [[null hypothesis]], by using a [[Resampling (statistics)#Permutation tests|permutation test]]. An advantage of this approach is that it automatically takes into account the number of tied data values there are in the sample, and the way they are treated in computing the rank correlation.\n\nAnother approach parallels the use of the [[Fisher transformation]] in the case of the Pearson product-moment correlation coefficient. That is, [[confidence intervals]] and [[hypothesis test]]s relating to the population value ρ can be carried out using the Fisher transformation:\n\n: <math>F(r) = {1 \\over 2}\\ln{1+r \\over 1-r} = \\operatorname{artanh}(r).</math>\n\nIf \'\'F\'\'(\'\'r\'\') is the Fisher transformation of \'\'r\'\', the sample Spearman rank correlation coefficient, and \'\'n\'\' is the sample size, then\n\n:<math>z = \\sqrt{\\frac{n-3}{1.06}}F(r)</math>\n\nis a [[standard score|z-score]] for \'\'r\'\' which approximately follows a standard [[normal distribution]] under the [[null hypothesis]] of [[statistical independence]] ({{math|1=\'\'ρ\'\' = 0}}).<ref>{{cite journal |last=Choi |first=S. C. |year=1977 |title=Tests of Equality of Dependent Correlation Coefficients |journal=[[Biometrika]] |volume=64 |issue=3 |pages=645–647 |doi=10.1093/biomet/64.3.645 }}</ref><ref>{{cite journal |last=Fieller |first=E. C. |last2=Hartley |first2=H. O. |last3=Pearson |first3=E. S. |year=1957 |title=Tests for rank correlation coefficients. I |journal=Biometrika |volume=44 |issue= |pages=470–481 |doi=10.1093/biomet/44.3-4.470}}</ref>\n\nOne can also test for significance using\n\n:<math>t = r \\sqrt{\\frac{n-2}{1-r^2}}</math>\n\nwhich is distributed approximately as [[Student\'s t distribution]] with {{math|\'\'n\'\' − 2}} degrees of freedom under the [[null hypothesis]].<ref>{{cite book |last=Press |last2=Vettering |last3=Teukolsky |last4=Flannery |year=1992 |title=Numerical Recipes in C: The Art of Scientific Computing |edition=2nd |page=640 }}</ref> A justification for this result relies on a permutation argument.<ref>{{cite book |last=Kendall |first=M. G. |last2=Stuart |first2=A. |year=1973 |title=The Advanced Theory of Statistics, Volume 2: Inference and Relationship |publisher=Griffin |isbn=0-85264-215-6 }} (Sections 31.19, 31.21)</ref>\n\npvrank<ref>{{cite web|last1=Amerise|first1=I.L.|last2=Marozzi|first2=M.|last3=Tarsitano|first3=A.|title=R package pvrank|url=https://cran.r-project.org/web/packages/pvrank/index.html}}</ref> is a very recent [[R (programming language)|R]] package that computes rank correlations and their p-values with various options for tied ranks. It is possible to compute exact Spearman coefficient test p-values for \'\'n\'\' ≤ 26.\n\nA generalization of the Spearman coefficient is useful in the situation where there are three or more conditions, a number of subjects are all observed in each of them, and it is predicted that the observations will have a particular order.  For example, a number of subjects might each be given three trials at the same task, and it is predicted that performance will improve from trial to trial.  A test of the significance of the trend between conditions in this situation was developed by E. B. Page<ref>{{cite journal |author=Page, E. B. |title=Ordered hypotheses for multiple treatments: A significance test for linear ranks |journal=Journal of the American Statistical Association |volume=58 |pages=216–230 |year=1963 |doi=10.2307/2282965 |issue=301}}\n</ref> and is usually referred to as [[Page\'s trend test]] for ordered alternatives.\n\n==Correspondence analysis based on Spearman\'s rho==\nClassic [[correspondence analysis]] is a statistical method that gives a score to every value of two nominal variables. In this way the Pearson [[Pearson product-moment correlation coefficient|correlation coefficient]] between them is maximized.\n\nThere exists an equivalent of this method, called [[grade correspondence analysis]], which maximizes Spearman\'s rho or [[Kendall\'s tau]].<ref>{{cite book|editor1-last=Kowalczyk|editor1-first=T.|editor2-last=Pleszczyńska|editor2-first=E.|editor3-last=Ruland|editor3-first=F.| year=2004|title=Grade Models and Methods for Data Analysis with Applications for the Analysis of Data Populations|series=Studies in Fuzziness and Soft Computing |volume=151|publisher=Springer Verlag|location=Berlin Heidelberg New York|isbn=978-3-540-21120-4}}</ref>\n\n==See also==\n{{Portal|Statistics}}\n* [[Kendall tau rank correlation coefficient]]\n* [[Chebyshev\'s sum inequality]], [[rearrangement inequality]] (These two articles may shed light on the mathematical properties of Spearman\'s ρ.)\n*[[Distance correlation]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* Corder, G.W. & Foreman, D.I. (2014). Nonparametric Statistics: A Step-by-Step Approach, Wiley. ISBN 978-1118840313.\n* {{cite book |last=Daniel |first=Wayne W. |chapter=Spearman rank correlation coefficient |title=Applied Nonparametric Statistics |location=Boston |publisher=PWS-Kent |edition=2nd |year=1990 |isbn=0-534-91976-6 |pages=358–365 |chapterurl=https://books.google.com/books?id=0hPvAAAAMAAJ&pg=PA358 }}\n* {{Cite journal |author=Spearman C |title=The proof and measurement of association between two things |journal=American Journal of Psychology |volume=15 |year=1904 |pages=72–101 |doi=10.2307/1412159}}\n* {{Cite journal |author=Bonett DG, Wright, TA |title=Sample size requirements for Pearson, Kendall, and Spearman correlations |journal=Psychometrika |volume=65 |year=2000 |pages=23–28 |doi=10.1007/bf02294183}}\n* {{Cite book |author=Kendall MG |title=Rank correlation methods |location=London |publisher=Griffin |year=1970 |edition=4th |isbn=978-0-852-6419-96 |oclc=136868}}\n* {{Cite book |vauthors=Hollander M, Wolfe DA |title=Nonparametric statistical methods |location=New York |publisher=Wiley |year=1973 |isbn=978-0-471-40635-8 |oclc=520735}}\n* {{Cite journal |vauthors=Caruso JC, Cliff N |title=Empirical size, coverage, and power of confidence intervals for Spearman\'s Rho |journal=Educational and Psychological Measurement |volume=57 |year=1997 |pages=637–654 |doi=10.1177/0013164497057004009}}\n\n==External links==\n{{Wikiversity}}\n*[http://www.crystalballservices.com/Resources/ConsultantsCornerBlog/EntryId/73/Copulas-Vs-Correlation.aspx "Understanding Correlation vs. Copulas in Excel"] by Eric Torkia, Technology Partnerz 2011\n*[http://www.sussex.ac.uk/Users/grahamh/RM1web/Rhotable.htm Table of critical values of ρ for significance with small samples]\n*[http://www.maccery.com/maths Spearman\'s rank online calculator]\n*[https://www.answerminer.com/calculators/correlation-test Spearman correlation calculator with human-readable explanation]\n*[http://faculty.vassar.edu/lowry/webtext.html Chapter 3 part 1 shows the formula to be used when there are ties]\n*[http://statistical-research.com/wp-content/uploads/2012/08/Spearman.pdf An example of how to calculate Spearman\'s Rho along with basic R code.]\n* [https://www.rgs.org/NR/rdonlyres/4844E3AB-B36D-4B14-8A20-3A3C28FAC087/0/OASpearmansRankExcelGuidePDF.pdf Spearman’s Rank Correlation Coefficient – Excel Guide]: sample data and formulae for Excel, developed by the [[Royal Geographical Society]].\n*[http://udel.edu/~mcdonald/statspearman.html Spearman\'s rank correlation]: Simple notes for students with an example of usage by biologists and a spreadsheet for [[Microsoft Excel]] for calculating it (a part of materials for a \'\'Research Methods in Biology\'\' course).\n{{Statistics|descriptive}}\n\n{{DEFAULTSORT:Spearman\'s Rank Correlation Coefficient}}\n[[Category:Covariance and correlation]]\n[[Category:Information retrieval evaluation]]\n[[Category:Nonparametric statistics]]\n[[Category:Statistical tests]]']
['Category:Ranking functions', '19988453', '[[Category:Information retrieval techniques]]\n[[Category:Rankings]]']
['Stemming', '30874683', '{{about||the skiing technique|Stem (skiing)|the climbing technique|Glossary of climbing terms#stem}}\n{{Expert needed|date=October 2010}}\nIn [[linguistic morphology]] and [[information retrieval]], \'\'\'stemming\'\'\' is the process of reducing inflected (or sometimes derived) words to their [[word stem]], base or [[root (linguistics)|root]] form—generally a written word form. The stem need not be identical to the [[morphological root]] of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. [[Algorithm]]s for stemming have been studied in [[computer science]] since the 1960s. Many [[search engine]]s treat words with the same stem as [[synonym]]s as a kind of [[query expansion]], a process called conflation.\n\nStemming programs are commonly referred to as stemming algorithms or stemmers.\n\n==Examples==\nA stemmer for English, for example, should identify the [[string literal|string]] "cats" (and possibly "catlike", "catty" etc.) as based on the root "cat", and "stems", "stemmer", "stemming", "stemmed" as based on "stem". A stemming algorithm reduces the words "fishing", "fished", and "fisher" to the root word, "fish". On the other hand, "argue", "argued", "argues", "arguing", and "argus" reduce to the stem "argu" (illustrating the case where the stem is not itself a word or root) but "argument" and "arguments" reduce to the stem "argument".<!-- using the Porter algorithm -->\n\n==History==\nThe first published stemmer was written by [[Julie Beth Lovins]] in 1968.<ref>{{cite journal |first=Julie Beth |last=Lovins |year=1968 |title=Development of a Stemming Algorithm |journal=Mechanical Translation and Computational Linguistics |volume=11 |pages=22–31 }}</ref> This paper was remarkable for its early date and had great influence on later work in this area.\n\nA later stemmer was written by [[Martin Porter]] and was published in the July 1980 issue of the journal \'\'Program\'\'. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the [[Tony Kent Strix award]] in 2000 for his work on stemming and information retrieval.\n\nMany implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official [[free software]] (mostly [[BSD licenses|BSD]]-licensed) implementation<ref>http://tartarus.org/~martin/PorterStemmer/</ref> of the algorithm around the year 2000. He extended this work over the next few years by building [[Snowball programming language|Snowball]], a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages.\n\n==Algorithms==\nThere are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome.\n\nA simple stemmer looks up the inflected form in a [[lookup table]]. The advantages of this approach are that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. iPads ~ iPad), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root.\n\nA lookup approach may use preliminary part-of-speech tagging to avoid overstemming.<ref>Yatsko, V. A.; [http://yatsko.zohosites.com/y-stemmer.html \'\'Y-stemmer\'\']</ref>\n\n===The production technique===\n\nThe lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is "run", then the inverted algorithm might automatically generate the forms "running", "runs", "runned", and "runly". The last two forms are valid constructions, but they are unlikely.\n\n===Suffix-stripping algorithms===\nSuffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of "rules" is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include:\n* if the word ends in \'ed\', remove the \'ed\'\n* if the word ends in \'ing\', remove the \'ing\'\n* if the word ends in \'ly\', remove the \'ly\'\n\nSuffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like \'ran\' and \'run\'). The solutions produced by suffix stripping algorithms are limited to those [[lexical category|lexical categories]] which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. [[Lemmatisation]] attempts to improve upon this challenge.\n\nPrefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing.\n\n====Additional algorithm criteria====\nSuffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.\n\nIt can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term \'\'friendlies\'\', the algorithm may identify the \'\'ies\'\' suffix and apply the appropriate rule and achieve the result of \'\'friendl\'\'. \'\'friendl\'\' is likely not found in the lexicon, and therefore the rule is rejected.\n\nOne improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces \'\'ies\'\' with \'\'y\'\'. How this affects the algorithm varies on the algorithm\'s design. To illustrate, the algorithm may identify that both the \'\'ies\'\' suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, \'\'friendlies\'\' becomes \'\'friendly\'\' instead of \'\'friendl\'\'.\n\nDiving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term \'\'friendly\'\', where the \'\'ly\'\' stripping rule is likely identified and accepted. In summary, \'\'friendlies\'\' becomes (via substitution) \'\'friendly\'\' which becomes (via stripping) \'\'friend\'\'.\n\nThis example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for \'\'friendlies\'\' in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form \'\'friend\'\'. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.\n\n===Lemmatisation algorithms===\nA more complex approach to the problem of determining a stem of a word is [[lemmatisation]]. This process involves first determining the [[part of speech]] of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word\'s part of speech.\n\nThis approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem).\n\n===Stochastic algorithms===\n[[Stochastic]] algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they "learn") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).\n\nSome lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.\n\n===\'\'n\'\'-gram analysis===\nSome stemming techniques use the [[n-gram]] context of a word to choose the correct stem for a word.<ref name="CEUR Proceedings">{{cite journal|last1=McNamee|first1=Paul|title=Exploring New Languages with HAIRCUT at CLEF 2005|journal=CEUR Workshop Proceedings|date=September 21–22, 2005|volume=1171|url=http://ceur-ws.org/Vol-1171/CLEF2005wn-adhoc-McNamee2005.pdf|accessdate=3/6/15}}</ref>\n\n===Hybrid approaches===\nHybrid approaches use two or more of the approaches described above in unison. A simple example is a suffix tree algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of "frequent exceptions" like "ran => run". If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result.\n\n===Affix stemmers===\nIn [[linguistics]], the term [[affix]] refers to either a [[prefix]] or a [[suffix]]. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word \'\'indefinitely\'\', identify that the leading "in" is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name \'\'\'affix stripping\'\'\'. A study of affix stemming for several European languages can be found here.<ref>Jongejan, B.; and Dalianis, H.; \'\'Automatic Training of Lemmatization Rules that Handle Morphological Changes in pre-, in- and Suffixes Alike\'\', in the \'\'Proceeding of the ACL-2009, Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Singapore, August 2–7, 2009\'\', pp. 145-153\n[http://www.aclweb.org/anthology/P/P09/P09-1017.pdf]</ref>\n\n===Matching algorithms===\nSuch algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the "brows" in "browse" and in "browsing"). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix "be", which is the stem of such words as "be", "been" and "being", would not be considered as the stem of the word "beside").\n\n==Language challenges==\nWhile much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.<ref>Dolamic, Ljiljana; and Savoy, Jacques; [http://clef.isti.cnr.it/2007/working_notes/DolamicCLEF2007.pdf \'\'Stemming Approaches for East European Languages (CLEF 2007)\'\']</ref><ref>Savoy, Jacques; [http://portal.acm.org/citation.cfm?doid=1141277.1141523 \'\'Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages\'\'], ACM Symposium on Applied Computing, SAC 2006, ISBN 1-59593-108-2</ref><ref>Popovič, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract \'\'The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data\'\'], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp. 384–390</ref><ref>[http://staff.science.uva.nl/~mdr/Publications/Files/clef2005-proc-adhoc.pdf \'\'Stemming in Hungarian at CLEF 2005\'\']</ref><ref>Viera, A. F. G. & Virgil, J. (2007); [http://InformationR.net/ir/12-3/paper315.html \'\'Uma revisão dos algoritmos de radicalização em língua portuguesa\'\'], Information Research, 12(3), paper 315</ref>\n\nHebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as "dries" being the third-person singular present form of the verb "dry", "axes" being the plural of "axe" as well as "axis"); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun [[declension]]s), a Hebrew one is even more complex (due to [[nonconcatenative morphology]], a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on.\n\n===Multilingual stemming===\nMultilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist{{Citation needed|date=October 2013}}.\n\n==Error metrics==\nThere are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have been—a [[false positive]]. Understemming is an error where two separate inflected words should be stemmed to the same root, but are not—a [[false negative]]. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.\n\nFor example, the widely used Porter stemmer stems "universal", "university", and "universe" to "univers". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.\n\nAn example of understemming in the Porter stemmer is "alumnus" → "alumnu", "alumni" → "alumni", "alumna"/"alumnae" → "alumna".  This English word keeps Latin morphology, and so these near-synonyms are not conflated.\n\n==Applications==\nStemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning "daffodils" is probably closely related to a text mentioning "daffodil" (without the s). But in some cases, words with the same morphological stem have [[idiom]]atic meanings which are not closely related: a user searching for "marketing" will not be satisfied by most documents mentioning "markets" but not "marketing".\n\n===Information retrieval===\nStemmers are common elements in [[Information Retrieval|query systems]] such as [[World Wide Web|Web]] [[search engine]]s. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early [[information retrieval]] researchers to deem stemming irrelevant in general.<ref>Baeza-Yates, Ricardo; and Ribeiro-Neto, Berthier (1999); \'\'Modern Information Retrieval\'\', ACM Press/Addison Wesley</ref> An alternative approach, based on searching for [[n-gram]]s rather than stems, may be used instead. Also, stemmers may provide greater benefits in other languages than English.<ref>Kamps, Jaap; Monz, Christof; de Rijke, Maarten; and Sigurbjörnsson, Börkur (2004); \'\'Language-Dependent and Language-Independent Approaches to Cross-Lingual Text Retrieval\'\', in Peters, C.; Gonzalo, J.; Braschler, M.; and Kluck, M. (eds.); \'\'Comparative Evaluation of Multilingual Information Access Systems\'\', Springer Verlag, pp. 152–165</ref><ref>Airio, Eija (2006); \'\'Word Normalization and Decompounding in Mono- and Bilingual IR\'\', Information Retrieval \'\'\'9\'\'\':249–271</ref>\n\n===Domain Analysis===\nStemming is used to determine domain vocabularies in [[domain analysis]].\n<ref>Frakes, W.; Prieto-Diaz, R.; & Fox, C. (1998); \'\'DARE: Domain Analysis and Reuse Environment\'\', Annals of Software Engineering (5), pp. 125-141</ref>\n\n===Use in commercial products===\nMany commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages.<ref>[http://www.dtsearch.co.uk/language.htm \'\'Language Extension Packs\'\'], dtSearch</ref><ref>[http://technet2.microsoft.com/Office/en-us/library/87065c9d-d39d-479d-909b-02160ec6d7791033.mspx?mfr=true \'\'Building Multilingual Solutions by using Sharepoint Products and Technologies\'\'], Microsoft Technet</ref>\n\nThe [[Snowball (programming language)|Snowball]] stemmers have been compared with commercial lexical stemmers with varying results.<ref>[http://clef.isti.cnr.it/2003/WN_web/19.pdf CLEF 2003: Stephen Tomlinson compared the Snowball stemmers with the Hummingbird lexical stemming (lemmatization) system]</ref><ref>[http://clef.isti.cnr.it/2004/working_notes/WorkingNotes2004/21.pdf CLEF 2004: Stephen Tomlinson "Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServer"]</ref>\n\n[[Google search]] adopted word stemming in 2003.<ref>[http://www.google.com/support/bin/static.py?page=searchguides.html&ctx=basics#stemming \'\'The Essentials of Google Search\'\'], Web Search Help Center, [[Google|Google Inc.]]</ref> Previously a search for "fish" would not have returned "fishing". Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find "fish" in "fishing" but when searching for "fishes" will not find occurrences of the word "fish".\n\n==See also==\n* [[Root (linguistics)]] - linguistic definition of the term "root"\n* [[Stem (linguistics)]] - linguistic definition of the term "stem"\n* [[Morphology (linguistics)]]\n* [[Lemma (morphology)]] - linguistic definition\n* [[Lemmatization]]\n* [[Lexeme]]\n* [[Inflection]]\n* [[Derivation (linguistics)|Derivation]] - stemming is a form of reverse derivation\n* [[Natural language processing]] - stemming is generally regarded as a form of NLP\n* [[Text mining]] - stemming algorithms play a major role in commercial NLP software\n* [[Computational linguistics]]\n* [[Snowball (programming language)]] - designed for creating stemming algorithms\n\n{{Natural Language Processing}}\n\n==References==\n{{reflist|2}}\n\n==Further reading==\n{{refbegin|2}}\n* Dawson, J. L. (1974); \'\'Suffix Removal for Word Conflation\'\', Bulletin of the Association for Literary and Linguistic Computing, 2(3): 33–46\n* Frakes, W. B. (1984); \'\'Term Conflation for Information Retrieval\'\', Cambridge University Press\n* Frakes, W. B. & Fox, C. J. (2003); \'\'Strength and Similarity of Affix Removal Stemming Algorithms\'\', SIGIR Forum, 37: 26–30\n* Frakes, W. B. (1992); \'\'Stemming algorithms, Information retrieval: data structures and algorithms\'\', Upper Saddle River, NJ: Prentice-Hall, Inc.\n* Hafer, M. A. & Weiss, S. F. (1974); \'\'Word segmentation by letter successor varieties\'\', Information Processing & Management 10 (11/12), 371–386\n* Harman, D. (1991); \'\'How Effective is Suffixing?\'\', Journal of the American Society for Information Science 42 (1), 7–15\n* Hull, D. A. (1996); \'\'Stemming Algorithms&nbsp;– A Case Study for Detailed Evaluation\'\', JASIS, 47(1): 70–84\n* Hull, D. A. & Grefenstette, G. (1996); \'\'A Detailed Analysis of English Stemming Algorithms\'\', Xerox Technical Report\n* Kraaij, W. & Pohlmann, R. (1996); \'\'Viewing Stemming as Recall Enhancement\'\', in Frei, H.-P.; Harman, D.; Schauble, P.; and Wilkinson, R. (eds.); \'\'Proceedings of the 17th ACM SIGIR conference held at Zurich, August 18–22\'\', pp.&nbsp;40–48\n* Krovetz, R. (1993); \'\'Viewing Morphology as an Inference Process\'\', in \'\'Proceedings of ACM-SIGIR93\'\', pp.&nbsp;191–203\n* Lennon, M.; Pierce, D. S.; Tarry, B. D.; & Willett, P. (1981); \'\'An Evaluation of some Conflation Algorithms for Information Retrieval\'\', Journal of Information Science, 3: 177–183\n* Lovins, J. (1971); \'\'[http://www.eric.ed.gov/sitemap/html_0900000b800c571a.html Error Evaluation for Stemming Algorithms as Clustering Algorithms]\'\', JASIS, 22: 28–40\n* Lovins, J. B. (1968); \'\'Development of a Stemming Algorithm\'\', Mechanical Translation and Computational Linguistics, 11, 22—31\n* Jenkins, Marie-Claire; and Smith, Dan (2005); [http://www.uea.ac.uk/polopoly_fs/1.85493!stemmer25feb.pdf \'\'Conservative Stemming for Search and Indexing\'\']\n* Paice, C. D. (1990); \'\'[http://www.comp.lancs.ac.uk/computing/research/stemming/paice/article.htm Another Stemmer]\'\', SIGIR Forum, 24: 56–61\n* Paice, C. D. (1996) \'\'[http://www3.interscience.wiley.com/cgi-bin/abstract/57804/ABSTRACT Method for Evaluation of Stemming Algorithms based on Error Counting]\'\', JASIS, 47(8): 632–649\n* Popovič, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract \'\'The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data\'\'], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp.&nbsp;384–390\n* Porter, Martin F. (1980); \'\'[http://telemat.det.unifi.it/book/2001/wchange/download/stem_porter.html An Algorithm for Suffix Stripping]\'\', Program, 14(3): 130–137\n* Savoy, J. (1993); \'\'[http://www3.interscience.wiley.com/cgi-bin/abstract/10049824/ABSTRACT?CRETRY=1&SRETRY=0 Stemming of French Words Based on Grammatical Categories]\'\' Journal of the American Society for Information Science, 44(1), 1–9\n* Ulmschneider, John E.; & Doszkocs, Tamas (1983); \'\'[http://www.eric.ed.gov/sitemap/html_0900000b8007ea83.html A Practical Stemming Algorithm for Online Search Assistance]\'\', Online Review, 7(4), 301–318\n* Xu, J.; & Croft, W. B. (1998); \'\'[http://portal.acm.org/citation.cfm?doid=267954.267957 Corpus-Based Stemming Using Coocurrence of Word Variants]\'\', ACM Transactions on Information Systems, 16(1), 61–81\n{{refend}}\n\n==External links==\n*[http://opennlp.apache.org/index.html Apache OpenNLP] includes Porter and Snowball stemmers\n* [http://smile-stemmer.appspot.com SMILE Stemmer] - free online service, includes Porter and Paice/Husk\' Lancaster stemmers (Java API)\n* [http://code.google.com/p/ir-themis/ Themis] - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API)\n* [http://snowballstem.org Snowball] - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages\n* [http://www.iveonik.com/blog/2011/08/snowball-stemmers-on-csharp-free-download/ Snowball on C#] - port of Snowball stemmers for C# (14 languages)\n* [http://snowball.tartarus.org/wrappers/guide.html Python bindings to Snowball API]\n* [http://locknet.ro/archive/2009-10-29-ann-ruby-stemmer.html Ruby-Stemmer] - Ruby extension to Snowball API\n* [http://pecl.php.net/package/stem/ PECL] - PHP extension to the Snowball API\n* [http://www.oleandersolutions.com/stemming.html Oleander Porter\'s algorithm] - stemming library in C++ released under BSD\n* [http://www.cs.waikato.ac.nz/~eibe/stemmers/index.html Unofficial home page of the Lovins stemming algorithm] - with source code in a couple of languages\n* [http://www.tartarus.org/~martin/PorterStemmer/index.html Official home page of the Porter stemming algorithm] - including source code in several languages\n* [http://www.comp.lancs.ac.uk/computing/research/stemming/index.htm Official home page of the Lancaster stemming algorithm] - Lancaster University, UK\n* [https://www.uea.ac.uk/computing/word-stemming/ Official home page of the UEA-Lite Stemmer ] - University of East Anglia, UK\n* [http://www.comp.lancs.ac.uk/computing/research/stemming/general/index.htm Overview of stemming algorithms]\n* [http://code.google.com/p/ptstemmer/ PTStemmer] - A Java/Python/.Net stemming toolkit for the Portuguese language\n* [http://mazko.github.com/jssnowball/ jsSnowball] - open source JavaScript implementation of Snowball stemming algorithms for many languages\n* [http://trimc-nlp.blogspot.com/2013/08/snowball-stemmer-for-java.html Snowball Stemmer] - implementation for Java\n* [http://hlt.di.fct.unl.pt/luis/hindi_stemmer/ hindi_stemmer] - open source stemmer for Hindi\n* [http://hlt.di.fct.unl.pt/luis/czech_stemmer/ czech_stemmer] - open source stemmer for Czech\n* [http://www.comp.leeds.ac.uk/eric/sawalha08coling.pdf Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers]\n* [https://github.com/rdamodharan/tamil-stemmer Tamil Stemmer]\n\n{{FOLDOC}}\n\n[[Category:Linguistic morphology]]\n[[Category:Natural language processing]]\n[[Category:Tasks of natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Information retrieval techniques]]']
['Term Discrimination', '15101979', '\'\'\'Term Discrimination\'\'\' is a way to rank keywords in how useful they are for [[information retrieval]].\n\n== Overview ==\n\nThis is a method similar to [[tf-idf]] but it deals with finding keywords suitable for [[information retrieval]] and ones that are not.  Please refer to [[Vector Space Model]] first.\n\nThis method uses the concept of \'\'Vector Space Density\'\' that the less dense an [[occurrence matrix]] is, the better an information retrieval query will be.\n\nAn optimal index term is one that can distinguish two different documents from each other and relate two similar documents.  On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents.\n\nThe discrimination value is the difference in the occurrence matrix\'s vector-space density versus the same matrix\'s vector-space without the index term\'s density.\n\n Let:\n <math>A</math> be the occurrence matrix\n <math>A_k</math> be the occurrence matrix without the index term <math>k</math>\n and <math>Q(A)</math> be density of <math>A</math>.\n Then:\n The discrimination value of the index term <math>k</math> is: \n <math>DV_k = Q(A) - Q(A_k)</math>\n\n== How to compute ==\n\nGiven an [[occurrency matrix]]: <math>A</math> and one keyword: <math>k</math>\n* Find the global document [[centroid]]: <math>C</math> (this is just the average document vector)\n* Find the average [[euclidean distance]] from every document vector, <math>D_i</math> to <math>C</math>\n* Find the average euclidean distance from every document vector, <math>D_i</math> to <math>C</math> \'\'IGNORING\'\' <math>k</math>\n* The difference between the two values in the above step is the \'\'discrimination value\'\' for keyword <math>K</math>\n\nA higher value is better because including the keyword will result in better information retrieval.\n\n== Qualitative Observations ==\nKeywords that are \'\'[[Sparse matrix|sparse]]\'\' should be poor discriminators because they have poor \'\'[[Precision and recall|recall]],\'\'\nwhereas\nkeywords that are \'\'frequent\'\' should be poor discriminators because they have poor \'\'[[Precision and recall|precision]].\'\'\n\n== References ==\n* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," \'\'Communications of the ACM\'\', vol. 18, nr. 11, pages 613–620. \'\'(The article in which the vector space model was first presented)\'\'\n* Can, F., Ozkarahan, E. A (1987), "Computation of term/document discrimination values by use of the cover coefficient concept." \'\'Journal of the American Society for Information Science\'\', vol. 38, nr. 3, pages 171-183.\n\n[[Category:Information retrieval techniques]]']
['Category:Concordances (publishing)', '43967942', '{{Cat main|Concordance (publishing)}}\n[[Category:Biblical studies]]\n[[Category:Index (publishing)]]\n[[Category:Textual criticism]]\n[[Category:Reference works]]\n[[Category:Information retrieval techniques]]\n[[Category:Hypertext]]\n[[Category:Corpus linguistics]]']
['Learning to rank', '25050663', '{{machine learning bar}}\n\'\'\'Learning to rank\'\'\'<ref name="liu">{{citation\n|author=Tie-Yan Liu\n|title=Learning to Rank for Information Retrieval\n|series=Foundations and Trends in Information Retrieval\n|year=2009\n|isbn=978-1-60198-244-5\n|doi=10.1561/1500000016\n|pages=225–331\n|journal=Foundations and Trends in Information Retrieval\n|volume=3\n|issue=3\n}}. Slides from Tie-Yan Liu\'s talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://www2009.org/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]\n</ref> or \'\'\'machine-learned ranking\'\'\' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) \'\'Foundations of Machine Learning\'\', The\nMIT Press ISBN 9780262018258.</ref> [[Training data]] consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model\'s purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.\n\n== Applications ==\n\n=== In information retrieval ===\n[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]\nRanking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], and [[online advertising]].\n\nA possible architecture of a machine-learned search engine is shown in the figure to the right.\n\nTraining data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human \'\'assessors\'\' (or \'\'raters\'\', as [[Google]] calls them),\n<!-- "assessor" is the more standard term, used e.g. by TREC conference -->\nwho check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used — only the top few documents, retrieved by some existing ranking models are checked. <!--\n  TODO: write something about selection bias caused by pooling\n--> Alternatively, training data may be derived automatically by analyzing \'\'clickthrough logs\'\' (i.e. search results which got clicks from users),<ref name="Joachims2002">{{citation\n | author=Joachims, T.\n | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]\n | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf\n | title=Optimizing Search Engines using Clickthrough Data\n | year=2002\n}}</ref> \'\'query chains\'\',<ref>{{citation\n |author1=Joachims T. |author2=Radlinski F. | title=Query Chains: Learning to Rank from Implicit Feedback\n | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf\n | year=2005\n | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]\n}}</ref> or such search engines\' features as Google\'s [[Google SearchWiki|SearchWiki]].\n\nTraining data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.\n\nTypically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.<ref>{{citation\n |author1=B. Cambazoglu |author2=H. Zaragoza |author3=O. Chapelle |author4=J. Chen |author5=C. Liao |author6=Z. Zheng |author7=J. Degenhardt. | title=Early exit optimizations for additive machine learned ranking systems\n | journal=WSDM \'10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. \n | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf\n}}</ref> First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,<ref>{{citation\n |author1=Broder A. |author2=Carmel D. |author3=Herscovici M. |author4=Soffer A. |author5=Zien J. | title=Efficient query evaluation using a two-level retrieval process\n | journal=Proceedings of the twelfth international conference on Information and knowledge management\n | year=2003\n | pages=426–434\n | isbn=1-58113-723-0\n | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf\n }}</ref> and [[Okapi BM25|BM25]]. This phase is called \'\'top-<math>k</math> document retrieval\'\' and many heuristics were proposed in the literature to accelerate it, such as using a document\'s static quality score and tiered indexes.<ref name="manning-q-eval">{{citation\n |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. | title=Introduction to Information Retrieval\n | publisher=Cambridge University Press\n | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]</ref> In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.\n\n=== In other areas ===\nLearning to rank algorithms have been applied in areas other than information retrieval:\n* In [[machine translation]] for ranking a set of hypothesized translations;<ref name="Duh09">{{citation\n | author=Kevin K. Duh\n | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data\n | year=2009\n | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf\n}}</ref>\n* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.<ref name="Duh09" />\n* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.<ref>Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf \'\'Learning to Model Relatedness for News Recommendation\'\'], in International Conference on World Wide Web (WWW), 2011.</ref>\n\n== Feature vectors ==\nFor convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called \'\'[[feature vector]]s\'\'. Such an approach is sometimes called \'\'bag of features\'\' and is analogous to the [[bag of words]] model and [[vector space model]] used in information retrieval for representation of documents.\n\nComponents of such vectors are called \'\'[[feature (machine learning)|feature]]s\'\', \'\'factors\'\' or \'\'ranking signals\'\'. They may be divided into three groups (features from [[document retrieval]] are shown as examples):\n* \'\'Query-independent\'\' or \'\'static\'\' features — those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document\'s length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document\'s \'\'static quality score\'\' (or \'\'static rank\'\'), which is often used to speed up search query evaluation.<ref name="manning-q-eval" /><ref>\n{{cite conference\n | first=M. |last=Richardson |author2=Prakash, A. |author3=Brill, E.\n | title=Beyond PageRank: Machine Learning for Static Ranking\n | booktitle=Proceedings of the 15th International World Wide Web Conference\n | pages=707–715\n | publisher=\n | year=2006\n | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf\n | accessdate=\n }}</ref>\n* \'\'Query-dependent\'\' or \'\'dynamic\'\' features — those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.\n* \'\'Query level features\'\' or \'\'query features\'\', which depend only on the query. For example, the number of words in a query. \'\'Further information: [[query level feature]]\'\'\n\nSome examples of features, which were used in the well-known [[LETOR]] dataset:<ref name="letor3">[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]</ref>\n* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document\'s [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;\n* Lengths and [[Inverse document frequency|IDF]] sums of document\'s zones;\n* Document\'s [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.\n\nSelecting and designing good features is an important area in machine learning, which is called [[feature engineering]].\n\n== Evaluation measures ==\nThere are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.\n\nExamples of ranking quality measures:\n* [[Information retrieval#Mean average precision|Mean average precision]] (MAP);\n* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];\n* [[Precision (information retrieval)|Precision]]@\'\'n\'\', NDCG@\'\'n\'\', where "@\'\'n\'\'" denotes that the metrics are evaluated only on top \'\'n\'\' documents;\n* [[Mean reciprocal rank]];\n* [[Kendall\'s tau]]\n* [[Spearman\'s rank correlation coefficient|Spearman\'s Rho]]\n\nDCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.<ref>http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt</ref> Other metrics such as MAP, MRR and precision, are defined only for binary judgements.\n\nRecently, there have been proposed several new evaluation metrics which claim to model user\'s satisfaction with search results better than the DCG metric:\n* [[Expected reciprocal rank]] (ERR);<ref>{{citation\n|author1=Olivier Chapelle |author2=Donald Metzler |author3=Ya Zhang |author4=Pierre Grinspan |title=Expected Reciprocal Rank for Graded Relevance\n|url=https://web.archive.org/web/20120224053008/http://research.yahoo.com/files/err.pdf\n|journal=CIKM\n|year=2009\n|pages=\n}}</ref>\n* [[Yandex]]\'s pfound.<ref>{{citation\n|author1=Gulin A. |author2=Karpovich P. |author3=Raskovalov D. |author4=Segalovich I. |title=Yandex at ROMIP\'2009: optimization of ranking algorithms by machine learning methods\n|url=http://romip.ru/romip2009/15_yandex.pdf\n|journal=Proceedings of ROMIP\'2009\n|year=2009\n|pages=163–168\n}} (in Russian)</ref>\nBoth of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.\n\n== Approaches ==\n{{Expand section|date=December 2009}}\nTie-Yan Liu of [[Microsoft Research Asia]] has analyzed existing algorithms for learning to rank problems in his paper "Learning to Rank for Information Retrieval".<ref name="liu" /> He categorized them into three groups by their input representation and [[loss function]]:\n\n=== Pointwise approach ===\nIn this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.\n\nA number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.\n\n=== Pairwise approach ===\nIn this case learning-to-rank problem is approximated by a classification problem — learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize average number of [[Permutation#Inversions|inversions]] in ranking.\n\n=== Listwise approach ===\nThese algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model\'s parameters, and so continuous approximations or bounds on evaluation measures have to be used.\n\n=== List of methods ===\nA partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:\n:{|class="wikitable sortable"\n! Year || Name || Type || Notes\n|-\n| 1989 || OPRF <ref name="Fuhr1989">{{citation\n | last=Fuhr\n | first=Norbert\n | journal=ACM Transactions on Information Systems\n | title=Optimum polynomial retrieval functions based on the probability ranking principle\n | volume=7\n | number=3\n | pages=183–204 \n | year=1989\n | doi=10.1145/65943.65944\n}}</ref> || <span style="display:none">2</span> pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)\n|-\n| 1992 || SLR <ref name="Cooperetal1992">{{citation\n |author1=Cooper, William S. |author2=Gey, Frederic C. |author3=Dabney, Daniel P. | journal=SIGIR \'92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval \n | title=Probabilistic retrieval based on staged logistic regression\n | pages=198–210 \n | year=1992\n | doi=10.1145/133160.133199\n}}</ref>   || <span style="display:none">2</span> pointwise || Staged logistic regression\n|-\n| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || <span style="display:none">2</span> pairwise ||  A more recent exposition is in,<ref name="Joachims2002" /> which describes an application to ranking using clickthrough logs.\n|-\n| 2002 || Pranking<ref>{{cite journal | citeseerx = 10.1.1.20.378 | title = Pranking }}</ref> || <span style="display:none">1</span> pointwise || Ordinal regression.\n|-\n| 2003 <!-- or 1998? --> || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || <span style="display:none">2</span> pairwise ||\n|-\n| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || <span style="display:none">2</span> pairwise ||\n|-\n| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || <span style="display:none">2</span> pairwise || Ranking SVM with query-level normalization in the loss function.\n|-\n| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || pairwise/listwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.\n|-\n| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || <span style="display:none">3</span> listwise ||\n|-\n| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || <span style="display:none">2</span> pairwise || Based on RankNet, uses a different loss function - fidelity loss.\n|-\n| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || <span style="display:none">2</span> pairwise || \n|-\n| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || <span style="display:none">3</span> listwise ||\n|-\n| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || <span style="display:none">1</span> pointwise ||\n|-\n| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || <span style="display:none">2</span> pairwise ||\n|-\n| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || <span style="display:none">3</span> listwise ||\n|-\n| 2007 || RankGP<ref>{{cite journal | citeseerx = 10.1.1.90.220 | title = RankGP }}</ref> || <span style="display:none">3</span> listwise ||\n|-\n| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || <span style="display:none">2</span> pairwise ||\nRegularized least-squares based ranking. The work is extended in\n<ref name=pahikkala2009efficient>{{Citation|last=Pahikkala|first=Tapio |author2=Tsivtsivadze, Evgeni |author3=Airola, Antti |author4=Järvinen, Jouni |author5=Boberg, Jorma |title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129–165|doi=10.1007/s10994-008-5097-z|postscript=.}}</ref> to learning to rank from general preference graphs.\n|-\n| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM<sup>map</sup>] || <span style="display:none">3</span> listwise ||\n|-\n| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || pairwise/listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.<ref>C. Burges. (2010). [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].</ref>\n|-\n| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || <span style="display:none">3</span> listwise || Based on ListNet.\n|-\n| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || <span style="display:none">3</span> listwise ||\n|-\n| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || <span style="display:none">3</span> listwise ||\n|-\n| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]<ref>Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf \'\'Ranking Refinement and Its Application for Information Retrieval\'\'], in International Conference on World Wide Web (WWW), 2008.</ref> || <span style="display:none">2</span> pairwise || A semi-supervised approach to learning to rank that uses Boosting.\n|-\n| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]<ref>Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf \'\'A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data\'\'], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.</ref>  || <span style="display:none">2</span> pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)\n|-\n| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]<ref>Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF "SortNet: learning to rank by a neural-based sorting algorithm"], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008</ref> || <span style="display:none">2</span> pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. \n|-\n| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || <span style="display:none">2</span> pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.\n|-\n| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || <span style="display:none">3</span> listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.\n|-\n| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || <span style="display:none">3</span> listwise || A method combines Plackett-Luce Model and neural network to minimize the expected Bayes risk, related to NDCG, from the decision-making aspect.\n|-\n| 2010 || [https://people.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]<ref>Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf \'\'Learning to Rank by Optimizing NDCG Measure\'\'], in Proceeding of Neural Information Processing Systems (NIPS), 2010.</ref> || <span style="display:none">3</span> listwise || A boosting approach to optimize NDCG.\n|-\n| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || <span style="display:none">2</span> pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.\n|-\n| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || <span style="display:none">2</span> pairwise & listwise || \n|-\n| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || <span style="display:none">2</span> pointwise & pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.\n|}\n\nNote: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.\n\n== History ==\n[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;<ref name="Fuhr1992">{{citation\n | last=Fuhr\n | first=Norbert\n | journal=Computer Journal\n | title=Probabilistic Models in Information Retrieval\n | volume=35\n | number=3\n | pages=243–255\n | year=1992\n | doi=10.1093/comjnl/35.3.243\n}}</ref> a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.<ref name="Fuhr1989" /> Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 <ref name="Cooperetal1992" /> and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.<ref>{{citation |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]</ref>  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.\n\nSeveral conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).\n\n=== Practical usage by search engines ===\nCommercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.<ref>Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]</ref><ref>{{US Patent|7197497}}</ref>\n\n[[Bing (search engine)|Bing]]\'s search is said to be powered by [[RankNet]] algorithm,<ref>[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]</ref>{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.\n\nIn November 2009 a Russian search engine [[Yandex]] announced<ref name="snezhinsk">[http://webmaster.ya.ru/replies.xml?item_no=5707&ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)</ref> that it had significantly increased its [[search quality]] due to deployment of a new proprietary [[MatrixNet]] algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.<ref>The algorithm wasn\'t disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].</ref> Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"<ref>[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex\'s Internet Mathematics 2009 competition page]</ref> based on their own search engine\'s production data. Yahoo has announced a similar competition in 2010.<ref>[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]</ref>\n\nAs of 2008, [[Google]]\'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.<ref>{{cite web\n  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html\n  | archiveurl = http://www.webcitation.org/5sq8irWNM\n  | archivedate = 2010-09-18\n  | title = Are Machine-Learned Models Prone to Catastrophic Errors?\n  | date = 2008-05-24\n  | last = Rajaraman\n  | first = Anand\n  | authorlink = Anand Rajaraman}}</ref> [[Cuil]]\'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".<ref>{{cite web\n  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing\n  | archiveurl = http://www.webcitation.org/5sq7DX3Pj\n  | archivedate = 2010-09-15\n  | title = Cuil Blog: So how is Bing doing?\n  | date = 2009-06-26\n  | last = Costello\n  | first = Tom}}</ref>\n\n== References ==\n{{reflist|2}}\n\n== External links ==\n; Competitions and public datasets\n* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]\n* [http://imat2009.yandex.ru/en/ Yandex\'s Internet Mathematics 2009]\n* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]\n* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]\n\n; Open Source code\n* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]\n* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]\n* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]\n\n[[Category:Information retrieval techniques]]\n[[Category:Machine learning]]\n[[Category:Ranking functions]]']
['Category:Music information retrieval', '46973988', '{{Commons category|Music information retrieval}}\n[[Category:Information retrieval genres]]\n[[Category:Information retrieval techniques]]']
['Contextual searching', '44571310', '{{Orphan|date=May 2015}}\n\n\'\'\'Contextual search\'\'\' is a form of optimizing web-based search results based on context provided by the user and the computer being used to enter the query.<ref>{{cite journal | first = Susan E. | last = Feldman | title = The Answer Machine | journal = Synthesis Lectures on Information Concepts, Retrieval, and Services | doi = 10.2200/S00442ED1V01Y201208ICR023 }}</ref> Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their [[Relevance (information retrieval)|relevance]] to the query. Rather, contextual search attempts to increase the [[Precision and recall|precision]] of results based on how valuable they are to individual users.<ref>{{cite journal | last1 = Pitokow | first1 = James | first2 = Hinrich | last2 = Schütze | first3 = Todd | last3 = Cass | first4 = Rob | last4 = Cooley | first5 = Don | last5 = Turnbull | first6 = Andy | last6 = Edmonds | first7 = Eytan | last7 = Adar | first8 = Thomas | last8 = Breuel | date = 2002 | title = Personalized search | url = http://www.cond.org/p50-pitkow.pdf | journal = Communications of the ACM (CACM) | volume = 45 | issue = 9 | pages = 50–55 }}</ref>\n\n== Basic Contextual Search ==\nThe basic form of contextual search is the process of scanning the full-text of a query in order to understand what the user needs. Web search engines scan HTML pages for content and return an index rating based on how relevant the content is to the entered query. HTML pages that have a higher occurrence of query keywords within their content are not rated higher. Users have limited control over the context of their query based on the words they use to search with.<ref>Steve Lawrence. \'\'Context in Web Search\'\', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 25, 2000.</ref>  For example, users looking for the menu portion of a website can add “menu” to the end of their query to provide the search engine with context of what they need. The next step in contextualizing search is for the search service itself to request information that narrows down the results, such as Google asking for a time range to search within.\n\n== Explicitly Supplied Context ==\nCertain search services, including many Meta search engines, request individual contextual information from users to increase the precision of returned documents. Inquirus 2 is a Meta search engine that acts as a mediator between the user query and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats.<ref>Steve Lawrence. \'\'Context in Web Search\'\', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 27, 2000.</ref> For example, a user looking for research papers can specify documents with “references” or “abstracts” to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.<ref>Steve Lawrence, C. Lee Giles. \'\'Inquirus, the NECI meta search engine\'\'[http://www7.scu.edu.au/1906/com1906.htm]</ref>\n\nExplicitly supplied context effectively increases the precision of results, however, these search services tend to suffer from poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps.<ref>[https://support.google.com/websearch/answer/142143?hl=en https://support.google.com/websearch/answer/142143?hl=en], Filter your search results</ref> Google has an extensive [https://support.google.com/websearch/answer/2466433?rd=1 list of search operators] that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words.<ref>[https://support.google.com/websearch/answer/2466433?rd=1 https://support.google.com/websearch/answer/2466433?rd=1], Search Operators</ref> Bing also uses a similar set of search operators to assist users in explicitly narrowing down the context of their queries. Bing allows users to search within a time range, by file type, by location, language, and more.<ref>[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/], Bing Tricks</ref>\n\n== Automatically Inferred Context ==\nThere are other systems being developed that are working on automatically inferring the context of user queries based on the content of other documents they view or edit. [[Watson (computer)|IBM\'s Watson Project]] aims to create a cognitive technology that dynamically learns as it processes user queries. When presented with a query Watson creates a hypothesis that is evaluated against its present bank of knowledge based on previous questions. As related terms and relevant documents are matched against the query, Watson\'s hypothesis is modified to reflect the new information provided through unstructured data based on information it has obtained in previous situations.<ref>[http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html], How Watson Works - IBM</ref> Watson\'s ability to build off previous knowledge allows queries to be automatically filtered for similar contexts in order to supply precise results.\n\nMajor search services such as Google, Bing, and Yahoo also have a system of automatically inferring the context of particular user queries. Google tracks user\'s previous queries and selected results to further personalize results for those individuals. For example, if a user consistently searches for articles related to animals, wild animals, or animal care a search for "jaguar" would rank an article on jaguar cats higher than links to Jaguar Cars.<ref>{{cite journal | first1 = Eric J. | last1 = Glover | first2 = Steve | last2 = Lawrence | first3 = Michael D. | last3 = Gordon | first4 = William P. | last4 = Birmingham | first5 = C. Lee | last5 = Giles | title = Web Search - Your Way | publisher = NEC Research Institution | citeseerx = 10.1.1.41.7499 }}</ref> Similar to Watson, search services strive to learn from users based on previous experiences to automatically provide context on current queries. Bing also provides automatic context for particular queries based on content of the query itself. A [http://www.bing.com/search?q=pizza&go=Submit&qs=n&form=GEOMA1&pq=pizza&sc=8-1&sp=-1&sk=&cvid=883269b61529466e810bc096e371ec19 search of "pizza"] returns an interactive list of restaurants and their ratings based on the approximate location of the user\'s computer. The Bing server automatically infers that when a user searches for a food item they are interested in documents within the context of purchasing that food item or finding restaurants that sell that particular item.\n\n=== Contextual Mobile Search ===\nThe drive to develop better contextualized search coincides with the increasing popularity of using mobile phones to complete searches. BIA/Kelsey research marketing firm projects that by 2015 mobile local search will "exceed local search by more than 27 billion queries".<ref>[http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp], Mobile Search to Surpass Desktop</ref> Mobile phones provide the opportunity to provide search services with a broader supply of contextual information, particularly for location services but also personalized searches based on the wealth of information stored locally on the phone including contacts information, geometric analysis such as speed and elevation, and installed apps.<ref>[http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/ http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/], Contextually Aware Mobile Devices</ref>\n\n== References ==\n{{reflist}}\n\n{{Internet search}}\n\n{{DEFAULTSORT:Contextual Searching}}\n[[Category:Internet search engines]]\n[[Category:Semantic Web]]\n[[Category:Information retrieval techniques]]\n[[Category:Internet terminology]]']
['Negative search', '21692300', '{{Multiple issues|\n{{unreferenced|date=March 2009}}\n{{orphan|date=February 2009}}\n{{confusing|date=March 2009}}\n}}\n\n\'\'\'Negative Search\'\'\' is the elimination of information which is not relevant from a mass of content in order to present to a user a range of relevant content.\n\nNegative Search is different from both Positive Search and Discovery Search. Positive Search uses the selection of relevant content as its primary mechanism. Discovery calculates relatedness (between user intent and content) to present users with relevant alternatives of which they may not have been aware.\n\nNegative Search applies to those forms of searches where the user has the intention of finding a specific, actionable piece information but lacks the knowledge of what that specific information is or might be.\n\nNegative Search can also apply to searches where the user has a clear understanding of \'\'\'Negative Intent\'\'\' (what they don\'t want) rather than what they do.\n\nExamples of Negative Intent are:\n\n- Job searching: someone knows they want a new job but they have no idea what it might be. They just know what they don\'t want.\n\n- Online dating: someone is looking for a dating partner, but cannot identify what criteria they are looking for. They just know what they don\'t want.\n\n- An investigator is looking for a car but has no other information on that car on which to base a search.\n\n==Negative Search Classifiers==\n\nIf there are two forms of search (positive and negative) it follows that there are two forms of classifier models: \'\'\'Inclusive Classifiers\'\'\' and \'\'\'Exclusive Classifiers\'\'\'.\n\n[[List of countries|Countries of the World]] are a good example of a MECE list. A positive search for the country Kenya would identify content referencing Kenya and present it. A Negative Search for the country Kenya would exclude all content relating to other countries in the world leaving the user with content of some relevance to Kenya.\n\n==Irrelevancy as a Desirable Construct==\n\nPositive Search tends to view Irrelevancy as undesirable. Having a system actively identify and pursue irrelevant content for the purpose of elimination from a [[user experience]] may prove a highly powerful mechanism.\n\nIt follows that Positive and Negative Search are not mutually exclusive and that a more powerful search may result from the combination of selection and elimination as tools to empower user experience in Negative Searches.\n\n==Degrees of Passivity==\n\nPositive Search involves an active search by a user with no degree of passivity (or openness). For example: "I am only interested in the Hilton Hotel in Vientiane on [[New Year\\\'s Eve|New Years Eve]]."\n\nDiscovery involves a simultaneous secondary more Passive search by the user while they are involved in a Positive search. For example: "I am interested in the Hilton Hotel in Vientiane on New Years Eve but if there\'s a better hotel, let me know"\n\nNegative Search also involves an Active search but with a much higher degree of Passivity (or openness to discovery). For example: "I need a holiday and really don\'t care where as long as its good."\n\nSearchers can be active in one dimension (Positive Search) while simultaneously being passive to alternatives or what they don\'t know they\'re looking for in many dimensions. In Discovery they are Passive in a small number of dimensions but in Negative Search they are Passive in many or all dimensions.\n\n==References==\n{{Reflist}}\n\n[[Category:Information retrieval techniques]]']
['Anchor text', '1225632', '{{Use dmy dates|date=February 2013}}\nThe \'\'\'anchor text\'\'\', \'\'\'link label\'\'\', \'\'\'link text\'\'\', or \'\'\'link title\'\'\' is the visible, clickable text in a [[hyperlink]]. The words contained in the anchor text can determine the ranking that the page will receive by search engines. Since 1998, some [[web browser]]s have added the ability to show a [[tooltip]] for a hyperlink before it is selected. Not all links have anchor texts because it may be obvious where the link will lead due to the context in which it is used. Anchor texts normally remain below 60 [[Character (computing)|characters]]. Different browsers will display anchor texts differently. Usually, web search engines analyze anchor text from hyperlinks on web pages. Other services apply the basic principles of anchor text analysis as well. For instance, [[List of academic databases and search engines|academic search engines]] may use [[citation]] context to classify [[Academic publishing|academic articles]],<ref>{{cite web|author1=Bader Aljaber |author2=Nicola Stokes |author3=James Bailey |author4=Jian Pei |url=http://www.springerlink.com/content/p278617582u5x3x1/|title=Document clustering of scientific texts using citation contexts |date=1 April 2010|publisher=Springer}}</ref> and anchor text from documents linked in [[mind maps]] may be used too.<ref>Needs new reference link</ref> [[File:Anchor text.png|thumb|Visual implementation of anchor text]]\n\n==Overview==\nAnchor text usually gives the user relevant descriptive or contextual information about the content of the link\'s destination. The anchor text may or may not be related to the actual text of the [[Uniform Resource Locator|URL]] of the link. For example, a hyperlink to the [[English Wikipedia|English-language Wikipedia]]\'s [[homepage]] might take this form:\n\n:<code><nowiki><a href="http://en.wikipedia.org/wiki/Main_Page">Wikipedia</a></nowiki></code>\n\nThe anchor text in this example is "Wikipedia"; the longer, but vital, URL <code><nowiki>http://en.wikipedia.org/wiki/Main_Page</nowiki></code> needed to locate the target page, displays on the web page as {{srlink|Main Page|Wikipedia}}, contributing to clean, easy-to-read text.\n\n==Common misunderstanding of the concept==\n\nThis proper method of linking is beneficial to users and [[webmaster]]s as anchor text holds significant [[weight]] in [[search engine]] rankings. The limit of the [[concept]] is building [[Sentence (linguistics)|sentence]]s only composed with linked [[word]]s.{{citation needed|date=September 2011}}\n\n==Search engine algorithms==\nAnchor text is weighted (ranked) highly in [[search engine]] [[algorithm]]s, because the linked text is usually relevant to the [[landing page]]. The objective of search engines is to provide highly relevant search results; this is where anchor text helps, as the tendency was, more often than not, to hyperlink words relevant to the landing page. Anchor text can also serve the purpose of directing the user to internal pages on the site, which can also help to rank the website higher in the search rankings.<ref name="Search Engine Watch 1">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2169750/How-the-Web-Uses-Anchor-Text-in-Internal-Linking-Study|title=\nHow the Web Uses Anchor Text in Internal Linking [Study]|accessdate=6 July 2012}}</ref>\n\n[[Webmaster]]s may use anchor text to procure high results in [[search engine results page]]s. [[Google]]\'s [[Google Webmaster Tools|Webmaster Tools]] facilitate this optimization by letting [[website]] owners view the most common words in anchor text linking to their site.<ref>{{cite web\n|last=Fox\n|first=Vanessa\n|url=http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html\n|title=Get a more complete picture about how other sites link to you\n|date=15 March 2007\n|publisher=Official Google Webmaster Central Blog\n|accessdate=2007-03-27\n| archiveurl= https://web.archive.org/web/20070331195216/http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html| archivedate= 31 March 2007 <!--DASHBot-->| deadurl= no}}</ref>\nIn the past, [[Google bomb]]ing was possible through anchor text manipulation; however, in January 2007, Google announced it had updated its algorithm to minimize the impact of Google bombs, which refers to a prank where people attempt to cause someone else\'s site to rank for an obscure or meaningless query.<ref>{{cite web\n|last=Cutts\n|first=Matt\n|url=http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html\n|title=A quick word about Googlebombs\n|date=25 January 2007\n|publisher=Official Google Webmaster Central Blog\n|accessdate=2007-03-27\n| archiveurl= https://web.archive.org/web/20070324043013/http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html| archivedate= 24 March 2007 <!--DASHBot-->| deadurl= no}}</ref>\n\nIn April 2012, Google announced in its March "[[Google Penguin|Penguin]]" update that it would be changing the way it handled anchor text, implying that anchor text would no longer be as important an element for their ranking metrics.<ref>{{cite web|url=http://insidesearch.blogspot.co.uk/2012/04/search-quality-highlights-50-changes.html|title=Google\'s March Update|publisher=Google}}</ref> Moving forward, Google would be paying more attention to a diversified link profile which has a mix of anchor text and other types of links.\n.<ref name="Search Engine Watch 2">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2172839/Google-Penguin-Update-Impact-of-Anchor-Text-Diversity-Link-Relevancy|title=\nGoogle Penguin Update: Impact of Anchor Text Diversity & Link Relevancy|accessdate=6 July 2012}}</ref>\n\nHowever a 2016 study of anchor text influence across 16,000 keywords found that presence of exact and partial match anchor links continues to have a strong correlation with Google rankings.<ref>{{cite web|publisher=Ahrefs|url=https://ahrefs.com/blog/anchor-text|title=\nEverything You Ever Wanted To Know About Anchor Text|accessdate=27 July 2016}}</ref>\n\nAugust 2016 study conducted by Moz, found that Exact and partial match domains can be affected by over optimization penalty since Google considers domain Brand and naked URL links as Exact match.<ref>{{Cite news|url=https://moz.com/ugc/case-study-the-interconnectedness-of-local-seo-and-exact-match-domains|title=Case Study: The Interconnectedness of Local SEO and Exact Match Domains|newspaper=Moz|access-date=2016-12-12}}</ref>\n\n==Terminology==\nThere are different classifications of anchor text that are used within the search engine optimization community such as the following:\n\n;Exact Match: an anchor that is used with a keyword that mirrors the page that is being linked to. Example: "[[search engine optimization]]" is an exact match anchor because it\'s linking to a page about "search engine optimization.\n;Branded: a brand that is used as the anchor. "[[Wikipedia]]" is a branded anchor text.\n;Naked Link: a URL that is used as an anchor. "[[www.wikipedia.com]]" is a naked link anchor.\n;Generic: a generic word or phrase that is used as the anchor. "Click here" is a generic anchor. Other variations may include "go here", "visit this website", etc.\n;Images: whenever an image is linked, Google will use the "ALT" tag as the anchor text.\n\n==References==\n\n{{reflist|colwidth=30em}}\n\n[[Category:Information retrieval techniques]]\n[[Category:Internet search engines]]\n[[Category:Internet terminology]]\n[[Category:Search engine optimization]]\n[[Category:Hypertext]]']
['Webometrics', '703145', '{{For|Webometrics Ranking of World Universities|Webometrics Ranking of World Universities}}\n{{refimprove|date=May 2014}}\nThe science of \'\'\'webometrics\'\'\' (also \'\'\'cybermetrics\'\'\') tries to measure the [[World Wide Web]] to get knowledge about the number and types of [[hyperlink]]s, structure of the World Wide Web and usage patterns. According to Björneborn and Ingwersen (2004), the definition of \'\'\'webometrics\'\'\' is "the study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [[Bibliometrics|bibliometric]] and [[informetrics|informetric]] approaches." The term \'\'webometrics\'\' was first coined by Almind and Ingwersen (1997). A second definition of webometrics has also been introduced, "the study of web-based content with primarily quantitative methods for social science research goals using techniques that are not specific to one field of study" (Thelwall, 2009), which emphasizes the development of applied methods for use in the wider social sciences. The purpose of this alternative definition was to help publicize appropriate methods outside of the information science discipline rather than to replace the original definition within information science.\n\nSimilar scientific fields are [[Bibliometrics]], [[Informetrics]], [[Scientometrics]], [[Virtual ethnography]], and [[Web mining]].\n[[File:Site based graph relationship.jpg|thumb|Site based graph relationship. The idea was taken from paper "Web-communicator creation costs sharing problem as a cooperative game"{{sfn|Mazalov|Pechnikov|Chirkov|Chuyko|2010|p=189}}]]\n\nOne relatively straightforward measure is the "Web Impact Factor" (WIF) introduced by Ingwersen (1998). The WIF measure may be defined as the number of web pages in a web site receiving links from other web sites, divided by the number of web pages published in the site that are accessible to the crawler. However the use of WIF has been disregarded due to the mathematical artifacts derived from power law distributions of these variables. Other similar indicators using size of the institution instead of number of webpages have been proved more useful.\n\n== See also ==\n* [[Altmetrics]]\n* [[Impact factor]]\n* [[PageRank]]\n* [[Network mapping]]\n* [[Search engine]]\n* [[Webometrics Ranking of World Universities]]\n\n== References ==\n<references />\n\n== Bibliography ==\n\n* {{cite journal |author1=Tomas C. Almind  |author2=Peter Ingwersen  |lastauthoramp=yes | year = 1997 | title = Informetric analyses on the World Wide Web: Methodological approaches to \'webometrics\' | journal = Journal of Documentation | volume = 53 | issue = 4 | pages = 404–426 | doi = 10.1108/EUM0000000007205}}\n* {{cite journal |author1=Lennart Björneborn  |author2=Peter Ingwersen  |lastauthoramp=yes | year = 2004 | title = Toward a basic framework for webometrics | journal = Journal of the American Society for Information Science and Technology | volume = 55 | issue = 14 | pages = 1216–1227 | url = http://www3.interscience.wiley.com/cgi-bin/abstract/109594194/ABSTRACT | doi = 10.1002/asi.20077}}\n*{{cite journal |author = Peter Ingwersen | year = 1998 |title = The calculation of web impact factors | journal = Journal of Documentation |volume = 54 |issue = 2 | pages = 236–243 |doi = 10.1108/EUM0000000007167}}\n*{{cite journal |author1=Mike Thelwall |author2=Liwen Vaughan |author3=Lennart Björneborn | year = 2005 |title = Webometrics | journal = Annual Review of Information Science and Technology |volume = 39 | pages = 81–135 |doi = 10.1002/aris.1440390110}}\n* {{cite book |author=Mike Thelwall |title= Introduction to Webometrics: Quantitative Web Research for the Social Sciences |publisher= Morgan & Claypool |year= 2009 |isbn= 978-1-59829-993-9 |url=http://www.morganclaypool.com/doi/abs/10.2200/S00176ED1V01Y200903ICR004}}\n* {{cite conference \n|url            = http://www.mtas.ru/upload/library/UBS30112.pdf\n|title          = Web-communicator creation costs sharing problem as a cooperative game (in Russian)\n|last1=Mazalov |first1= Vladimir\n|last2=Pechnikov |first2=Andrey\n|last3=Chirkov |first3=Alexandr\n|last4=Chuyko |first4=Julia\n|year           = 2010\n|booktitle      = Управление большими системами: сборник трудов\n|pages          = \n|location       = \n|ref = harv\n}}\n\n* {{cite web\n |url        = http://eprints.rclis.org/7554/\n |title      = Webometrics: ten years of expansion\n |last       = Ingwersen\n |first      = Peter\n |year       = 2006\n |accessdate = 2013\n |ref = harv\n}}\n\n[[Category:World Wide Web]]\n[[Category:Information science]]\n[[Category:Information retrieval techniques]]\n\n\n{{web-stub}}\n\n[[pt:Webometria]]']
['Latent semantic analysis', '689427', '{{semantics}}\n\'\'\'Latent semantic analysis\'\'\' (\'\'\'LSA\'\'\') is a technique in [[natural language processing]], in particular [[distributional semantics]], of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text.  A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called [[singular value decomposition]] (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Words are then compared by taking the cosine of the angle between the two vectors (or the [[dot product]] between the [[Unit vector|normalizations]] of the two vectors) formed by any two rows.  Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.<ref>{{cite journal | title=Latent Semantic Analysis | author=Susan T. Dumais |year=2005 | doi=10.1002/aris.1440380105 | journal=Annual Review of Information Science and Technology | volume=38 | pages=188–230}}</ref>\n\nAn information retrieval technique using latent semantic structure was patented in 1988 ([http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853 US Patent 4,839,853], now expired) by [[Scott Deerwester]], [[Susan Dumais]], [[George Furnas]], [[Richard Harshman]], [[Thomas Landauer]], [[Karen Lochbaum]] and [[Lynn Streeter]]. In the context of its application to [[information retrieval]], it is sometimes called [[Latent semantic indexing|Latent Semantic Indexing \'\'\'(LSI)\'\'\']].<ref>{{cite web | url=http://lsa.colorado.edu/ | title=The Latent Semantic Indexing home page}}</ref>\n\n== Overview ==\n\n=== Occurrence matrix ===\nLSA can use a [[term-document matrix]] which describes the occurrences of terms in documents; it is a [[sparse matrix]] whose rows correspond to [[terminology|terms]] and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is [[tf-idf]] (term frequency–inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.\n\nThis matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.\n\n=== Rank lowering ===\nAfter the construction of the occurrence matrix, LSA finds a [[low-rank approximation]]<ref>Markovsky I. (2012) Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 {{page needed|date=January 2012}}</ref> to the [[term-document matrix]]. There could be various reasons for these approximations:\n\n* The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an \'\'approximation\'\' (a "least and necessary evil").\n* The original term-document matrix is presumed \'\'noisy\'\': for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a \'\'de-noisified matrix\'\' (a better matrix than the original).\n* The original term-document matrix is presumed overly [[Sparse matrix|sparse]] relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually \'\'in\'\' each document, whereas we might be interested in all words \'\'related to\'\' each document—generally a much larger set due to [[synonymy]].\n\nThe consequence of the rank lowering is that some dimensions are combined and depend on more than one term:\n\n:: {(car), (truck), (flower)} -->  {(1.3452 * car + 0.2828 * truck), (flower)}\n\nThis mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with [[polysemy]], since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.\n\n=== Derivation ===\nLet <math>X</math> be a matrix where element <math>(i,j)</math> describes the occurrence of term <math>i</math> in document <math>j</math> (this can be, for example, the frequency). <math>X</math> will look like this:\n\n:<math>\n\\begin{matrix} \n & \\textbf{d}_j \\\\\n & \\downarrow \\\\\n\\textbf{t}_i^T \\rightarrow &\n\\begin{bmatrix} \nx_{1,1} & \\dots & x_{1,n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_{m,1} & \\dots & x_{m,n} \\\\\n\\end{bmatrix}\n\\end{matrix}\n</math>\n\nNow a row in this matrix will be a vector corresponding to a term, giving its relation to each document:\n\n:<math>\\textbf{t}_i^T = \\begin{bmatrix} x_{i,1} & \\dots & x_{i,n} \\end{bmatrix}</math>\n\nLikewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:\n\n:<math>\\textbf{d}_j = \\begin{bmatrix} x_{1,j} \\\\ \\vdots \\\\ x_{m,j} \\end{bmatrix}</math>\n\nNow the [[dot product]] <math>\\textbf{t}_i^T \\textbf{t}_p</math> between two term vectors gives the [[correlation]] between the terms over the set of documents. The [[matrix product]] <math>X X^T</math> contains all these dot products. Element <math>(i,p)</math> (which is equal to element <math>(p,i)</math>) contains the dot product <math>\\textbf{t}_i^T \\textbf{t}_p</math> (<math> = \\textbf{t}_p^T \\textbf{t}_i</math>). Likewise, the matrix <math>X^T X</math> contains the dot products between all the document vectors, giving their correlation over the terms: <math>\\textbf{d}_j^T \\textbf{d}_q = \\textbf{d}_q^T \\textbf{d}_j</math>.\n\nNow, from the theory of linear algebra, there exists a decomposition of <math>X</math> such that <math>U</math> and <math>V</math> are [[orthogonal matrix|orthogonal matrices]] and <math>\\Sigma</math> is a [[diagonal matrix]]. This is called a [[singular value decomposition]] (SVD):\n\n:<math>\n\\begin{matrix}\nX = U \\Sigma V^T\n\\end{matrix}\n</math>\n\nThe matrix products giving us the term and document correlations then become\n\n:<math>\n\\begin{matrix}\nX X^T &=& (U \\Sigma V^T) (U \\Sigma V^T)^T = (U \\Sigma V^T) (V^{T^T} \\Sigma^T U^T) = U \\Sigma V^T V \\Sigma^T U^T = U \\Sigma \\Sigma^T U^T = U \\Sigma^2 U^T \\\\\nX^T X &=& (U \\Sigma V^T)^T (U \\Sigma V^T) = (V^{T^T} \\Sigma^T U^T) (U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^T \\Sigma V^T = V \\Sigma^2 V^T\n\\end{matrix}\n</math>\n\nSince <math>\\Sigma \\Sigma^T</math> and <math>\\Sigma^T \\Sigma</math> are diagonal we see that <math>U</math> must contain the [[eigenvector]]s of <math>X X^T</math>, while <math>V</math> must be the eigenvectors of <math>X^T X</math>. Both products have the same non-zero eigenvalues, given by the non-zero entries of <math>\\Sigma \\Sigma^T</math>, or equally, by the non-zero entries of <math>\\Sigma^T\\Sigma</math>. Now the decomposition looks like this:\n\n:<math>\n\\begin{matrix} \n & X & & & U & & \\Sigma & & V^T \\\\\n & (\\textbf{d}_j) & & & & & & & (\\hat{\\textbf{d}}_j) \\\\\n & \\downarrow & & & & & & & \\downarrow \\\\\n(\\textbf{t}_i^T) \\rightarrow \n&\n\\begin{bmatrix} \nx_{1,1} & \\dots & x_{1,n} \\\\\n\\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\\\\nx_{m,1} & \\dots & x_{m,n} \\\\\n\\end{bmatrix}\n&\n=\n&\n(\\hat{\\textbf{t}}_i^T) \\rightarrow\n&\n\\begin{bmatrix} \n\\begin{bmatrix} \\, \\\\ \\, \\\\ \\textbf{u}_1 \\\\ \\, \\\\ \\,\\end{bmatrix} \n\\dots\n\\begin{bmatrix} \\, \\\\ \\, \\\\ \\textbf{u}_l \\\\ \\, \\\\ \\, \\end{bmatrix}\n\\end{bmatrix}\n&\n\\cdot\n&\n\\begin{bmatrix} \n\\sigma_1 & \\dots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\dots & \\sigma_l \\\\\n\\end{bmatrix}\n&\n\\cdot\n&\n\\begin{bmatrix} \n\\begin{bmatrix} & & \\textbf{v}_1 & & \\end{bmatrix} \\\\\n\\vdots \\\\\n\\begin{bmatrix} & & \\textbf{v}_l & & \\end{bmatrix}\n\\end{bmatrix}\n\\end{matrix}\n</math>\n\nThe values <math>\\sigma_1, \\dots, \\sigma_l</math> are called the singular values, and <math>u_1, \\dots, u_l</math> and <math>v_1, \\dots, v_l</math> the left and right singular vectors.\nNotice the only part of <math>U</math> that contributes to <math>\\textbf{t}_i</math> is the <math>i\\textrm{\'th}</math> row.\nLet this row vector be called <math>\\hat{\\textrm{t}}^T_i</math>.\nLikewise, the only part of <math>V^T</math> that contributes to <math>\\textbf{d}_j</math> is the <math>j\\textrm{\'th}</math> column, <math>\\hat{ \\textrm{d}}_j</math>.\nThese are \'\'not\'\' the eigenvectors, but \'\'depend\'\' on \'\'all\'\' the eigenvectors.\n\nIt turns out that when you select the <math>k</math> largest singular values, and their corresponding singular vectors from <math>U</math> and <math>V</math>, you get the rank <math>k</math> approximation to <math>X</math> with the smallest error ([[Frobenius norm]]). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a "semantic space". The row "term" vector <math>\\hat{\\textbf{t}}^T_i</math> then has <math>k</math> entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the "document" vector <math>\\hat{\\textbf{d}}_j</math> is an approximation in this lower-dimensional space. We write this approximation as\n\n:<math>X_k = U_k \\Sigma_k V_k^T</math>\n\nYou can now do the following:\n* See how related documents <math>j</math> and <math>q</math> are in the low-dimensional space by comparing the vectors <math>\\Sigma_k \\hat{\\textbf{d}}_j </math> and <math>\\Sigma_k \\hat{\\textbf{d}}_q </math> (typically by [[vector space model|cosine similarity]]).\n* Comparing terms <math>i</math> and <math>p</math> by comparing the vectors <math>\\Sigma_k \\hat{\\textbf{t}}_i</math> and <math>\\Sigma_k \\hat{\\textbf{t}}_p</math>. Note that <math>\\hat{\\textbf{t}}</math> is now a column vector.\n* Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.\n* Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.\n\nTo do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:\n\n:<math>\\hat{\\textbf{d}}_j = \\Sigma_k^{-1} U_k^T \\textbf{d}_j</math>\n\nNote here that the inverse of the diagonal matrix <math>\\Sigma_k</math> may be found by inverting each nonzero value within the matrix.\n\nThis means that if you have a query vector <math>q</math>, you must do the translation <math>\\hat{\\textbf{q}} = \\Sigma_k^{-1} U_k^T \\textbf{q}</math> before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:\n\n:<math>\\textbf{t}_i^T = \\hat{\\textbf{t}}_i^T \\Sigma_k V_k^T</math>\n\n:<math>\\hat{\\textbf{t}}_i^T = \\textbf{t}_i^T V_k^{-T} \\Sigma_k^{-1} = \\textbf{t}_i^T V_k \\Sigma_k^{-1}</math>\n\n:<math>\\hat{\\textbf{t}}_i = \\Sigma_k^{-1}  V_k^T \\textbf{t}_i</math>\n\n== Applications ==\n\nThe new low-dimensional space typically can be used to:\n* Compare the documents in the low-dimensional space ([[data clustering]], [[document classification]]).\n* Find similar documents across languages, after analyzing a base set of translated documents ([[cross language retrieval]]).\n* Find relations between terms ([[synonymy]] and [[polysemy]]).\n* Given a query of terms, translate it into the low-dimensional space, and find matching documents ([[information retrieval]]).\n* Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions [[Multiple choice question|MCQ]] answering model.<ref name="Alain2009">{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/38/41/43/PDF/eLSA1-brm20.pdf |format=PDF| title=Effect of tuned parameters on an LSA multiple choice questions answering model |author1=Alain Lifchitz |author2=Sandra Jhean-Larose |author3=Guy Denhière | journal=Behavior Research Methods | volume=41 | issue=4 | pages=1201–1209 | year=2009  | doi=10.3758/BRM.41.4.1201 | pmid=19897829 }}</ref>\n* Expand the feature space of machine learning / text mining systems <ref name="Galvez2017">{{cite journal | url=http://www.sciencedirect.com/science/article/pii/S1877750317300091 | title=Assessing the usefulness of online message board mining in automatic stock prediction systems |author1=Ramiro H. Gálvez |author2=Agustín Gravano | journal=Journal of Computational Science | volume=19 | pages=1877-7503 | year=2017  | doi=10.1016/j.jocs.2017.01.001}}</ref>\n\nSynonymy and polysemy are fundamental problems in [[natural language processing]]: \n* Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "[[physicians]]", even though the words have the same meaning.\n* Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.\n\n=== Commercial applications ===\n\nLSA has been used to assist in performing [[prior art]] searches for [[patents]].<ref name="Gerry2007">{{Cite journal | author=Gerry J. Elman | title=Automated Patent Examination Support - A proposal | journal=Biotechnology Law Report | date=October 2007 | doi=10.1089/blr.2007.9896 | volume=26 | issue=5 | pages=435–436 | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref>\n\n=== Applications in human memory ===\n\nThe use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of [[free recall]] and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the [[Semantic Proximity Effect]].<ref>{{cite journal | url=http://psycnet.apa.org/journals/xlm/25/4/923.pdf |format=PDF| title=Contextual Variability and Serial Position Effects in Free Recall |author1=Marc W. Howard |author2=Michael J. Kahana |year=1999}}</ref>\n\nWhen participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.<ref>{{cite journal | url=https://memory.psych.upenn.edu/files/pubs/ZaroEtal06.pdf |format=PDF| title=Temporal Associations and Prior-List Intrusions in Free Recall | author=Franklin M. Zaromb| booktitle=Interspeech\'2005|year=2006|display-authors=etal}}</ref>\n\nAnother model, termed [[Word Association Spaces]] (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.<ref>{{cite web|last=Nelson|first=Douglas|title=The University of South Florida Word Association, Rhyme and Word Fragment Norms|url=http://w3.usf.edu/FreeAssociation/Intro.html|accessdate=May 8, 2011}}</ref>\n\n== Implementation ==\n\nThe [[Singular Value Decomposition|SVD]] is typically computed using large matrix methods (for example, [[Lanczos method]]s) but may also be computed incrementally and with greatly reduced resources via a [[neural network]]-like approach, which does not require the large, full-rank matrix to be held in memory.<ref name="Genevi2005">{{cite conference | url=http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf |format=PDF| title=Generalized Hebbian Algorithm for Latent Semantic Analysis |author1=Geneviève Gorrell |author2=Brandyn Webb | booktitle=Interspeech\'2005 |year=2005}}</ref>\nA fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.<ref name="brand2006">{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20–30 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}</ref> [http://web.mit.edu/~wingated/www/resources.html MATLAB] and [http://radimrehurek.com/gensim Python] implementations of these fast algorithms are available. Unlike Gorrell and Webb\'s (2005) stochastic approximation, Brand\'s algorithm (2003) provides an exact solution.\nIn recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.<ref>{{cite journal | doi = 10.1109/ICCSNT.2011.6182070 | title=A parallel implementation of Singular Value Decomposition based on Map-Reduce and PARPACK | journal=Proceedings of 2011 International Conference on Computer Science and Network Technology}}</ref>\n\n== Limitations ==\nSome of LSA\'s drawbacks include:\n\n* The resulting dimensions might be difficult to interpret. For instance, in\n:: {(car), (truck), (flower)} ↦  {(1.3452 * car + 0.2828 * truck), (flower)}\n:the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to\n:: {(car), (bottle), (flower)} ↦  {(1.3452 * car + 0.2828 * \'\'\'bottle\'\'\'), (flower)}\n:will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.\n\n* LSA cannot capture [[polysemy]] (i.e., multiple meanings of a word) because each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space.  For example, the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same.  The behavior results in the vector representation being an \'\'average\'\' of all the word\'s different meanings in the corpus, which can make it difficult for comparison.  However, the effect is often lessened due to words having a [[word sense disambiguation|predominant sense]] throughout a corpus (i.e. not all meanings are equally likely).\n* Limitations of [[bag of words model]] (BOW), where a text is represented as an unordered collection of words. To address some of the limitation of [[bag of words model]] (BOW), [[N-gram|multi-gram]] dictionary can be used to find direct and indirect association as well as [[Higher-order statistics|higher-order]] [[co-occurrence]]s among terms.<ref>{{cite journal|url=http://www.translational-medicine.com/content/12/1/324|title=Empirical study using network of semantically related associations in bridging the knowledge gap|first1=Vida|last1=Abedi|first2=Mohammed|last2=Yeasin|first3=Ramin|last3=Zand|date=27 November 2014|publisher=|volume=12|issue=1|doi=10.1186/s12967-014-0324-9|pmid=25428570|pmc=4252998|journal=Journal of Translational Medicine}}</ref>\n* The [[probabilistic model]] of LSA does not match observed data: LSA assumes that words and documents form a joint [[normal distribution|Gaussian]] model ([[ergodic hypothesis]]), while a [[Poisson distribution]] has been observed.  Thus, a newer alternative is [[probabilistic latent semantic analysis]], based on a [[multinomial distribution|multinomial]] model, which is reported to give better results than standard LSA.<ref name="Thomas1999">{{cite conference | url=http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf |format=PDF| title=Probabilistic Latent Semantic Analysis | author=Thomas Hofmann | booktitle=Uncertainty in Artificial Intelligence |year=1999}}</ref>\n\n==Alternative methods==\n\n===Semantic hashing===\nIn semantic hashing <ref>Salakhutdinov, Ruslan, and Geoffrey Hinton. "Semantic hashing." RBM 500.3 (2007): 500.</ref> documents are mapped to memory addresses by means of a [[neural network]] in such a way that semantically similar documents are located at nearby addresses. [[Deep learning|Deep neural network]] essentially builds a [[graphical model]] of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than [[locality sensitive hashing]], which is the fastest current method.\n\n== Latent semantic indexing ==\n\'\'\'Latent semantic indexing\'\'\' (\'\'\'LSI\'\'\') is an indexing and retrieval method that uses a mathematical technique called [[singular value decomposition]] (SVD) to identify patterns in the relationships between the [[terminology|term]]s and [[concept]]s contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a [[Text corpus|body of text]] by establishing associations between those terms that occur in similar [[context (language use)|context]]s.<ref name=deerwester1988>Deerwester, S., et al, Improving Information Retrieval with Latent Semantic Indexing, Proceedings of the 51st Annual Meeting of the American Society for Information Science 25, 1988, pp. 36–40.</ref>\n\nLSI is also an application of [[correspondence analysis]], a multivariate statistical technique developed by [[Jean-Paul Benzécri]]<ref>{{ cite book\n | author = Benzécri, J.-P.\n | publisher=Dunod |location= Paris, France\n | year = 1973\n | title = L\'Analyse des Données. Volume II. L\'Analyse des Correspondences\n }}</ref> in the early 1970s, to a [[contingency table]] built from word counts in documents.\n\nCalled Latent Semantic Indexing because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at [[Bellcore]] in the late 1980s.   The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don’t share a specific word or words with the search criteria.\n\n== Benefits of LSI ==\n\nLSI overcomes two of the most problematic constraints of Boolean [[keyword search|keyword queries]]:  multiple words that have similar meanings ([[synonymy]]) and words that have more than one meaning ([[polysemy]]).  Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of [[information retrieval]] systems.<ref>{{Cite journal | last1 = Furnas | first1 = G. W. | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | title = The vocabulary problem in human-system communication | doi = 10.1145/32206.32212 | journal = Communications of the ACM | volume = 30 | issue = 11 | pages = 964–971 | year = 1987 | pmid =  | pmc = }}</ref>  As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.\n\nLSI is also used to perform automated [[document categorization]].  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.<ref name=landauer2008>Landauer, T., et al., Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report, M. I. Jordan, M. J. Kearns & S. A. Solla (Eds.), Advances in Neural Information Processing Systems 10, Cambridge: MIT Press, 1998, pp. 45–51.</ref>    Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.<ref>{{Cite book | last1 = Dumais | first1 = S. | last2 = Platt | first2 = J. | last3 = Heckerman | first3 = D. | last4 = Sahami | first4 = M. | chapter = Inductive learning algorithms and representations for text categorization | doi = 10.1145/288627.288651 | title = Proceedings of the seventh international conference on Information and knowledge management  - CIKM \'98 | pages = 148 | year = 1998 | isbn = 1581130619 | url = http://research.microsoft.com/en-us/um/people/jplatt/cikm98.pdf| pmid =  | pmc = }}</ref>   LSI uses \'\'example\'\' documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.\n\nDynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text.\n\nBecause it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic concept searching and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.{{Citation needed|date=July 2015}}\n\nLSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space.<ref>Zukas, Anthony, Price, Robert J., Document Categorization Using Latent Semantic Indexing, White Paper, [[Content Analyst Company]], LLC</ref>   For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.<ref>{{Cite journal | last1 = Homayouni | first1 = R. | last2 = Heinrich | first2 = K. | last3 = Wei | first3 = L. | last4 = Berry | first4 = M. W. | title = Gene clustering by Latent Semantic Indexing of MEDLINE abstracts | doi = 10.1093/bioinformatics/bth464 | journal = Bioinformatics | volume = 21 | issue = 1 | pages = 104–115 | year = 2004 | pmid =  15308538| pmc = }}</ref>\n\nLSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).<ref>{{Cite book | last1 = Price | first1 = R. J. | last2 = Zukas | first2 = A. E. | chapter = Application of Latent Semantic Indexing to Processing of Noisy Text | doi = 10.1007/11427995_68 | title = Intelligence and Security Informatics | series = Lecture Notes in Computer Science | volume = 3495 | pages = 602 | year = 2005 | isbn = 978-3-540-25999-2 | pmid =  | pmc = }}</ref>   This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data.\n\nText does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.\n\nLSI has proven to be a useful solution to a number of conceptual matching problems.<ref>Ding, C., A Similarity-based Probability Model for Latent Semantic Indexing, Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999, pp. 59–65.</ref><ref>Bartell, B., Cottrell, G., and Belew, R., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, Proceedings, ACM SIGIR Conference on Research and Development in Information Retrieval, 1992, pp. 161–167.</ref>  The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.<ref>{{cite journal|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.5444&rep=rep1&type=pdf|author1=Graesser, A. |author2=Karnavat, A.|title=Latent Semantic Analysis Captures Causal, Goal-oriented, and Taxonomic Structures|journal=Proceedings of CogSci 2000|pages=184–189}}</ref>\n\n== LSI timeline ==\n\n*\'\'\'Mid-1960s\'\'\' – Factor analysis technique first described and tested (H. Borko and M. Bernick)\n*\'\'\'1988\'\'\' – Seminal paper on LSI technique published <ref name=deerwester1988/>\n*\'\'\'1989\'\'\' – Original patent granted <ref name=deerwester1988/>\n*\'\'\'1992\'\'\' – First use of LSI to assign articles to reviewers<ref>{{cite journal|last1=Dumais |first1=S. |last2=Nielsen |first2=J. |title=Automating the Assignment of Submitted Manuscripts to Reviewers|journal=Proceedings of the Fifteenth Annual International Conference on Research and Development in Information Retrieval|year=1992|pages=233–244|doi=10.1145/133160.133205|isbn=0897915232 }}</ref>\n*\'\'\'1994\'\'\' – Patent granted for the cross-lingual application of LSI (Landauer et al.)\n*\'\'\'1995\'\'\' – First use of LSI for grading essays (Foltz, et al., Landauer et al.)\n*\'\'\'1999\'\'\' – First implementation of LSI technology for intelligence community for analyzing unstructured text ([[Science Applications International Corporation|SAIC]]).\n*\'\'\'2002\'\'\' – LSI-based product offering to intelligence-based government agencies (SAIC)\n*\'\'\'2005\'\'\' – First vertical-specific application – publishing – EDB (EBSCO, [[Content Analyst Company]])\n\n== Mathematics of LSI ==\n\nLSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a [[Singular value decomposition|\'\'\'Singular Value Decomposition\'\'\']] on the matrix, and using the matrix to identify the concepts contained in the text.\n\n=== Term-document matrix ===\n\nLSI begins by constructing a term-document matrix, <math>A</math>, to identify the occurrences of the <math>m</math> unique terms within a collection of <math>n</math> documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, <math>a_{ij}</math>, initially representing the number of times the associated term appears in the indicated document, <math>\\mathrm{tf_{ij}}</math>.  This matrix is usually very large and very sparse.\n\nOnce a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell, <math>a_{ij}</math> of <math>A</math>, to be the product of a local term weight, <math>l_{ij}</math>, which describes the relative frequency of a term in a document, and a global weight, <math>g_i</math>, which describes the relative frequency of the term within the entire collection of documents.\n\nSome common local weighting functions <ref>\nBerry, M. W., and Browne, M., Understanding Search Engines: Mathematical Modeling and Text Retrieval, Society for Industrial and Applied Mathematics, Philadelphia, (2005).</ref> are defined in the following table.\n\n{| style="width:60%" cellpadding="25" cellspacing="5" align="center"\n|-\n|  style="width:22%" | \'\'\'Binary\'\'\' ||\n| <math>l_{ij} = 1</math> if the term exists in the document, or else <math>0</math>\n|-\n|  style="width:22%" | \'\'\'TermFrequency\'\'\' ||\n| <math>l_{ij} = \\mathrm{tf}_{ij}</math>, the number of occurrences of term <math>i</math> in document <math>j</math>\n|-\n|  style="width:22%" | \'\'\'Log\'\'\' ||\n| <math>l_{ij} = \\log(\\mathrm{tf}_{ij} + 1)</math>\n|-\n|  style="width:22%" | \'\'\'Augnorm\'\'\' ||\n| <math>l_{ij} = \\frac{\\Big(\\frac{\\mathrm{tf}_{ij}}{\\max_i(\\mathrm{tf}_{ij})}\\Big) + 1}{2}</math>\n|}\n\nSome common global weighting functions are defined in the following table.\n\n{| style="width:60%" cellpadding="25" cellspacing="5" align="center"\n|-\n| style="width:22%" | \'\'\'Binary\'\'\' ||\n| <math>g_i = 1</math>\n|-\n| style="width:22%" | \'\'\'Normal\'\'\' ||\n| <math>g_i = \\frac{1}{\\sqrt{\\sum_j \\mathrm{tf}_{ij}^2}}</math>\n|-\n| style="width:22%" | \'\'\'GfIdf\'\'\' ||\n| <math>g_i = \\mathrm{gf}_i / \\mathrm{df}_i</math>, where <math>\\mathrm{gf}_i</math> is the total number of times term <math>i</math> occurs in the whole collection, and <math>\\mathrm{df}_i</math> is the number of documents in which term <math>i</math> occurs.\n|-\n| style="width:22%" | \'\'\'[[Tf–idf#Inverse document frequency 2|Idf (Inverse Document Frequency)]]\'\'\' ||\n| <math>g_i = \\log_2 \\frac{n}{1+ \\mathrm{df}_i}</math>\n|-\n| style="width:22%" | \'\'\'Entropy\'\'\' ||\n| <math>g_i = 1 + \\sum_j \\frac{p_{ij} \\log p_{ij}}{\\log n}</math>, where <math>p_{ij} = \\frac{\\mathrm{tf}_{ij}}{\\mathrm{gf}_i}</math>\n|}\n\nEmpirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets.<ref>Landauer, T., et al., Handbook of Latent Semantic Analysis, Lawrence Erlbaum Associates, 2007.</ref>  In other words, each entry <math>a_{ij}</math> of <math>A</math> is computed as:\n\n:<math>g_i = 1 + \\sum_j \\frac{p_{ij} \\log p_{ij}}{\\log n}</math>\n\n:<math>a_{ij} = g_i \\ \\log (\\mathrm{tf}_{ij} + 1)</math>\n\n=== Rank-reduced singular value decomposition ===\n\nA rank-reduced, [[singular value decomposition]] is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI.<ref>Berry, Michael W., Dumais, Susan T., O\'Brien, Gavin W., Using Linear Algebra for Intelligent Information Retrieval, December 1994, SIAM Review 37:4 (1995), pp. 573–595.</ref>   It computes the term and document vector spaces by approximating the single term-frequency matrix, <math>A</math>, into three other matrices— an \'\'\'\'\'m\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\'  term-concept vector matrix <math>T</math>, an \'\'\'\'\'r\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' singular values matrix <math>S</math>, and a \'\'\'\'\'n\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' concept-document vector matrix, <math>D</math>, which satisfy the following relations:\n\n<math>A \\approx TSD^T</math>\n\n<math>T^T T = I_r \\quad D^T D = I_r </math>\n\n<math>S_{1,1} \\geq S_{2,2} \\geq \\ldots \\geq  S_{r,r} > 0 \\quad S_{i,j} = 0 \\; \\text{where} \\; i \\neq j</math>\n\nIn the formula, \'\'\'A\'\'\' is the supplied \'\'\'\'\'m\'\'\'\'\' by \'\'\'\'\'n\'\'\'\'\' weighted matrix of term frequencies in a collection of text where \'\'\'\'\'m\'\'\'\'\' is the number of unique terms, and \'\'\'\'\'n\'\'\'\'\' is the number of documents.  \'\'\'T\'\'\' is a computed \'\'\'\'\'m\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' matrix of term vectors where \'\'\'\'\'r\'\'\'\'\' is the rank of \'\'\'A\'\'\'—a measure of its unique dimensions \'\'\'≤ min(\'\'m,n\'\')\'\'\'.  \'\'\'S\'\'\' is a computed \'\'\'\'\'r\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' diagonal matrix of decreasing singular values, and \'\'\'D\'\'\' is a computed \'\'\'\'\'n\'\'\'\'\' by \'\'\'\'\'r\'\'\'\'\' matrix of document vectors.\n\nThe SVD is then [[Singular value decomposition#Truncated SVD|truncated]] to reduce the rank by keeping only the largest \'\'\'\'\'k\'\'\'\'\' « \'\'\'\'\'r\'\'\'\'\' diagonal entries in the singular value matrix \'\'\'S\'\'\',\nwhere \'\'\'\'\'k\'\'\'\'\' is typically on the order 100 to 300 dimensions.\nThis effectively reduces the term and document vector matrix sizes to \'\'\'\'\'m\'\'\'\'\' by \'\'\'\'\'k\'\'\'\'\' and \'\'\'\'\'n\'\'\'\'\' by \'\'\'\'\'k\'\'\'\'\' respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of \'\'\'A\'\'\'.  This reduced set of matrices is often denoted with a modified formula such as:\n\n:::::::\'\'\'A ≈ A\'\'<sub>k\'\'</sub> = T\'\'<sub>k\'\'</sub> S\'\'<sub>k\'\'</sub> D\'\'<sub>k\'\'</sub><sup>T</sup>\'\'\'\n\nEfficient LSI algorithms only compute the first \'\'\'\'\'k\'\'\'\'\' singular values and term and document vectors as opposed to computing a full SVD and then truncating it.\n\nNote that this rank reduction is essentially the same as doing [[Principal Component Analysis]] (PCA) on the matrix \'\'\'A\'\'\', except that PCA subtracts off the means.  PCA loses the sparseness of the \'\'\'A\'\'\' matrix, which can make it infeasible for large lexicons.\n\n== Querying and augmenting LSI vector spaces ==\n\nThe computed \'\'\'T\'\'<sub>k\'\'</sub>\'\'\' and \'\'\'D\'\'<sub>k\'\'</sub>\'\'\' matrices define the term and document vector spaces, which with the computed singular values, \'\'\'S\'\'<sub>k\'\'</sub>\'\'\', embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.\n\nThe same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the \'\'\'A = T S D<sup>T</sup>\'\'\' equation into the equivalent \'\'\'D = A<sup>T</sup> T S<sup>−1</sup>\'\'\' equation, a new vector, \'\'\'\'\'d\'\'\'\'\', for a query or for a new document can be created by computing a new column in \'\'\'A\'\'\' and then multiplying the new column by \'\'\'T S<sup>−1</sup>\'\'\'.  The new column in \'\'\'A\'\'\' is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.\n\nA drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.\n\nThe process of augmenting the document vector spaces for an LSI index with new documents in this manner is called \'\'folding in\'\'.  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in <ref name="brand2006"/>) be used.\n\n== Additional uses of LSI ==\n\nIt is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.\n\nLSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, 2004, Chapter 4.</ref>   Below are some other ways in which LSI is being used:\n\n* Information discovery<ref>Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery, the Sedona Conference, 2007, pp. 189–223.</ref>  ([[Electronic Discovery|eDiscovery]], Government/Intelligence community, Publishing)\n* Automated document classification (eDiscovery, Government/Intelligence community, Publishing)<ref>Foltz, P. W. and Dumais, S. T. Personalized Information Delivery:  An analysis of information filtering methods, Communications of the ACM, 1992, 34(12), 51-60.</ref>\n* Text summarization<ref>Gong, Y., and Liu, X., Creating Generic Text Summaries, Proceedings, Sixth International Conference on Document Analysis and Recognition, 2001, pp. 903–907.</ref>  (eDiscovery, Publishing)\n* Relationship discovery<ref>Bradford, R., Efficient Discovery of New Information in Large Text Databases, Proceedings, IEEE International Conference on Intelligence and Security Informatics, Atlanta, Georgia, LNCS Vol. 3495, Springer, 2005, pp. 374–380.</ref>  (Government, Intelligence community, Social Networking)\n* Automatic generation of link charts of individuals and organizations<ref>Bradford, R., Application of Latent Semantic Indexing in Generating Graphs of Terrorist Networks, in: Proceedings, IEEE International Conference on Intelligence and Security Informatics, ISI 2006, San Diego, CA, USA, May 23–24, 2006, Springer, LNCS vol. 3975, pp. 674–675.</ref>  (Government, Intelligence community)\n* Matching technical papers and grants with reviewers<ref>Yarowsky, D., and Florian, R., Taking the Load off the Conference Chairs: Towards a Digital Paper-routing Assistant, Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very-Large Corpora, 1999, pp. 220–230.</ref>  (Government)\n* Online customer support<ref>Caron, J., Applying LSA to Online Customer Support: A Trial Study, Unpublished Master\'s Thesis, May 2000.</ref>  (Customer Management)\n* Determining document authorship<ref>Soboroff, I., et al, Visualizing Document Authorship Using N-grams and Latent Semantic Indexing,   Workshop on New Paradigms in Information Visualization and Manipulation, 1997, pp. 43–48.</ref>  (Education)\n* Automatic keyword annotation of images<ref>Monay, F., and Gatica-Perez, D., On Image Auto-annotation with Latent Space Models, Proceedings of the 11th ACM international conference on Multimedia, Berkeley, CA, 2003, pp. 275–278.</ref>\n* Understanding software source code<ref>{{cite journal|author1=Maletic, J. |author2=Marcus, A.|title=Using Latent Semantic Analysis to Identify Similarities in Source Code to Support Program Understanding|journal=Proceedings of 12th IEEE International Conference on Tools with Artificial Intelligence|location=Vancouver, British Columbia|date=November 13–15, 2000|pages= 46–53|doi=10.1109/TAI.2000.889845|isbn=0-7695-0909-6}}</ref>  (Software Engineering)\n* Filtering [[Spam (electronic)|spam]]<ref>Gee, K., Using Latent Semantic Indexing to Filter Spam, in: Proceedings, 2003 ACM Symposium on Applied Computing, Melbourne, Florida, pp. 460–464.</ref>  (System Administration)\n* Information visualization<ref name=landauer2004>Landauer, T., Laham, D., and Derr, M., From Paragraph to Graph: Latent Semantic Analysis for Information Visualization, Proceedings of the National Academy of Sciences, 101, 2004, pp. 5214–5219.</ref>\n* [[Automated essay scoring|Essay scoring]]<ref>Foltz, Peter W., Laham, Darrell, and Landauer, Thomas K., Automated Essay Scoring: Applications to Educational Technology, Proceedings of EdMedia,  1999.</ref>  (Education)\n* [[Literature-based discovery]]<ref>Gordon, M., and Dumais, S., Using Latent Semantic Indexing for Literature Based Discovery, Journal of the American Society for Information Science, 49(8), 1998, pp. 674–685.</ref>\n* Stock resturns prediction<ref name="Galvez2017">{{cite journal | url=http://www.sciencedirect.com/science/article/pii/S1877750317300091 | title=Assessing the usefulness of online message board mining in automatic stock prediction systems |author1=Ramiro H. Gálvez |author2=Agustín Gravano | journal=Journal of Computational Science | volume=19 | pages=1877-7503 | year=2017  | doi=10.1016/j.jocs.2017.01.001}}</ref>\n\nLSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.<ref>There Has to be a Better Way to Search, 2008, White Paper, Fios, Inc.</ref>\n\n== Challenges to LSI ==\n\nEarly challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.<ref>Karypis, G., Han, E., Fast Supervised Dimensionality Reduction Algorithm with Applications to Document Categorization and Retrieval, Proceedings of CIKM-00, 9th ACM Conference on Information and Knowledge Management.</ref>  However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source [[gensim]] software package.<ref name="rehurek2011">{{cite journal | url=http://dx.doi.org/10.1007/978-3-642-20161-5_29 |format=PDF| title=Subspace Tracking for Latent Semantic Analysis | author=Radim Řehůřek | journal=Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011 | volume=6611 | pages=289–300 | year=2011 | doi=10.1007/978-3-642-20161-5_29 |series=Lecture Notes in Computer Science|isbn=978-3-642-20160-8}}</ref>\n\nAnother challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).<ref>Bradford, R., An Empirical Study of Required Dimensionality for Large-scale Latent Semantic Indexing Applications, Proceedings of the 17th ACM Conference on Information and Knowledge Management, Napa Valley, California, USA, 2008, pp. 153–162.</ref>   However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.<ref name=landauer2008>Landauer, Thomas K., and Dumais, Susan T., Latent Semantic Analysis, Scholarpedia, 3(11):4356, 2008.</ref>\n\nChecking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain.  The variance contained in the data can be viewed by plotting the singular values (S) in a [[scree plot]].  Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain.  Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain.  Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.<ref>Cangelosi, R., Goriely A., Component Retention In Principal Component Analysis With Application to Cdna Microarray Data, BMC Biology Direct 2(2) (2007).</ref><ref>Jolliffe, L. T., Principal Component Analysis, Springer-Verlag, New York, (1986).</ref><ref>Hu, X., Z. Cai, et al., LSA: First Dimension and Dimensional Weighting, 25th Annual Meeting of the Cognitive Science Society, Boston, MA.</ref>\n\n== See also ==\n* [[Compound term processing]]\n* [[Explicit semantic analysis]]\n* [[Latent semantic mapping]]\n* [[Latent Semantic Structure Indexing]]\n* [[Principal components analysis]]\n* [[Probabilistic latent semantic analysis]]\n* [[Spamdexing]]\n* [[Topic model]]\n** [[Latent Dirichlet allocation]]\n* [[Distributional semantics]]\n* [[Coh-Metrix]]\n\n== References ==\n{{Reflist|30em}}\n\n==Further reading==\n* {{cite journal\n | url=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf\n | format=PDF\n | title=Introduction to Latent Semantic Analysis\n | author-link1= Thomas Landauer |first1=Thomas |last1=Landauer |first2=Peter W. |last2=Foltz |first3=Darrell |last3=Laham\n | journal=Discourse Processes\n | volume=25\n | pages=259–284\n | year=1998\n | doi=10.1080/01638539809545028\n | issue=2–3\n}}\n* {{cite journal\n | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf \n | format=PDF| title=Indexing by Latent Semantic Analysis\n | first1=Scott |last1=Deerwester |first2=Susan T. |last2=Dumais |first3=George W. |last3=Furnas |first4=Thomas K. |last4=Landauer |first5=Richard |last5=Harshman\n | author-link1=Scott Deerwester |author-link2=Susan Dumais |author-link3=George Furnas |author-link4=Thomas Landauer |author-link5=Richard Harshman\n | journal=Journal of the American Society for Information Science\n | volume=41\n | issue=6\n | pages=391–407\n | year=1990 \n | doi=10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9\n}} Original article where the model was first exposed.\n* {{cite journal\n | url=http://citeseer.ist.psu.edu/berry95using.html\n | title=Using Linear Algebra for Intelligent Information Retrieval\n | first1=Michael |last1= Berry |first2=Susan T. |last2=Dumais  |first3=Gavin W. |last3=O\'Brien\n | author-link1=Susan Dumais\n |year=1995\n}} [http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf (PDF)]. Illustration of the application of LSA to document retrieval.\n* {{cite web\n | url=http://iv.slis.indiana.edu/sw/lsa.html\n | title=Latent Semantic Analysis\n | publisher=InfoVis\n}}\n* {{cite web\n | url=http://cran.at.r-project.org/web/packages/lsa/index.html\n | title=An Open Source LSA Package for R\n | publisher=CRAN\n | author=Fridolin Wild\n | date=November 23, 2005\n | accessdate=November 20, 2006\n}}\n* {{ cite web\n | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM\n | title=A Solution to Plato\'s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge\n | author=[[Thomas Landauer]], [[Susan Dumais|Susan T. Dumais]]\n | accessdate=2007-07-02\n}}\n\n==External links==\n\n===Articles on LSA===\n* [http://www.scholarpedia.org/article/Latent_semantic_analysis Latent Semantic Analysis], a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.\n\n===Talks and demonstrations===\n* [http://videolectures.net/slsfs05_hofmann_lsvm/ LSA Overview], talk by Prof. [http://www.cs.brown.edu/~th/ Thomas Hofmann] describing LSA, its applications in Information Retrieval, and its connections to [[probabilistic latent semantic analysis]].\n* [http://www.semanticquery.com/archive/semanticsearchart/researchLSA.html Complete LSA sample code in C# for Windows]. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.\n\n===Implementations===\n\nDue to its cross-domain applications in [[Information Retrieval]], [[Natural Language Processing]] (NLP), [[Cognitive Science]] and [[Computational Linguistics]], LSA has been implemented to support many different kinds of applications.\n* [http://www.d.umn.edu/~tpederse/senseclusters.html Sense Clusters], an Information Retrieval-oriented perl implementation of LSA\n* [http://code.google.com/p/airhead-research/ S-Space Package], a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA\n* [http://code.google.com/p/semanticvectors/ Semantic Vectors] applies Random Projection, LSA, and Reflective Random Indexing to [[Lucene]] term-document matrices\n* [http://infomap-nlp.sourceforge.net/ Infomap Project], an NLP-oriented C implementation of LSA (superseded by semanticvectors project)\n* [http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page Text to Matrix Generator], A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA\n* [[Gensim]] contains a Python implementation of LSA for matrices larger than RAM.\n\n[[Category:Information retrieval techniques]]\n[[Category:Natural language processing]]\n[[Category:Latent variable models]]']
['Desktop search', '1274156', '{{Multiple issues|\n{{technical|date=October 2014}}\n{{manual||date=October 2016}}\n}}\n[[File:AdunaAutoFocus5.png|thumb|OSL Desktop Search engines software Aduna AutoFocus 5]]\n\'\'\'Desktop search\'\'\' tools search within a user\'s own [[computer files]] as opposed to searching the Internet. These tools are designed to find information on the user\'s PC, including web browser history, e-mail archives, text documents, sound files, images, and video.\n\nOne of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.\n\nA variety of desktop search programs are now available; see [[List of search engines#Desktop search engines|this list]] for examples.  Most desktop search programs are standalone applications, whereas a few also provide search capabilities in an [[integrated writing environment]] (IWE).\n\nDesktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users need to be able to quickly find relevant files, but on the other hand, they shouldn\'t have access to restricted files. According to analyst firm Gartner, up to 80% of some companies\' data is locked up inside [[unstructured data]] — the information stored on an end user\'s PC, the directories (folders) and files they\'ve created on a [[Computer network|network]], documents stored in repositories such as corporate [[intranet]]s and a multitude of other locations.<ref>{{Citation | url = http://www.computerweekly.com/Articles/2006/04/25/215622/security-special-report-who-sees-your-data.htm | title = Security special report: Who sees your data? | newspaper = Computer Weekly | date = 2006-04-25}}.</ref>  Moreover, many companies have structured or unstructured information stored in older [[file formats]] to which they don\'t have ready access.\n\nCompanies doing business in the [[United States]] are frequently required under regulatory mandates like [[Sarbanes-Oxley]], [[Health Insurance Portability and Accountability Act|HIPAA]] and [[FERPA]] to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users [[downloading]] tools from the [[Internet]]. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate [[Firewall (computing)|firewall]] and share those indexes with unauthorized users. In some cases, end users are able to index — but not preview — items they should not even know exist.{{Citation needed|date = November 2009}}\n\nHistorically, full desktop search comes from the work of [[Apple inc.|Apple Computer\'s]] [[Apple Advanced Technology Group|Advanced Technology Group]], resulting in the underlying [[AppleSearch]] technology in the early 1990s. It was used to build the [[Sherlock (software)|Sherlock]] search engine and then developed into [[Spotlight (software)|Spotlight]], which brought automated, non-timer-based full indexing into the operating system.\n\n== Technologies ==\nMost desktop search engines build and maintain an [[Index (search engine)|index database]] to achieve reasonable performance when searching several [[gigabyte]]s of [[data]]. Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools\' Everything Search Engine,<ref>{{cite web|title=Everything Search Engine|url=http://www.voidtools.com/|publisher=voidtools|accessdate=27 December 2013}}</ref> which performs searches over only filenames &mdash; not the files\' contents &mdash; for NTFS volumes only, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine,<ref>{{cite web|title=Vegnos|url=http://www.vegnos.com|publisher=Vegnos|accessdate=27 December 2013}}</ref> which performs searches over filenames and files\' contents without building any indices. The benefits to not having indices is that, in addition to not requiring persistent storage, more powerful queries (e.g., [[regular expressions]]) can be issued, whereas indexed search engines are limited to keyword-based queries. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive.\n\nDesktop search tools typically collect three types of information about files:\n* file and folder names\n* [[metadata]], such as titles, authors, comments in file types such as [[MP3]], [[Portable Document Format|PDF]] and [[JPEG]]\n* file content (for supported types of documents only)\n\nTo search effectively within documents, the tools need to be able to parse many different types of documents. This is achieved by using filters that interpret selected file formats. For example, a \'\'Microsoft Office Filter\'\' might be used to search inside [[Microsoft Office]] documents.\n\nLong-term goals for desktop search include the ability to search the contents of image files, sound files and video by context.<ref>{{cite web|url=http://www.niallkennedy.com/blog/archives/2006/10/video-search.html|title=The current state of video search|author=Niall Kennedy|date=17 October 2006|work=Niall Kennedy|accessdate=24 June 2015}}</ref><ref>{{cite web|url=http://www.niallkennedy.com/blog/archives/2006/10/audio-search.html|title=The current state of audio search|author=Niall Kennedy|date=15 October 2006|work=Niall Kennedy|accessdate=24 June 2015}}</ref>\n\nThe sector attracted considerable attention from the struggle between Microsoft and Google.<ref>{{cite web|url=http://news.bbc.co.uk/1/hi/technology/3952285.stm|title=BBC NEWS - Technology - Search wars hit desktop computers|work=bbc.co.uk|accessdate=24 June 2015}}</ref> According to market analysts, both companies were attempting to leverage their monopolies (of [[web browser]]s and [[search engine]]s, respectively) to strengthen their dominance. Due to [[Google]]\'s complaint that users of Windows Vista cannot choose any competitor\'s desktop search program over the built-in one, an agreement was reached between [[US Justice Department]] and [[Microsoft]] that [[Windows Vista Service Pack 1]] would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default.<ref>{{cite web|url=http://goebelgroup.com/searchtoolblog/2007/06/20/microsoft-agrees-to-change-vista-desktop-search-tool/|title=SearchMax|work=goebelgroup.com|accessdate=24 June 2015}}</ref>\n\nAs of September, 2011, Google ended life for Google Desktop, a program designed to make it easy for users to search their own PCs for emails, files, music, photos, Web pages and more.<ref>[http://googledesktop.blogspot.com/2011/09/google-desktop-update.html/ "Google Desktop Update" (Sept 2011)]</ref>\n\nDesktop search products are software alternatives to Windows Desktop and Outlook Search, helping business professional sift through desktop files, emails, attachments, SharePoint data, and more.,<ref>[http://www.brianmadden.com/blogs/brianmadden/archive/2015/03/11/what-do-you-do-for-desktop-search-in-vdi-and-rdsh.aspx  „What do you do for desktop search in VDI and RDSH?“]. Blogpost by Brian Madden on brainmadden.com. Retrieved on March 25, 2015.</ref><ref>{{cite web|url=http://venturebeat.com/2008/06/02/lookeen-offers-a-new-way-way-for-outlook-users-to-search/|title=Lookeen offers a new way for Outlook users to search|author=Anthony Ha|date=2 June 2008|work=VentureBeat|accessdate=8 March 2016}}</ref><ref>{{cite web|url=http://www.computerworld.com/article/2475293/desktop-apps/x1-rises-again-with-desktop-search-8--virtual-edition.html/|title=X1 rises again with Desktop Search 8, Virtual Edition|author=Robert L. Mitchell|date=8 May 2013|work=Computerworld|accessdate=24 June 2015}}</ref>\n\n==Platforms & their histories==\nThere are three main platforms that desktop search falls into. [[Microsoft Windows|Windows]], [[Mac OS|Mac]] OS & [[Linux]]. This article will focus on the history of these search platforms, the features they had, and how those features evolved.\n\n=== Windows ===\nToday\'s [[Windows Search]] replaced WDS ([[Windows Desktop Search]]). WDS, in turn, replaced [[Indexing Service]]. A "a base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching"<ref>{{cite web|url=https://msdn.microsoft.com/en-us/library/ee805985%28v=vs.85%29.aspx|title=Indexing Service|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}</ref> Indexing service was originally released in August 1996, it was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation.<ref>{{cite web|url=https://msdn.microsoft.com/en-us/library/dd582937%28v=office.11%29.aspx|title=Indexing with Microsoft Index Server|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}</ref> This made indexing large amounts of files require extremely powerful hardware and very long wait times.\n\nIn 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought "Instant searching" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters.<ref>{{cite web|url=http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx|archiveurl=https://web.archive.org/web/20110924212903/http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx|title=Windows Search: Technical FAQ|archivedate=24 September 2011|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}</ref> Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed. For example, if the indexed files amounted to around 100GB of space, the index would, itself, be 10GB large.\n\nWith the release of [[Windows Vista]] came Windows Search 3.1. Unlike its predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the [[RAM]] and [[CPU]] requirements were greatly reduced, cutting back indexing times immensely. Windows Search 4.0 is currently running on all PCs with [[Windows 7]] and up.\n\n=== Mac OS ===\nMac OS was the first to implement Desktop Search with its [[AppleSearch]] search engine, allowing users to fully search all documents within their Macintosh computer, including file format types, meta-data on those files, and content within the files. AppleSearch was a [[Client–server model|client/server application]], and as such required a server separate from the main device in order to function. The biggest issue with AppleSearch were its large resource requirements: "AppleSearch requires at least a 68040 processor and 5MB of RAM."<ref>{{cite web|url=http://infomotions.com/musings/tricks/manuscript/1600-0001.html|title=AppleSearch|work=infomotions.com|accessdate=24 June 2015}}</ref> At the time, a Macintosh computer with these specifications was priced at approximately $1400; equivalent to $2050 in 2015.<ref>{{cite web|url=http://stats.areppim.com/calc/calc_usdlrxdeflator.php|title=Converter of current to real US dollars - using the GDP deflator|author=eduardo casais|work=areppim.com|accessdate=24 June 2015}}</ref> On top of this, the software itself cost an additional $1400 for a single license.\n\nIn 1997, [[Sherlock (software)|Sherlock]] was released alongside Mac OS 8.5. Sherlock (named after the famous fictional detective [[Sherlock Holmes]]) was integrated into Mac OS\'s file browser&nbsp;– [[Finder (software)|Finder]]. Sherlock extended the desktop search function to the World Wide Web, allowing users to search both locally and externally. Adding additional functions—such as internet access—to Sherlock was relatively simple, as this was done through plugins written as plain text files. Sherlock was included in every release of Mac OS from [[Mac OS 8]], before being deprecated and replaced by [[Spotlight (software)|Spotlight]] and [[Dashboard (Mac OS)|Dashboard]] in [[Mac OS X Tiger|Mac OS X 10.4 Tiger]]. It was officially removed in [[Mac OS X Leopard|Mac OS X 10.5 Leopard]]\n\n[[Spotlight (software)|Spotlight]] was released in 2005 as part of [[Mac OS X Tiger|Mac OS X 10.4 Tiger]]. It is a Selection-based search tool, which means the user invokes a query using only the mouse. Spotlight allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage, and uses a built-in calculator and Oxford American Dictionary to offer quick access to small calculations and word definitions.<ref>{{cite web|url=http://www.apple.com/pr/library/2005/04/12Apple-to-Ship-Mac-OS-X-Tiger-on-April-29.html|title=Apple - Press Info - Apple to Ship Mac OS X "Tiger" on April 29|work=apple.com|accessdate=24 June 2015}}</ref> While Spotlight initially has a long startup time, this decreases as the hard disk is indexed. As files are added by the user, the index is constantly updated in the background using minimal CPU & RAM resources.\n\n=== Linux ===\nThere are a wide range of desktop search options for Linux users, depending upon the skill level of the user, their preference to use desktop tools which tightly integrate into their desktop environment, command-shell functionality (often with advanced scripting options), or browser-based users interfaces to locally running software.  In addition, many users create their own indexing from a variety of indexing packages (e.g. one which does extraction and indexing of PDF/DOC/DOCX/[[OpenDocument|ODT]] documents well, another search engine which works w/ vcard, LDAP, and other directory/contact databases, as well as the conventional <tt>find</tt> and <tt>locate</tt> commands.\n\n====Ubuntu====\nThe [[Ubuntu distribution]] is a popular version of Linux. Strangely enough, Ubuntu didn\'t have desktop search until Feisty Fawn 7.04. Using [[Tracker (search software)|Tracker]]<ref>{{cite web|url=http://arstechnica.com/information-technology/2007/07/afirst-look-at-tracker-0-6-0/|title=A first look at Tracker 0.6.0|work=Ars Technica|accessdate=24 June 2015}}</ref> desktop search, the desktop search feature was very similar to Mac OS\'s AppleSearch and Sherlock. Considering the fact that both are UNIX-based systems. Tracker, released in late 2007, was built to have a relatively low impact on system resources. But unfortunately occasionally had sporadic control over what resources it was using. It not only featured the basic features of file format sorting and meta-data matching, but support for searching through emails and instant messages was added. Years later, in 2014 [[Recoll]]<ref>{{cite web|url=http://www.lesbonscomptes.com/recoll/usermanual/index.html#RCL.INDEXING|title=Recoll user manual|work=lesbonscomptes.com|accessdate=24 June 2015}}</ref> was added to Linux distributions, it works with other search programs such as Tracker and [[Beagle (software)|Beagle]] to provide efficient full text search. This greatly increased the types of queries that Linux desktop searches could handle as well as file types. A major advantage of Recoll is that it allows for greater customization of what is indexed. For example, Recoll will index the entire hard disk by default, but will and can index just a few select directories instead of wasting time indexing directories you know you will never need to look at. It also allows for more search options, you may actually narrow down what kind of query you want to ask. For example, you could search for just file types or by content.<ref>{{cite web|url=http://archive09.linux.com/feature/114283|title=Linux.com|work=linux.com|accessdate=24 June 2015}}</ref>\n\n====[[openSUSE]]<ref>http://www.opensuse.org/</ref>====\n<!--TODO! Prior desktop search before KDE 3.5-->\nStarting with [[KDE4]], the [[NEPOMUK (software)|NEPOMUK]] was introduced.  It provided the ability to index a wide range of desktop content, email, and use semantic web technologies (e.g. [[Resource Description Framework|RDF]]) to annotate the database.  The introduction faced a few glitches, much of which seemed to be based on the [[triplestore]].  Performance improved (at least for queries) by switching the backend to a stripped own version of the [[Virtuoso]] Open Source Edition, however indexing remained a common user complaint.  \nBased on user feedback, the Nepomuk indexing and search has been replaced with the Baloo framework<ref>https://community.kde.org/Baloo</ref> based on [[Xapian]].\n\n==See also==\n*[[List of search engines#Desktop search engines|List of desktop search engines]]\n\n== References ==\n{{reflist|2}}\n\n{{Navigationbox Desktopsearch}}\n\n{{DEFAULTSORT:Desktop Search}}\n[[Category:Desktop search engines| ]]\n[[Category:Information retrieval genres]]']
['Visual search engine', '17813833', '{{multiple issues|\n{{original research|date=February 2012}}\n{{refimprove|date=February 2012}}\n}}\n\nA \'\'\'Visual Search Engine\'\'\' is a [[search engine (computing)|search engine]] designed to search for information on the [[World Wide Web]] through the input of an image or a search engine with a visual display of the search results. Information may consist of [[web page]]s, locations, other images and other types of documents. This type of search engines is mostly used to search on the mobile Internet through an image of an unknown object (unknown search query). Examples are buildings in a foreign city. These search engines often use techniques for [[CBIR|Content Based Image Retrieval]].\n\nA visual search engine searches images, patterns based on an algorithm which it could recognize and gives relative information based on the selective or apply pattern match technique.\n\n== Classification ==\nDepending on the nature of the search engine there are two main groups, those which aim to find visual information  and those with a visual display of results.\n\n== Visual information searchers ==\n[[File:Imatge cercadors 1.jpg|thumb|Screenshot of results shown by the image searcher through example GOS]]\n\n=== Image search ===\nAn image search is a search engine that is designed to find an image. The search can be based on keywords, a picture, or a web [[Hyperlink|link]] to a picture. The results depend on the search criterion, such as [[metadata]], distribution of color, shape, etc., and the search technique which the browser uses.\n[[File:Imatge wiki 2.png|thumb|Diagram of a search realized through example based on detectable regions from an image]]\n\n==== Image search techniques ====\nTwo techniques currently used in image search:\n\n\'\'\'Search by metadata:\'\'\' Image search is based on comparison of metadata associated with the image as keywords, text, etc.. and it is obtained a set of images sorted by relevance. The metadata associated with each image can reference the title of the image, format, color, etc.. and can be generated manually or automatically. This metadata generation process is called audiovisual indexing.\n\n\'\'\'Search by example:\'\'\' In this technique, also called [[content-based image retrieval]], the search results are obtained through the comparison between images using computer vision techniques. During the search it is examined the content of the image such as color, shape, texture or any visual information that can be extracted from the image. This system requires a higher [[Computational complexity theory|computational complexity]], but is more efficient and reliable than search by metadata.\n\nThere are image searchers that combine both search techniques, as the first search is done by entering a text, and then, from the images obtained can refine the search using as search parameters the images which appear as a result.\n\n=== Video search ===\nA video search is a [[search engine (computing)|search engine]] designed to search video on the net. Some video searchers process the search directly in the Internet, while others shelter the videos from which the search is done. Some searchers also enable to use as search parameters the [[File format|format]] or the length of the video. Usually the results come with a miniature capture of the video.\n\n==== Video search techniques ====\nCurrently, almost all video searchers are based on keywords (search by metadata) to perform searches. These keywords can be found in the title of the video, text accompanying the video or can be defined by the author. An example of this type of search is [[YouTube]].\n\nSome searchers generate keywords manually, while others use [[algorithms]] to analyze the audiovisual content of the video and to generate labels. The combination of these two processes improves the reliability of the search.\n\n=== 3D Models searcher ===\nA searcher of 3D models aims to find the file of a 3D modeling object from a [[database]] or network. At first glance the implementation of this type of searchers may seem unnecessary, but due to the continuous documentary inflation of the Internet, every day it becomes more necessary indexing information.\n\n==== 3D Models search techniques ====\nThese have been used with traditional text-based searchers (keywords / tags), where the authors of the indexed material, or Internet users, have contributed these tags or keywords.  Because it is not always effective, it has recently been investigated in the implementation of search engines that combine the search using text with the search compared to 2D drawings, 3D drawings and 3D models.\n\n[[Princeton University]] has developed a search engine that combines all these parameters to perform the search, thus increasing the efficiency of search.<ref name= funk>{{cite journal | last= Funkhouser | first= Thomas  | first2 = Patrick | last2 = Min | first3 = Michael | last3 = Kazhdan | first4 = Joyce | last4 = Chen | first5 = Alex | last5 = Halderman | first6 = David | last6 = Dobkin | first7 = David | last7 = Jacobs  | year= 2002 | title=  A Search Engine for 3D Models | journal= ACM Transactions on Graphics |url=https://www.cs.princeton.edu/~funk/tog03.pdf | volume= 22 | issue =1  | pages= 83-105  |doi = 10.1145/588272.588279 }}</ref>\n\nImaginestics LLC created the world\'s first online shape search engine in the fall of 2005.<ref>{{cite web|url=http://www.purdue.edu/uns/html3month/2006/060824.Imaginestics.grant.html|title=Purdue Research Park\'s Imaginestics wins grant for research on search engines|work=purdue.edu}}</ref> They currently use VizSeek search engine technology in the industrial and manufacturing settings to help discover parts using shape as the matching criteria.\n\n=== Mobile visual search ===\nA mobile image searcher is a type of [[search engine]] designed exclusively for mobile phones, through which you can find any information on [[Internet]], through an image made with the own [[mobile phone]] or using certain words ([[Keyword (computer programming)|keywords]]).\n\n==== Introduction ====\nMobile phones have evolved into powerful image and video processing devices equipped with high-resolution cameras, color displays, and hardware-accelerated graphics. They are also increasingly equipped with a global positioning system and connected to broadband wireless networks. All this enables a new class of applications that use the camera phone to initiate search queries about objects in visual proximity to the user (Figure 1). Such applications can be used, e.g., for identifying products, comparison shopping, finding information about movies, compact disks (CDs), real estate, print media, or artworks.\n\n==== Process ====\nTypically, this type of search engine uses techniques of [[query by example]] or [[Content-based image retrieval|Image query by example]], which use the content, shape, texture and color of the image to compare them in a [[database]] and then deliver the approximate results from the query.\n\nThe process used in these searches in the [[mobile phone]]s is as follows:\n\nFirst, the image is sent to the server application. Already on the server, the image will be analyzed by different analytical teams, as each one is specialized in different fields that make up an image. Then, each team will decide if the submitted image contains the fields of their speciality or not.\n\nOnce this whole procedure is done, a central computer will analyze the data and create a page of the results sorted with the efficiency of each team, to eventually be sent to the [[mobile phone]].\n\n==== Applications ====\n[[Google Goggles]] is the most popular application of image search engines{{citation required|date=November 2016}}, developed by [[Google labs|Google Labs]]. Available for [[Android (operating system)|Android]] only today. [[CamFind]] is a similar application available for both [[Android (operating system)|Android]] and [[iOS]].\n\nJustVisual.com (formerly known as \'Superfish\') and its LikeThat showcase apps are API for developers to create their own visual-search mobile app.\n\nOther companies in the image recognition space are the [[reverse image search]]-engines [[TinEye]] and [[Google]]\'s [[Google Images#Search by image|"search by image" feature of Google Images]].\n\n== Visual display searchers ==\nAnother type of visual search is a search engine that shows results with a visual display image. This is an alternative to the traditional results of a sequence of links. Through some kind of image display, such as graphs, diagrams, previews of the websites, etc., it presents the results visually so that it is easier to find the desired material.\nSuch search engines like [http://www.kiddle.co/ Kiddle] and Manzia<ref>{{cite web|title=Manzia Search|website=http://www.manzia.com|accessdate=8 August 2014}}</ref> present a new concept in the presentation of results, but the search techniques used are the same as in other search engines.\n\n==References==\n{{reflist}}\n\n[[Category:Information retrieval genres]]\n[[Category:Internet search engines]]\n[[Category:Multimedia]]']
['Question answering', '360030', '{{other uses|question|answer}}\n{{multiple issues|\n{{cleanup|date=January 2012|reason=\'\'\'appearance of plagiarised text (now in extensive footnote), use of draft rather than published sources, extensive appearance of text violating [[WP:VERIFY]] and/or [[WP:OR]], use of jargon to define jargon, etc.\'\'\'}}\n{{cleanup rewrite|date=January 2012}}\n{{more footnotes|date=February 2014}}\n{{citation style|date=January 2016}}\n}}\n\n\'\'\'Question Answering\'\'\' (\'\'\'QA\'\'\') is a computer science discipline within the fields of [[information retrieval]] and [[natural language processing]] (NLP), which is concerned with building systems that automatically answer questions posed by humans in a [[natural language]].\n\nA QA implementation, usually a computer program, may construct its answers by querying a structured [[database]] of knowledge or information, usually a [[knowledge base]]. More commonly, QA systems can pull answers from an unstructured collection of natural language documents.\n\nSome examples of natural language document collections used for QA systems include:\n* a local collection of reference texts\n* internal organization documents and web pages\n* compiled [[newswire]] reports\n* a set of [[Wikipedia]] pages\n* a subset of [[World Wide Web]] pages\n\n with a wide range of question types including: fact, list, [[definition]], \'\'How\'\', \'\'Why\'\', hypothetical, semantically constrained, and cross-lingual questions.\n\n* \'\'Closed-domain\'\' question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, \'\'closed-domain\'\' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease <ref>Roser Morante , Martin Krallinger , Alfonso Valencia and  Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimer\'s Disease. CLEF 2012 Evaluation Labs and Workshop. September 17, 2012</ref>\n* \'\'[[Open domain#References|Open-domain]]\'\' question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.\n\n==History==\n{{unreferenced section|date=January 2016}}\nTwo early QA systems were BASEBALL<ref>{{cite journal|last1=GREEN JR|first1=Bert F|title=Baseball: an automatic question-answerer.|journal=western joint IRE-AIEE-ACM computer conference|date=1961|pages=219–224|display-authors=etal}}</ref> and LUNAR.<ref>{{cite journal|last1=Woods|first1=William A|last2=Kaplan|first2=R.|title=Lunar rocks in natural English: Explorations in natural language question answering|journal=Linguistic structures processing 5|date=1977|volume=5|pages=521–569}}</ref> BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to [[ELIZA]] and [[DOCTOR]], the first [[chatterbot]] programs.\n\n[[SHRDLU]] was a highly successful question-answering program developed by [[Terry Winograd]] in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.\n\nIn the 1970s, [[knowledge base]]s were developed that targeted narrower domains of knowledge. The QA systems developed to interface with treport[[expert system|<nowiki/>]]s produced more repeatable and valid responses to questions within an area of knowledge. These [[expert systems]] closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized [[knowledge base]]s, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.\n\nThe 1970s and 1980s saw the development of comprehensive theories in [[computational linguistics]], which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by [[Robert Wilensky]] at [[U.C. Berkeley]] in the late 1980s. The system answered questions pertaining to the [[Unix]] operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.\n\nRecently, specialized natural language QA systems have been developed, such as [http://bitem.hesge.ch/content/eagli-eagle-eye EAGLi] for health and life scientists.\n\n==Architecture==\n{{refimprove section|date=January 2016}}\nMost modern QA systems use [[natural language]] text documents as their underlying knowledge source.{{citation needed|date=January 2016}}  [[Natural language processing]] techniques are used to both process the question and index or process the text [[Text corpus|corpus]] from which answers are extracted.{{citation needed|date=January 2016}} An increasing number of QA systems use the [[World Wide Web]] as their corpus of text and knowledge; however, many of these tools do not produce a human-like answer, but rather employ "shallow" methods (keyword-based techniques, templates, etc.) to produce a list of documents or a list of document excerpts containing the probable answer highlighted.{{citation needed|date=January 2016}}\n\nIn an alternative QA implementation, human users assemble knowledge in a structured database, called a [[knowledge base]], similar to those employed in the [[expert systems]] of the 1970s.{{citation needed|date=January 2016}} It is also possible to employ a combination of structured databases and natural language text documents in a hybrid QA system.{{citation needed|date=January 2016}} Such a hybrid system may employ data mining algorithms to populate a structured knowledge base that is also populated and edited by human contributors.{{citation needed|date=January 2016}} An example hybrid QA system is the [[Wolfram Alpha]] QA system which employs natural language processing to transform human questions into a form that is processed by a curated knowledge base.{{citation needed|date=January 2016}}\n\nAs of 2001, QA systems typically included a \'\'question classifier\'\' module that determines the type of question and the type of answer.<ref>Hirschman, L. & Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.</ref> After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text; thus, a \'\'document retrieval module\'\' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer, and a \'\'filter\'\' preselects small text fragments that contain strings of the same type as the expected answer.{{citation needed|date=January 2016}} For example, if the question is "Who invented\npenicillin?", the filter returns text that contain names of people. Finally, an \'\'answer extraction\'\' module looks for further clues in the text to determine if the answer candidate can indeed answer the question.{{citation needed|date=January 2016}}\n\nA \'\'multiagent\'\' question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge; a meta–agent controls the cooperation between question answering agents and chooses the most relevant answer(s).<ref>{{vcite journal |author=Galitsky B, Pampapathi R|title=Can many agents answer questions better than one|journal=First Monday |volume = 10| Number=1 |date=2005 | url = http://firstmonday.org/ojs/index.php/fm/article/view/1204/1124 |doi=10.5210/fm.v10i1.1204\n}}</ref>\n\n==Question answering methods==\nQA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,<ref>Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).</ref> leading to two benefits:\n# By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.\n# Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.\n\nQuestion answering heavily relies on [[reasoning]]. There are a number of question answering systems designed in [[Prolog]],<ref>{{cite book |last=Galitsky |first=Boris |title=Natural Language Question Answering System: Technique of Semantic Headers |url=https://books.google.com/books?id=LkNmAAAACAAJ |series=International Series on Advanced Intelligence |volume=Volume 2 |year=2003 |publisher=Advanced Knowledge International |location=Australia |isbn=978-0-86803-979-4}}</ref> a [[logic programming]] language associated with [[artificial intelligence]].\n\n===Open domain question answering===\n{{unreferenced section|date=January 2016}}\nIn [[information retrieval]], an open domain question answering system aims at returning an answer in response to the user\'s question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from [[computational linguistics]], [[information retrieval]] and [[knowledge representation]] for finding answers.\n\nThe system takes a [[natural language]] question as an input rather than a set of keywords, for example, "When is the national day of China?" The sentence is then transformed into a query through its [[logical form]]. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.\n\nKeyword [[Data extraction|extraction]] is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. "Who", "Where" or "How many", these words tell the system that the answers should be of type "Person", "Location", "Number" respectively. In the example above, the word "When" indicates that the answer should be of type "Date". POS (Part of Speech) tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is "Chinese National Day", the predicate is "is" and the adverbial modifier is "when", therefore the answer type is "Date". Unfortunately, some interrogative words like "Which", "What" or "How" do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as [[WordNet]] can then be used for understanding the context.\n\nOnce the question type has been identified, an [[Information retrieval]] system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as "Who" or "Where", a Named Entity Recogniser is used to find relevant "Person" and "Location" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.\n\nA [[vector space model]] can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is "1st Oct."\n\n==Issues==\nIn 2002, a group of researchers presented an unpublished and largely unsourced report as a funding support document, in which they describe a 5-year roadmap of research current to the state of the question answering filed at that time.<ref>Burger, J., Cardie, C., Chaudhri, V., Gaizauskas, R., Harabagiu, S., Israel, D., Jacquemin, C., Lin, C-Y., Maiorano, S., Miller, G., Moldovan, D., Ogden, B., Prager, J., Riloff, E., Singhal, A., Shrihari, R., Strzalkowski, T., Voorhees, E., Weishedel, R., date unknown, "Tasks and Program Structures to Roadmap Research in Question Answering (QA)," at [http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Issues] [DRAFT DOCUMENT], accessed 1 January 2016.</ref><ref>Here is some content taken verbatim from that roadmap (see preceding citation): "[1] Question classes: Different types of questions (e.g., "What is the capital of [[Liechtenstein]]?" vs. "Why does a [[rainbow]] form?" vs. "Did [[Marilyn Monroe]] and [[Cary Grant]] ever appear in a movie together?") require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.{{example needed|date=February 2011}} [2] Question processing: The same information request can be expressed in various ways, some interrogative ("Who is the King of Lesotho?") and some assertive ("Tell me the name of the King of Lesotho."). A semantic model of question understanding and processing would recognize equivalent questions, regardless of how they are presented. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification. [3] Context and QA : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions. (For example, the question, "Why did Joe Biden visit Iraq in January 2010?" might be asking why Vice President Biden visited and not President Obama, why he went to Iraq and not Afghanistan or some other country, why he went in January 2010 and not before or after, or what Biden was hoping to accomplish with his visit. If the question is one of a series of related questions, the previous questions and their answers might shed light on the questioner\'s intent.) [4] Data sources for QA: Before a question can be answered, it must be known what knowledge sources are available and relevant. If the answer to a question is not present in the data sources, no matter how well the question processing, information retrieval and answer extraction is performed, a correct result will not be obtained. [4] Answer extraction: Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context.{{example needed|date=February 2011}} [5] Answer formulation: The result of a QA system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc.), a quantity (monetary value, length, size, distance, etc.) or a date (e.g. the answer to the question, "On what day did Christmas fall in 1989?") the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents. [6] Real time question answering: There is need for developing Q&A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question. [7] Multilingual (or cross-lingual) question answering: The ability to answer a question posed in one language using an answer corpus in another language (or even several). This allows users to consult information that they cannot use directly. (See also [[Machine translation]].) [8] Interactive QA: It is often the case that the information need is not well captured by a QA system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but to have a dialogue with the system. In addition, system may also use previously answered questions. (For example, the system might ask for a clarification of what sense a word is being used, or what type of information is being asked for.) [9] Advanced reasoning for QA: More sophisticated questioners expect answers that are outside the scope of written texts or structured databases. To upgrade a QA system with such capabilities, it would be necessary to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms, as well as knowledge specific to a variety of domains. [[Evi (software)|Evi]] is an example of such as system. [10] Information clustering for QA: Information clustering for question answering systems is a new trend that originated to increase the accuracy of question answering systems through search space reduction. In recent years this was widely researched through development of question answering systems which support information clustering in their basic flow of process. [11] User profiling for QA: The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user, and so forth. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.{{example needed|date=February 2011}} [12] Deep Question Answering: Deep QA complement traditional Question Answering by adding some machine learning capabilities within a standard factoid question answering pipeline. The idea is to leverage curated data repositories or knowledge bases, which can be general ones such as Wikipedia, or domain-specific (e.g. molecular biology) in order to provide more accurate answers to the end-users.</ref>\n<ref>On the subject of interactive QA, see also Perera, R. and Nand, P. (2014). "Interaction History Based Answer Formulation for Question Answering," at [http://rivinduperera.com/publications/kesw2014.html] [DRAFT DOCUMENT], accessed 1 January 2015.{{full citation needed|date=January 2016}}</ref>{{full citation needed|date=January 2016}}\n<ref>On the subject of information clustering for QA, see also Perera, R. (2012). "IPedagogy: Question Answering System Based on Web Information Clustering," at [http://rivinduperera.com/publications/t4e2012.html] [DRAFT DOCUMENT], accessed 1 January 2015.{{full citation needed|date=January 2016}}</ref>{{full citation needed|date=January 2016}}\n<ref>On the subject of deep question answering, see the following citation.</ref><ref>{{cite journal | pmc = 4572360 | pmid=26384372 | doi=10.1093/database/bav081 | volume=2015 | title=Deep Question Answering for protein annotation | year=2015 | journal=Database (Oxford) |vauthors=Gobeill J, Gaudinat A, Pasche E, Vishnyakova D, Gaudet P, Bairoch A, Ruch P }}</ref>\n<!-- \nBecause much of the text in this section was copied and pasted from the "roadmap" document, which itself is a draft and unpublished document, the text was moved into a footnote. \n-->\n\n==Progress==\nQA systems have been extended in recent years to encompass additional domains of knowledge<ref>Maybury, M. T. editor. 2004. [http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321055 New Directions in Question Answering.] AAAI/MIT Press.</ref>  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:\n\n* interactivity—clarification of questions or answers\n* answer reuse or caching\n* knowledge representation and reasoning\n* social media analysis with QA systems\n* [[sentiment analysis]]<ref>{{webarchive |url=https://web.archive.org/web/20121027153311/http://totalgood.com/bitcrawl/ |date=October 27, 2012 |title=BitCrawl by Hobson Lane }}</ref>\n* utilization of thematic roles<ref>Perera, R. and Perera, U. 2012. [http://rivinduperera.com/publications/qacd_coling2012.html Towards a thematic role based target identification model for question answering.]</ref>\n* semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts<ref>{{cite conference |author1=Bahadorreza Ofoghi |author2=John Yearwood |author3=Liping Ma  |last-author-amp=yes | year=2008 | conference=The 30th European Conference on Information Retrieval (ECIR\'08)| pages= 430–437 | publisher=Springer Berlin Heidelberg | url=http://link.springer.com/chapter/10.1007/978-3-540-78646-7_40 | title=The impact of semantic class identification and semantic role labeling on natural language answer extraction}}</ref>\n* utilization of linguistic resources,<ref>{{cite journal |author1=Bahadorreza Ofoghi |author2=John Yearwood |author3=Liping Ma  |last-author-amp=yes |title=The impact of frame semantic annotation levels, frame‐alignment techniques, and fusion methods on factoid answer processing | journal=Journal of the American Society for Information Science and Technology |volume=60 |issue=2 |pages=247–263 |year =2009 |url=http://onlinelibrary.wiley.com/doi/10.1002/asi.20989/abstract;jsessionid=099F3D167FD0511A48FB1C19C1060676.f02t02?deniedAccessCustomisedMessage=&userIsAuthenticated=false |doi=10.1002/asi.20989}}</ref> such as [[WordNet]], [[FrameNet]], and the similar\n\nIBM\'s question answering system, [[Watson (computer)|Watson]], defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.\n<ref>http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=0</ref>\n\n==References==\n{{reflist}}\n\n==Further reading==\n{{citation style|section|date=January 2016}}\n* Dragomir R. Radev, John Prager, and Valerie Samn. [http://clair.si.umich.edu/~radev/papers/anlp00.pdf Ranking suspected answers to natural language questions using predictive annotation]. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.\n* John Prager, Eric Brown, Anni Coden, and Dragomir Radev. [http://clair.si.umich.edu/~radev/papers/sigir00.pdf Question-answering by predictive annotation]. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.\n*{{cite book | last = Hutchins | first = W. John | authorlink = John Hutchins |author2=Harold L. Somers  | year = 1992 | title = An Introduction to Machine Translation | url = http://www.hutchinsweb.me.uk/IntroMT-TOC.htm | publisher = Academic Press | location = London | isbn = 0-12-362830-X}}\n* L. Fortnow, Steve Homer (2002/2003).   [http://people.cs.uchicago.edu/~fortnow/papers/history.pdf A Short History of Computational Complexity].  In D. van Dalen, J. Dawson, and A. Kanamori, editors, \'\'The History of Mathematical Logic\'\'. North-Holland, Amsterdam.\n\n==External links==\n* [http://aclia.lti.cs.cmu.edu/ntcir8 Question Answering Evaluation at NTCIR]\n* [http://trec.nist.gov/data/qamain.html Question Answering Evaluation at TREC]\n* [http://nlp.uned.es/clef-qa/ Question Answering Evaluation at CLEF]\n* [http://www.gyanibano.com Quiz Question Answers]\n\n{{Computable knowledge}}\n{{Natural Language Processing}}\n\n\n[[Category:Artificial intelligence applications]]\n[[Category:Natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Information retrieval genres]]']
['Noisy text analytics', '6026708', '{{multiple issues|\n{{COI|date=December 2015}}\n{{notability|date=December 2015}}\n{{Orphan|date=June 2016}}\n}}\n\'\'\'Noisy text analytics\'\'\' is a process of [[information extraction]] whose goal is to automatically extract structured or semistructured information from [[noisy text|noisy unstructured text data]]. While [[Text analytics]] is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as [[online chat]], [[Text messaging|text messages]], [[e-mail]]s, [[message boards]], [[newsgroups]], [[blogs]], [[wikis]] and [[web pages]]. Also, text produced by processing spontaneous speech using [[automatic speech recognition]] and printed or handwritten text using [[optical character recognition]] contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, [[abbreviation]]s, non-standard words, false starts, repetitions, missing [[punctuation]]s, missing [[letter case]] information, pause filling words such as “um” and “uh” and other texting and [[speech disfluencies]]. Such text can be seen in large amounts in [[contact centre (business)|contact centers]], [[chat room]]s, [[optical character recognition]] (OCR) of text documents, [[short message service]] (SMS) text, etc. Documents with [[historical language]] can also be considered noisy with respect to today’s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.\n\n== Techniques for noisy text analysis ==\nMissing punctuation and the use of non-standard words can often hinder standard [[natural language processing]] tools such as [[part-of-speech tagging]]\nand [[parsing]]. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed.\n\n== Possible source of noisy text ==\n* [[World wide web]]: Poorly written text is found in web pages, [[online chat]], [[blogs]], [[wikis]], [[discussion forum]]s, [[newsgroups]]. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, [[trend estimation]], etc. Also, because of the large amount of data, it is necessary to find efficient methods of [[information extraction]], [[Statistical classification|classification]], [[automatic summarization]] and analysis of these data.\n* [[Contact centre (business)|Contact centers]]: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, [[online chat]] and [[E-mail]]. The contact center industry produces gigabytes of data in the form of [[E-mails]], chat logs, voice conversation [[Transcription (linguistics)|transcription]]s, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art [[automatic speech recognition]] results in text with 30-40% [[word error rate]]. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.\n* Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of [[hard copy]] documents. To retrieve and process the content from such documents, they need to be processed using [[Optical Character Recognition]]. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% [[word error rate]]s to as high as 50-60% [[word error rate]]s. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.\n* [[Text messaging|Short Messaging Service]] (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.\n\n== References ==\n*[http://www.springerlink.com/content/ql711884654q/?p=c6beb20b8dfa4389b5e4daf2dd63618e&pi=0 "Special Issue on Noisy Text Analytics - International Journal on Document Analysis and Recognition (2007), Springer, Guest Editors Craig Knoblock, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, Vol. 10, No. 3-4, December 2007."]\n*[http://arXiv.org/abs/0810.0332 "Wong, W., Liu, W. & Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND), 2007; Hyderabad, India."].\n*"L. V. Subramaniam, S. Roy, T. A. Faruquie, S. Negi, A survey of types of text noise and techniques to handle noisy text. In: Third Workshop on Analytics for Noisy Unstructured Text Data (AND), 2009".\n<references />\n\n==See also==\n* [[Text analytics]]\n* [[Information extraction]]\n* [[Computational linguistics]]\n* [[Natural language processing]]\n* [[Named entity recognition]]\n* [[Text mining]]\n* [[Automatic summarization]]\n* [[Statistical classification]]\n* [[Data quality]]\n\n[[Category:Artificial intelligence applications]]\n[[Category:Natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Information retrieval genres]]\n[[Category:Statistical natural language processing]]']
['Full-text search', '1315248', '{{Multiple issues|\n{{refimprove|date=August 2012}}\n{{cleanup|date=September 2009}}\n}}\n\nIn [[text retrieval]], \'\'\'full-text search\'\'\' refers to techniques for searching a single [[computer]]-stored [[document]] or a collection in a [[full text database]]. Full-text search is distinguished from searches based on [[metadata]] or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references).\n\nIn a full-text search, a [[search engine]] examines all of the words in every stored document as it tries to match search criteria (for example, text specified by a user). Full-text-searching techniques became common in online [[bibliographic databases]] in the 1990s.{{Verify source|date=October 2008}} Many websites and application programs (such as [[word processing]] software) provide full-text-search capabilities. Some web search engines, such as [[AltaVista]], employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.<ref>In practice, it may be difficult to determine how a given search engine works. The [[search algorithms]] actually employed by web-search services are seldom fully disclosed out of fear that web entrepreneurs will use [[search engine optimization]] techniques to improve their prominence in retrieval lists.</ref>\n\n==Indexing==\nWhen dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each [[Information retrieval|query]], a strategy called "[[Serial memory processing|serial scanning]]". This is what some tools, such as [[grep]], do when searching.\n\nHowever, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an [[Search index|index]], but more correctly named a [[concordance (publishing)|concordance]]). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents.<ref name="Capabilities of Full Text Search System ">[http://www.lucidimagination.com/full-text-search Capabilities of Full Text Search System] {{webarchive |url=https://web.archive.org/web/20101223192214/http://www.lucidimagination.com/full-text-search |date=December 23, 2010 }}</ref>\n\nThe indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore [[stop words]] (such as "the" and "and") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific [[stemming]] on the words being indexed. For example, the words "drives", "drove", and "driven" will be recorded in the index under the single concept word "drive".\n\n==The precision vs. recall tradeoff==\n[[Image:Full-text-search-results.png|150px|thumb|right|Diagram of a low-precision, low-recall search]]\nRecall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned to all relevant results. Precision is the number of relevant results returned to the total number of results returned.\n\nThe diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only 1 relevant result of 3 possible relevant results was returned, so the recall is a very low ratio of 1/3, or 33%. The precision for the example is a very low 1/4, or 25%, since only 1 of the 4 results returned was relevant.<ref name="isbn1430215941">{{cite book|last=Coles|first=Michael|year=2008|title=Pro Full-Text Search in SQL Server 2008|edition=Version 1|publisher=[[Apress|Apress Publishing Company]]|isbn=1-4302-1594-1}}</ref>\n\nDue to the ambiguities of [[natural language]], full-text-search systems typically includes options like [[stop words]] to increase precision and [[stemming]] to increase recall. [[Controlled vocabulary|Controlled-vocabulary]] searching also helps alleviate low-precision issues by [[tag (metadata)|tagging]] documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision.<ref name="YuwonoLee">{{Cite conference | first = Yuwono | last = B. |author2=Lee, D. L. | title = Search and ranking algorithms for locating resources on the World Wide Web | pages = 164 | publisher = 12th International Conference on Data Engineering (ICDE\'96) | year = 1996}}</ref>\n\n{{See also|Precision and recall}}\n\n==False-positive problem==\n\nFree text searching is likely to retrieve many documents that are not [[relevance|relevant]] to the \'\'intended\'\' search question. Such documents are called \'\'false positives\'\' (see [[Type I and type II errors#Type I error|Type I error]]). The retrieval of irrelevant documents is often caused by the inherent ambiguity of [[natural language]]. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background).\n\nClustering techniques based on [[Bayesian inference|Bayesian]] algorithms can help reduce false positives. For a search term of "bank", clustering can be used to categorize the document/data universe into "financial institution", "place to sit", "place to store" etc. Depending on the occurrences of words relevant to the categories, search terms or a search result can be placed in one or more of the categories. This technique is being extensively deployed in the [[Electronic discovery|e-discovery]] domain.{{clarify|date=January 2012}}\n\n==Performance improvements==\n\nThe deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision.\n\n===Improved querying tools===\n\n*[[Index term|Keyword]]s. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject. Keywords improve recall, particularly if the keyword list includes a search word that is not in the document text.\n* [[Field-restricted search]]. Some search engines enable users to limit free text searches to a particular [[field (computer science)|field]] within a stored [[Record (computer science)|data record]], such as "Title" or "Author."\n* [[Boolean query|Boolean queries]]. Searches that use [[Boolean logic|Boolean]] operators (for example, <tt>"encyclopedia" [[Logical conjunction|AND]] "online" [[Negation|NOT]] "Encarta"<tt>) can dramatically increase the precision of a free text search. The <tt>AND</tt> operator says, in effect, "Do not retrieve any document unless it contains both of these terms." The <tt>NOT</tt> operator says, in effect, "Do not retrieve any document that contains this word." If the retrieval list retrieves too few documents, the <tt>OR</tt> operator can be used to increase [[recall (information retrieval)|recall]]; consider, for example, <tt>"encyclopedia" AND "online" [[Logical disjunction|OR]] "Internet" NOT "Encarta"</tt>. This search will retrieve documents about online encyclopedias that use the term "Internet" instead of "online." This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall.<ref>Studies have repeatedly shown that most users do not understand the negative impacts of boolean queries.[http://eprints.cs.vt.edu/archive/00000112/]</ref>\n* [[Phrase search]]. A phrase search matches only those documents that contain a specified phrase, such as <tt>"Wikipedia, the free encyclopedia."</tt>\n* [[Concept search]]. A search that is based on multi-word concepts, for example [[Compound term processing]]. This type of search is becoming popular in many e-Discovery solutions.\n* [[Concordance search]]. A concordance search produces an alphabetical list of all principal words that occur in a [[Plain text|text]] with their immediate context.\n* [[Proximity search (text)|Proximity search]]. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for <tt>"Wikipedia" WITHIN2 "free"<tt> would retrieve only those documents in which the words <tt>"Wikipedia" and "free"</tt> occur within two words of each other.\n* [[Regular expression]]. A regular expression employs a complex but powerful querying [[syntax]] that can be used to specify retrieval conditions with precision.\n* [[Fuzzy search]] will search for document that match the given terms and some variation around them (using for instance [[edit distance]] to threshold the multiple variation)\n* [[Wildcard character|Wildcard search]]. A search that substitutes one or more characters in a search query for a wildcard character such as an [[asterisk]]. For example, using the asterisk in a search query <tt>"s*n"</tt> will find "sin", "son", "sun", etc. in a text.\n\n===Improved search algorithms===\nThe [[PageRank]] algorithm developed by [[Google]] gives more prominence to documents to which other [[Web page]]s have linked.<ref>{{Cite patent | inventor-last = Page | inventor-first = Lawrence | publication-date = 1/9/1998 | issue-date = 9/4/2001 | title = Method for node ranking in a linked database | country-code = US | description = A method assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database. The rank assigned to a document is calculated from the ranks of documents citing it. In addition, the rank of a document is... | patent-number = 6285999 | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref> See [[Search engine]] for additional examples.\n\n==Software==\n\nThe following is a partial list of available software products whose predominant purpose is to perform full text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full text search may be accomplished.\n\n{{col-float}}\n\n=== Free and open source software ===\n<!--\n\nPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\n\n-->\n* [[BaseX]]\n* [[Clusterpoint|Clusterpoint Database]]\n* [[Elasticsearch]]\n* [[Ht-//Dig|ht://Dig]]\n* [[KinoSearch]]\n* [[Lemur Project|Lemur/Indri]]\n* [[Lucene]]\n* [[mnoGoSearch]]\n* [[Searchdaimon]]\n* [[Sphinx (search engine)|Sphinx]]\n* [[Swish-e]]\n* [[Xapian]]\n* [[Apache Solr]]\n\n{{col-float-break}}\n\n=== Proprietary software ===\n<!--\n\nPlease do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.\n\n-->\n* [[Algolia]]\n* [[Autonomy Corporation]]\n* [[Azure Search]]\n* [[Bar Ilan Responsa Project]]\n* [[Brainware]]\n* [[BRS/Search]] \n* [[Concept Searching Limited]]\n* [[Dieselpoint]]\n* [[dtSearch]]\n* [[Endeca]]\n* [[Exalead]]\n* [[Funnelback]]\n* [[Fast Search & Transfer]]\n* [[Inktomi (company)|Inktomi]]\n* [[Dan Wagner#Locayta|Locayta]](rebranded to [[ATTRAQT]] in 2014)\n* [[Lucid Imagination]]\n* [[MarkLogic]]\n* [[SAP HANA]]<ref>http://www.martechadvisor.com/news/databases-big-data/sap-adds-hanabased-software-packages-to-iot-portfolio/</ref>\n* [[Swiftype]]\n* [[Thunderstone Software LLC.]]\n* [[Vivísimo]]\n{{col-float-end}}\n\n==Notes==\n{{Reflist}}\n\n==See also==\n*[[Pattern matching]] and [[string matching]]\n*[[Compound term processing]]\n*[[Enterprise search]]\n*[[Information extraction]]\n*[[Information retrieval]]\n*[[Faceted search]]\n*[[List of enterprise search vendors]]\n*[[WebCrawler]], first FTS engine\n*[[Search engine indexing]] - how search engines generate indices to support full text searching\n\n{{DEFAULTSORT:Full Text Search}}\n[[Category:Text editor features]]\n[[Category:Information retrieval genres]]']
['Cyril Cleverdon', '20632884', "{{Infobox scientist\n| name = Cyril Cleverdon\n| image =\n| caption = \n| birth_date = {{birth date|1914|9|9|df=y}}\n| birth_place = [[Bristol]], [[United Kingdom|UK]]\n| death_date = {{death date and age|1997|12|4|1914|9|9|df=y}}\n| death_place = [[Cranfield]], [[United Kingdom|UK]]\n| residence = United Kingdom\n| nationality = British\n| field = Computer Science\n| work_institution = [[Cranfield Institute of Technology]]\n| known_for  = work on the evaluation of information retrieval systems\n| prizes = Professional Award of the Special Libraries Association (1962), Award of Merit of the American Society for Information Science (1971), The [[Gerard Salton Award]] of the Special Interest Group on Information Retrieval of the Association for Computing Machinery (1991)\n}}\n'''Cyril Cleverdon''' (9 September 1914 – 4 December 1997) was a [[United Kingdom|British]] librarian and computer scientist who is best known for his work on the evaluation of [[information retrieval]] systems.\n\nCyril Cleverdon was born in [[Bristol]], [[England]]. He worked at the Bristol Libraries from 1932 to 1938, and from 1938 to 1946 he was the librarian of the Engine Division of the Bristol Aeroplane Co. Ltd. In 1946 he was appointed librarian of the College of Aeronautics at Cranfield (later the [[Cranfield Institute of Technology]] and [[Cranfield University]]), where he served until his retirement in 1979, the last two years as professor of Information Transfer Studies.\n\nWith the help of NSF funding, Cleverdon started a series of projects in 1957 that lasted for about 10 years in which he and his colleagues set the stage for information retrieval research. In the Cranfield project, retrieval experiments were conducted on test databases in a controlled, laboratory-like setting. The aim of the research was to improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods. The components of the experiments were:\n# a collection of documents,\n# a set of user requests or queries, and \n# a set of relevance judgments—that is, a set of documents judged to be [[Relevance (information retrieval)|relevant]] to each query. \nTogether, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: [[Precision (information retrieval)|precision]] and [[Recall (information retrieval)|recall]]. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful [[Text Retrieval Conference]] series that began in 1992.\n\nNot only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the [[automatic indexing]] as done in today's [[search engine]]s. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:\n\n''This conclusion is so controversial and so unexpected that it is bound to throw considerable doubt on the methods which have been used (...) A complete recheck has failed to reveal any discrepancies (...) there is no other course except to attempt to explain the results which seem to offend against every canon on which we were trained as librarians.'' \n\nCyril Cleverdon also ran, for many years, the Cranfield conferences, which provided a major international forum for discussion of ideas and research in information retrieval. This function was taken over by the [[Special Interest Group on Information Retrieval|SIGIR]] conferences in the 1970s.\n\n==References==\n* {{cite journal|author=Cyril Cleverdon|title=Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems|publisher=The College of Aeronautics, Cranfield|year=1960|url=http://www.sigir.org/museum/pdfs/Report_on_the_Testing_and_Analysis_of_an_Investigation_Into_the_Comparative_Efficiency_of_Indexing_Systems/pdfs/frontmatter.pdf}}\n* Cyril Cleverdon and Michael Keen, Factors Determining the Performance of Indexing Systems, Volume 2, ''The College of Aeronautics, Cranfield'', 1966\n* Stephen Robertson, In Memoriam Cyril W. Cleverdon, ''Journal of the American Society for Information Science 49''(10):866, 1998\n\n{{DEFAULTSORT:Cleverdon, Cyril}}\n[[Category:1914 births]]\n[[Category:1997 deaths]]\n[[Category:British computer scientists]]\n[[Category:English librarians]]\n[[Category:People associated with Cranfield University]]\n[[Category:People from Bristol]]\n[[Category:Information retrieval researchers]]"]
['Norbert Fuhr', '37022703', '\'\'\'Norbert Fuhr\'\'\' (born 1956) is a professor of computer science\nand the leader of the Duisburg Information Engineering Group based at\nthe [[University of Duisburg-Essen]], Germany.\n\n==Education==\nHis first  degree is in  technical computer science, which he got  from the Electrical\nEngineering Department of  the [[Technical University of Darmstadt]] in 1980, and in 1986 he finished his PhD\n(Dr.-Ing) in the Computer Science Department of the same university on "Probabilistic\nIndexing and Retrieval".<ref name="Fuhr1986">{{citation\n | author=Fuhr, Norbert\n | publisher=Fachinformationszentrum Karlsruhe\n | title=Probabilistisches Indexing und Retrieval\n | year=1986\n}}</ref>\n\n==Profession==\nHe held a PostDoc position in Darmstadt until\n1991, when he was appointed Associate Professor in the  Computer\nScience Department  of  the [[Technical University of Dortmund]]. Since 2002, he is a full professor at the\n[[University of Duisburg-Essen]].\n\n==Honors and awards==\nFuhr\'s dissertation was awarded the  "Gerhard Pietsch Award" of the German Society\nof Documentation in 1987. In 2012, he received the  \n[[Gerard Salton Award]].<ref name="Fuhr2012">{{citation\n | author=Fuhr, Norbert\n | journal=SIGIR \'12 Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval\n | title=Salton award lecture: information retrieval as engineering science\n | pages=1–2\n | year=2012\n | doi=10.1145/2348283.2348285\n}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.is.inf.uni-due.de/staff/fuhr.html Norbert Fuhr - University of Duisburg-Essen]\n\n{{Authority control}}\n{{DEFAULTSORT:Fuhr, Norbert}}\n[[Category:German computer scientists]]\n[[Category:1956 births]]\n[[Category:Living people]]\n[[Category:University of Duisburg-Essen faculty]]\n[[Category:Technical University of Dortmund faculty]]\n[[Category:Technische Universität Darmstadt alumni]]\n[[Category:Information retrieval researchers]]\n\n\n{{Compu-bio-stub}}']
['Colon classification', '6888', '\'\'\'Colon classification\'\'\' (\'\'\'CC\'\'\') is a system of [[library classification]] developed by [[S. R. Ranganathan]]. It was the first ever [[Faceted classification|faceted]] (or analytico-synthetic) [[Taxonomic classification|classification]]. The first edition was published in 1933. Since then six more editions have been published. It is especially used in [[library|libraries]] in [[India]].\n\nIts name "colon classification" comes from the use of [[Colon (punctuation)|colons]] to separate facets in class numbers. However, many other classification schemes, some of which are completely unrelated, also use colons and other [[punctuation]] in various functions. They should not be confused with colon classification.\n\nIn CC, facets describe "personality" (the most specific subject), matter, energy, space, and time (PMEST).  These facets are generally associated with every item in a library, and so form a reasonably universal sorting system.<ref>GOPINATH (M A). Colon classification: Its theory and practice.\nLibrary Herald\n.\n26, 1\n-\n2; 1987; 1\n-\n3.</ref>\n\nAs an example, the subject "research in the cure of tuberculosis of lungs by x-ray conducted in India in 1950" would be categorized as:\n\n:Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India\'1950\n\nThis is summarized in a specific call number:\n\n:L,45;421:6;253:f.44\'N5\n\n== Organization ==\n\nThe colon classification uses 42 main classes that are combined with other letters, numbers and marks in a manner resembling the [[Library of Congress Classification]] to sort a publication.\n\n=== Facets ===\n\nCC uses five primary categories, or facets, to further specify the sorting of a publication. Collectively, they are called \'\'PMEST\'\':\n\n:<nowiki>,</nowiki> Personality, the most specific or focal subject.\n:<nowiki>;</nowiki> Matter or property, the substance, properties or materials of the subject.\n:<nowiki>:</nowiki> Energy, including the processes, operations and activities.\n:<nowiki>.</nowiki> Space, which relates to the geographic location of the subject.\n:<nowiki>\'</nowiki> Time, which refers to the dates or seasons of the subject.\n\n=== Classes ===\n\nThe following are the main classes of CC, with some subclasses, the main method used to sort the subclass using the PMEST scheme and examples showing application of PMEST.\n\n:z Generalia\n:1 Universe of Knowledge\n:2 [[Library Science]]\n:3 Book science\n:4 [[Journalism]]\n:B [[Mathematics]]\n::B2 [[Algebra]]\n:C [[Physics]]\n:D [[Engineering]]\n:E [[Chemistry]]\n:F [[Technology]]\n:G [[Biology]]\n:H [[Geology]]\n::HX [[Mining]]\n:I [[Botany]]\n:J [[Agriculture]]\n::J1 [[Horticulture]]\n::J2 Feed\n::J3 Food\n::J4 Stimulant\n::J5 Oil\n::J6 Drug\n::J7 Fabric\n::J8 Dye\n:K [[Zoology]] \n::KZ Animal Husbandry \n:L Medicine\n::LZ3 [[Pharmacology]]\n::LZ5 [[Pharmacopoeia]]\n:M [[Useful arts]]\n::M7 Textiles \'\'[material]:[work]\'\'\n:Δ Spiritual experience and [[mysticism]] \'\'[religion],[entity]:[problem]\'\'\n:N [[Fine arts]]\n::ND Sculpture\n::NN Engraving\n::NQ Painting\n::NR Music\n:O Literature\n:P Linguistics\n:Q [[Religion]]\n:R [[Philosophy]]\n:S [[Psychology]]\n:T [[Education]]\n:U [[Geography]]\n:V [[History]]\n:W [[Political science]]\n:X [[Economics]]\n:Y [[Sociology]]\n:YZ [[Social Work]]\n:Z [[Law]]\n\n== Example ==\n\nA common example of the colon classification is:\n\n* "Research in the cure of the tuberculosis of lungs by x-ray conducted in India in 1950s":\n* Main classification is Medicine\n** (Medicine)\n* Within Medicine, the Lungs are the main concern\n** (Medicine,Lungs)\n* The property of the Lungs is that they are afflicted with Tuberculosis\n** (Medicine,Lungs;Tuberculosis)\n* The Tuberculosis is being performed (:) on, that is the intent is to cure (Treatment)\n** (Medicine,Lungs;Tuberculosis:Treatment)\n* The matter that we are treating the Tuberculosis with are X-Rays\n** (Medicine,Lungs;Tuberculosis:Treatment;X-ray)\n* And this discussion of treatment is regarding the Research phase\n** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research)\n* This Research is performed within a geographical space (.) namely India\n** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India)\n* During the time (\') of 1950\n** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India\'1950)\n* And translating into the codes listed for each subject and facet the classification becomes\n** L,45;421:6;253:f.44\'N5\n\n==See also==\n*[[Bliss bibliographic classification]]\n*[[Subject (documents)]]\n*[[Universal Decimal Classification]]\n\n== References ==\n{{Reflist|2}}\n* [http://www.essessreference.com/servlet/esGetBiblio?bno=000374 \'\'Colon Classification\'\' (6th Edition)] by Dr. S.R. Ranganathan, published by Ess Ess Publications, Delhi, India\n* Chan, Lois Mai. \'\'Cataloging and Classification: An Introduction\'\'. 2nd ed. New York: McGraw-Hill, c1994. ISBN 0-07-010506-5.\n\n==External links==\n* [http://www.iskoi.org/doc/colon.htm More Detail about the Colon Classification at ISKO Italia]\n\n{{Library classification systems}}\n\n[[Category:Classification systems]]\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]']
['Nippon Decimal Classification', '2471086', "The '''Nippon Decimal Classification''' ('''NDC''', also called the '''Nippon Decimal System''') is a system of [[library classification]] developed for mainly Japanese language books maintained by the Japan Library Association since 1956. It is based on the [[Dewey Decimal Classification|Dewey Decimal System]]. The system is based upon using each successive digit to divide into nine divisions with the digit zero used for those not belonging to any of the divisions.\n\n== Main classes ==\nThe system is made up of ten categories:\n\n* 000 General\n* 100 [[Philosophy]]\n* 200 [[History]]\n* 300 [[Social sciences]]\n* 400 [[Natural sciences]]\n* 500 [[Technology]] and [[engineering]]\n* 600 [[Industry]] and [[commerce]]\n* 700 [[Arts]]\n* 800 [[Language]]\n* 900 [[Literature]]\n\n== Description of the classes ==\n\n*000 General\n**010 [[Libraries]], [[Library and information science|Library & information science]]\n**020 [[Books]], [[Bibliography]]\n**030 [[Encyclopaedia]]s\n**040 General collected [[essays]]\n**050 General [[Periodical literature|serial publications]]\n**060 [[Organizations]]\n**070 [[Journalism]], [[Newspapers]]\n**080 General collections\n**090 [[Rare books]], Local collections, [[Special collections]]\n*100 Philosophy\n**110 Special treatises on philosophy\n**120 [[Oriental philosophy]]\n**130 [[Western philosophy]]\n**140 [[Psychology]]\n**150 [[Ethics]] & morals\n**160 [[Religion]]\n**170 [[Shintoism]]\n**180 [[Buddhism]]\n**190 [[Christianity]]\n*200 History\n**210 [[History of Japan]]\n**220 [[History of Asia]] and the Orient\n**230 [[History of Europe]] and the West\n**240 [[History of Africa]]\n**250 [[History of North America]]\n**260 [[History of South America]]\n**270 [[History of Oceania]] & [[Polar region]]s\n**280 [[Biography]]\n**290 [[Geography]], [[Topography]], [[Travel]]\n*300 Social Sciences\n**310 [[Politics]]\n**320 [[Law]]\n**330 [[Economics]]\n**340 [[Finance]]\n**350 [[Statistics]]\n**360 [[Sociology]]\n**370 [[Education]]\n**380 [[Customs]], [[Folklore]], [[Ethnology]]\n**390 National defence, [[Military science]]\n*400 Natural Sciences\n**410 [[Mathematics]]\n**420 [[Physics]]\n**430 [[Chemistry]]\n**440 [[Astronomy]], [[Space science]]\n**450 [[Earth science]]\n**460 [[Biology]]\n**470 [[Botany]]\n**480 [[Zoology]]\n**490 [[Medicine]], [[Pharmacology]]\n*500 Technology & Engineering\n**510 [[Construction]], [[Civil engineering]]\n**520 [[Architecture]]\n**530 [[Mechanical engineering]], [[Nuclear engineering]]\n**540 [[Electrical engineering|Electrical]] & [[Electronic engineering]]\n**550 Maritime & [[Naval engineer]]ing\n**560 Metal & [[Mining engineering]]\n**570 [[Chemical technology]]\n**580 [[Manufacturing]]\n**590 [[Domestic science|Domestic arts and sciences]]\n*600 Industry and Commerce\n**610 [[Agriculture]]\n**620 [[Horticulture]]\n**630 [[Sericulture|Silk industry]]\n**640 [[Animal husbandry]]\n**650 [[Forestry]]\n**660 [[Fishing]]\n**670 [[Commerce]]\n**680 [[Transportation]] & [[Traffic]]\n**690 [[Communications]]\n*700 Arts\n**710 [[Plastic arts]] (sculpture)\n**720 [[Painting]] & [[Calligraphy]]\n**730 [[Engraving]]\n**740 [[Photography]] & [[Printing]]\n**750 [[Craft]]\n**760 [[Music]] & [[Dance]]\n**770 [[Theatre]], [[Motion Pictures]]\n**780 [[Sports]], [[Physical Education]]\n**790 [[Recreation]], Amusements\n*800 Language\n**810 [[Japanese language|Japanese]]\n**820 [[Chinese language|Chinese]], other [[oriental languages]]\n**830 [[English language|English]]\n**840 [[German language|German]]\n**850 [[French language|French]]\n**860 [[Spanish language|Spanish]]\n**870 [[Italian language|Italian]]\n**880 [[Russian language|Russian]]\n**890 Other languages\n*900 Literature\n**910 [[Japanese literature]]\n**920 [[Chinese literature]], Other [[Oriental literature]]\n**930 [[English literature|English]] & [[American literature]]\n**940 [[German literature]]\n**950 [[French literature]]\n**960 [[Spanish literature]]\n**970 [[Italian literature]]\n**980 [[Russian literature|Russian]] & [[Soviet literature]]\n**990 Other language literature\n\n== External links ==\n*[http://www.jla.or.jp/index-e.html Japan Library Association]\n*[http://www.asahi-net.or.jp/~ax2s-kmtn/ref/ndc/e_ndc.html CyberLibrarian]\n\n{{Library classification systems}}\n\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]\n[[Category:Classification systems]]"]
['Category:Classification systems', '3615452', "'''Classification systems''' are [[system]]s with a distribution of classes created according to common relations or affinities.\n{{Commons cat|Classification systems}}\nSee also: \n* [[Controlled vocabulary]]\n* [[Scientific classification (disambiguation)]]\n* [[Taxonomy (biology)|Taxonomy]]\n\n[[Category:Classification|systems]]\n[[Category:Conceptual systems]]\n[[Category:Formal sciences]]\n[[Category:Knowledge representation]]\n[[Category:Information science]]\n[[Category:Information systems]]"]
['Frame language', '485226', '{{Duplication|dupe=Frame (artificial intelligence)}}\n\nA \'\'\'frame language\'\'\' is a technology used for [[knowledge representation]] in [[artificial intelligence]]. Frames are stored as [[Ontology (information science)|ontologies]] of [[Set theory|sets]] and subsets of the [[Frame_(artificial_intelligence)|frame concepts]]. They are similar to class hierarchies in [[object-oriented languages]] although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on [[Encapsulation (object-oriented programming)|encapsulation]] and [[information hiding]]. Frames originated in AI research and objects primarily in [[software engineering]]. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly.\n\n==Description==\nEarly work on Frames was inspired by psychological research going back to the 1930s that indicated people use stored stereotypical knowledge to interpret and act in new cognitive situations.<ref>{{cite book|last=Bartlett|first=F.C.|title=Remembering: A Study in Experimental and Social Psychology|year=1932|publisher=Cambridge University Press|location=Cambridge, England}}</ref>  The term Frame was first used by [[Marvin Minsky]] as a paradigm to understand visual reasoning and natural language processing.<ref>{{cite book|last=Minsky|first=Marvin|title=The Psychology of Computer Vision|year=1975|publisher=McGraw Hill|location=New York|pages=211–277|editor=Pat Winston|chapter=A Framework for Representing Knowledge}}</ref> In these and many other types of problems the potential solution space for even the smallest problem is huge. For example, extracting the phonemes from a raw audio stream or detecting the edges of an object. Things which seem trivial to humans are actually quite complex. In fact, how difficult they really were was probably not fully understood until AI researchers began to investigate the complexity of getting computers to solve them.\n\nThe initial notion of Frames or Scripts as they were also called is that they would establish the context for a problem and in so doing automatically reduce the possible search space significantly. The idea was also adopted by Schank and Abelson who used it to illustrate how an AI system could process common human interactions such as ordering a meal at a restaurant.<ref>{{cite book|last=Schank|first=Roger|title=Scripts, Plans, Goals, and Understanding|year=1977|publisher=Lawrence Erlbaum|location=Hillsdale, New Jersey|author2=R. P. Abelson}}</ref>  These interactions were standardized as Frames with slots that stored relevant information about each Frame. Slots are analogous to object properties in object-oriented modeling and to relations in entity-relation models. Slots often had default values but also required further refinement as part of the execution of each instance of the scenario. I.e., the execution of a task such as ordering at a restaurant was controlled by starting with a basic instance of the Frame and then instantiating and refining various values as appropriate. Essentially the abstract Frame represented an object class and the frame instances an object instance. In this early work the emphasis was primarily on the static data descriptions of the Frame. Various mechanisms were developed to define the range of a slot, default values, etc. However, even in these early systems there were procedural capabilities. One common technique was to use "triggers" (similar to the database concept of triggers) attached to slots. A trigger was simply procedural code that was attached to a slot. The trigger could fire either before and/or after a slot value was accessed or modified.\n\nAs with object classes, Frames were organized in [[Subsumption relation|subsumption]] hierarchies. For example, a basic frame might be ordering at a restaurant. An instance of that would be Joe goes to McDonalds. A specialization (essentially a [[Subclass (computer science)|subclass]]) of the restaurant frame would be a frame for ordering at a fancy restaurant. The fancy restaurant frame would inherit all the default values from the restaurant frame but also would either add more slots or change one or more of the default values (e.g., expected price range) for the specialized frame.<ref>{{cite book|last=Feigenbaum|first=Edward|title=The Handbook of Artificial Intelligence, Volume III|publisher=Addison-Wesley|isbn=0201118114|pages=216–222|url=https://archive.org/stream/handbookofartific01barr#page/156/mode/2up|author2=Avron Barr|date=September 1, 1986}}</ref><ref>{{cite journal|last=Bobrow|first=D.G.|author2=Terry Winograd|title=An Overview of KRL: A Knowledge Representation Language|journal=Cognitive Science|year=1977|volume=1|pages=3–46|doi=10.1207/s15516709cog0101_2}}</ref>\n\nMuch of the early Frame language research (e.g. Schank and Abelson) had been driven by findings from experimental psychology and attempts to design knowledge representation tools that corresponded to the patterns humans were thought to use to function in daily tasks. These researchers were less interested in mathematical formality since they believed such formalisms were not necessarily good models for the way the average human conceptualizes the world. The way humans use language for example is often far from truly logical.\n\nSimilarly, in linguistics, [[Charles J. Fillmore]] in the mid-1970s started working on his theory of [[Frame semantics (linguistics)|frame semantics]], which later would lead to computational resources like [[FrameNet]].<ref>{{cite news|last=Lakoff|first=George|title=Charles Fillmore, Discoverer of Frame Semantics, Dies in SF at 84: He Figured Out How Framing Works|url=http://www.huffingtonpost.com/george-lakoff/charles-fillmore-discover_b_4807590.html|accessdate=7 March 2014|newspaper=The Huffington Post|date=18 February 2014}}</ref> Frame semantics was motivated by reflections on human language and human cognition.\n\nResearchers such as [[Ron Brachman]] on the other hand wanted to give AI researchers the mathematical formalism and computational power that were associated with Logic. Their aim was to map the Frame classes, slots, constraints, and rules in a Frame language to set theory and logic.  One of the benefits of this approach is that the validation and even creation of the models could be automated using theorem provers and other automated reasoning capabilities. The drawback was that it could be more difficult to initially specify the model in a language with a formal semantics.\n\nThis evolution also illustrates a classic divide in AI research known as the "[[neats vs. scruffies]]". The "neats" were researchers who placed the most value on mathematical precision and formalism which could be achieved via [[First Order Logic]] and [[Set Theory]]. The "scruffies" were more interested in modeling knowledge in representations that were intuitive and psychologically meaningful to humans.<ref>{{cite book|last=Crevier|first=Daniel|title=AI: The Tumultuous Search for Artificial Intelligence|year=1993|publisher=Basic Books|location=New York|isbn=0-465-02997-3|page=168}}</ref>\n\nThe most notable of the more formal approaches was the [[KL-ONE]] language.<ref>{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}</ref> KL-ONE later went on to spawn several subsequent Frame languages. The formal semantics of languages such as KL-ONE gave these frame languages a new type of automated reasoning capability known as the [[Deductive classifier|classifier]]. The classifier is an engine that analyzes the various declarations in the frame language: the definition of sets, subsets, relations, etc. The classifier can then automatically deduce various additional relations and can detect when some parts of a model are inconsistent with each other. In this way many of the tasks that would normally be executed by forward or backward chaining in an inference engine can instead be performed by the classifier.<ref>{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=87683&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}</ref>\n\nThis technology is especially valuable in dealing with the Internet. It is an interesting result that the formalism of languages such as KL-ONE can be most useful dealing with the highly informal and unstructured data found on the Internet. On the Internet it is simply not feasible to require all systems to standardize on one data model. It is inevitable that terminology will be used in multiple inconsistent forms. The automatic classification capability of the classifier engine provides AI developers with a powerful toolbox to help bring order and consistency to a very inconsistent collection of data (i.e., the Internet). The vision for an enhanced Internet, where pages are ordered not just by text keywords but by classification of concepts is known as the [[Semantic Web]]. Classification technology originally developed for Frame languages is a key enabler of the Semantic Web.<ref>{{cite journal|last=Berners-Lee |first=Tim |author2=James Hendler |author3=Ora Lassila |title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities |journal=Scientific American |date=May 17, 2001 |url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html |doi=10.1038/scientificamerican0501-34 |volume=284 |pages=34–43 |deadurl=yes |archiveurl=https://web.archive.org/web/20130424071228/http://www.cs.umd.edu/%7Egolbeck/LBSC690/SemanticWeb.html |archivedate=2013-04-24 |df= }}</ref><ref>{{cite web|last=Horridge|first=Mathew|title=Protégé OWL Tutorial A step-by-step guide to modelling in OWL using the popular Protégé OWL tools.|url=http://130.88.198.11/tutorials/protegeowltutorial/|work=Manchester University|publisher=Manchester University|accessdate=9 December 2013}}</ref> The "neats vs. scruffies" divide also emerged in Semantic Web research, culminating in the creation of the [[Linking Open Data]] community—their focus was on exposing data on the Web rather than modeling.\n\n==Example==\nA simple example of concepts modeled in a frame language is the [[FOAF (ontology)|Friend of A Friend (FOAF) ontology]] defined as part of the Semantic Web as a foundation for social networking and calendar systems. The primary frame in this simple example is a \'\'Person\'\'. Example slots are the person\'s \'\'email\'\', \'\'home page, phone,\'\' etc. The interests of each person can be represented by additional frames describing the space of business and entertainment domains. The slot \'\'knows\'\' links each person with other persons. Default values for a person\'s interests can be inferred by the web of people they are friends of.<ref>{{cite web|title=FOAF|url=http://semanticweb.org/wiki/FOAF|website=http://semanticweb.org|accessdate=7 June 2014}}</ref>\n\n==Implementations==\nThe earliest Frame based languages were custom developed for specific research projects and were not packaged as tools to be re-used by other researchers. Just as with [[expert system]] [[inference engine]]s, researchers soon realized the benefits of extracting part of the core infrastructure and developing general purpose frame languages that were not coupled to specific applications. One of the first general purpose frame languages was KRL.<ref>{{cite journal|last=Bobrow|first=D.G.|author2=Terry Winograd|title=An Overview of KRL: A Knowledge Representation Language|journal=Cognitive Science|year=1977|volume=1|pages=3–46|doi=10.1207/s15516709cog0101_2}}</ref> One of the most influential early Frame languages was [[KL-ONE]]<ref>{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}</ref> KL-ONE spawned several subsequent Frame languages. One of the most widely used successors to KL-ONE was the [[LOOM (ontology)|Loom language]] developed by Robert MacGregor at the [[Information Sciences Institute]].<ref>{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=87683&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}</ref>\n\nIn the 1980s Artificial Intelligence generated a great deal of interest in the business world fueled by expert systems. This led to the development of many commercial products for the development of knowledge-based systems. These early products were usually developed in Lisp and integrated constructs such as IF-THEN rules for logical reasoning with Frame hierarchies for representing data. One of the most well known of these early Lisp knowledge-base tools was the [[Knowledge Engineering Environment]] (KEE) from [[IntelliCorp (software)|Intellicorp]]. KEE provided a full Frame language with multiple inheritance, slots, triggers, default values, and a rule engine that supported backward and forward chaining. As with most early commercial versions of AI software KEE was originally deployed in [[Lisp (programming language)|Lisp]] on [[Lisp machine]] platforms but was eventually ported to PCs and Unix workstations.<ref>{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}</ref>\n\nThe research agenda of the [[Semantic Web]] spawned a renewed interest in automatic classification and frame languages.  An example is the [[Web Ontology Language]] (OWL) standard for describing information on the Internet. OWL is a standard to provide a semantic layer on top of the Internet. The goal is that rather than organizing the web using keywords as most applications (e.g. Google) do today the web can be organized by concepts organized in an ontology.\n\nThe name of the OWL language itself provides a good example of the value of a Semantic Web. If one were to search for "OWL" using the Internet today most of the pages retrieved would be on the bird [[Owl]] rather than the standard [[Web Ontology Language|OWL]]. With a Semantic Web it would be possible to specify the concept "Web Ontology Language" and the user would not need to worry about the various possible acronyms or synonyms as part of the search. Likewise the user would not need to worry about homonyms crowding the search results with irrelevant data such as information about birds of prey as in this simple example.\n\nIn addition to OWL various standards and technologies that are relevant to the Semantic Web and were influenced by Frame languages include [[Ontology Inference Layer|OIL]] and [[DARPA Agent Markup Language|DAML]].  The [[Protégé (software)|Protege]] Open Source software tool from Stanford University provides an ontology editing capability that is built on OWL and has the full capabilities of a classifier.<ref>{{cite web|last=Horridge|first=Mathew|title=Protégé OWL Tutorial A step-by-step guide to modelling in OWL using the popular Protégé OWL tools.|url=http://130.88.198.11/tutorials/protegeowltutorial/|work=Manchester University|publisher=Manchester University|accessdate=9 December 2013}}</ref>\n\n==Comparison of frames and objects==\nFrame languages have a significant overlap with [[object-oriented]] languages. The terminologies and goals of the two communities were different but as they moved from the academic world and labs to the commercial world developers tended to not care about philosophical issues and focused primarily on specific capabilities, taking the best from either camp regardless of where the idea began. What both paradigms have in common is a desire to reduce the distance between concepts in the real world and their implementation in software. As such both paradigms arrived at the idea of representing the primary software objects in taxonomies starting with very general types and progressing to more specific types.\n\nThe following table illustrates the correlation between standard terminology from the object-oriented and frame language communities:\n\n{| class="wikitable"\n|-\n! Frame Terminology !! OO Terminology\n|-\n| Frame || Object Class\n|-\n| Slot || Object property or attribute\n|-\n| Trigger || Accessor and Mutator methods\n|-\n| Method (e.g. Loom, KEE) || Method\n|}\n\nThe primary difference between the two paradigms was in the degree that encapsulation was considered a major requirement. For the object-oriented paradigm encapsulation was one of the if not the most critical requirement. The desire to reduce the potential interactions between software components and hence manage large complex systems was a key driver of object-oriented technology. For the frame language camp this requirement was less critical than the desire to provide a vast array of possible tools to represent rules, constraints, and programming logic. In the object-oriented world everything is controlled by methods and the visibility of methods. So for example, accessing the data value of an object property must be done via an accessor method. This method controls things such as validating the data type and constraints on the value being retrieved or set on the property. In Frame languages these same types of constraints could be handled in multiple ways. Triggers could be defined to fire before or after a value was set or retrieved. Rules could be defined that managed the same types of constraints. The slots themselves could be augmented with additional information (called "facets" in some languages) again with the same type of constraint information.\n\nThe other main differeniator between frame and OO languages was multiple inheritance (allowing a frame or class to have two or more superclasses). For frame languages multiple inheritance was a requirement.  This follows from the desire to model the world the way humans do, human conceptualizations of the world seldom fall into rigidly defined non-overlapping taxonomies. For many OO languages, especially in the later years of OO, single inheritance was either strongly desired or required. Multiple inheritance was seen as a possible step in the analysis phase to model a domain but something that should be eliminated in the design and implementation phases in the name of maintaining encapsulation and modularity.<ref>{{cite web|title=The Unified Modeling Language|url=http://www.essentialstrategies.com/publications/modeling/uml.htm|work=essentialstrategies.com|publisher=Essential Strategies Inc.|accessdate=10 December 2013|year=1999|quote=In your author’s experience, nearly all examples that appear to require multiple inheritance or multiple type hierarchies can be solved by attacking the model from a different direction.}}</ref>\n\nAlthough the early frame languages such as KRL did not include message passing, driven by the demands of developers, most of the later frame languages (e.g. Loom, KEE) included the ability to define messages on Frames.<ref>{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}</ref>\n\nOn the object-oriented side, standards have also emerged that provide essentially the equivalent functionality that frame languages provided, albeit in a different format and all standardized on object libraries. For example, the [[Object Management Group]] has standardized specifications for capabilities such as associating test data and constraints with objects (analogous to common uses for facets in Frames and to constraints in Frame languages such as Loom) and for integrating rule engines.<ref>{{cite web|last=Macgregor|first=Robert|title=Retrospective on Loom|url=http://www.isi.edu/isd/LOOM/papers/macgregor/Loom_Retrospective.html|work=isi.edu|publisher=Information Sciences Institute|accessdate=10 December 2013|date=August 13, 1999}}</ref><ref>{{cite web|title=OMG Formal Specifications|url=http://www.omg.org/spec/|work=omg.org|publisher=Object Management Group|accessdate=10 December 2013}}</ref>\n\n==See also==\n*[[Description logic]]\n* [[Deductive classifier]]\n*[[First-order logic]]\n*[[Knowledge base]]\n*[[Knowledge-based system]]\n*[[Ontology language]]\n*[[Semantic Networks]]\n\n==References==\n{{reflist}}\n\n==Additional References==\n* Marvin Minsky, [http://web.media.mit.edu/~minsky/papers/Frames/frames.html A Framework for Representing Knowledge], MIT-AI Laboratory Memo 306, June, 1974.\n* Daniel G. Bobrow, Terry Winograd, [ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/76/581/CS-TR-76-581.pdf An Overview of KRL, A Knowledge Representation Language],  Stanford Artificial Intelligence Laboratory Memo AIM 293, 1976.\n* R. Bruce Roberts and Ira P. Goldstein, [ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-408.pdf The FRL Primer], 1977\n* R. Bruce Roberts and Ira P. Goldstein, [ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-409.pdf The FRL Manual], 1977\n* {{cite journal | last1 = Brachman | first1 = R. | last2 = Schmolze | first2 = J. | year = 1985 | title = An overview of the KL-ONE Knowledge Representation System | url = | journal = Cognitive science | volume = 9 | issue = | pages = 171–216 | doi=10.1016/s0364-0213(85)80014-8}}\n* {{cite journal | last1 = Fikes | first1 = R. E. | last2 = Kehler | first2 = T. | year = 1985 | title = The role of frame-based representation in knowledge representation and reasoning | url = | journal = Communications of the ACM | volume = 28 | issue = 9| pages = 904–920 | doi=10.1145/4284.4285}}\n* Peter Clark & Bruce Porter:  KM - The Knowledge Machine 2.0: Users Manual,  http://www.cs.utexas.edu/users/mfkb/RKF/km.html.\n* Peter D. Karp, [http://www.ai.sri.com/pub_list/236 The Design Space of Frame Knowledge Representation Systems], Technical Note 520. [[Artificial Intelligence Center]], [[SRI International]], 1992\n\n==External links==\n*[http://www.cs.umbc.edu/771/papers/nebel.html Frame-Based Systems]\n*[http://www.ai.sri.com/~gfp/spec/paper/paper.html The Generic Frame Protocol]\n*[http://protege.stanford.edu/ The Protégé Ontology Editor]\n*[http://www.csee.umbc.edu/courses/771/current/presentations/frames.pdf Intro Presentation to Frame Languages]\n\n[[Category:Artificial intelligence]]\n[[Category:Knowledge engineering]]\n[[Category:Knowledge representation]]']
['Korean decimal classification', '6978390', "{{Unreferenced stub|date=December 2009}}\nThe '''Korean decimal classification''' ('''KDC''') is a system of [[library classification]] used in [[South Korea]]. The main classes are the same as in the Dewey Decimal Classification but these are in a different order: Natural sciences 400; Technology and engineering 500; Arts 600; Language 700.\n\n==Main classes==\n* 000 General\n* 100 Philosophy\n* 200 Religion\n* 300 Social sciences\n* 400 Natural sciences\n* 500 Technology and engineering\n* 600 Arts\n* 700 Language\n* 800 Literature\n* 900 History\n\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]\n[[Category:Classification systems]]\n[[Category:Libraries in North Korea]]\n[[Category:Libraries in South Korea]]\n\n{{Library classification systems}}\n{{Library-stub}}"]
['MultiNet', '2473220', "'''Multilayered extended semantic networks''' ('''MultiNets''') are both a [[knowledge representation]] paradigm and a language for meaning representation of [[natural language]] expressions that has been developed by Prof. Dr. Hermann Helbig on the basis of earlier [[Semantic network|Semantic Networks]].\n\nMultiNet is claimed to be one of the most comprehensive and thoroughly described knowledge representation systems. It specifies conceptual structures by means of about 140 predefined relations and functions, which are systematically characterized and underpinned by a formal [[axiomatic]] apparatus. Apart from their relational connections, the concepts are embedded in a multidimensional space of layered attributes and their values. Another characteristic of MultiNet discerning it from simple semantic networks is the possibility to encapsulate whole partial networks and represent the resulting conceptual capsule as a node of higher order, which itself can be an argument of relations and functions. MultiNet has been used in practical [[Natural language processing|NLP]] applications such as natural language interfaces to the Internet or [[question answering]] systems over large semantically annotated [[Corpus linguistics|corpora]] with millions of sentences. MultiNet is also a cornerstone of the commercially available search engine SEMPRIA Search, where it is used for the description of the computational lexicon and the background knowledge, for the syntactic-semantic analysis, for logical answer finding, as well as for the generation of natural language answers.\n\nMultiNet is supported by a set of [[software tools]] and has been used to build large semantically based computational lexicons. The tools include a semantic interpreter WOCADI which translates natural language expressions (phrases, sentences, texts) into formal MultiNet expressions, a workbench MWR+ for the knowledge engineer (comprising modules for automatic knowledge acquisition and reasoning), and a workbench LIA+ for the computer [[lexicographer]] supporting the creation of large semantically based computational lexica.\n\n== References ==\n* Hermann Helbig, ''Die semantische Struktur natürlicher Sprache - Wissensrepräsentation mit MultiNet''. Springer, Heidelberg, 2001.\n* Hermann Helbig. ''Knowledge Representation and the Semantics of Natural Language'', (2006) Springer, Berlin\n* Sven Hartrumpf, Hermann Helbig, Johannes Leveling, Rainer Osswald. ''An Architecture for Controlling Simple Language in Web Pages'', eMinds: International Journal on Human-Computer Interaction, 1(2), 2006.\n\n== External links ==\n* [http://pi7.fernuni-hagen.de/forschung/multinet/multinet_en.html MultiNet] and its software environment\n\n[[Category:Semantic Web]]\n[[Category:Knowledge representation]]\n\n\n{{software-stub}}"]
['Futures wheel', '8612764', "[[Image:Futures wheel 01.svg|thumb|right|250px|A futures wheel as described by Jerome C. Glenn.]]\n'''The Futures wheel''' is a method for graphical [[visualization (graphic)|visualisation]] of direct and indirect [[future]] '''consequences''' of a particular change or development. It was invented by [[Jerome C. Glenn]] in 1971, when he was a [[student]] at the Antioch Graduate School of Education (now [[Antioch University New England]]).\n<blockquote>The Futures Wheel is a way of organizing thinking and questioning about the future – a kind of structured brainstorming. (Jerome C. Glenn (1994) The Futures Wheel)</blockquote>\n\n==Description==\n\nTo start a Futures wheel the central [[terminology|term]] describing the change to evaluate is positioned in the center of the page (or drawing area). Then, events or consequences following directly from that development are positioned around it. Next, the (indirect) consequences of the direct consequences are positioned around the first level consequences. The terms may be connected as nodes in a tree (or even a web). The levels will often be marked by concentric circles.\n\n==Usage==\n\nThe Futures wheel is usually used to organize [[thought]]s about a future development or trend. With it, possible impacts can be collected and put down in a structured way. The use of interconnecting lines makes it possible to visualize interrelationships of the causes and resulting changes. Thus, Futures wheels can assist in developing multi-concepts about possible future development by offering a futures-conscious perspective and aiding in group [[brainstorming]].\n\n==See also==\n\n* [[Mind Mapping]]\n\n==Bibliography==\n\n* Glenn, Jerome C. ''Futurizing Teaching vs Futures Course'', Social Science Record, Syracuse University, Volume IX, No. 3 Spring 1972.\n* Snyder, David Pearce. Monograph: ''The Futures Wheel: A Strategic Thinking Exercise'', The Snyder Family Enterprise, Bethesda, Maryland 1993.\n* Glenn, Jerome C. ''Futures Wheel'', Futures Research Methodology Version 3.0, The Millennium Project, Washington, D.C. 2009.\n\n==External links==\n* [https://web.archive.org/web/20080612175450/http://www.ltag.education.tas.gov.au/glossary.htm Learning, Teaching and Assessment Guide Glossary] at Tasmania's [[Department of Education (Tasmania)|Department of Education]]'s homepage.\n* Downloadable template of a [https://web.archive.org/web/20070927143447/http://www.globaleducation.edna.edu.au/globaled/go/cache/offonce/pid/1835;jsessionid=050A14CB101EAF863AE979C80461FCB3 Futures wheel] at the [[Australia]]n [http://www.globaleducation.edna.edu.au/ Global Education] website.\n* Futures Wheel, Futures Research Methodology Version 3.0, The Millennium Project, Washington, DC 2009 [http://millennium-project.org/millennium/FRM-V3.html] \n\n[[Category:Knowledge representation]]\n[[Category:Diagrams]]\n[[Category:Futurology]]"]
['Category:Grouping', '11284359', '[[Category:Knowledge representation]]']
['Qualification problem', '731287', '{{one source|date=July 2011}}\nIn [[philosophy]] and [[Artificial intelligence|AI]] (especially, knowledge based systems), the \'\'\'qualification problem\'\'\' is concerned with the impossibility of listing \'\'all\'\' the [[precondition]]s required for a real-world action to have its intended effect. It might be posed as \'\'how to deal with the things that prevent me from achieving my intended result\'\'. It is strongly connected to, and opposite the [[ramification problem|ramification side]] of, the [[frame problem]]. John McCarthy gives the following motivating example, in which it is impossible to enumerate all the circumstances that may prevent a rowboat from performing its ordinary function:\n\n:"[T]he successful use of a boat to cross a river requires, if the boat is a rowboat, that the oars and rowlocks be present and unbroken, and that they fit each other. Many other qualifications can be added, making the rules for using a rowboat almost impossible to apply, and yet anyone will still be able to think of additional requirements not yet stated."\n\n==See also==\n*[[Non-monotonic logic]]\n*[[Circumscription (logic)|Circumscription]]\n\n==External links==\n* John McCarthy "[http://www-formal.stanford.edu/jmc/circumscription/node1.html Introduction: The Qualification Problem]" \n\n[[Category:Knowledge representation]]\n[[Category:Logic programming]]\n[[Category:Epistemology]]\n\n{{epistemology-stub}}']
['Event calculus', '2897680', 'The \'\'\'event calculus\'\'\' is a [[logic]]al language for representing and reasoning about events and their effects first presented by [[Robert Kowalski]] and [[Marek Sergot]] in 1986.\nIt was extended by [[Murray Shanahan]] and [[Rob Miller (Computer Scientist)|Rob Miller]] in the 1990s. Similar to other languages for reasoning about change, the event calculus represents the effects of [[Action (artificial intelligence)|action]]s on [[fluent (artificial intelligence)|fluent]]s. However, [[Event (computing)|event]]s can also be external to the system. In the event calculus, one can specify the value of fluents at some given time points, the events that take place at given time points, and  their effects.\n\n==Fluents and events==\n\nIn the event calculus, fluents are [[Reification (knowledge representation)|reified]]. This means that they are not formalized by means of [[Predicate (mathematics)|predicate]]s but by means of [[function (mathematics)|function]]s. A separate predicate <math>HoldsAt</math> is used to tell which fluents hold at a given time point. For example, <math>HoldsAt(on(box,table),t)</math> means that the box is on the table at time <math>t</math>; in this formula, <math>HoldsAt</math> is a predicate while <math>on</math> is a function.\n\nEvents are also represented as terms. The effects of events are given using the predicates <math>Initiates</math> and <math>Terminates</math>. In particular, <math>Initiates(e,f,t)</math> means that,\nif the event represented by the term <math>e</math> is executed at time <math>t</math>,\nthen the fluent <math>f</math> will be true after <math>t</math>.\nThe <math>Terminates</math> predicate has a similar meaning, with the only difference \nbeing that <math>f</math> will be false and not true after <math>t</math>.\n\n==Domain-independent axioms==\n\nLike other languages for representing actions, the event calculus formalizes the correct evolution of the fluent via formulae telling the value of each fluent after an arbitrary action has been performed. The event calculus solves the [[frame problem]] in a way that is similar to the [[successor state axiom]]s of the [[situation calculus]]: a fluent is true at time <math>t</math> if and only if it has been made true in the past and has not been made false in the meantime.\n \n:<math>HoldsAt(f,t) \\leftarrow\n[Happens(e,t_1) \\wedge Initiates(e,f,t_1) \n\\wedge (t_1<t) \\wedge \\neg Clipped(t_1,f,t)]</math>\n\nThis formula means that the fluent represented by the term <math>f</math> is true at time <math>t</math> if:\n\n# an event <math>e</math> has taken place: <math>Happens(e,t_1)</math>;\n# this took place in the past: <math>t_1<t</math>;\n# this event has the fluent <math>f</math> as an effect: <math>Initiates(e,f,t_1)</math>; \n# the fluent has not been made false in the meantime: <math>Clipped(t_1,f,t)</math>\n\nA similar formula is used to formalize the opposite case in which a fluent is false at a given time. Other formulae are also needed for correctly formalizing fluents before they have been effects of an event. These formulae are similar to the above, but <math>Happens(e,t_1) \\wedge Initiates(e,f,t_1)</math> is replaced by <math>HoldsAt(f,t_1)</math>.\n\nThe <math>Clipped</math> predicate, stating that a fluent has been made false during an interval, can be axiomatized, or simply taken as a shorthand, as follows:\n\n:<math>Clipped(t_1,f,t_2) \\equiv\n\\exists e,t \n[Happens(e,t) \\wedge (t_1 \\leq t < t_2) \\wedge Terminates(e,f,t)]</math>\n\n==Domain-dependent axioms==\n\nThe axioms above relate the value of the predicates <math>HoldsAt</math>, <math>Initiates</math> and <math>Terminates</math>, but do not specify which fluents are known to be true and which events actually make fluents true or false. This is done by using a set of domain-dependent axioms. The known values of fluents are stated as simple literals <math>HoldsAt(f,t)</math>. The effects of events are stated by formulae relating the effects of events with their preconditions. For example, if the event <math>open</math> makes the fluent <math>isopen</math> true, but only if <math>haskey</math> is currently true, the corresponding formula in the event calculus is:\n\n:<math>Initiates(e,f,t) \\equiv\n[ e=open \\wedge f=isopen \\wedge HoldsAt(haskey, t)] \\vee \\cdots\n</math>\n\nThe right-hand expression of this equivalence is composed of a disjunction: for each event and fluent that can be made true by the event, there is a disjunct saying that <math>e</math> is actually that event, that <math>f</math> is actually that fluent, and that the precondition of the event is met.\n\nThe formula above specifies the [[truth value]] of <math>Initiates(e,f,t)</math> for every possible event and fluent. As a result, all effects of all events have to be combined in a single formulae. This is a problem, because the addition of a new event requires modifying an existing formula rather than adding new ones. This problem can be solved by the application of [[Circumscription (logic)|circumscription]] to a set of formulae each specifying one effect of one event:\n\n: <math>Initiates(open, isopen, t) \\leftarrow HoldsAt(haskey, t)</math>\n: <math>Initiates(break, isopen, t) \\leftarrow HoldsAt(hashammer, t)</math>\n: <math>Initiates(break, broken, t) \\leftarrow HoldsAt(hashammer, t)</math>\n\nThese formulae are simpler than the formula above, because each effect of each event can be specified separately. The single formula telling which events <math>e</math> and fluents <math>f</math> make <math>Initiates(e,f,t)</math> true has been replaced by a set of smaller formulae, each one telling the effect of an event on a fluent.\n \nHowever, these formulae are not equivalent to the formula above. Indeed, they only specify sufficient conditions for <math>Initiates(e,f,t)</math> to be true, which should be completed by the fact that <math>Initiates</math> is false in all other cases. This fact can be formalized by simply circumscribing the predicate <math>Initiates</math> in the formula above. It is important to note that this circumscription is done only on the formulae specifying <math>Initiates</math> and not on the domain-independent axioms. The predicate <math>Terminates</math> can be specified in the same way <math>Initiates</math> is.\n\nA similar approach can be taken for the <math>Happens</math> predicate. The evaluation of this predicate can be enforced by formulae specifying not only when it is true and when it is false:\n\n:<math>Happens(e,t) \\equiv\n(e=open \\wedge t=0) \\vee (e=exit \\wedge t=1) \\vee \\cdots</math>\n\nCircumscription can simplify this specification, as only necessary conditions can be specified:\n\n:<math>Happens(open, 0)</math>\n:<math>Happens(exit, 1)</math>\n\nCircumscribing the predicate <math>Happens</math>, this predicate will be false at all points in which it is not explicitly specified to be true. This circumscription has to be done separately from the circumscription of the other formulae. In other words, if <math>F</math> is the set of formulae of the kind <math>Initiates(e,f,t) \\leftarrow \\cdots</math>, <math>G</math> is the set of formulae <math>Happens(e, t)</math>, and <math>H</math> are the domain independent axioms, the correct formulation of the domain is:\n\n:<math>Circ(F; Initiates, Terminates) \\wedge\nCirc(G; Happens) \\wedge H</math>\n\n==The event calculus as a logic program==\n\nThe event calculus was originally formulated as a set of [[Horn clauses]] augmented with [[negation as failure]] and could be run as a [[Prolog]] program. \nIn fact, circumscription is one of the several semantics that can be given to negation as failure, and is closely related to the completion semantics (in which "if" is interpreted as "if and only if" &mdash; see [[logic programming]]).\n\n==Extensions and applications==\n\nThe original event calculus paper of Kowalski and Sergot focused on applications to database updates and narratives. Extensions of the event \ncalculus can also formalize non-deterministic actions, concurrent actions, actions with delayed effects, gradual changes, actions with duration, continuous change, and non-inertial fluents.\n\nKave Eshghi showed how the event calculus can be used for planning, using [[Abduction (logic)|abduction]] to generate hypothetical events in [[Abductive Logic Programming|abductive logic programming]]. Van Lambalgen and Hamm showed how the event calculus can also be used to give an algorithmic semantics to tense and aspect in natural language using constraint logic programming.\n\n==Reasoning tools==\n\nIn addition to Prolog and its variants, several other tools for reasoning using the event calculus are also available:\n* [http://www.doc.ic.ac.uk/~mpsha/planners.html Abductive Event Calculus Planners]\n* [http://decreasoner.sourceforge.net/ Discrete Event Calculus Reasoner]\n* [http://reasoning.eas.asu.edu/ecasp/ Event Calculus Answer Set Programming]\n* [https://www.inf.unibz.it/~montali/tools.html Reactive Event Calculus]\n\n==See also==\n\n* [[First-order logic]]\n* [[Frame problem]]\n* [[Situation calculus]]\n\n==References==\n* Brandano, S. (2001) "[http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?isnumber=20130&arnumber=930691&count=35&index=2 The Event Calculus Assessed,]" \'\'IEEE TIME Symposium\'\': 7-12.\n* Eshghi, K. (1988) "Abductive Planning with Event Calculus," \'\'ICLP/SLP\'\': 562-79.\n* Kowalski, R. (1992) "Database updates in the event calculus," \'\'Journal of Logic Programming 12 (162)\'\': 121-46.\n* -------- and M. Sergot (1986) "[http://www.doc.ic.ac.uk/~rak/papers/event%20calculus.pdf A Logic-Based Calculus of Events,]" \'\'New Generation Computing 4\'\': 67–95.\n* -------- and F. Sadri (1995) "Variants of the Event Calculus," \'\'ICLP\'\': 67-81.\n* Miller, R., and M. Shanahan (1999) "[http://www.ida.liu.se/ext/epa/ej/etai/1999/016/epapage.html The event-calculus in classical logic — alternative axiomatizations,]" \'\'[[Electronic Transactions on Artificial Intelligence]]\'\' 3(1): 77-105.\n* Mueller, Erik T. (2015). \'\'Commonsense Reasoning: An Event Calculus Based Approach (2nd Ed.)\'\'. Waltham, MA: Morgan Kaufmann/Elsevier. ISBN 978-0128014165. (Guide to using the event calculus)\n* Shanahan, M. (1997) \'\'Solving the frame problem: A mathematical investigation of the common sense law of inertia\'\'. MIT Press.\n* -------- (1999) "[http://www.springerlink.com/content/1bxk8gd0n6pajxbq/?p=8f3428a89bad4589a949d74b6f0ec98d&pi=0 The Event Calculus Explained,]" Springer Verlag, LNAI (1600): 409-30.\n* Van Lambalgen, M., and F. Hamm (2005) \'\'The proper treatment of events\'\'. Oxford and Boston: Blackwell Publishing.\n\n[[Category:1986 introductions]]\n[[Category:Logic in computer science]]\n[[Category:Logic programming]]\n[[Category:Knowledge representation]]\n[[Category:Logical calculi]]']
['Semantic analysis (knowledge representation)', '13645056', "{{Cleanup|date=November 2008}}\n{{context|date=February 2014}}\n{{Vague|date=February 2009}}\n'''Semantic analysis''' is a method for eliciting and representing [[knowledge]] about [[organisation]]s.{{Vague|date=February 2009}}<ref>[[Liu Kecheng]], (2000) Semiotics in [[information systems engineering]], Cambridge University Press.</ref> \n\nInitially the problem must be defined by domain experts and passed to the project analyst(s). The next step is the generation of candidate affordances. This step will generate a list of semantic units that may be included in the schema. The candidate grouping follows where some of the semantic units that will appear in the schema are placed in simple groups. Finally the groups will be integrated together into an [[Ontology_(information_science)|ontology]] chart. \n\nSemantic analysis always starts from the problem definition which if not clear, require the analyst to employ relevant [[literature]], [[interview]]s with the [[Stakeholder (corporate)|stakeholders]] and other techniques towards collecting supplementary [[information]]. All assumptions made must be genuine and not limiting the system. \n\n== See also ==\n* [[Semantic analysis (machine learning)]]\n* [[Ontology chart]]\n\n==References==\n{{reflist}}\n\n[[Category:Knowledge representation]]\n\n{{Library-stub}}"]
['DogmaModeler', '15967932', "{{unreferenced|date=March 2014}}\n[[Image:DogmaModeler1.jpg|thumb|DogmaModeler Screenshot]]\n'''DogmaModeler''' is a free and open source ([[GNU GPL]]) [[Ontology (computer science)|ontology]] modeling tool based on [[object-role modeling]] (ORM). The philosophy of DogmaModeler is to enable non-IT experts to model ontologies with a little or no involvement of an ontology engineer. This challenge is tackled in DogmaModeler through well-defined methodological principles: the (1) [[Ontology double articulation|double-articulation]] and the (2) [[modularization]] principles. Other important features are: (3) the use of ORM as a graphical notation for ontology modeling; (4) the verbalization of ORM diagrams into pseudo natural language (supporting flexible verbalization templates for 11 human languages, including English, Dutch, German, French, Spanish, Arabic, Russian, etc.) that allows non-experts to check, validate, or build ontologies; (5)the automatic composition of ontology modules, through a well-defined composition operator; (6) the incorporation of linguistic resources in [[ontology engineering]]; (7) the automatic mapping of ORM diagrams into the DIG [[description logic]] interface and reasoning using [[RACER system|Racer]]; and many other functionalities.\n\nThe first version of DogmaModeler was developed at the [[Vrije Universiteit Brussel]].\n\n== See also ==\n* [[DOGMA]] \n* [[NORMA (software modeling tool)]]\n* [[Protégé (software)|Protégé]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* {{Official website|http://www.jarrar.info/Dogmamodeler/index.htm}}\n\n[[Category:Knowledge representation]]\n[[Category:Free integrated development environments]]\n[[Category:Ontology (information science)]]"]
['New Classification Scheme for Chinese Libraries', '17618735', '{{Unreferenced|date=May 2009}}\nThe \'\'\'New Classification Scheme for Chinese Libraries\'\'\' is a system of [[library classification]] developed by Yung-Hsiang Lai since 1956.  It is modified from "[[:zh:中国图书分类法|A System of Book Classification for Chinese Libraries]]" of [[Liu Guojun]], which is based on the [[Dewey Decimal Classification|Dewey Decimal System]]. \n\nThe scheme is developed for Chinese books, and commonly used in [[Taiwan]], [[Hong Kong]] and [[Macau]]. \n\n==Main classes==\n*000 Generalities\n*100 [[Philosophy]]\n*200 [[Religion]]\n*300 [[Sciences]]\n*400 [[Applied sciences]]\n*500 [[Social sciences]]\n*600-700 [[History]] and [[Geography]]\n*800 [[Linguistics]] and [[Literature]]\n*900 [[Arts]]\n\n==Outline of the classification tables==\n*\'\'\'000 Generalities\'\'\'\n**000 Special collections\n**010 [[Bibliography]]; [[Literacy]] ([[Documentation]])\n**020 [[Library science|Library]] and [[information science]]; Archive management\n**030 [[Sinology]]\n**040 [[General encyclopedia]]\n**050 [[Serial (publishing)|Serial publications]]; [[Periodicals]]\n**060 General [[organization]]; [[Museology]]\n**070 General collected [[essays]]\n**080 General [[Book series|series]]\n**090 Collected [[Chinese classics]]\n*\'\'\'100 [[Philosophy]]\'\'\'\n**100 Philosophy: general\n**110 [[Thought]]; [[Learning]]\n**120 [[Chinese philosophy]]\n**130 [[Oriental philosophy]]\n**140 [[Western philosophy]]\n**150 [[Logic]]\n**160 [[Metaphysics]]\n**170 [[Psychology]]\n**180 [[Esthetics]]\n**190 [[Ethics]]\n*\'\'\'200 [[Religion]]\'\'\'\n**200 Religion: general\n**210 Science of religion\n**220 [[Buddhism]]\n**230 [[Taoism]]\n**240 [[Christianity]]\n**250 [[Islam]] ([[Mohammedanism]])\n**260 [[Judaism]]\n**270 Other religions\n**280 [[Mythology]]\n**290 [[Astrology]]; [[Superstition]]\n*\'\'\'300 [[Sciences]]\'\'\'\n**300 Sciences: general\n**310 [[Mathematics]]\n**320 [[Astronomy]]\n**330 [[Physics]]\n**340 [[Chemistry]]\n**350 [[Earth science]]; [[Geology]]\n**360 [[Biological science]]\n**370 [[Botany]]\n**380 [[Zoology]]\n**390 [[Anthropology]]\n*\'\'\'400 [[Applied sciences]]\'\'\'\n**400 Applied sciences: general\n**410 [[Medical sciences]]\n**420 [[Home economics]]\n**430 [[Agriculture]]\n**440 [[Engineering]]\n**450 [[Mining]] and [[metallurgy]]\n**460 [[Chemical engineering]]\n**470 [[Manufacture]]\n**480 [[Commerce]]: various business\n**490 Commerce: [[Administration (business)|administration]] and [[management]]\n*\'\'\'500 [[Social sciences]]\'\'\'\n**500 Social sciences: general\n**510 [[Statistics]]\n**520 [[Education]]\n**530 [[Rite]] and [[Convention (norm)|custom]]\n**540 [[Sociology]]\n**550 [[Economy]]\n**560 [[Finance]]\n**570 [[Political science]]\n**580 [[Law]]; [[Jurisprudence]]\n**590 [[Military science]]\n*\'\'\'600-700 [[History]] and [[geography]]\'\'\'\n**600 History and geography: General \n*\'\'\'History and geography of [[China]]\'\'\'\n**610 General [[history of China]]\n**620 Chinese history by period\n**630 History of Chinese civilization\n**640 Diplomatic history of China\n**650 Historical sources\n**660 [[Geography of China]]\n**670 Local history\n**680 Topical topography\n**690 Chinese travels\n*\'\'\'[[World history]] and geography\'\'\'\n**710 World: general history and geography\n**720 [[Oceans]] and [[sea]]s\n**730 [[Asia]]: history and geography\n**740 [[Europe]]: history and geography\n**750 [[Americas|America]]: history and geography\n**760 [[Africa]]: history and geography\n**770 [[Oceania]]: history and geography\n**780 [[Biography]]\n**790 [[Antiquities]] and [[archaeology]]\n*\'\'\'800 [[Linguistics]] and [[literature]]\'\'\'\n**800 Linguistics: general\n**810 Literature: general\n**820 [[Chinese literature]]\n**830 Chinese literature: general collections\n**840 Chinese literature: individual works\n**850 Various Chinese literature\n**860 Oriental literature\n**870 [[Western literature]]\n**880 Other countries literatures\n**890 [[Journalism]]\n*\'\'\'900 [[Arts]]\'\'\'\n**900 Arts: general\n**910 [[Music]]\n**920 [[Architecture]]\n**930 [[Sculpture]]\n**940 [[Drawing]] and [[painting]]; [[Calligraphy]]\n**950 [[Photography]]; [[Computer art]]\n**960 [[Decorative arts]]\n**970 [[Arts and Crafts movement]]\n**980 [[Theatre]]\n**990 [[Recreation]] and [[leisure]]\n\n==See also==\n===Decimal systems===\n*[[Dewey Decimal Classification]]\n*[[Nippon Decimal Classification]]\n*[[Korean Decimal Classification]] \n===Non-decimal systems===\n*[[Library of Congress Classification]]\n*[[Chinese Library Classification]]\n\n{{Library classification systems}}\n\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]']
['Category:Knowledge bases', '20750664', "A '''[[Knowledge base]]''' is a special kind of [[database]] for [[knowledge management]]. It provides the means for the computerized collection, organization, and [[retrieval]]{{dn|date=August 2016}} of [[knowledge]].  It is also used for specified information and as a [[personal knowledge base]]\n\n[[Category:Online databases]]\n[[Category:Semantic Web]]\n[[Category:Knowledge representation]]\n[[Category:Types of databases]]"]
['Sears Subject Headings', '22257394', '#REDIRECT [[Minnie Earl Sears]] {{R with possibilities}}\n\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]']
['Category:Minimum Information Standards', '23474509', '{{Cat main|Minimum Information Standards}}\n\n[[Category:Knowledge representation]]\n[[Category:Standards by type]]']
['Category:Knowledge representation languages', '23890667', '== See also ==\n\n* [[:Category:Constraint programming languages]]\n* [[:Category:Domain-specific programming languages]]\n\n[[Category:Knowledge representation]]\n[[Category:Engineered languages]]\n[[Category:Markup languages]]']
['Cognitive map', '1385766', 'A \'\'\'cognitive map\'\'\' (sometimes called a [[mental map]] or [[mental model]]) is a type of [[mental representation]] which serves an individual to acquire, code, store, recall, and decode information about the relative locations and attributes of phenomena in their everyday or metaphorical spatial environment. The concept was introduced by [[Edward C. Tolman|Edward Tolman]] in 1948.<ref name="pmid18870876">{{cite journal |last=Tolman |first=Edward C. |authorlink=Edward C. Tolman |title=Cognitive maps in rats and men |journal=[[Psychological Review]] |volume=55 |issue=4 |pages=189–208 |date=July 1948 |pmid=18870876| doi=10.1037/h0061626}}</ref> The term was later generalized by some researchers, especially in the field of [[operations research]], to refer to a kind of [[semantic network]] representing an individual\'s personal knowledge or [[Schema (psychology)|schemas]].<ref>{{cite journal |last=Eden |first=Colin |date=July 1988 |title=Cognitive mapping |journal=[[European Journal of Operational Research]] |volume=36 |issue=1 |pages=1–13 |doi=10.1016/0377-2217(88)90002-1 |quote=In the practical setting of work in with a team of busy managers cognitive mapping is a tool for building interest from all team members in the problem solving activity. [...] The cycle of \'\'problem construction\'\', \'\'making sense\'\', \'\'defining the problem\'\', and declaring a \'\'portfolio of solutions\'\', which I have discussed elsewhere (Eden, 1982) is the framework that guides the process of working with teams. Thus building and working with the cognitive maps of each individual is primarily aimed at helping each team member reflectively \'construct\' and \'make sense\' of the situation they believe the team is facing. (pp. 7–8)}}</ref><ref>{{cite journal |last1=Fiol |first1=C. Marlene |last2=Huff |first2=Anne Sigismund |date=May 1992 |title=Maps for managers: Where are we? Where do we go from here? |journal=[[Journal of Management Studies]] |volume=29 |issue=3 |pages=267–285 |doi=10.1111/j.1467-6486.1992.tb00665.x |quote=For geographers, a map is a means of depicting the world so that people understand where they are and where they can go. For cognitive researchers, who often use the idea of a \'map\' as an analogy, the basic idea is the same. Cognitive maps are graphic representations that locate people in relation to their information environments. Maps provide a frame of reference for what is known and believed. They highlight some information and fail to include other information, either because it is deemed less important, or because it is not known. (p. 267)}}</ref><ref>{{cite book |last1=Ambrosini |first1=Véronique |last2=Bowman |first2=Cliff |date=2002 |chapter=Mapping successful organizational routines |editor1-last=Huff |editor1-first=Anne Sigismund |editor2-last=Jenkins |editor2-first=Mark |title=Mapping strategic knowledge |location=London; Thousand Oaks, CA |publisher=[[Sage Publications]] |pages=19–45 |isbn=0761969497 |oclc=47900801 |quote=We shall not explain here what cognitive maps are about as this has been done extensively elsewhere (Huff, 1990). Let us just say that cognitive maps are the representation of an individual\'s personal knowledge, of an individual\'s own experience (Weick and Bougon, 1986), and they are ways of representing individuals\' views of reality (Eden et al., 1981). There are various types of cognitive maps (Huff, 1990). (pp. [//books.google.com/books?id=LE95fcRz_IcC&pg=PA21 21–22])}}</ref>\n\n== Overview ==\n\nCognitive maps have been studied in various fields, such as psychology, education, archaeology, planning, geography, cartography, architecture, landscape architecture, urban planning, management and history.<ref>{{cite book |title=Conspiracy nation: the politics of paranoia in Postwar America |last=Knight |first=Peter |year=2002 |publisher=[[New York University Press]] |location=New York and London |isbn=0814747353}}</ref>{{Page needed|date=August 2016}} As a consequence, these mental models are often referred to, variously, as cognitive  maps, [[mental map]]s, [[Behavioral script|script]]s, [[Schema (psychology)|schemata]], and [[Frame of reference|frames of reference]].\n\nCognitive maps serve the construction and accumulation of spatial knowledge, allowing the "[[mind\'s eye]]" to visualize images in order to reduce [[cognitive load]], enhance [[recollection|recall]] and [[learning]] of information. This type of spatial thinking can also be used as a metaphor for non-spatial tasks, where people performing non-spatial tasks involving [[memory]] and imaging use spatial knowledge to aid in processing the task.<ref>{{cite journal |last=Kitchin |first=Robert M. |title=Cognitive maps: what are they and why study them? |journal=[[Journal of Environmental Psychology]] |year=1994 |volume=14 |issue=1 |pages=1–19 |doi=10.1016/S0272-4944(05)80194-X}}</ref>\n\nThe [[neural correlate]]s of a cognitive map have been speculated to be the [[place cell]] system in the [[hippocampus]]<ref name="O\'Keefe">{{cite book |last1=O\'Keefe |first1=John |authorlink1=John O\'Keefe (neuroscientist) |last2=Nadel |first2=Lynn |authorlink2=Lynn Nadel |date=1978 |title=The hippocampus as a cognitive map |location=Oxford; New York |publisher=[[Clarendon Press]]; [[Oxford University Press]] |isbn=0198572069 |oclc=4430731 |url=http://www.cognitivemap.net/}}</ref> and the recently discovered [[grid cells]] in the [[entorhinal cortex]].<ref name="pmid16675704">{{cite journal |last1=Sargolini |first1=Francesca |last2=Fyhn |first2=Marianne |last3=Hafting |first3=Torkel |last4=McNaughton |first4=Bruce L. |last5=Witter |first5=Menno P. |last6=Moser |first6=May-Britt |last7=Moser |first7=Edvard I. |title=Conjunctive representation of position, direction, and velocity in entorhinal cortex |journal=[[Science (journal)|Science]] |volume=312 |issue=5774 |pages=758–762 |date=May 2006 |pmid=16675704 |doi=10.1126/science.1125572 |bibcode=2006Sci...312..758S }}</ref>\n\n== Neurological basis ==\n\nCognitive mapping is believed to largely be a function of the hippocampus. The hippocampus is connected to the rest of the brain in such a way that it is ideal for integrating both spatial and nonspatial information. Connections from the [[postrhinal cortex]] and the medial entorhinal cortex provide spatial information to the hippocampus. Connections from the [[perirhinal cortex]] and lateral entorhinal cortex provide nonspatial information. The integration of this information in the hippocampus makes the hippocampus a practical location for cognitive mapping, which necessarily involves combining information about an object\'s location and its other features.<ref name="Manns">{{cite journal |last1=Manns |first1=Joseph R. |last2=Eichenbaum |first2=Howard |authorlink2=Howard Eichenbaum |date=October 2009 |title=A cognitive map for object memory in the hippocampus |journal=[[Learning & Memory]] |volume=16 |issue=10 |pages=616–624 |doi=10.1101/lm.1484509 |pmc=2769165 |pmid=19794187 }}</ref>\n\nO\'Keefe and Nadel were the first to outline a relationship between the hippocampus and cognitive mapping.<ref name="O\'Keefe" /> Many additional studies have shown additional evidence that supports this conclusion.<ref name=Moser>{{cite journal |last1=Moser |first1=Edvard I. |authorlink1=Edvard Moser |last2=Kropff |first2=Emilio |last3=Moser |first3=May-Britt |authorlink3=May-Britt Moser |date=2008 |title=Place cells, grid cells, and the brain\'s spatial representation system |journal=[[Annual Review of Neuroscience]] |volume=31 |pages=69–89 |doi=10.1146/annurev.neuro.31.061307.090723 |pmid=18284371 |url=http://www.annualreviews.org/eprint/7t2VcSrTYa8V8yACMweG/full/10.1146/annurev.neuro.31.061307.090723}}</ref> Specifically, [[pyramidal cells]] ([[place cells]], [[boundary cell]]s, and [[grid cells]]) have been implicated as the neuronal basis for cognitive maps within the hippocampal system.\n\nNumerous studies by O\'Keefe have implicated the involvement of place cells. Individual place cells within the hippocampus correspond to separate locations in the environment with the sum of all cells contributing to a single map of an entire environment. The strength of the connections between the cells represents the distances between them in the actual environment. The same cells can be used for constructing several environments, though individual cells\' relationships to each other may differ on a map by map basis.<ref name="O\'Keefe" /> The possible involvement of place cells in cognitive mapping has been seen in a number of mammalian species, including rats and macaque monkeys.<ref name=Moser /> Additionally, in a study of rats by Manns and Eichenbaum, pyramidal cells from within the hippocampus were also involved in representing object location and object identity, indicating their involvement in the creation of cognitive maps.<ref name=Manns /> However, there has been some dispute as to whether such studies of mammalian species indicate the presence of a cognitive map and not another, simpler method of determining one\'s environment.<ref name=Bennet />\n\nWhile not located in the hippocampus, grid cells from within the medial entorhinal cortex have also been implicated in the process of [[path integration]], actually playing the role of the path integrator while place cells display the output of the information gained through path integration.<ref name=McNaughton>{{cite journal |last1=McNaughton |first1=Bruce L. |last2=Battaglia |first2=Francesco P. |last3=Jensen |first3=Ole |last4=Moser |first4=Edvard I. |authorlink4=Edvard Moser |last5=Moser |first5=May-Britt |authorlink5=May-Britt Moser |date=August 2006 |title=Path integration and the neural basis of the \'cognitive map\' |journal=[[Nature Reviews Neuroscience]] |volume=7 |issue=8 |pages=663–678 |doi=10.1038/nrn1932 |pmid=16858394 }}</ref> The results of path integration are then later used by the hippocampus to generate the cognitive map.<ref name=Jacobs /> The cognitive map likely exists on a circuit involving much more than just the hippocampus, even if it is primarily based there. Other than the medial entorhinal cortex, the presubiculum and parietal cortex have also been implicated in the generation of cognitive maps.<ref name=Moser />\n\n=== Parallel map theory ===\n\nThere has been some evidence for the idea that the cognitive map is represented in the [[hippocampus]] by two separate maps. The first is the bearing map, which represents the environment through self-movement cues and [[gradient]] cues. The use of these [[vector (mathematics)|vector]]-based cues creates a rough, 2D map of the environment. The second map would be the sketch map that works off of positional cues. The second map integrates specific objects, or [[landmark]]s, and their relative locations to create a 2D map of the environment. The cognitive map is thus obtained by the integration of these two separate maps.<ref name=Jacobs />\n\n==Generation==\n\nThe cognitive map is generated from a number of sources, both from the [[visual system]] and elsewhere. Much of the cognitive map is created through self-generated movement [[sensory cue|cues]]. Inputs from senses like vision, [[proprioception]], olfaction, and hearing are all used to deduce a person\'s location within their environment as they move through it. This allows for path integration, the creation of a vector that represents one\'s position and direction within one\'s environment, specifically in comparison to an earlier reference point. This resulting vector can be passed along to the hippocampal place cells where it is interpreted to provide more information about the environment and one\'s location within the context of the cognitive map.<ref name=Jacobs>{{cite journal |last1=Jacobs |first1=Lucia F. |last2=Schenk |first2=Françoise |date=April 2003 |title=Unpacking the cognitive map: the parallel map theory of hippocampal function |journal=[[Psychological Review]] |volume=110 |issue=2 |pages=285–315 |doi=10.1037/0033-295X.110.2.285 |pmid=12747525}}</ref>\n\nDirectional cues and positional landmarks are also used to create the cognitive map. Within directional cues, both explicit cues, like markings on a compass, as well as gradients, like shading or magnetic fields, are used as inputs to create the cognitive map. Directional cues can be used both statically, when a person does not move within his environment while interpreting it, and dynamically, when movement through a gradient is used to provide information about the nature of the surrounding environment. Positional landmarks provide information about the environment by comparing the relative position of specific objects, whereas directional cues give information about the shape of the environment itself. These landmarks are processed by the hippocampus together to provide a graph of the environment through relative locations.<ref name=Jacobs />\n\n== History ==\n\nThe idea of a cognitive map was first developed by [[Edward C. Tolman]]. Tolman, one of the early cognitive psychologists, introduced this idea when doing an experiment involving rats and mazes. In Tolman\'s experiment, a rat was placed in a cross shaped maze and allowed to explore it. After this initial exploration, the rat was placed at one arm of the cross and food was placed at the next arm to the immediate right. The rat was conditioned to this layout and learned to turn right at the intersection in order to get to the food. When placed at different arms of the cross maze however, the rat still went in the correct direction to obtain the food because of the initial cognitive map it had created of the maze. Rather than just deciding to turn right at the intersection no matter what, the rat was able to determine the correct way to the food no matter where in the maze it was placed.<ref>{{cite book |last=Goldstein |first=E. Bruce |date=2011 |title=Cognitive psychology: connecting mind, research, and everyday experience |edition=3rd |location=Belmont, CA |publisher=[[Wadsworth Cengage Learning]] |isbn=9780840033550 |oclc=658234658 |pages=11–12}}</ref>\n\n== Criticism ==\n\nIn a review, Andrew T.D. Bennett argued that there are no clear evidence for cognitive maps in non-human animals (i.e. cognitive map according to Tolman\'s definition).<ref name=Bennet>{{cite journal |last=Bennett |first=Andrew T. D. |date=January 1996 |title=Do animals have cognitive maps? |journal=[[The Journal of Experimental Biology]] |volume=199 |issue=Pt 1 |pages=219–224 |pmid=8576693}}</ref> This argument is based on analyses of studies where it has been found that simpler explanations can account for experimental results. Bennett highlights three simpler alternatives that cannot be ruled out in tests of cognitive maps in non-human animals "These alternatives are (1) that the apparently novel short-cut is not truly novel; (2) that path integration is being used; and (3) that familiar landmarks are being recognised from a new angle, followed by movement towards them."\n\n== Related term ==\n{{Refimprove section|date=August 2016}}\n\nA cognitive map is a spatial representation of the outside world that is kept within the mind, until an actual manifestation (usually, a drawing) of this perceived knowledge is generated, a mental map. Cognitive mapping is the implicit, mental mapping the explicit part of the same process. In most cases, a cognitive map exists independently of a mental map, an article covering just cognitive maps would remain limited to theoretical considerations.\n\nIn some uses, mental map refers to a practice done by urban theorists by having city dwellers draw a map, from memory, of their city or the place they live. This allows the theorist to get a sense of which parts of the city or dwelling are more substantial or imaginable. This, in turn, lends itself to a decisive idea of how well urban planning has been conducted.\n\n==See also==\n* [[Cognitive geography]]\n* [[Fuzzy cognitive map]]\n* [[Motion perception]]\n* [[Repertory grid]]\n\n==References==\n{{Reflist|30em}}\n\n== External links ==\n* {{commonscat-inline|Cognitive maps}}\n\n{{DEFAULTSORT:Cognitive Map}}\n[[Category:Cognitive science]]\n[[Category:Mnemonics]]\n[[Category:Knowledge representation]]']
['IDIS (software)', '2889751', "{{Unreferenced|date=December 2009}}\n\n'''IDIS''' is a [[software]] tool for direct [[data exchange]] between [[CDS/ISIS]] and [[IDAMS]]. It is developed, maintained and disseminated by [[UNESCO]].\n\n==See also==\n*[[CDS/ISIS]] - database software\n\n{{DEFAULTSORT:Idis (Software)}}\n[[Category:Knowledge representation]]\n\n\n{{network-software-stub}}"]
['Polythematic Structured Subject Heading System', '27847641', '[[Image:PSH logo kratke.gif|thumb|right|PSH logo]]\n\'\'\'Polythematic Structured Subject Heading System\'\'\' (abbreviated as \'\'\'PSH\'\'\' from the [[Czech language|Czech]] \'\'Polytematický Strukturovaný Heslář\'\') is a bilingual Czech–English [[controlled vocabulary]] of [[Index term|subject headings]] developed and maintained by the National Technical Library (the former State Technical Library) in [[Prague]]. It was designed for describing and searching information resources according to their subject. PSH contains more than 13,900 terms, which cover the main fields of human knowledge.\n\n[[Image:Lod-datasets 2010-09-22 colored.png|thumb|The Linking Open Data cloud diagram]]\nThanks to its release in [[Simple Knowledge Organization System|SKOS]], PSH can be used not only for describing [[document]]s in a [[library]], but also for [[Web indexing|indexing web pages]]. Everyone can use PSH for free. PSH is a part of the Linking Open Data cloud diagram (LOD cloud diagaram). The image of the LOD cloud diagram shows datasets that have been published in [[Linked Data]] format, by contributors to the [http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData Linking Open Data] community project and other individuals and organisations.\n\n== History and development ==\nThe PSH preparation project started in 1993, supported by several grants from the Czech Ministry of Culture and Czech Ministry of Education, Youth and Sport. Since 1995, PSH has been used for indexing the State Technical Library’s documents. Starting 1997,<ref>KLOUČKOVÁ, Zdenka. Polytematický strukturovaný heslář Státní technické knihovny. Čtenář. 1997, vol. 49, no. 4, p. 128-129. ISSN 0011-2321.</ref> PSH has been distributed to other libraries and companies, originally as a commercial, paid product; since 2009<ref>MYNARZ, Jindřich; KAMRÁDKOVÁ, Kateřina; KOŽUCHOVÁ, Kristýna. [http://www.techlib.cz/files/download/id/649/psh-cc.pdf Polythematic Structured Subject Heading System & Creative Commons]. In Seminář ke zpřístupňování šedé literatury [online]. 2008– [retrieved 2010-05-28]. Praha : Státní technická knihovna, 2008.</ref> for free. In 2000, the State Technical Library received a grant from the Ministry of Culture to translate PSH into English. The next milestone in its development was its releasing in the [[Simple Knowledge Organization System|SKOS]] format, in 2009.<ref name="mynarz">MYNARZ, Jindřich; KOŽUCHOVÁ, Kristýna; KAMRÁDKOVÁ, Kateřina. [http://www.ikaros.cz/node/5591 Novinky z oblasti Polytematického strukturovaného hesláře]. Ikaros [online]. 2009, vol. 13, no. 7 [retrieved 2010-05-28]. URN-NBN:cz-ik5591. ISSN 1212-5075.</ref>\n\nThe vast majority of new subject headings is suggested and approved by the indexing experts from the National Technical Library. However, the users and public can also make suggestions, using an online form, which are then assessed by the experts. The main decisions about the development and the future of PSH are done by the Committee for Coordination of Polythematic Structured Subject Heading System. The Committee consists of specialists from the National Technical Library and cooperating institutions, and representatives from the libraries and companies which use PSH. The Committee meets once a year in the National Technical Library; in the meantime, the members communicate using an [[electronic mailing list]].<ref name="mynarz" />\n\n== Browsing PSH ==\n[http://psh.ntkcz.cz/skos/ PSH Browser] was released in June 2009. It serves for browsing the PSH system and its distribution in SKOS format. This tool navigates users through PSH from general to specific terms. Users can also use the Search field. [http://pshmanager.techlib.cz/ PSH manager] tool was released in 2012. It serves as an indexing tool especially to catalogers. Catalogers can easy orient in its clear structure. All the terms in PSH manager contain link to the catalogue of NTK. There can be also viewed the record in MARC21 format.\n\n== Autoindexing ==\nIn 2012 was released beta version of autoindexing application. It is accessible on [http://invenio.ntkcz.cz/indexer/ Autoindexing]. Users enter chosen text into indexing field and activate indexing. In few seconds the terms describing content are displayed.\n\n== PSH structure ==\nPSH is a [[tree structure]] with 44 thematic sections. Subject headings are included in a hierarchy of six (or seven) levels according to their [[Semantics|semantic]] content and specificity. There are hierarchical, associative ("see also") and [[equivalence relation|equivalence]] ("see") relations in PSH. Hierarchical relations are represented by broader and narrower terms (e.g. \'\'physical diagnostic methods\'\' is broader term to \'\'electrocardiography\'\', and on the other hand, \'\'electrocardiography\'\' is narrower term to \'\'physical diagnostic methods\'\'). Equivalence relations link subject headings with their nonpreferred versions (e.g. \'\'electrocardiography\'\' and \'\'ECG\'\'). Moreover, associative relations are used to link related subject headings from different parts of PSH, regardless their affiliation to a section, (e.g. \'\'electrocardiography\'\': see also \'\'cardiology\'\'). Every subject heading belongs to just one section, which has its own two-character abbreviation, assigned to every subject heading of the section. This enables users to recognize affiliation of subject headings from lower levels to the thematic sections. The 44 thematic sections have following [[Tree (data structure)|root nodes]]: \n{{Col-begin}}\n{{Col-break}}\n* agriculture\n* anthropology\n* architecture and town planning\n* art\n* astronomy\n* biology\n* chemistry\n* civil engineering\n* communications\n* computer technology\n* consumer industry\n* economic sciences\n* electronics\n* electrotechnics\n* food industry\n{{Col-break}}\n* generalities\n* geography\n* geology\n* geophysics\n* health services\n* history\n* informatics\n* information science\n* law\n* linguistics\n* literature\n* mathematics\n* mechanical engineering\n* metallurgy\n* military affairs\n{{Col-break}}\n* mining engineering\n* pedagogy\n* philosophy\n* physics\n* politology\n* power engineering\n* psychology\n* religion\n* science and technology\n* sociology\n* sport\n* theory of systems\n* transport\n* water management\n{{Col-end}}\n\n== PSH formats ==\nThe main format for storage, maintenance and sharing PSH is the [[MARC standards|MARC 21 Format for Authority Data]], which is implemented in [[integrated library system|library automated systems]]. PSH is also available in [[Simple Knowledge Organization System|SKOS]], using [[RDF/XML]] syntax, which is a version suitable for web distribution. Single headings can be accessed on the PSH website through [[Uniform Resource Identifier|URI]] links. Alternatively, the whole vocabulary can be downloaded in one file. It is possible to display tags from PSH ([[metadata]] snippets – [[Dublin Core]] and CommonTag), which can be embedded in an HTML document to provide its semantic description in a machine-readable way.\n\n== New subject headings ==\nNew subject headings are primarily obtained through the log analysis in the National Technical Library\'s on-line catalogue of documents, which are the terms used by end-users when searching various documents. Google Analytics service is now used for gaining search queries used by users. Within the data analysis, users queries are divided into seven categories that contain the title of the document, person, subject, action, institution, geographical terms and others. Then the candidates for new preferred terms and non-preferred terms are identified in the subject category.\n\nUsers can suggest preferred or non-preferred terms through the [https://www.techlib.cz/en/82958-tech-subject-headings#tab_heading web form] or via e-mail psh(@)techlib.cz.\n\n== PSH & Creative Commons ==\nPSH/SKOS has been available under the Creative Commons License CC BY 3.0 CZ (Attribution-ShareAlike 3.0 Czech Republic)since 2011. Users are free to copy, distribute, display and perform the work and make derivative works, but they must give the original author credit and if they alter, transform, or build upon this work, they have to distribute the resulting work only under a licence identical to this one. Users can download all data in one [https://www.techlib.cz/en/82958-tech-subject-headings#tab_documentation zip file], which is continuously updated.\n\n== See also ==\n*[[Thesaurus]]\n*[[Library of Congress Subject Headings]]\n*[[Information retrieval]]\n*[[Semantic Web]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [https://www.techlib.cz/en/82958-tech-subject-headings/ PSH official web page]\n\n[[Category:Index (publishing)]]\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]']
['Figurative system of human knowledge', '464119', '[[File:ENC SYSTEME FIGURE.jpeg|right|200px|thumb|[[Classification chart]] with the original "figurative system of human knowledge" tree, in French.]]\n\nThe \'\'\'"figurative system of human knowledge"\'\'\', sometimes known as \'\'\'the tree of Diderot and d\'Alembert\'\'\', was a tree developed to represent the structure of [[knowledge]] itself, produced for the \'\'[[Encyclopédie]]\'\' by [[Jean le Rond d\'Alembert]] and [[Denis Diderot]].\n\nThe tree was a [[Taxonomy (general)|taxonomy]] of human knowledge, inspired by [[Francis Bacon]]\'s \'\'[[The Advancement of Learning]]\'\'. The three main branches of knowledge in the tree are: "Memory"/[[History]], "Reason"/[[Philosophy]], and "Imagination"/[[Poetry]].\n\nNotable is the fact that [[theology]] is ordered under \'Philosophy\'. The historian [[Robert Darnton]] has argued that this categorization of [[religion]] as being subject to human reason, and not a source of knowledge in and of itself ([[revelation]]), was a significant factor in the controversy surrounding the work.<ref>Robert Darnton, "Philosophers Trim the Tree of Knowledge: The Epistemological Strategy of the \'\'Encyclopedie\'\'," \'\'The Great Cat Massacre and Other Episodes in French Cultural History\'\' (New York: Basic Books, Inc., 1984), 191-213.</ref>  Additionally notice that \'Knowledge of God\' is only a few nodes away from \'Divination\' and \'Black Magic\'.\n\nThe original version, in [[French (language)|French]], can be seen in the graphic on the right. An [http://www.hti.umich.edu/d/did/tree.html image of the diagram with English translations superimposed over the French text] is available. Another example of English translation of the tree is available in literature (see the reference by Schwab). Below is a version of it rendered in [[English (language)|English]] as a bulleted outline.\n\n== \'\'The Tree of Diderot and d\'Alembert\'\' ==\n\'\'\'"Detailed System of Human Knowledge"\'\'\'\nfrom the [[Encyclopédie]].\n* [[Understanding]]\n:* [[Memory]].\n::* [[History]].\n:::* [[Sacred history|Sacred]] (History of [[Prophet]]s).\n:::* [[History of Christianity|Ecclesiastical]].\n:::* [[Civilization#History|Civil]], [[Ancient history|Ancient]] and [[Modern history|Modern]].\n::::* [[Civilization#History|Civil History]], properly said. \'\'(See also: [[Civil society#History|History of civil society]])\'\'\n::::* [[Literary History]].\n:::::* [[Memoirs]].\n:::::* [[Antiquities]]. \'\'(See also: [[Classical antiquity]])\'\'\n:::::* Complete Histories.\n:::* [[Natural history|Natural]].\n::::* Uniformity of Nature. \'\'(See: [[Uniformitarianism]])\'\'\n:::::* [[Cosmology|Celestial History]].\n:::::* History...\n::::::* of [[Meteoroid#History|Meteors]].\n::::::* of the [[History of the Earth|Earth]] and the [[World Ocean|Sea]] \'\'(See also: [[Origin of water on Earth]])\'\'\n::::::* of [[Minerals]]. \'\'(See also:  [[Geological history of Earth]])\'\'\n::::::* of [[Vegetable]]s. \'\'(See also: [[History of agriculture]])\'\'\n::::::* of [[Animal]]s.  \'\'(See also: [[Evolutionary history of life]])\'\'\n::::::* of the [[Chemical element|Elements]]. \'\'(See also: [[Classical element]], [[History of alchemy]], and [[History of chemistry]])\'\'\n::::* Deviations of Nature.\n:::::* [[Celestial object|Celestial Wonders]].\n:::::* [[Meteoroid#Frequency of large meteors|Large Meteors]]. \'\'(See also: [[Asteroid]]s)\'\'\n:::::* Wonders of Land and Sea. \'\'(See: [[Wonders of the World]])\'\'\n:::::* [[Mineral#Other properties|Monstrous Mineral]]s.\n:::::* Monstrous Vegetables. \'\'(See: [[Largest organisms#Plants|Largest plants]], [[Poisonous plant]]s, and [[Carnivorous plant]]s)\'\'\n:::::* Monstrous Animals. (See: \'\'[[Largest organisms#Animals|Largest animals]] and [[Predator]]s)\'\'\n:::::* Wonders of the Elements. \'\'(See: [[Natural disaster]]s)\'\'\n::::* Uses of Nature (See \'\'[[Technology]] and [[Applied science]]s)\'\'\n:::::* Arts, [[Craft]]s, Manufactures.\n::::::* Work and Uses of [[Gold]] and [[Silver]].\n:::::::* [[Mint (coin)|Minting]].\n:::::::* [[Goldsmith]].\n:::::::* Gold Spinning.\n:::::::* Gold Drawing.\n:::::::* [[Silversmith]]\n:::::::* [[Planishing|Planisher]], etc.\n::::::* Work and Uses of Precious Stones.\n:::::::* [[Lapidary]].\n:::::::* [[Diamond cutting]].\n:::::::* [[Jewellery|Jeweler]], etc.\n::::::* Work and Uses of [[Iron]].\n:::::::* Large [[Forge|Forges]].\n:::::::* [[Locksmithing|Locksmith]].\n:::::::* Tool Making.\n:::::::* Armorer.\n:::::::* Gun Making, etc.\n::::::* Work and Uses of [[Glass]].\n:::::::* [[Glass|Glassmaking]].\n:::::::* [[Plate-Glass|Plate-Glassmaking]].\n:::::::* [[Mirror#Manufacture|Mirror Making]].\n:::::::* [[Optician]].\n:::::::* [[Glazier]], etc.\n::::::* Work and Uses of Skin.\n:::::::* [[Tanner (occupation)|Tanner]].\n:::::::* [[Chamois leather|Chamois Maker]].\n:::::::* Leather Merchant.\n:::::::* [[Glove]] Making, etc.\n::::::* Work and Uses of [[Stonemasonry|Stone]], [[Plaster#Uses|Plaster]], [[Slate#Uses|Slate]], etc.\n:::::::* Practical [[Architecture]].\n:::::::* Practical [[Sculpture]].\n:::::::* [[Masonry|Mason]].\n:::::::* [[Tiler]], etc.\n::::::* Work and Uses of [[Silk#Uses|Silk]].\n:::::::* Spinning.\n:::::::* Milling.\n:::::::* Work like.\n:::::::* [[Velvet]].\n:::::::* Brocaded Fabrics, etc.\n::::::* Work and Uses of [[Wool]].\n:::::::* Cloth-Making.\n:::::::* Bonnet-Making, etc.\n::::::* Working and Uses, etc.\n:* [[Reason]]\n::* [[Philosophy]]\n:::* General [[Metaphysics]], or [[Ontology]], or Science of Being in General, of Possibility, of [[Existence]], of Duration, etc.\n:::* Science of [[God]].\n::::* [[Natural Theology]].\n::::* Revealed [[Theology]].\n::::* Science of Good and Evil Spirits.\n:::::* [[Divination]].\n:::::* [[Black Magic]].\n:::* Science of Man.\n::::* [[Pneumatology]] or Science of the [[Soul]].\n:::::* Reasonable.\n:::::* Sensible.\n::::* [[Logic]].\n:::::* Art of [[Outline of thought|Thinking]].\n::::::* [[Apprehension (understanding)|Apprehension]].\n:::::::* Science of [[Idea]]s\n::::::* [[Judgement]].\n:::::::* Science of [[Proposition]]s.\n::::::* [[Reasoning]].\n:::::::* [[Inductive reasoning|Induction]].\n::::::* [[Reasoning#Logical_reasoning_methods_and_argumentation|Method]].\n:::::::* Demonstration.\n::::::::* [[Analysis]].\n::::::::* [[:wikt:-synthesis|Synthesis]].\n:::::* Art of Remembering.\n::::::* [[Memory]].\n:::::::* Natural.\n:::::::* [[Art of memory|Artificial]].\n::::::::* Prenotion.\n::::::::* Emblem.\n::::::* Supplement to Memory.\n:::::::* [[Writing]].\n:::::::* [[Printing]].\n::::::::* [[Alphabet]].\n::::::::* Cipher.\n:::::::::* Arts of [[Writing]], Printing, [[Reading (process)|Reading]], Deciphering.\n::::::::::* [[Orthography]].\n:::::* Art of [[Communication]]\n::::::* Science of the Instrument of [[Discourse]].\n:::::::* [[Grammar]].\n::::::::* [[Sign]]s.\n:::::::::* [[Gesture]].\n::::::::::* [[Mime|Pantomime]].\n::::::::::* Declamation.\n:::::::::* Characters.\n::::::::::* [[Ideogram]]s.\n::::::::::* [[Hieroglyphics]].\n::::::::::* [[Heraldry]] or Blazonry.\n::::::::* [[Prosody (linguistics)|Prosody]].\n::::::::* Construction.\n::::::::* [[Syntax]].\n::::::::* [[Philology]].\n::::::::* Critique.\n:::::::* [[Pedagogy]].\n::::::::* [[Curriculum|Choice of Studies]].\n::::::::* [[Teaching method|Manner of Teaching]].\n::::::* Science of Qualities of [[Discourse]].\n:::::::* [[Rhetoric]].\n:::::::* Mechanics of [[Poetry]].\n::::* [[Outline of ethics|Ethics]].\n:::::* [[Contemporary ethics|General]].\n::::::* General Science of [[Good and evil|Good and Evil]], of duties in general, of [[Virtue]], of the necessity of being Virtuous, etc.\n:::::* [[Outline of ethics#Branches of ethics|Particular]].\n::::::* Science of [[Law]]s or [[Jurisprudence]].\n:::::::* [[Natural law|Natural]].\n:::::::* [[Economic forces|Economic]]. \'\'(See also [[commercial law]])\'\'\n:::::::* [[Politics|Political]]. \'\'(See also [[political law]])\'\'\n::::::::* [[Domestic politics|Internal]] and [[International politics|External]]. \'\'(See also [[foreign policy]])\'\'\n::::::::* [[Commerce]] on Land and [[Maritime industry|Sea]].\n:::* [[Natural science|Science of Nature]]\n::::* [[Metaphysics]] of Bodies or, General Physics, of Extent, of Impenetrability, of Movement, of Word, etc.\n::::* [[Outline of mathematics|Mathematics]].\n:::::* [[Pure mathematics|Pure]].\n::::::* [[Outline of arithmetic|Arithmetic]].\n:::::::* [[Number|Numeric]].\n:::::::* [[Algebra]].\n::::::::* [[Elementary algebra|Elementary]].\n::::::::* [[Infinitesimal]].\n:::::::::* [[Differential algebra|Differential]].\n:::::::::* [[Integral]].\n::::::* [[Outline of geometry|Geometry]].\n:::::::* Elementary (Military Architecture, Tactics).\n:::::::* Transcendental (Theory of Courses).\n:::::* Mixed.\n::::::* [[Mechanics]].\n::::::::* [[Statics]].\n:::::::::* Statics, properly said.\n:::::::::* [[Hydrostatics]].\n::::::::* [[Dynamics (mechanics)|Dynamics]].\n:::::::::* Dynamics, properly said.\n:::::::::* [[Ballistics]].\n:::::::::* [[Hydrodynamics]].\n::::::::::* [[Hydraulics]].\n::::::::::* [[Navigation]], Naval Architecture.\n::::::* Geometric [[Astronomy]].\n:::::::* [[Cosmography]].\n::::::::* [[Celestial cartography|Uranography]].\n::::::::* [[Geography]].\n::::::::* [[Hydrography]].\n:::::::* [[Chronology]].\n:::::::* [[Gnomon]]ics.\n::::::* [[Optics]].\n:::::::* Optics, properly said.\n:::::::* [[Dioptrics]], Perspective.\n:::::::* [[Catoptrics]].\n::::::* [[Acoustics]].\n::::::* [[Pneumatics]].\n::::::* Art of Conjecture. [[probability|Analysis of Chance]].\n:::::* Physicomathematics.\n::::* Particular Physics.\n:::::* [[Outline of zoology|Zoology]].\n::::::* [[Anatomy]].\n:::::::* Simple.\n:::::::* [[Comparative anatomy|Comparative]].\n::::::* [[Physiology]].\n::::::* [[Outline of medicine|Medicine]].\n:::::::* Hygiene.\n::::::::* [[Hygiene]], properly said.\n::::::::* Cosmetics (Orthopedics).\n::::::::* Athletics (Gymnastics).\n:::::::* Pathology.\n:::::::* Semiotics.\n:::::::* Treatment.\n::::::::* Diete.\n::::::::* [[Surgery]].\n::::::::* Pharmacy.\n::::::* [[Veterinary medicine|Veterinary Medicine]].\n::::::* [[Horse care|Horse Management]].\n::::::* [[Hunting]].\n::::::* [[Outline of fishing|Fishing]].\n::::::* [[Falconry]].\n:::::* Physical [[Astronomy]].\n::::::* [[Astrology]].\n:::::::* Judiciary Astrology.\n:::::::* Physical Astrology.\n:::::* [[Meteorology]].\n:::::* [[Cosmology]].\n::::::* Uranology.\n::::::* [[Aerology]].\n::::::* [[Geology]].\n::::::* [[Hydrology]].\n:::::* [[Botany]].\n::::::* [[Agriculture]].\n::::::* [[Gardening]].\n:::::* [[Mineralogy]].\n:::::* [[Chemistry]].\n::::::* Chemistry, properly said, ([[Pyrotechnics]], Dyeing, etc.).\n::::::* [[Metallurgy]].\n::::::* [[Alchemy]].\n::::::* Natural Magic.\n:* Imagination.\n::* [[Poetry]].\n:::* Sacred, Profane.\n::::* Narrative.\n:::::* [[Epic poetry|Epic Poem]]\n:::::* [[Madrigal (poetry)|Madrigal]]\n:::::* [[Epigram]]\n:::::* [[Novel]], etc.\n::::* [[Drama|Dramatic]]\n:::::* [[Tragedy]]\n:::::* [[Comedy]]\n:::::* [[Pastoral]], etc.\n::::* Parable\n:::::* [[Allegory]]\n(NOTE: THIS NEXT BRANCH SEEMS TO BELONG TO BOTH THE NARRATIVE AND DRAMATIC TREE AS DEPICTED BY THE LINE DRAWN CONNECTING THE TWO.)\n::::* [[Outline of music|Music]]\n:::::* [[Music theory|Theoretical]]\n:::::* Practical \'\'(see also [[musical technique]])\'\'\n:::::** [[Instrumental]]\n:::::** [[vocal music|Vocal]]\n::::* [[Outline of painting|Painting]]\n::::* [[Outline of sculpture|Sculpture]]\n::::* [[Engraving]]\n\n== See also ==\n* [[Classification chart]]\n* [[Instauratio magna]]\n* [[Propædia]]\n* [[Pierre Mouchon]]\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n* Robert Darnton, "Epistemological angst: From encyclopedism to advertising," in Tore Frängsmyr, ed., \'\'The structure of knowledge: classifications of science and learning since the Renaissance\'\' (Berkeley, CA: Office for the History of Science and Technology, University of California, Berkeley, 2001).\n* Adams, David (2006) \'The Système figuré des Connaissances humaines and the structure of Knowledge in the Encyclopédie\',  in Ordering the World, ed. Diana Donald and Frank O\'Gorman, London: Macmillan, p.&nbsp;190-215. \n* \'\'Preliminary discourse to the Encyclopedia of Diderot\'\', Jean Le Rond d\'Alembert, translated by Richard N. Schwab, 1995. ISBN 0-226-13476-8\n\n==External links==\n\n* [http://quod.lib.umich.edu/d/did/tree.html The \'\'Tree\'\' translated into English]\n* [http://artfl.uchicago.edu/cactus/ ESSAI D\'UNE DISTRIBUTION GÉNÉALOGIQUE DES SCIENCES ET DES ARTS PRINCIPAUX, published as a fold-out frontispiece in volume 1 of Pierre Mouchon, \'\'Table analytique et raisonnée des matieres contenues dans les XXXIII volumes in-folio du Dictionnaire des sciences, des arts et des métiers, et dans son supplément\'\', Paris, Panckoucke 1780.]\n\n{{DEFAULTSORT:Figurative System Of Human Knowledge}}\n[[Category:Taxonomy]]\n[[Category:Age of Enlightenment]]\n[[Category:Trees (data structures)]]\n[[Category:Knowledge representation]]']
['Mental mapping', '8090717', '{{about|the geographical concept|the diagram|Mind map}}\nIn [[behavioral geography]], a \'\'\'mental map\'\'\' is a person\'s [[Perspective (cognitive)|point-of-view]] perception of their area of interaction. Although this kind of subject matter would seem most likely to be studied by fields in the [[social sciences]], this particular subject is most often studied by modern day [[geographers]]. They study it to determine [[Subjectivity|subjective]] qualities from the public such as personal preference and practical uses of geography like driving directions. [[Mass media]] also have a virtually direct effect on a person\'s mental map of the geographical world.<ref>[http://mentalmaps.info Mental Maps Resource Site]</ref> The perceived geographical dimensions of a foreign nation (relative to one\'s own nation) may often be heavily influenced by the amount of time and relative news coverage that the news media may spend covering news events from that foreign region. For instance, a person might perceive a small island to be nearly the size of a continent, merely based on the amount of news coverage that he or she is exposed to on a regular basis.<ref>[http://geography.about.com/cs/culturalgeography/a/mentalmaps.htm Mental Maps on About.com]</ref>\n\nIn [[Experimental psychology|psychology]], the term names the information maintained in the mind of an organism by means of which it may plan activities, select routes over previously traveled territories, etc. The rapid traversal of a familiar [[maze]] depends on this kind of mental map if scents or other markers laid down by the subject are eliminated before the maze is re-run.\n\n==Background==\nMental maps are an outcome of the field of behavioral geography. The imagined maps are considered one of the first studies that intersected geographical settings with human action.<ref name="Gregory">{{cite book|last=Gregory|first=Derek|title=Dictionary of Human Geography: Mental maps/Cognitive Maps|year=2009|publisher=Wiley-Blackwell|location=Hoboken|edition=5th |author2=Johnston, Rom |author3=Pratt, Geraldine |page=455}}</ref>  The most prominent contribution and study of mental maps was in the writings of [[Kevin A. Lynch|Kevin Lynch]]. In \'\'[[The Image of the City]]\'\', Lynch used simple sketches of maps created from memory of an urban area to reveal five elements of the city; nodes, edges, districts, paths and landmarks.<ref>{{cite book|last=Lynch|first=Kevin|title=The Image of the City|year=1960|publisher=MIT Press|location=Cambridge MA}}</ref>  Lynch claimed that “Most often our perception of the city is not sustained, but rather partial, fragmentary, mixed with other concerns. Nearly every sense is in operation, and the image is the composite of them all.” (Lynch, 1960, p 2.) The creation of a mental map relies on memory as opposed to being copied from a preexisting map or image. In \'\'The Image of the City\'\', Lynch asks a participant to create a map as follows: “Make it just as if you were making a rapid description of the city to a stranger, covering all the main features. We don’t expect an accurate drawing- just a rough sketch.” (Lynch 1960, p 141) In the field of human geography mental maps have led to an emphasizing of social factors and the use of social methods versus quantitative or positivist methods.<ref name="Gregory" /> Mental maps have often led to revelations regarding social conditions of a particular space or area. Haken and Portugali (2003) developed an information view, which \nargued that the face of the city is its information <ref>{{cite journal|last=Haken|first=Herman|author2=Portugali, Juval|title=The face of the city is its information|journal=Journal of Environmental Psychology|date=August 2003|volume=23|pages=385–408|doi=10.1016/s0272-4944(03)00003-3}}</ref>\n. Bin Jiang (2012) argued that the image of the city (or mental map) arises out of the scaling of city artifacts and locations.<ref>{{cite journal|last=Jiang|first=Bin|title=The image of the city out of the underlying scaling of city artifacts or locations|journal=Annals of the Association of American Geographers|year=2012|volume=103|pages= 1552–1566| doi = 10.1080/00045608.2013.779503 |arxiv=1209.1112}}</ref> He addressed that why the image of city can be formed \n<ref>{{cite journal|last=Jiang|first=Bin|title=Why can the image of the city be formed| arxiv=1212.3703}}</ref>\n, and he even suggested ways of computing the image of the city, or more precisely the kind of collective image of the city, using increasingly available geographic information such as Flickr and Twitter\n<ref>{{cite journal|last=Jiang|first=Bin|title=Computing the image of the city| arxiv=1212.0940}}</ref>\n.\n\n==Research applications==\nMental maps have been used in a collection of spatial research. Many studies have been performed that focus on the quality of an environment in terms of feelings such as fear, desire and stress. A study by Matei et al. in 2001 used mental maps to reveal the role of media in shaping urban space in Los Angeles. The study used Geographic Information Systems (GIS) to process 215 mental maps taken from seven neighborhoods across the city. The results showed that people’s fear perceptions in Los Angeles are not associated with high crime rates but are instead associated with a concentration of certain ethnicities in a given area.<ref>{{cite journal|last=Matei|first=Sorin |author2=Ball-Rokeach, Sandra |author3=Qiu Linchuan, Jack|title=Fear and Misperception of Los Angeles Urban Space: A Spatial-Statistical Study of Communication-Shaped Mental Maps|journal=Communication Research|date=August 2001|volume=28|issue=4|pages=429–463|accessdate=4 November 2012|url=http://mentalmap.org/files/matei_fear_CR.pdf|doi=10.1177/009365001028004004}}</ref>  The mental maps recorded in the study draw attention to these areas of concentrated ethnicities as parts of the urban space to avoid or stay away from. \n\t\nMental maps have also been used to describe the urban experience of children. In a 2008 study by Olga den Besten mental maps were used to map out the fears and dislikes of children in Berlin and Paris. The study looked into the absence of children in today’s cities and the urban environment from a child’s perspective of safety, stress and fear.<ref>{{cite journal|last=Den Besten|first=Olga den|title=Local belonging and ‘geographies of emotions’: Immigrant children’s experience of their neighbourhoods in Paris and Berlin|journal=Childhood|date=May 2010|volume=17|issue=2|pages=181–195|url=http://chd.sagepub.com/content/17/2/181|accessdate=4 November 2012|doi=10.1177/0907568210365649}}</ref>\n\nPeter Gould and Rodney White have performed prominent analyses in the book “Mental Maps.” The book is an investigation into people’s spatial desires. The book asks of its participants: “Suppose you were suddenly given the chance to choose where you would like to live- an entirely free choice that you could make quite independently of the usual constraints of income or job availability. Where would you choose to go?” (Gould, 1974, p 15) Gould and White use their findings to create a surface of desire for various areas of the world. The surface of desire is meant to show people’s environmental preferences and regional biases.<ref>{{cite book|last=Gould|first=Peter|title=Mental Maps|year=1993|publisher=Rutledge|location=New York|author2=White, Rodney|page=93}}</ref>\n\nIn an experiment done by [[Edward C. Tolman]], the development of a mental map was seen in rats.<ref>Goldstein, B. (2011). \'\'Cognitive Psychology: Connecting Mind, Research, and Everyday Experience--with coglab manual. (3rd ed.).\'\' Belmont, CA: Wadsworth.</ref> A rat was placed in a cross shaped maze and allowed to explore it. After this initial exploration, the rat was placed at one arm of the cross and food was placed at the next arm to the immediate right. The rat was conditioned to this layout and learned to turn right at the intersection in order to get to the food. When placed at different arms of the cross maze however, the rat still went in the correct direction to obtain the food because of the initial mental map it had created of the maze. Rather than just deciding to turn right at the intersection no matter what, the rat was able to determine the correct way to the food no matter where in the maze it was placed.\n\nThe idea of mental maps is also used in strategic analysis. David Brewster, an Australian strategic analyst, has applied the concept to strategic conceptions of South Asia and Southeast Asia.  He argues that popular mental maps of where regions begin and end can have a significant impact on the strategic behaviour of states.<ref>{{cite web|author=David Brewster|url=https:// www.academia.edu/7697999/Dividing_Lines_Evolving_Mental_Maps_of_the_Bay_of_Bengal|title=Dividing Lines: Evolving Mental Maps of the Bay of Bengal. Retrieved 21 September 2014}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Knowledge representation]]\n[[Category:Cognitive psychology]]\n[[Category:Psychology terminology]]']
['Ishikawa diagram', '57535', '{{Cleanup|date=March 2012}}\n{{Infobox quality tool\n| image =     Cause and effect diagram for defect XXX.svg\n| category =  One of the [[Seven Basic Tools of Quality]]\n| describer = [[Kaoru Ishikawa]]\n| purpose =   To break down (in successive layers of detail) root causes that potentially contribute to a particular effect\n}}\n\'\'\'Ishikawa diagrams\'\'\' (also called \'\'\'fishbone diagrams\'\'\', \'\'\'herringbone diagrams\'\'\', \'\'\'cause-and-effect diagrams\'\'\', or \'\'\'Fishikawa\'\'\') are [[causal diagram]]s created by [[Kaoru Ishikawa]] (1968) that show the [[cause]]s of a specific [[wikt:event|event]].<ref>{{cite book | last = Ishikawa |first = Kaoru | title= Guide to Quality Control | year = 1968 | publisher = JUSE | location = Tokyo\n}}</ref><ref>{{cite book | last = Ishikawa | first = Kaoru | title = Guide to Quality Control | publisher = Asian Productivity Organization | year =  1976 | isbn = 92-833-1036-5}}</ref> Common uses of the Ishikawa diagram are [[product design]] and quality defect prevention to identify potential factors causing an overall effect. Each cause or reason for imperfection is a source of variation. Causes are usually grouped into major categories to identify these sources of variation. The categories typically include\n*People: Anyone involved with the process\n*Methods: How the process is performed and the specific requirements for doing it, such as policies, procedures, rules, regulations and laws\n*Machines: Any equipment, computers, tools, etc. required to accomplish the job\n*Materials: Raw materials, parts, pens, paper, etc. used to produce the final product\n*Measurements: Data generated from the process that are used to evaluate its quality\n*Environment: The conditions, such as location, time, temperature, and culture in which the process operates\n\n==Overview==\n[[File:Ishikawa Fishbone Diagram.svg|280px|thumb|left|Ishikawa diagram, in fishbone shape, showing factors of Equipment, Process, People, Materials, Environment and Management, all affecting the overall problem. Smaller arrows connect the sub-causes to major causes.]]\nIshikawa diagrams were popularized in the 1960s by [[Kaoru Ishikawa]],<ref>{{cite book |year=2001 |title=Infusion Therapy in Clinical Practice |first=Judy |last=Hankins |pages=42}}</ref> who pioneered quality management processes in the [[Kawasaki Heavy Industries|Kawasaki]] shipyards, and in the process became one of the founding fathers of modern management.\n\nThe basic concept was first used in the 1920s, and is considered one of the [[Seven Basic Tools of Quality|seven basic tools]] of [[quality control]].<ref>{{cite web | url = http://www.asq.org/learn-about-quality/seven-basic-quality-tools/overview/overview.html |first=Nancy R. | last=Tague | title = Seven Basic Quality Tools | year = 2004 | work = The Quality Toolbox | publisher = American Society for Quality | location = Milwaukee, Wisconsin | page = 15 | accessdate = 2010-02-05}}</ref> It is known as a fishbone diagram because of its shape, similar to the side view of a fish skeleton.\n\n[[Mazda]] Motors famously used an Ishikawa diagram in the development of the [[Miata]] sports car, where the required result was "Jinba Ittai" (Horse and Rider as One — jap. 人馬一体). The main causes included such aspects as "touch" and "braking" with the lesser causes including highly granular factors such as "50/50 weight distribution" and "able to rest elbow on top of driver\'s door". Every factor identified in the diagram was included in the final design.{{citation needed|date=November 2015}}\n\n==Causes==\nCauses in the diagram are often categorized, such as to the 5 M\'s, described below. Cause-and-effect diagrams can reveal key relationships among various variables, and the possible causes provide additional insight into process behavior.\n\nCauses can be derived from brainstorming sessions. These groups can then be labeled as categories of the fishbone.  They will typically be one of the traditional categories mentioned above but may be something unique to the application in a specific case.  Causes can be traced back to root causes with the [[5 Whys]] technique.\n\nTypical categories are\n\n===The 5 Ms (used in manufacturing industry)===\n*Machine (technology)\n*Method (process)\n*Material (Includes Raw Material, Consumables and Information.)\n*Man Power (physical work)/Mind Power (brain work): [[Kaizen]]s, Suggestions\n*Measurement (Inspection)\nThe original 5 Ms used by the Toyota Production System have been expanded by some to include the following and are referred to as the 8 Ms. However, this is not globally recognized. It has been suggested to return to the roots of the tools and to keep the teaching simple while recognizing the original intent; most programs do not address the 8Ms.\n*Milieu/Mother Nature(Environment)\n*Management/Money Power\n*Maintenance\n\n"Milieu" is also used as the 6th M by industries for investigations taking the environment into account.\n\n===The 8 Ps (used in marketing industry)===\n*Product/Service\n*Price \n*Place\n*Promotion\n*People/personnel\n*Process\n*Physical Evidence\n*Packaging\nThe 8 Ps are primarily used in service marketing.\n\n===The 5 Ss (used in service industry)===\n*Surroundings\n*Suppliers\n*Systems\n*Standard documentation skills\n*Scope of work\n\n==See also==\n{{Portal|Thinking}}\n* [[Seven Basic Tools of Quality]]\n* [[Five whys]]\n\n== References ==\n\n=== Citations ===\n{{Reflist|30em}}\n\n=== Sources ===\n* Ishikawa, Kaoru (1990); (Translator: J. H. Loftus); \'\'Introduction to Quality Control\'\'; 448 p; ISBN 4-906224-61-X {{OCLC|61341428}}\n* Dale, Barrie G. et al. (2007); \'\'Managing Quality 5th ed\'\'; ISBN 978-1-4051-4279-3 {{OCLC|288977828}}\n\n==External links==\n{{Commons category|Ishikawa diagrams}}\n\n{{DEFAULTSORT:Ishikawa Diagram}}\n[[Category:Causal diagrams]]\n[[Category:Causality]]\n[[Category:Knowledge representation]]\n[[Category:Quality control tools]]']
['Visual hierarchy', '18587056', "'''Visual hierarchy''' refers to the arrangement or presentation of elements in a way that implies importance.<ref>{{cite web|url = http://support.esri.com/en/knowledgebase/GISDictionary/term/visual%20hierarchy|title = GIS Dictionary|accessdate = 2014-08-13|publisher=ESRI}}</ref> In other words, visual hierarchy influences the order in which the human eye perceives what it sees. This order is created by the visual [[Contrast (vision)|contrast]] between forms in a field of perception. Objects with highest contrast to their surroundings are recognized first by the human mind. The term visual hierarchy is used most frequently in the discourse of the visual arts fields, notably so within the field of [[graphic design]].\n\n==Theory==\nThe concept of visual hierarchy is based in [[Gestalt psychology|Gestalt psychological theory]], an early 20th-century German theory that proposes that the human brain has innate organizing tendencies that “structure individual elements, shapes or forms into a coherent, organized whole.” <ref>Jackson, Ian. “Gestalt—A  Learning Theory for Graphic Design Education.” ''International Journal of Art and Design Education''. Volume 27. Issue 1 (2008): 63-69. Digital.</ref> The German word Gestalt translates into “form,” “pattern,” or “shape” in English.<ref>Pettersson, Rune. “Information Design—Principles and Guidelines.” ''Journal of Visual Literacy''. Volume 29. Issue 2 (2010): 167-182. Digital.</ref> When an element in a visual field disconnects from the ‘whole’ created by the brain’s perceptual organization, it “stands out” to the viewer. The shapes that disconnect most severely from their surroundings stand out the most.\n\n==Physical characteristics==\nThe brain disassociates objects from one another based upon the differences between their physical characteristics. These characteristics fall into four categories: color, size, alignment, and character. The category of color encompasses the [[hue]], [[Colorfulness|saturation]], [[Lightness (color)|value]], and perceived [[Texture (visual arts)|texture]] of forms. Size describes the surface area of a form. Alignment is the arrangement of forms with respect to their direction, orientation, or pattern.<ref>Feldsted, CJ. ''Design Fundamentals''. New York: Pittman Publishing Corporation, 1950.</ref> Character is the [[Rectilinear polygon|rectilinearity]] and [[Curvilinear coordinates|curvilinearity]] of forms. Forms that have differences in these characteristics contrast each other.\n\n==Application==\n{{Unreferenced section|date=February 2015}}\nVisual hierarchy is an important concept in the field of [[graphic design]], a field that specializes in visual organization. Designers attempt to control visual hierarchy to guide the eye to information in a specific order for a specific purpose. One could compare visual hierarchy in graphic design to grammatical structure in writing in terms of the importance of each principle to these fields.\n\n===Examples===\n[[Fluorescence|Fluorescent]] color contrasts highly against most naturally occurring colors. Fluorescent substances achieve this contrast by emitting light. Forms of this type of color are almost always high in visual hierarchy. [[Tennis ball]]s are fluorescent green for the perceptual ease of players, match officials, and spectators.\n\n[[Camouflage]] patterns diminish the contrast between themselves and their surroundings. Camouflage describes a form that mimics the physical characteristics of its environment. These patterns are difficult and sometimes impossible to perceive. Certain animals and military forces have both developed their own camouflaged patterns as mechanisms of defense.\n\n==See also==\n*[[Bauhaus]]\n*[[Cognitive psychology]]\n*[[Pattern recognition]]\n\n==References==\n{{Reflist}}\n\n[[Category:Page layout]]\n[[Category:Knowledge representation]]"]
['OntoWiki', '12105194', '{{Infobox Software | name = OntoWiki\n| logo = \n| screenshot =<!-- Deleted image removed:  [[Image:OntoWiki-screenshot.jpg|180px]] -->\n| caption = \n| founder = Sören Auer\n| current maintainer = Sebastian Tramp\n| latest_release_version = 0.9.11\n| latest_release_date = January 31, 2014\n| operating_system = [[Cross-platform]]\n| programming language=[[PHP]]\n| database=[[MySQL]]\n| genre = [[knowledge management system]]\n| license = [[GNU General Public License|GPL]]\n| website = http://ontowiki.net\n}}\n\n\'\'\'OntoWiki\'\'\' is a free, [[open source software|open-source]] [[semantic wiki]] application, meant to serve as an [[ontology (computer science)|ontology]] editor and a [[knowledge management|knowledge acquisition]] system. It is a web-based application written in [[PHP]] and using either a [[MySQL]] database or a [[Virtuoso Universal Server|Virtuoso triple store]]. OntoWiki is form-based rather than syntax-based, and thus tries to hide as much of the complexity of knowledge representation formalisms from users as possible. OntoWiki is mainly being developed by the [http://aksw.org Agile Knowledge Engineering and Semantic Web (AKSW) research group] at the [[University of Leipzig]], a group also known for the [[DBpedia]] project among others, in collaboration with volunteers around the world.\n\nIn 2009 the AKSW research group got a budget of €425,000 from the [[Federal Ministry of Education and Research (Germany)|Federal Ministry of Education and Research of Germany]] for the development of the OntoWiki.<ref>[http://idw-online.de/pages/de/news300375 "OntoWiki" hilft Daten im Web zu verknüpfen] (German)</ref>\n\nIn 2010 OntoWiki became part of the technology stack supporting the [[Framework Programmes for Research and Technological Development#LOD2|LOD2]] (Linked Open Data) project. Leipzig University is one of the consortium members of the project, which is funded by a €6.5m EU grant.<ref>{{cite web |url=http://cordis.europa.eu/fetch?CALLER=PROJ_ICT&ACTION=D&CAT=PROJ&RCN=95562 |publisher=European Commission |title=CORDIS FP7 ICT Projects - LOD2 |date=2010-04-20}}</ref>\n\n==See also==\n* [[Semantic MediaWiki]]\n* [[DBpedia]]\n\n== External links ==\n* {{official website|http://OntoWiki.net}}\n* [https://github.com/AKSW/OntoWiki#ontowiki About page on GitHub]\n* [http://blog.aksw.org AKSW blog]\n\n== References ==\n<references/>\n\n{{DEFAULTSORT:Ontowiki}}\n[[Category:Semantic wiki software]]\n[[Category:Free integrated development environments]]\n[[Category:Knowledge representation]]\n[[Category:Ontology (information science)]]']
['Document classification', '1331441', '\'\'\'Document classification\'\'\' or \'\'\'document categorization\'\'\' is a problem in [[library science]], [[information science]] and [[computer science]]. The task is to assign a [[document]] to one or more [[Class (philosophy)|classes]] or [[Categorization|categories]]. This may be done "manually" (or "intellectually") or [[algorithmically]]. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.\n\nThe documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.\n\nDocuments may be classified according to their [[Subject (documents)|subjects]] or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.\n\n=="Content-based" versus "request-based" classification==\n\'\'\'Content-based classification\'\'\' is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned.<ref>Library of Congress (2008). The subject headings manual. Washington, DC.: Library of Congress, Policy and Standards Division. (Sheet H 180: "Assign headings only for topics that comprise at least 20% of the work.")</ref> In automatic classification it could be the number of times given words appears in a document.\n\n\'\'\'Request-oriented classification\'\'\' (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks himself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p.&nbsp;230<ref>Soergel, Dagobert (1985). Organizing information: Principles of data base and retrieval systems. Orlando, FL: Academic Press.</ref>).\n\nRequest-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library.  It is probably better, however, to understand request-oriented classification as \'\'policy-based classification\'\': The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.\n\n==Classification versus indexing==\nSometimes a distinction is made between assigning documents to classes ("classification") versus assigning [[Subject (documents)|subjects]] to documents ("[[subject indexing]]") but as [[Frederick Wilfrid Lancaster]] has argued, this distinction is not fruitful. "These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p.&nbsp;21<ref>Lancaster, F. W. (2003). Indexing and abstracting in theory and practice. Library Association, London.</ref>). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a [[thesaurus]] and vice versa (cf., Aitchison, 1986,<ref>Aitchison, J. (1986). "A classification as a source for thesaurus: The Bibliographic Classification of H. E. Bliss as a source of thesaurus terms and structure." Journal of Documentation, Vol. 42 No. 3, pp. 160-181.</ref> 2004;<ref>Aitchison, J. (2004). "Thesauri from BC2: Problems and possibilities revealed in an experimental thesaurus derived from the Bliss Music schedule." Bliss Classification Bulletin, Vol. 46, pp. 20-26.</ref> Broughton, 2008;<ref>Broughton, V. (2008). "A faceted classification as the basis of a faceted terminology: Conversion of a classified structure to thesaurus format in the Bliss Bibliographic Classification (2nd Ed.)." Axiomathes, Vol. 18 No.2, pp. 193-210.</ref> Riesthuis & Bliedung, 1991<ref>Riesthuis, G. J. A., & Bliedung, St. (1991). "Thesaurification of the UDC." Tools for knowledge organization and the human interface, Vol. 2, pp. 109-117. Index Verlag, Frankfurt.</ref>). Therefore, is the act of labeling a document (say by assigning a term from a [[controlled vocabulary]] to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).\n\n==Automatic document classification (ADC)==\nAutomatic document classification tasks can be divided into three sorts: \'\'\'supervised document classification\'\'\' where some external mechanism (such as human feedback) provides information on the correct classification for documents, \'\'\'unsupervised document classification\'\'\' (also known as [[document clustering]]), where the classification must be done entirely without reference to external information, and \'\'\'semi-supervised document classification\'\'\',<ref>\nRossi, R. G., Lopes, A. d. A., and Rezende, S. O. (2016). Optimization and label propagation in bipartite heterogeneous networks to improve transductive classification of texts.\nInformation Processing & Management, 52(2):217–257.\n</ref> where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.<ref>[http://www.ling.ohio-state.edu/~kbaker/Automatic_Interactive_Document_Classification.pdf An Interactive Automatic Document Classification Prototype]</ref><ref>[https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An Interactive Automatic Document Classification Prototype] {{wayback|url=https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An |date=20150424122349 }}</ref><ref>[http://www.artsyltech.com/da_classification.htmlAutomatic Document Classification - Artsyl]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref><ref>[http://www.abbyy.com/ocr_sdk_windows/what_is_new/classification/ ABBYY FineReader Engine 11 for Windows]</ref>\n\n== Techniques ==\nAutomatic document classification techniques include:\n* [[Expectation maximization]] (EM)\n* [[Naive Bayes classifier]]\n* [[tf–idf]]\n* [[Instantaneously trained neural networks]]\n* [[Latent semantic indexing]]\n* [[Support vector machines]] (SVM)\n* [[Artificial neural network]]\n* [[k-nearest neighbor algorithm|K-nearest neighbour algorithms]]\n* [[Decision tree learning|Decision trees]] such as [[ID3 algorithm|ID3]] or [[C4.5 algorithm|C4.5]]\n* [[Concept Mining]]\n* [[Rough set]]-based classifier\n* [[Soft set]]-based classifier\n* [[Multiple-instance learning]]\n* [[Natural language processing]] approaches\n\n== Applications ==\nClassification techniques have been applied to\n* [[spam filter]]ing, a process which tries to discern [[E-mail spam]] messages from legitimate emails\n* email [[routing]], sending an email sent to a general address to a specific address or mailbox depending on topic<ref>Stephan Busemann, Sven Schmeier and Roman G. Arens (2000). Message classification in the call center. In Sergei Nirenburg, Douglas Appelt, Fabio Ciravegna and Robert Dale, eds., Proc. 6th Applied Natural Language Processing Conf. (ANLP\'00), pp. 158-165, ACL.</ref>\n* [[language identification]], automatically determining the language of a text\n* genre classification, automatically determining the genre of a text<ref>{{Citation|last = Santini| first = Marina | last2 = Rosso| first2 = Mark| title = Testing a Genre-Enabled Application: A Preliminary Assessment| url = http://www.bcs.org/upload/pdf/ewic_fd08_paper7.pdf| series = BCS IRSG Symposium: Future Directions in Information Access| place = London, UK | pages= 54–63| year = 2008 }}</ref>\n* [[Readability|readability assessment]], automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger [[text simplification]] system\n* [[sentiment analysis]], determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document.\n* Article triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology.<ref>{{Cite journal\n | pmid = 18834495\n| year = 2008\n| author1 = Krallinger\n| first1 = M\n| title = Overview of the protein-protein interaction annotation extraction task of Bio \'\'Creative\'\' II\n| journal = Genome Biology\n| volume = 9 Suppl 2\n| pages = S4\n| last2 = Leitner\n| first2 = F\n| last3 = Rodriguez-Penagos\n| first3 = C\n| last4 = Valencia\n| first4 = A\n| doi = 10.1186/gb-2008-9-s2-s4\n| pmc = 2559988\n}}</ref>\n\n== See also ==\n{{colbegin}}\n* [[Categorization]]\n* [[Classification (disambiguation)]]\n* [[Compound term processing]]\n* [[Concept-based image indexing]]\n* [[Content-based image retrieval]]\n* [[Document]]\n* [[Supervised learning]], [[unsupervised learning]]\n* [[Document retrieval]]\n* [[Document clustering]]\n* [[Information retrieval]]\n* [[Knowledge organization]]\n* [[Knowledge Organization System]]\n* [[Library classification]]\n* [[Machine learning]]\n* [[Native Language Identification]]\n* [[String metrics]]\n* [[Subject (documents)]]\n* [[Subject indexing]]\n* [[Text mining]], [[web mining]], [[concept mining]]\n{{colend}}\n\n== Further reading ==\n* Fabrizio Sebastiani. [http://arxiv.org/pdf/cs.ir/0110053 Machine learning in automated text categorization]. ACM Computing Surveys, 34(1):1–47, 2002.\n* Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, 2010.\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node11.html Introduction to document classification]\n* [http://www.cs.technion.ac.il/~gabr/resources/atc/atcbib.html Bibliography on Automated Text Categorization]\n* [http://liinwww.ira.uka.de/bibliography/Ai/query-classification.html Bibliography on Query Classification]\n* [http://www.gabormelli.com/RKB/Text_Classification_Task Text Classification] analysis page\n* [http://www.nltk.org/book/ch06.html Learning to Classify Text - Chap. 6 of the book Natural Language Processing with Python] (available online)\n* [http://techtc.cs.technion.ac.il TechTC - Technion Repository of Text Categorization Datasets]\n* [http://www.daviddlewis.com/resources/testcollections/ David D. Lewis\'s Datasets]\n* [http://www.biocreative.org/tasks/biocreative-iii/ppi/ BioCreative III ACT (article classification task) dataset]\n\n[[Category:Information science]]\n[[Category:Natural language processing]]\n[[Category:Knowledge representation]]\n[[Category:Data mining]]\n[[Category:Machine learning]]']
['POSC Caesar', '23872172', '{{advert|date=September 2011}}\n\n{{Infobox Organization\n|name         = POSC Caesar Association\n|image        = POSC Caesar logo.gif\n|size         = 200\n|alt          = Logo for POSC Caesar Association.\n|caption      = Logo for POSC Caesar Association.\n|abbreviation = PCA\n|formation    = 1997-11-14\n|status       = Association\n|purpose      = Promote the development of open specifications to be used as standards for enabling the interoperability of data, software and related matters\n|location     = Bærum, Norway\n|region_served = Worldwide\n|membership   = 36\n|language     = English\n|leader_title = General Manager\n|leader_name  = Nils Sandsmark\n|main_organ   = Board of Directors\n|affiliations = <!-- if any -->\n|num_staff    =\n|num_volunteers =\n|budget       =\n|website      = http://www.posccaesar.org\n}}\n\'\'\'POSC Caesar Association\'\'\' (PCA) is an international, open, [[Non-profit organization|not-for-profit]], member organization that promotes the development of open specifications to be used as standards for enabling the interoperability of data, software and related matters.\n\nPCA is the initiator of [[ISO 15926]] "Integration of life-cycle data for process plants including oil and gas production facilities" and is committed to its maintenance and enhancement.\n\nNils Sandsmark has been the General Manager of POSC Caesar Association since 1999<ref name="BioSandsmarkDaraTech">\n{{cite web\n|url=http://www.daratech.com/plant2007/bios/sandsmark_nils.html\n|title=Biography Dr. Ing. Nils Sandsmark\n|author=Daratech\n|accessdate=2009-08-11\n}}</ref> and Thore Langeland, [[Norwegian Oil Industry Association]] ({{lang-no|Oljeindustriens Landsforening}}, OLF), is the Chairman of the Board.\n\n== History ==\n\n=== Caesar Offshore ===\nThe first predecessor of POSC Caesar Association, the \'\'\'Caesar Offshore program\'\'\', started in 1993.<ref name="PCAHis">\n{{cite web\n|url=http://www.posccaesar.org/wiki/PCA/History\n|title=History of POSC Caesar\n|author=POSC Caesar Association (PCA)\n|accessdate=2009-08-05\n}}</ref><ref name=POSCStatus98>{{cite web\n  | title = POSC/CAESAR project - Information and Status - January 1998\n  | publisher = Petrotechnical Open Software Corporation (POSC)\n  | date = 1998-03-17\n  | url = http://posc.org/caesar/caesar_info.html\n  | accessdate = 2009-08-06}}</ref><ref name=Pein>{{cite book\n  | last = Pein\n  | first = Martin\n  | title = On data models, their transformations and consistency preserving programming interfaces\n  | publisher = Books on Demand GmbH\n  | year = 2002\n  | location = Norderstedt, Germany\n  | pages = 228\n  | url = http://www.amazon.com/transformations-consistency-preserving-programming-interfaces/dp/3831139288/ref=sr_1_1?ie=UTF8&s=books&qid=1249538783&sr=1-1\n  | isbn = 3-8311-3928-8}}</ref><ref>{{cite journal\n  | title = Data warehouse manages offshore project information\n  | journal = Oil & Gas Journal\n  | volume = 96\n  | issue = 18\n  | pages = 94\n  | url = http://www.ogj.com/index/current-issue/s-oil-gas-journal/s-volume-96/s-issue-18.html\n  | publisher = Pennwell Corporation\n  | location = Tulsa, OK\n  | date = 1998-05-04\n  | issn = 0030-1388\n}}</ref><ref name=Stanley>{{cite web\n  | last = Port\n  | first = Stanley\n  | title = Plant Information Management at Statoil Norway\n  | date = 1998-04-13\n  | url = http://www.hydrocarbononline.com/article.mvc/Plant-Information-Management-at-Statoil-Norwa-0001\n  | accessdate = }}</ref>\nThe original focus was on standardizing technical data definitions for capital intensive projects at the handover from the [[EPC (contract)|EPC]] contractor to the owner/operators of onshore and offshore oil and gas production facilities. The program was sponsored by [[The Research Council of Norway]], two [[EPC (contract)|EPC]] contractors ([[Aker Maritime]] and [[Kværner]]), three owners/operators ([[Norsk Hydro]], [[Saga Petroleum]] and [[Statoil]]) and [[DNV]] as service provider and project owner.\n\n=== POSC Caesar project ===\nDuring the period 1994-96, Caesar Offshore Program was defined as a project of [[POSC|Petrotechnical Open Software Corporation (POSC)]] (now [[Energistics]]), and changed its name to the \'\'\'POSC Caesar Project\'\'\'.<ref name="PCAHis"/><ref name=POSCStatus98/><ref name="Pein"/><ref name="Stanley"/>\n\nIn 1995 the project was joined by [[BP]], [[Brown and Root]] and [[Elf Aquitaine]] and in 1997 by [[Intergraph]], [[IBM]], [[Oracle Corporation|Oracle]], [[Lloyd\'s]], [[Royal Dutch Shell|Shell]], [[ABB Group|ABB]] and [http://www.umoe.no/ UMOE Technologies].<ref name=POSCStatus98/>\n\nDuring that time, POSC Caesar also became a member of European Process Industries STEP Technical Liaison Executive (EPISTLE) where it collaborates with PISTEP (UK), and USPI-NL (The Netherlands) on the development of [[ISO 10303]], also known as "Standard for the Exchange of Product model data (STEP)."\n\n=== POSC Caesar Association ===\nIn 1997, POSC Caesar Association was founded as an independent, global, non-profit, member organization. POSC Caesar Association serves an international membership and collaborates with other international organizations. It has its main office in Norway.\n\nAlbeit the name of POSC Caesar Association still hints to its past as a project within the [[POSC|Petrotechnical Open Software Corporation (POSC)]] (now [[Energistics]]<ref name="POSCEnergistics">\n{{cite web\n|url=http://www.energistics.org/posc/NewsBot.asp?MODE=VIEW&ID=606&SnID=2\n|title=POSC Rebrands as Energistics - Press release\n|author=Energistics\n|accessdate=2009-09-01\n|date=2006-11-09\n}}</ref>), from 1997 onwards, the organization has been independent. Energistics and POSC Caesar Association do collaborate, and are formally member in each other\'s organization.<ref name="EnergisticsMembers">\n{{cite web\n|url=http://www.energistics.org//assnfe/companydirectory.asp?MODE=FINDRESULTS&SEARCH_TYPE=1&COMPANY_TYPE=2,3,6&SnID=1941691677\n|title=Energistics Member Directory\n|author=Energistics\n|accessdate=2009-08-11\n}}</ref><ref name="PCAMembers">\n{{cite web\n|url=http://www.posccaesar.org/wiki/PCA/Membership\n|title=POSC Caesar Association Membership\n|author=POSC Caesar Association\n|accessdate=2009-08-11\n}}</ref>\n\n== Membership ==\nPOSC Caesar Association has with its current 36 members<ref name="PCAMem">\n{{cite web\n|url=http://www.posccaesar.org/wiki/PCA/Membership\n|title=POSC Caesar Association - Membership\n|author=POSC Caesar Association (PCA)\n|accessdate=2009-08-13\n}}</ref> from around the world established an international footprint (with a strong membership in Norway) that includes a wide range from academia, solution providers to engineering contractors and owners/operators. The members are (subdivided by organization type):\n* Associations: Energistics (USA) and [[Norwegian Oil Industry Association|The Norwegian Oil Industry Association (OLF, Norway)]],\n* [[Universities]] and [[Research institute|Research Institutes]]: International Research Institute of Stavanger (IRIS, Norway), [[Norwegian University of Science and Technology|Norwegian University of Science and Technology (NTNU, Norway)]], [[KAIST|Korea Advanced Institute of Science and Technology (KAIST, Korea)]], [[SINTEF|SINTEF (Norway)]], [[University of Bergen|University of Bergen (Norway)]], [[University of Oslo|University of Oslo (Norway)]], [[University of Stavanger|University of Stavanger (Norway)]], [[University of Tromsø|University of Tromsø (Norway)]] and Western Norway Research Institute (Norway),\n* [[Petroleum Industry|Oil and Gas Companies]]: [[BP|BP (UK)]], [[Petronas|Petronas (Malaysia)]] and [[Statoil|Statoil (Norway)]],\n* Engineering contractors and consultants:  Akvaplan-niva (Norway), [[Aker Solutions|Aker Solutions (Norway)]], Asset Life Cycle Information Management (ALCIM, Malaysia), CAESAR systems (USA), [[Bechtel|Bechtel (USA)]], [[Det Norske Veritas|Det Norske Veritas (DNV, Norway)]], Information Logic (USA) and iXIT Engineering Technology (Germany), Phusion IM Ltd (UK).<ref>https://www.posccaesar.org/wiki/PCA/Membership</ref>\n* Solution providers: [[Aveva|Aveva (UK)]], [[Bentley Systems|Bentley Systems (USA)]], Jotne EPM Technology (Norway), Epsis (Norway), Eurostep (Sweden), [[IBM|International Business Machines Corporation (IBM, USA)]], Siemens - Comos Industry Solutions (before Innotec) (Germany), [[Intergraph|Intergraph (USA)]], Invenia (Norway), Keel Solution (Denmark), Noumenon (UK), NRX (Canada), Octaga (Norway) and Tektonisk (Norway).\n\nIn general, the organization holds three membership meetings a year;<ref name="PCAAgenda">{{cite web|url=http://www.posccaesar.org/wiki/PCA/Agenda |title=POSC Caesar Association - Agenda |author=POSC Caesar Association (PCA) |accessdate=2009-08-13 }}{{dead link|date=June 2016|bot=medic}}{{cbignore|bot=medic}}</ref> one in January / February in North-America (typically USA), one in April / May in Europe (typically Norway) and one in October in Asia (typically Malaysia).\n\n== Activities and services ==\n\n=== Initiator and custodian of ISO 15926 ===\n\nIn consultation with the other EPISTLE members and the [[International Organization for Standardization|International Organization for Standardization (ISO)]], it was decided in 2003 (some say already in 1997{{Citation needed|date=August 2009}}) that for modeling-technical reasons it was better to discontinue the development of [[ISO 10303]]<ref name="SC4Legacy">\n{{cite web\n|url=http://www.tc184-sc4.org/SC4_Open/SC4%20Legacy%20Products%20(2001-08)/STEP_(10303)/\n|title=ISO - SC4- Legacy products - STEP (ISO 10303)\n|author=ISO\n|accessdate=2009-08-05\n}}</ref> and to initiate the development of [[ISO 15926]] "Integration of life-cycle data for process plants including oil and gas production facilities."\n\nOver the years, the scope of the standard has increased from the initial capital-intensive projects in the [[Upstream (oil industry)|upstream oil and gas industry]], to include also relevant terminology for [[Downstream (oil industry)|downstream oil and gas industry]] applications and to deal with real-time data related to the actual oil and gas production.\n\n[[ISO 15926]] has also over the years evolved from a dictionary (a list of terms with definitions), over a [[Taxonomy (general)|taxonomy]] (added hierarchy) to an [[Ontology (information science)|ontology]] (a formal representation of a set of concepts within a domain and the relationships between those concepts). [[ISO 15926]] is therefore sometimes nicknamed the "Oil and Gas Ontology.",<ref name="OLFIOOntology">\n{{cite web\n|url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf\n|title=Integrated Operations and the Oil & Gas Ontology\n|author=[[Norwegian Oil Industry Association]] (OLF)\n|accessdate=2009-08-05\n}}</ref> for some considered to be an essential prerequisite together with [[Semantic Web]] technologies<ref name="W3C">\n{{cite web\n|url=http://www.w3.org/2008/12/ogws-report\n|title=W3C workshop on Semantic Web in Oil and Gas Industry - Report\n|author=[[W3C]]\n|date=2009-01-13\n|accessdate=2009-08-05\n}}</ref>\nto get to better interoperability, an optimal use of all available data across boundaries and an increase in efficiency. This is what some call the next generation of [[Integrated Operations]].<ref name="OLFIOOntology"/>\n\n=== Reference data services ===\nPlaceholders:\n* Flow scheme of WIP - RDS - ISO and role of SIGs\n* RDS\n* Standards in database pilot (ISO)\n\n=== Special interest groups ===\nPlaceholders:\n* Overview of SIGs\n* Drilling and Completion\n* Reservoir and Production\n* Operations and Maintenance\n\n== Projects ==\nThere are a number of projects (co-)organized by POSC Caesar Association working on the extension of the [[ISO 15926]] standard in different application areas.\n\n=== Capital intensive projects application domain ===\nThe following projects are running at the moment (August 2009):\n\n* The ADI Project of FIATECH, to build the tools (which will then be made available in the public domain)\n* The IDS Project of POSC Caesar Association, to define product models required for data sheets\n* A joint collaboration project between FIATECH POSC Caesar Association is the ADI-IDS project is the [[ISO 15926 WIP]]\n\n=== Upstream oil and gas industry application domain ===\nThe following projects are currently running (August 2009):\n\n* The [[Integrated Operations in the High North|Integrated Operations in the High North (IOHN)]] project is working on extending ISO 15926 to handle real-time data transmission and (pre-)processing to enable the next generation of [[Integrated operations|Integrated Operations]].<ref>\n{{cite web\n|url=http://www.olf.no/news/norway-takes-a-leading-role-in-next-generation-integrated-operations-article18586-291.html\n|title=Norway takes a leading role in next generation Integrated Operations\n|author=The [[Norwegian Oil Industry Association]] (OLF)\n|accessdate=2009-08-05\n|date=2008-08-26\n}}</ref><ref name="Rigzone">\n{{cite web\n|url=http://www.rigzone.com/news/article.asp?a_id=65883\n|title=Norway Takes Reign to Provide Next Generation Integrated Operations\n|author=Rigzone E&P News\n|accessdate=2009-08-05\n|date=2008-08-26\n}}</ref><ref name="DEJ">\n{{cite web\n|url=http://www.digitalenergyjournal.com/displaynews.php?NewsID=758&PHPSESSID=9hhp6qe4qqpi7qnbgffgqhv1h7\n|title=Norway developing next generation Integrated Operations\n|author=Digital Energy Journal\n|accessdate=2009-08-05\n|date=2008-08-27\n}}</ref><ref name="EPMag">\n{{cite web\n|url=http://www.epmag.com/Magazine/2008/12/item24047.php\n|title=Offshore R&D pushes the limits\n|author=E&P Magazine\n|accessdate=2009-08-05\n|date=2008-12-02\n}}</ref>\n* The Environment Web project to include environmental reporting terms and definitions as used in EPIM\'s EnvironmentWeb in ISO 15926.\n\nFinalised projects include:\n\n* The Integrated Information Platform (IIP) project working on establishing a real-time information pipeline based on open standards. It worked among others on:\n** Daily Drilling Report (DDR) to including all terms and definitions in ISO 15926. This standard became mandatory on February 1, 2008<ref>{{cite web |url=http://www.npd.no/English/Produkter+og+tjenester/Skjemaer/CDRS_reporting_oct_2007.htm |title=Drilling Reporting to the authorities |author=Norwegian Petroleum Directorate |accessdate=2009-08-05 }}</ref> for reporting on the [[Norwegian Continental Shelf]] by the [[Norwegian Petroleum Directorate|Norwegian Petroleum Directorate (NPD)]] and Safety Authority Norway (PSA). NPD says that the quality of the reports has improved considerably since.\n** Daily Production Report (DPR) to including all terms and definitions in ISO 15926. This standard was tested successfully on the [[Valhall oil field|Valhall]] ([[BP]]-operated) and Åsgard ([[StatoilHydro]]-operated) fields offshore Norway. The terminology and XML schemata developed have also been included in [http://www.Energistics.org Energistics]’ [[PRODML]] standard.\n\n== Conferences and events ==\n\n=== Semantic Days ===\n{{Empty section|date=January 2011}}\n\n=== Sogndal academic network meeting ===\n{{Empty section|date=January 2011}}\n\n== Collaborations ==\n\nPOSC Caesar is collaborating with a number of standardization bodies,<ref name="PCAColl">\n{{cite web\n|url=http://www.posccaesar.org/wiki/PCA/Collaboration\n|title=POSC Caesar - Collaboration\n|author=POSC Caesar Association (PCA)\n|accessdate=2009-08-05\n}}</ref> including:\n* Mimosa: collaboration on open information standards for Operations and Maintenance mainly for the [[Downstream (oil industry)|downstream oil and gas industry]],\n* FIATECH: collaboration on open information standards for life cycle data of capital projects<ref name="IDSADI">\n{{cite web\n|url=http://www.posccaesar.org/wiki/IdsAdiProject\n|title=POSC Caesar / FIATECH IDS-ADI Projects\n|author=FIATECH & POSC Caesar Association\n|accessdate=2009-08-05\n}}</ref><ref name="iRing">\n{{cite press release\n|url=http://fiatech.org/press-releases/364-iring-version100.html\n|title=iRING Version 1.0.0 Available Now\n|publisher=FIATECH & POSC Caesar Association\n|accessdate=2009-08-05\n|date=2009-06-05\n}}</ref>\n* Energistics: collaboration on information standards for the [[Upstream (oil industry)|upstream oil and gas industry]], including [[WITSML]] and [[PRODML]]\n* OASIS: collaboration on e-business standards,\n* [[ISO TC 184/SC 4|ISO TC184/SC4]]: the host of the ISO 15926 standard.\n\n== See also ==\n* [[ISO 15926]]\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* [http://www.posccaesar.org/ POSC Caesar Association] website\n\n[[Category:Semantic Web]]\n[[Category:Knowledge engineering]]\n[[Category:Information science]]\n[[Category:Ontology (information science)]]\n[[Category:Knowledge representation]]\n[[Category:Standards organizations]]']
['Thesaurus', '30334', '{{about|thesauri for general or literary applications|thesauri designed for information retrieval|Thesaurus (information retrieval)|the Clare Fischer album|Thesaurus (album)}}\n[[File:Historical Thesaurus.jpg|thumb|\'\'Historical Thesaurus of the Oxford English Dictionary\'\', two-volume set]]\nIn general usage, a \'\'\'thesaurus\'\'\' is a [[reference work]] that lists words grouped together according to similarity of meaning (containing [[synonyms]] and sometimes [[antonyms]]), in contrast to a [[dictionary]], which provides [[definitions]] for words, and generally lists them in alphabetical order. The main purpose of such reference works is to help the user "to find the word, or words, by which [an] idea may be most fitly and aptly expressed"&nbsp;– to quote [[Peter Mark Roget]], architect of the best known thesaurus in the English language.<ref name="Roget">Roget, Peter. 1852. \'\'Thesaurus of English Language Words and Phrases\'\'.</ref>\n\nAlthough including synonyms, a thesaurus should not be taken as a complete list of all the synonyms for a particular word. The entries are also designed for drawing distinctions between similar words and assisting in choosing exactly the right word. Unlike a [[dictionary]], a thesaurus entry does not give the definition of words.\n\nIn [[library science]] and [[information science]], thesauri have been widely used to specify domain models.  Recently, thesauri have been implemented with [[Simple Knowledge Organization System]] (SKOS).{{citation needed|date=January 2016}}\n\n== Etymology ==\nThe word "thesaurus" is derived from 16th-century [[New Latin]], in turn from [[Latin]] \'\'[[wikt:en:thesaurus#Latin|thēsaurus]]\'\', which is the [[Latinisation (literature)|Latinisation]] of the [[Ancient Greek|Greek]] {{lang|grc|[[wikt:en:θησαυρός#Ancient Greek|θησαυρός]]}} (\'\'thēsauros\'\'), "treasure, treasury, storehouse".<ref name="Harper">[http://www.etymonline.com/index.php?term=thesaurus "thesaurus"]. \'\'[[Online Etymology Dictionary]]\'\'.</ref> The word \'\'thēsauros\'\' is of uncertain etymology. [[Douglas Harper]] derives it from the root of the Greek verb τιθέναι \'\'tithenai\'\', "to put, to place."<ref name="Harper" /> [[Robert S. P. Beekes|Robert Beekes]] rejected an [[Proto-Indo-European language|Indo-European]] derivation and suggested a [[Pre-Greek]] suffix {{nowrap|\'\'*-ar<sup>w</sup>o-\'\'}}.<ref>[[Robert S. P. Beekes|R. S. P. Beekes]], \'\'Etymological Dictionary of Greek\'\', Brill, 2009, p. 548.</ref>\n\nFrom the 16th to the 19th centuries, the term "thesaurus" was applied to any [[dictionary]] or [[encyclopedia]], as in the \'\'[[Thesaurus linguae latinae]]\'\' (1532), and the \'\'[[Thesaurus linguae graecae]]\'\' (1572). The meaning "collection of words arranged according to sense" is first attested in 1852 in Roget\'s title and \'\'thesaurer\'\' is attested in [[Middle English]] for "[[treasurer]]".<ref name="Harper" />\n\n== History ==\n[[File:Roget P M.jpg|150px|right|thumb|[[Peter Mark Roget]], author of the first thesaurus.]]\nIn antiquity, [[Philo of Byblos]] authored the first text that could now be called a thesaurus. In [[Sanskrit]], the [[Amarakosha]] is a thesaurus in verse form, written in the 4th century.\n\nThe first modern thesaurus was \'\'[[Roget\'s Thesaurus]]\'\', first compiled in 1805 by [[Peter Mark Roget]], and published in 1852. Since its publication it has never been out of print and is still a widely used work across the English-speaking world.<ref>http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199254729.001.0001/acprof-9780199254729-chapter-1</ref> Entries in \'\'Roget\'s Thesaurus\'\' are listed conceptually rather than alphabetically.\nRoget described his thesaurus in the foreword to the first edition:\n\n<blockquote>It is now nearly fifty years since I first projected a system of verbal classification similar to that on which the present work is founded. Conceiving that such a compilation might help to supply my own deficiencies, I had, in the year 1805, completed a classed catalogue of words on a small scale, but on the same principle, and nearly in the same form, as the Thesaurus now published.<ref>Lloyd 1982, p. xix{{Full citation needed|date=August 2014}}</ref>\n</blockquote>\n\n== See also ==\n* [[AGRIS]]\n* [[Controlled vocabulary]]\n* [[Knowledge Organization Systems]]\n* [[Ontology (computer science)]]\n* [[Simple Knowledge Organisation System]]\n* [[ISO 25964]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{Wiktionary-inline|thesaurus}}\n\n{{Lexicography}}\n\n[[Category:Thesauri| ]]\n[[Category:Information science]]\n[[Category:Knowledge representation]]\n[[Category:Reference works]]\n[[Category:Dictionaries by type]]\n[[Category:Lexical semantics]]']
['Composite Capability/Preference Profiles', '4476270', '\'\'\'Composite Capability/Preference Profiles\'\'\' (\'\'\'CC/PP\'\'\') is a specification for defining capabilities and preferences (also known as \'delivery context\') of [[user agents]]. CC/PP is a [[vocabulary extension]] of the [[Resource Description Framework]] (RDF). Delivery context can be used to guide the process of tailoring content for a [[user agent]].\n\nThe CC/PP specification is maintained by the [[World Wide Web Consortium|W3C]]\'s [[UWAWG|Ubiquitous Web Applications Working Group (UWAWG)]] Working Group.\n\n== History ==\n* Composite Capability/Preference Profiles (CC/PP): Structure and Vocabularies 1.0 became a W3C recommendation on 15 January 2004.\n* A "Last-Call Working-Draft" of CC/PP 2.0 was issued in April 2007\n\n== See also ==\n* [[Resource Description Framework|Resource Description Framework (RDF)]]\n* [[UAProf|User Agent Profile (UAProf)]]\n* [[WURFL|Wireless Universal Resource File (WURFL)]]\n\n== External links ==\n* [http://www.w3.org/Mobile/CCPP/ W3C CC/PP Information Page]\n* [http://www.w3.org/TR/CCPP-struct-vocab2/ Newest version of CC/PP: Structure and Vocabularies]\n* [http://www.w3.org/TR/2004/REC-CCPP-struct-vocab-20040115/ Composite Capability/Preference Profiles (CC/PP): Structure and Vocabularies 1.0]\n* [http://www.w3.org/2001/di/ W3C Device Independence working group]\n* [http://www.w3.org/2003/07/ccpp-SV-PR/test-suite/ CC/PP: Structure and Vocabularies Test Suite]\n* [http://www.w3.org/2003/07/ccpp-SV-PR/test-suite-20030827/implementation-report.html CC/PP: Structure and Vocabularies Implementation Report]\n* [http://www.w3.org/Consortium/Offices/Presentations/CCPP/ CC/PP presentation]\n* [http://java.sun.com/j2ee/ccpp/ Sun J2EE CC/PP Processing Tools]\n\n{{DEFAULTSORT:Composite Capability Preference Profiles}}\n[[Category:Knowledge representation]]\n[[Category:World Wide Web Consortium standards]]']
['Type–token distinction', '14934822', '[[File:Flock of birds at Rome.ogg|right|300px|thumb|Although this flock is made of the same \'\'type\'\' of bird, each individual bird is a different \'\'token\'\'. (50 MB video of a [[Flock (birds)|flock of birds]] in Rome)|thumbtime=6]]\nThe \'\'\'type–token distinction\'\'\' is used in disciplines such as [[logic]], [[linguistics]], [[metalogic]], [[typography]], and [[computer programming]] to clarify what words mean.\n\nThe sentence "\'\'they drive the same car\'\'" is ambiguous. Do they drive the same \'\'type\'\' of car (the same model) or the same instance of a car type (a single vehicle)? Clarity requires us to distinguish words that represent abstract types from words that represent objects that embody or exemplify types. The type–token distinction separates types (representing abstract descriptive concepts) from tokens (representing objects that instantiate concepts).\n\nFor example: "bicycle" is a type that represents the concept of a bicycle; whereas "my bicycle" is a token that represents an object that instantiates that type. In the sentence "the bicycle is becoming more popular" the word "bicycle" is a type representing a concept; whereas in the sentence "the bicycle is in the garage" the word "bicycle" is a token representing a particular object.\n\n(The distinction in [[computer programming]] between [[class (software)|classes]] and [[object (computer science)|objects]] is related, though in this context, "class" sometimes refers to a set of objects (with class-level attribute or operations) rather than a description of an object in the set.)\n\nThe words type, concept, property, quality, feature and attribute (all used in describing things) tend to be used with different verbs. E.g. Suppose a rose bush is defined as a plant that is "thorny", "flowering" and "bushy". You might say a rose bush \'\'instantiates\'\' these three types, or \'\'embodies\'\' these three concepts, or \'\'exhibits\'\' these three properties, or \'\'possesses\'\' these three qualities, features or attributes.\n\nProperty types (e.g "height in metres" or "thorny") are often understood [[ontology|ontologically]] as concepts. Property instances (e.g. height = 1.74) are sometimes understood as measured values, and sometimes understood as sensations or observations of reality.\n\nSome say types exist in descriptions of objects, but not as tangible [[physical object]]s. They say one can show someone a particular bicycle, but cannot show someone the type "bicycle", as in "\'\'the bicycle\'\' is popular.". However types do exist in sense that they appear in mental and documented models.\n\nSome say tokens represent objects that are tangible, exist in space and time as physical matter and/or energy. However, tokens can represent intangible objects of types such as "thought", "tennis match", "government" and "act of kindness".\n\n== Occurrences ==\nThere is a related distinction very closely connected with the type-token distinction. This distinction is the distinction between an object, or type of object, and an \'\'\'\'\'occurrence\'\'\'\'\' of it. In this sense, an occurrence is not necessarily a token. Considering the sentence: "[[A rose is a rose is a rose]]". We may equally correctly state that there are eight or three words in the sentence. There are, in fact, three word types in the sentence: "rose", "is" and "a". There are eight word tokens in a token copy of the line. The line itself is a type. There are not eight word types in the line. It contains (as stated) only the three word types, \'a\', \'is\' and \'rose\', each of which is unique. So what do we call what there are eight of? They are occurrences of words. There are three occurrences of the word type \'a\', two of \'is\' and three of \'rose\'.\n\nThe need to distinguish tokens of types from occurrences of types arises, not just in linguistics, but whenever types of things have other types of things occurring in them.<ref>Stanford Encyclopedia of Philosophy, \'\'[http://plato.stanford.edu/entries/types-tokens Types and Tokens]\'\'</ref> Reflection on the simple case of occurrences of [[numeral]]s is often helpful.{{citation needed|date=January 2017|reason=Claim made with no support or citation, term used without definition.}}\n\n== Typography ==\nIn [[typography]], the type–token distinction is used to determine the presence of a text printed by [[movable type]]:<ref>[[Herbert E. Brekle|Brekle, Herbert E.]]: \'\'Die Prüfeninger Weiheinschrift von 1119. Eine paläographisch-typographische Untersuchung\'\', Scriptorium Verlag für Kultur und Wissenschaft, Regensburg 2005, ISBN 3-937527-06-0, p.&nbsp;23</ref>\n\n{{quote|The defining criteria which a typographic print has to fulfill is that of the type identity of the various [[letter form]]s which make up the printed text. In other words: each letter form which appears in the text has to be shown as a particular instance ("token") of one and the same type which contains a reverse image of the printed [[Letter (alphabet)|letter]].}}\n\n== Charles Sanders Peirce ==\n:\'\'There are only 26 letters in the [[English alphabet]] and yet there are more than 26 letters in this [[sentence (linguistics)|sentence]]. Moreover, every time a child writes the alphabet 26 new letters have been created.\'\'\n\nThe word \'letters\' was used three times in the above paragraph, each time in a different meaning. The word \'letters\' is one of many words having "type–token ambiguity". This section disambiguates \'letters\' by separating the three senses using terminology standard in logic today. The key distinctions were first made by the American logician-philosopher [[Charles Sanders Peirce]] in 1906 using terminology that he established.<ref>Charles Sanders Peirce, Prolegomena to an apology for pragmaticism, Monist, vol.16 (1906), pp. 492–546.</ref>\n\nThe letters that are created by writing are physical objects that can be destroyed by various means: these are letter TOKENS or letter INSCRIPTIONS. The 26 letters of the alphabet are letter TYPES or letter FORMS.\n\n\'\'\'Peirce\'s type–token distinction\'\'\', also applies to words, sentences, paragraphs, and so on: to anything in a universe of discourse of character-string theory, or [[concatenation theory]]. There is only one word type spelled el-ee-tee-tee-ee-ar,<ref>Using a variant of [[Alfred Tarski]]\'s structural-descriptive naming found in [[John Corcoran (logician)|John Corcoran]] , Schemata: the Concept of Schema in the History of Logic, Bulletin of Symbolic Logic, vol. 12 (2006), pp. 219–40.</ref> namely, \'letter\'; but every time that word type is written, a new word token has been created.\n\nSome logicians consider a word type to be the class of its tokens. Other logicians counter that the word type has a permanence and constancy not found in the class of its tokens. The type remains the same while the class of its tokens is continually gaining new members and losing old members.\n\nThe word type \'letter\' uses only four letter types: el, ee, tee, and ar. Nevertheless, it uses ee twice and tee twice. In standard terminology, the word type \'letter\' has six letter OCCURRENCES and the letter type ee OCCURS twice in the word type \'letter\'. Whenever a word type is inscribed, the number of letter tokens created equals the number of letter occurrences in the word type.\n\nPeirce\'s original words are the following.\n"A common mode of estimating the amount of matter in a ... printed book is to count the number of words. There will ordinarily be about twenty \'thes\' on a page, and, of course, they count as twenty words. In another sense of the word \'word,\' however, there is but one word \'the\' in the English language; and it is impossible that this word should lie visibly on a page, or be heard in any voice .... Such a ... Form, I propose to term a Type. A Single ... Object ... such as this or that word on a single line of a single page of a single copy of a book, I will venture to call a Token. .... In order that a Type may be used, it has to be embodied in a Token which shall be a sign of the Type, and thereby of the object the Type signifies." – Peirce 1906, Ogden-Richards, 1923, 280-1.\n\nThese distinctions are subtle but solid and easy to master. This section ends using the new terminology to disambiguate the first paragraph.\n\n:\'\'There are 26 letter types in the English alphabet and yet there are more than 26 letter occurrences in this sentence type. Moreover, every time a child writes the alphabet 26 new letter tokens have been created.\'\'\n\n== See also ==\n* [[Formalism (philosophy)]]\n* [[Is-a]]\n* [[Class (philosophy)]]\n* [[Type theory]]\n* [[Type physicalism]]\n* [[Mental model]]\n* [[Map–territory relation]]\n* [[Problem of universals#Peirce]]\n\n==References==\n{{reflist}}\n\n===Sources===\n*Baggin J., and Fosl, P. (2003) \'\'The Philosopher\'s Toolkit\'\'. Blackwell: 171-73. ISBN 978-0-631-22874-5.\n*Peper F., Lee J., Adachi S.,Isokawa T. (2004) \'\'Token-Based Computing on Nanometer Scales\'\', Proceeding of the ToBaCo 2004 Workshop on Token Based Computing, Vol.1 pp.&nbsp;1–18.\n\n== External links ==\n*[[The Stanford Encyclopedia of Philosophy]]: "[http://plato.stanford.edu/entries/types-tokens/ Types and Tokens]" by Linda Wetzel.\n\n{{Metalogic}}\n{{Metaphysics}}\n\n{{DEFAULTSORT:Type-token distinction}}\n[[Category:Metalogic]]\n[[Category:Conceptual distinctions]]\n[[Category:Knowledge representation]]\n[[Category:Abstraction]]\n[[Category:Concepts in metaphysics]]\n[[Category:Articles containing video clips]]\n[[Category:Philosophy of logic]]\n[[Category:Philosophy of language]]\n[[Category:Linguistics]]']
['Retrievability', '39585214', "'''Retrievability''' is a term associated with the ease with which information can be found or retrieved using an information system, specifically a [[search engine]] or [[information retrieval]] system.\n\nA document (or information object) has high retrievability if there are many queries which retrieve the document via the search engine, and the document is ranked sufficiently high that a user would encounter the document. Conversely, if there are few queries that retrieve the document, or when the document is retrieved the documents are not high enough in the ranked list, then the document has low retrievability.\n\nRetrievability can be considered as one aspect of [[findability]].\n\nApplications of retrievability include detecting [[Web search engine#Search engine bias|search engine bias]], measuring algorithmic bias, evaluating the influence of search technology, tuning information retrieval systems and evaluating the quality of documents in a [[text corpus|collection]].\n\n==See also==\n* [[Information retrieval]]\n* [[Knowledge mining]]\n* [[Search engine optimization]]\n* [[Findability]]\n\n==References==\n*{{cite book|author1=Azzopardi, L.  |author2=Vinay, V.  |lastauthoramp=yes | chapter=Retrievability: an evaluation measure for higher order information access tasks| title=Proceedings of the 17th ACM conference on Information and knowledge management| year=2008| pages=561–570| publisher=ACM| location=Napa Valley, California, USA| series=CIKM '08| doi=10.1145/1458082.1458157| url=http://doi.acm.org/10.1145/1458082.1458157| accessdate=5 June 2013}}\n*{{cite book|author1=Azzopardi, L.  |author2=Vinay, V.  |lastauthoramp=yes | chapter=Accessibility in information retrieval| title=Proceedings of the IR research, 30th European conference on Advances in information retrieval| year=2008| pages=482–489| publisher=Springer| location=Glasgow,UK| series=ECIR '08| url=http://dl.acm.org/citation.cfm?id=1793333| accessdate=7 Dec 2016}}\n\n[[Category:Web design]]\n[[Category:Knowledge representation]]\n[[Category:Information science]]"]
['Transaction logic', '12817496', '\'\'\'Transaction Logic\'\'\' is an extension of [[predicate logic]] that accounts in a clean and declarative way for the phenomenon of state changes in [[logic program]]s and [[database]]s. This extension adds connectives specifically designed for combining simple actions into complex transactions and for providing control over their execution. The logic has a natural [[model theory]] and a sound and complete [[proof theory]]. Transaction Logic has a [[Horn clause]] subset, which has a procedural as well as a declarative semantics. The important features of the logic include hypothetical and committed updates, dynamic constraints on transaction execution, non-determinism, and bulk updates.  In this way, Transaction Logic is able to declaratively capture a number of non-logical phenomena, including [[procedural knowledge]] in [[artificial intelligence]], [[active database]]s, and methods with side effects in [[object database]]s.\n\nTransaction Logic was originally proposed in <ref name="tr-iclp1993">A.J. Bonner and M. Kifer (1993), \'\'Transaction Logic Programming\'\', International Conference on Logic Programming (ICLP), 1993.</ref> by [http://www.cs.toronto.edu/~bonner/ Anthony Bonner] and [http://www.cs.stonybrook.edu/~kifer/ Michael Kifer] and later described in more detail in <ref>A.J. Bonner and M. Kifer (1994), \'\'An Overview of Transaction Logic\'\', Theoretical Computer Science, 133:2, 1994.</ref> and.<ref>A.J. Bonner and M. Kifer (1998), [http://www.cs.sunysb.edu/~kifer/TechReports/tr-chomicki.pdf \'\'Logic Programming for Database Transactions\'\'] in Logics for Databases and Information Systems, J. Chomicki and G. Saake (eds.), Kluwer Academic Publ., 1998.</ref> The most comprehensive description appears in.<ref>A.J. Bonner and M. Kifer (1995), [http://www.cs.sunysb.edu/~kifer/TechReports/transaction-logic.pdf \'\'Transaction Logic Programming (or A Logic of Declarative and Procedural Knowledge)\'\']. Technical Report CSRI-323, November 1995, Computer Science Research Institute, University of Toronto.</ref>\n\nIn later years, Transaction Logic was extended in various ways, including [[concurrency]]{{dn|date=November 2016}},<ref name="concurrentTR">A.J. Bonner and M. Kifer (1996), [http://www.cs.sunysb.edu/~kifer/TechReports/concurrent-trans-logic.pdf \'\'Concurrency and communication in Transaction Logic\'\'], Joint Intl. Conference and Symposium on Logic Programming, Bonn, Germany, September 1996</ref> [[defeasible reasoning]],<ref>P. Fodor and M. Kifer (2011), [http://drops.dagstuhl.de/opus/volltexte/2011/3159/ \'\'Transaction Logic with Defaults and Argumentation Theories\'\']. In Technical communications of the 27th International Conference on Logic Programming (ICLP), July 2011.</ref> partially defined actions,<ref>M. Rezk and M. Kifer (2012), [http://link.springer.com/article/10.1007%2Fs13740-012-0007-8 \'\'Transaction Logic with Partially Defined Actions\'\']. Journal on Data Semantics, August 2012, vol. 1, no. 2, Springer.</ref> and other features.<ref>H. Davulcu, M. Kifer and I.V. Ramakrishnan (2004), [http://www.www2004.org/proceedings/docs/2p144.pdf CTR-S: A Logic for Specifying Contracts in Semantic Web Services\'\']. Proceedings of the 13-th World Wide Web Conference (WWW2004), May 2004.</ref><ref>P. Fodor and M. Kifer (2010), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.6968 \'\'Tabling for Transaction Logic\'\']. In Proceedings of the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming (PPDP), July 2010.</ref>\n\nIn 2013, the original paper on Transaction Logic <ref name="tr-iclp1993"/> has won the 20-year Test of Time Award as the most influential paper from the proceedings of [http://www.informatik.uni-trier.de/~ley/db/conf/iclp/iclp93.html ICLP 1993 conference] in the preceding 20 years.{{citation needed|date=February 2014}}\n\n== Examples ==\n\nGraph coloring. Here <tt>tinsert</tt> denotes the elementary update operation of \'\'transactional insert\'\'. The connective ⊗ is called \'\'serial conjunction\'\'.\n colorNode <-  // color one node correctly\n     node(N) ⊗ &neg; colored(N,_) ⊗ color(C)\n     ⊗ ¬(adjacent(N,N2) ∧ colored(N2,C))\n     ⊗ tinsert(colored(N,C)).\n colorGraph <- ¬uncoloredNodesLeft.\n colorGraph <- colorNode ⊗ colorGraph.\n\nPyramid stacking. The elementary update <tt>tdelete</tt> represents the \'\'transactional delete\'\' operation.\n stack(N,X) <- N>0 ⊗ move(Y,X) ⊗ stack(N-1,Y).\n stack(0,X).\n move(X,Y) <- pickup(X) ⊗ putdown(X,Y).\n pickup(X) <- clear(X) ⊗ on(X,Y) ⊗\n              ⊗ tdelete(on(X,Y)) ⊗ tinsert(clear(Y)).\n putdown(X,Y) <-  wider(Y,X) ⊗ clear(Y) \n                  ⊗ tinsert(on(X,Y)) ⊗ tdelete(clear(Y)).\n\nHypothetical execution. Here <tt>&lt;&gt;</tt> is the modal operator of possibility: If both <tt>action1</tt> and <tt>action2</tt> are possible, execute <tt>action1</tt>. Otherwise, if only <tt>action2</tt> is possible, then execute it.\n  execute <- <>action1 ⊗ <>action2 ⊗ action1.\n  execute <- ¬<>action1 ⊗ <>action2 ⊗ action2.\n\nDining philosophers. Here | is the logical connective of parallel conjunction of Concurrent Transaction Logic.<ref name="concurrentTR"/>\n diningPhilosophers <- phil(1) | phil(2) | phil(3) | phil(4).\n\n== Implementations ==\n\nA number of implementations of Transaction Logic exist. The original implementation is available [http://www.cs.toronto.edu/~bonner/transaction-logic.html here]. An implementation of Concurrent Transaction Logic is available [http://www.cs.toronto.edu/~bonner/ctr/index.html here]. Transaction Logic enhanced with [[tabling]] is available [http://flora.sourceforge.net/tr-interpreter-suite.tar.gz here]. An implementation of Transaction Logic has also been incorporated as part of the [[Flora-2]] knowledge representation and reasoning system. All these implementations are [[open source]].\n\nAdditional papers on Transaction Logic can be found on the [http://flora.sourceforge.net Flora-2 Web site].\n\n== References ==\n{{Reflist}}\n\n[[Category:Logic programming languages]]\n[[Category:Declarative programming languages]]\n[[Category:Knowledge representation]]']
['Enactive interfaces', '5510317', '[[File:Enactive Human Machine Interface.png|thumb|300px|Enactive human-machine interface translating the aspects of a knowledge base into modalities of perception for a human operator. The auditory, visual, and tactile presentations by the system respond to tactile input from the operator, which user input in turn depends upon the auditory, visual, and tactile feedback from the system.<ref name=Bordegoni/><ref name=Fukuda/>]]\n\'\'\'Enactive interfaces\'\'\' are interactive systems that allow organization and transmission of knowledge obtained through action. Examples are interfaces that couple a human with a machine to do things usually done unaided, such as shaping a three-dimensional object using multiple modality interactions with a data base,<ref name=Fukuda/> or using interactive video to allow a student to visually engage with mathematics concepts.<ref name=Held/> Enactive interface design can be approached through the idea of raising awareness of [[affordances]], that is, optimization of the awareness of possible actions available to someone using the enactive interface.<ref name=Stoffregen/> This optimization involves visibility, affordance, and feedback.<ref name=Stone/><ref name=Zudilova/>\n\nThe enactive interface in the figure interprets manual input and provides a response in perceptual terms in the form of images, sounds, and haptic (tactile) feedback. The system is called enactive because of the feedback loop in which the system response is decided by the user input, and the user input is driven by the perceived system responses.<ref name=Bordegoni/>\n\nEnactive interfaces are new types of [[Human–computer interaction|human-computer interface]] that express and transmit the enactive knowledge by integrating different sensory aspects. The driving concept of enactive interfaces is then the fundamental role of motor action for storing and acquiring knowledge (action driven interfaces). Enactive interfaces are then capable of conveying and understanding gestures of the user, in order to provide an adequate response in perceptual terms. Enactive interfaces can be considered a new step in the development of the human-computer interaction because they are characterized by a closed loop between the natural gestures of the user (efferent component of the system) and the perceptual modalities activated (afferent component). Enactive interfaces can be conceived to exploit this direct loop and the capability of recognizing complex gestures.\n\nThe development of such interfaces requires the creation of a common vision between different research areas like [[computer vision]], [[Haptic perception|haptic]] and sound processing, giving more attention on the motor action aspect of interaction. An example of prototypical systems that are able to introduce enactive interfaces are reactive robots, robots that are always in contact with the human hand (like current play console controllers, [[Wii Remote]]) and are capable of interpreting the human movements and guiding the human for the completion of a manipulation task.\n\n==Enactive knowledge==\nEnactive knowledge is information gained through perception–action interaction in the environment. In many aspects the enactive knowledge is more natural than the other forms both in terms of the learning process and in the way it is applied in the world. Such knowledge is inherently [[multimodal interaction|multimodal]] because it requires the co-ordination of the various senses. Two key characteristics of enactive knowledge are that it is \'\'experential\'\': it relates to doing and depends on the user\'s experience, and it is \'\'cultural\'\': the way of doing is itself dependent upon social aspects, attitudes, values, practices, and legacy.<ref name=Bordegoni/>\n\nEnactive interfaces are related to a fundamental interaction concept that often is not exploited by existing [[Human–computer interaction|human-computer interface]] technologies. As stated by cognitive psychologist [[Jerome Bruner]], the traditional interaction with the information mediated by a computer is mostly based on symbolic or iconic knowledge, and not on enactive knowledge.<ref name=Slee/> While in the symbolic way of learning knowledge is stored as words, mathematical symbols or other symbol systems, in the iconic stage knowledge is stored in the form of visual images, such as diagrams and illustrations that can accompany verbal information. On the other hand, enactive knowledge is a form of knowledge based on active participation, knowing by doing, by living rather than thinking.<ref name=Slee2/>\n:"Any domain of knowledge (or any problem within that domain of knowledge) can be represented in three ways: by a set of actions appropriate for achieving a certain result (enactive representation); by a set of summary images or graphics that stand for a concept without defining it fully (iconic representation); and by a set of symbolic or logical propositions drawn from a symbolic system that is governed by rules or laws for forming and transforming propositions (symbolic representation)"<ref name=Bruner/>\n\nA particular form of knowledge is a \'\'[[skill]]\'\', juggling being a simple example, and the acquisition of a skill is one area where enactive knowledge is evident. The sensorimotor and cognitive activities involved in acquiring skills are tabulated by the SKILLS FP6 European skills project.<ref name=Bardy>\n{{cite journal |title=An enactive approach to perception-action and skill acquisition in virtual reality environments |author1=B Bardy |author2=D Delignières |author3=J Lagarde |author4=D Mottet |author5=G Zelic |journal= Third International Conference on Applied Human Factors and Ergonomics |location=Miami |date=July 2010 |url=http://didier.delignieres.perso.sfr.fr/Colloques-docs/Bardy%20et%20al.%20%282010%29%20Skills%20apprentissage.pdf }}\n</ref>\n\n==Multimodal interfaces==\nMultimodal interfaces are a good candidate for the creation of \'\'Enactive interfaces\'\' because of their coordinated use of [[Haptic perception|haptic]], sound and vision. Such research is the main objective of the ENACTIVE [[Framework Programmes for Research and Technological Development|Network of Excellence]], a European consortium of more than 20 research laboratories that are joining their research effort for the definition, development and exploitation of enactive interfaces.\n\n==ENACTIVE Network of Excellence==\nThe research on enactive knowledge and enactive interfaces is the objective of the ENACTIVE Network of Excellence. A Network of Excellence is a [[European Economic Community|European Community]] research instrument that provides fundings for the integration of the research activities of different research laboratories and institutions. The ENACTIVE NoE started in 2004 with more than 20 partners with the objective of \'\'the creation of a multidisciplinary research community with the aim of structuring the research on a new generation of human-computer interfaces called Enactive Interfaces.\'\'. The aim of this NoE is not only the research on enactive interfaces by itself, but also the integration of the partners through a Virtual Laboratory and the spreading of the expertise and knowledge of the Network.\n\nSince 2004, the partners, coordinated by the PERCRO laboratory, have improved both the theoretical aspects of enaction, through seminars and the creation of a [[lexicon]], and the technological aspects necessary for the creation of enactive interfaces. Every year the status of the ENACTIVE NoE is presented through an international conference.<ref name=PERCRO/>\n\n== See also ==\n* [[Enactivism]]\n\n==References==\n{{reflist|refs=\n\n<ref name=Bordegoni>\n{{cite book |title=Emotional Engineering: Service Development |editor=Shuichi Fukuda |url=https://books.google.com/books?id=ow-UFDj15rUC&pg=PA76 |page=76 |chapter=§4.4.2: PDP [Product Development Process] scenario based on user-centered design |author=Monica Bordegoni |isbn=9781849964234 |publisher=Springer |year=2010}}\n</ref>\n\n<ref name=Bruner>\n{{cite book |title=Toward a Theory of Instruction |author=Jerome Seymour Bruner |url=http://h.uib.no/examplewiki/en/images/5/5a/Bruner_1966_Theory_of_Instruction.pdf |isbn=9780674897014 |publisher=Harvard University Press |year=1966 |page=44}}. Quoted in {{cite book |title=Fundamental Constructs in Mathematics Education |author=J Bruner |editor1=John Mason |editor2=Sue Johnston-Wilder |url=https://books.google.com/books?id=EA3LtKYTa7YC&pg=PA260 |page=260 |chapter=Chapter 10: Sustaining mathematical activity |year=2004 |publisher=Taylor & Francis |isbn= 0415326982 |edition=Paperback}}\n</ref>\n\n<ref name=Fukuda>\n{{cite book |title=Emotional Engineering: Service Development |chapter=§4.5.2 Design tools based upon enactive interfaces |url=https://books.google.com/books?id=ow-UFDj15rUC&pg=PA78 |pages=78 \'\'ff\'\' |isbn=9781849964234 |year=2010 |publisher=Springer |author=Monica Bordegoni |editor=Shuichi Fukuda}}\n</ref>\n\n<ref name=Held>\n{{cite book |title=Research on Technology and the Teaching and Learning of Mathematics  |editor1=Mary Kathleen Heid |editor2=Glendon W. Blume |url=https://books.google.com/books?id=RGqFJ9inaQQC&pg=PA213 |pages=213 \'\'ff\'\' |chapter=Enactive control |authors=D Tall, D Smith, C Piez |isbn=9781931576192 |year=2008 |publisher=Information Age Publishing Inc }}\n</ref>\n\n<ref name=PERCRO>\n{{cite web |title=Research on haptic interfaces and virtual environments |url=http://www.percro.org/node/24 |publisher=PERCRO Perceptual Robotics Laboratory |accessdate=April 30, 2014}}\n</ref>\n\n<ref name=Slee>\nBruner\'s list of six characteristics of iconic knowledge is found in {{cite book |chapter=Iconic representation |title=Child, Adolescent and Family Development |author1=Phillip T. Slee |author2=Marilyn Campbell |author3=Barbara Spears |url=https://books.google.com/books?id=iLd7XILh7QkC&pg=PA176 |page=176 |isbn=9781107402164 |year=2012 |publisher=Cambridge University Press}}\n</ref>\n\n<ref name=Slee2>\n{{cite book |chapter=Enactive representation |title=Child, Adolescent and Family Development |author1=Phillip T. Slee |author2=Marilyn Campbell |author3=Barbara Spears |url=https://books.google.com/books?id=iLd7XILh7QkC&pg=PA176 |page=176 |isbn=9781107402164 |year=2012 |publisher=Cambridge University Press}}\n</ref>\n\n<ref name=Stoffregen>\n{{cite journal |url=http://link.springer.com/article/10.1007/s10055-006-0025-7 |title=Affordances in the design of enactive systems |author1=TA Stoffregen |author2=BG Bardy |author3=B Mantel |journal=Virtual Reality |volume=10 |issue=1 |year=2006 |pages=4–10 |doi=10.1007/s10055-006-0025-7}}\n</ref>\n\n<ref name=Stone>\n{{cite book |author1=Debbie Stone |author2=Caroline Jarrett |author3=Mark Woodroffe |author4=Shailey Minocha Morgan Kaufmann |year=2005 |title=User Interface Design and Evaluation |publisher=Morgan Kaufmann |isbn=9780080520322 |url=https://books.google.com/books?id=VvSoyqPBPbMC&pg=PA97 |pages=97 \'\'ff\'\' |chapter=Chapter 5; §3: Three principles from experience: visibility, affordance, and feedback}}\n</ref>\n\n<ref name=Zudilova>\n{{cite book |title=Trends in Interactive Visualization: State-of-the-Art Survey |pages=166 \'\'ff\'\' |chapter=Perceptual and design principles for effective interactive visualizations |author1=Elena Zudilova-Seinstra |author2=Tony Adriaansen |author3=Robert van Liere |year=2008 |publisher=Springer |isbn=9781848002692 |url=https://books.google.com/books?id=mFtS7uN8ybsC&pg=PA166}}\n</ref>\n\n}}\n\n==External links==\n* [http://vimeo.com/79179138 Vimeo], video of a three-dimensional dynamic interactive graphical display allowing a human operator to visualize and manipulate data.\n\n==Additional reading==\n*{{cite book |title=Orchestrating Human-Centered Design |author=Guy Boy |url=https://books.google.com/books?id=I5gCTZCIL3AC&pg=PA118&lpg=PA118 |isbn=9781447143383 |year=2012 |publisher=Springer |page=118}} "The organization producing the system can itself be defined as an autopoietic system in Maturana and Varela\'s sense. An autopoietic system is producer and product at the same time. HCD [Human Centered Design] is both the process of design and the design itself." \n*{{cite journal |title=The systemics of dialogism: On the prevalence of the self in HCI design |author=Colin T Schmidt |journal=Journal of the American society for information science |volume=48 |issue=11 |pages=1073–1081 |year=1997 |url=http://www.researchgate.net/publication/220433804_The_Systemics_of_Dialogism_On_the_Prevalence_of_the_Self_in_HCI_Design |doi=10.1002/(sici)1097-4571(199711)48:11<1073::aid-asi9>3.0.co;2-t}} Autopoiesic systems.\n*{{cite journal |title=An autopoietic approach for knowledge management systems in manufacturing enterprises |author1=Markus Thannhuber |author2=Mitchell M Tseng |author3=Hans-Jörg Bullinger |url=http://www.researchgate.net/publication/223035600_An_Autopoietic_Approach_for_Building_Knowledge_Management_Systems_in_Manufacturing_Enterprises/file/50463525a5a320287e.pdf%26sa%3DX%26scisig%3DAAGBfm3GtB0hiqz1jul4MXuCQxnRzPbcHQ%26oi%3Dscholarr&rct=j&q=&esrc=s&sa=X&ei=N-h_U6HtHIiEogSy_oHAAw&ved=0CCcQgAMoADAA&usg=AFQjCNEt_M1NOffumXQSxrJIVuZI48XRGQ&cad=rja |journal=Annals of the CIRP-Manufacturing Technology |volume=50 |issue=1 |year=2001 |pages=313 \'\'ff\'\' |doi=10.1016/s0007-8506(07)62129-5}}\n\n[[Category:Enactive cognition]]\n[[Category:Knowledge representation]]\n[[Category:Educational psychology]]\n[[Category:Motor cognition]]\n[[Category:User interface techniques]]']
['Closed-world assumption', '2526582', '{{more footnotes|date=August 2011}}\nThe \'\'\'closed-world assumption\'\'\' (CWA), in a [[Mathematical logic|formal system of logic]] used for [[knowledge representation]], is the presumption that a statement that is true is also known to be true. Therefore, conversely, what is not currently known to be true, is false. The same name also refers to a [[formal logic|logical]] formalization of this assumption by [[Raymond Reiter]]. The opposite of the closed-world assumption is the [[open-world assumption]] (OWA), stating that lack of knowledge does not imply falsity. Decisions on CWA vs. OWA determine the understanding of the actual semantics of a conceptual expression with the same notations of concepts. A successful formalization of natural language semantics usually cannot avoid an explicit revelation of whether the implicit logical backgrounds are based on CWA or OWA.\n\n[[Negation as failure]] is related to the closed-world assumption, as it amounts to believing false every predicate that cannot be proved to be true.\n\n== Example ==\n\nIn the context of [[knowledge management]], the closed-world assumption is used in at least two situations: (1) when the knowledge base is known to be complete (e.g., a corporate database containing records for every employee), and (2) when the knowledge base is known to be incomplete but a "best" definite answer must be derived from incomplete information. For example, if a [[database]] contains the following table reporting editors who have worked on a given article, a query on the people not having edited the article on Formal Logic is usually expected to return "Sarah Johnson".\n\n{| border="1" cellspacing="0" cellpadding="2" align="center"\n! colspan="2" style="background:#ffdead;" | Edit\n|-\n! style="background:#efefef;" | Editor\n! style="background:#efefef;" | Article\n|-\n| John Doe || Formal Logic\n|-\n| Joshua A. Norton || Formal Logic\n|-\n| Sarah Johnson || Introduction to Spatial Databases\n|-\n| Charles Ponzi || Formal Logic\n|-\n| Emma Lee-Choon || Formal Logic\n|}\n<br /> <!-- The preceding tag creates a whitespace line separating the table above from the text paragraph below -->\nIn the closed-world assumption, the table is assumed to be [[Complete knowledge base|complete]] (it lists all editor-article relationships), and Sarah Johnson is the only editor who has not edited the article on Formal Logic. In contrast, with the open-world assumption the table is not assumed to contain all editor-article tuples, and the answer to who has not edited the Formal Logic article is unknown. There is an unknown number of editors not listed in the table, and an unknown number of articles edited by Sarah Johnson that are also not listed in the table.\n\n==Formalization in logic==\n\nThe first formalization of the closed-world assumption in [[logic|formal logic]] consists in adding to the knowledge base the negation of the literals that are not currently [[logical consequence|entailed]] by it. The result of this addition is always [[consistent]] if the knowledge base is in [[Horn clause|Horn form]], but is not guaranteed to be consistent otherwise. For example, the knowledge base\n:<math>\\{English(Fred) \\vee Irish(Fred)\\}</math>\nentails neither <math>English(Fred)</math> nor <math>Irish(Fred)</math>.\n\nAdding the negation of these two literals to the knowledge base leads to\n:<math>\\{English(Fred) \\vee Irish(Fred), \\neg English(Fred), \\neg Irish(Fred)\\}</math>\nwhich is inconsistent. In other words, this formalization of the closed-world assumption sometimes turns a consistent knowledge base into an inconsistent one. The closed-world assumption does not introduce an inconsistency on a knowledge base <math>K</math> exactly when the intersection of all [[Herbrand model]]s of <math>K</math> is also a model of <math>K</math>; in the propositional case, this condition is equivalent to <math>K</math> having a single minimal model, where a model is minimal if no other model has a subset of variables assigned to true.\n\nAlternative formalizations not suffering from this problem have been proposed. In the following description, the considered knowledge base <math>K</math> is assumed to be propositional. In all cases, the formalization of the closed-world assumption is based on adding to <math>K</math> the negation of the formulae that are “free for negation” for <math>K</math>, i.e., the formulae that can be assumed to be false. In other words, the closed-world assumption applied to a [[propositional formula]] <math>K</math> generates the formula:\n:<math>K \\wedge \\{\\neg f ~|~ f \\in F\\}</math>.\nThe set <math>F</math> of formulae that are free for negation in <math>K</math> can be defined in different ways, leading to different formalizations of the closed-world assumption. The following are the definitions of <math>f</math> being free for negation in the various formalizations.\n\n; CWA (closed-world assumption) : <math>f</math> is a positive literal not entailed by <math>K</math>;\n\n; GCWA (generalized CWA) : <math>f</math> is a positive literal such that, for every positive clause <math>c</math> such that <math>K \\not\\vdash c</math>, it holds <math>K \\not\\vdash c \\vee f</math>;<ref>{{Citation\n | first = Jack | last = Minker\n | author-link = Jack Minker\n | title = On indefinite databases and the closed world assumption\n | publisher = [[Springer Berlin Heidelberg]]\n | series = Lecture Notes in Computer Science\n | volume = 138\n | year = 1982\n | pages = 292–308\n | url = http://link.springer.com/chapter/10.1007/BFb0000066\n | doi = 10.1007/BFb0000066\n | isbn = 978-3-540-11558-8 }}</ref>\n\n; EGCWA (extended GCWA): same as above, but <math>f</math> is a conjunction of positive literals;\n\n; CCWA (careful CWA): same as GCWA, but a positive clause is only considered if it is composed of positive literals of a given set and (both positive and negative) literals from another set;\n\n; ECWA (extended CWA): similar to CCWA, but <math>f</math> is an arbitrary formula not containing literals from a given set.\n\nThe ECWA and the formalism of [[Circumscription (logic)|circumscription]] coincide on propositional theories. The complexity of query answering (checking whether a formula is entailed by another one under the closed-world assumption) is typically in the second level of the [[polynomial hierarchy]] for general formulae, and ranges from [[P (complexity)|P]] to [[coNP]] for [[Horn clause|Horn formulae]]. Checking whether the original closed-world assumption introduces an inconsistency requires at most a logarithmic number of calls to an [[Oracle machine|NP oracle]]; however, the exact complexity of this problem is not currently known.\n\n==See also==\n\n* [[Open-world assumption]]\n* [[Non-monotonic logic]]\n* [[Circumscription (logic)]]\n* [[Negation as failure]]\n* [[Default logic]]\n* [[Stable model semantics]]\n* [[Unique name assumption]]\n\n==References==\n{{Reflist}}\n*{{cite journal |last1=Cadoli |first1=Marco |last2=Lenzerini |first2=Maurizio |title=The complexity of propositional closed world reasoning and circumscription |journal=Journal of Computer and System Sciences |date=April 1994 |volume=48 |issue=2 |pages=255–310 |doi=10.1016/S0022-0000(05)80004-2 |url=http://www.sciencedirect.com/science/article/pii/S0022000005800042 |accessdate=20 February 2013 |issn=0022-0000}}\n*{{cite journal |last1=Eiter |first1=Thomas |last2=Gottlob |authorlink2=Georg Gottlob |first2=Georg |title=Propositional circumscription and extended closed-world reasoning are <math>\\Pi^p_2</math>-complete |journal=Theoretical Computer Science |date=June 1993 |volume=114 |issue=2 |pages=231–245 |doi=10.1016/0304-3975(93)90073-3 |url=http://www.sciencedirect.com/science/article/pii/0304397593900733 |accessdate=20 February 2013 |issn=0304-3975}}\n*{{cite journal |last1=Rajasekar |first1=Arcot |last2=Lobo |first2=Jorge |last3=Minker |first3=Jack |authorlink3=Jack Minker |title=Weak Generalized Closed World Assumption |journal=Journal of Automated Reasoning |date=September 1989 |volume=5 |issue=3 |pages=293–307 |doi=10.1007/BF00248321 |url=http://link.springer.com/article/10.1007/BF00248321 |accessdate=20 February 2013 |publisher=Kluwer Academic Publishers |issn=0168-7433}}\n*{{cite journal |last=Lifschitz |first=Vladimir |authorlink=Vladimir Lifschitz |title=Closed-world databases and circumscription |journal=Artificial Intelligence |date=November 1985 |volume=27 |issue=2 |pages=229–235 |doi=10.1016/0004-3702(85)90055-4 |url=http://www.sciencedirect.com/science/article/pii/0004370285900554 |accessdate=20 February 2013 |issn=0004-3702}}\n*{{cite book |last=Reiter |first=Raymond |authorlink=Raymond Reiter |editor1-last=Gallaire |editor1-first=Hervé |editor2-last=Minker |editor2-first=Jack |editor2-link=Jack Minker |title=Logic and Data Bases |year=1978 |publisher=Plenum Press |isbn=9780306400605 |url=http://www.springer.com/computer/security+and+cryptology/book/978-0-306-40060-5 |accessdate=21 February 2013 |chapter=On Closed World Data Bases |pages=119–140}}\n*{{cite journal |last1=Duan |first1=Yucong |last2=Cruz |first2=Christophe |title=Formalizing Semantic of Natural Language through Conceptualization from Existence |journal=International Journal of Innovation, Management and Technology |date=February 2011 |volume=2 |issue=1 |pages=37–42 |doi=10.7763/IJIMT.2011.V2.100 |url=http://ijimt.org/abstract/100-E00187.htm |accessdate=21 February 2013 |issn=2010-0248}}\n\n==External links==\n* https://web.archive.org/web/20090624113015/http://www.betaversion.org:80/~stefano/linotype/news/91/\n* [http://owl1-1.googlecode.com/svn-history/r374/trunk/www.webont.org/owled/2005/sub12.pdf Closed World Reasoning in the Semantic Web through Epistemic Operators]\n* [http://books.hammerpig.com/the-closed-world-assumption-of-databases.html Excerpt from Reiter\'s 1978 talk on the closed world assumption]\n\n{{DEFAULTSORT:Closed-world assumption}}\n[[Category:Logic programming]]\n[[Category:Knowledge representation]]']
['Flex expert system', '43757720', '{{multiple issues|\n{{COI|date=September 2014}}\n{{notability|Products|date=September 2014}}\n{{third-party|date=September 2014}}\n}}\nFlex is a hybrid expert system toolkit developed by [[Logic Programming Associates|LPA]] which incorporates [[Frame language|frame-based]] reasoning with inheritance, [[rule-based programming]] and data-driven procedures.\n\nFlex supports both forwards and backward chaining, and they can be interleaved.\n\nFlex provides its own English-like Knowledge Specification Language, KSL, which helps ensure that knowledge-bases are readable by domain experts. Flex KSL can now be generated automatically for certain classes of problems from [[VisiRule]].\n\nFlex is implemented in, and has access to, [[Prolog]]. As opposed to most expert system shells, which tend to be constrained, Flex is an open toolkit.\n\nFlex has proved very popular in education and was licensed to the Open University as part of T396: \'Artificial intelligence for technology\'.\n\nMuch of the material for this course is described by Prof Adrian Hopgood in his book: Intelligent Systems for Engineers and Scientists, Third Edition, and on his web-site.<ref name = "AI Toolkit">{{citation |url=http://www.adrianhopgood.com/aitoolkit/aitoolkit.shtml | title=AI toolkit: support site for Intelligent Systems for Engineers and Scientists, Third Edition by Adrian Hopgood}}</ref>\n\nThere is also a Flex tutorial on the LPA web-site.<ref name = "Flex Tutorial">{{citation |url=http://www.lpa.co.uk/ftp/5000/flx_tut.pdf | title=Flex Tutorial by Clive Spenser on LPA web-site}}</ref>\n\nFlex has been used to power AllerGenius, an expert system specifically developed by leading allergologists to help interpret the results of modern in vitro allergy tests such as the ImmunoCAP ISAC. These tests can typically measure specific antibodies to more than 100 allergen components from more than 50 pre-selected allergen sources and require a lot of expert interpretation.<ref name = "Allergenius">{{citation |url=http://www.allergenius.it/new/index.php/en/general-conceps/2-non-categorizzato/139-the-structure-of-the-exper-system | title=Allergenius web site}}</ref>\n\n==External links==\n*[http://www.generation5.org/content/2001/prg04.asp "Introduction to Flex/KSL (Part I)", James Mathhews, Generation5]\n*[http://www.lpa.co.uk/flx_det.htm Flex Technical Details], LPA\n*[http://www.lpa.co.uk/wfs_dem.htm WebFlex demos], LPA\n*[http://www.intbis.com/intbis_pages/train.php Flex Training], IbIS\n*[http://dl.acm.org/citation.cfm?id=297981 "A flex-based expert system for sewage treatment works support", Dixon et al, PCAI Magazine]\n*[http://www.lamsade.dauphine.fr/~tsoukias/papers/esse.pdf ESSE: An Expert System for Software Evaluation]\n*[http://4c.ucc.ie/web/upload/publications/inProc/KCCP-2007%20Dokas-Nordlander-Wallace.pdf "Fuzzy Fault Tree Representation and Maintenance based on Frames and Constraint Technologies: A Case Study", Dokas, Nordlander and Wallace]\n*[https://repositorium.sdum.uminho.pt/bitstream/1822/8868/1/A%20Knowledge-Based%20System%20for%20Spinning%20Management.pdf “A Knowledge-Based System for Spinning Management”]\n*[http://iraj.in/up_proc/pdf/86-140412387293-96.pdf A NOVEL APPROACH FOR EXPERT SYSTEM AIDED DATACENTER DESIGN]\n*[http://www.cscjournals.org/csc/manuscript/Journals/IJAE/volume1/Issue2/IJAE-10.pdf “An Expert System using A Decision Logic Charting Approach for Indian Legal Domain With specific reference to Transfer of Property Act”], N B Bilgi, Dr. R V Kulkarni & C. Spenser\n*[http://www.ijser.org/researchpaper%5CAN-EXPERT-SYSTEM-FOR-SEISMIC-DATA-INTERPRETATION.pdf “An expert system for Seismic data interpretation using visual and analytical tools”], Neelu Jyothi Ahuja and Parag Diwan\n*[http://pubcouncil.kuniv.edu.kw/jer/files/19Nov2012102247An%20expert%20system%20machinability%20data%20bank%20%28ESMDB%29%20approach..pdf “An expert system machinability data bank”]\n*[http://www.iis.sinica.edu.tw/APEC02/Program/chingyeh.pdf “Development of an Ontology-Based Portal for Digital Archive Services”], Ching-Long Yeh\n*[http://orbit.dtu.dk/fedora/objects/orbit:88354/datastreams/file_7703263/content “A development process meta-model for Web based expert systems: the Web engineering point of view”], Ioannis M. Dokas and Alexandre Alapetite\n*[http://www.pacis-net.org/file/1997/75.pdf “Behavioural issues in Information Systems Design”], Mike McGrath\n*[http://www.slaai.lk/proc/2006/chatura.pdf “Artificial Intelligence Approach to Effective Career Guidance”], Chathra Hendahewa et al\n*[http://www.icsd.aegean.gr/kkemalis/pubs/SETN_CAMES.pdf “DYNAMIC ACCESS CONTROL MANAGEMENT USING EXPERT SYSTEM TECHNOLOGY”], Prof. G. Pangalos et al\n*[http://www.waojournal.org/content/7/1/15 “Allergenius, an expert system for the interpretation of allergen microarray results”], Giovanni Melioli, Clive Spenser et al\n\n== References ==\n{{Reflist}}\n\n[[Category:Expert systems]]\n[[Category:Rule engines]]\n[[Category:Knowledge engineering]]\n[[Category:Knowledge representation]]\n\n\n{{compu-ai-stub}}']
['PowToon', '38027627', '{{Underlinked|date=March 2014}}\n{{Infobox company\n| name = PowToon\n| type = [[Private company|Private]]\n| foundation = {{Start date and age|2012}}\n| location_city =  28 Church Rd<br />[[London, UK]]\n| location_country = [[United Kingdom]]\n| key_people = Ilya Spitalnik (Co-Founder and CEO), Daniel Zaturansky (Co-Founder and COO), Sven Hoffman (Co-Founder and CTO)\n| industry = [[Internet Marketing]]\n| products = PowToon Web-based animation software\n| homepage = {{URL|www.powtoon.com}}\n}}\n\'\'\'PowToon\'\'\' is a company which sells cloud-based software [[Software as a service|(SaaS)]] for creating animated presentations and animated explainer videos.<ref>Perez, Sarah. [http://techcrunch.com/2012/06/26/now-everyone-can-make-marketing-videos-powtoon-launches-diy-presentation-tool/ TechCrunch], June 26th, 2012, "Now Everyone Can Make Marketing Videos: PowToon Launches DIY Presentation Tool"</ref>\n\n== History ==\nPowToon was founded in January 2012. The company released a [[Software_release_life_cycle#BETA|beta]] version in August 2012 and has seen fast subscriber growth since.<ref name="powtoon">[http://www.powtoon.com Powtoon Website]</ref> In December 2012 PowToon secured $600,000 investment from LA based Venture Capital firm Startup Minds.<ref>Perez, Sarah, [http://techcrunch.com/2012/12/14/diy-animation-platform-powtoon-grabs-600k-for-its-video-creation-software/ TechCrunch] , Dec 14, 2012, "DIY Animation Platform PowToon Grabs $600K For Its Video Creation Software"</ref> In February 2013 PowToon introduced a free account option allowing users to create animated videos that can be exported to [[YouTube]]. The free videos include the PowToon branding.\n\n== Product ==\nPowToon is Web-based animation software that allows users to create animated presentations  by manipulating pre-created objects, imported images, provided music and user created voice-overs.<ref>{{cite web|last=Mersand |first=Shannon |title=Product Review: PowToon|url=http://www.techlearning.com/product-reviews/0072/product-review-powtoon-/54971|publisher=\'\'Tech and Learning\'\'|accessdate=12 May 2014|date=May 2014}}</ref> \nPowtoon uses an [[Apache Flex]] engine to generate an XML file that can be played in the Powtoon online viewer, exported to YouTube or downloaded as an MP4 file.<ref name="powtoon" />\n\nPowToon is also available on the Google Chrome Store<ref>{{citation |title=PowToon - Chrome Web Store|url=https://chrome.google.com/webstore/detail/powtoon/aomfhbjiekjcbeefclbidjgnikfbooem?hl=en|accessdate=25 February 2015|date=Feb 2015}}</ref>  and has an application on Edmodo.com.<ref>{{citation |title=PowToon by PowToon Ltd|url=https://www.edmodo.com/store/app/powtoon-1|accessdate=25 February 2015|date=Feb 2015}}</ref>\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{official website|http://www.powtoon.com/}}\n\n[[Category:Animation software]]\n[[Category:Companies established in 2012]]\n[[Category:Cloud applications]]\n[[Category:Computer animation]]\n[[Category:Presentation software]]\n[[Category:Knowledge representation]]\n\n{{animation-stub}}']
['Category:Library of Congress Classification', '1138791', '{{Cat main|Library of Congress Classification}}\n[[Category:Library of Congress]]\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]']
['Unified Modeling Language', '32169', '{{Use American English|date=January 2012}}\n[[File:UML logo.gif|thumb|UML logo]]\nThe \'\'\'Unified Modeling Language\'\'\' (\'\'\'UML\'\'\') is a general-purpose, developmental,  [[modeling language]] in the field of [[software engineering]], that is intended to provide a standard way to visualize the design of a system.<ref name=":1" />\n\nUML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design developed by [[Grady Booch]], [[Ivar Jacobson]] and [[James Rumbaugh]] at [[Rational Software]] in 1994–1995, with further development led by them through 1996.<ref name=":1" />\n\nIn 1997 UML was adopted as a standard by the [[Object Management Group]] (OMG), and has been managed by this organization ever since. In 2005 UML was also published by the [[International Organization for Standardization]] (ISO) as an approved ISO standard.<ref>{{cite web|url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=32620 |title=ISO/IEC 19501:2005 - Information technology - Open Distributed Processing - Unified Modeling Language (UML) Version 1.4.2 |publisher=Iso.org |date=2005-04-01 |accessdate=2015-05-07}}</ref> Since then it has been periodically revised to cover the latest revision of UML.<ref>{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=32624 |title=ISO/IEC 19505-1:2012 - Information technology - Object Management Group Unified Modeling Language (OMG UML) - Part 1: Infrastructure |publisher=Iso.org |date=2012-04-20 |accessdate=2014-04-10}}</ref>\n\n== History ==\n\n[[File:OO Modeling languages history.jpg|thumb|320px|History of object-oriented methods and notation]]\n\n=== Before UML 1.x ===\n\nUML has been evolving since the second half of the 1990s and has its roots in the object-oriented methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation.\n\nIt is originally based on the notations of the [[Booch method]], the [[object-modeling technique]] (OMT) and [[object-oriented software engineering]] (OOSE), which it has integrated into a single language.<ref name=":0" />\n\n[[Rational Software Corporation]] hired [[James Rumbaugh]] from [[General Electric]] in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day:<ref>Andreas Zendler (1997) \'\'Advanced Concepts, Life Cycle Models and Tools for Objeckt-Oriented Software Development\'\'. p.122</ref> Rumbaugh\'s [[object-modeling technique]] (OMT) and [[Grady Booch]]\'s method. They were soon assisted in their efforts by [[Ivar Jacobson]], the creator of the [[object-oriented software engineering]] (OOSE) method, who joined them at Rational in 1995.<ref name=":1">{{cite book\n | title = Unified Modeling Language User Guide, The\n | publisher = Addison-Wesley\n | edition = 2\n | year = 2005\n | page = 496\n | url = http://www.informit.com/store/unified-modeling-language-user-guide-9780321267979\n | isbn = 0321267974\n}}\n, See the sample content, look for history</ref>\n\n=== UML 1.x ===\n\nUnder the technical leadership of those three (Rumbaugh, Jacobson and Booch), a consortium called the [[UML Partners]] was organized in 1996 to complete the \'\'Unified Modeling Language (UML)\'\' specification, and propose it to the Object Management Group (OMG) for standardisation. The partnership also contained additional interested parties (for example [[Hewlett-Packard|HP]], [[Digital Equipment Corporation|DEC]], [[IBM]] and [[Microsoft]]). The UML Partners\' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group, designed to define the exact meaning of language constructs, chaired by [[Cris Kobryn]] and administered by Ed Eykholt, to finalize the specification and integrate it with other standardization efforts. The result of this work, UML 1.1, was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.<ref name=":1" /><ref>{{cite web|url=http://www.omg.org/cgi-bin/doc?ad/97-08-11 |title=UML Specification version 1.1 (OMG document ad/97-08-11) |publisher=Omg.org |accessdate=2011-09-22}}</ref>\n\nAfter the first release a task force was formed<ref name=":1" /> to improve the language, which released several minor revisions, 1.3, 1.4, and 1.5.<ref>{{cite web|url=http://www.omg.org/spec/UML/ |title=UML |publisher=Omg.org |accessdate=2014-04-10}}</ref>\n\nThe standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent.<ref>Génova et alia 2004 "Open Issues in Industrial Use Case Modeling"</ref><ref>{{cite web|url=http://www.uml-forum.com/docs/papers/CACM_Jan02_p107_Kobryn.pdf |title=Will UML 2.0 Be Agile or Awkward? |format=PDF |accessdate=2011-09-22}}</ref>\n\n=== UML 2.x ===\n\nUML 2.0 major revision replaced version 1.5 in 2005, which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.<ref>{{cite web|url=http://www.omg.org/spec/UML/2.0/ |title=UML 2.0 |publisher=Omg.org |accessdate=2011-09-22}}</ref>\n\nAlthough UML 2.1 was never released as a formal specification, versions 2.1.1 and 2.1.2 appeared in 2007, followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010.<ref name="spec">{{cite web|url=http://www.omg.org/spec/UML/ |title=UML |publisher=Omg.org |accessdate=2011-09-22}}</ref> UML 2.4.1 was formally released in August 2011.<ref name="spec"/> UML 2.5 was released in October 2012 as an "In process" version and was officially released in June 2015.<ref name="spec"/>\n\nThere are four parts to the UML 2.x specification:\n\n# The Superstructure that defines the notation and semantics for diagrams and their model elements\n# The Infrastructure that defines the core metamodel on which the Superstructure is based\n# The [[Object Constraint Language]] (OCL) for defining rules for model elements\n# The UML Diagram Interchange that defines how UML 2 diagram layouts are exchanged\n\nThe current versions of these standards follow: UML Superstructure version 2.4.1, UML Infrastructure version 2.4.1, OCL version 2.3.1, and UML Diagram Interchange version 1.0.<ref name="Versions">{{cite web|author=OMG|title=OMG Formal Specifications (Modeling and Metadata paragraph)|url=http://www.omg.org/spec/#M&M|accessdate = 2016-02-12}}</ref> It continues to be updated and improved by the revision task force, who resolve any issues with the language.<ref>{{cite web|url=http://www.omg.org/issues/uml2-rtf.open.html |title=Issues for UML 2.6 Revision task Force mailing list |publisher=Omg.org |accessdate=2014-04-10}}</ref>\n\n== Design ==\n\nUML offers a way to visualize a system\'s architectural blueprints in a diagram (see image), including elements such as:<ref name=":0">{{cite web|url=http://www.omg.org/spec/UML/2.4.1/Superstructure/PDF |title=OMG Unified Modeling Language (OMG UML), Superstructure. Version 2.4.1 |publisher=Object Management Group |accessdate=9 April 2014}}</ref>\n\n* any [[Activity (UML)|activities]] (jobs);\n* individual [[Component (UML)|components]] of the system;\n** and how they can interact with other [[Component-based software engineering|software components]];\n* how the system will run;\n* how entities interact with others (components and interfaces);\n* external [[user interface]].\n\nAlthough originally intended for object-oriented design documentation, UML has been extended to a larger set of design documentation (as listed above),<ref>Satish Mishra (1997). [http://www2.informatik.hu-berlin.de/~hs/Lehre/2004-WS_SWQS/20050107_Ex_UML.ppt "Visual Modeling & Unified Modeling Language (UML): Introduction to UML"]. Rational Software Corporation. Accessed 9 November 2008.</ref> and been found useful in many contexts.<ref name="UML, Success Stories">{{cite web|url=http://www.uml.org/uml_success_stories/index.htm|title=UML, Success Stories|accessdate=9 April 2014}}</ref>\n\n=== Software development methods ===\n\nUML is not a development method by itself;<ref>John Hunt (2000). \'\'The Unified Process for Practitioners: Object-oriented Design, UML and Java\'\'. Springer, 2000. ISBN 1-85233-275-1. p.5.door</ref> however, it was designed to be compatible with the leading object-oriented software development methods of its time, for example [[Object-modeling technique|OMT]], [[Booch method]], [[Objectory]] and especially [[Rational Unified Process|RUP]] that it was originally intended to be used with when work began at Rational Software.\n\n=== Modeling ===\n\nIt is important to distinguish between the UML model and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases).\n\nUML diagrams represent two different views of a system model:<ref>Jon Holt Institution of Electrical Engineers (2004). \'\'UML for Systems Engineering: Watching the Wheels\'\' IET, 2004, ISBN 0-86341-354-4. p.58</ref>\n\n* Static (or \'\'structural\'\') view: emphasizes the static structure of the system using objects, attributes, operations and relationships. It includes [[class diagram]]s and [[composite structure diagram]]s.\n* Dynamic (or \'\'behavioral\'\') view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes [[sequence diagram]]s, [[activity diagram]]s and [[UML state machine|state machine diagrams]].\n\nUML models can be exchanged among UML tools by using the [[XML Metadata Interchange]] (XMI) format.\n\n== Diagrams ==\n{{UML diagram types}}\n\nUML 2 has many types of diagrams, which are divided into two categories.<ref name=":0" /> Some types represent \'\'structural\'\' information, and the rest represent general types of \'\'behavior\'\', including a few that represent different aspects of \'\'interactions\'\'. These diagrams can be categorized hierarchically as shown in the following class diagram:<ref name=":0" />\n\n[[File:UML diagrams overview.svg|center|600px|Hierarchy of UML 2.2 Diagrams, shown as a [[class diagram]]]]\n\nThese diagrams may all contain comments or notes explaining usage, constraint, or intent.\n\n=== Structure diagrams ===\n\n[[Structure diagram]]s emphasize the things that must be present in the system being modeled. Since structure diagrams represent the structure, they are used extensively in documenting the [[software architecture]] of software systems. For example, the [[component diagram]] describes how a software system is split up into components and shows the dependencies among these components.\n\n<gallery class="center">\nPolicy Admin Component Diagram.PNG|[[Component diagram]]\nBankAccount1.svg|[[Class diagram]]\n</gallery>\n\n=== Behavior diagrams ===\n\nBehavior diagrams emphasize what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system, they are used extensively to describe the functionality of software systems. As an example, the [[activity diagram]] describes the business and operational step-by-step activities of the components in a system.\n\n<gallery class="center">\nActivity conducting.svg|[[Activity diagram]]\nUML Use Case diagram.svg|[[Use case diagram]]\n</gallery>\n\n==== Interaction diagrams ====\n\nInteraction diagrams, a subset of behavior diagrams, emphasize the flow of control and data among the things in the system being modeled. For example, the [[sequence diagram]] shows how objects communicate with each other in terms of a sequence of messages.\n\n<gallery class="center">\nCheckEmail.svg|[[Sequence diagram]]\nUML Communication diagram.svg|[[Communication diagram]]\n</gallery>\n\n== Meta modeling ==\n{{Main article|Meta-Object Facility}}\n\n[[File:M0-m3.png|thumb|320px|Illustration of the Meta-Object Facility]]\n\nThe Object Management Group (OMG) has developed a [[metamodeling]] architecture to define the UML, called the [[Meta-Object Facility]].<ref>Iman Poernomo (2006) "[http://calcium.dcs.kcl.ac.uk/1259/1/acm-paper.pdf The Meta-Object Facility Typed]" in: \'\'Proceeding SAC \'06 Proceedings of the 2006 ACM symposium on Applied computing\'\'. pp. 1845-1849</ref> MOF is designed as a four-layered architecture, as shown in the image at right. It provides a meta-meta model at the top, called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels, called M2-models.\n\nThe most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel, which describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.<ref>{{cite web|url=http://www.omg.org/spec/UML/2.4.1/Infrastructure/PDF/ |title=UML 2.4.1 Infrastructure |publisher=Omg.org |date=2011-08-05 |accessdate=2014-04-10}}</ref>\n\nThe meta-model can be extended using a mechanism called [[stereotype (UML)|stereotyping]]. This has been criticised as being insufficient/untenable by [[Brian Henderson-Sellers]] and Cesar Gonzalez-Perez in "Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0".<ref name="UsesAbusesStereotype">B. Henderson-Sellers; C. Gonzalez-Perez (2006). "Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0". in: \'\'Model Driven Engineering Languages and Systems\'\'. Springer Berlin / Heidelberg.</ref>\n\n== Adoption ==\n\nUML has been found useful in many design contexts.<ref name="UML, Success Stories"/><ref>{{Cite web|url = http://www.drdobbs.com/architecture-and-design/uml-25-do-you-even-care/240163702?queryText=uml|title = UML 2.5: Do you even care?}} "UML truly is ubiquitous"</ref>\n\nIt has been treated, at times, as a design [[no silver bullet|silver bullet]], which has led to problems in its usage. Misuse of it includes excessive usage of it (design every little part of the system\'s [[Programming code|code]] with it, which is unnecessary) and assuming that anyone can design anything with it (even those who haven\'t [[Programmer|programmed]]).<ref>{{Cite web|url = http://queue.acm.org/detail.cfm?id=984495|title = Death by UML Fever}}</ref>\n\nIt is seen to be a large language, with many [[Syntax (programming languages)|constructs]] in it. Some (including [[Ivar Jacobson|Jacobson]]) feel that there are too many and that this hinders the learning (and therefore usage) of it.<ref>{{Cite web|url = http://www.infoq.com/interviews/Ivar_Jacobson|title = Ivar Jacobson on UML, MDA, and the future of methodologies}}</ref>\n\n== Criticisms ==\n{{Criticism section|date=December 2010}}\n\nCommon criticisms of UML from industry include:<ref name="petre">{{Cite conference| quote=The majority of those interviewed simply do not use UML, and those who do use it tend to do so selectively and often informally|conference=35th International Conference on Software Engineering 18–26 May 2013 |url=http://oro.open.ac.uk/35805/8/UML%20in%20practice%208.pdf|title=UML in practice|first=Marian|last=Petre| date=2013|pages=722–731}}</ref>\n\n* not useful: "[does] not offer them advantages over their current, evolved practices and representations"\n* too complex, particularly for communication with clients: "unnecessarily complex" and "The best reason not to use UML is that it is not ‘readable’ for all stakeholders. How much is UML worth if a business user (the customer) can not understand the result of your modelling effort?"\n* need to keep UML and code in sync, as with documentation generally\n\n=== Critique of UML 1.x ===\n\n; Cardinality notation: As with database Chen, Bachman, and ISO [[ER diagram]]s, class models are specified to use "look-across" [[Cardinality (data modeling)|cardinalities]], even though several authors ([[Merise]],<ref>Hubert Tardieu, Arnold Rochfeld and René Colletti La methode MERISE: Principes et outils (Paperback - 1983)</ref> Elmasri & Navathe<ref>Elmasri, Ramez, B. Shamkant, Navathe, Fundamentals of Database Systems, third ed., Addison-Wesley, Menlo Park, CA, USA, 2000.</ref> amongst others<ref>[https://books.google.com/books?id=odZK99osY1EC&pg=PA52&img=1&pgis=1&dq=genova&sig=ACfU3U3tDC_q8WOMqUJW4EZCa5YQywoYLw&edge=0 ER 2004 : 23rd International Conference on Conceptual Modeling, Shanghai, China, 8-12 November 2004] {{webarchive |url=https://web.archive.org/web/20130527133330/https://books.google.com/books?id=odZK99osY1EC&pg=PA52&img=1&pgis=1&dq=genova&sig=ACfU3U3tDC_q8WOMqUJW4EZCa5YQywoYLw&edge=0 |date=27 May 2013 }}</ref>) prefer same-side or "look-here" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer,<ref>{{cite web|url=http://publik.tuwien.ac.at/files/pub-inf_4582.pdf |title=A Formal Treatment of UML Class Diagrams as an Efficient Method for Configuration Management 2007 |format=PDF |accessdate=2011-09-22}}</ref> Dullea et. alia<ref>{{cite web|url=http://www.ischool.drexel.edu/faculty/song/publications/p_DKE_03_Validity.pdf |title=James Dullea, Il-Yeol Song, Ioanna Lamprou - An analysis of structural validity in entity-relationship modeling 2002 |format=PDF |accessdate=2011-09-22}}</ref>) have shown that the "look-across" technique used by UML and ER diagrams is less effective and less coherent when applied to \'\'n\'\'-ary relationships of order strictly greater than 2.\n\n: Feinerer says: "Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann<ref>{{cite web|url=http://crpit.com/confpapers/CRPITV17Hartmann.pdf |title="Reasoning about participation constraints and Chen\'s constraints" S Hartmann - 2003 |format=PDF |accessdate=2013-08-17}}</ref> investigates this situation and shows how and why different transformations fail.", and: "As we will see on the next few pages, the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to \'\'n\'\'-ary associations."\n\n== See also ==\n{{Portal|Software}}\n\n* [[Object Oriented Role Analysis and Modeling]]\n* [[Model-based testing]]\n* [[Model-driven engineering]]\n* [[Applications of UML]]\n* [[List of Unified Modeling Language tools]]\n\n== References ==\n{{FOLDOC}}\n\n{{reflist|colwidth=30em}}\n\n== Further reading ==\n\n* {{cite book\n\n | first= Scott William\n | last = Ambler\n | year = 2004\n | url = http://www.ambysoft.com/books/theObjectPrimer.html\n | title = The Object Primer: Agile Model Driven Development with UML 2\n | publisher = Cambridge University Press\n | isbn=0-521-54018-6\n}}\n\n* {{cite book\n\n | first= Michael Jesse\n | last = Chonoles\n | author2=James A. Schardt\n | year = 2003\n | title = UML 2 for Dummies\n | publisher = Wiley Publishing\n | isbn=0-7645-2614-6\n}}\n\n* {{cite book\n\n | first = Martin\n | last = Fowler\n | authorlink = Martin Fowler\n | title = UML Distilled: A Brief Guide to the Standard Object Modeling Language\n | edition = 3rd\n | publisher = Addison-Wesley\n | isbn = 0-321-19368-7\n}}\n\n* {{cite book\n\n | first= Ivar\n | last = Jacobson |author2=Grady Booch |author3=James Rumbaugh\n | authorlink = Ivar Jacobson\n | year = 1998\n | title = The Unified Software Development Process\n | publisher = Addison Wesley Longman\n | isbn=0-201-57169-2\n}}\n\n* {{cite book\n\n | first = Robert Cecil\n | last = Martin\n | authorlink = Robert Cecil Martin\n | year = 2003\n | title = UML for Java Programmers\n | publisher = Prentice Hall\n | isbn = 0-13-142848-9\n}}\n\n* {{cite web\n\n | author = Noran, Ovidiu S.\n | url = http://www.cit.gu.edu.au/~noran/Docs/UMLvsIDEF.pdf\n | title = Business Modelling: UML vs. IDEF\n | format = PDF\n | accessdate = 2005-12-28\n}}\n\n* {{cite web\n\n | author = Horst Kargl\n | url = http://umlnotation.sparxsystems.eu/\n | title = Interactive UML Metamodel with additional Examples\n }}\n\n* {{cite book\n\n | first = Magnus\n | last = Penker\n | author2=Hans-Erik Eriksson\n | author-link2= Hans-Erik Eriksson\n | year = 2000\n | title = Business Modeling with UML\n | publisher = John Wiley & Sons\n | isbn = 0-471-29551-5\n}}\n\n== External links ==\n{{Commons}}\n\n{{Wikiversity|UML}}\n\n* {{Official website}}\n\n{{UML}}\n\n{{Software engineering}}\n\n{{ISO standards}}\n\n{{Use dmy dates|date=July 2011}}\n\n{{Authority control}}\n\n[[Category:Architecture description language]]\n[[Category:Data modeling languages]]\n[[Category:Data modeling diagrams]]\n[[Category:Diagrams]]\n[[Category:Knowledge representation]]\n[[Category:ISO standards]]\n[[Category:Specification languages]]\n[[Category:Unified Modeling Language| ]]\n[[Category:Software modeling language]]']
['Parallel Tree Contraction', '48789236', '{{Orphan|date=December 2015}}\n\nIn [[computer science]], \'\'\'parallel tree contraction\'\'\' is a broadly applicable technique for the parallel solution of a large number of [[tree]] problems, and is used as an algorithm design technique for the design of a large number of parallel [[graph (discrete mathematics)|graph]] algorithms.  Parallel tree contraction was introduced by [[Gary L. Miller]] and [[John H. Reif]],<ref name="Miller89book">[[Gary L. Miller]] and [[John H. Reif]], Parallel Tree Contraction--Part I: Fundamentals., 1989</ref>  and has subsequently been modified to improve efficiency by X. He and Y. Yesha,<ref>X. He and Y. Yesha, "Binary tree algebraic computation and parallel algorithms for simple graphs.", Journal of Algorithms, 1988, pp 92-113</ref> Hillel Gazit, Gary L. Miller and Shang-Hua Teng<ref>Hillel Gazit, Gary L. Miller and Shang-Hua Teng, Optimal tree contraction in the EREW model, Springer, 1988</ref> and many others.<ref>Karl Abrahamson and et al., "A simple parallel tree contraction algorithm.", Journal of Algorithms, 1989, pp 287-302</ref>\n\nTree contraction has been used in designing many efficient [[parallel algorithms]], including [[Expression (mathematics)|expression]] evaluation, finding [[lowest common ancestors]], tree isomorphism, [[graph isomorphism]], [[maximal subtree isomorphism]], [[common subexpression elimination]], computing the 3-connected components of a graph, and finding an explicit planar embedding of a [[planar graph]]<ref name="Reif94dynamic">John H. Reif and Stephen R. Tate, Dynamic parallel tree contraction, Proceedings of the sixth annual ACM symposium on Parallel algorithms and architectures (ACM), 1994</ref>\n\nBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic. This article hereby focuses on a particular solution, which is a variant of the algorithm by Miller and Reif, and its application.\n\n==Introduction==\nOver the past several decades there has been significant research on deriving new parallel algorithms for a variety of problems, with the goal of designing highly parallel ([[polylogarithmic depth]]), work-efficient (linear in the sequential running time) algorithms.<ref name="Miller89book" /> For some problems, tree turns out to be a nice solution. Addressing these problems, we can sometimes get more parallelism simply by representing our problem as a tree.\n\nConsidering a generic definition of a tree, there is a root vertex, and several child vertices attached to the root.<ref>[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. \'\'[[Introduction to Algorithms]]\'\', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&nbsp;214–217. Chapters 12–14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&nbsp;253–320.</ref> And the child vertices might have children themselves, and so on so forth. Eventually, the paths come down to leaves, which are defined to be the terminal of a tree. Then based on this generic tree, we can further come up with some special cases: (1) [[balanced binary tree]]; (2) [[linked list]].<ref>[[Donald Knuth]]. \'\'[[The Art of Computer Programming]]: Fundamental Algorithms\'\', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&nbsp;308–423.</ref> A balanced binary tree has exactly two branches for each vertex except for leaves. This gives a O(log n) bound on the depth of the tree.<ref>{{citation\n | last1 = Nešetřil | first1 = Jaroslav | author1-link = Jaroslav Nešetřil\n | last2 = Ossona de Mendez | first2 = Patrice | author2-link = Patrice Ossona de Mendez\n | contribution = Chapter 6. Bounded height trees and tree-depth\n | doi = 10.1007/978-3-642-27875-4\n | isbn = 978-3-642-27874-7\n | location = Heidelberg\n | mr = 2920058\n | pages = 115–144\n | publisher = Springer\n | series = Algorithms and Combinatorics\n | title = Sparsity: Graphs, Structures, and Algorithms\n | volume = 28\n | year = 2012}}.</ref> A linked list is also a tree where every vertex has only one child. We can also achieve O(log n) depth using [[symmetry breaking]].<ref>Andrew Goldberg, Serge Plotkin, and Gregory Shannon, Parallel symmetry-breaking in sparse graphs, Proceedings of the nineteenth annual ACM symposium on Theory of computing (ACM), 1987</ref>\n\nGiven the general case of a tree, we would like to keep the bound at O(log n) no matter it is unbalanced or list-like or a mix of both. To address this problem, we make use of an algorithm called [[prefix sum]] by using the [[Euler tour technique]].<ref>[http://courses.csail.mit.edu/6.851/spring07/scribe/lec05.pdf Euler tour trees] - in Lecture Notes in Advanced Data Structures. Prof. Erik Demaine; Scribe: Katherine Lai.</ref> With the Euler tour technique, a tree could be represented in a flat style, and thus prefix sum could be applied to an arbitrary tree in this format. In fact, prefix sum can be used on any set of values and binary operation which form a group: the binary operation must be associative, every value must have an inverse, and there exists an identity value.\n\nWith a bit of thought, we can find some exceptional cases where prefix sum becomes incapable or inefficient. Consider the example of multiplication when the set of values includes 0. Or there are some commonly desired operations are max() and min() which do not have [[inverses]]. The goal is to seek an algorithm which works on all trees, in expected O(n) work and O(log n) depth. In the following sections, a Rake/Compress algorithm will be proposed to fulfill this goal.<ref name="Miller85app">Gary L. Miller and John H. Reif, Parallel tree contraction and its application, Defense Technical Information Center, 1985</ref>\n\n==Definitions==\n\n[[File:Rake-1.png|480*360px|thumbnail|right|Fig. 1: Rake Operation]]\n[[File:Compress-1.png|480*360px|thumbnail|right|Fig. 2: Compress Operation]]\nBefore going into the algorithm itself, we first look at a few terminologies that will be used later.\n\n* \'\'\'Rake\'\'\'<ref name="cmutrees">[https://www.cs.cmu.edu/afs/cs/academic/class/15499-s09/www/scribe/lec11/lec11.pdf Parallel Algorithms: Tree Operations], Guy Blelloch, Carnegie Mellon University, 2009</ref> – Rake step joins every left leaf of binary nodes to the parent. By join, we mean that it undergoes a functional process which achieves the operation we want to make. An example of rake is given in Figure 1.\n* \'\'\'Compress\'\'\'<ref name="cmutrees" /> – Compress step is actually a sequence of several events: (1) Find an independent set of unary nodes. (Independence here is defined such that no two are neighbors, meaning no parent to child relation) (2) Join each node in independent set with its child (Note that independent set is not unique). An example of compress is given in Figure 2.\n\nAnd in order to solve actual problems using tree contraction, the algorithm has a structure:\n\n<pre>\nRepeat until tree becomes a unary node\n{\n    Rake;\n    Compress;\n}\n</pre>\n\n\n==Analysis==\nFor the moment, let us assume that all nodes have less than three children, namely binary. Generally speaking, as long as the degree is bounded, the bounds will hold.<ref>MORIHATA, Akimasa, and Kiminori MATSUZAKI, A Parallel Tree Contraction Algorithm on Non-Binary Trees, MATHEMATICAL ENGINEERING\nTECHNICAL REPORTS, 2008</ref> But we will analyze the binary case for simplicity. In the two “degenerate” cases listed above, the rake is the best tool for dealing with balanced binary trees, and compress is the best for linked lists. However, arbitrary trees will have to require a combination of these operations. By this combination, we claim a theorem that\n* \'\'\'Theorem\'\'\': After O(log n) expected rake and compress steps, a tree is reduced to a single node.\nNow rephrase the tree contraction algorithm as follows:\n* Input: A binary tree rooted at r\n* Output: A single node\n* Operation:  A sequence of contraction steps, each consisting of a rake operation and a compress operation (in any order). The rake operation removes all the leaf nodes in parallel. The compress operation finds an [[Independent set (graph theory)|independent set]] of unary nodes and splice out the selected nodes.\nTo approach the theorem, we first take a look at a property of a binary tree. Given a binary tree T, we can partition the nodes of T into 3 groups: {{tmath|T_0}} contains all leaf nodes, {{tmath|T_1}} contains all nodes with 1 child, and {{tmath|T_2}} contains all nodes with 2 children. It is easy to see that: <math>V(T) = T_0  \\cup T_1 \\cup T_2</math>. Now we propose:\n* Claim: <math>|T_0| = |T_2|  + 1</math>\nThis claim can be proved by strong induction on the number of nodes. It is easy to see that the base case of n=1 trivially holds. And we further assume the claim also holds for any tree with at most n nodes. Then given a tree with n+1 nodes rooted at r, there appears to be two cases:\n# If r has only one subtree, consider the subtree of r. We know that the subtree has the same number of binary nodes and the same number of leaf nodes as the whole tree itself. This is true since the root is a unary node. And based the previous assumption, a unary node does not change either {{tmath|T_0}} or {{tmath|T_2}}.\n# If r has two subtrees, we define {{tmath|T_0^L, T_2^L}} to be the leaf nodes and binary nodes in the left subtree, respectively. Similarly, we define the same {{tmath|T_0^R, T_2^R}} for the right subtree. From previous, there is <math>|T_0^L| = |T_2^L| + 1</math> and <math>|T_0^R| = |T_2^R| + 1</math>. Also we know that T has <math>|T_0^L| + |T_0^R|</math> leaf nodes and <math>|T_2^L| + |T_2^R| + 1</math> binary nodes. Thus, we can derive:\n\n:<math>|T_0^L| + |T_0^R| = |T_2^L| + 1 + |T_2^R| + 1 = (|T_2^L| + |T_2^R| + 1) + 1</math>\n\nwhich proves the claim.\n\nFollowing the claim, we then prove a lemma, which leads us to the theorem.\n* Lemma: The number of nodes of after a contraction step is reduced by a constant factor in expectation.\nAssume the number of nodes before the contraction to be m, and m\' after the contraction. By definition, the rake operation deletes all {{tmath|T_0}} and the compress operation deletes at least 1/4 of {{tmath|T_1}} in expectation. All {{tmath|T_2}} remains. Therefore, we can see:\n\n:<math>E[m\'] \\leq |T_2| + \\tfrac{3}{4}*|T_1| \\leq \\tfrac{3}{4} + \\tfrac{3}{4}*|T_1| + \\tfrac{3}{2}*|T_2| = \\tfrac{3}{4}(1 + |T_1| + 2*|T_2|) = \\tfrac{3}{4}(|T_0| + |T_1| + |T_2|) = \\tfrac{3}{4}m</math>\n\nFinally, based on this lemma, we can conclude that if the nodes are reduced by a constant factor in each iteration, after {{tmath|O(\\log n)}}, there will be only one node left.<ref>[https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/scribe/lec9/lecture9.pdf Parallel Algorithms: Analyzing Parallel Tree Contraction], Guy Blelloch, 2007</ref>\n\n==Applications==\n\n===Expression Evaluation===\nTo evaluate an expression given as a binary tree (this problem also known as [[binary expression tree]]),<ref>S Buss, Algorithms for boolean formula evaluation and for tree contraction, Arithmetic, Proof Theory, and Computational Complexity, 1993, pp. 96-115</ref> consider that:\nAn arithmetic expression is a tree where the leaves have values from some domain and each internal vertex has two children and a label from {+, x, %}. And further assume that these binary operations can be performed in constant time.\n\nWe now show the evaluation can be done with parallel tree contraction.<ref>Bader, David A., Sukanya Sreshta, and Nina R. Weisse-Bernstein, Evaluating arithmetic expressions using tree contraction: A fast and scalable parallel implementation for symmetric multiprocessors (SMPs), High Performance Computing—HiPC 2002. Springer Berlin Heidelberg, 2002, pp. 63-75.</ref>\n* Step 1. Assign expressions to every node. The expression of a leaf is simply the value that it contains. Write L + R, L − R, or L × R for the operators, where L and R are the values of the expressions in the left and right subtrees, respectively.\n* Step 2. When a left (right) child with 0 children is merged into an operator, replace L (R) with the value of the child.\n* Step 3. When a node has 1 child, it has an expression that is a function of one variable. When a left (right) child with 1 child is merged into an operator, replace L (R) with the expression and change the variable in the expression to L (R) if appropriate.\n\nIn a node with 2 children, the operands in the expression are f(L) and g(R), where f and g are linear functions, and in a node with 1 child, the expression is h(x), where h is a linear function and x is either L or R. We prove this invariant by induction. At the beginning, the invariant is clearly satisfied. There are three types of merges that result in a not fully evaluated expression. (1) A 1-child node is merged into a 2-children node. (2) A leaf is merged into a 2-children node. (3) A 1-child node is merged into a 1-child node. All three types of merges do not change the invariant. Therefore, every merge simply evaluates or composes linear functions, which takes constant time <ref>[http://math.mit.edu/~rpeng/18434/applicationsParallelTreeContraction.pdf Applications of Parallel Tree Contraction], Samuel Yeom, 2015</ref>\n\n==References==\n{{Reflist}}\n{{refbegin}}\n{{refend}}\n\n==External links==\n{{Commons category|Tree structures}}\n* [http://courses.csail.mit.edu/6.851/spring07/scribe/lec05.pdf 6.851: Advanced Data Structures] by Prof. Erik Demaine\n\n{{CS-Trees}}\n\n[[Category:Knowledge representation]]\n\n[[de:Baum (Graphentheorie)]]']
['OntoUML', '33378172', "{{Infobox technology standard\n| name = OntoUML\n| year_started = 2005\n| domain = [[Conceptual_model|Conceptual Modeling]]\n| base_standards = [[Unified Foundational Ontology (UFO)]]\n| related_standards = [[Unified Modeling Language|UML]]\n| organization = [[Ontology & Conceptual Modeling Research Group (NEMO)]]\n| website = {{URL|http://nemo.inf.ufes.br/}}\n}}\n'''OntoUML''' is a [[Ontology|ontologically]] well-founded language for [[Ontology_(information_science)|Ontology]]-driven [[Conceptual_model|Conceptual Modeling]]. '''OntoUML''' is built as a [[Unified Modeling Language|UML]] extension based on the [[Unified Foundational Ontology (UFO)]]. UFO was created by Giancarlo Guizzardi in his Ph.D. thesis<ref>{{cite book|last1=Guizzardi|first1=Giancarlo|title=Ontological foundations for structural conceptual models|date=2005|publisher=Enschede: Telematica Instituut Fundamental Research Series|url=http://doc.utwente.nl/50826/1/thesis_Guizzardi.pdf}}</ref> and used to evaluate and re-design a fragment of the UML 2.0 metamodel. Giancarlo Guizzardi is a lead researcher at the [[Ontology & Conceptual Modeling Research Group (NEMO)]]<ref>{{cite web|title=Ontology & Conceptual Modeling Research Group (NEMO)|url=http://nemo.inf.ufes.br/}}</ref> located at the [[Federal University of Espírito Santo|Federal University of Espírito Santo (UFES)]] in [[Vitória,_Espírito_Santo|Vitória]] city, state of [[Espírito Santo]], [[Brazil]].\n\nNEMO created an OntoUML infrastructure<ref>{{cite web|title=OntoUML infrastructure|url=http://code.google.com/p/rcarraretto/}}</ref> using [[Eclipse_Modeling_Framework|Eclipse EMF]] for '''OntoUML''' model manipulation which serve as a basis for its tool support. NEMO has been actively working on tool support for the '''OntoUML''' Conceptual Modeling Language, respectively on:\n\n# extensions of [[Unified Modeling Language|UML]] production-grade tools to support OntoUML, namely, the MDG for [[Enterprise_Architect_(software)|Enterprise Architect]].\n# a standalone tool called [[OntoUML Lightweight Editor (OLED)]]<ref>{{cite web|title=OntoUML lightweight editor (OLED) repository|url=https://github.com/nemo-ufes/ontouml-lightweight-editor}}</ref> to the development, evaluation and implementation of domain ontologies.\n# a legacy OntoUML editor<ref>{{cite web|title=Legacy OntoUML editor|url=https://github.com/nemo-ufes/ontouml-editor-eclipse}}</ref> based on an old version of [[Graphical_Modeling_Framework|Eclipse/GMF]].\n\nCheck out a list of all publications of NEMO about ontologies and OntoUML: <ref>{{cite web|title=NEMO publications|url=http://nemo.inf.ufes.br/publications/}}</ref>\n\n== References ==\n<references />\n\n[[Category:UML tools]]\n[[Category:Knowledge representation]]\n[[Category:Ontology (information science)]]\n[[Category:Ontology editors]]"]
['Library system', '21140383', '[[File:Boston Public Library Reading Room.jpg|thumb|Reading Room at [[Boston Public Library, McKim Building|McKim Building]] in 2013]]Library system is a central organization created to manage and coordinate operations and services in or between different centers, buildings or [[Library branch|libraries branches]] and libraries patrons. It uses a [[Library classification]] to organize their volumes and nowadays also uses a [[Integrated library system]], an [[enterprise resource planning]] system for a [[library]], used to track items owned, orders made, bills paid, and patrons who have borrowed.<ref>Adamson, Veronica, \'\'et al.\'\' (2008). {{cite web|url= http://www.jisc.ac.uk/media/documents/programmes/resourcediscovery/lmsstudy.pdf |title=\'\'JISC & SCONUL Library Management Systems Study\'\' }}&nbsp;{{small|(1&nbsp;MB)}}. Sheffield, UK: Sero Consulting. p. 51. Retrieved on 21 January 2009. "... a Library Management System (LMS or ILS \'Integrated Library System\' in US parlance)."\nSome useful library automation software are: KOHA ,Grennstone .LIBsis, and granthlaya.</ref> Many counties, states or Universities have developed their own libraries systems, among them can be named [[Los Angeles Public Library|Los Angeles Public Library System]],<ref>{{Cite web|url=http://www.lapl.org/about-lapl/press/2013-library-facts|title=Los Angeles Public Library Facts 2013 (for fiscal year 2012-13) {{!}} Los Angeles Public Library|website=www.lapl.org|access-date=2016-03-06}}</ref> [[Harvard Library|Harvard Library System]],.<ref name="AR2013">{{cite web|title=Harvard Library Annual Report FY 2013 |url=http://library.harvard.edu/annual-report-fy-2013 |date=2013 |website=Harvard Library |author=Harvard University |accessdate=17 March 2015}}</ref>\n\nMost of [[County|counties]] of every country have their own \'\'\'library system\'\'\'s that usually have between 10 to 30 libraries on every city of their counties, some of them are; [[London Public Library]] on [[Canada]] with 16 library branches, [[Helsinki Metropolitan Area Libraries]], in [[Finland]], with 63 libraries,<ref>{{cite web| url=http://www.iii.com/news/pr_display.php?id=559 | title=Helsinki Metropolitan Area Libraries (Finland) Upgrades to Sierra Services Platform | publisher=Innovative | type= Press release | date=5 February 2013 | accessdate=1 August 2014 }}</ref> and some countries, like Venezuela has only one library system for the whole country as is [[National Library of Venezuela]] with 685 branches.  In the United States can be named [[Boston Public Library|Boston Public Library System]], [[New York Public Library|New York Public Library System]], [[District of Columbia Public Library|District of Columbia Public Library System]], among others.\n\n==See also==\n* [[Integrated library system]]\n* [[Library classification]]\n* [[Library branch]]\n* [[List of the largest libraries in the United States]]\n\n==References==\n{{reflist}}\n\n[[Category:Public libraries]]\n[[Category:Private libraries]]\n[[Category:Libraries]]\n[[Category:Culture]]\n[[Category:Knowledge representation]]\n\n\n{{Library-stub}}']
