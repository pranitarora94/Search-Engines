['Category:Data management', '762162', "{{Commons cat|Data management}}\n*'''[[Data management]]''' — all the disciplines related to managing '''{{C|Data|data}}''' as a valuable resource.\n\n{{clr}}\n:::::{{Catmain|Data management}}\n{{catdiffuse}}\n{{CategoryTOC}}\n\n{{Database}}\n{{Databases}}\n\n[[Category:Data|Management]]\n[[Category:Computer data|Management]]\n[[Category:Information retrieval]]\n[[Category:Information technology management]]"]
['Category:Directories', '3119166', 'A directory maintains a list for reference or commercial purposes.  This category contains articles about directories.\n{{Cat main|Directories}}\n{{Commons cat|Directories}}\n\n[[Category:Telephony]]\n[[Category:Reference works]]\n[[Category:Data management]]\n[[Category:Information retrieval]]']
['Category:Information retrieval evaluation', '46965336', 'The main overview for this category is at {{section link|Information retrieval|Performance and correctness_measures}}.\n\n[[Category:Information retrieval]]']
['Evaluation measures (information retrieval)', '50716473', '{{Orphan|date=June 2016}}\n\nThe \'\'\'evaluation measures\'\'\' of an information retrieval system is the process of assessing how well the search results satisfied the user\'s query intent. The metrics are often split in to multiple categories. Online metrics measure actual users\' interactions with the search system. Offline metrics measure the relevance of the search engine by having expert judges measure how likely each result (or the SERP page as a whole) is to meet the information needs of the user.\n\nThe mathematical symbols used in the formulas below mean:\n* <math>X \\cap Y</math> - [[Intersection (set theory)|Intersection]] - in this case, specifying the documents in \'\'both\'\' sets X and Y\n* <math>| X |</math> - [[Cardinality]] - in this case, the number of documents in set X\n* <math>\\int</math> - [[Integral]]\n* <math>\\sum</math> - [[Summation]]\n* <math>\\Delta</math> - [[Symmetric difference]]\n\n== Online metrics ==\nOnline metrics are generally created from data mined from search logs. The metrics are often used to determine the success of an [[A/B testing|A/B test]].\n\n=== Session abandonment rate ===\nSession abandonment rate is a ratio of search session which do not result in a click.\n\n=== Click-through rate ===\nClick-through rate (\'\'\'CTR\'\'\') is the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an [[online advertising]] campaign for a particular website as well as the effectiveness of email campaigns.<ref name=AMA>[[American Marketing Association]] Dictionary. http://www.marketingpower.com/_layouts/Dictionary.aspx.{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} Retrieved 2012-11-02. The [[Marketing Accountability Standards Board (MASB)]] endorses this definition as part of its ongoing [http://www.commonlanguage.wikispaces.net/ Common Language in Marketing Project].</ref>\n\n=== Session success rate ===\nSession success rate measures the ratio of user sessions that lead to a success. Defining "success" is often dependent on context, but for search a successful result is often measured using [[Dwell time (information retrieval)|dwell time]] as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet.\n\n=== Zero result rate ===\nZero result rate (\'\'\'ZRR\'\'\') is the ratio of [[Search engine results page|SERPs]] which returned with zero results. The metric either indicates a [[Precision and recall|recall]] issue, or that the information being searched for is not in the index.\n\n== Offline metrics ==\nOffline metrics are generally created from relevance judgement sessions where the judges score the quality of the search results.  The judges often score each result of a query as either binary (good/bad), or on a multi-level scale of satisfying the needs of the searcher. In practice, queries may be [[ill-posed]] and there may be different shades of relevancy. For instance there is ambiguity in the query "mars", the judge does not know if the user is search for [[Mars]] the planet, [[Mars (chocolate bar)|Mars]] the chocolate bar, or [[Bruno Mars]] the singer.\n\n=== Precision ===\n{{main|Precision and recall}}\n\nPrecision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user\'s information need.\n\n:<math> \\mbox{precision}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{retrieved documents}\\}|} </math>\n\nIn [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called \'\'precision at n\'\' or \'\'P@n\'\'.\n\nNote that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].\n\n=== Recall ===\n{{main|Precision and recall}}\n\nRecall is the fraction of the documents that are relevant to the query that are successfully retrieved.\n\n:<math>\\mbox{recall}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{relevant documents}\\}|} </math>\n\nIn binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as \'\'the probability that a relevant document is retrieved by the query\'\'.\n\nIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\n\n=== Fall-out ===\nThe proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:\n\n:<math> \\mbox{fall-out}=\\frac{|\\{\\mbox{non-relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{non-relevant documents}\\}|} </math>\n\nIn binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to <math>(1-\\mbox{specificity})</math>. It can be looked at as \'\'the probability that a non-relevant document is retrieved by the query\'\'.\n\nIt is trivial to achieve fall-out of 0% by returning zero documents in response to any query.\n\n=== F-score / F-measure ===\n{{main|F-score}}\nThe weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:\n\n:<math>F = \\frac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{(\\mathrm{precision} + \\mathrm{recall})}</math>\n\nThis is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.\n\nThe general formula for non-negative real <math>\\beta</math> is:\n:<math>F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\mathrm{precision} \\cdot \\mathrm{recall})}{(\\beta^2 \\cdot \\mathrm{precision} + \\mathrm{recall})}\\,</math>\n\nTwo other commonly used F measures are the <math>F_{2}</math> measure, which weights recall twice as much as precision, and the <math>F_{0.5}</math> measure, which weights precision twice as much as recall.\n\nThe F-measure was derived by van Rijsbergen (1979) so that <math>F_\\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen\'s effectiveness measure <math>E = 1 - \\frac{1}{\\frac{\\alpha}{P} + \\frac{1-\\alpha}{R}}</math>.  Their relationship is:\n:<math>F_\\beta = 1 - E</math> where <math>\\alpha=\\frac{1}{1 + \\beta^2}</math>\n\nF-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.{{citation needed|date=June 2015}}\n\n=== Average precision ===\n<!-- [[Average precision]] redirects here -->\nPrecision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision <math>p(r)</math> as a function of recall <math>r</math>. Average precision computes the average value of <math>p(r)</math> over the interval from <math>r=0</math> to <math>r=1</math>:<ref name="zhu2004">{{cite journal |first=Mu |last=Zhu |title=Recall, Precision and Average Precision |url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}</ref>\n:<math>\\operatorname{AveP} = \\int_0^1 p(r)dr</math>\nThat is the area under the precision-recall curve.\nThis integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:\n:<math>\\operatorname{AveP} = \\sum_{k=1}^n P(k) \\Delta r(k)</math>\nwhere <math>k</math> is the rank in the sequence of retrieved documents, <math>n</math> is the number of retrieved documents, <math>P(k)</math> is the precision at cut-off <math>k</math> in the list, and <math>\\Delta r(k)</math> is the change in recall from items <math>k-1</math> to <math>k</math>.<ref name="zhu2004" />\n\nThis finite sum is equivalent to:\n:<math> \\operatorname{AveP} = \\frac{\\sum_{k=1}^n (P(k) \\times \\operatorname{rel}(k))}{\\mbox{number of relevant documents}} \\!</math>\nwhere <math>\\operatorname{rel}(k)</math> is an indicator function equaling 1 if the item at rank <math>k</math> is a relevant document, zero otherwise.<ref name="Turpin2006">{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06–11, 2006) |publisher=ACM |location=New York, NY |pages=11–18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}</ref> Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.\n\nSome authors choose to interpolate the <math>p(r)</math> function to reduce the impact of "wiggles" in the curve.<ref name=voc2010>{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303–338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}</ref><ref name="nlpbook">{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}</ref> For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:<ref name="voc2010" /><ref name="nlpbook" />\n:<math>\\operatorname{AveP} = \\frac{1}{11} \\sum_{r \\in \\{0, 0.1, \\ldots, 1.0\\}} p_{\\operatorname{interp}}(r)</math>\nwhere <math>p_{\\operatorname{interp}}(r)</math> is an interpolated precision that takes the maximum precision over all recalls greater than <math>r</math>:\n:<math>p_{\\operatorname{interp}}(r) = \\operatorname{max}_{\\tilde{r}:\\tilde{r} \\geq r} p(\\tilde{r})</math>.\n\nAn alternative is to derive an analytical <math>p(r)</math> function by assuming a particular parametric distribution for the underlying decision values. For example, a \'\'binormal precision-recall curve\'\' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.<ref>K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves] {{webarchive |url=https://web.archive.org/web/20121208201457/http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf |date=December 8, 2012 }}. \'\'Proceedings of the 20th International Conference on Pattern Recognition\'\', 4263-4266.</ref>\n\n=== Precision at K ===\n\nFor modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 or "Precision at 10" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.{{citation needed|date=June 2015}}  Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.<ref name="stanford" />  It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.\n\n=== R-Precision ===\n\nR-precision requires knowing all documents that are relevant to a query.  The number of relevant documents, <math>R</math>, is used as the cutoff for calculation, and this varies from query to query.  For example, if there are 15 documents relevant to "red" in a corpus (R=15), R-precision for "red" looks at the top 15 documents returned, counts the number that are relevant <math>r</math> turns that into a relevancy fraction: <math>r/R = r/15</math>.<ref name="trec15"/>\n\nPrecision is equal to recall at the \'\'\'R\'\'\'-th position.<ref name="stanford">{{cite web|url=http://nlp.stanford.edu/IR-book/pdf/08eval.pdf|title=Chapter 8: Evaluation in information retrieval|accessdate=2015-06-14|date=2009|authors=Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze}}  Part of \'\'Introduction to Information Retrieval\'\' [http://nlp.stanford.edu/IR-book/]</ref>\n\nEmpirically, this measure is often highly correlated to mean average precision.<ref name="stanford" />\n\n=== Mean average precision ===\n<!-- [[Mean average precision]] redirects here -->\nMean average precision for a set of queries is the mean of the average precision scores for each query.\n:<math> \\operatorname{MAP} = \\frac{\\sum_{q=1}^Q \\operatorname{AveP(q)}}{Q} \\!</math>\nwhere \'\'Q\'\' is the number of queries.\n\n=== Discounted cumulative gain ===\n{{main|Discounted cumulative gain}}\nDCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\n\nThe DCG accumulated at a particular rank position <math>p</math> is defined as:\n\n:<math> \\mathrm{DCG_{p}} = rel_{1} + \\sum_{i=2}^{p} \\frac{rel_{i}}{\\log_{2}i}. </math>\n\nSince result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (<math>IDCG_p</math>), which normalizes the score:\n\n:<math> \\mathrm{nDCG_{p}} = \\frac{DCG_{p}}{IDCG{p}}. </math>\n\nThe nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.\n\n=== Other measures ===\n{{Confusion matrix terms}}\n* [[Mean reciprocal rank]]\n* [[Spearman\'s rank correlation coefficient]]\n* bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents<ref name="trec15">http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf</ref>\n* GMAP - geometric mean of (per-topic) average precision<ref name="trec15" />\n* Measures based on marginal relevance and document diversity - see {{section link|Relevance (information retrieval)|Problems and alternatives}}\n\n===Visualization===\n\nVisualizations of information retrieval performance include:\n* Graphs which chart precision on one axis and recall on the other<ref name="trec15" />\n* Histograms of average precision over various topics<ref name="trec15" />\n* [[Receiver operating characteristic]] (ROC curve)\n* [[Confusion matrix]]\n\n== Non-metrics ==\n\n=== Top queries list ===\nTop queries is noting the most common queries over a fixed amount of time. The top queries list assists in knowing the style of queries entered by users.\n\n== Non-relevance metrics ==\n\n=== Queries per time ===\nMeasuring how many queries are performed on the search system per (month/day/hour/minute/sec) tracks the utilization of the search system. It can be used for diagnostics to indicate an unexpected spike in queries, or simply as a baseline when comparing with other metrics, like query latency. For example, a spike in query traffic, may be used to explain a spike in query latency.\n\n==References==\n<references />\n\n[[Category:Information retrieval]]\n[[Category:Information retrieval evaluation]]\n[[Category:Internet search engines]]']
['Navigational database', '622805', '{{Refimprove|date=July 2007}}\nA \'\'\'navigational database\'\'\' is a type of [[database]] in which [[Record (computer science)|records]] or [[Object (computer science)|objects]] are found primarily by following references from other objects. They were a common type of database in the era when data was stored on [[magnetic tape]]; the navigational references told the computer where the next record on the tape was stored, allowing fast-forwarding (and in some cases, reversing) through the records without having to read every record along the way to see if it matched a given criterion.\n\nThe introduction of low-cost [[hard drive]]s that provided semi-random access to data led to new models of database storage better suited to these devices. Among these, the [[relational database]] and especially [[SQL]] became the canonical solution from the 1980s through to about 2010. At that time a reappraisal of the entire database market began, the various [[NoSQL]] concepts, which has led to the navigational model being reexamined. Offshoots of the concept, especially the [[graph database]], are finding new uses in modern [[transaction processing]] workloads.\n\n==Description==\nNavigational interfaces are usually procedural, though some modern systems like [[XPath]] can be considered to be simultaneously navigational and declarative. \n\nNavigational access is traditionally associated with the [[network model]] and [[hierarchical model]] of [[database]] interfaces, and some have even acquired set-oriented features.<ref>{{cite book | author = Błażewicz, Jacek |author2=Królikowski, Zbyszko |author3=Morzy, Tadeusz | title = Handbook on Data Management in Information Systems  | publisher = Springer  | year = 2003  | location =  | page = 18  | url = https://books.google.com/books?id=AvLziHKyuLcC&pg=PA18&dq=%22Navigational+database%22+-wikipedia+network+model+and+hierarchical+model&ie=ISO-8859-1| doi =  | isbn = 3-540-43893-9 }}</ref> Navigational techniques use "pointers" and "paths" to navigate among data records (also known as "nodes"). This is in contrast to the [[relational model]] (implemented in [[relational database]]s), which strives to use "declarative" or [[logic programming]] techniques that ask the system for \'\'what\'\' to fetch instead of \'\'how\'\' to navigate to it.  \n\nFor example, to give directions to a house, the navigational approach would resemble something like "Get on highway 25 for 8 miles, turn onto Horse Road, left at the red barn, then stop at the 3rd house down the road", whereas the declarative approach would resemble "Visit the green house(s) within the following coordinates...."\n\nHierarchical models are also considered navigational because one "goes" up (to parent), down (to leaves), and there are "paths", such as the familiar file/folder paths in hierarchical file systems. In general, navigational systems will use combinations of paths and prepositions such as "next", "previous", "first", "last", "up", "down", "owner", etc.\n\n"Paths" are often formed by concatenation of [[Node (computer science)|node]] names or node addresses. Example:\n\n[[File:6n-graf.svg|thumb|250px|Sample database nodes: A labeled graph on 6 vertices and 7 edges. (Numbers are used for illustration purposes only. In practice more meaningful names are often used. Other potential attributes are not shown.)]]\n\n  Node6.Node4.Node5.Node1\n\nOr\n\n  Node6/Node4/Node5/Node1\n\nIf there is no link between given nodes, then an error condition is usually triggered with a message such as "Invalid Path".  The path "Node6.Node2.Node1" would be invalid in most systems because there is no direct link between Node 6 and Node 2.\n\nThe usage of the term "navigational" allegedly is derived from a statement by [[Charles Bachman]] in which he describes the "programmer as navigator" while accessing his favored type of database.<ref>{{cite web|url=http://portal.acm.org/citation.cfm?id=362534&coll=portal&dl=ACM |title=The programmer as navigator |doi=10.1145/355611.362534 |publisher=Portal.acm.org |accessdate=2012-10-01}}</ref>\n\nExcept for hierarchical file systems (which some consider a form of database), navigational techniques fell out of favor by the 1980s. However, [[object oriented programming]] and [[XML]] have kindled a renewed, but controversial interest in navigational techniques.\n\nCritics of navigational techniques view them as "unstructured spaghetti messes", and liken them to the "[[Goto (command)|goto]]" of pre-[[structured programming]]. In other words, they are allegedly to data organization what goto\'s were to behavior flow. In this view, relational techniques provide improved discipline and consistency to data organization and usage because of its roots in [[set theory]] and [[predicate calculus]]. \n\nSome also suggest that navigational database engines are easier to build and take up less memory (RAM) than relational equivalents. However, the existence of relational or relational-based products of the late 1980s that possessed small engines (by today\'s standards) because they didn\'t use SQL suggest this is not necessarily the case. Whatever the reason, navigational techniques are still the preferred way to handle smaller-scale structures.\n\nA current example of navigational structuring can be found in the [[Document Object Model]] (DOM) often used in web browsers and closely associated with [[JavaScript]]. The DOM "engine" is essentially a light-weight navigational database. The [[World Wide Web]] itself and Wikipedia could potentially be considered forms of navigational databases, though they focus on human-readable text rather than data (on a large scale, the Web is a network model and on smaller or local scales, such as domain and URL partitioning, it uses hierarchies).  In contrast, the [[Linked Data]] facet of the [[Semantic Web]] is specifically concerned with network-scale [[machine-readable data]], and follows precisely the \'follow your nose\' paradigm implied by the navigational idea.\n\nA new kind of navigational databases{{fact|date=August 2015}} has recently{{when|date=August 2015}} emerged, the [[graph databases]]. This category of databases is often included as one of the four family of the [[NoSQL]] databases.\n\n==Examples==\n* [[IBM Information Management System]]\n* [[IDMS]]\n\n==See also==\n* [[CODASYL]]\n* [[Graph database]]\n* [[Network database]]\n* [[Object database]]\n* [[Relational database]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://db-engines.com/en/ranking/navigational+dbms DB-Engines Ranking of Navigational DBMS] by popularity, updated by month\n\n\n[[Category:Data management]]\n[[Category:Types of databases]]']
['Schedule (computer science)', '400457', '{{refimprove|date=November 2012}}\n\nIn the fields of [[database]]s and [[transaction processing]] (transaction management), a \'\'\'schedule\'\'\' (or \'\'\'history\'\'\') of a system is an abstract model to describe execution of transactions running in the system. Often it is a \'\'list\'\' of operations (actions) ordered by time, performed by a set of [[Database transaction|transactions]] that are executed together in the system. If order in time between certain operations is not determined by the system, then a \'\'[[partial order]]\'\' is used. Examples of such operations are requesting a read operation, reading, writing, aborting, committing, requesting lock, locking, etc. Not all transaction operation types should be included in a schedule, and typically only selected operation types (e.g., data access operations) are included, as needed to reason about and describe certain phenomena. Schedules and schedule properties are fundamental concepts in database [[concurrency control]] theory.\n\n==Formal description==\n\nThe following is an example of a schedule:\n\n:<math>D = \\begin{bmatrix}\nT1 & T2 & T3 \\\\\nR(X) &  &  \\\\\nW(X) &  &  \\\\\nCom. &  &  \\\\\n & R(Y) & \\\\\n & W(Y) & \\\\\n & Com. & \\\\\n && R(Z) \\\\\n && W(Z) \\\\\n && Com. \\end{bmatrix}</math>\n\nIn this example, the horizontal axis represents the different transactions in the schedule D. The vertical axis represents time order of operations. Schedule D consists of three transactions T1, T2, T3.  The schedule describes the actions of the transactions as seen by the [[DBMS]].\nFirst T1 Reads and Writes to object X, and then Commits. Then T2 Reads and Writes to object Y and Commits, and finally T3 Reads and Writes to object Z and Commits.  This is an example of a \'\'serial\'\' schedule, i.e., sequential with no overlap in time, because the actions of in all three transactions are sequential, and the transactions are not interleaved in time.\n\nRepresenting the schedule D above by a table (rather than a list) is just for the convenience of identifying each transaction\'s operations in a glance. This notation is used throughout the article below. A more common way in the technical literature for representing such schedule is by a list:\n\n:::D = R1(X) W1(X) Com1 R2(Y) W2(Y) Com2 R3(Z) W3(Z) Com3\n\nUsually, for the purpose of reasoning about concurrency control in databases, an operation is modeled as \'\'[[Atomic operation|atomic]]\'\', occurring at a point in time, without duration. When this is not satisfactory start and end time-points and possibly other point events are specified (rarely). Real executed operations always have some duration and specified respective times of occurrence of events within them (e.g., "exact" times of beginning and completion), but for concurrency control reasoning usually only the precedence in time of the whole operations (without looking into the quite complex details of each operation) matters, i.e., which operation is before, or after another operation. Furthermore, in many cases the before/after relationships between two specific operations do not matter and should not be specified, while being specified for other pairs of operations.\n\nIn general operations of transactions in a schedule can interleave (i.e., transactions can be executed concurrently), while time orders between operations in each transaction remain unchanged as implied by the transaction\'s program. Since not always time orders between all operations of all transactions matter and need to be specified, a schedule is, in general, a \'\'[[partial order]]\'\' between operations rather than a \'\'[[total order]]\'\' (where order for each pair is determined, as in a list of operations). Also in the general case each transaction may consist of several processes, and itself be properly represented by a partial order of operations, rather than a total order. Thus in general a schedule is a partial order of operations, containing ([[embedding]]) the partial orders of all its transactions.\n\nTime-order between two operations can be represented by an \'\'[[ordered pair]]\'\' of these operations (e.g., the existence of a pair (OP1,OP2) means that OP1 is always before OP2), and a schedule in the general case is a [[set (mathematics)|set]] of such ordered pairs. Such a set, a schedule, is a [[partial order]] which can be represented by an \'\'[[acyclic directed graph]]\'\' (or \'\'directed acyclic graph\'\', DAG) with operations as nodes and time-order as a [[directed edge]] (no cycles are allowed since a cycle means that a first (any) operation on a cycle can be both before and after (any) another second operation on the cycle, which contradicts our perception of [[Time]]). In many cases a graphical representation of such graph is used to demonstrate a schedule.\n\n\'\'\'Comment:\'\'\' Since a list of operations (and the table notation used in this article) always represents a total order between operations, schedules that are not a total order cannot be represented by a list (but always can be represented by a DAG).\n\n==Types of schedule==\n\n===Serial===\n\nThe transactions are executed non-interleaved (see example above)\ni.e., a serial schedule is one in which no transaction starts until a running transaction has ended.\n\n===Serializable===<!-- This section is linked from [[Concurrency control]] -->\n\nA schedule that is equivalent (in its outcome) to a serial schedule has the [[serializability]] property.\n\nIn schedule E, the order in which the actions of the transactions are executed is not the same as in D, but in the end, E gives the same result as D.\n:<math>E = \\begin{bmatrix}\nT1 & T2 & T3 \\\\\nR(X) &  &  \\\\\n   & R(Y) & \\\\\n && R(Z) \\\\\n\nW(X) &  &  \\\\\n & W(Y) & \\\\\n && W(Z) \\\\\nCom. & Com. & Com. \\end{bmatrix}</math>\n\n====Conflicting actions====\n\nTwo actions are said to be in conflict (conflicting pair) if: \n\n# The actions belong to different transactions.\n# At least one of the actions is a write operation.\n# The actions access the same object (read or write).\n\nThe following set of actions is conflicting: \n* R1(X), W2(X), W3(X) (3 conflicting pairs)\n\nWhile the following sets of actions are not: \n* R1(X), R2(X), R3(X)\n* R1(X), W2(Y), R3(X)\n\n====Conflict equivalence====\n\nThe schedules S1 and S2 are said to be conflict-equivalent if the following two conditions are satisfied: \n\n# Both schedules S1 and S2 involve the same set of transactions (including ordering of actions within each transaction).\n# Both schedules have same set of conflicting operations.\n\n====Conflict-serializable====\n\nA schedule is said to be conflict-serializable when the schedule is conflict-equivalent to one or more serial schedules. \n\nAnother definition for conflict-serializability is that a schedule is conflict-serializable if and only if its [[precedence graph]]/serializability graph, when only committed transactions are considered, is acyclic (if the graph is defined to include also uncommitted transactions, then cycles involving uncommitted transactions may occur without conflict serializability violation).\n\n:<math>G = \\begin{bmatrix}\nT1 & T2 \\\\\nR(A) &   \\\\\n & R(A) \\\\\nW(B) & \\\\\nCom. & \\\\\n & W(A) \\\\\n & Com. \\\\\n &\\end{bmatrix}</math>\n\nWhich is conflict-equivalent to the serial schedule <T1,T2>, but not <T2,T1>.\n\n====Commitment-ordered====\n{{POV-section|Commitment ordering|date=November 2011}}\nA schedule is said to be commitment-ordered (commit-ordered), or commitment-order-serializable, if it obeys the [[Commitment ordering]] (CO; also commit-ordering or commit-order-serializability) schedule property. This means that the order in time of transactions\' commitment events is compatible with the precedence (partial) order of the respective transactions, as induced by their schedule\'s acyclic precedence graph (serializability graph, conflict graph). This implies that it is also conflict-serializable. The CO property is especially effective for achieving [[Global serializability]] in distributed systems.\n\n\'\'\'Comment:\'\'\' [[Commitment ordering]], which was discovered in 1990, is obviously not mentioned in ([[#Bern1987|Bernstein et al. 1987]]). Its correct definition appears in ([[#Weikum2001|Weikum and Vossen 2001]]), however the description there of its related techniques and theory is partial, inaccurate, and misleading.{{Says who|date=December 2011}} For an extensive coverage of commitment ordering and its sources see \'\'[[Commitment ordering]]\'\' and \'\'[[The History of Commitment Ordering]]\'\'.\n\n====View equivalence====\n\nTwo schedules S1 and S2 are said to be view-equivalent when the following conditions are satisfied:\n\n# If the transaction <math>T_i</math> in S1 reads an initial value for object X, so does the transaction <math>T_i</math> in S2. \n# If the transaction <math>T_i</math> in S1 reads the value written by transaction <math>T_j</math> in S1 for object X, so does the transaction <math>T_i</math> in S2.\n# If the transaction <math>T_i</math> in S1 is the final transaction to write the value for an object X, so is the transaction <math>T_i</math> in S2.\n\n====View-serializable====\n\nA schedule is said to be view-serializable if it is view-equivalent to some serial schedule. \nNote that by definition, all conflict-serializable schedules are view-serializable. \n\n:<math>G = \\begin{bmatrix}\nT1 & T2 \\\\\nR(A) &   \\\\\n & R(A) \\\\\nW(B) & \\\\\n \\end{bmatrix}</math>\n\nNotice that the above example (which is the same as the example in the discussion of conflict-serializable) is both view-serializable and conflict-serializable at the same time.) There are however view-serializable schedules that are not conflict-serializable: those schedules with a transaction performing a [[blind write]]:\n \n:<math>H = \\begin{bmatrix}\nT1 & T2 & T3 \\\\\nR(A) & & \\\\\n & W(A) & \\\\\n & Com. & \\\\\nW(A) & & \\\\\nCom. & & \\\\\n & & W(A) \\\\\n & & Com. \\\\\n & & \\end{bmatrix}</math>\n\nThe above example is not conflict-serializable, but it is view-serializable since it has a view-equivalent serial schedule <T1,&nbsp;T2,&nbsp;T3>. \n\nSince determining whether a schedule is view-serializable is [[NP-complete]], view-serializability has little practical interest.{{citation needed|date=April 2015}}\n\n===Recoverable===<!-- This section is linked from [[Concurrency control]] -->\n\nTransactions commit only after all transactions whose changes they read, commit.\n\n:<math>F = \\begin{bmatrix}\nT1 & T2 \\\\\nR(A) &   \\\\\nW(A) &   \\\\\n & R(A) \\\\\n & W(A) \\\\\nCom. & \\\\\n & Com.\\\\\n &\\end{bmatrix} \nF2 = \\begin{bmatrix}\nT1 & T2 \\\\\nR(A) &   \\\\\nW(A) &   \\\\\n & R(A) \\\\\n & W(A) \\\\\nAbort &  \\\\\n& Abort \\\\\n &\\end{bmatrix}</math>\n\nThese schedules are recoverable.  F is recoverable because T1 commits before T2, that makes the value read by T2 correct.  Then T2 can commit itself.  In F2, if T1 aborted, T2 has to abort because the value of A it read is incorrect.  In both cases, the database is left in a consistent state.\n\n====Unrecoverable====\n\nIf a transaction T1 aborts, and a transaction T2 commits, but T2 relied on T1, we have an unrecoverable schedule.\n\n:<math>G = \\begin{bmatrix}\nT1 & T2 \\\\\nR(A) &   \\\\\nW(A) &   \\\\\n & R(A) \\\\\n & W(A) \\\\\n & Com. \\\\\nAbort & \\\\\n &\\end{bmatrix}</math>\n\nIn this example, G is unrecoverable, because T2 read the value of A written by T1, and committed.  T1 later aborted, therefore the value read by T2 is wrong, but since T2 committed, this schedule is unrecoverable.\n\n====Avoids cascading aborts / rollbacks (ACA)====\n\nAlso named cascadeless. Avoids that a single transaction abort leads to a series of transaction rollbacks. A strategy to prevent cascading aborts is to disallow a transaction from reading uncommitted changes from another transaction in the same schedule. \n\nThe following examples are the same as the ones in the discussion on recoverable: \n\n:<math>F = \\begin{bmatrix}\nT1 & T2 \\\\\nR(A) &   \\\\\nW(A) &   \\\\\n & R(A) \\\\\n & W(A) \\\\\nCom. & \\\\\n & Com.\\\\\n &\\end{bmatrix} \nF2 = \\begin{bmatrix}\nT1 & T2 \\\\\nR(A) &   \\\\\nW(A) &   \\\\\n & R(A) \\\\\n & W(A) \\\\\nAbort &  \\\\\n& Abort \\\\\n &\\end{bmatrix}</math>\n\nIn this example, although F2 is recoverable, it does not avoid \ncascading aborts. It can be seen that if T1 aborts, T2 will have to \nbe aborted too in order to maintain the correctness of the schedule \nas T2 has already read the uncommitted value written by T1. \n\nThe following is a recoverable schedule which avoids cascading abort. Note, however, that the update of A by T1 is always lost (since T1 is aborted).\n\n:<math>F3 = \\begin{bmatrix}\nT1 & T2 \\\\\n & R(A) \\\\\nR(A) &   \\\\\nW(A) &   \\\\\n & W(A) \\\\\nAbort &  \\\\\n& Commit \\\\\n &\\end{bmatrix}</math>\nNote that this Schedule would not be serializable if T1 would be committed.\nCascading aborts avoidance is sufficient but not necessary for a schedule to be recoverable.\n\n====Strict====\n\nA schedule is strict - has the strictness property - if for any two transactions T1, T2, if a write operation of T1 precedes a \'\'conflicting\'\' operation of T2 (either read or write), then the commit or abort event of T1 also precedes that conflicting operation of T2.\n\nAny strict schedule is cascadeless, but not the converse. Strictness allows efficient recovery of databases from failure.\n\n==Hierarchical relationship between serializability classes==\n\nThe following expressions illustrate the hierarachical (containment) relationships between [[serializability]] and [[Serializability#Correctness - recoverability|recoverability]] classes: \n\n* Serial &sub; commitment-ordered &sub; conflict-serializable &sub; view-serializable &sub; all schedules\n* Serial &sub; strict &sub; avoids cascading aborts &sub; recoverable &sub; all schedules\n\nThe [[Venn diagram]] (below) illustrates the above clauses graphically. \n\n[[File:Schedule-serializability.png|frame|none|Venn diagram for serializability and recoverability classes]]\n\n==Practical implementations==\n\nIn practice, most general purpose database systems employ conflict-serializable and recoverable (primarily strict) schedules.\n\n==See also==\n* [[schedule (project management)]]\n\n==References==\n\n*<cite id=Bern1987>[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman: [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  \'\'Concurrency Control and Recovery in Database Systems\'\'], Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5</cite>\n*<cite id=Weikum2001>[[Gerhard Weikum]], Gottfried Vossen: [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  \'\'Transactional Information Systems\'\'], Elsevier, 2001, ISBN 1-55860-508-8</cite>\n\n[[Category:Data management]]\n[[Category:Transaction processing]]']
['Category:Metadata', '2388558', '{{Infobox library classification|CC = |DDC = |LCC = |UDC = 001.103.2}}\n{{catdiffuse}}\n{{Cat main|Metadata}}\n\n[[Category:Data management]]\n[[Category:Data]]']
['Savepoint', '1544409', '{{For|save points in video games|Saved game}}\n{{refimprove|date=September 2014}}\n\nA \'\'\'savepoint\'\'\' is a way of implementing subtransactions (also known as [[nested transaction]]s) within a [[relational database management system]] by indicating a point within a [[database transaction|transaction]] that can be "[[Rollback (data management)|rolled back to]]" without affecting any work done in the transaction before the savepoint was created. Multiple savepoints can exist within a single transaction. Savepoints are useful for implementing complex error recovery in database applications. If an error occurs in the midst of a multiple-statement transaction, the application may be able to recover from the error (by rolling back to a savepoint) without needing to abort the entire transaction.\n\nA savepoint can be declared by issuing a <code>SAVEPOINT \'\'name\'\'</code> statement. All changes made after a savepoint has been declared can be undone by issuing a <code>ROLLBACK TO SAVEPOINT \'\'name\'\'</code> command. Issuing <code>RELEASE SAVEPOINT \'\'name\'\'</code> will cause the named savepoint to be discarded, but will not otherwise affect anything. Issuing the commands <code>ROLLBACK</code> or <code>COMMIT</code> will also discard any savepoints created since the start of the main transaction.[http://docs.oracle.com/cd/B19306_01/appdev.102/b14261/savepoint_statement.htm]\n\nSavepoints are supported in some form or other in database systems like [[PostgreSQL]], [[Oracle database|Oracle]], [[Microsoft SQL Server]], [[MySQL]], [[IBM DB2|DB2]], [[SQLite]] (since 3.6.8), [[Firebird (database server)|Firebird]], [[H2_(DBMS)|H2 Database Engine]], and [[Informix]] (since version 11.50xC3). Savepoints are also defined in the [[SQL#Interoperability_and_standardization|SQL standard]].\n\n{{databases}}\n\n[[Category:Data management]]\n[[Category:Transaction processing]]']
['Enterprise information integration', '773166', '{{multiple issues|{{refimprove|date=February 2015}}\n{{POV|date=February 2011}}}}\n\'\'\'Enterprise information integration\'\'\' (\'\'\'EII\'\'\') is the ability to support a unified view of data and information for an entire organization.  In a [[data virtualization]] application of EII, a process of [[information integration]], using [[data abstraction]] to provide a unified interface (known as [[uniform data access]]) for viewing all the data within an organization, and a single set of structures and naming conventions (known as [[uniform information representation]]) to represent this data; the goal of EII is to get a large set of [[heterogeneous]] data sources to appear to a user or system as a single, homogeneous data source.\n\n== Overview ==\n[[Data]] within an [[Enterprise architecture|enterprise]] can be stored in heterogeneous formats, including [[relational database]]s (which themselves come in a large number of varieties), text files, [[XML]] files, [[spreadsheet]]s and a variety of proprietary [[data storage device|storage]] methods, each with their own [[index (database)|index]]ing and [[data access]] methods.\n\nStandardized data access [[application programming interface|API]]s have emerged, that offer a specific set of commands to retrieve and modify data from a generic data source. Many applications exist that implement these APIs\' commands across various data sources, most notably relational databases. Such APIs include [[ODBC]], [[JDBC]], [[XQJ]], [[OLE DB]], and more recently [[ADO.NET]].\n\nThere are also standard formats for representing data within a file, that are very important to information integration. The best-known of these is XML, which has emerged as a standard universal representation format. There are also more specific XML "grammars" defined for specific types of data, such as [[Geography Markup Language]] for expressing geographical features, and [[Directory Service Markup Language]], for holding directory-style information. In addition, non-XML standard formats exist, such as [[iCalendar]], for representing calendar information, and [[vCard]], for [[business card]] information.\n\nEnterprise Information Integration (EII) applies [[data integration]] commercially.  Despite the theoretical problems described above, the private sector shows more concern with the problems of data integration as a viable product.<ref name="refthree">{{cite conference | author=Alon Y. Halevy | title=Enterprise information integration: successes, challenges and controversies | booktitle=SIGMOD 2005 | year=2005 | pages=778–787 | url=http://www.cs.washington.edu/homes/alon/files/eiisigmod05.pdf|display-authors=etal}}</ref>\nEII emphasizes neither on correctness nor tractability, but speed and simplicity. An EII industry has emerged, but many professionals{{Who|date=June 2009}} believe it does not perform to its full potential.  Practitioners cite the following major issues which  EII must address for the industry to become mature:{{Citation needed|date=June 2009}}\n\n; Combining disparate data sets : Each data source is disparate and as such is not designed to support EII.  Therefore, data virtualization as well as [[Federated database system|data federation]] depends upon accidental data commonality to support combining data and information from disparate data sets.  Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.\n\n:  One solution is to recast disparate databases to integrate these databases without the need for [[Extract, transform, load|ETL]]. The recast databases support commonality constraints where referential integrity may be enforced between databases.  The recast databases provide designed data access paths with data value commonality across databases.\n; Simplicity of understanding : Answering queries with views arouses interest from a theoretical standpoint, but difficulties in understanding how to incorporate it as an "enterprise solution".{{Citation needed|date=June 2009}}  Some developers{{Who|date=June 2009}} believe it should be merged with [[Enterprise application integration|EAI]].  Others{{Who|date=June 2009}} believe it should be incorporated with ETL systems, citing customers\' confusion over the differences between the two services.{{Citation needed|date=June 2009}}\n; Simplicity of deployment : Even if recognized as a solution to a problem, EII {{as of | 2009 | lc = on}} currently takes time to apply and offers complexities in deployment.  People have proposed a variety of schema-less solutions such as "Lean Middleware",<ref name="reffour">{{cite conference | author=David A. Maluf | title=Lean middleware | booktitle=SIGMOD 2005 | year=2005 | pages=788–791 | url=http://portal.acm.org/citation.cfm?id=1066157.1066247&coll=portal&dl=ACM&type=series&idx=1066157&part=Proceedings&WantType=Proceedings&title=International%20Conference%20on%20Management%20of%20Data&CFID=15151515&CFTOKEN=6184618|display-authors=etal}}</ref> but ease-of-use and speed of employment appear inversely proportional to the generality of such systems.{{Citation needed|date=June 2009}}  Others{{Who|date=June 2009}} cite the need for standard data interfaces to speed and simplify the integration process in practice.\n; Handling higher-order information : Analysts experience difficulty—even with a functioning information integration system—in determining whether the sources in the database will  satisfy a given application.  Answering these kinds of questions about a set of repositories requires semantic information like [[metadata]] and/or ontologies.  The few commercial tools{{Which|date=June 2009}} that leverage this information remain in their infancy.\n\n== Applications ==\nEII products enable [[loose coupling]] between [[wiktionary:Homogeneous|homogeneous]]-data consuming client applications and services and heterogeneous-data stores.  Such client applications and services include Desktop Productivity Tools (spreadsheets, [[word processor]]s, presentation software, etc.), [[Integrated development environment|development environment]]s and [[Software framework|framework]]s ([[Java EE]], [[Microsoft .NET|.NET]], [[Mono (software)|Mono]], [[SOAP]] or [[Representational State Transfer|REST]]ful [[Web service]]s, etc.), [[business intelligence]] (BI), [[business activity monitoring]] (BAM) software, [[enterprise resource planning]] (ERP), [[Customer relationship management]] (CRM), [[business process management]] (BPM and/or BPEL) Software, and [[web content management]] (CMS).\n\n==  Example technology vendors ==\n*[[Capsenta]]\n*[[Composite Software]]\n*[[Denodo]]\n*[[MetaMatrix]]\n*[[XAware]]\n\n== Data access technologies ==\n*[[XQuery]] and [[XQuery API for Java]]\n*[[Service Data Objects]] (SDO) for Java, C++ and .Net clients and any type of data source\n\n== See also ==\n{{div col|3}}\n* [[Business Intelligence 2.0]] (BI 2.0)\n* [[Data warehouse]]\n* [[Disparate system]]\n* [[Enterprise integration]]\n* [[Federated database system]]\n* [[Resource Description Framework]]\n* [[Semantic heterogeneity]]\n* [[Semantic integration]]\n* [[Semantic Web]]\n* [[Web 2.0]]\n* [[Web services]]\n{{div col end}}\n\n==References==\n\n<references/>\n\n[[Category:Data management]]']
['Data steward', '6212365', '{{merge|Data custodian|date=February 2016}}\n\nA \'\'\'data steward\'\'\' is a person responsible for the management and fitness of [[data element]]s  - both the content and [[metadata]]. Data stewards have a specialist role that incorporates processes, policies, guidelines and responsibilities for administering organizations\' entire data in compliance with policy and/or regulatory obligations. A data steward may share some responsibilities with a [[data custodian]].\n\nThe overall objective of a data steward is [[data quality]], in regard to the key/critical data elements existing within a specific enterprise operating structure, of the elements in their respective domains. This includes capturing/documenting (meta)information for their elements (such as: definitions, related rules/governance, physical manifestation, related data models, etc. With most of these properties being specific to an attribute/concept relationship), identifying owners/custodians/various  responsibilities, relations insight pertaining to attribute quality, aiding with project requirement data facilitation and documentation of capture rules. \n\nData stewards begin the [[stewardship|stewarding]] process with the identification of the elements which they will steward, with the ultimate result being standards, [[Control (disambiguation)|control]]s and [[Data entry clerk|data entry]].{{citation needed|date=October 2014}}  The steward works closely with business glossary standards analysts (for standards), with [[data architect]]/[[Data modeling|modeler]]s (for standards), with  [[Data quality|DQ]] analysts (for controls) and with [[Computer operator|operations team member]]s (good-quality data going in per business rules) while entering data.\n\nData stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data-related resources.{{citation needed|date=October 2014}}  [[Master data management]] often{{quantify|date=October 2014}} makes references to the need for data stewardship for its implementation to succeed. Data stewardship must have precise purpose, fit for purpose or fitness.\n\n==Data Steward Responsibilities==\nA data steward ensures that each assigned data element:\n# Has clear and unambiguous [[data element definition]].\n# Does not conflict with other data elements in the metadata registry (removes duplicates, overlap etc.)\n# Has clear enumerated value definitions if it is of type [[Code (metadata)|Code]].\n# Is still being used (remove unused data elements)\n# Is being used consistently in various computer systems\n# Is being used, fit for purpose = Data Fitness.\n# Has adequate documentation on appropriate usage and notes\n# Documents the origin and sources of authority on each metadata element\n# Is protected against unauthorised access or change\n\n==Benefits of data stewardship==\n\nSystematic data stewardship can foster fitness through:\n\n# consistent use of data management resources\n# easy mapping of data between computer systems and exchange documents\n# lower costs associated with migration to (for example) [[Service Oriented Architecture]] (SOA)\n\nAssignment of each data element to a person sometimes seems like an unimportant process. But many groups{{Which|date=July 2010}} have found that users have greater trust and usage rates in systems where they can contact a person with questions on each data element.\n\n== Examples ==\n{{Expand section|date=July 2010}}\n\nThe [http://www.epa.gov/edr [[United States Environmental Protection Agency|EPA]] metadata registry] furnishes an example of data stewardship.  Note that each data element therein has a "POC"  (point of contact).\n\n== Data Stewardship Applications ==\nA new market for data governance applications is emerging, one in which both technical and business staff — stewards — manage policies. These new applications, like previous generations, deliver a strong business glossary capability, but they don\'t stop there. Vendors are introducing additional features addressing the roles of business in addition to technical stewards\' concerns.<ref>{{Cite web|url=https://www.forrester.com/report/The+Forrester+Wave+Data+Governance+Stewardship+Applications+Q1+2016/-/E-RES117915|title=The Forrester Wave™: Data Governance Stewardship Applications, Q1 2016|website=www.forrester.com|access-date=2016-12-20}}</ref>\n\nInformation stewardship applications are business solutions used by business users acting in the role of information steward (interpreting and enforcing information governance policy, for example). These developing solutions represent, for the most part, an amalgam of a number of disparate, previously IT-centric tools already on the market, but are organized and presented in such a way that information stewards (a business role) can support the work of information policy enforcement as part of their normal, business-centric, day-to-day work in a range of use cases.\n\nThe initial push for the formation of this new category of packaged software came from operational use cases — that is, use of business data in and between transactional and operational business applications. This is where most of the master data management (MDM) efforts are undertaken in organizations. However, there is also now a faster-growing interest in the new data lake arena for more analytical use cases.<ref>{{Cite web|url=https://www.gartner.com/document/3284717?ref=TypeAheadSearch&qid=744b6ad6c678d064cc2d6eb831a4c959|title=Market Guide for Information Stewardship Applications|last=De Simoni|first=Guido|date=15 April 2016|website=www.gartner.com|publisher=Gartner|access-date=}}</ref>\n\n==See also==\n* [[Metadata]]\n* [[Metadata registry]]\n* [[Data curation]]\n* [[Data element]]\n* [[Data element definition]]\n* [[Representation term]]\n* [[ISO/IEC 11179]]\n\n==References==\n* \'\'Universal Meta Data Models\'\', by David Marco and Michael Jennings, Wiley, 2004, page 93-94 ISBN 0-471-08177-9\n* \'\'Metadata Solution\'\' by Adrinne Tannenbaum, Addison Wesley, 2002, page 412\n* \'\'Building and Managing the Meta Data Repository\'\', by David Marco, Wiley, 2000, pages 61–62\n* \'\'The Data Warehouse Lifecycle Toolkit\'\', by [[Ralph Kimball]] et. el., Wiley, 1998, also briefly mentions the role of data steward in the context of data warehouse project management on page 70.\n* \'\'Developing Geospatial Intelligence Stewardship for Multinational Operations\'\', by Jeff Thomas, US Army Command General Staff College, 2010, www.dtic.mil/dtic/tr/fulltext/u2/a524227.pdf.\n\n==Notes==\n{{reflist}}\n\n[[Category:Data management]]\n[[Category:Information technology governance]]\n[[Category:Knowledge representation]]\n[[Category:Library occupations]]\n[[Category:Metadata]]\n[[Category:Technical communication]]']
['Master data', '7617930', '\'\'\'Master data\'\'\' represents the business objects which are agreed on and shared across the enterprise.<ref>[http://mitiq.mit.edu/ICIQ/Documents/IQ%20Conference%202010/Papers/2B1_EnterpriseMasterDataArchitecture.pdf "ENTERPRISE MASTER DATA ARCHITECTURE: DESIGN DECISIONS AND OPTIONS"], Boris Otto & Alexander Schmidt, Institute of Information Management, University of St. Gallen</ref> It  can cover relatively static reference data, [[Dynamic data|transactional]], [[Unstructured data|unstructured]], analytical, [[Hierarchical database model|hierarchical]] and [[Metadata|meta]] data.<ref>"What Is Master Data?", Roger Wolter and Kirk Haselden, Microsoft Corporation, [http://msdn.microsoft.com/en-us/library/bb190163.aspx "The What, Why, and How of Master Data Management"], November 2006</ref> It is the primary focus of the [[Information Technology]] (IT) discipline of [[Master Data Management]] (MDM). \n\nWhile master data is often non-transactional in nature, it is not limited to non-transactional data, and often \'\'supports\'\' transactional processes and operations. For example, Master data may be about: customers, products, employees, materials, suppliers, and vendors, and it may also cover: sales, documents and aggregated sales.\n\n==Types of master data ==\n\n\'\'\'Reference Data\'\'\' is the set of permissible values to be used by other (master or transaction) data fields. Reference data normally changes slowly, reflecting changes in the modes of operation of the business, rather than changing in the normal course of business.\n\n\'\'\'Master Data\'\'\' is a \'\'single source\'\' of common business data used across \'\'\'multiple\'\'\' systems, applications, and/or processes.\n\n\'\'\'Enterprise Master Data\'\'\' is the \'\'single source\'\' of common business data used across \'\'\'all\'\'\' systems, applications, and processes for an entire enterprise (all departments, divisions, companies, and countries).\n\n\'\'\'Market Master Data\'\'\' is the \'\'single source\'\' of common business data for an entire marketplace. Market master data is used among enterprises within the value chain. An example of Market Master Data is the UPC (Universal Product Code) found on consumer products.\n\nMarket Master Data is compatible with enterprise-specific and domain-specific systems, compliant with or linked to industry standards, and incorporated within market research analytics. Market master data also facilitates integration of multiple data sources and literally puts everyone in the market on the same page.\n\nExcerpted from \'\'Master Data Management for Media: A Call to Action for Business Leaders in Marketing, Advertising, and the Media,\'\' a Microsoft White Paper by Scott Taylor and Robin Laylin, January 2010\n\n==Master data and Master reference data==\nMaster data is also called \'\'\'Master reference data\'\'\'. This is to avoid confusion with the usage of the term Master data for \'\'\'[[original data]]\'\'\', like an original recording (see also: [[Master Tape]]). Master data is nothing but unique data, i.e., there are no duplicate values.{{Citation needed|date=January 2011}}\n\n\'\'\'Material Master Data\'\'\' is a specific data set holding structured information about spare parts, raw materials and products within Enterprise Resource Planning (ERP) software. The data is held centrally and used across organisations.\n\n\'\'\'Vendor Master\'\'\' refers to the centralised location of information pertinent to the Vendor. Often this will include the Legal entity name, Tax identification and contact information.\n\n==Master Data Management==\n{{main|Master Data Management}}\nCurating and managing master data is key to ensuring master data quality. Analysis and reporting is greatly dependent on the quality of an organization\'s master data. Master data may either be stored in a central repository, sourced from one or more systems, or referenced centrally using an index. However, when it is used by several functional groups it may be distributed and redundantly stored in different applications across an organization and this copy data may be inconsistent (and if so, inaccurate).<ref>[http://blogs.gartner.com/andrew_white/2014/01/14/the-elephant-in-the-room-master-data-and-application-data/#comments "The Elephant in the Room – Master Data and Application Data"], Andrew White, Gartner, 14 January 2014</ref> Thus Master Data should have an agreed-upon view that is shared across the organization. Care should be taken to properly version Master Data, if the need arises to modify it, to avoid issues with distributed copies.\n\n==See also==\n*[[Master data management]]\n*[[Customer data integration]]\n*[[Product information management]]\n*[[Data governance]]\n\n== External links==\n{{reflist}}\n* [http://www.stibosystems.com/Files/Billeder/Stibo%20Systems%20images/UK_Resource_Library_Images/What-is-Master-Data-Management_EN.png What is Master Data?]\n* [http://www.semarchy.com/overview/what-is-master-data/ Semarchy: What is Master Data?]\n* [http://www.orchestranetworks.com/rdm/ Managing Reference Data (RDM)]\n* [https://www.youtube.com/watch?v=2tzVUqWAovg Aaron Zornes: Understanding Reference Data]\n\n[[Category:Data management]]\n\n[[fr:Données de référence]]']
['Database administration and automation', '8078610', '{{More footnotes|date=March 2011}}\n\'\'\'Database administration\'\'\' is the function of managing and maintaining [[database management system]]s (DBMS) software. Mainstream DBMS software such as [[Oracle database|Oracle]], [[IBM DB2]] and [[Microsoft SQL Server]] need ongoing management. As such, corporations that use DBMS software often hire specialized IT ([[Information technology|Information Technology]]) personnel called [[Database administrator|Database Administrators]] or DBAs.\n\n==DBA Responsibilities==\n\n* Installation, configuration and upgrading of Database server software and related products. \n* Evaluate Database features and Database related products.\n* Establish and maintain sound backup and recovery policies and procedures. \n* Take care of the [[Database design]] and implementation. \n* Implement and maintain database security (create and maintain users and roles, assign privileges). \n* [[Database tuning]] and performance monitoring. \n* Application tuning and performance monitoring. \n* Setup and maintain documentation and standards. \n* Plan growth and changes (capacity planning). \n* Work as part of a team and provide 24x7 support when required. \n* Do general technical troubleshooting and give cons.\n* Database recovery.\n\n== Types of database administration ==\nThere are three types of DBAs:\n\n#Systems DBAs (also referred to as Physical DBAs, Operations DBAs or Production Support DBAs): focus on the physical aspects of database administration such as DBMS installation, configuration, patching, upgrades, backups, restores, refreshes, performance optimization, maintenance and disaster recovery.\n#Development DBAs: focus on the logical and development aspects of database administration such as [[data model]] design and maintenance, DDL ([[Data Definition Language|data definition language]]) generation, SQL writing and tuning, coding [[stored procedure]]s, collaborating with developers to help choose the most appropriate DBMS feature/functionality and other pre-production activities. \n#Application DBAs: usually found in organizations that have purchased [[Third-party developer|3rd party]] [[application software]] such as ERP ([[enterprise resource planning]]) and CRM ([[customer relationship management]]) systems. Examples of such application software includes [[Oracle Applications]], Siebel and [[PeopleSoft]] (both now part of Oracle Corp.) and SAP. Application DBAs straddle the fence between the DBMS and the application software and are responsible for ensuring that the application is fully optimized for the database and vice versa. They usually manage all the [[Software componentry|application components]] that interact with the database and carry out activities such as application installation and patching, application upgrades, database cloning, building and running data cleanup routines, data load [[process management]], etc.\n\nWhile individuals usually specialize in one type of database administration, in smaller organizations, it is not uncommon to find a single individual or group performing more than one type of database administration.\n\n== Nature of database administration ==\nThe degree to which the administration of a database is automated dictates the skills and personnel required to manage databases.  On one end of the spectrum, a system with minimal automation will require significant experienced resources to manage; perhaps 5-10 databases per DBA.  Alternatively an organization might choose to automate a significant amount of the work that could be done manually therefore reducing the skills required to perform tasks.  As automation increases, the personnel needs of the organization splits into highly [[skilled worker]]s to create and manage the automation and a group of lower skilled "line" DBAs who simply execute the automation.\n\nDatabase administration work is complex, repetitive, time-consuming and requires significant training. Since databases hold valuable and mission-critical data, companies usually look for candidates with multiple years of experience. Database administration often requires DBAs to put in work during off-hours (for example, for planned after hours downtime, in the event of a database-related outage or if performance has been severely degraded). DBAs are commonly well compensated for the long hours\n\nOne key skill required and often overlooked when selecting a DBA is database recovery (under disaster recovery).  It is not a case of “if” but a case of “when” a database suffers a failure, ranging from a simple failure to a full catastrophic failure.  The failure may be data corruption, media failure, or user induced errors.  In either situation the DBA must have the skills to recover the database to a given point in time to prevent a loss of data.  A highly skilled DBA can spend a few minutes or exceedingly long hours to get the database back to the operational point.\n\n== Database administration tools ==\nOften, the DBMS software comes with certain tools to help DBAs manage the DBMS. Such tools are called native tools. For example, Microsoft SQL Server comes with SQL Server Management Studio and Oracle has tools such as [[SQL*Plus]] and Oracle Enterprise Manager/Grid Control. In addition, 3rd parties such as BMC, [[Quest Software]], [[Embarcadero Technologies]], [[EMS Database Management Solutions]] and SQL Maestro Group offer GUI tools to monitor the DBMS and help DBAs carry out certain functions inside the database more easily.\n\nAnother kind of database software exists to manage the provisioning of new databases and the management of existing databases and their related resources.  The process of creating a new database can consist of hundreds or thousands of unique steps from satisfying prerequisites to configuring backups where each step must be successful before the next can start.  A human cannot be expected to complete this procedure in the same exact way time after time - exactly the goal when multiple databases exist.  As the number of DBAs grows, without automation the number of unique configurations frequently grows to be costly/difficult to support.  All of these complicated procedures can be modeled by the best DBAs into database automation software and executed by the standard DBAs.  Software has been created specifically to improve the reliability and repeatability of these procedures such as [[Stratavia]]\'s [[Data Palette]] and [[GridApp Systems]] Clarity.\n\n== The impact of IT automation on database administration ==\nRecently, automation has begun to impact this area significantly. Newer technologies such as [[Stratavia]]\'s [[Data Palette]] suite and [[GridApp Systems]] Clarity have begun to increase the automation of databases causing the reduction of database related tasks. However at best this only reduces the amount of mundane, repetitive activities and does not eliminate the need for DBAs. The intention of DBA automation is to enable DBAs to focus on more proactive activities around database architecture, deployment, performance and service level management.\n\n\'\'Every database requires a database owner account that can perform all schema management operations. This account is specific to the database and cannot log in to Data Director. You can add database owner accounts after database creation. Data Director users must log in with their database-specific credentials to view the database, its entities, and its data or to perform database management tasks.\nDatabase administrators and application developers can manage databases only if they have appropriate permissions and roles granted to them by the organization administrator. The permissions and roles must be granted on the database group or on the database, and they only apply within the organization in which they are granted.\'\'\n\n== Learning database administration ==\nThere are several education institutes that offer professional courses, including late-night programs, to allow candidates to learn database administration. Also, DBMS vendors such as Oracle, Microsoft and IBM offer certification programs to help companies to hire qualified DBA practitioners.  College degree in Computer Science or related field is helpful but not necessarily a prerequisite.\n\n==See also==\n\n*[[Column-oriented DBMS]]\n*[[Data warehouse]]\n*[[Directory service]]\n*[[Distributed database management system]]\n*[[Hierarchical model]]\n*[[Navigational database]]\n*[[Network model]]\n*[[Object model]]\n*[[Object database]] (OODBMS)\n*[[Object-relational database]] (ORDBMS)\n*[[Run Book Automation]] (RBA)\n*[[Relational model]] (RDBMS)\n*[[Comparison of relational database management systems]]\n*[[Comparison of database tools]]\n*[[SQL]] is a language for database management\n\n== External links ==\n* {{cite journal | publisher = ACM [[Special Interest Group on Information Retrieval]] | work = SIGIR Forum | volume = 7 | issue = 4 | date = Winter 1972 | pages = 45–55 | url = http://portal.acm.org/citation.cfm?id=1095495.1095500 | title = A set theoretic data structure and retrieval language }}\n* {{cite journal | publisher = [[SIGMOD|ACM Special Interest Group on Management of Data]] | work = SIGMOD Record | volume = 35 | issue = 2 | date = June 2006 | url = http://www.tomandmaria.com/tom/Writing/VeritableBucketOfFactsSIGMOD.pdf | title = Origins of the Data Base Management System | format=PDF | author = Thomas Haigh }}\n\n{{Databases}}\n{{FOLDOC}}\n\n[[Category:Database management systems]]\n[[Category:Data management]]']
['Comparison of ADO and ADO.NET', '10701295', '\'\'Note: The following content requires a knowledge of [[database]] technologies.\'\'\n\nThe following is a comparison of two different database access technologies from [[Microsoft]], namely, [[ActiveX Data Objects|ActiveX Data Objects (ADO)]] and [[ADO.NET]]. Before comparing the two technologies, it is essential to get an overview of [[Microsoft Data Access Components]] (MDAC) and the [[.NET Framework]]. [[Microsoft Data Access Components]] provide a uniform and comprehensive way of developing applications for accessing almost any data store entirely from [[Managed code#Managed and unmanaged|unmanaged code]]. The [[.NET Framework]] is an [[Virtual machine#Application virtual machine|application virtual machine]]-based software environment that provides security mechanisms, [[memory management]], and [[exception handling]] and is designed so that developers need not consider the capabilities of the specific CPU that will execute the .NET application. The .NET [[Virtual machine#Application virtual machine|application virtual machine]] turns [[intermediate language]] (IL) into machine code. High-level language compilers for [[C Sharp (programming language)|C#]], [[Visual Basic .NET|VB.NET]] and [[C++]] are provided to turn source code into IL. [[ADO.NET]] is shipped with the Microsoft NET Framework.\n\n[[ActiveX Data Objects|ADO]] relies on [[Component Object Model|COM]] whereas [[ADO.NET]] relies on managed-providers defined by the .NET [[Common Language Runtime|CLR]]. ADO.NET does not replace ADO for the COM programmer; rather, it provides the .NET programmer with access to relational data sources, XML, and application data.\n\n{| class="wikitable"\n|-\n! \n! ADO\n! ADO.NET\n|-\n| Business Model\n| Connection-oriented Models used mostly\n| Disconnected models are used:Message-like Models.\n|-\n| Disconnected Access\n| Provided by Record set\n| Provided by Data Adapter and Data set\n|-\n| [[XML]] Support\n| Limited\n| Robust Support\n|-\n|Connection Model\n|Client application needs to be connected always to data-server while working on the data, unless using client-side cursors or a disconnected Record set\n|Client disconnected as soon as the data is processed. DataSet is disconnected at all times.\n|-\n|Data Passing\n|ADO objects communicate in binary mode.\n|ADO.NET uses XML for passing the data.\n|-\n|Control of data access behaviors\n|Includes implicit behaviors that may not always be required in an application and that may therefore limit performance.\n|Provides well-defined, factored components with predictable behavior, performance, and semantics.\n|-\n|Design-time support\n|Derives information about data implicitly at run time, based on metadata that is often expensive to obtain.\n|Leverages known metadata at design time in order to provide better run-time performance and more consistent run-time behavior.\n|}\n\n== References ==\n* [http://msdn2.microsoft.com/en-us/library/ms973217.aspx ADO.NET for the ADO programmer]\n\n[[Category:Data management]]\n[[Category:.NET Framework]]\n[[Category:Microsoft application programming interfaces]]\n[[Category:SQL data access]]\n[[Category:Software comparisons|ADO and ADO.NET]]']
['Category:Recording', '1479108', '{{Commons category|Recording}}\n{{Cat main|Recording}}\n\n[[Category:Data management]]\n[[Category:Information storage]]']
['Ontology merging', '11270991', "{{Unreferenced|date=March 2013}}\n\n'''Ontology merging''' defines the act of bringing together two conceptually divergent [[ontology (computer science)|ontologies]] or the instance data associated to two ontologies. This is similar to work in database merging ([[schema matching]]). This merging process can be performed in a number of ways, manually, semi automatically, or automatically. Manual ontology merging although ideal is extremely labour-intensive and current research attempts to find semi or entirely automated techniques to merge ontologies. These techniques are statistically driven often taking into account similarity of concepts and raw similarity of instances through textual [[string metrics]] and semantic knowledge. These techniques are similar to those used in [[information integration]] employing [[string metrics]] from [[open source]] similarity libraries.\n\n==See also==\n*[[ontology mapping]]\n*[[data integration]]\n\n[[Category:Ontology (information science)]]\n[[Category:Data management]]"]
['Microsoft SQL Server Master Data Services', '13430116', '{{multiple issues|\n{{Advert|date=March 2011}}\n{{Update|inaccurate=yes|date=April 2010}}\n}}\n\n\'\'\'Microsoft SQL Server Master Data Services\'\'\' is a [[Master Data Management]] (MDM) product from [[Microsoft]] that ships as a part of the [[Microsoft SQL Server]] relational database management system.<ref>https://msdn.microsoft.com/en-us/library/ms130214.aspx</ref>  Master Data Services (MDS) is the SQL Server solution for master data management. Master data management (MDM) enables your organization to discover and define non-transactional lists of data, and compile maintainable, reliable master lists. Master Data Services first shipped with Microsoft SQL Server 2008 R2.  Microsoft SQL Server 2016 includes many enhancements to Master Data Services, such as improved performance and security, and the ability to clear transaction logs, create custom indexes, share entity data between different models, and support for many-to-many relationships. For more information, see [https://msdn.microsoft.com/en-us/library/ff929136.aspx What\'s New in Master Data Services (MDS)]\n\n==Overview==\nIn Master Data Services, the model is the highest level container in the structure of your master data. You create a model to manage groups of similar data. A model contains one or more entities, and entities contain members that are the data records. An entity is similar to a table.\n\nLike other MDM products, Master Data Services aims to create a centralized data source and keep it synchronized, and thus reduce redundancies, across the applications which process the data.{{cn|date=January 2015}}\n\nSharing the architectural core with Stratature +EDM, Master Data Services uses a [[Microsoft SQL Server]] database as the physical data store. It is a part of the \'\'Master Data Hub\'\', which uses the database to store and manage data [[Entity Data Model|entities]].{{cn|date=January 2015}} It is a database with the software to validate and manage the data, and keep it synchronized with the systems that use the data.<ref name="arch">{{cite web | url = http://msdn2.microsoft.com/en-us/library/bb410798.aspx | title = Master Data Management (MDM) Hub Architecture | author = Roger Walter | publisher = MSDN TechNet | accessdate = 2007-09-25}}</ref> The master data hub has to extract the data from the source system, validate, sanitize and shape the data, remove duplicates, and update the hub repositories, as well as synchronize the external sources.<ref name="arch"/> The entity schemas, attributes, data hierarchies, validation rules and access control information are specified as [[metadata]] to the Master Data Services runtime. Master Data Services does not impose any limitation on the data model. Master Data Services also allows custom \'\'Business rules\'\', used for validating and sanitizing the data entering the data hub, to be defined, which is then run against the data matching the specified criteria. All changes made to the data are validated against the rules, and a log of the transaction is stored persistently. Violations are logged separately, and optionally the owner is notified, automatically. All the data entities can be [[Revision control system|versioned]].{{cn|date=January 2015}}\n\nMaster Data Services allows the master data to be categorized by hierarchical relationships, such as employee data are a subtype of organization data. Hierarchies are generated by relating data attributes. Data can be automatically categorized using rules, and the categories are introspected programmatically. Master Data Services can also expose the data as [[Microsoft SQL Server]] [[view (database)|views]], which can be pulled by any [[SQL]]-compatible client. It uses a role-based access control system to restrict access to the data. The views are generated dynamically, so they contain the latest data entities in the master hub. It can also push out the data by writing to some external journals. Master Data Services also includes a web-based UI for viewing and managing the data. It uses [[AJAX]] in the front-end and [[ASP.NET]] in the back-end.{{cn|date=January 2015}}\n\nMaster Data Services also includes certain features not available in the Stratature +EDM product. It gains a [[Web service]] interface to expose the data, as well as an [[API]], which internally uses the exposed web services, exposing the feature set, programmatically, to access and manipulate the data. It also integrates with [[Active Directory]] for authentication purposes. Unlike +EDM, Master Data Services supports [[Unicode]] characters, as well as support multilingual user interfaces.{{cn|date=January 2015}}\n\nThere has been a significant [http://www.faceofit.com/why-is-sql-server-2016-is-faster-than-ever performance increase] in Master Data Services in SQL Server 2016 as well as the Excel Add-In.<ref>http://www.faceofit.com/why-is-sql-server-2016-is-faster-than-ever</ref>\n\n== Terminology ==\n\n* \'\'Model\'\' is the highest level of an MDS instance. It is the primary container for specific groupings of master data. In many ways it is very similar to the idea of a database. \n* \'\'Entities\'\' are containers created within a model. Entities provide a home for members, and are in many ways analogous to database tables. (e.g. Customer)\n* \'\'Members\'\' are analogous to the records in a database table (Entity) e.g. Will Smith. Members are contained within entities. Each member is made up of two or more attributes. \n* \'\'Attributes\'\' are analogous to the columns within a database table (Entity) e.g. Surname. Attributes exist within entities and help describe members (the records within the table). Name and Code attributes are created by default for each entity and serve to describe and uniquely identify leaf members. Attributes can be related to other attributes from other entities which are called \'domain-based\' attributes. This is similar to the concept of a foreign key.\nOther attributes however, will be of type \'free-form\' (most common) or \'file\'.\n* \'\'Attribute Groups\'\' are explicitly defined collections of particular attributes. Say you have an entity "customer" that has 50 attributes &mdash; too much information for many of your users. Attribute groups enable the creation of custom sets of hand-picked attributes that are relevant for specific audiences. (e.g. "customer - delivery details" that would include just their name and last known delivery address). This is very similar to a database view.\n*  \'\'Hierarchies\'\' organize members into either Derived or Explicit hierarchical structures. Derived hierarchies, as the name suggests, are derived by the MDS engine based on the relationships that exist between attributes. Explicit hierarchies are created by hand using both leaf and consolidated members.\n*  \'\'Business Rules\'\' can be created and applied against model data to ensure that custom business logic is adhered to. In order to be committed into the system data must pass all business rule validations applied to them. e.g. Within the Customer Entity you may want to create a business rule that ensures all members of the \'Country\' Attribute contain either the text "USA" or "Canada". The Business Rule once created and ran will then verify all the data is correct before it accepts it into the approved model.\n*  \'\'Versions\'\' provide system owners / administrators with the ability to Open, Lock or Commit a particular version of a model and the data contained within it at a particular point in time. As the content within a model varies, grows or shrinks over time versions provide a way of managing metadata so that subscribing systems can access to the correct content.\n\n== References ==\n{{Reflist}}\n\n==External links==\n*[https://msdn.microsoft.com/en-us/library/ee633763.aspx Microsoft SQL Server 2016 Master Data Services]\n\n[[Category:Data management]]\n[[Category:Microsoft software|SQL Server Master Data Services]]\n[[Category:2010 software]]']
['Category:Data modeling', '1116481', "{{Commons cat|Data modeling}}\nIn information system design, '''[[data modeling]]''' is the analysis and design of the information in the system, concentrating on the logical entities and the logical dependencies between these entities\n\n{{catdiffuse}}\n\n<!--  -->\n\n[[Category:Computer-aided software engineering tools]]\n[[Category:Data management|Modeling]]\n[[Category:Scientific modeling]]\n[[Category:Software design]]"]
['Core data integration', '14124151', "{{Citations missing|date=December 2007}}\n'''Core data integration''' is the use of [[data integration]] technology for a significant, centrally planned and managed IT initiative within a company. Examples of core data integration initiatives could include:\n\n* ETL ([[Extract, transform, load]]) implementations\n* EAI ([[Enterprise Application Integration]]) implementations\n* SOA ([[Service-Oriented Architecture]]) implementations\n* ESB ([[Enterprise Service Bus]]) implementations\n\nCore data integrations are often designed to be enterprise-wide integration solutions. They may be designed to provide a data abstraction layer, which in turn will be used by individual core data integration implementations, such as ETL servers or applications integrated through EAI.\n\nBecause it is difficult to promptly roll out a centrally managed data integration solution that anticipates and meets all data integration requirements across an organization, IT engineers and even business users create [[edge data integration]], using technology that may be incompatible with that used at the core. In contrast to a core data integration, an edge data integration is not centrally planned and is generally completed with a smaller budget and a tighter deadline. \n\n==See also==\n* [[data integration]]\n* [[edge data integration]]\n\n== References ==\n* http://searchsoa.techtarget.com/tip/0,289483,sid26_gci1171085,00.html* \n* \n\n[[Category:Data management]]"]
['Transaction data', '15348791', "{{Unreferenced|date=July 2010}}\n\n'''Transaction data''' are data describing an event (the change as a result of a [[Transaction processing|transaction]]) and is usually described with verbs. Transaction data always has a time dimension, a numerical value and refers to one or more objects (i.e. the [[reference data]]).\n\nTypical transactions are:\n* Financial: orders, invoices, payments\n* Work: Plans, activity records\n* [[Logistics]]: Deliveries, storage records, travel records, etc.\n\nTypical [[transaction processing system]]s (systems generating transactions) are [[SAP ERP|SAP]] and [[Oracle Financials]].\n\n==Records management==\n{{Main|Records management}}\nRecording and retaining transactions is called [[records management]]. The record of the transaction is stored in a place where the [[wikt:retention|retention]] can be guaranteed and where data are archived/removed following a [[retention period]]. The format of the transaction can be data (to be stored in a database), but it can also be a document.\n\n==Data Warehousing==\nTransaction data can be summarised in a [[Data warehouse]], which helps accessibility and analysis of the data.\n\n==See also==\n* [[Data modeling]]\n* [[Data architecture]]\n* [[Information Lifecycle Management]]\n* [[reference data]]\n\n[[Category:Data management]]\n[[Category:Transaction processing]]\n\n\n{{compsci-stub}}"]
['Data proliferation', '13651081', '\'\'\'Data proliferation\'\'\' refers to the prodigious amount of [[data]], [[structured data|structured]] and unstructured, that businesses and governments continue to generate at an unprecedented rate and the [[usability]] problems that result from attempting to store and manage that data. While originally pertaining to problems associated with paper [[documentation]], data proliferation has become a major problem in primary and secondary [[data storage device|data storage]] on computers.\n\nWhile digital storage has become cheaper, the associated costs, from raw power to maintenance and from metadata to search engines, have not kept up with the proliferation of data. Although the power required to maintain a unit of data has fallen, the cost of facilities which house the digital storage has tended to rise.<ref>{{cite web |url =http://www.deloitte.co.uk/TMTPredictions/technology/Downsizing-digital-attic-data-storage.cfm\n|title=Downsizing the digital attic |work=Deloitte Technology Predictions |archiveurl=https://web.archive.org/web/20110722194032/http://www.deloitte.co.uk/TMTPredictions/technology/Downsizing-digital-attic-data-storage.cfm |archivedate=July 22, 2011}}</ref>\n\n{{rquote|right| At the simplest level, company [[e-mail]] systems spawn large amounts of data. Business e-mail – some of it important to the enterprise, some much less so – is estimated to be growing at a rate of 25-30% annually. And whether it’s relevant or not, the load on the system is being magnified by practices such as multiple addressing and the attaching of large text, audio and even [[video file formats|video file]]s.|IBM Global Technology Services<ref name=IBM>[https://web.archive.org/web/20090206010415/http://www-03.ibm.com/systems/resources/systems_storage_solutions_pdf_toxic_tb.pdf “The Toxic [[Terabyte]]”, IBM Global Technology Services, July 2006]</ref>}}\n\nData proliferation has been documented as a problem for the [[U.S. military]] since August 1971, in particular regarding the excessive documentation submitted during the acquisition of major weapon systems.<ref name=DODPP>[http://stinet.dtic.mil/oai/oai?&verb=getRecord&metadataPrefix=html&identifier=AD0892652 Evolution of the Data Proliferation Problem within Major Air Force Acquisition Programs.]</ref> Efforts to mitigate data proliferation and the problems associated with it are ongoing.<ref>[http://www.thic.org/pdf/Jun02/dod.rroderique.020612.pdf Data Proliferation: Stop That]</ref>\n\n==Problems caused==\nThe problem of data proliferation is affecting all areas of commerce as the result of the availability of relatively inexpensive data storage devices. This has made it very easy to dump data into secondary storage immediately after its window of usability has passed. This masks problems that could gravely affect the profitability of businesses and the efficient functioning of health services, police and security forces, local and national governments, and many other types of organizations.<ref name=IBM /> Data proliferation is problematic for several reasons:\n*Difficulty when trying to find and retrieve information. At [[Xerox]], on average it takes employees more than one hour per week to [[document retrieval|find]] hard-copy documents, costing $2,152 a year to manage and store them. For businesses with more than 10 employees, this increases to almost two hours per week at $5,760 per year.<ref>[http://www.itbusiness.ca/it/client/en/home/News.asp?id=40615&cid=13 “Dealing with data proliferation”; Vawn Himmelsbach. it business.ca: Canadian Technology News, September 19, 2006]</ref> In large [[storage network|networks]] of primary and secondary data storage, problems finding electronic data are analogous to problems finding hard copy data.\n*[[Data loss]] and legal liability when data is disorganized, not properly replicated, or cannot be found in a timely manner. In April 2005, the [[TD Ameritrade|Ameritrade Holding Corporation]] told 200,000 current and past customers that a [[Magnetic tape data storage|tape]] containing confidential information had been lost or destroyed in transit. In May of the same year, [[Time Warner Incorporated]] reported that 40 tapes containing personal data on 600,000 current and former employees had been lost en route to a storage facility. In March 2005, a Florida judge hearing a $2.7 billion lawsuit against Morgan Stanley issued an "[[adverse inference]] order" against the company for "willful and gross abuse of its discovery obligations." The judge cited Morgan Stanley for repeatedly finding misplaced tapes of e-mail messages long after the company had claimed that it had turned over all such tapes to the court.<ref>[http://www.computerworld.com/printthis/2005/0,4814,103541,00.html “Data: Lost, Stolen or Strayed”, Computer World, Security]</ref>\n*Increased manpower requirements to manage increasingly chaotic data storage resources.\n*Slower networks and application performance due to excess traffic as users search and search again for the material they need.<ref name=IBM />\n*High cost in terms of the energy resources required to operate storage hardware. A 100 terabyte system will cost up to $35,040 a year to run—not counting cooling costs.<ref>[http://findarticles.com/p/articles/mi_m0BRZ/is_10_23/ai_111062988 "Power and storage: the hidden cost of ownership”, Computer Technology Review, October 2003]</ref>\n\n==Proposed solutions==\n*Applications that better utilize modern technology\n*Reductions in duplicate data (especially as caused by data movement)\n*Improvement of [[metadata]] structures\n*Improvement of file and storage transfer structures\n*User education and discipline<ref name=DODPP />\n*The implementation of [[Information Lifecycle Management]] solutions to eliminate low-value information as early as possible before putting the rest into actively managed long-term storage in which it can be quickly and cheaply accessed.<ref name=IBM />\n\n==See also==\n*[[Backup]]\n* [[Digital Asset Management]]\n*[[Disk storage]]\n*[[Document management system]]\n*[[Hierarchical storage management]]\n*[[Information Lifecycle Management]]\n*[[Information repository]]\n*[[Magnetic tape data storage]]\n*[[Retention period|Retention schedule]]\n\n==References==\n{{reflist}}\n\n[[Category:Information technology management]]\n[[Category:Content management systems]]\n[[Category:Data management]]']
['Sales intelligence', '17420819', "'''Sales intelligence''' (SI) refers to technologies, applications and practices for the collection, integration, analysis, and presentation of information to help salespeople keep up to date with clients, [[Prospect research|prospect data]] and drive business. In addition to providing [[Performance metric|metric]]s for win-loss and sales confidence,<ref>[http://chapmanhq.com/solutions/strategic-account-management-sam/metrics-and-measurements Metrics for sales intelligence]</ref> SI can present contextually relevant customer and product information.\n\nThe 2008 survey of 300 companies by the [[Aberdeen Group]]<ref>[http://www.aberdeen.com/Aberdeen-Library/5379/RA-sales-intelligence-nirvana.aspx Sales Intelligence, Aberdeen Group study (2008)]</ref> show that the recent economic downturn has lengthened traditional sales cycles. As businesses have been forced to reduce spending, sales representatives have been challenged to meet [[Sales quota|quota]]s. Top performing companies have implemented sales intelligence programs to improve the quality and quantity of sales leads. SI contextualizes opportunities by providing relevant industry, corporate and personal information. Frequently SI's fact-based information is integrated or includes [[customer relationship management]] (CRM).\n\nAlthough some aspects of sales intelligence overlaps business intelligence (BI), SI is specifically designed for the use of salespeople and sales managers. Unlike [[customer relationship management]] (CRM) and traditional [[business intelligence]] (BI) applications, SI provides real-time analysis of current sales data and assists with suggesting and delivering actionable, relevant information.\n\nSales intelligence solutions are predominantly designed for companies in the [[manufacturing]], [[distribution (business)|distribution]] and [[wholesale]] sectors. These are highly competitive markets, where volumes are high, [[Profit margin|margin]]s are low. (SI) solutions provide unique insight into customer [[buying pattern]]s. By automatically analysing and evaluating these patterns, Sales Intelligence pro-actively identifies and delivers [[up-sell]], [[cross-sell]] and switch-sell opportunities.\n\n==See also==\n* [[Analytics]]\n* [[Augmented learning]]\n* [[Business intelligence tools]]\n* [[Dashboards (management information systems)]]\n* [[Location intelligence]]\n* [[Market intelligence]]\n* [[Marketing intelligence]]\n* [[Operational Intelligence]]\n* [[OODA Loop]]\n* [[Predictive analytics]]\n* [[Business Intelligence 2.0]]\n* [[Process mining]]\n* [[Right-time marketing]]\n* [[Integrated business planning]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://blog.findable.me/post/52963306183/sales-intelligence-a-short-primer Sales Intelligence A Short Primer]\n\n[[Category:Business intelligence]]\n[[Category:Data management]]\n\n[[da:Business intelligence]]\n[[de:Business-Intelligence]]\n[[es:Inteligencia empresarial]]\n[[fr:Informatique décisionnelle]]\n[[ko:경영 정보학]]\n[[hr:Poslovna inteligencija]]\n[[id:Intelijen bisnis]]\n[[it:Business intelligence]]\n[[lt:Verslo analitika]]\n[[nl:Business intelligence]]\n[[pl:Business intelligence]]\n[[pt:Business intelligence]]\n[[ru:Business Intelligence]]\n[[fi:Business intelligence]]\n[[sv:Business intelligence]]"]
['SIGMOD Edgar F. Codd Innovations Award', '18927887', 'The [[Association for Computing Machinery|ACM]] \'\'\'[[SIGMOD]] [[Edgar F. Codd]] Innovations Award\'\'\' is a lifetime research achievement award given by the ACM Special Interest Group on Management of Data, at its yearly flagship conference (also called SIGMOD). According to its homepage, it is given "for innovative and highly significant contributions of enduring value to the development, understanding, or use of database systems and databases".<ref>http://www.sigmod.org/sigmod-awards</ref> The award has been given since 1992.\n\n== Recipients ==\n{| class="wikitable"\n! Year\n! Name\n|-\n|2016\n|[[Gerhard Weikum]]\n|-\n| 2015\n| [[Laura M. Haas]]<ref>[http://www.sigmod.org/all-news/dr.-laura-haas-is-the-recipient-of-the-2015-sigmod-edgar-f.-codd-innovation-award Dr. Laura Haas is the recipient of the 2015 SIGMOD Edgar F. Codd Innovation Award], [[SIGMOD]], retrieved 2015-06-21.</ref>\n|-\n| 2014\n| [[Martin L. Kersten]]<ref>{{cite web |url=https://www.cwi.nl/news/2014/international-innovation-award-big-data-research-martin-kersten |title=International innovation award to Martin Kersten |date=26 June 2014 |website=CWI Amsterdam |accessdate=26 June 2014}}</ref>\n|-\n| 2013\n| [[Stefano Ceri]]\n|-\n| 2012\n| Bruce Lindsay\n|-\n| 2011\n| [[Surajit Chaudhuri]]\n|-\n| 2010\n| [http://www.hpl.hp.com/people/umesh_dayal/ Umeshwar Dayal]\n|-\n| 2009\n| [[Masaru Kitsuregawa]]\n|-\n| 2008\n| [[Moshe Y. Vardi]]\n|-\n| 2007\n| [[Jennifer Widom]]\n|-\n| 2006\n| [[Jeffrey D. Ullman]]\n|-\n| 2005\n| Michael Carey\n|-\n| 2004\n| [[Ronald Fagin]]\n|-\n| 2003\n| [[Don Chamberlin]]\n|-\n| 2002\n| [[Patricia Selinger]]\n|-\n| 2001\n| [[Rudolf Bayer]]\n|-\n| 2000\n| [[Rakesh Agrawal (computer scientist)|Rakesh Agrawal]]\n|-\n| 1999\n| [[Hector Garcia-Molina]]\n|-\n| 1998\n| [[Serge Abiteboul]]\n|-\n| 1997\n| [[David Maier]]\n|-\n| 1996\n| [[C. Mohan]]\n|-\n| 1995\n| [[David DeWitt]]\n|-\n| 1994\n| [[Phil Bernstein|Philip Bernstein]]\n|-\n| 1993\n| [[Jim Gray (computer scientist)|Jim Gray]]\n|-\n| 1992\n| [[Michael Stonebraker]]\n|}\n\n== References ==\n<references/>\n\n[[Category:Association for Computing Machinery]]\n[[Category:Awards established in 1992]]\n[[Category:Computer science awards]]\n[[Category:Data management]]']
['UI data binding', '18644154', "{{Refimprove|date=February 2015}}\n\n'''UI data binding''' is a [[Design pattern (computer science)|software design pattern]] to simplify development of [[GUI]] applications.  UI [[data binding]] binds UI elements to an application [[domain model]]. Most frameworks employ the [[Observer pattern]] as the underlying binding mechanism.  To work efficiently, UI data binding has to address [[Data validation|input validation]] and data type mapping.\n\nA ''bound control'' is a [[GUI widget|widget]] whose value is tied or [[data binding|bound]] to a field in a [[recordset]] (e.g., a [[column (database)|column]] in a [[row (database)|row]] of a [[table (database)|table]]).  Changes made to data within the control are automatically saved to the database when the control's exit [[event trigger]]s.\n\n== Data binding frameworks and tools ==\n\n=== [[Embarcadero Delphi|Delphi]] ===\n* [[DSharp (data binding)|DSharp]] 3rd party Data Binding tool{{cn|date=December 2016}}\n* [[OpenWire (library)|OpenWire]] Visual Live Binding - 3rd party Visual Data Binding tool\n\n=== Java ===\n* [[JFace]] Data Binding\n* [[JavaFX]] Property<ref>https://docs.oracle.com/javafx/2/binding/jfxpub-binding.htm</ref>\n\n=== .NET ===\n* [[Windows Forms]] data binding overview\n* [[Windows Presentation Foundation|WPF]] data binding overview\n* Unity 3D data binding framework (available in modifications for NGUI, iGUI and EZGUI libraries){{cn|date=December 2016}}\n\n=== JavaScript ===\n* [[AngularJS]]\n* [[Backbone.js]]\n* [[Ember.js]]\n* Datum.js<ref>{{cite web |url=http://datumjs.com|title=Datum.js|accessdate=7 November 2016}}</ref>\n* [[knockout.js]]\n* [[Meteor (web framework)|Meteor]], via its ''Blaze'' live update engine<ref>{{cite web|title=Meteor Blaze|url=https://www.meteor.com/blaze|quote=Meteor Blaze is a powerful library for creating live-updating user interfaces. Blaze fulfills the same purpose as Angular, Backbone, Ember, React, Polymer, or Knockout, but is much easier to use. We built it because we thought that other libraries made user interface programming unnecessarily difficult and confusing.}}</ref>\n* [[OpenUI5]]\n* [[React (JavaScript library)|React]]\n\n==See also==\n*[[Data binding]]\n\n==References==\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:Software design patterns]]\n\n\n{{compu-prog-stub}}\n{{database-stub}}"]
['Data verification', '22601719', "Data''' Verification''' is a process in which different types of data are checked for accuracy and [[data consistency|inconsistencies]] after [[data migration]] is done.<ref>http://www.datacap.com/products/features/verify/</ref>\n\nIt helps to determine whether data was accurately translated when data is [[data transfer|transferred]] from one source to another, is complete, and supports processes in the new system. During verification, there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous [[data loss]].\n\nA type of Data Verification is [[double entry]] and [[proofreading]] data. Proofreading data involves someone checking the data entered against the original document. This is also time consuming and costly.\n\n==References==\n{{reflist|2}}\n\n==External links==\n* [http://www.pcguide.com/care/bu/howVerification-c.html PC Guide article]\n\n[[Category:Data management]]\n[[Category:Data quality]]"]
['Data aggregation', '10186403', '\'\'\'Data aggregation\'\'\' is the compiling of [[information]] from [[databases]] with intent to prepare combined datasets for [[data processing]].<ref>{{cite journal|author1=Stanley, Jay  |author2=Steinhardt, Barry|title=Bigger Monster, Weaker Chains: The Growth of an American Surveillance Society|publisher=American Civil Liberties Union|date=January 2003}}</ref>\n\n==Description==\nThe source information for data aggregation may originate from public records and [[criminal]] databases. The information is packaged into aggregate reports and then sold to [[business]]es, as well as to [[Local government|local]], [[State government|state]], and government agencies. This information can also be useful for [[marketing]] purposes. In the United States, many data brokers\' activities fall under the [[Fair Credit Reporting Act]] (FCRA) which regulates [[Credit bureau|consumer reporting agencies]]. The agencies then gather and package personal information into [[consumer]] reports that are sold to [[creditor]]s, [[employer]]s, [[insurer]]s, and other businesses.\n\nVarious reports of information are provided by database aggregators. Individuals may request their own consumer reports which contain basic [[biographical]] information such as name, date of birth, current address, and phone number. Employee [[background check]] reports, which contain highly detailed information such as past addresses and length of residence, [[professional]] [[Licensure|licenses]], and criminal history, may be requested by eligible and qualified third parties. Not only can this data be used in employee background checks, but it may also be used to make decisions about insurance coverage, pricing, and law enforcement. [[Privacy]] activists argue that database aggregators can provide erroneous information.<ref>{{cite web|url=http://www.privacyactivism.org/docs/DataAggregatorsStudy.html |title=Data Aggregators: A Study of Data Quality and Responsiveness |author1=Pierce, Deborah |author2=Ackerman, Linda |publisher=Privacyactivism.org |date=2005-05-19 |accessdate=2007-04-02 |archiveurl=https://web.archive.org/web/20070319220412/http://www.privacyactivism.org/docs/DataAggregatorsStudy.html |archivedate=2007-03-19 |deadurl=yes |df= }}</ref>\n\n==Role of the Internet==\nThe potential of the [[Internet]] to consolidate and manipulate information has a new application in data aggregation, also known as \'\'screen scraping\'\'. The Internet gives users the opportunity to consolidate their [[username]]s and [[password]]s, or PINs. Such consolidation enables consumers to access a wide variety of PIN-protected [[website]]s containing personal information by using one master PIN on a single website. Online account providers include [[financial institution]]s, [[stockbroker]]s, [[airline]] and frequent flyer and other reward programs, and [[e-mail]] accounts. Data aggregators can gather account or other information from designated websites by using account holders\' PINs, and then making the users\' account information available to them at a single website operated by the aggregator at an account holder\'s request. Aggregation services may be offered on a standalone basis or in conjunction with other financial services, such as [[portfolio (finance)|portfolio]] tracking and [[Bill (payment)|bill]] payment provided by a specialized website, or as an additional service to augment the online presence of an enterprise established beyond the virtual world. Many established companies with an Internet presence appear to recognize the value of offering an aggregation service to enhance other web-based services and attract visitors. Offering a data aggregation service to a website may be attractive because of the potential that it will frequently draw users of the service to the hosting website.\n\n==Local business data aggregation==\nWhen it comes to compiling location information on local businesses, there are several major data aggregators that collect information such as the business name, address, phone number, website, description and hours of operation. They then validate this information using various validation methods. Once the business information has been verified to be accurate, the data aggregators make it available to publishers like [[Google]] and [[Yelp]].\n\nWhen Yelp, for example, goes to update their Yelp listings, they will pull data from these local data aggregators. Publishers take local business data from different sources and compare it to what they currently have in their database. They then update their database it with what information they deem accurate.\n\n==Legal implications==\nFinancial institutions are concerned about the possibility of [[legal liability|liability]] arising from data aggregation activities, potential [[security]] problems, infringement on [[intellectual property]] rights and the possibility of diminishing traffic to the institution\'s website. The aggregator and financial institution may agree on a data feed arrangement activated on the customer\'s request, using an Open Financial Exchange (OFX) standard to request and deliver information to the site selected by the customer as the place from which they will view their account data. Agreements provide an opportunity for institutions to negotiate to protect their customers\' interests and offer aggregators the opportunity to provide a robust service. Aggregators who agree with information providers to extract data without using an OFX standard may reach a lower level of consensual relationship; therefore, "screen scraping" may be used to obtain account data, but for business or other reasons, the aggregator may decide to obtain prior consent and negotiate the terms on which customer data is made available. "Screen scraping" without consent by the content provider has the advantage of allowing subscribers to view almost any and all accounts they happen to have opened anywhere on the Internet through one website.\n\n==Outlook==\nOver time, the transfer of large amounts of account data from the account provider to the aggregator\'s server could develop into a comprehensive profile of a user, detailing their banking and [[credit card]] transactions, balances, securities transactions and portfolios, and [[travel]] history and preferences. As the sensitivity to data protection considerations grows, it is likely there will be a considerable focus on the extent to which data aggregators may seek to use this data either for their own purposes or to share it on some basis with the operator of a website on which the service is offered or with other third parties.<ref>{{cite web|url=http://www.ffhsj.com/bancmail/bmarts/aba_art.htm|title=Scrape It, Scrub It and Show It: The Battle Over Data Aggregation|author1=Ledig, Robert H.  |author2=Vartanian, Thomas P.|publisher=Fried Frank|date=2002-09-11|accessdate=2007-04-02}}</ref>\n\n==References==\n<references />\n\n{{DEFAULTSORT:Data Aggregator}}\n[[Category:Data management]]\n[[Category:Information privacy]]\n[[Category:Data laws]]']
['Vocabulary-based transformation', '3219147', '{{unreferenced|date=July 2013}}\nIn [[metadata]], a \'\'\'vocabulary-based transformation (VBT)\'\'\' is a transformation aided by the use of a [[semantic equivalence]] statements within a [[controlled vocabulary]].\n\nMany organizations today require communication between two or more computers.  Although many standards exist to exchange data between computers such as [[HTML]] or [[email]], there is still much structured information that needs to be exchanged between computers that is not standardized.  The process of mapping one source of data into another is often a slow and labor-intensive process.\n\nVBT is a possible way to avoid much of the time and cost of manual data mapping using traditional [[Extract, transform, load]] technologies.\n\n== History ==\n\nThe term \'\'vocabulary-based transformation\'\' was first defined by Roy Shulte of the [[Gartner Group]] around May 2003 and appeared in annual "[[hype cycle|hype]]-cycle" for [[data integration|integration]].\n\n== Application ==\nVBT allows computer systems integrators to more automatically "look up" the definitions of data elements in a centralized [[data dictionary]] and use that definition and the equivalent mappings to transform that data element into a foreign [[namespace]].\n\nThe [[Web Ontology Language]] (OWL) language also support three [[semantic equivalence]] statements.\n\n== Companies or products ==\n* [[IONA Technologies]]\n* [http://liaison.com/products/transform Contivo and Delta] by [http://liaison.com/ Liaison Technologies]\n* enLeague Systems\n* ItemField\n* Unicorn Solutions\n* Vitria Technology\n* Zonar\n\n== See also ==\n* [[metadata]]\n* [[Controlled vocabulary]]\n* [[Data dictionary]]\n* [[Semantic spectrum]]\n* [[Semantic equivalence]]\n* [[XSLT]]\n* [[Enterprise Application Integration]]\n\n==External links==\n* [http://www.gartner.com/6_help/glossary/GlossaryV.jsp Gartner Glossary of Terms] Gartner definition Vocabulary-based transformation\n* [http://www.sun.com/service/openwork/analyst/Gartner_Hype_Cycle.pdf Gartner Hype Cycle 2003]\n\n{{DEFAULTSORT:Vocabulary-Based Transformation}}\n[[Category:Data management]]']
['Modular serializability', '24906307', '#REDIRECT [[Global serializability]]\n\n[[Category:Data management]]\n[[Category:Databases]]\n[[Category:Transaction processing]]\n[[Category:Concurrency control]]']
['Operational system', '14190268', '{{distinguish|Operating system}}\nAn \'\'\'operational system\'\'\' is a term used in [[data warehousing]] to refer to a system that is used to process the day-to-day transactions of an organization. These systems are designed in a manner that processing of day-to-day transactions is performed efficiently and the integrity of the transactional data is preserved.\n== Synonyms ==\nSometimes operational systems are referred to as [[operational database]]s, [[transaction processing system]]s, or [[online transaction processing]] systems (OLTP). However, the use of the last two terms as synonyms may be confusing, because operational systems can be [[batch processing]] systems as well.\n\nAny enterprise must necessarily maintain a lot of data about its operation. This is its "operational data".\n\n{| class="wikitable" border="1"\n|-\n! Organization\n! Probably\n|-\n| Manufacturing Company \n| Product data\n|-\n| Bank\n| Account Data\n|-\n| Hospital\n| Patient Data\n|-\n| University\n| Student Data\n|-\n| Government Department\n| Planning data\n|}\n\n==See also==\n* [[Operating system]] (OS)\n* [[Data warehouse#Data warehouses versus operational systems|Data warehouses versus operational systems]]\n\n{{database-stub}}\n\n{{DEFAULTSORT:Operational System}}\n[[Category:Data warehousing]]\n[[Category:Data management]]\n[[Category:Information technology management]]\n[[Category:Business intelligence]]']
['Concurrency control', '217356', '{{pp-pc1}}\nIn [[information technology]] and [[computer science]],  especially in the fields of [[computer programming]], [[operating systems]], [[multiprocessor]]s, and [[database]]s, \'\'\'concurrency control\'\'\' ensures that correct results for [[Concurrent computing|concurrent]] operations are generated, while getting those results as quickly as possible.\n\nComputer systems, both [[software]] and [[computer hardware|hardware]], consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules. When components that operate concurrently interact by messaging or by sharing accessed data (in [[Computer memory|memory]] or [[Computer data storage|storage]]), a certain component\'s consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and [[Scientific theory|theories]] to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a [[concurrent algorithm]] compared to the simpler [[sequential algorithm]].\n\nFor example, a failure in concurrency control can result in [[data corruption]] from [[Torn data-access operation|torn read or write operations]].\n\n==Concurrency control in databases==\n\'\'\'Comments:\'\'\'\n# This section is applicable to all transactional systems, i.e., to all systems that use \'\'[[database transaction]]s\'\' (\'\'atomic transactions\'\'; e.g., transactional objects in [[Systems management]] and in networks of [[smartphone]]s which typically implement private, dedicated database systems), not only general-purpose [[database management system]]s (DBMSs).\n# DBMSs need to deal also with concurrency control issues not typical just to database transactions but rather to operating systems in general. These issues (e.g., see \'\'[[Concurrency control#Concurrency control in operating systems|Concurrency control in operating systems]]\'\' below) are out of the scope of this section.\n\nConcurrency control in [[Database management system]]s (DBMS; e.g., [[#Bern87|Bernstein et al. 1987]], [[#Weikum01|Weikum and Vossen 2001]]), other [[database transaction|transactional]] objects, and related distributed applications (e.g., [[Grid computing]] and [[Cloud computing]]) ensures that \'\'[[database transaction]]s\'\' are performed [[Concurrency (computer science)|concurrently]] without violating the [[data integrity]] of the respective [[database]]s. Thus concurrency control is an essential element for correctness in any system where two database transactions or more, executed with time overlap, can access the same data, e.g., virtually in any general-purpose database system. Consequently, a vast body of related research has been accumulated since database systems emerged in the early 1970s. A well established concurrency control [[Scientific theory|theory]] for database systems is outlined in the references mentioned above: [[Serializability|serializability theory]], which allows to effectively design and analyze concurrency control methods and mechanisms. An alternative theory for concurrency control of atomic transactions over [[abstract data type]]s is presented in ([[#Lynch1993|Lynch et al. 1993]]), and not utilized below. This theory is more refined, complex, with a wider scope, and has been less utilized in the Database literature than the classical theory above. Each theory has its pros and cons, emphasis and [[insight]]. To some extent they are complementary, and their merging may be useful.\n\nTo ensure correctness, a DBMS usually guarantees that only \'\'[[Serializability|serializable]]\'\' transaction [[Schedule (computer science)|schedule]]s are generated, unless \'\'serializability\'\' is [[Serializability#Relaxing serializability|intentionally relaxed]] to increase performance, but only in cases where application correctness is not harmed. For maintaining correctness in cases of failed (aborted) transactions (which can always happen for many reasons) schedules also need to have the \'\'[[Serializability#Correctness - recoverability|recoverability]]\'\' (from abort) property. A DBMS also guarantees that no effect of \'\'committed\'\' transactions is lost, and no effect of \'\'aborted\'\' ([[Rollback (data management)|rolled back]]) transactions remains in the related database. Overall transaction characterization is usually summarized by the [[ACID]] rules below. As databases have become [[Distributed database|distributed]], or needed to cooperate in distributed environments (e.g., [[Federated database]]s in the early 1990, and [[Cloud computing]] currently), the effective distribution of concurrency control mechanisms has received special attention.\n\n===Database transaction and the ACID rules===\n{{main|Database transaction|ACID}}\nThe concept of a \'\'database transaction\'\' (or \'\'atomic transaction\'\') has evolved in order to enable both a well understood database system behavior in a faulty environment where crashes can happen any time, and \'\'recovery\'\' from a crash to a well understood database state. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction\'s programmer via special transaction commands). Every database transaction obeys the following rules (by support in the database system; i.e., a database system is designed to guarantee them for the transactions it runs):\n*\'\'\'[[Atomicity (database systems)|Atomicity]]\'\'\' - Either the effects of all or none of its operations remain ("all or nothing" semantics) when a [[database transaction|transaction]] is completed (\'\'committed\'\' or \'\'aborted\'\' respectively). In other words, to the outside world a committed transaction appears (by its effects on the database) to be indivisible (atomic), and an aborted transaction does not affect the database at all.\n*\'\'\'[[Consistency (database systems)|Consistency]]\'\'\' - Every transaction must leave the database in a consistent (correct) state, i.e., maintain the predetermined integrity rules of the database (constraints upon and among the database\'s objects). A transaction must transform a database from one consistent state to another consistent state (however, it is the responsibility of the transaction\'s programmer to make sure that the transaction itself is correct, i.e., performs correctly what it intends to perform (from the application\'s point of view) while the predefined integrity rules are enforced by the DBMS). Thus since a database can be normally changed only by transactions, all the database\'s states are consistent.\n*\'\'\'[[Isolation (database systems)|Isolation]]\'\'\' - Transactions cannot interfere with each other (as an end result of their executions). Moreover, usually (depending on concurrency control method) the effects of an incomplete transaction are not even visible to another transaction. Providing isolation is the main goal of concurrency control.\n*\'\'\'[[Durability (database systems)|Durability]]\'\'\' - Effects of successful (committed) transactions must persist through [[Crash (computing)|crash]]es (typically by recording the transaction\'s effects and its commit event in a [[non-volatile memory]]).\n\nThe concept of atomic transaction has been extended during the years to what has become [[Business transaction management|Business transactions]] which actually implement types of [[Workflow]] and are not atomic. However also such enhanced transactions typically utilize atomic transactions as components.\n\n===Why is concurrency control needed?===\nIf transactions are executed \'\'serially\'\', i.e., sequentially with no overlap in time, no transaction concurrency exists. However, if concurrent transactions with interleaving operations are allowed in an uncontrolled manner, some unexpected, undesirable result may occur, such as:\n# The lost update problem: A second transaction writes a second value of a data-item (datum) on top of a first value written by a first concurrent transaction, and the first value is lost to other transactions running concurrently which need, by their precedence, to read the first value. The transactions that have read the wrong value end with incorrect results.\n# The dirty read problem: Transactions read a value written by a transaction that has been later aborted. This value disappears from the database upon abort, and should not have been read by any transaction ("dirty read"). The reading transactions end with incorrect results.\n# The incorrect summary problem: While one transaction takes a summary over the values of all the instances of a repeated data-item, a second transaction updates some instances of that data-item. The resulting summary does not reflect a correct result for any (usually needed for correctness) precedence order between the two transactions (if one is executed before the other), but rather some random result, depending on the timing of the updates, and whether certain update results have been included in the summary or not.\n\nMost high-performance transactional systems need to run transactions concurrently to meet their performance requirements. Thus, without concurrency control such systems can neither provide correct results nor maintain their databases consistent.\n\n===Concurrency control mechanisms===\n\n====Categories====\nThe main categories of concurrency control mechanisms are:\n* \'\'\'[[Optimistic concurrency control|Optimistic]]\'\'\' - Delay the checking of whether a transaction meets the isolation and other integrity rules (e.g., [[serializability]] and [[Serializability#Correctness - recoverability|recoverability]]) until its end, without blocking any of its (read, write) operations ("...and be optimistic about the rules being met..."), and then abort a transaction to prevent the violation, if the desired rules are to be violated upon its commit. An aborted transaction is immediately restarted and re-executed, which incurs an obvious overhead (versus executing it to the end only once). If not too many transactions are aborted, then being optimistic is usually a good strategy.\n* \'\'\'Pessimistic\'\'\' - Block an operation of a transaction, if it may cause violation of the rules, until the possibility of violation disappears. Blocking operations is typically involved with performance reduction.\n*\'\'\'Semi-optimistic\'\'\' - Block operations in some situations,  if they may cause violation of some rules, and do not block in other situations while delaying rules checking (if needed) to transaction\'s end, as done with optimistic.\n\nDifferent categories provide different performance, i.e., different average transaction completion rates (\'\'throughput\'\'), depending on transaction types mix, computing level of parallelism, and other factors. If selection and knowledge about trade-offs are available, then category and method should be chosen to provide the highest performance.\n\nThe mutual blocking between two transactions (where each one blocks the other) or more results in a [[deadlock]], where the transactions involved are stalled and cannot reach completion. Most non-optimistic mechanisms (with blocking) are prone to deadlocks which are resolved by an intentional abort of a stalled transaction (which releases the other transactions in that deadlock), and its immediate restart and re-execution. The likelihood of a deadlock is typically low.\n\nBlocking, deadlocks, and aborts all result in performance reduction, and hence the trade-offs between the categories.\n\n====Methods====\nMany methods for concurrency control exist. Most of them can be implemented within either main category above. The major methods,<ref name=Bern2009>[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 \'\'Principles of Transaction Processing\'\', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (page 145)</ref> which have each many variants, and in some cases may overlap or be combined, are:\n#Locking (e.g., \'\'\'[[Two-phase locking]]\'\'\' - 2PL) - Controlling access to data by [[Lock (computer science)|locks]] assigned to the data. Access of a transaction to a data item (database object) locked by another transaction may be blocked (depending on lock type and access operation type) until lock release.\n#\'\'\'Serialization [[Serializability#Testing conflict serializability|graph checking]]\'\'\' (also called Serializability, or Conflict, or Precedence graph checking) - Checking for [[Cycle (graph theory)|cycles]] in the schedule\'s [[Directed graph|graph]] and breaking them by aborts.\n#\'\'\'[[Timestamp-based concurrency control|Timestamp ordering]]\'\'\' (TO) - Assigning timestamps to transactions, and controlling or checking access to data by timestamp order.\n#\'\'\'[[Commitment ordering]]\'\'\' (or Commit ordering; CO) - Controlling or checking transactions\' chronological order of commit events to be compatible with their respective [[Serializability#Testing conflict serializability|precedence order]].\n\nOther major concurrency control types that are utilized in conjunction with the methods above include:\n\n* \'\'\'[[Multiversion concurrency control]]\'\'\' (MVCC) - Increasing concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions\' read operations of several last relevant versions (of each object) depending on scheduling method.\n* \'\'\'[[Index locking|Index concurrency control]]\'\'\' - Synchronizing access operations to [[Index (database)|index]]es, rather than to user data. Specialized methods provide substantial performance gains.\n* \'\'\'Private workspace model\'\'\' (\'\'\'Deferred update\'\'\') - Each transaction maintains a private workspace for its accessed data, and its changed data become visible outside the transaction only upon its commit (e.g., [[#Weikum01|Weikum and Vossen 2001]]). This model provides a different concurrency control behavior with benefits in many cases.\n\nThe most common mechanism type in database systems since their early days in the 1970s has been \'\'[[Two-phase locking|Strong strict Two-phase locking]]\'\' (SS2PL; also called \'\'Rigorous scheduling\'\' or \'\'Rigorous 2PL\'\') which is a special case (variant) of both [[Two-phase locking]] (2PL) and [[Commitment ordering]] (CO). It is pessimistic. In spite of its long name (for historical reasons) the idea of the \'\'\'SS2PL\'\'\' mechanism is simple: "Release all locks applied by a transaction only after the transaction has ended." SS2PL (or Rigorousness) is also the name of the set of all schedules that can be generated by this mechanism, i.e., these are SS2PL (or Rigorous) schedules, have the SS2PL (or Rigorousness) property.\n\n===Major goals of concurrency control mechanisms===\nConcurrency control mechanisms firstly need to operate correctly, i.e., to maintain each transaction\'s integrity rules (as related to concurrency; application-specific integrity rule are out of the scope here) while transactions are running concurrently, and thus the integrity of the entire transactional system. Correctness needs to be achieved with as good performance as possible. In addition, increasingly a need exists to operate effectively while transactions are [[Distributed transaction|distributed]] over [[Process (computing)|processes]], [[computer]]s, and [[computer network]]s. Other subjects that may affect concurrency control are [[Data recovery|recovery]] and [[Replication (computer science)|replication]].\n\n====Correctness====\n\n=====Serializability=====\n{{Main|Serializability}}\n\nFor correctness, a common major goal of most concurrency control mechanisms is generating [[Schedule (computer science)|schedule]]s with the \'\'[[Serializability]]\'\' property. Without serializability undesirable phenomena may occur, e.g., money may disappear from accounts, or be generated from nowhere. \'\'\'Serializability\'\'\' of a schedule means equivalence (in the resulting database values) to some \'\'serial\'\' schedule with the same transactions (i.e., in which transactions are sequential with no overlap in time, and thus completely isolated from each other: No concurrent access by any two transactions to the same data is possible). Serializability is considered the highest level of [[isolation (database systems)|isolation]] among [[database transaction]]s, and the major correctness criterion for concurrent transactions. In some cases compromised, [[serializability#Relaxing serializability|relaxed forms]] of serializability are allowed for better performance (e.g., the popular \'\'[[Snapshot isolation]]\'\' mechanism) or to meet [[availability]] requirements in highly distributed systems (see \'\'[[Eventual consistency]]\'\'), but only if application\'s correctness is not violated by the relaxation (e.g., no relaxation is allowed for [[money]] transactions, since by relaxation money can disappear, or appear from nowhere).\n\nAlmost all implemented concurrency control mechanisms achieve serializability by providing \'\'[[Serializability#View and conflict serializability|Conflict serializablity]]\'\', a broad special case of serializability (i.e., it covers, enables most serializable schedules, and does not impose significant additional delay-causing constraints) which can be implemented efficiently.\n\n=====Recoverability=====\n:See \'\'[[Serializability#Correctness - recoverability|Recoverability]]\'\' in \'\'[[Serializability]]\'\'\n\n\'\'\'Comment:\'\'\' While in the general area of systems the term "recoverability" may refer to the ability of a system to recover from failure or from an incorrect/forbidden state, within concurrency control of database systems this term has received a specific meaning.\n\nConcurrency control typically also ensures the \'\'[[Serializability#Correctness - recoverability|Recoverability]]\'\' property of schedules for maintaining correctness in cases of aborted transactions (which can always happen for many reasons). \'\'\'Recoverability\'\'\' (from abort) means that no committed transaction in a schedule has read data written by an aborted transaction. Such data disappear from the database (upon the abort) and are parts of an incorrect database state. Reading such data violates the consistency rule of ACID. Unlike Serializability, Recoverability cannot be compromised, relaxed at any case, since any relaxation results in quick database integrity violation upon aborts. The major methods listed above provide serializability mechanisms. None of them in its general form automatically provides recoverability, and special considerations and mechanism enhancements are needed to support recoverability. A commonly utilized special case of recoverability is \'\'[[Schedule (computer science)#Strict|Strictness]]\'\', which allows efficient database recovery from failure (but excludes optimistic implementations; e.g., [[Commitment ordering#Strict CO (SCO)|Strict CO (SCO)]] cannot have an optimistic implementation, but [[The History of Commitment Ordering#Semi-optimistic database scheduler|has semi-optimistic ones]]).\n\n\'\'\'Comment:\'\'\' Note that the \'\'Recoverability\'\' property is needed even if no database failure occurs and no database \'\'recovery\'\' from failure is needed. It is rather needed to correctly automatically handle transaction aborts, which may be unrelated to database failure and recovery from it.\n\n====Distribution====\nWith the fast technological development of computing the difference between local and distributed computing over low latency [[Computer network|networks]] or [[Bus (computing)|buses]] is blurring. Thus the quite effective utilization of local techniques in such distributed environments is common, e.g., in [[computer cluster]]s and [[multi-core processor]]s. However the local techniques have their limitations and use multi-processes (or threads) supported by multi-processors (or multi-cores) to scale. This often turns transactions into distributed ones, if they themselves need to span multi-processes. In these cases most local concurrency control techniques do not scale well.\n\n=====Distributed serializability and Commitment ordering=====\n{{POV-section|Commitment ordering|date=November 2011}}\n:See \'\'[[Serializability#Distributed serializability|Distributed serializability]]\'\' in \'\'[[Serializability]]\'\'\n{{Main|Global serializability}} {{Main|Commitment ordering}}\nAs database systems have become [[Distributed database|distributed]], or started to cooperate in distributed environments (e.g., [[Federated database]]s in the early 1990s, and nowadays [[Grid computing]], [[Cloud computing]], and networks with [[smartphone]]s), some transactions have become distributed. A [[distributed transaction]] means that the transaction spans [[Process (computing)|processes]], and may span [[computer]]s and geographical sites. This generates a need in effective [[distributed concurrency control]] mechanisms. Achieving the Serializability property of a distributed system\'s schedule (see \'\'[[Serializability#Distributed serializability|Distributed serializability]]\'\' and \'\'[[Global serializability]]\'\' (\'\'Modular serializability\'\')) effectively poses special challenges typically not met by most of the regular serializability mechanisms, originally designed to operate locally. This is especially due to a need in costly distribution of concurrency control information amid communication and computer [[latency (engineering)|latency]]. The only known general effective technique for distribution is Commitment ordering, which was disclosed publicly in 1991 (after being [[patent]]ed). \'\'\'[[Commitment ordering]]\'\'\' (Commit ordering, CO; [[#Raz92|Raz 1992]]) means that transactions\' chronological order of commit events is kept compatible with their respective [[Serializability#Testing conflict serializability|precedence order]]. CO does not require the distribution of concurrency control information and provides a general effective solution ([[Reliability engineering|reliable]], high-performance, and [[Scalability|scalable]]) for both distributed and global serializability, also in a heterogeneous environment with database systems (or other transactional objects) with different (any) concurrency control mechanisms.<ref name=Bern2009/> CO is indifferent to which mechanism is utilized, since it does not interfere with any transaction operation scheduling (which most mechanisms control), and only determines the order of commit events. Thus, CO enables the efficient distribution of all other mechanisms, and also the distribution of a mix of different (any) local mechanisms, for achieving distributed and global serializability. The existence of such a solution has been considered "unlikely" until 1991, and by many experts also later, due to misunderstanding of the [[Commitment ordering#Summary|CO solution]] (see [[Global serializability#Quotations|Quotations]] in \'\'[[Global serializability]]\'\'). An important side-benefit of CO is [[Commitment ordering#Exact characterization of voting-deadlocks by global cycles|automatic distributed deadlock resolution]]. Contrary to CO, virtually all other techniques (when not combined with CO) are prone to [[Deadlock#Distributed deadlock|distributed deadlocks]] (also called global deadlocks) which need special handling. CO is also the name of the resulting schedule property: A schedule has the CO property if the chronological order of its transactions\' commit events is compatible with the respective transactions\' [[Serializability#Testing conflict serializability|precedence (partial) order]].\n\n[[Two-phase locking|SS2PL]] mentioned above is a variant (special case) of CO and thus also effective to achieve distributed and global serializability. It also provides automatic distributed deadlock resolution (a fact overlooked in the research literature even after CO\'s publication), as well as Strictness and thus Recoverability. Possessing these desired properties together with known efficient locking based implementations explains SS2PL\'s popularity. SS2PL has been utilized to efficiently achieve Distributed and Global serializability since the 1980, and has become the [[de facto standard]] for it. However, SS2PL is blocking and constraining (pessimistic), and with the proliferation of distribution and utilization of systems different from traditional database systems (e.g., as in [[Cloud computing]]), less constraining types of CO (e.g., [[Commitment ordering#Distributed optimistic CO (DOCO)|Optimistic CO]]) may be needed for better performance.\n\n\'\'\'Comments:\'\'\'\n# The \'\'Distributed conflict serializability\'\' property in its general form is difficult to achieve efficiently, but it is achieved efficiently via its special case \'\'Distributed CO\'\': Each local component (e.g., a local DBMS) needs both to provide some form of CO, and enforce a special \'\'vote ordering strategy\'\' for the \'\'[[Two-phase commit protocol]]\'\' (2PC: utilized to commit [[distributed transaction]]s). Differently from the general Distributed CO, \'\'Distributed SS2PL\'\' [[Commitment ordering#Strong strict two phase locking (SS2PL)|exists automatically when all local components are SS2PL based]] (in each component CO exists, implied, and the vote ordering strategy is now met automatically). This fact has been known and utilized since the 1980s (i.e., that SS2PL exists globally, without knowing about CO) for efficient Distributed SS2PL, which implies Distributed serializability and strictness (e.g., see [[#Raz92|Raz 1992]], page 293; it is also implied in [[#Bern87|Bernstein et al. 1987]], page 78). Less constrained Distributed serializability and strictness can be efficiently achieved by Distributed [[Commitment ordering#Strict CO (SCO)|Strict CO (SCO)]], or by a mix of SS2PL based and SCO based local components.\n# About the references and Commitment ordering: ([[#Bern87|Bernstein et al. 1987]]) was published before the discovery of CO in 1990. The CO schedule property is called \'\'[[The History of Commitment Ordering#Dynamic atomicity|Dynamic atomicity]]\'\' in ([[#Lynch1993|Lynch et al. 1993]], page 201). CO is described in ([[#Weikum2001|Weikum and Vossen 2001]], pages 102, 700), but the description is partial and misses [[Commitment ordering#Summary|CO\'s essence]]. ([[#Raz92|Raz 1992]]) was the first refereed and accepted for publication article about CO algorithms (however, publications about an equivalent Dynamic atomicity property can be traced to 1988). Other [[Commitment ordering#References|CO articles]] followed. (Bernstein and Newcomer 2009)<ref name=Bern2009/> note CO as one of the four major concurrency control methods, and CO\'s ability to provide interoperability among other methods.\n\n=====Distributed recoverability=====\nUnlike Serializability, \'\'Distributed recoverability\'\' and \'\'Distributed strictness\'\' can be achieved efficiently in a straightforward way, similarly to the way Distributed CO is achieved: In each database system they have to be applied locally, and employ a vote ordering strategy for the [[Two-phase commit protocol]] (2PC; [[#Raz92|Raz 1992]], page 307).\n\nAs has been mentioned above, Distributed [[Two-phase locking|SS2PL]], including Distributed strictness (recoverability) and Distributed [[commitment ordering]] (serializability), automatically employs the needed vote ordering strategy, and is achieved (globally) when employed locally in each (local) database system (as has been known and utilized for many years; as a matter of fact locality is defined by the boundary of a 2PC participant ([[#Raz92|Raz 1992]]) ).\n\n====Other major subjects of attention====\nThe design of concurrency control mechanisms is often influenced by the following subjects:\n\n=====Recovery=====\n{{Main|Data recovery}}\nAll systems are prone to failures, and handling \'\'[[Data recovery|recovery]]\'\' from failure is a must. The properties of the generated schedules, which are dictated by the concurrency control mechanism, may affect the effectiveness and efficiency of recovery. For example, the Strictness property (mentioned in the section [[Concurrency control#Recoverability|Recoverability]] above) is often desirable for an efficient recovery.\n\n=====Replication=====\n{{Main|Replication (computer science)}}\nFor high availability database objects are often \'\'[[Replication (computer science)|replicated]]\'\'. Updates of replicas of a same database object need to be kept synchronized. This may affect the way concurrency control is done (e.g., Gray et al. 1996<ref name=Gray1996>{{cite conference\n | author = [[Jim Gray (computer scientist)|Gray, J.]]\n |author2=Helland, P. |author3=[[Patrick O\'Neil|O\'Neil, P.]] |author4=[[Dennis Shasha|Shasha, D.]]\n | year = 1996\n | conference = The dangers of replication and a solution\n | title = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]\n | pages = 173–182\n | conference-url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf\n | doi = 10.1145/233269.233330\n }}</ref>).\n\n=== See also ===\n* [[Schedule (computer science)|Schedule]]\n* [[Isolation (computer science)]]\n* [[Distributed concurrency control]]\n* [[Global concurrency control]]\n\n===References===\n*<cite id=Bern87>[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  \'\'Concurrency Control and Recovery in Database Systems\'\'] (free PDF download), Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5 </cite>\n*<cite id=Weikum01>[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  \'\'Transactional Information Systems\'\'], Elsevier, ISBN 1-55860-508-8 </cite>\n*<cite id=Lynch1993>[[Nancy Lynch]], Michael Merritt, William Weihl, Alan Fekete (1993): [http://www.elsevier.com/wps/find/bookdescription.cws_home/680521/description#description \'\'Atomic Transactions in Concurrent and Distributed Systems \'\'], Morgan Kauffman (Elsevier), August 1993, ISBN 978-1-55860-104-8, ISBN 1-55860-104-X </cite>\n*<cite id=Raz92>[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html  "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment."]  ([http://www.vldb.org/conf/1992/P292.PDF  PDF]), \'\'Proceedings of the Eighteenth International Conference on Very Large Data Bases\'\' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) </cite>\n\n===Footnotes===\n{{Reflist}}\n\n== Concurrency control in operating systems ==\n{{Expand section|date=December 2010}}\n[[Computer multitasking|Multitasking]] operating systems, especially [[real-time operating system]]s, need to maintain the illusion that all tasks running on top of them are all running at the same time, even though only one or a few tasks really are running at any given moment due to the limitations of the hardware the operating system is running on. Such multitasking is fairly simple when all tasks are independent from each other. However, when several tasks try to use the same resource, or when tasks try to share information, it can lead to confusion and inconsistency. The task of [[concurrent computing]] is to solve that problem. Some solutions involve "locks" similar to the locks used in databases, but they risk causing problems of their own such as [[deadlock]]. Other solutions are [[Non-blocking algorithm]]s and [[Read-copy-update]].\n\n=== See also ===\n* [[Linearizability]]\n* [[Mutual exclusion]]\n* [[Semaphore (programming)]]\n* [[Lock (computer science)]]\n* [[Software transactional memory]]\n* [[Transactional Synchronization Extensions]]\n\n===References===\n*  Andrew S. Tanenbaum, Albert S Woodhull (2006): \'\'Operating Systems Design and Implementation, 3rd Edition\'\', [[Prentice Hall]], ISBN 0-13-142938-8\n* {{cite book | last = Silberschatz | first = Avi |author2=Galvin, Peter |author3=Gagne, Greg | title = Operating Systems Concepts, 8th edition | publisher = [[John Wiley & Sons]] | year = 2008 | isbn = 0-470-12872-0 }}\n\n{{Databases}}\n\n{{DEFAULTSORT:Concurrency Control}}\n[[Category:Concurrency control| ]]\n[[Category:Data management]]\n[[Category:Databases]]\n[[Category:Transaction processing]]']
['Query language', '494528', "{{redirect|Database language|other types of database languages|Database#Languages}}\n{{Multiple issues|\n{{prose|date=October 2010}}\n{{refimprove|date=October 2010}}\n}}\n\n'''Query languages''' are [[computer language]]s used to make queries in [[database]]s and [[information system]]s.\n\n==Types==\nBroadly, query languages can be classified according to whether they are database query languages or [[information retrieval query language]]s. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.\n\n==Examples==\nExamples include:\n* [[.QL]] is a proprietary object-oriented query language for querying [[relational database]]s; successor of Datalog;\n* [[Contextual Query Language]] (CQL) a formal language for representing queries to [[information retrieval]] systems such as web indexes or bibliographic catalogues.\n* CQLF (CODYASYL Query Language, Flat) is a query language for [[CODASYL]]-type databases;\n* [[Concept-Oriented Query Language]] (COQL) is used in the concept-oriented model (COM). It is based on a novel [[data modeling]] construct, concept, and uses such operations as projection and de-projection for multi-dimensional analysis, analytical operations and inference;\n* [[Cypher Query Language|Cypher]] is a query language for the [[Neo4j]] graph database;\n* [[Data Mining Extensions|DMX]] is a query language for [[Data Mining]] models;\n* [[Datalog]] is a query language for [[deductive database]]s;\n* [[F-logic]] is a declarative object-oriented language for [[deductive database]]s and [[knowledge representation]].\n* [[Facebook Query Language|FQL]] enables you to use a [[SQL]]-style interface to query the data exposed by the [[Graph API]]. It provides advanced features not available in the [[Graph API]].<ref>{{cite web|url=https://developers.facebook.com/docs/technical-guides/fql/|title=FQL Overview|work=Facebook Developers}}</ref>\n* [[Gellish English]] is a language that can be used for queries in Gellish English Databases, for dialogues (requests and responses) as well as for information modeling and [[knowledge modeling]];<ref>http://gellish.wiki.sourceforge.net/Querying+a+Gellish+English+database{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n* [[Gremlin (programming language)|Gremlin]] is an [[Apache Software Foundation]] graph traversal language for OLTP and OLAP graph systems.\n* [[HTSQL]] is a query language that translates [[HTTP]] queries to [[SQL]];\n* [[ISBL]] is a query language for [[PRTV]], one of the earliest relational database management systems;\n* [[LINQ]] query-expressions is a way to query various data sources from [[.NET Framework|.NET]] languages\n* [[LDAP]] is an [[application protocol]] for querying and modifying [[directory services]] running over [[TCP/IP]];\n* LogiQL is a variant of Datalog and is the query language for the LogicBlox system.\n* [[Molecular Query Language|MQL]] is a [[cheminformatics]] query language for a [[substructure search]] allowing beside nominal properties also numerical properties;\n* [[MultiDimensional eXpressions|MDX]] is a query language for [[OLAP]] databases;\n* [[N1QL]] is a [[Couchbase, Inc.|Couchbase]]'s query language finding data in [[Couchbase Server]]s;\n* [[Object Query Language|OQL]] is Object Query Language;\n* [[Object Constraint Language|OCL]] (Object Constraint Language). Despite its name, OCL is also an object query language and an [[Object Management Group|OMG]] standard;\n* [[OPath]], intended for use in querying [[WinFS]] ''Stores'';\n* [[OttoQL]], intended for querying tables, [[XML]], and databases;\n* [[Poliqarp Query Language]] is a special query language designed to analyze annotated text. Used in the [[Poliqarp]] search engine;\n* [[PQL]] is a [[special-purpose programming language]] for managing [[process model]]s based on information about [[wiktionary:Scenario|scenarios]] that these models describe;\n* [[QUEL query languages|QUEL]] is a [[relational database]] access language, similar in most ways to [[SQL]];\n* [[RDQL]] is a [[Resource Description Framework|RDF]] query language;\n* [[ReQL]] is a query language used in [http://rethinkdb.com/docs/introduction-to-reql/ RethinkDB];\n* [[Smiles arbitrary target specification|SMARTS]] is the [[cheminformatics]] standard for a [[substructure search]];\n* [[SPARQL]] is a query language for [[Resource Description Framework|RDF]] [[Graph (discrete mathematics)|graphs]];\n* [[SPL (Search Processing Language)|SPL]] is a search language for machine-generated [[big data]], based upon Unix Piping and SQL.\n* SCL is the Software Control Language to query and manipulate [[Endevor]] objects\n* [[SQL]] is a well known query language and [[Data Manipulation Language]] for [[relational database]]s;\n* [[SuprTool]] is a proprietary query language for SuprTool, a database access program used for accessing data in ''Image/SQL'' (formerly [[TurboIMAGE]]) and Oracle databases;\n* [[TMQL]] Topic Map Query Language is a query language for [[Topic Maps]];\n* TQL is a language used to [http://cmshelpcenter.saas.hp.com/CMS/10.21/ucmdb-docs/docs/eng/doc_lib/Content/modeling/Tql_c_Overview.htm query topology for HP products] \n* [[D (data language specification)|Tutorial D]] is a query language for [[Relational database management system|truly relational database management systems]] (TRDBMS);\n* [[XQuery]] is a query language for [[XML database|XML data sources]];\n* [[XPath]] is a declarative language for navigating XML documents;\n* [[XSPARQL]] is an integrated query language combining XQuery with SPARQL to query both XML and RDF data sources at once;\n* [[Yahoo! query language|YQL]] is an [[SQL]]-like query language created by [[Yahoo!]]\n* Search engine query languages, e.g., as used by [[Google Search|Google]]<ref>\n{{cite web\n| title = Search operators\n| url = https://support.google.com/websearch/answer/2466433?hl=en\n| accessdate = August 22, 2015\n| publisher = Google\n}}</ref> or [[Bing (search engine)|Bing]]<ref>\n{{cite web\n| title = Bing Query Language\n| url = https://msdn.microsoft.com/en-us/library/ff795667.aspx\n| accessdate = August 22, 2015\n| publisher = Microsoft\n}}</ref>\n\n== See also ==\n* [[Data manipulation language]]\n\n== References ==\n{{Reflist}}\n\n{{Database}}\n{{Databases}}\n{{Computer language}}\n{{Query languages}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Query Language}}\n[[Category:Computer languages]]\n[[Category:Data management]]\n[[Category:Query languages|*]]\n\n[[no:Database#Spørrespråk]]"]
['Virtual directory', '943527', 'In [[computing]], the term \'\'\'virtual directory\'\'\' has a couple of meanings. It may simply designate (for example in [[Internet Information Services|IIS]]) a [[Folder (computing)|folder]] which appears in a [[Path (computing)|path]] but which is not actually a subfolder of the preceding folder in the path. However, this article will discuss the term in the context of [[directory service]]s and [[identity management]].\n\nA virtual directory or \'\'\'virtual directory server\'\'\' in this context is a software layer that delivers a single access point for [[identity management]] applications and service platforms. A virtual directory operates as a high-performance, lightweight abstraction layer that resides between client applications and disparate types of identity-data repositories, such as proprietary and standard directories, databases, web services, and applications.\n\nA virtual directory receives queries and directs them to the appropriate data sources by abstracting and virtualizing data. The virtual directory integrates identity data from multiple heterogeneous data stores and presents it as though it were coming from one source. This ability to reach into disparate repositories makes virtual directory technology ideal for consolidating data stored in a distributed environment. \n\n{{As of | 2011}}, virtual directory servers most commonly use the [[Lightweight Directory Access Protocol|LDAP]] protocol, but more sophisticated virtual directories can also support [[SQL]] as well as [[Directory Services Markup Language|DSML]] and [[Service Provisioning Markup Language|SPML]].\n\nIndustry experts have heralded the importance of the virtual directory in modernizing the identity infrastructure. According to Dave Kearns of Network World, "Virtualization is hot and a virtual directory is the building block, or foundation, you should be looking at for your next identity management project."<ref>{{cite web | url=http://www.networkworld.com/article/2305608/access-control/virtual-directory-finally-gains-recognition.html | title=Virtual directory finally gains recognition | publisher=NetworkWorld | date=7 August 2006 | accessdate=14 July 2014 | author=Kearns, Dave}}</ref> In addition, Gartner analyst, Bob Blakley<ref>The Emerging Architecture of Identity Management, Bob Blakley, April 16, 2010.</ref> said that virtual directories are playing an increasingly vital role. In his report, “The Emerging Architecture of Identity Management,” Blakley wrote: “In the first phase, production of identities will be separated from consumption of identities through the introduction of a virtual directory interface.”\n\n==Capabilities==\nVirtual directories can have some or all of the following capabilities:<ref>{{cite web|url=http://optimalidm.com/resources/blog/virtual-directory-server-2/|title=An Introduction To Virtual Directories|publisher=Optimal Idm|accessdate=15 July 2014}}</ref>\n* Aggregate identity data across sources to create a single point of access.\n* Create high-availability for authoritative data stores.\n* Act as identity firewall by preventing [[denial-of-service attack]]s on the primary data stores through an additional virtual layer.\n* Support a common searchable namespace for centralized authentication.\n* Present a unified virtual view of user information stored across multiple systems.\n* Delegate authentication to backend sources through source-specific security means.\n* Virtualize data sources to support migration from legacy data stores without modifying the applications that rely on them.\n* Enrich identities with attributes pulled from multiple data stores, based on a link between user entries. \n\nSome advanced identity virtualization platforms can also:\n* Enable application-specific, customized views of identity data without violating internal or external regulations governing identity data. Reveal contextual relationships between objects through hierarchical directory structures.\n* Develop advanced correlation across diverse sources using correlation rules. \n* Build a global user identity by correlating unique user accounts across various data stores, and enrich identities with attributes pulled from multiple data stores, based on a link between user entries. \n* Enable constant data refresh for real-time updates through a persistent cache.\n\n==Advantages ==\nVirtual directories:\n* Enable faster deployment because users do not need to add and sync additional application-specific data sources \n* Leverage existing identity infrastructure and security investments to deploy new services \n* Deliver high availability of data sources \n* Provide application-specific views of identity data which can help avoid the need to develop a master enterprise schema\n* Allow a single view of identity data without violating internal or external regulations governing identity data\n* Act as identity firewalls by preventing denial-of-service attacks on the primary data-stores and providing further security on access to sensitive data\n* Can reflect changes made to authoritative sources in real-time\n* Present a unified virtual view of user information from multiple systems so that it appears to reside in a single system\n* Can secure all backend storage locations with a single security policy\n\n==Disadvantages==\nAn original disadvantage is public perception of "push & pull technologies" which is the general classification of "virtual directories" depending on the nature of their deployment. Virtual directories were initially designed and later deployed with "push technologies" in mind, which also contravened with [[privacy laws of the United States]]. This is no longer the case. There are, however, other disadvantages in the current technologies.\n\n* The classical virtual directory based on proxy cannot modify underlying data structures or create new views based on the relationships of data from across multiple systems. So if an application requires a different structure, such as a flattened list of identities, or a deeper hierarchy for delegated administration, a virtual directory is limited. \n* Many virtual directories cannot correlate same-users across multiple diverse sources in the case of duplicate users\n* Virtual directories without advanced caching technologies cannot scale to heterogeneous, high-volume environments.\n\n==Sample terminology==\n{{Overly detailed|section=yes|date=July 2014}}\n* Unify metadata: Extract schemas from the local data source, map them to a common format, and link the same identities from different data silos based on a unique identifier.\n* Namespace joining: Create a single large directory by bringing multiple directories together at the namespace level. For instance, if one directory has the namespace "ou=internal,dc=domain,dc=com" and a second directory has the namespace "ou=external,dc=domain,dc=com," then creating a virtual directory with both namespaces is an example of namespace joining.\n* Identity joining: Enrich identities with attributes pulled from multiple data stores, based on a link between user entries.  For instance if the user joeuser exists in a directory as "cn=joeuser,ou=users" and in a database with a username of "joeuser" then the "joeuser" identity can be constructed from both the directory and the database.\n* Data remapping: The translation of data inside of the virtual directory. For instance, mapping “uid” to “samaccountname,” so a client application that only supports a standard LDAP-compliant data source is able to search an Active Directory namespace, as well.\n* Query routing: Route requests based on certain criteria, such as “write operations going to a master, while read operations are forwarded to replicas.”\n* Identity routing: Virtual directories may support the routing of requests based on certain criteria (such as write operations going to a master while read operations being forwarded to replicas).\n* Authoritative source: A "virtualized" data repository, such as a directory or database, that the virtual directory can trust for user data.\n* Server groups: Group one or more servers containing the same data and functionality. A typical implementation is the multi-master, multi-replica environment in which replicas process "read" requests and are in one server group, while masters process "write" requests and are in another, so that servers are grouped by their response to external stimuli, even though all share the same data.\n\n==Use cases==\nThe following are sample use cases of virtual directories:\n* Integrating multiple directory namespaces to create a central enterprise directory.\n* Supporting infrastructure integrations after mergers and acquisitions. \n* Centralizing identity storage across the infrastructure, making identity information available to applications through various protocols (including LDAP, JDBC, and web services). \n* Creating a single access point for [[Web Access Management|web access management]] (WAM) tools. \n* Enabling web [[single sign-on]] (SSO) across varied sources or domains.\n* Supporting role-based, fine-grained authorization policies\n* Enabling authentication across different security domains using each domain’s specific credential checking method.\n* Improving secure access to information both inside and outside of the firewall.\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Virtual Directory}}\n[[Category:Data management]]']
['Reference table', '1785206', "{{Unreferenced|auto=yes|date=December 2009}}\n\nA '''reference table''' (or table of reference) may mean a set of references that an author may have cited or gained inspiration from whilst writing an article, similar to a [[bibliography]].\n\nIt can also mean an [[Table (information)|information table]] that is used as a quick and easy reference for things that are difficult to remember such as comparing [[Imperial unit|imperial]] with [[SI|metric]] measurements. This kind of data is known as [[reference data]].\n\nIn the context of [[database design]] a reference table is a table into which an [[enumeration|enumerated]] set of possible values of a certain field data type is divested. For example, in a [[relational model|relational database model]] of a warehouse the entity 'Item' may have a field called 'status' with a predefined set of values such as 'sold', 'reserved', 'out of stock'. In a purely designed database these values would be divested into an extra entity or Reference Table called 'status' in order to achieve [[database normalisation]]. The entity 'status' in this case has no true representative in the real world but rather would an exceptional case where the attribute of a certain database entity is divested into its own table. The advantage of doing this is that internal functionality and optional conditions within the database and the software which utilizes it are easier to modify and extend on that particular aspect.  Establishing an enterprise-wide view of reference tables is called [[master data management]].\n\n{{DEFAULTSORT:Reference Table}}\n[[Category:Data management]]\n\n{{Publish-stub}}"]
['Data auditing', '6890125', "{{Unreferenced|date=December 2009}}\n'''Data auditing''' is the process of conducting a data audit to assess how company's data is fit for given purpose.  This involves [[data profiling|profiling]] the data and assessing the impact of [[data quality|poor quality data]] on the organization's performance and profits.\n\n{{DEFAULTSORT:Data Auditing}}\n{{Tech-stub}}\n\n[[Category:Data management]]\n[[Category:Data quality]]"]
['Photo recovery', '24862923', '{{Advert|date=August 2010}}\n\n\'\'\'Photo recovery\'\'\' is the process of salvaging digital photographs from damaged, failed, corrupted, or inaccessible [[Computer data storage#Secondary storage|secondary storage]] media when it cannot be accessed normally. Photo Recovery can be considered a subset of\nthe overall [[Data Recovery]] field. \n\nPhoto loss or deletion failures may be due to both hardware or software failures.\n\n==Recovering data after hardware failure==\nAn excellent explanation of hardware failures is provided in the section for [[Data Recovery|data recovery]]. Typically, if your\ndrive or card is so badly damaged that your computer can not recognize that a drive/card has been connected, you\nwill need to consult a data recovery service provider.\n\n==Recovering data after logical failure==\nLogical Damage or the inability to view photos can occur due to many reasons. The most common reasons are:\n\n# Deletion of photos.\n# Corruption of boot sector of media.\n# Corruption of [[file system]].\n# [[Disk formatting]].\n# Move or Copy errors.\n\n=== Photo Recovery Using File Carving ===\nThe majority of photo recovery programs work by using a technique called [[file carving|file carving (data carving)]].\nThere are many different file carving techniques that are used to recover photos. Most of these techniques\nfail in the presence of [[file system fragmentation]]. Simson Garfinkel showed that on average 16% of [[JPEG]]s are fragmented,<ref name=garfinkel_dfrws2007>[[Simson Garfinkel]], \'\'Carving Contiguous and Fragmented Files with Fast Object Validation\'\', in Proceedings of the 2007 digital forensics research workshop, DFRWS, Pittsburgh, PA, August 2007</ref> which\nmeans on average 16% of jpegs are recovered partially or appear corrupt when recovered using techniques that\ncan\'t handle fragmented photos.\n\n==== Header-Footer Carving ====\nIn Header-Footer Carving, a recovery program attempts to recover photos based on the standard starting and ending byte\nsignature of the photo format. To take an example, all [[JPEG]]s always begin with the hex sequence "FFD8" and they must\nend with the hex sequence "FFD9".\n\nHeader-Footer Carving cannot be used to recover fragmented photos, and fragmented\nphotos will appear to be partially recovered or corrupt if incorrect data is added. Header-Footer Carving, along\nwith Header-Size Carving, are by far the most common techniques for photo recovery. One of the first non-gui/console\nbased programs to use this technique is [[PhotoRec]].\nUse of footers can often truncate a photo, as many JPEGs contain thumbnails as an embedded object.  If a file is terminated with a FFD9 it will be corrupted, unless nested FFD8/FFD9s are counted.\n\n==== Header-Size Carving ====\nIn Header-Size Carving, a recovery program attempts to recover photos based on the standard starting byte signature of\nthe photo format, along with the size of the photo that is either derived or explicitly stated in the photo format.\nTo take an example all 24-bit Windows Bitmaps (*.bmp), begin with the letters "BM", and store the size of the file in\nthe header. Header-Size Carving cannot be used to recover fragmented photos, and fragmented photos will appear to be\npartially recovered or corrupt if incorrect data is added.\n\n==== File-Structure Based Carving ====\nA more advanced form of carving, a recovery program attempts to recover photos based on detailed knowledge of the\nstructure rules of the photo format. This will enable a recovery program to identify when a photo is not complete or\nfragmented, but more needs to be done to see if a fragmented photo can be recovered. This technique is rarely\nused by most photo recovery programs.\n\n==== Validated Carving ====\nIn validated carving, a decoder is used to detect any errors in recovery of a photo. More advanced forms of validated\ncarving occur when each part of the recovered photo is compared against the rest of the photo to see if it "fits"\nvisually. Validated carving is superb at detecting photos that are either fragmented or have parts over-written or\nmissing. Validated carving alone cannot be used to recover fragmented photos.<ref name=pal_ieee_ip>A. Pal and N. Memon, [http://digital-assembly.com/technology/research/pubs/ieee-trans-2006.pdf "Automated reassembly of file fragmented images using greedy algorithms"] in IEEE Transactions on Image processing, February 2006, pp 385393</ref>\n\n==== Log Carving ====\nLog Carving occurs when a recovery program uses information left over in either file system structures or the log\nto recover a deleted photo. For example, occasionally NTFS will store in the logs the exact location of where the\nfile was located prior to its deletion. A program using Log Carving will be able to then recover the photo. To be\nsure about the quality of recovery, Validated Carving or File-Structure based carving should also be used to\nvalidate the recovered photo.\n\n==== Bi-Fragment Gap Carving ====\nA fragmented photo recovery technique where a header and footer are identified and then all combinations of blocks\nbetween the header and footer are validated to determine which combination results in the correct recovery of the\nphoto.<ref name="garfinkel_dfrws2007"/> This technique will only work if the file is fragmented into two parts.\n\n==== SmartCarving ====\nA process by which fragmented photos are recovered by looking at blocks on the disk and determining which block\nis the best visual match for the photo being recovered. This is done in parallel for all blocks that are not part\nof a recovered file.<ref name=pal_dfrws2008>A. Pal, T. Sencar, N. Memon, [http://digital-assembly.com/technology/research/pubs/dfrws2008.pdf "Detecting File Fragmentation Point Using Sequential Hypothesis Testing"] Digital Forensic Research Workshop, August 2008</ref>\n\n==References==\n<references />\n\n==Further reading==\n* Tanenbaum, A. & Woodhull, A. S. (1997). \'\'Operating Systems: Design And Implementation,\'\' 2nd ed. New York: Prentice Hall.\n*[http://www.informationweek.com/news/windows/showArticle.jhtml?articleID=200000329 What To Do When Windows Vista Crashes: Little-Known Recovery Strategies], from Information Week\n\n[[Category:Data recovery|photo]]\n[[Category:Computer data]]\n[[Category:Data management]]\n[[Category:Hard disk software|*]]\n[[Category:Photography]]']
['Data independence', '1786411', '{{multiple issues|\n{{Cleanup|date=January 2008}}\n{{Unreferenced|date=December 2009}}\n}}\n\n\'\'\'Data independence\'\'\' is the type of [[data]] transparency that matters for a centralised [[Database management system|DBMS]]. It refers to the immunity of user [[application software|applications]] to changes made in the definition and organization of data.\n\nPhysical data independence deals with hiding the details of the storage structure from user applications. The application should not be involved with these issues, since there is no difference in the operation carried out against the data.\n\nThe data independence and operation independence together gives the feature of [[data abstraction]]. There are two levels of data independence.\n\n==First Level of Data Independence==\nThe [[logical]] structure of the data is known as the \'schema definition\'. In general, if a user application operates on a subset of the [[Attribute (computing)|attributes]] of a [[Relation (database)|relation]], it should not be affected later when new attributes are added to the same relation.\nLogical data independence indicates that the conceptual schema can be changed without affecting the existing schemas.\n\n==Second Level of Data Independence==\nThe physical structure of the data is referred to as "physical data description". Physical data independence deals with hiding the details of the storage structure from user applications. The application should not be involved with these issues since, conceptually, there is no difference in the operations carried out against the data. There are three types of data independence:\n# Logical data independence: The ability to change the logical (conceptual) schema without changing the External schema (User View) is called logical data independence. For example, the addition or removal of new entities, attributes, or relationships to the conceptual schema should be possible without having to change existing external schemas or having to rewrite existing application programs.\n# Physical data independence: The ability to change the physical schema without changing the logical schema is called physical data independence. For example, a change to the internal schema, such as using different file organization or storage structures, storage devices, or indexing strategy, should be possible without having to change the conceptual or external schemas.\n#View level data independence: always independent no effect, because there doesn\'t exist any other level above view level.\n\n===Data Independence===\n\nData independence can be explained as follows: Each higher level of the data architecture is immune to changes of the next lower level of the architecture.\n\nThe logical scheme stays unchanged even though the storage space or type of some data is changed for reasons of optimization or reorganization. In this external schema does not change. In this internal schema changes may be required due to some physical schema were reorganized here.  Physical data independence is present in most databases and file environment in which hardware storage of encoding, exact location of data on disk,merging of records, so on this are hidden from user.\n\nOne of the biggest advantage of databases is data independence. It means we can change the conceptual schema at one level without affecting the data at another level. It also means we can change the structure of a database without affecting the data required by users and programs. This feature was not available in the file oriented approach.\n\n==Data Independence Types==\n\nThe ability to modify schema definition in one level without affecting schema definition in the next higher level is called data independence. There are two levels of data independence, they are Physical data independence and Logical data independence.\n\n# Physical data independence is the ability to modify the physical schema without causing application programs to be rewritten. Modifications at the physical level are occasionally necessary to improve performance. It means we change the physical storage/level without affecting the conceptual or external view of the data. The new changes are absorbed by mapping techniques.\n# Logical data independence is the ability to modify the logical schema without causing application program to be rewritten. Modifications at the logical level are necessary whenever the logical structure of the database is altered (for example, when money-market accounts are added to banking system).  Logical Data independence means if we add some new columns or remove some columns from table then the user view and programs should not change. For example: consider two users A & B. Both are selecting the fields "EmployeeNumber" and "EmployeeName". If user B adds a new column (e.g. salary) to his table, it will not effect the external view for user A, though the internal schema of the database has been changed for both users A & B. \n\nLogical data independence is more difficult to achieve than physical data independence, since application programs are heavily dependent on the logical structure of the data that they access.\n\nPhysical data independence means we change the physical storage/level without affecting the conceptual or external view of the data. Mapping techniques absorbs the new changes.\n\n==See also==\n* [[Network transparency]]\n* [[Replication transparency]]\n* [[Codd\'s 12 rules]]\n* [[ANSI-SPARC_Architecture]]\n\n\n{{DEFAULTSORT:Data Independence}}\n[[Category:Data management]]']
['Association rule learning', '577053', '{{Redirect|OneR|filmmaking technique|Long take}}\n{{machine learning bar}}\n\'\'\'Association rule learning\'\'\' is a [[rule-based machine learning]] method for discovering interesting relations between variables in large databases.  It is intended to identify strong rules discovered in databases using some measures of interestingness.<ref name="piatetsky">Piatetsky-Shapiro, Gregory (1991), \'\'Discovery, analysis, and presentation of strong rules\'\', in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., \'\'Knowledge Discovery in Databases\'\', AAAI/MIT Press, Cambridge, MA.</ref>  Based on the concept of strong rules, [[Rakesh Agrawal (computer scientist)|Rakesh Agrawal]], [[Tomasz Imieliński]] and Arun Swami <ref name="mining">{{Cite book | last1 = Agrawal | first1 = R. | last2 = Imieliński | first2 = T. | last3 = Swami | first3 = A. | doi = 10.1145/170035.170072 | chapter = Mining association rules between sets of items in large databases | title = Proceedings of the 1993 ACM SIGMOD international conference on Management of data  - SIGMOD \'93 | pages = 207 | year = 1993 | isbn = 0897915925 | pmid =  | pmc = }}</ref> introduced association rules for discovering regularities between products in large-scale transaction data recorded by [[point-of-sale]] (POS) systems in supermarkets. For example, the rule <math>\\{\\mathrm{onions, potatoes}\\} \\Rightarrow \\{\\mathrm{burger}\\}</math> found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional [[pricing]] or [[product placement]]s. In addition to the above example from [[market basket analysis]] association rules are employed today in many application areas including [[Web usage mining]], [[intrusion detection]], [[Continuous production]], and [[bioinformatics]]. In contrast with [[sequence mining]], association rule learning typically does not consider the order of items either within a transaction or across transactions.\n\n== Definition ==\n{|class="wikitable" style="float: right; margin-left: 1em;"\n|+ Example database with 5 transactions and 5 items\n|-\n! transaction ID !! milk !! bread !! butter !! beer !! diapers\n|-\n| 1 || 1 || 1 || 0 || 0 || 0\n|-\n| 2 || 0 || 0 || 1 || 0 || 0\n|-\n| 3 || 0 || 0 || 0 || 1 || 1\n|-\n| 4 || 1 || 1 || 1 || 0 || 0\n|-\n| 5 || 0 || 1 || 0 || 0 || 0\n|-\n|}\n\nFollowing the original definition by Agrawal et al.<ref name="mining" /> the problem of association rule mining is defined as:\n\nLet <math>I=\\{i_1, i_2,\\ldots,i_n\\}</math> be a set of <math>n</math> binary attributes called \'\'items\'\'.\n\nLet <math>D = \\{t_1, t_2, \\ldots, t_m\\}</math> be a set of transactions called the \'\'database\'\'.\n\nEach \'\'transaction\'\' in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>.\n\nA \'\'rule\'\' is defined as an implication of the form:\n\n<math>X \\Rightarrow Y</math>, where <math>X, Y \\subseteq I</math>.\n\nIn Agrawal et al.,<ref name="mining" /> a \'\'rule\'\' is defined only between a set and a single item, <math>X \\Rightarrow i_j</math> for <math>i_j \\in I</math>.\n\nEvery rule is composed by two different sets of items, also known as \'\'itemsets\'\', <math>X</math> and <math>Y</math>, where <math>X</math> is called \'\'antecedent\'\' or left-hand-side (LHS) and <math>Y</math> \'\'consequent\'\' or right-hand-side (RHS).\n\nTo illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \\{\\mathrm{milk, bread, butter, beer, diapers}\\}</math> and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction.\n\nAn example rule for the supermarket could be <math>\\{\\mathrm{butter, bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> meaning that if butter and bread are bought, customers also buy milk.\n\nNote: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant{{Citation needed|date=February 2015}}, and datasets often contain thousands or millions of transactions.\n\n== Useful Concepts ==\nIn order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.\n\nLet <math>X</math> be an itemset, <math>X \\Rightarrow Y</math> an association rule and <math>T</math> a set of transactions of a given database.\n\n=== Support ===\nSupport is an indication of how frequently the itemset appears in the database.\n\nThe support of <math>X</math> with respect to <math>T</math> is defined as the proportion of transactions <math>t</math> in the database which contains itemset <math>X</math>.\n\n<math>\\mathrm{supp}(X) = \\frac{|\\{t \\in T; X \\subseteq t\\}|}{|T|}</math>\n\nIn the example database, the itemset <math>X=\\{\\mathrm{beer, diapers}\\}</math> has a support of <math>1/5=0.2</math> since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of <math>\\mathrm{supp}()</math> is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).<ref name=":0">{{Cite journal|last=Hahsler|first=Michael|date=2005|title=Introduction to arules – A computational environment for mining association rules and frequent item sets|url=https://mran.revolutionanalytics.com/web/packages/arules/vignettes/arules.pdf|journal=Journal of Statistical Software|doi=|pmid=|access-date=}}</ref>\n\n=== Confidence ===\nConfidence is an indication of how often the rule has been found to be true.\n\nThe \'\'confidence\'\' value of a rule, <math>X \\Rightarrow Y</math> , with respect to a set of transactions <math>T</math>, is the proportion of the transactions that contains <math>X</math> which also contains <math>Y</math>.\n\nConfidence is defined as:\n\n<math>\\mathrm{conf}(X \\Rightarrow Y) = \\mathrm{supp}(X \\cup Y) / \\mathrm{supp}(X)</math>.\n\nFor example, the rule <math>\\{\\mathrm{butter,  bread}\\} \\Rightarrow \\{\\mathrm{milk}\\}</math> has a confidence of <math>0.2/0.2=1.0</math> in the database, which means that for 100% of the transactions containing butter and bread the rule is correct (100% of the times a customer buys butter and bread, milk is bought as well).\n\nNote that <math>\\mathrm{supp}(X \\cup Y)</math> means the support of the union of the items in X and Y. This is somewhat confusing since we normally think in terms of probabilities of [[Event (probability theory)|events]] and not sets of items. We can rewrite <math>\\mathrm{supp}(X \\cup Y)</math> as the joint probability <math>P(E_X \\cup E_Y)</math>, where <math>E_X</math> and <math>E_Y</math> are the events that a transaction contains itemset <math>X</math> or <math>Y</math>, respectively.<ref name="michael.hahsler.net">Michael Hahsler (2015).  A Probabilistic Comparison of Commonly Used Interest Measures for Association Rules. http://michael.hahsler.net/research/association_rules/measures.html</ref>\n\nThus confidence can be interpreted as an estimate of the conditional probability <math>P(E_Y | E_X)</math>, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.<ref name=":0" /><ref name="hipp">{{Cite journal | last1 = Hipp | first1 = J. | last2 = Güntzer | first2 = U. | last3 = Nakhaeizadeh | first3 = G. | title = Algorithms for association rule mining --- a general survey and comparison | doi = 10.1145/360402.360421 | journal = ACM SIGKDD Explorations Newsletter | volume = 2 | pages = 58 | year = 2000 | pmid =  | pmc = }}</ref>\n\n=== Lift ===\nThe \'\'[[lift (data mining)|lift]]\'\' of a rule is defined as:\n\n<math> \\mathrm{lift}(X\\Rightarrow Y) = \\frac{ \\mathrm{supp}(X \\cup Y)}{ \\mathrm{supp}(X) \\times \\mathrm{supp}(Y) } </math>\n\nor the ratio of the observed support to that expected if X and Y were [[Independence (probability theory)|independent]].{{citation needed|reason=I couldn\'t find this in \'Witten: Data Mining - Practical Machine Learning Tools and Techniques\'|date=May 2016}}\n\nFor example, the rule <math>\\{\\mathrm{milk, bread}\\} \\Rightarrow \\{\\mathrm{butter}\\}</math> has a lift of <math>\\frac{0.2}{0.4 \\times 0.4} = 1.25 </math>.\n\nIf the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.\n\nIf the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.\n\nThe value of lift is that it considers both the confidence of the rule and the overall data set.<ref name=":0" />\n\n=== Conviction ===\nThe \'\'conviction\'\' of a rule is defined as <math> \\mathrm{conv}(X\\Rightarrow Y) =\\frac{ 1 - \\mathrm{supp}(Y) }{ 1 - \\mathrm{conf}(X\\Rightarrow Y)}</math>.\n\nFor example, the rule <math>\\{\\mathrm{milk, bread}\\} \\Rightarrow \\{\\mathrm{butter}\\}</math> has a conviction of <math>\\frac{1 - 0.4}{1 - 0.5} = 1.2 </math>, and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule <math>\\{\\mathrm{milk, bread}\\} \\Rightarrow \\{\\mathrm{butter}\\}</math> would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.\n\n== Process ==\n[[File:FrequentItems.png|thumb|Frequent itemset lattice, where the color of the box indicates how many transactions contain the combination of items. Note that lower levels of the lattice can contain at most the minimum number of their parents\' items; e.g. {ac} can have only at most <math>min(a,c)</math> items. This is called the \'\'downward-closure property\'\'.<ref name="mining" />]] Association rules are usually required to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Association rule generation is usually split up into two separate steps:\n# A minimum support threshold is applied to find all \'\'frequent itemsets\'\' in a database.\n# A minimum confidence constraint is applied to these frequent itemsets in order to form rules.\nWhile the second step is straightforward, the first step needs more attention.\n\nFinding all frequent itemsets in a database is difficult since it involves searching all possible itemsets (item combinations).  The set of possible itemsets is the [[power set]] over <math>I</math> and has size <math>2^n-1</math> (excluding the empty set which is not a valid itemset). Although the size of the power-set grows exponentially in the number of items <math>n</math> in <math>I</math>, efficient search is possible using the \'\'\'\'\'downward-closure property\'\'\'\'\' of support<ref name="mining" /><ref>{{cite book |last1=Tan |first1=Pang-Ning |last2=Michael |first2=Steinbach |last3=Kumar |first3=Vipin |title=Introduction to Data Mining |publisher=[[Addison-Wesley]] |year=2005 |isbn=0-321-32136-7 |chapter=Chapter 6. Association Analysis: Basic Concepts and Algorithms |chapterurl=http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf }}</ref> (also called \'\'anti-monotonicity\'\'<ref name="pei">Pei, Jian; Han, Jiawei; and Lakshmanan, Laks V. S.; \'\'Mining frequent itemsets with convertible constraints\'\', in \'\'Proceedings of the 17th International Conference on Data Engineering, April 2–6, 2001, Heidelberg, Germany\'\', 2001, pages 433-442</ref>) which guarantees that for a frequent itemset, all its subsets are also frequent and thus no infrequent itemset can be a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori<ref name="apriori">Agrawal, Rakesh; and Srikant, Ramakrishnan; [http://rakesh.agrawal-family.com/papers/vldb94apriori.pdf \'\'Fast algorithms for mining association rules in large databases\'\'], in Bocca, Jorge B.; Jarke, Matthias; and Zaniolo, Carlo; editors, \'\'Proceedings of the 20th International Conference on Very Large Data Bases (VLDB), Santiago, Chile, September 1994\'\', pages 487-499</ref> and Eclat<ref name="eclat">{{Cite journal | last1 = Zaki | first1 = M. J. | title = Scalable algorithms for association mining | doi = 10.1109/69.846291 | journal = IEEE Transactions on Knowledge and Data Engineering | volume = 12 | issue = 3 | pages = 372–390 | year = 2000 | pmid =  | pmc = }}</ref>) can find all frequent itemsets.\n\n==History==\nThe concept of association rules was popularised particularly due to the 1993 article of Agrawal et al.,<ref name="mining" /> which has acquired more than 18,000 citations according to Google Scholar, as of August 2015, and is thus one of the most cited papers in the Data Mining field. However, it is possible that what is now called "association rules" is similar to what appears in  the 1966 paper<ref name="guha_oldest">Hájek, Petr; Havel, Ivan; Chytil, Metoděj; \'\'The GUHA method of automatic hypotheses determination\'\', Computing 1 (1966) 293-308</ref> on GUHA, a general data mining method developed by [[Petr Hájek]] et al.<ref name="pospaper">Hájek, Petr; Feglar, Tomas; Rauch, Jan; and Coufal, David; \'\'The GUHA method, data preprocessing and mining\'\', Database Support for Data Mining Applications, Springer, 2004, ISBN 978-3-540-22479-2</ref>\n\nAn early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with <math>\\mathrm{supp}(X)</math> and <math>\\mathrm{conf}(X \\Rightarrow Y)</math> greater than user defined constraints.<ref>{{cite journal|last1=Webb|first1=Geoffrey|title=A Machine Learning Approach to Student Modelling|journal=Proceedings of the Third Australian Joint Conference on Artificial Intelligence (AI 89)|date=1989|pages=195–205}}</ref>\n\n== Alternative measures of interestingness ==\n<!-- would be nice to explain each measure -->\nIn addition to confidence, other measures of \'\'interestingness\'\' for rules have been proposed. Some popular measures are:\n\n*  All-confidence<ref name="allconfidence">Omiecinski, Edward R.; \'\'Alternative interest measures for mining associations in databases\'\', IEEE Transactions on Knowledge and Data Engineering, 15(1):57-69, Jan/Feb 2003</ref>\n* Collective strength<ref name="collectivestrength">Aggarwal, Charu C.; and Yu, Philip S.; \'\'A new framework for itemset generation\'\', in \'\'PODS 98, Symposium on Principles of Database Systems, Seattle, WA, USA, 1998\'\', pages 18-24</ref>\n*  Conviction<ref name="brin-dynamic-itemset1">Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; \'\'Dynamic itemset counting and implication rules for market basket data\'\', in \'\'SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997\'\', pp. 255-264</ref>\n*  Leverage<ref name="leverage">Piatetsky-Shapiro, Gregory; \'\'Discovery, analysis, and presentation of strong rules\'\', Knowledge Discovery in Databases, 1991, pp. 229-248</ref>\n*  Lift (originally called interest)<ref name="brin-dynamic-itemset2">Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; \'\'Dynamic itemset counting and implication rules for market basket data\'\', in \'\'SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997\'\', pp. 265-276</ref>\n\nSeveral more measures are presented and compared by Tan et al.<ref name="measurescomp">Tan, Pang-Ning; Kumar, Vipin; and Srivastava, Jaideep; \'\'Selecting the right objective measure for association analysis\'\', Information Systems, 29(4):293-313, 2004</ref> and by Hahsler.<ref name="michael.hahsler.net"/> Looking for techniques that can model what the user has known  (and using these models as interestingness measures) is currently an active research trend under the name of "Subjective Interestingness."\n\n== Statistically sound associations ==\n\nOne limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations.  These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance.  For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side.  There are approximately 1,000,000,000,000 such rules.  If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association.  If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules.  Statistically sound association discovery<ref>Webb, Geoffrey I. (2007); \'\'Discovering Significant Patterns\'\', Machine Learning 68(1), Netherlands: Springer, pp. 1-33 [http://link.springer.com/article/10.1007%2Fs10994-007-5006-x online access]</ref><ref>Gionis, Aristides; [[Heikki Mannila|Mannila, Heikki]]; Mielikäinen, Taneli; and Tsaparas, Panayiotis; \'\'Assessing Data Mining Results via Swap Randomization\'\', ACM Transactions on Knowledge Discovery from Data (TKDD), Volume 1, Issue 3 (December 2007), Article No. 14</ref> controls this risk, in most cases reducing the risk of finding \'\'any\'\' spurious associations to a user-specified significance levels.\n\n== Algorithms ==\n\nMany algorithms for generating association rules were presented over time.\n\nSome well-known algorithms are [[Apriori algorithm|Apriori]], Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.\n\n=== Apriori algorithm ===\n{{Main article|Apriori algorithm}}\n\nApriori<ref name="apriori" /> uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.\n\n=== Eclat algorithm ===\n\nEclat<ref name="eclat" /> (alt. ECLAT, stands for Equivalence Class Transformation) is a depth-first search algorithm using set intersection. It is a naturally elegant algorithm suitable for both sequential as well as parallel execution with locality-enhancing properties. It was first introduced by Zaki, Parthasarathy, Li and Ogihara in a series of papers written in 1997.\n\nMohammed Javeed Zaki, Srinivasan Parthasarathy, M. Ogihara, Wei Li:\nNew Algorithms for Fast Discovery of Association Rules. KDD 1997.\n\nMohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, Wei Li:\nParallel Algorithms for Discovery of Association Rules. Data Min. Knowl. Discov. 1(4): 343-373 (1997)\n\n=== FP-growth algorithm ===\n\nFP stands for frequent pattern.<ref>{{cite journal|last1=Han|title=Mining Frequent Patterns Without Candidate Generation|journal=Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data|date=2000|volume=SIGMOD \'00|pages=1–12|doi=10.1145/342009.335372}}</ref>\n\nIn the first pass, the algorithm counts occurrence of items (attribute-value pairs) in the dataset, and stores them to \'header table\'. In the second pass, it builds the FP-tree structure by inserting instances.\nItems in each instance have to be sorted by descending order of their frequency in the dataset, so that the tree can be processed quickly.\nItems in each instance that do not meet minimum coverage threshold are discarded.\nIf many instances share most frequent items, FP-tree provides high compression close to tree root.\n\nRecursive processing of this compressed version of main dataset grows large item sets directly, instead of generating candidate items and testing them against the entire database.\nGrowth starts from the bottom of the header table (having longest branches), by finding all instances matching given condition.\nNew tree is created, with counts projected from the original tree corresponding to the set of instances that are conditional on the attribute, with each node getting sum of its children counts.\nRecursive growth ends when no individual items conditional on the attribute meet minimum support threshold, and processing continues on the remaining header items of the original FP-tree.\n\nOnce the recursive process has completed, all large item sets with minimum coverage have been found, and association rule creation begins.<ref>Witten, Frank, Hall: Data mining practical machine learning tools and techniques, 3rd edition</ref>\n\n=== Others ===\n\n==== AprioriDP ====\nAprioriDP<ref name="dharmesh2013" /> utilizes [[Dynamic Programming]]  in Frequent itemset mining. The working principle is to eliminate the candidate generation like FP-tree, but it stores support count in specialized data structure instead of tree.\n\n==== Context Based Association Rule Mining Algorithm ====\n{{Main article|Context Based Association Rules}}\n\nCBPNARM is an algorithm, developed in 2013, to mine association rules on the basis of context. It uses context variable on the basis of which the support of an itemset is changed on the basis of which the rules are finally populated to the rule set.\n\n==== Node-set-based algorithms ====\nFIN,<ref name="deng2014" /> PrePost <ref name="deng2012" /> and PPV <ref name="deng2010" /> are three algorithms based on node sets. They use nodes in a coding FP-tree to represent itemsets, and employ a depth-first search strategy to discovery frequent itemsets using "intersection" of node sets.\n\n==== GUHA procedure ASSOC ====\n\n[[GUHA]] is a general method for exploratory data analysis that has theoretical foundations in [[observational calculi]].<ref name="ObservationalCalculi">Rauch, Jan; \'\'Logical calculi for knowledge discovery in databases\'\', in \'\'Proceedings of the First European Symposium on Principles of Data Mining and Knowledge Discovery\'\', Springer, 1997, pp. 47-57</ref>\n\nThe ASSOC procedure<ref>{{cite book |last=Hájek |first=Petr |author2=Havránek, Tomáš |title=Mechanizing Hypothesis Formation: Mathematical Foundations for a General Theory |publisher=Springer-Verlag |year=1978 |isbn=3-540-08738-9 |url=http://www.cs.cas.cz/hajek/guhabook/ }}</ref> is a GUHA method which mines for generalized association rules using fast [[bitstring]]s operations. The association rules mined by this method are more general than those output by apriori, for example "items" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.\n\n==== OPUS search ====\n\nOPUS is an efficient algorithm for rule discovery    that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support.<ref name=OPUS>Webb, Geoffrey I. (1995); \'\'OPUS: An Efficient Admissible Algorithm for Unordered Search\'\', Journal of Artificial Intelligence Research 3, Menlo Park, CA: AAAI Press, pp. 431-465 [http://www.cs.washington.edu/research/jair/abstracts/webb95a.html online access]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> Initially used to find rules for a fixed consequent<ref name="OPUS" /><ref name="Bayardo">{{Cite journal |doi=10.1023/A:1009895914772 |last1=Bayardo |first1=Roberto J., Jr. |last2=Agrawal |first2=Rakesh |last3=Gunopulos |first3=Dimitrios |year=2000 |title=Constraint-based rule mining in large, dense databases |journal=Data Mining and Knowledge Discovery |volume=4 |issue=2 |pages=217–240 }}</ref> it has subsequently been extended to find rules with any item as a consequent.<ref name="webb">Webb, Geoffrey I. (2000); \'\'Efficient Search for Association Rules\'\', in Ramakrishnan, Raghu; and Stolfo, Sal; eds.; \'\'Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2000), Boston, MA\'\', New York, NY: The Association for Computing Machinery, pp. 99-107 [http://www.csse.monash.edu/~webb/Files/Webb00b.pdf online access]</ref> OPUS search is the core technology in the popular Magnum Opus association discovery system.\n\n== Lore ==\nA famous story about association rule mining is the "beer and diaper" story.  A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true.<ref name="dss">http://www.dssresources.com/newsletters/66.php</ref> Daniel Powers says:<ref name="dss" />\n\n<blockquote>In 1992, Thomas Blischok, manager of a retail consulting group at [[Teradata]], and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis "did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.</blockquote>\n\n== Other types of association mining ==\n\n\'\'\'Multi-Relation Association Rules\'\'\': Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations \'\'live in\'\', \'\'nearby\'\' and \'\'humid\'\': “Those who \'\'live in\'\' a place which is \'\'near by\'\' a city with \'\'humid\'\' climate type and also are \'\'younger\'\' than 20 -> their \'\'health condition\'\' is good”. Such association rules are extractable from RDBMS data or semantic web data.<ref name="MRAR: Mining Multi-Relation Association Rules">Ramezani, Reza, Mohamad Saraee, and Mohammad Ali Nematbakhsh; \'\'MRAR: Mining Multi-Relation Association Rules\'\', Journal of Computing and Security, 1, no. 2 (2014)</ref>\n\n\'\'\'[[Context Based Association Rules]]\'\'\' is a form of association rule. \'\'\'Context Based Association Rules\'\'\' claims more accuracy in association rule mining by considering a hidden variable named context variable which changes the final set of association rules depending upon the value of context variables. For example the baskets orientation in market basket analysis reflects an odd pattern in the early days of month.This might be because of abnormal context i.e. salary is drawn at the start of the month <ref name="Context Based Positive and Negative Spatio Temporal Association Rule Mining">Shaheen, M; Shahbaz, M; and Guergachi, A; \'\'Context Based Positive and Negative Spatio Temporal Association Rule Mining\'\', Elsevier Knowledge-Based Systems, Jan 2013, pp. 261-273</ref>\n\n\'\'\'[[Contrast set learning]]\'\'\' is a form of associative learning. \'\'\'Contrast set learners\'\'\' use rules that differ meaningfully in their distribution across subsets.<ref name="webb03">{{cite conference\n | author = GI Webb and S. Butler and D. Newlands\n | year = 2003\n | title = On Detecting Differences Between Groups\n | conference = KDD\'03 Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n| url= http://portal.acm.org/citation.cfm?id=956781\n }}\n</ref><ref name="busy">Menzies, Tim; and Hu, Ying; \'\'Data Mining for Very Busy People\'\', IEEE Computer, October 2003, pp. 18-25</ref>\n\n\'\'\'Weighted class learning\'\'\' is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.\n\n\'\'\'High-order pattern discovery\'\'\' facilitate the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.\n<ref name="discovere">{{cite journal |last=Wong |first=Andrew K.C. |author2=Wang, Yang |title=High-order pattern discovery from discrete-valued data |journal=IEEE Transactions on Knowledge and Data Engineering (TKDE) |year=1997 |pages=877–893 }}</ref>\n\n\'\'\'[[K-optimal pattern discovery]]\'\'\' provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data.\n\n\'\'\'Approximate Frequent Itemset\'\'\' mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.<ref>Jinze Liu, Susan Paulsen, Xing Sun, Wei Wang, Andrew Nobel, J. P. (2006). Mining approximate frequent itemsets in the presence of noise: Algorithm and analysis. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.3805{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n\n\'\'\'Generalized Association Rules\'\'\' hierarchical taxonomy (concept hierarchy)\n\n\'\'\'Quantitative Association Rules\'\'\' categorical and quantitative data\n<ref name="quantminer">{{cite journal |last=Salleb-Aouissi |first=Ansaf |author2=Vrain, Christel|author3= Nortet, Cyril |title=QuantMiner: A Genetic Algorithm for Mining Quantitative Association Rules |journal=International Joint Conference on Artificial Intelligence (IJCAI) |year=2007 |pages=1035–1040 }}</ref>\n\n\'\'\'Interval Data Association Rules\'\'\' e.g. partition the age into 5-year-increment ranged\n\n\'\'\'Maximal Association Rules\'\'\'\n\n\'\'\'Sequential pattern mining \'\'\' discovers subsequences that are common to more than minsup sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.<ref name="sequence">Zaki, Mohammed J. (2001); \'\'SPADE: An Efficient Algorithm for Mining Frequent Sequences\'\', Machine Learning Journal, 42, pp. 31–60</ref>\n\n\'\'\'Sequential Rules\'\'\' discovering relationships between items while considering the time ordering. It is generally applied on a sequence database. For example, a sequential rule found in database of sequences of customer transactions can be that customers who bought a computer and CD-Roms, later bought a webcam, with a given confidence and support.\n\n\'\'\'Subspace Clustering\'\'\', a specific type of [[Clustering high-dimensional data]], is in many variants also based on the downward-closure property for specific clustering models.<ref name="ZimekAssent2014">{{cite journal|last1=Zimek|first1=Arthur|last2=Assent|first2=Ira|last3=Vreeken|first3=Jilles|title=Frequent Pattern Mining Algorithms for Data Clustering|year=2014|pages=403–423|doi=10.1007/978-3-319-07821-2_16}}</ref>\n\n\'\'\'Warmr \'\'\'is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.<ref>{{cite journal | pmid = 11272703 | volume=15 | issue=2 | title=Warmr: a data mining tool for chemical data. | date=Feb 2001 | journal=J Comput Aided Mol Des | pages=173–81}}</ref>\n\n==See also==\n* [[Sequence mining]]\n* [[Production system (computer science)]]\n* [[Learning classifier system]]\n* [[Rule-based machine learning]]\n\n==References==\n{{reflist|3|refs=\n<ref name="deng2014">Z. H. Deng and S. L. Lv. Fast mining frequent itemsets using Nodesets.[http://www.sciencedirect.com/science/article/pii/S0957417414000463]. Expert Systems with Applications, 41(10): 4505–4512, 2014.</ref>\n<ref name="deng2012">Z. H. Deng, Z. Wang，and J. Jiang. A New Algorithm for Fast Mining Frequent Itemsets Using N-Lists [http://info.scichina.com:8084/sciFe/EN/abstract/abstract508369.shtml]. SCIENCE CHINA Information Sciences, 55 (9): 2008 - 2030, 2012.</ref>\n<ref name="deng2010">Z. H. Deng and Z. Wang.   A New Fast Vertical Method for Mining Frequent Patterns [http://www.tandfonline.com/doi/abs/10.1080/18756891.2010.9727736]. International Journal of Computational Intelligence Systems, 3(6): 733 - 744, 2010.</ref>\n<ref name="dharmesh2013">D. Bhalodiya, K. M. Patel and C. Patel. An Efficient way to Find Frequent Pattern with Dynamic Programming Approach [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6780102&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6780102]. NIRMA UNIVERSITY INTERNATIONAL CONFERENCE ON ENGINEERING, NUiCONE-2013, 28-30 NOVEMBER, 2013.</ref>\n}}\n\n==External links==\n\n===Bibliographies===\n* [http://www.uco.es/grupos/kdis/ARMBibliography Extensive Bibliography on Association Rules] by J.M. Luna\n* [http://michael.hahsler.net/research/bib/association_rules/ Annotated Bibliography on Association Rules] by M. Hahsler\n* [http://www.statsoft.com/textbook/association-rules/ Statsoft Electronic Statistics Textbook: Association Rules] by [[Dell]] Software\n\n{{Prone to spam|date=February 2016}}\n{{Z148}}<!--     {{No more links}}\n\n       Please be cautious adding more external links.\n\nWikipedia is not a collection of links and should not be used for advertising.\n\n     Excessive or inappropriate links will be removed.\n\n See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.\n\nIf there are already suitable links, propose additions or replacements on\nthe article\'s talk page, or submit your link to the relevant category at\nDMOZ (dmoz.org) and link there using {{Dmoz}}.\n\n-->\n\n{{DEFAULTSORT:Association Rule Learning}}\n[[Category:Data management]]\n[[Category:Data mining]]']
['Novell File Reporter', '28208309', '{{Infobox software\n|name                       = Novell File Reporter\n|logo                       =\n|screenshot                 =\n|caption                    =\n|collapsible                =\n|author                     =\n|developer                  = [[Novell]]\n|released                   = {{Start date|2010|01}}\n|discontinued               =\n|latest release version     = 2.6.1\n|latest release date        = {{Start date|2015|10|02}}\n|latest preview version     =\n|latest preview date        = <!-- {{Start date|YYYY|MM|DD}} -->\n|frequently updated         =\n|programming language       =\n|operating system           =\n|platform                   =\n|size                       =\n|language                   =\n|status                     =\n|genre                      = [[System Software]]\n|license                    =\n|website                    = [http://www.novell.com/products/file-reporter/ Novell File Reporter]\n}}\n\n\'\'\'Novell File Reporter\'\'\' (a.k.a. \'\'\'NFR\'\'\') is software that allows network administrators to identify files stored on the network and generates reports regarding the size of individual files, [[File format | file type]], when files were last accessed, and where duplicates exist. Additionally, the File Reporter tracks storage volume capacity and usage. It is a component of the [[Novell File Management Suite]].\n\n==How It Works==\n\nNovell File Reporter examines and reports on terabytes of data via a central reporting engine (NFR Engine) and distributed agents (NFR Agents). <ref>{{Citation| title = Novell File Reporter Reports on Terabytes of Data | url= http://www.novell.com/products/file-reporter/terabytes_data.html | accessdate = 31 July 2010}}</ref> The NFR Engine schedules the scans of file instances conducted by NFR Agents, processes and compiles the scans for reporting purposes, and provides report information to the user interface. \n\nIn addition to the standard reports <ref>{{Citation| title = Novell File Report Standard Reports | url= http://www.novell.com/products/file-reporter/standard_reports.html | accessdate = 31 July 2010}}</ref> it can generate, the NFR Engine can also produce "trigger reports" in response to specific events (a server volume crossing a capacity threshold, for example). Accordingly, the NFR Engine monitors the data gathered by the NFR Agents in order to identify these "triggers."\n\nThe NFR Engine when working in either [[Novell eDirectory | eDirectory]] or [[Active Directory]] connects to the directory via a Directory Services Interface (DSI) and thus can monitor and check file permissions.<ref>{{Citation | last = Huber | first= Matthias | journal= Linux Magazine | title= Novell File Management Suite Optimizes Storage | date= 25 January 2010| url=http://www.linux-magazine.com/Online/News/Novell-File-Management-Suite-Optimizes-Storage | accessdate= 31 July 2010}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.novell.com/products/file-reporter/technicalinfo/ Novell File Reporter: Product page] Overview, features, and technical information\n*[http://www.novell.com/documentation/filereporter2/ Novell File Reporter: Documentation]\n*[http://www.filereportersupport.com/nfr/ Novell File Reporter: Support]\n\n{{Novell}}\n\n[[Category:Novell]]\n[[Category:Novell software]]\n[[Category:Storage software]]\n[[Category:Data management]]']
['Document capture software', '23392007', '{{primary sources|date=August 2009}}\n\'\'\'Document Capture Software\'\'\' refers to applications that provide the ability and feature set to automate the process of [[Image scanner|scanning]] paper documents. Most scanning [[Personal computer hardware|hardware]], both scanners and [[copier]]s, provides the basic ability to scan to any number of [[image file formats]], including: [[PDF]], [[TIFF]], [[JPG]], [[BMP file format|BMP]], etc. This basic functionality is augmented by document capture software, which can add efficiency and standardization to the process.\n\n==Typical features==\nTypical features of Document Capture Software include:\n* [[Barcode]] recognition\n* Patch Code recognition\n* Separation\n* [[Optical character recognition|Optical Character Recognition (OCR)]]\n* [[Optical mark recognition|Optical Mark Recognition (OMR)]]\n* Quality Assurance\n* Indexing\n* Migration\n\n===Goal for Implementation of a Document Capture Solution===\nThe goal for implementing a document capture solution is to reduce the amount of time spent in the scanning and capture process, and produce metadata along with an image file, and/or OCR text. This information is then migrated to a [[Document management system|Document Management]] or [[Enterprise content management|Enterprise Content Management]] system. These systems often provide a search function, allowing search of the assets based on the produced [[metadata]], and then viewed using [[document imaging]] software.\n\n== Document Capture System Solutions - General ==\n\n===Integration with Document Management System===\n{{main|Enterprise content management}}\nECM (Enterprise Content management) and their DMS component (Document Management System) are being adopted by many organizations as a corporate document management system for all types of electronic files, e.g. MS word, PDF ... However, much of the information held by organisations is on paper and this needs to be integrated within the same document repository.\n\nBy converting paper documents into digital format through scanning companies can convert paper into image formats such as TIF and JPG and also extract valuable index information or business data from the document using OCR technology. Digital documents and associated metadata can easily be stored in the ECM in a variety of formats. The most popular of these formats is PDF which not only provides an accurate representation of the document but also allows all the OCR text in the document to be stored behind the PDF image. This format is known as PDF with hidden text or text-searchable PDF. This allows users to search for documents by using keywords in the metadata fields or by searching the content of PDF files across the repository.\n\n====Advantages of scanning documents into a ECM/DMS====\n\nInformation held on paper is usually just as valuable to organisations as the electronic documents that are generated internally. Often this information represents a large proportion of the day to day correspondence with suppliers and customers. Having the ability to manage and share this information internally through a document management system such as [[SharePoint]] can improve collaboration between departments or employees and also eliminate the risk of losing this information through disasters such as floods or fire.\n\nOrganisations adopting an ECM/DMS  often implement electronic workflow which allows the information held on paper to be included as part of an electronic business process and incorporated into a customer record file along with other associated office documents and emails.\nFor business critical documents, such as purchase orders and supplier invoices, digitising documents can help speed up business transactions as well as reduce manual effort involved in keying data into business systems, such as CRM, ERP and Accounting. Scanned invoices can also be routed to managers for payment approval via email or an electronic workflow.\n\n==Distributed Capture Solutions==\n\nDistributed document capture is a technology which allows the scanning of documents into a central server through the use of individual capture stations. A variation of distributed capture is thin-client document capture in which documents are scanned into a central server through the use of web browser.  One of these web-based products was reviewed by AIIM.  They said, "(this product) is a thin-client distributed capture system that streamlines the process of acquiring and creating documents."<ref>Association for Information and Image Management [http://www.aiim.org/community/product-guide/Capture/Prevalent-Software-Quillix "Prevalent Software - Quillix"], accessed August 29, 2011.</ref>  The streamlining is a result of several factors including the lack of software which needs to be installed at every scanning station and the variety of input sources from which documents can be captured.  This includes things like email, fax, or a watched folder.\n\nJeff Shuey, Director of Business Development at Kodak, makes a distinction between distributed capture and what he calls "remote" capture.  In an article publishing in [[AIIM]], he said that the key difference between the two is whether or not the information that is captured from scanning needs to be sent to the centralized server. If, as he points out in his article, the document just needs to be scanned and committed to a [[SharePoint]] system and doesn\'t need to be sent to some other centralized server, this is just a remote capture situation.<ref>Association for Information and Image Management [http://www.aiim.org/community/blogs/expert/Remote-or-Distributed-Scanning-Are-they-Different "Remote or Distributed Scanning - Are They Different?"], accessed August 29, 2011.</ref>\n\nThere are Document Capture Software comparisons available, featuring some of the most relevant products (EMC Captiva, IBM Datacap, Artsyl Technologies or Ephesoft) and extracting performance facts and their most relevant features.\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Document Capture Software}}\n[[Category:Artificial intelligence applications]]\n[[Category:Optical character recognition]]\n[[Category:Data management]]\n[[Category:SharePoint]]']
['Learning object metadata', '1955471', '[[Image:LOM base schema.svg|340px|right|thumb|A schematic representation of the hierarchy of elements in the LOM data model]]\n\n\'\'\'Learning Object Metadata\'\'\' is a data model, usually encoded in XML, used to describe a [[learning object]] and similar digital resources used to support learning. The purpose of learning object metadata is to support the reusability of learning objects, to aid discoverability, and to facilitate their interoperability, usually in the context of online [[learning management systems]] (LMS).\n\nThe IEEE 1484.12.1 – 2002 Standard for Learning Object Metadata is an internationally recognised open standard (published by the [[Institute of Electrical and Electronics Engineers]] Standards Association, New York) for the description of “[[learning object]]s”. Relevant attributes of learning objects to be described include: type of object; author; owner; terms of distribution; format; and [[pedagogy|pedagogical]] attributes, such as teaching or interaction style.\n\n== IEEE 1484.12.1 – 2002 Standard for Learning Object Metadata ==\n\n=== In brief ===\nThe IEEE working group that developed the standard defined learning objects, \'\'for the purposes of the standard,\'\' as being “any entity, digital or non-digital, that may be used for learning, education or training." This definition has struck many commentators as being rather broad in its scope, but the definition was intended to provide a broad class of objects to which LOM metadata might usefully be associated rather than to give an instructional or pedagogic definition of a learning object. \'\'IEEE 1484.12.1\'\' is the first part of a multipart standard, and describes the LOM data model. The LOM data model specifies which aspects of a learning object should be described and what vocabularies may be used for these descriptions; it also defines how this data model can be amended by additions or constraints. Other parts of the standard are being drafted to define bindings of the LOM data model, i.e. define how LOM records should be represented in [[XML]] and [[Resource Description Framework|RDF]] (\'\'IEEE 1484.12.3\'\' and \'\'IEEE 1484.12.4\'\' respectively). This article focuses on the LOM data model rather than issues relating to XML or other bindings.\n\nIMS Global Learning Consortium is an international consortium that contributed to the drafting of the IEEE Learning Object Metadata (together with the ARIADNE Foundation) and endorsed early drafts of the data model as part of the IMS Learning Resource Meta-data specification (IMS LRM, versions 1.0 – 1.2.2). Feedback and suggestions from the implementers of IMS LRM fed into the further development of the LOM, resulting in some drift between version 1.2 of the IMS LRM specification and what was finally published at the LOM standard. Version 1.3 of the IMS LRM specification realigns the IMS LRM data model with the IEEE LOM data model and specifies that the IEEE XML binding should be used. Thus, we can now use the term \'LOM\' in referring to both the IEEE standard and version 1.3 of the IMS specification. The IMS LRM specification also provides an extensive \'\'Best Practice and Implementation Guide\'\', and an \'\'XSL transform\'\' that can be used to migrate metadata instances from the older versions of the IMS LRM XML binding to the IEEE LOM XML binding.\n\n== Technical details ==\n\n=== How the data model works ===\nThe LOM comprises a \'\'\'hierarchy of elements\'\'\'<!--, as shown in the diagram (top right)-->. At the first level, there are nine categories, each of which contains sub-elements; these sub-elements may be simple elements that hold data, or may themselves be aggregate elements, which contain further sub-elements. The semantics of an element are determined by its context: they are affected by the parent or container element in the hierarchy and by other elements in the same container. For example, the various \'\'Description\'\' elements (1.4, 5.10, 6.3, 7.2.2, 8.3 and 9.3) each derive their context from their parent element. In addition, description element 9.3 also takes its context from the value of element 9.1 \'\'Purpose\'\' in the same instance of \'\'Classification\'\'.\n\nThe data model specifies that some elements may be repeated either individually or as a group; for example, although the elements 9.2 (\'\'Description\'\') and 9.1 (\'\'Purpose\'\') can only occur once within each instance of the \'\'Classification\'\' container element, the \'\'Classification\'\' element may be repeated - thus allowing many descriptions for different purposes.\n\nThe data model also specifies the \'\'\'value space\'\'\' and \'\'\'datatype\'\'\' for each of the simple data elements. The value space defines the restrictions, if any, on the data that can be entered for that element. For many elements, the value space allows any string of [[Unicode]] character to be entered, whereas other elements entries must be drawn from a declared list (i.e. a [[controlled vocabulary]]) or must be in a specified format (e.g. date and language codes). Some element datatypes simply allow a string of characters to be entered, and others comprise two parts, as described below:\n* \'\'\'LangString\'\'\' items contain Language and String parts, allowing the same information to be recorded in multiple languages\n* \'\'\'Vocabulary\'\'\' items are constrained in such a way that their entries have to be chosen from a controlled list of terms - composed of Source-Value pairs - with the Source containing the name of the list of terms being used and the Value containing the chosen term\n* \'\'\'DateTime\'\'\' and \'\'\'Duration\'\'\' items contain one part that allows the date or duration to be given in a machine readable format, and a second that allows a description of the date or duration (for example “mid summer, 1968”).\n\nWhen implementing the LOM as a data or service provider, it is not necessary to support all the elements in the data model, nor need the LOM data model limit the information which may be provided. The creation of an [[application profile]] allows a community of users to specify which elements and vocabularies they will use. Elements from the LOM may be dropped and elements from other metadata schemas may be brought in; likewise, the vocabularies in the LOM may be supplemented with values appropriate to that community.\n\n=== Requirements ===\nThe key requirements for exploiting the LOM as a data or service provider are to:\n* Understand user/community needs and to express these as an application profile\n* Have a strategy for creating high quality metadata\n* Store this metadata in a form which can be exported as LOM records\n* Agree a binding for LOM instances when they are exchanged\n* Be able to exchange records with other systems either as single instances or \'\'en masse\'\'.\n\n=== Related specifications ===\nThere are many metadata specifications; of particular interest is the [[Dublin Core]] Metadata Element Set (commonly known as Simple Dublin Core, standardised as \'\'ANSI/NISO Z39.85 – 2001\'\'). Simple Dublin Core (DC) provides a non-complex, loosely defined set of elements which is useful for sharing metadata across a wide range of disparate services. Since the LOM standard used Dublin Core as a starting point, refining the Simple DC schema with qualifiers relevant to learning objects, there is some overlap between the LOM and DC standards.<ref>{{cite book|last1=Miller|first1=Steven J.|title=Metadata for Digital Collections: A How-To-Do-It Manual|date=2011|publisher=ALA Neal-Schuman|location=Chicago|isbn=978-1-55570-746-0|pages=56}}</ref> The Dublin Core Metadata Initiative is also working on a set of terms which allow the Dublin Core Element Set to be used with greater semantic precision (Qualified Dublin Core). The Dublin Education Working Group aims to provide refinements of [[Dublin Core]] for the specific needs of the education community.\n\nMany other education-related specifications allow for LO metadata to be embedded within XML instances, such as: describing the resources in an IMS Content Package or Resource List; describing the vocabularies and terms in an [[IMS VDEX]] (Vocabulary Definition and Exchange) file; and describing the question items in an IMS QTI (Question and Test Interoperability) file.\n\nThe [[IMS VDEX|IMS Vocabulary Definition and Exchange (VDEX) specification]] has a double relation with the LOM, since not only can the LOM provide metadata on the vocabularies in a VDEX instance, but VDEX can be used to describe the controlled vocabularies which are the value space for many LOM elements.\n\nLOM records can be transported between systems using a variety of protocols, perhaps the most widely used being [[OAI-PMH]].\n\n=== Application profiles ===\n\n==== UK LOM Core ====\nFor UK Further and Higher Education, the most relevant family of application profiles are those based around the \'\'UK LOM Core\'\'.<ref>http://zope.cetis.ac.uk/profiles/uklomcore/</ref> The UK LOM Core is currently a draft schema researched by a community of practitioners to identify common UK practice in learning object content, by comparing 12 metadata schemas. UK LOM is currently legacy work, it is not in active development.\n\n==== CanCore ====\n\'\'CanCore\'\' provides detailed guidance for the interpretation and implementation of each data element in the LOM standard.<ref name="CanCore">{{cite web | url = http://cancore.tru.ca/en/guidelines.html| title = CanCore Guidelines: Introduction | author = [[Norm Friesen]]| publisher = Athabasca University| date = 2003-01-20 | accessdate = 2009-02-23 |display-authors=etal}}</ref> These guidelines (2004) constitute a 250-page document, and have been developed over three years under the leadership of [[Norm Friesen]], and through consultation with experts across Canada and throughout the world. These guidelines are also available at no charge from the CanCore Website.\n\n==== ANZ-LOM ====\nANZ-LOM is a metadata profile developed for the education sector in Australia and New Zealand. The profile sets obligations for elements and illustrates how to apply controlled vocabularies, including example regional vocabularies used in the "classification" element. The ANZ-LOM profile was first published by The Le@rning Federation (TLF) in January, 2008.\n\n==== Vetadata ====\nThe Australian Vocational Training and Education (VET) sector uses an application profile of the IEEE LOM called Vetadata. The profile contains five mandatory elements, and makes use of a number of vocabularies specific to the Australian VET sector. This application profile was first published in 2005. The Vetadata and ANZ-LOM profiles are closely aligned.\n\n==== NORLOM ====\nNORLOM is the Norwegian LOM profile.\nThe profile is managed by NSSL (The Norwegian Secretariat for Standardization of Learning Technologies)\n\n==== ISRACore ====\nISRACORE is the Israeli LOM profile.\nThe Israel Internet Association (ISOC-IL) and Inter University Computational Center (IUCC) have teamed up to manage and establish an e-learning objects database.\n\n====SWE-LOM====\nSWE-LOM is the Swedish LOM profile that is managed by IML at [[Umeå University]] as a part of the work with the national standardization group TK450 at [[Swedish Standards Institute]].\n\n====TWLOM====\nTWLOM is the Taiwanese LOM profile that is managed by Industrial Development and Promotion of Archives and e-Learning Project\n\n====LOM-FR====\nLOM-FR is a metadata profile developed for the education sector in France. This application profile was first published in 2006.\n\n====NL LOM====\nNL LOM is the Dutch metadata profile for educational resources in the Netherlands. This application profile was the result of merging the Dutch higher education LOM profile with the one used in primary and secondary Dutch education. The final version was released in 2011.\n\n====LOM-CH====\nLOM-CH is a metadata profile developed for the education sector in Switzerland. It is currently available in French and German. This application profile was published in July 2014.\n\n====LOM-ES====\nLOM-ES is a metadata profile developed for the education sector in Spain. It is available in Spanish.\n\n====LOM-GR====\nLOM-GR, also known as "LOM-GR \'\'Photodentro\'\'" is the Greek LOM application profile for educational resources, currently being used for resources related to school education. It was published in 2012 and is currently available in Greek and English.<ref>https://git.dschool.edu.gr/photodentro/LOM-GR</ref> It is maintained by [[CTI DIOPHANTUS]] as part of the "[[Photodentro]] Federated Architecture for Educational Content for Schools" that includes a number of educational content repositories (for Learning Objects, Educational Video, and User Generated Content) and the Greek National Aggregator of Educational Content accumulating metadata from collections stored in repositories of other organizations.<ref name="Photodentro LOR">{{cite journal|last1=Megalou|first1=Elina|last2=Kaklamanis|first2=Christos|title=PHOTODENTRO LOR, THE GREEK NATIONAL LEARNING OBJECT REPOSITORY|journal=INTED2014 Proceedings|date=10–12 March 2014|pages=309–319|url=https://library.iated.org/view/MEGALOU2014PHO|accessdate=7 April 2016|series=8th International Technology, Education and Development Conference|publisher=IATED|location=Valencia, Spain|issn=2340-1079}}</ref> LOM-GR is a working specification of the TC48/WG3 working group of the [[Hellenic Organization for Standardization]].\n\n==== Others ====\nOther application profiles are those developed by the Celebrate project<ref>European Schoolnet, [http://web.archive.org/web/20071225053548/http://www.eun.org/ww/en/pub/celebrate_help/application_profile.htm CELEBRATE Application Profile] (2003).</ref> and the metadata profile that is part of the SCORM reference model.<ref>ADL, [http://www.adlnet.gov/capabilities/scorm#tab-learn SCORM].</ref>\n\n== See also ==\n* [[Application profile]]\n* [[Content package]]\n* [[Dublin Core]]\n* IMS Global\n* [[Learning object]]\n* [http://dublincore.org/dcx/lrmi-terms/1.1/ LRMI (Learning Resource Metadata Initiative)]\n* [[Metadata]]\n* [[Metadata standards|Metadata Standards]]\n* [[OAI-PMH]]\n* [[SCORM]]\n* [[XML]]\n* [[:m:Learning Object Metadata]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n{{wikiversity|Introduction to Learning Objects}}\n* [http://cancore.athabascau.ca/en/ cancore.athabascau.ca] is a thorough element-by-element guide to implementing the IEEE LOM.\n* [http://www.imsglobal.org/metadata/ www.imsglobal.org: IMS Global Learning Consortium Learning resource meta-data specification].\n* [http://ltsc.ieee.org/wg12/files/IEEE_1484_12_03_d8_submitted.pdf ltsc.ieee.org: XML Binding Specification].\n* [http://www.intrallect.com/support/metadata/ims2lom_metadata_mapping.htm www.intrallect.com: A mapping between the IEEE LOM and IMS Learning Resource Metadata]\n* [http://www.ontopia.net/topicmaps/materials/tm-vs-thesauri.html www.ontopia.net: Metadata? Thesauri? Taxonomies? Topic Maps! Making sense of it all], 2004.\n{{Prone to spam|date=October 2014}}\n{{Z148}}<!--     {{No more links}}\n\n       Please be cautious adding more external links.\n\nWikipedia is not a collection of links and should not be used for advertising.\n\n     Excessive or inappropriate links will be removed.\n\n See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.\n\nIf there are already suitable links, propose additions or replacements on\nthe article\'s talk page, or submit your link to the relevant category at\nDMOZ (dmoz.org) and link there using {{Dmoz}}.\n\n-->\n\n{{Use dmy dates|date=October 2010}}\n\n{{DEFAULTSORT:Learning Object Metadata}}\n[[Category:Data management]]\n[[Category:Educational technology]]\n[[Category:Knowledge representation]]\n[[Category:Library science]]\n[[Category:Metadata]]\n[[Category:Standards]]\n[[Category:Technical communication]]']
['Grid-oriented storage', '11962687', '{{primary sources|article|date=March 2009}}\n\n\'\'\'Grid-oriented Storage\'\'\' (\'\'\'GOS\'\'\') was a term used for data storage by a university project during the era when the term [[grid computing]] was popular.\n\n== Description ==\nGOS was a successor of the term [[network-attached storage]] (NAS). GOS systems contained hard disks, often [[RAID]]s (redundant arrays of independent disks), like traditional file servers. \n[[Image:gosongrid.jpg |thumb |upright=1.4]]\n\nGOS was designed to deal with long-distance, cross-domain and single-image file operations, which is typical in Grid environments. GOS behaves like a file server via the file-based GOS-FS protocol to any entity on the grid. Similar to [[Advanced Resource Connector|GridFTP]], GOS-FS integrates a parallel stream engine and [[Grid Security Infrastructure]] (GSI). \n\nConforming to the universal VFS (Virtual Filesystem Switch), GOS-FS can be pervasively used as an underlying platform to best utilize the increased transfer bandwidth and accelerate the [[Network File System (protocol)|NFS]]/[[CIFS]]-based applications. GOS can also run over [[SCSI]], [[Fibre Channel]] or [[iSCSI]], which does not affect the acceleration performance, offering both file level protocols and block level protocols for [[storage area network]] (SAN) from the same system.\n\nIn a grid infrastructure, resources may be geographically distant from each other, produced by differing manufacturers, and have differing access control policies. This makes access to grid resources dynamic and conditional upon local constraints. Centralized management techniques for these resources are limited in their scalability both in terms of execution efficiency and fault tolerance. Provision of services across such platforms requires a distributed resource management mechanism and the peer-to-peer clustered GOS appliances allow a single storage image to continue to expand, even if a single GOS appliance reaches its capacity limitations. The cluster shares a common, aggregate presentation of the data stored on all participating GOS appliances. Each GOS appliance manages its own internal storage space. The major benefit of this aggregation is that clustered GOS storage can be accessed by users as a single mount point. \n\nGOS products fit the thin-server categorization. Compared with traditional “fat server”-based storage architectures, thin-server GOS appliances deliver numerous advantages, such as the alleviation of potential network/grid bottle-necks, CPU and OS optimized for I/O only, ease of installation, remote management and minimal maintenance, low cost and Plug and Play, etc. Examples of similar innovations include NAS, printers, fax machines, routers and switches.\n\nAn [[Apache server]] has been installed in the GOS operating system, ensuring an HTTPS-based communication between the GOS server and an administrator via a Web browser. Remote management and monitoring makes it easy to set up, manage, and monitor GOS systems.\n\n== History ==\n[[Frank Zhigang Wang]] and Na Helian proposed a funding proposal to the UK government titled “Grid-Oriented Storage (GOS): Next Generation Data Storage System Architecture for the Grid Computing Era” in 2003. The proposal was approved and granted one million pounds{{citation needed|date=March 2009}} in 2004. The first prototype was constructed in 2005 at Centre for Grid Computing, Cambridge-Cranfield High Performance Computing Facility. The first conference presentation was at IEEE Symposium on Cluster Computing and Grid (CCGrid), 9–12 May 2005, Cardiff, UK. As one of the five best work-in-progress, it was included in the IEEE Distributed Systems Online. In 2006, the GOS architecture and its implementations was published in IEEE Transactions on Computers, titled “Grid-oriented Storage: A Single-Image, Cross-Domain, High-Bandwidth Architecture”.  \nStarting in January 2007, demonstrations were presented at [[Princeton University]], Cambridge University Computer Lab and others.\nBy 2013, the Cranfield Centre still used future tense for the project.<ref name="Cranfield CGC">{{cite web |url= http://www.cranfield.ac.uk/soe/departments/appliedmaths/gridcomputing/index.html |title= Centre for Grid Computing |accessdate= June 14, 2013 |publisher=Cranfield University}} <!--  --></ref>\n\n[[Peer-to-peer file sharing]]s use similar techniques.\n\n==Notes==\n{{reflist}}\n\n==Further reading==\n* Frank Wang, Na Helian, Sining Wu, Yuhui Deng, Yike Guo, Steve Thompson, Ian Johnson, Dave Milward & Robert Maddock, Grid-Oriented Storage, IEEE Distributed Systems Online,  Volume 6,  Issue 9, Sept. 2005.\n* Frank Wang, Sining Wu, Na Helian, Andy Parker, Yike Guo, Yuhui Deng, Vineet Khare, Grid-oriented Storage: A Single-Image, Cross-Domain, High-Bandwidth Architecture, IEEE Transaction on Computers, Vol.56, No.4, pp.&nbsp;474–487, 2007.\n* Frank Zhigang Wang, Sining Wu, Na Helian, An Underlying Data-Transporting Protocol for Accelerating Web Communications, International Journal of Computer Networks, Elsevier, 2007.\n* Frank Zhigang Wang, Sining Wu, Na Helian, Yuhui Deng, Vineet Khare, Chris Thompson and Michael Parker, Grid-based Data Access to Nucleotide Sequence Database with 6x Improvement in Response Times, New Generation Computing, No.2, Vol.25, 2007.\n* Frank Wang, Yuhui Deng, Na Helian, Evolutionary Storage: Speeding up a Magnetic Disk by Clustering Frequent Data, IEEE Transactions on Magnetics, Issue.6, Vol.43, 2007.\n* Frank Zhigang Wang, Na Helian, Sining Wu, Yuhui Deng, Vineet Khare, Chris Thompson and Michael Parker, Grid-based Storage Architecture for Accelerating Bioinformatics Computing, Journal of VLSI Signal Processing Systems, No.1, Vol.48, 2007.\n* Yuhui Deng and   Frank Wang, A Heterogeneous Storage Grid Enabled by Grid Service, ACM Operating System Review, No.1, Vol.41, 2007.\n* Yuhui Deng & Frank Wang, Optimal Clustering Size of Small File Access in Network Attached Storage Device, Parallel Processing Letters, No.1, Vol.17, 2007.\n\n{{DEFAULTSORT:Grid-Oriented Storage}}\n[[Category:Data management]]']
['Category:Storage area networks', '30304657', '{{Commons category|Storage area networks}}\n{{See also category|Computer storage buses}}\n{{cat main|Storage area network}}\n\n[[Category:Computer data storage]]\n[[Category:Local area networks]]\n[[Category:Data management]]\n[[Category:Storage virtualization]]']
['Very large database', '30864622', "{{About|Large size databases|International Conference on Very Large Databases|VLDB}}\n\nA '''very large database''', or '''VLDB''', is a database that contains an extremely high number of [[tuple]]s (database rows), or occupies an extremely large physical [[filesystem]] storage space. The most common definition of VLDB is a database that occupies more than 1 [[terabyte]] or contains several billion rows, although naturally this definition changes over time.{{Citation needed|date=March 2013}}\n\nVery large databases are often, but not necessarily, a core component in [[big data]] analysis.\n\n==References==\n{{reflist}}\n\n{{Database}}\n\n{{DEFAULTSORT:Very Large Database}}\n[[Category:Data management]]\n[[Category:Types of databases]]"]
['Category:Data centers', '24125707', '{{Commons category|Data centers}}\n{{catmain|Data center}}\n\n[[Category:Data management|Centers]]\n[[Category:Servers (computing)]]\n[[Category:Computers]]']
['Master data management', '15103022', '{{refimprove|date=April 2012}}\nIn business, \'\'\'master data management\'\'\' (\'\'\'MDM\'\'\') comprises the processes, governance, policies, standards and tools that consistently define and manage the critical data of an [[organization]] to provide a single point of reference.<ref>"What is Master Data" SearchDataManagement, TechTarget, 22 November 2010, http://searchdatamanagement.techtarget.com/definition/master-data-management</ref>\n\nThe data that is mastered may include:\n\n* [[reference data]] &ndash; the business objects for transactions, and the dimensions for analysis\n* analytical data &ndash; supports decision making<ref>"Introduction to Master Data Management", Mark Rittman, Director, Rittman Mead Consulting, 9 May 2008 https://s3.amazonaws.com/rmc_docs/Introduction%20to%20Oracle%20Master%20Data%20Management.pdf</ref><ref>"[http://www.b-eye-network.com/view/2918 "Defining Master Data"], David Loshin, BeyeNetwork, May 2006</ref>\n\nIn [[computing]], a master data management tool can be used to support master data management by removing duplicates, standardizing data (mass maintaining), and incorporating rules to eliminate incorrect data from entering the system in order to create an authoritative source of master data. Master data are the products, accounts and parties for which the business transactions are completed. The root cause problem stems from business unit and product line segmentation, in which the same customer will be serviced by different product lines, with redundant data being entered about the customer (a.k.a. party in the role of customer) and account in order to process the transaction. The redundancy of party and account data is compounded in the front to back office life cycle, where the authoritative single source for the party, account and product data is needed but is often once again redundantly entered or augmented.\n\nMaster data management has the objective of providing processes for collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure consistency and control in the ongoing maintenance and application use of this information.\n\nThe term recalls the concept of a \'\'master file\'\' from an earlier computing era.\n\n==Definition==\nMaster data management (MDM) is a comprehensive method of enabling an enterprise to link all of its critical data to one file, called a master file, that provides a common point of reference. When properly done, master data management streamlines data sharing among personnel and departments. In addition, master data management can facilitate computing in multiple system architectures, platforms and applications.<ref>{{cite web|title=Master data management|url=http://www.ibm.com/software/data/master-data-management/overview.html|publisher=[[IBM]]}}</ref>\n\nAt its core Master Data Management (MDM) can be viewed as a "discipline for specialized quality improvement"<ref>DAMA-DMBOK Guide,2010 DAMA International</ref> defined by the policies and procedures put in place by a data governance organization.  The ultimate goal being to provide the end user community with a "trusted single version of the truth" from which to base decisions.\n\n==Issues==\nAt a basic level, master data management seeks to ensure that an organization does not use multiple (potentially [[Consistency (database systems)|inconsistent]]) versions of the same master data in different parts of its operations, which can occur in large organizations. A typical example of poor master data management is the scenario of a bank at which a [[customer]] has taken out a [[Mortgage loan|mortgage]] and the bank begins to send mortgage solicitations to that customer, ignoring the fact that the person already has a mortgage account relationship with the bank. This happens because the customer information used by the marketing section within the bank lacks integration with the customer information used by the customer services section of the bank.  Thus the two groups remain unaware that an existing customer is also considered a sales lead. The process of [[record linkage]] is used to associate different records that correspond to the same entity, in this case the same person.\n\nOther problems include (for example) issues with the [[data quality|quality of data]], consistent [[classification]] and identification of data, and [[Data validation and reconciliation|data-reconciliation]] issues.  Master data management of disparate data systems requires [[data transformation]]s as the data extracted from the disparate source data system is transformed and loaded into the master data management hub.  To synchronize the disparate source master data, the managed master data extracted from the master data management hub is again transformed and loaded into the disparate source data system as the master data is updated.  As with other [[Extract, Transform, Load]]-based data movement, these processes are expensive and inefficient to develop and to maintain which greatly reduces the [[return on investment]] for the master data management product.\n\nOne of the most common reasons some large corporations experience massive issues with master data management is growth through [[merger]]s or [[Takeover|acquisitions]].  Any organizations which merge will typically create an entity with duplicate master data (since each likely had at least one master database of its own prior to the merger).  Ideally, [[database administrator]]s resolve this problem through [[Data deduplication|deduplication]] of the master data as part of the merger. In practice, however, reconciling several master data systems can present difficulties because of the dependencies that existing applications have on the master databases.  As a result, more often than not the two systems do not fully merge, but remain separate, with a special reconciliation process defined that ensures consistency between the data stored in the two systems.  Over time, however, as further mergers and acquisitions occur, the problem multiplies, more and more master databases appear, and data-reconciliation processes become extremely complex, and consequently unmanageable and unreliable. Because of this trend, one can find organizations with 10, 15, or even as many as 100 separate, poorly integrated master databases, which can cause serious operational problems in the areas of [[customer satisfaction]], operational efficiency, [[decision support]], and regulatory compliance.\n\nAnother problem concerns determining the proper degree of detail and normalization to include in the master data schema. For example, in a federated HR environment, the enterprise may focus on storing people data as a current status, adding a few fields to identify date of hire, date of last promotion, etc. However this simplification can introduce business impacting errors into dependent systems for planning and forecasting. The stakeholders of such systems may be forced to build a parallel network of new interfaces to track onboarding of new hires, planned retirements, and divestment, which works against one of the aims of master data management.  \n==Solutions==\nProcesses commonly seen in master data management include source identification, data collection, [[data transformation]], [[database normalization|normalization]], rule administration, error detection and correction, data consolidation, [[data storage device|data storage]], data distribution, data classification, taxonomy services, item master creation, schema mapping, product codification, data enrichment and [[data governance]].\n\nThe selection of entities considered for master data management depends somewhat on the nature of an organization. In the common case of commercial enterprises, master data management may apply to such entities as customer ([[customer data integration]]), product ([[product information management]]), employee, and vendor. Master data management processes identify the sources from which to collect descriptions of these entities. In the course of transformation and normalization, administrators adapt descriptions to conform to standard formats and data domains, making it possible to remove duplicate instances of any entity. Such processes generally result in an organizational master data management repository, from which  all requests for a certain entity instance produce the same description, irrespective of the originating sources and the requesting destination.\n\nThe tools include [[data networks]], [[file systems]], a [[data warehouse]], [[data mart]]s, an [[operational data store]], [[data mining]], [[data analysis]], [[data visualization]], [[Federated database system|data federation]] and [[data virtualization]]. One of the newest tools, virtual master data management utilizes data virtualization and a persistent metadata server to implement a multi-level automated master data management hierarchy.\n\n==Transmission of master data==\nThere are several ways in which master data may be collated and distributed to other systems.<ref>[http://dama-ny.com/images/meeting/101509/damanyc_mdmprint.pdf "Creating the Golden Record: Better Data Through Chemistry"], DAMA, slide 26, Donald J. Soulsby, 22 October 2009</ref> This includes:\n\n* Data consolidation – The process of capturing master data from multiple sources and integrating into a single hub ([[operational data store]]) for replication to other destination systems.\n* [[Federated database system|Data federation]] – The process of providing a single virtual view of master data from one or more sources to one or more destination systems.\n* Data propagation – The process of copying master data from one system to another, typically through point-to-point interfaces in legacy systems.\n\n==See also==\n* [[Reference data]]\n* [[Master data]]\n* [[Record linkage]]\n* [[Data steward]]\n* [[Data visualization]]\n* [[Customer data integration]]\n* [[Data integration]]\n* [[Product information management]]\n* [[Identity resolution]]\n* [[Enterprise information integration]]\n* [[Linked data]]\n* [[Semantic Web]]\n* [[Data governance]]\n* [[Operational data store]]\n* [[Single customer view]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://msdn2.microsoft.com/en-us/library/bb190163.aspx#mdm04_topic4 Microsoft: The What, Why, and How of Master Data Management]\n* [http://msdn.microsoft.com/en-us/library/bb410798.aspx Microsoft: Master Data Management (MDM) Hub Architecture]\n* [http://mike2.openmethodology.org/wiki/Master_Data_Management_Solution_Offering Open Methodology for Master Data Management]\n* [http://www.semarchy.com/overview/why-do-i-need-mdm/ Semarchy: Why do I Need MDM? (Video)]\n* [http://www.mdmalliancegroup.com/ MDM Community]\n* [http://www.stibosystems.com/Global/explore-stibo-systems/master-data-management.aspx Multidomain Master Data Management]\n* [http://blogs.gartner.com/andrew_white/2014/06/05/reprise-when-is-master-data-and-mdm-not-master-data-or-mdm/ Reprise: When is Master Data and MDM Not Master Data or MDM?]\n* [http://www.orchestranetworks.com/mdm/ Master Data Management (Multidomain)]\n\n{{Data warehouse}}\n{{databases}}\n\n{{DEFAULTSORT:Master Data Management}}\n[[Category:Business intelligence]]\n[[Category:Data management]]\n[[Category:Data warehousing]]\n[[Category:Information technology management]]']
['Data management plan', '31808302', "A '''data management plan''' or '''DMP''' is a formal document that outlines how you will handle your [[data]] both during your research, and after the project is completed.<ref>http://www2.lib.virginia.edu/brown/data/plan.html</ref> The goal of a data management plan is to consider the many aspects of [[data management]], [[metadata]] generation, data preservation, and analysis before the project begins; this ensures that data are well-managed in the present, and prepared for preservation in the future.\n\n== Importance ==\n\nPreparing a data management plan before data are collected ensures that data are in the correct format, organized well, and better annotated.<ref>http://libraries.mit.edu/data-management/plan/why/</ref> This saves time in the long term because there is no need to re-organize, re-format, or try to remember details about data. It also increases research efficiency since both the data collector and other researchers will be able to understand and use well-annotated data in the future.  One component of a good data management plan is data archiving and preservation. By deciding on an archive ahead of time, the data collector can format data during collection to make its future submission to a database easier. If data are preserved, they are more relevant since they can be re-used by other researchers.  It also allows the data collector to direct requests for data to the database, rather than address requests individually.  Data that are preserved have the potential to lead to new, unanticipated discoveries, and they prevent duplication of scientific studies that have already been conducted. Data archiving also provides insurance against loss by the data collector.\n\nFunding agencies are beginning to require data management plans as part of the proposal and evaluation process.<ref>http://www.nsf.gov/bfa/dias/policy/dmpfaqs.jsp</ref>\n\n== Major Components ==\n\n=== Information about data & data format ===\n\n* Include a description of data to be produced by the project.<ref>{{Cite web|title = Elements of a Data Management Plan|url = http://www.icpsr.umich.edu/icpsrweb/content/datamanagement/dmp/elements.html|website = www.icpsr.umich.edu|accessdate = 2015-09-30}}</ref> This might include (but is not limited to) data that are:\n** Experimental\n** Observational\n** Raw or derived\n** Physical collections\n** Models\n** Simulations\n** Curriculum materials\n** Software\n** Images\n* How will the data be acquired? When and where will they be acquired?\n* After collection, how will the data be processed? Include information about\n** Software used\n** Algorithms\n** [[workflow|Scientific workflows]]\n* Describe the file formats that will be used, justify those formats, and describe the naming conventions used.\n* Identify the quality assurance & quality control measures that will be taken during sample collection, analysis, and processing.\n* If existing data are used, what are their origins? How will the data collected be combined with existing data? What is the relationship between the data collected and existing data?\n* How will the data be managed in the short-term? Consider the following:\n** [[Version control]] for files\n** Backing up data and data products\n** Security & protection of data and data products\n** Who will be responsible for management\n\n=== Metadata content and format ===\n\n[[Metadata]] are the contextual details, including any information important for using data. This may include descriptions of temporal and spatial details, instruments, parameters, units, files, etc. Metadata is commonly referred to as “data about data”.<ref>Michener,WK and JW Brunt. 2000. ''Ecological Data: Design, Management and Processing''. Blackwell Science, 180p.</ref> Consider the following:\n* What metadata are needed? Include any details that make data meaningful.\n* How will the metadata be created and/or captured? Examples include lab notebooks, GPS hand-held units, Auto-saved files on instruments, etc.\n* What format will be used for the metadata? Consider the [[metadata standards]] commonly used in the scientific discipline that contains your work. There should be justification for the format chosen.\n\n=== Policies for access, sharing, and re-use ===\n\n* Describe any obligations that exist for sharing data collected. These may include obligations from funding agencies, institutions, other professional organizations, and legal requirements.\n* Include information about how data will be shared, including when the data will be accessible, how long the data will be available, how access can be gained, and any rights that the data collector reserves for using data.\n* Address any ethical or privacy issues with data sharing\n* Address [[intellectual property]] & [[copyright]] issues. Who owns the copyright? What are the institutional, publisher, and/or funding agency policies associated with intellectual property? Are there embargoes for political, commercial, or patent reasons?\n* Describe the intended future uses/users for the data\n* Indicate how the data should be cited by others. How will the issue of persistent citation be addressed? For example, if the data will be deposited in a public archive, will the dataset have a [[digital object identifier]] (doi) assigned to it?\n\n=== Long-term storage and data management ===\n\n* Researchers should identify an appropriate archive for long-term preservation of their data. By identifying the archive early in the project, the data can be formatted, transformed, and documented appropriately to meet the requirements of the archive. Researchers should consult colleagues and professional societies in their discipline to determine the most appropriate database, and include a backup archive in their data management plan in case their first choice goes out of existence.\n* Early in the project, the primary researcher should identify what data will be preserved in an archive. Usually, preserving the data in its most raw form is desirable, although data derivatives and products can also be preserved.\n* An individual should be identified as the primary contact person for archived data, and ensure that contact information is always kept up-to-date in case there are requests for data or information about data.\n\n=== Budget ===\n\nData management and preservation costs may be considerable, depending on the nature of the project. By anticipating costs ahead of time, researchers ensure that the data will be properly managed and archived. Potential expenses that should be considered are\n* Personnel time for data preparation, management, documentation, and preservation\n* Hardware and/or software needed for data management, backing up, security, documentation, and preservation\n* Costs associated with submitting the data to an archive\nThe data management plan should include how these costs will be paid.\n\n== NSF Data Management Plan ==\n\nAll grant proposals submitted to [[National Science Foundation|NSF]] must include a Data Management Plan that is no more than two pages.<ref>http://www.nsf.gov/pubs/policydocs/pappguide/nsf11001/gpg_2.jsp#dmp</ref> This is a supplement (not part of the 15 page proposal) and should describe how the proposal will conform to the Award and Administration Guide policy (see below). It may include the following:\n# The types of data\n# The standards to be used for data and metadata format and content\n# Policies for access and sharing\n# Policies and provisions for re-use\n# Plans for archiving data\n\nPolicy summarized from the [[National Science Foundation|NSF]] Award and Administration Guide, Section 4 (Dissemination and Sharing of Research Results):<ref>http://www.nsf.gov/bfa/dias/policy/dmp.jsp</ref>\n# Promptly publish with appropriate authorship\n# Share data, samples, physical collections, and supporting materials with others, within a reasonable time frame\n# Share software and inventions\n# Investigators can keep their legal rights over their intellectual property, but they still have to make their results, data, and collections available to others\n# Policies will be implemented via\n## Proposal review\n## Award negotiations and conditions\n## Support/incentives\n\n== ESRC Data Management Plan ==\n\nSince 1995, the UK's [[Economic and Social Research Council]] (ESRC) have had a research data policy in place. The current ESRC Research Data Policy states that research data created as a result of ESRC-funded research should be openly available to the scientific community to the maximum extent possible, through long-term preservation and high quality data management.<ref>[http://www.esrc.ac.uk/about-esrc/information/data-policy.aspx ESRC Research Data Policy 2010]</ref>\n\nESRC requires a data management plan for all research award applications where new data are being created. Such plans are designed to promote a structured approach to data management throughout the data lifecycle, resulting in better quality data that is ready to archive for sharing and re-use. The [[UK Data Service]], the ESRC's flagship data service, provides practical guidance on research data management planning suitable for social science researchers in the UK and around the world.<ref>[http://ukdataservice.ac.uk/manage-data.aspx Prepare and manage data: Guidance from the UK Data Service]</ref><ref>[http://www.sagepub.com/books/Book240297 SAGE handbook: Managing and Sharing Data: A Guide to Good Practice]</ref>\n\nESRC has a longstanding arrangement with the [[UK Data Archive]], based at the [[University of Essex]], as a place of deposit for research data, with award holders required to offer data resulting from their research grants via the UK Data Service.<ref>[http://www.data-archive.ac.uk/deposit/who UK Data Archive: Who can deposit data?]</ref> The Archive enables data re-use by preserving data and making them available to the research and teaching communities.\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n{{Cite book|title = Delivering research data management services|last = Pryor|first = Graham|publisher = Facet Publishing|year = 2014|isbn = 9781856049337|location = |pages = }}\n\n== External links ==\n* [http://www.sagepub.com/books/Book240297?&subject=B00&sortBy=defaultPubDate%20desc&fs=1 SAGE handbook]: Managing and Sharing Research Data: A Guide to Good Practice\n* [http://dmp.cdlib.org DMPTool]: Guidance and resources for data management plans\n* [http://www.cdlib.org/services/uc3/dmp/index.html California Digital Library], University of California Curation Center (UC3)\n* [http://www.dataone.org/plans DataONE]\n* [http://www2.lib.virginia.edu/brown/data/plan.html University of Virginia Library]\n* [https://dmponline.dcc.ac.uk/ DMPonline]\n* [http://www.dcc.ac.uk/resources/data-management-plans Digital Curation Centre]\n* [http://www.lib.umich.edu/research-data-management-and-publishing-support/nsf-data-management-plans#directorate_guide University of Michigan Library]\n* [http://www.nsf.gov/pubs/policydocs/pappguide/nsf11001/gpg_2.jsp#dmp NSF Grant Proposal Guidelines]\n* [http://www.icpsr.umich.edu/icpsrweb/ICPSR/dmp/index.jsp Inter-University Consortium for Political and Social Research]\n* [http://lno.lternet.edu/node/269 LTER Blog: How to write a data management plan]\n* [http://www.gesis.org/en/archive-and-data-management-training-and-information-center/research-data-management/data-management-plan/ More information about data management plans at [[GESIS – Leibniz Institute for the Social Sciences]]]\n* [http://ukdataservice.ac.uk/manage-data.aspx UK Data Service]: Prepare and Manage Data: Guidance and tools for social science researchers\n* [http://www.consorciomadrono.es/pagoda Plan de Gestión de Datos PaGoDa]: DMP Toolkit of The Consortium of Universities of the Region of Madrid and the UNED for Library Cooperation (Madroño - Spain) \n\n<!--- Categories --->\n[[Category:Articles created via the Article Wizard]]\n[[Category:Data management]]"]
['PureXML', '19470961', "{{Lowercase title}}\n'''pureXML''' is the native [[XML]] storage feature in the [[IBM DB2]] data server.  pureXML provides [[query language]]s, storage technologies, indexing technologies, and other features to support XML data.  The word ''pure'' in pureXML was chosen to indicate that DB2 natively stores and natively processes XML data in its inherent hierarchical structure, as opposed to treating XML data as plain text or converting it into a relational format.<ref>http://www.ibm.com/developerworks/blogs/page/datastudioteam?entry=purexml_and_purequery_what_s</ref>\n\n== Technical information ==\nDB2 includes two distinct storage mechanisms: one for efficiently managing traditional SQL data types, and another for managing XML data.  The underlying storage mechanism is transparent to users and applications; they simply use SQL (including SQL with XML extensions or [[SQL/XML]]) or [[XQuery]] to work with the data.\n\nXML data is stored in columns of DB2 tables that have the XML data type.  XML data is stored in a parsed format that reflects the hierarchical nature of the original XML data.  As such, pureXML uses trees and nodes as its model for storing and processing XML data.  If you instruct DB2 to validate XML data against an XML schema prior to storage, DB2 annotates all nodes in the XML hierarchy with information about the schema types; otherwise, it will annotate the nodes with default type information.  Upon storage, DB2 preserves the internal structure of XML data, converting its tag names and other information into integer values. Doing so helps conserve disk space and also improves the performance of queries that use navigational expressions. However, users aren't aware of this internal representation.  Finally, DB2 automatically splits XML nodes across multiple database pages, as needed.\n\nXML schemas specify which XML elements are valid, in what order these elements should appear in XML data, which XML data types are associated with each element, and so on.  pureXML allows you to validate the cells in a column of XML data against no schema, one schema, or multiple schemas.  pureXML also provides tools to support evolving XML schemas.\n\nIBM has enhanced its [[programming language]] interfaces to support access to its XML data. These enhancements span [[Java (programming language)|Java]] ([[JDBC]]), [[C (programming language)|C]] (embedded SQL and call-level interface), [[COBOL]] (embedded SQL), [[PHP]], and [[Microsoft]]'s [[.NET Framework]] (through the DB2.NET provider).\n\n== History ==\npureXML was first included in the DB2 9 for [[Linux]], [[Unix]], and [[Microsoft Windows]] release, which was codenamed Viper, in June 2006.<ref>{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/19781.wss | title = IBM News room - 2006-06-08 IBM Transforms Database Market With Introduction of DB2 - United States | archiveurl= https://web.archive.org/web/20121011235127/http://www-03.ibm.com/press/us/en/pressrelease/19781.wss | archivedate= 2012-10-11 }}</ref>  It was available on DB2 9 for [[z/OS]] in March 2007.<ref>{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/21189.wss | title = IBM News room - 2007-03-06 IBM Unveils DB2 Viper for the Mainframe - United States | archiveurl= https://web.archive.org/web/20121011235143/http://www-03.ibm.com/press/us/en/pressrelease/21189.wss | archivedate= 2012-10-11 }}</ref>  In October 2007, IBM released DB2 9.5 with improved XML data transaction performance and improved storage savings.<ref>{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/22455.wss | title = IBM News room - 2007-10-15 IBM Extends Data Server Technology Lead With Introduction of DB2 &quot;Viper 2&quot; - United States | archiveurl= https://web.archive.org/web/20121011235149/http://www-03.ibm.com/press/us/en/pressrelease/22455.wss | archivedate= 2012-10-11 }}</ref> In June 2009, IBM released DB2 9.7 with XML supported for database-partitioned, range-partitioned, and multi-dimensionally clustered tables as well as compression of XML data and indices.<ref>{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/27279.wss | title = IBM News room - 2009-04-22 IBM Database Software Improves Operational Efficiency and Cuts Storage Costs by Up to 75% - United States | archiveurl= https://web.archive.org/web/20121121014600/http://www-03.ibm.com/press/us/en/pressrelease/27279.wss | archivedate= 2012-11-21 }}</ref>\n\n== Competition ==\n{{See also|XML database}}\nDB2 is a hybrid data server—it offers data management for traditional relational data, as well as providing native XML data management.  Other vendors that offer data management for both relational data and native XML storage include [[Oracle Corporation|Oracle]] with its [[Oracle Database|11g]] product and Microsoft with its [[Microsoft SQL Server|SQL Server]] product.\n\npureXML also competes with native XML databases like [[BaseX (database)|BaseX]], [[eXist]], [[MarkLogic]] or [[Sedna (database)|Sedna]].\n\n== User groups ==\nThe International DB2 Users Group (IDUG) is an independent, not-for-profit association of IT professionals who use IBM DB2.  IDUG provides education, technical resources, peer networking opportunities, online resources and other programs for DB2 users.\n\n== Books ==\nIBM International Technical Support Organization (ITSO) has published the following books, which are available in print or as free e-books:\n* [http://www.redbooks.ibm.com/abstracts/sg247298.html?Open DB2 9: pureXML Overview and Fast Start]\n* [http://www.redbooks.ibm.com/abstracts/sg247315.html?Open DB2 9 pureXML Guide]\n\nThe following books are also available for purchase:\n* [http://www.amazon.com/DB2-pureXML-Cookbook-Master-Hybrid/dp/0138150478/ DB2 pureXML Cookbook: Master the Power of IBM Hybrid Data Server]\n\n== Education and training ==\nThe following pureXML classroom and online courses are available from IBM Education:\n* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_description&courseCode=CG130 Query and Manage XML Data with DB2 9].  IBM course CG130.  Classroom.  Duration: 4 days.\n* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_description&courseCode=CG100 Query XML Data with DB2 9].  IBM course CG100.  Classroom.  Duration: 2 days (first 2 days of CG130).\n* Managing XML Data in DB2 9.  IBM course CG160.  Classroom.  Duration: 2 days (last 2 days of CG130).\n* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_search&sortBy=5&searchType=1&sortDirection=9&includeNotScheduled=15&rowStart=0&rowsToReturn=20&maxSearchResults=200&searchString=CT140&language=en&country=us DB2 pureXML].  IBM Course CT140.  Self-paced study plus Live Virtual Classroom.\n\n== See also ==\n* [[IBM DB2]]\n* [[XML database]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{official website|http://www.ibm.com/software/data/db2/xml}}\n* [http://www.ibm.com/developerworks/wikis/display/db2xml/Home pureXML Wiki]\n* [http://www.ibm.com/developerworks/forums/forum.jspa?forumID=1423 pureXML Forum]\n* [http://www.ibm.com/developerworks/blogs/page/purexml pureXML Team Blog]\n* [http://www.nativexmldatabase.com Native XML Database Blog]\n* [http://blog.4loeser.net Blog with pureXML Topics]\n\n=== Online communities ===\nOnline communities allow pureXML users to network with fellow professionals.\n* [http://www.linkedin.com/groups?gid=129185 pureXML Group on LinkedIn]\n\n{{IBM DB2 product family}}\n\n[[Category:XML software]]\n[[Category:Data management]]\n[[Category:Data modeling]]\n[[Category:IBM DB2]]\n[[Category:IBM software]]\n[[Category:XML databases]]"]
['Category:Data partitioning', '19073984', '[[Category:Data management]]']
['ADO.NET', '1434840', "{{No footnotes|date=March 2009}}\n{{Infobox software\n|\n| operating system       = [[Microsoft Windows]]\n| genre                  = [[Software framework]]\n| license                = [[Proprietary software]] ([[Base Class Library|BCL]] portion under [[MIT license]]; source code under [[Ms-RSL]])\n| website                = {{url|http://msdn2.microsoft.com/en-us/library/aa286484.aspx}}\n| Nonprofit              =\n}}\n'''ADO.NET''' is a data access technology from the [[Microsoft]] [[.NET Framework]] which provides communication between relational and non-relational systems through a common set of components.\nADO.NET is a set of computer software components that programmers can use to access data and data services from the database. It is a part of the [[Base Class Library|base class library]] that is included with the Microsoft .NET Framework. It is commonly used by programmers to access and modify data stored in [[Relational DBMS|relational database systems]], though it can also access data in non-relational sources. ADO.NET is sometimes considered an evolution of [[ActiveX Data Objects]] (ADO) technology, but was changed so extensively that it can be considered an entirely new product.\n\n== Architecture ==\n{{Main|ADO.NET data provider}}\n[[Image:DotNet3.0.svg|thumb|right|240px|This [[technology]] forms a part of [[.NET Framework 3.0]] (having been part of the framework since version 1.0)]]\n\nADO.NET is conceptually divided into ''[[ADO.NET consumer|consumers]]'' and ''[[ADO.NET provider|data providers]]''. The consumers are the applications that need access to the data, and the providers are the software components that implement the interface and thereby provide the data to the consumer.\n\nFunctionality exists in [[Microsoft Visual Studio|Visual Studio]] IDE to create specialized subclasses of the DataSet classes for a particular database schema, allowing convenient access to each field through strongly typed [[Property (programming)|properties]]. This helps catch more programming errors at compile-time and enhances the IDE's [[Intellisense]] feature.\n\n== O/R Mapping ==\n{{main|Object-relational mapping}}\n\n=== Entity Framework ===\n{{main|Entity Framework}}\n\nEntity Framework (EF) is an open source object-relational mapping (ORM) framework for ADO.NET, part of .NET Framework. It is a set of technologies in ADO.NET that support the development of data-oriented software applications. Architects and developers of data-oriented applications have typically struggled with the need to achieve two very different objectives. The Entity Framework enables developers to work with data in the form of domain-specific objects and properties, such as customers and customer addresses, without having to concern themselves with the underlying database tables and columns where this data is stored. With the Entity Framework, developers can work at a higher level of abstraction when they deal with data, and can create and maintain data-oriented applications with less code than in traditional applications.\n\n=== LINQ to SQL ===\n{{main|LINQ to SQL}}\n\nLINQ to SQL (formerly called DLINQ) allows [[LINQ]] to be used to query Microsoft SQL Server databases, including SQL Server Compact databases. Since SQL Server data may reside on a remote server, and because SQL Server has its own query engine, it does not use the query engine of LINQ. Instead, it converts a LINQ query to a SQL query that is then sent to SQL Server for processing. However, since SQL Server stores the data as relational data and LINQ works with data encapsulated in objects, the two representations must be mapped to one another. For this reason, LINQ to SQL also defines a mapping framework. The mapping is done by defining classes that correspond to the tables in the database, and containing all or a certain subset of the columns in the table as data members.\n\n==See also==\n* [[Comparison of ADO and ADO.NET]]\n\n==External links==\n;ADO.NET\n* [http://msdn2.microsoft.com/en-us/library/aa286484.aspx ADO.NET Overview on MSDN]\n* [http://msdn2.microsoft.com/en-us/library/ms973217.aspx ADO.NET for the ADO Programmer]\n* [http://www.devlist.com/ConnectionStringsPage.aspx ADO.NET Connection Strings]\n\n{{.NET Framework}}\n{{Microsoft APIs}}{{Windows-software-stub}}{{Microsoft-stub}}{{Programming-software-stub}}\n\n{{DEFAULTSORT:Ado.Net}}\n[[Category:Data management]]\n[[Category:.NET Framework terminology]]\n[[Category:Microsoft application programming interfaces]]\n[[Category:SQL data access]]\n[[Category:ADO.NET Data Access technologies]]"]
['National Information Governance Board for Health and Social Care', '34419120', 'The \'\'\'National Information Governance Board for Health and Social Care\'\'\' (NIGB) was established in the [[United Kingdom]] under section 157<ref>[http://www.legislation.gov.uk/ukpga/2008/14/section/157 Section 157 of the Health and Social Care Act 2008]</ref> of the Health and Social Care Act 2008, with effect from October 2008, with a range of advisory functions relating to [[information governance]].  From January 2009, the NIGB also gained functions under section 251<ref>[http://www.legislation.gov.uk/ukpga/2006/41/section/251 Section 251 of the NHS Act 2006]</ref> of the NHS Act 2006 which had previously been held by the [[Patient Information Advisory Group]] (PIAG) until its abolition. These functions were to advise the [[Secretary of State for Health]] on the use of powers to set aside the common law duty of confidentiality in [[England]] where identifiable patient information is needed and where consent is not practicable. From 1 April 2013, the NIGB\'s functions for monitoring and improving information governance practice have transferred to the [[Care Quality Commission]], which established a National Information Governance Committee to oversee this work. Functions relating to section 251 of the [[National Health Service Act 2006|NHS Act 2006]] (access to people’s personal and confidential information for research purposes) were transferred to the [[Health Research Authority]]\'s Confidentiality Advisory Group.<ref>[http://webarchive.nationalarchives.gov.uk/20130513181011/http://www.nigb.nhs.uk/ Webarchive page of the National Information Governance Board for Health and Social Care]</ref>\n\n==Terms of reference==\n\nThe key functions of the NIGB (excerpted from the legislation) were:\n<ol type="a">\n<li> to monitor the practice followed by relevant bodies in relation to the processing of relevant information;</li>\n<li> to keep the [[Secretary of State for Health]], and such bodies as the [[Secretary of State for Health]] may designate by direction, informed about the practice being followed by relevant bodies in relation to the processing of relevant information;</li>\n<li> to publish guidance on the practice to be followed in relation to the processing of relevant information;</li>\n<li> to advise the [[Secretary of State for Health]] on particular matters relating to the processing of relevant information by any person; and</li>\n<li> to advise persons who process relevant information on such matters relating to the processing of relevant information by them as the [[Secretary of State for Health]] may from time to time designate by direction.</li>\n</ol>\n\nThe definition of “relevant information” in the legislation covers patient information, any other information obtained or generated in the course of the provision of the health service, and any information obtained or generated in the course of the exercise by a local social services authority in [[England]] of its adult social services functions.\n\n==Ethics and Confidentiality Committee==\n\nSome areas of NIGB functions (d) and (e) above had been delegated to the NIGB’s Ethics and Confidentiality Committee (ECC).  These functions primarily related to applications to use identifiable patient information without consent, in specific circumstances within the bounds of section 251 of the NHS Act 2006.  These applications, which had been considered by PIAG before the NIGB, passed on to the [[Health Research Authority]]\'s Confidentiality Advisory Group (CAG) on 1 April 2013.\n\n==Care Record Development Board==\n\nThe NIGB had also replaced the Care Record Development Board (CRDB),<ref>[http://www.connectingforhealth.nhs.uk/crdb Care Record Development Board archive page hosted by NHS Connecting for Health]</ref> which had closed in September 2007. The NIGB had subsequently maintained the NHS Care Record Guarantee which was originally developed by the CRDB and developed a companion Social Care Record Guarantee.\n\n==Members==\n\nThe NIGB had consisted of a Chair, a number of Public Members appointed by the NHS Appointments Commission, and a number of Representative Members appointed by the [[Secretary of State for Health]] from a range of stakeholder organisations.  Representatives of several other stakeholder organisations had served as Corresponding Advisers to the NIGB but had not typically attended meetings.  Regular observers at meetings had included representatives from the [[Information Commissioner\'s Office]] and the devolved UK administrations.\n\nThe ECC had consisted of a Chair and a number of Members, all of whom had been appointed by the NIGB with advice from an NHS Appointments Commission approved independent assessor.  The ECC Chair and two ECC Members had also been NIGB Members.\n\nBetween 1 June 2011 and 31 March 2013 Dame [[Fiona Caldicott]]<ref>[http://www.connectingforhealth.nhs.uk/newsroom/news/nigbchair Appointment of Fiona Caldicott as new NIGB Chair - press release on NHS Connecting for Health website, June 2011]</ref> had been Chair of the NIGB, succeeding [[Harry Cayton]] who had chaired the NIGB since its inception.\n\n==Geography==\n\nMembers of the NIGB and ECC had been widely distributed nationally but had attended meetings at the NIGB office.  Since September 2011, this had been based at [[Skipton House]], London SE1.  The NIGB’s staff team had been predominantly based at this office.\n\n==Abolition==\n\nAs a result of the [[Health and Social Care Act 2012]] the NIGB was abolished with effect from 1 April 2013. The functions delegated to the ECC with respect to research transferred to the [[Health Research Authority]].<ref>[http://www.hra.nhs.uk/news/2012/12/17/further-update-on-transfer-of-s251-function-from-nigb-to-hra/ Transfer of s251 function from NIGB to HRA]</ref> The [[NHS Commissioning Board]] is now responsible for providing advice and guidance to NHS bodies. Other functions were transferred to the National Information Governance Committee hosted by the [[Care Quality Commission]].\n\n==References==\n\n{{Reflist}}\n\n==External links==\n*[http://webarchive.nationalarchives.gov.uk/20130513181011/http://www.nigb.nhs.uk/ NIGB website (archived)]\n*[http://www.hra.nhs.uk/ NHS Health Research Authority]\n*[http://www.cqc.org.uk/ Care Quality Commission]\n*[http://www.nres.nhs.uk National Research Ethics Service]\n*[http://www.commissioningboard.nhs.uk/ NHS Commissioning Board]\n\n\n[[Category:Data management]]\n[[Category:Medical privacy]]\n[[Category:2008 establishments in the United Kingdom]]\n[[Category:Organizations established in 2008]]\n[[Category:Governance in the United Kingdom]]\n[[Category:Social care in the United Kingdom]]']
['Classora', '35915041', "{{Refimprove|date=June 2012}}\n\n'''Classora''' is a [[knowledge base]] for the [[Internet]] oriented to [[data]] analysis. From a practical point of view, Classora is a [[digital repository]] that stores structured [[information]] and allows it to be displayed in multiple formats: analytically, graphically, geographically (through maps); as well as carry out [[OLAP]] analysis. The information contained in Classora comes from public sources<ref>[http://revista.mundo-r.com/contido/%E2%80%9Cclassora-evita-tener-que-bucear-entre-resultados-google-o-wikipedia%E2%80%9D Interview in R Technological Magazine (Spanish)]</ref> and is uploaded into the system through bots and [[Extract, transform, load|ETL]] processes. The [[Knowledge Base]] has a '''commercial API'''<ref>[http://blog.classora.com/2012/03/05/api-de-classora-para-desarrolladores/ Classora API in Official Weblog]</ref> for semantic enhancement, and an '''open web'''<ref>[http://www.classora.com Open Web of Classora Knowledge Base]</ref> through which any user can access to part of the information collected (it also allows users to complete data and share opinions).\n\nInternally, Classora is organized into '''Knowledge Units''' and '''Reports'''. A «Knowledge Unit» is any element of the World about which information may be stored and presented in the form of a data sheet (a person, a company, a country, etc.) A «Report» is a group of Knowledge Units: a ranking of companies, a sport classification table, a survey about people, etc. In fact, one of the technical capabilities of Classora is that it allows the comparison of reports and knowledge units gathered from different sources, thereby generating an added value for the media in which this information is published: digital media, interactive TV, etc.\n\n== Key definitions ==\n\n=== Knowledge unit ===\nThe '''units of knowledge''' (also known as ''entries'') in Classora are data sheets that have a certain semantic equivalence with the articles on the Wikipedia: they store information about any element of the world, be it a film, a country, a company or an animal. However, they differ from Wikipedia in that Classora stores structured information, enriched with a metadata layer; and therefore it is able to automatically interpret the meaning of each unit of knowledge.\n\n=== Data report ===\nA '''report''' is a group of units of knowledge in which the repetition of elements is not allowed. This definition includes any list, poll, ranking, etc.; and, in general, any consultation that involves more than one unit of knowledge. Classora excels at the reports management due to its visualization capabilities, being able to display data in the form of tables, graphs and maps. \n\nTypes of reports:\n\n* '''Sports scores''': Sports competitions results sanctioned by the competent institution.\n* '''Rankings and lists''': All types of interesting and curious lists, whether they have an implicit order or not.\n* '''Polls''': Units of knowledge that are ranked according to users’ votes.\n* '''Queries to the Knowledge Base''': Questions from users using [[Contextual Query Language|CQL]].\n* '''Networks of connections''': automatically calculated from the reports and the taxonomy of each Knowledge Unit.\n\n=== Organizational taxonomy ===\nAn '''organizational taxonomy''' (also referred to as '''entry type''') is a data sheet that brings together the common attributes of a set of units of knowledge. For instance, the organizational taxonomy ''F1 Driver'' displays attributes such as date of debut, team, etc.; and the organizational taxonomy ''Football Club'' presents attributes such as city, stadium, etc.\n\nIn Classora, taxonomies are hierarchically organized, so that they inherit attributes from their parent taxonomies. For instance, ''F1 Driver'' is a subsidiary taxonomy of ''Sportsperson'', which is a subsidiary taxonomy of ''Person'', which in turn is a subsidiary taxonomy of ''Organism''.\n\nThe simplest type of entry in Classora is '''Classora Object'''. All the other taxonomies are its subsidiaries and inherit its attributes. In fact, the only attribute Classora Object possesses is ''name'' (all units of knowledge are required to have one name at least).\n\n== Architecture of Classora ==\n\n=== Data Extraction Module ===\nThe Data Extraction Module consists of a set of robots coordinated by software that also manages the potential incidents. Most of the information available in Classora is automatically uploaded through those robots, which connect to the main online public sources to gather all types of data. There are three categories of robots:\n* '''Extraction robots''': responsible for the massive uploading of reports from official public sources (FIFA, CIA, IMF, Eurostat...). They are used for either absolute or incremental data uploading.\n* '''Data scanner robots''': responsible for looking for and updating the data of a unit of knowledge. They use specific sources to perform this task: Wikipedia, IMDB, World Bank, etc.\n* '''Content aggregators''':  they don’t connect to external sources. Instead, they generate new information using Classora’s internal database.\n\n=== Participatory Module ===\nIn Classora’s Open Website, Internet users may participate providing their knowledge as they would on the Wikipedia. There are different ways to participate: adding or correcting data in the Knowledge Base, voting in surveys (participatory rankings) and creating new Knowledge Units and Data Reports.\n\n=== Connectivity Module ===\nThe Knowledge Base is designed to be embedded in multi-platform, multi-channel systems, thus enabling its integration into mobile devices, tablets, interactive TV, etc. This integration may be carried out through specific plugins (for [[navigators]] or other devices) or an [[API]] [[REST]] that provides content in [[XML]] or [[JSON]] formats. The API is divided into three blocks of operations. The first one is the block of '''general utility tools''' (ranging from autosuggest components about geographical hierarchies to operations to obtain the list of today’s celebrity birthdays, using [[Contextual Query Language|CQL]]). The second one is the block of '''operations for widget generation''' (graphs, maps, rankings) using information from the knowledge base. Finally, there is a block of '''operations designed for the publication of free-source content'''.<ref>[http://blog.classora.com/2012/03/05/api-de-classora-para-desarrolladores/ Post about API in Classora official weblog]</ref>\n\n== Project statistics ==\nAs of April 2012, 2,000,000 Knowledge Units, 15,000 Reports, around 10,000 Maps and several million potential Comparative Analyses had been added to Classora. According to the site of web metrics Alexa, Classora Open Website is ranked at 100,557 globally and at 2,880 in the Spanish traffic ranking.<ref>[http://www.alexa.com/siteinfo/http%3A%2F%2Fwww.classora.com Alexa metrics for Classora Open Web]</ref> Users spend an average of 9 ½ minutes in Classora.\n\n== External links ==\n* [http://www.classora.com Open Website of Classora Knowledge Base]\n\n== References ==\n<references/>\n\n[[Category:Knowledge bases]]\n[[Category:Data management]]\n[[Category:Semantic Web]]\n[[Category:Knowledge representation software]]"]
['Data set (IBM mainframe)', '1042727', '{{about|mainframe computer file|a general meaning in computing field|Data set}}\n{{Refimprove|date=September 2014}}\n\nIn the context of [[IBM]] [[mainframe computer]]s, a \'\'\'data set\'\'\' (IBM preferred) or  \'\'\'dataset\'\'\' is a [[computer file]] having a [[record-oriented file|record organization]]. Use of this term began with [[OS/360]] and is still used by its successors, including the current [[z/OS]]. Documentation for these systems historically preferred this term rather than \'\'[[computer file|file]]\'\'.\n\nA data set is typically stored on a [[direct access storage device]] (DASD) or [[magnetic tape]], however unit record devices, such as punch card readers, card punch, and line printers can provide input/output (I/O) for a data set (file).<ref>http://publib.boulder.ibm.com/infocenter/zvm/v5r4/index.jsp?topic=/com.ibm.zvm.v54.hcpa7/hcse7b3050.htm</ref>\n\nData sets are not unstructured streams of [[byte]]s, but rather are organized in various logical record and block structures determined by the <code>DSORG</code> (data set organization), <code>RECFM</code> (record format), and other parameters. These parameters are specified at the time of the data set allocation (creation), for example with [[Job Control Language]] <code>DD</code> statements. Inside a job they are stored in the [[Data Control Block]] (DCB), which is a data structure used to access data sets, for example using [[access method]]s.\n\n==Data set organization==\nFor OS/360, the DCB\'s DSORG parameter specifies how the data set is organized. It may be physically sequential ("PS"), indexed sequential ("IS"), partitioned ("PO"), or Direct Access ("DA"). Data sets on tape may only be DSORG=PS. The choice of organization depends on how the data is to be accessed, and in particular, how it is to be updated.\n\nProgrammers utilize various [[access method]]s (such as [[Queued Sequential Access Method|QSAM]] or [[VSAM]]) in programs for reading and writing data sets. Access method depends on the given data set organization.\n\n==Record format (RECFM)==\nRegardless of organization, the physical structure of each record is essentially the same, and is uniform throughout the data set. This is specified in the DCB <code>RECFM</code> parameter. <code>RECFM=F</code> means that the records are of fixed length, specified via the <code>LRECL</code> parameter, and <code>RECFM=V</code> specifies a variable-length record. V records when stored on media are prefixed by a Record Descriptor Word (RDW) containing the integer length of the record in bytes. With <code>RECFM=FB</code> and <code>RECFM=VB</code>, multiple logical records are grouped together into a single [[Block (data storage)|physical block]] on tape or disk. FB and VB are <code>fixed-blocked</code>, and <code>variable-blocked</code>, respectively. The <code>BLKSIZE</code> parameter specifies the maximum length of the block. <code>RECFM=FBS</code> could be also specified, meaning <code>fixed-blocked standard</code>, meaning all the blocks except the last one were required to be in full <code>BLKSIZE</code> length. <code>RECFM=VBS</code>, or <code>variable-blocked spanned</code>, means a logical record could be spanned across two or more blocks, with flags in the RDW indicating whether a record segment is continued into the next block and/or was continued from the previous one.\n\nThis mechanism eliminates the need for using any "delimiter" byte value to separate records. Thus data can be of any type, including binary integers, floating point, or characters, without introducing a false end-of-record condition. The data set is an abstraction of a collection of records, in contrast to files as unstructured streams of bytes.\n\n=={{anchor|Partitioned datasets}}Partitioned data sets==\nA \'\'\'partitioned data set\'\'\' (\'\'\'PDS\'\'\') is a data set containing multiple \'\'members\'\', each of which holds a separate sub-data set, similar to a [[directory (file systems)|directory]] in other types of [[file system]]s. This type of data set is often used to hold executable programs (\'\'load modules\'\'), source program libraries (especially Assembler macro definitions), and [[Job Control Language]]. A PDS may be compared to a [[ZIP (file format)|Zip]] file or [[COM Structured Storage]].\n\nA Partitioned Data Set can only allocate on a single volume with the maximum size of 65535 tracks.\n\nBesides members, a PDS consists also of their directory. Each member can be accessed directly using the directory structure. Once a member is located, the data stored in that member is handled in the same manner as a PS (sequential) data set.\n\nWhenever a member is deleted, the space it occupied is unusable for storing other data. Likewise, if a member is re-written, it is stored in a new spot at the back of the PDS and leaves wasted “dead” space in the middle. The only way to recover “dead” space is to perform frequent file compression, that moves all members to the front of the data space and leaves free usable space at the back.  (Note that in modern parlance, this kind of operation might be called [[defragmentation]] or [[garbage collection (computer science)|garbage collection]]; [[data compression]] nowadays refers to a different, more complicated concept.)  PDS files can only reside on disk in order to use the directory structure to access individual members, not on tape. They are most often used for storing multiple JCL files, utility control statements and executable modules.\n\nAn improvement of this scheme is a Partitioned Data Set Extended (PDSE or PDS/E, sometimes just \'\'libraries\'\') introduced with [[MVS/XA]] system.\n\nPDS/E structure is similar to PDS and is used to store the same types of data. However, PDS/E files have a better directory structure which does not require pre-allocation of directory blocks when the PDS/E is defined (and therefore does not run out of directory blocks if not enough were specified). Also, PDS/E automatically stores members in such a way that compression operation is not needed to reclaim "dead" space. PDS/E files can only reside on disk in order to use the directory structure to access individual members.\n\n==See also==\n* [[Volume table of contents]] (VTOC), a structure describing data sets stored on the disk\n* [[Distributed Data Management Architecture]]\n\n==References==\n{{Reflist}}\n* [http://publib-b.boulder.ibm.com/Redbooks.nsf/RedbookAbstracts/sg246366.html Introduction to the New Mainframe: z/OS Basics], Ch. 5, "Working with data sets", March 29, 2011. ISBN 0738435341\n\n{{Mainframe I/O access methods}}\n\n{{DEFAULTSORT:Data Set (IBM Mainframe)}}\n[[Category:Data management]]\n[[Category:IBM mainframe operating systems]]\n[[Category:Computer file systems]]\n[[Category:Computer files]]']
['PL/Perl', '2537690', '\'\'\'PL/Perl (Procedural Language/Perl)\'\'\' is a procedural language supported by the [[PostgreSQL]] [[RDBMS]].\n\nPL/Perl, as an [[imperative programming language]], allows more control than the [[relational algebra]] of [[SQL]].\nPrograms created in the PL/Perl language are called functions and can use most of the features that the [[Perl|Perl programming language]] provides, including common flow control structures and syntax that has incorporated [[regular expressions]] directly.\nThese functions can be evaluated as part of a SQL statement, or in response to a [[Database trigger|trigger]] or [[Constraint (database)|rule]].\n\nThe design goals of PL/Perl were to create a loadable procedural language that:\n\n* can be used to create functions and trigger procedures,\n* adds control structures to the SQL language,\n* can perform complex computations,\n* can be defined to be either [http://www.postgresql.org/docs/current/static/plperl-trusted.html trusted or untrusted] by the server,\n* is easy to use.\n\nPL/Perl is one of many "PL" languages available for PostgreSQL\n[[PL/pgSQL]]\n[http://gborg.postgresql.org/project/pljava/projdisplay.php PL/Java], \n[http://plphp.commandprompt.com/ plPHP], \n[http://www.postgresql.org/docs/current/interactive/plpython.html PL/Python], \n[http://www.joeconway.com/plr/ PL/R], \n[http://raa.ruby-lang.org/list.rhtml?name=pl-ruby PL/Ruby], \n[http://plsh.projects.postgresql.org/ PL/sh], \nand [http://www.postgresql.org/docs/current/interactive/pltcl.html PL/Tcl].\n\n==References==\n* [http://www.postgresql.org/docs/current/static/plperl.html PostgreSQL PL/Perl documentation]\n\n{{DEFAULTSORT:PL Perl}}\n[[Category:Data management]]\n[[Category:PostgreSQL]]\n[[Category:Data-centric programming languages]]']
['Content-oriented workflow models', '38907061', '{{Orphan|date=May 2014}}\n\nThe goal of \'\'\'content-oriented workflow models\'\'\' is to articulate workflow progression by the presence of content units (like data-records/objects/documents).\nMost content-oriented workflow approaches provide a life-cycle model for content units, such that workflow progression can be qualified by conditions on the state of the units.\nMost approaches are research and work in progress and the content models and life-cycle models are more or less formalized.\n\nThe term \'\'content-oriented workflows\'\' is an umbrella term for several scientific workflow approaches, namely "data-driven", "resource-driven", "artifact-centric", "object-aware", and "document-oriented". Thus, the meaning of "content" ranges from simple data attributes to self-contained documents; the term "content-oriented workflows" appeared at first in <ref name="Neumann2010" /> as an umbrella term. Such general term, independent from a specific approach, is necessary to contrast the content-oriented modelling principle with traditional activity-oriented workflow models (like [[Petri net]]s or [[Business Process Model and Notation|BPMN]]) where a workflow is driven by a control flow and where the content production perspective is neglected or even missing.\n\nThe term "content" was chosen to subsume the different levels in granularity of the content units in the respective workflow models; it was also chosen to make associations with [[content management]]. Both terms "artifact-centric" and "data-driven" would also be good candidates for an umbrella term, but each is closely related to a specific approach of a single working group. The "artifact-centric" group itself (i.e. IBM Research) has generalized the characteristics of their approach and has used "information-centric" as an umbrella term in.<ref name="Kumaran2008" /> Yet, the term [[information]] is too unspecific in the context of computer science, thus, "content-orientated workflows" is considered as good compromise.\n\n== Workflow Model Approaches ==\n\n=== Data-driven ===\n\nThe data-driven process structures provides a sophisticated workflow model being specialized on hierarchical write-and-review-processes.\nThe approach provides interleaved synchronization of sub-processes and extends activity diagrams.\nUnfortunately, the COREPRO prototype implementation is not publicly available.\n\nResearch on the project had been ceased. The general idea has been continued by Reichert in form of the [[#Object-aware]] approach.\n\n; Synonyms\n: data-driven process structures / data-driven modeling and coordination\n;Protagonists\n: Dr. Dominic Müller (University of Twente), Joachim Herbst (DaimlerChrysler Research), and Manfred Reichert (at this time [http://wwwhome.cs.utwente.nl/~reichertm/index01.htm Assoc. Prof. at Univ. of Twente], currently [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/manfred-reichert.html Prof. at Ulm Univ.])\n;Organization(s)\n: University of Twente, DaimlerChrysler\n;Period\n: 2005 - 2007\n;Selected publications\n:<ref name="Mueller2006" /><ref name="Mueller2007" />\n;Implementation\n: [http://www.utwente.nl/ewi/is/research/completed_projects/completed_projects/corepro.doc/ COREPRO]\n\n=== Resource-driven ===\n\nThe resource-driven workflow system is an early approach that considered workflows from a content-oriented perspective and emphasizes on the missing support for plain document-driven processes by traditional activity-oriented workflow engines.\nThe resource-driven approach demonstrated the application of database triggers for handling workflow events.\nStill the system implementation is centralized and the workflow schema is statically defined.\nThe project appeared in 2005 but many aspects are considered future work by the authors.\n\nResearch did not continue on the project. Wang completed his PhD thesis in 2009, yet, his thesis does not mention the resource-driven approach to workflow modelling but is about discrete event simulation.\n\n;Synonyms\n: Resource-based Workflows / Document-Driven Workflow Systems\n;Protagonists\n: Jianrui Wang and [http://www.personal.psu.edu/axk41/ Prof. Akhil Kumar]\n;Organization\n: Pennsylvania State University\n;Period\n: 2005 - today\n;Selected publications\n:<ref name="Wang2005" /><ref name="Kumar2010" />\n;Implementation\n: N/A\n\n=== Artifact-centric ===\n{{See also|Artifact-centric business process model}}\n\nThe artifact-centric approach appears as a mature framework for general purpose content-oriented workflows.\nThe distribution of the enterprise application landscape with its business services is considered, yet, the workflow engine itself seems to be centralized.\nThe process enactment seems to be tightly coupled with a technically pre-integrated database management system infrastructure.\nThe latter makes it most suitable for manufacturing process or for organizational processes within a well-defined institutional scope.\nThe approach remains work in progress, still, it is a relatively old and established project on content-oriented workflows.\nFunded by IBM, it has comparably high number of developers.\nIt is a promising approach.\n\n;Synonyms\n: artifact-centric business process models / artifact-based business process (ACP) / artifact-centric workflows\n;Protagonists\n: [http://domino.research.ibm.com/comm/research_people.nsf/pages/hull.index.html Richard Hull] and Dr. Kamal Bhattacharya as well as Cagdas E. Gerede and Jianwen Su\n;Organization\n: IBM (T.J. Watson Research Center, NY)\n;Period\n: 2007 - today\n;Selected publications\n:<ref name="Bhattacharya2007" /><ref name="Calvanese2009" />\n;Implementation\n: [http://domino.research.ibm.com/comm/research_projects.nsf/pages/artifact.index.html ArtiFact]\n\n=== Object-aware ===\n\nThe object-aware approach manages a set of object types and generates forms for creating object instances.\nThe form completion flow is controlled by transitions between object configurations each describing a progressing set of mandatory attributes.\nEach object configuration is named by an object state.\nThe data production flow is user-shifting and it is discrete by defining a sequence of object states.\nThe discussion is currently limited to a centralized system, without any workflows across different organizations.\nHowever, the approach is of great relevance to many domains like concurrent engineering.\nFinally, the object-aware approach and its PHILharmonicFlows system are going to provide general-purpose workflow systems for generic enactment of data production processes.\n\n;Synonyms\n: object-aware process management / datenorientiertes Prozess-Management-System\n;Protagonists\n: [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/vera-kuenzle.html Vera Künzle] and [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/manfred-reichert.html Prof. Manfred Reichert]\n;Organization\n: Ulm University\n;Period\n: 2009 - today\n;Selected publications\n:<ref name="Kuenzle2009" /><ref name="Kuenzle2010" />\n;Implementation\n: [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/research/projects/philharmonic-flows.html PHILharmonicFlows]\n\n=== Distributed Document-oriented ===\n\nDistributed document-oriented process management (dDPM) enables distributed case handling in heterogeneous system environments and it is based on document-oriented [[semantic integration|integration]].\nThe workflow model reflects the paper-based working practice in inter-institutional healthcare scenarios.\nIt targets distributed knowledge-driven ad-hoc workflows, wherein distributed information systems are required to coordinate work with initially unknown sets of actors and activities.\n\nThe distributed workflow engine supports process planning & process history as well as participant management and process template creation with import/export.\nThe workflow engine embeds a functional fusion of 1) group-based instant messaging 2) with a shared work list editor 3) with version control.\nThe software implementation of dDPM is α-Flow which is available as open source.\ndDPM and α-Flow provide a content-oriented approach to schema-less workflows.\n\nThe complete distributed case handling application is provided in form of a single active Document ("&alpha;-Doc").\nThe α-Doc is a case file (as information carrier) with an embedded workflow engine (in form of active properties).\nInviting process participants is equivalent to providing them with a copy of an α-Doc, copying it like an ordinary desktop file.\nAll α-Docs that belong to the same case can synchronize each other, based on the participant management, electronic postboxes, store-and-forward messaging, and an offline-capable synchronization protocol.\n\n;Synonyms\n: distributed document-oriented process management (dDPM), distributed case handling via active documents\n;Protagonists\n: [http://www6.informatik.uni-erlangen.de/people/cpn/ Christoph P. Neumann] and [http://www6.informatik.uni-erlangen.de/people/lenz/ Prof. Richard Lenz]\n;Organization\n: Friedrich-Alexander-Universit&auml;t Erlangen-N&uuml;rnberg\n;Period\n: 2009 - 2012\n;Selected Publications\n:<ref name="Neumann2011" /><ref name="Neumann2012" /> and a PhD thesis <ref name="DissNeumann2012" />\n;Implementation\n: [https://github.com/cpnatwork/alphaflow_dev &alpha;-Flow (open source)]\n\n== Related Concepts ==\n\n=== Content Management ===\n\nThe bandwidth of [[Content management system]]s (CMS) reaches from [[Web content management system]]s (WCMS) and [[Document management system]] (DMS) to [[Enterprise Content Management]] (ECM). Mature DMS products support document production workflows in a basic form, primarily focusing on review cycle workflows concerning a single document. Market leaders are [[Alfresco (software)|Alfresco]] , [[eXo Platform]] and EMC with [[Documentum]].\n\n=== Groupware and Computer-Supported Cooperative Work ===\n[[Groupware]] focuses on messaging (like E-Mail, Chat, and Instant Messaging), shared calendars (e.g. Lotus Notes, Microsoft Outlook with Exchange Server), and conferencing (e.g. Skype).\nGroupware overlaps with [[Computer-supported cooperative work]] (CSCW), that originated from shared multimedia editors (for live drawing/sketching) and synchronous multi-user applications like [[desktop sharing]]. The extensive conceptual claim of CSWC must be put into perspective by its actual solution scope, that is available as the [[CSCW#CSCW Matrix|CSCW Matrix]].\n\n=== Case Handling ===\n\nThe case handling paradigm stems from Prof. van der Aalst and gained momentum in 2005. The core features are:\n(a) provide all information available, i.e. present the case as a whole rather than showing bits and pieces,\n(b) decide about activities on the basis of the information available rather than the activities already executed,\n(c) separate work distribution from authorization and allow for additional types of roles, not just the execute role, and\n(d) allow workers to view and add/modify data before or after the corresponding activities have been executed.\n\nIn healthcare, the flow of a patient between healthcare professionals is considered as a workflow - with activities that include all kinds of diagnostic or therapeutic treatments. The workflow is considered as a case, and workflow management in healthcare is to handle these cases.\n\nCase handling is orthogonal to content-oriented workflows. Some content-oriented workflow approaches are not related to case handling, but, for example, to automated manufacturing. In contrast, systems that are considered to be case handling systems (CHS) but which do not apply a content-oriented workflow model are, for example, BPMone (formerly PROTOS and FLOWer) from Pallas Athena, ECHO from Digital, CMDT from ICL, and Vectus from London Bridge Group. In conclusion, those content-oriented workflow approaches that are tightly related to case handling are the [[#Resource-driven]] workflow model and the [[#Distributed Document-oriented]] workflow model.\n\n;Protagonists\n: [http://wwwis.win.tue.nl/~wvdaalst/ Prof. Wil van der Aalst] and Associate Professor Dr. [http://reijers.com/ Hajo Reijers] (with focus on healthcare)\n;Organization\n: Univ. of Technology, Eindhoven\n;Period\n: 2001 - today\n;Selected publication\n:<ref name="Reijers2003" /><ref name="Aalst2005" />\n\n== See also ==\n* [[Process philosophy]]\n* [[Workflow]]\n* [[Adam Smith]]\n* [[Process control]]\n* [[Process management]]\n* [[Business process]]\n* [[Business process automation]]\n* [[Business process management]]\n* [[Business Process Model and Notation]]\n* [[Advanced case management]]\n* [[Content management system]]\n* [[Web content management system]]\n* [[Document management system]]\n* [[Enterprise Content Management]]\n\n== References ==\n<references>\n<ref name="Neumann2010">Christoph P. Neumann and Richard Lenz: [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5541988 The alpha-Flow Use-Case of Breast Cancer Treatment - Modeling Inter-Institutional Healthcare Workflows by Active Documents]. In: Proc of the 8th Int\'l Workshop on Agent-based Computing for Enterprise Collaboration (ACEC) at the 19th Int\'l Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises (WETICE 2010), Larissa, Greece, June 2010. ([http://www6.informatik.uni-erlangen.de/publications/public/2010/acec2010_neumann_alphaUC.pdf PDF])</ref>\n<ref name="Kumaran2008">Kumaran, S., R. Liu, and F. Wu. [http://www.springerlink.com/content/a837212173812011/ On the Duality of Information-Centric and Activity-Centric Models of Business Processes]. In: Advanced Information Systems Engineering. 2008. p. 32-47.</ref>\n<ref name="Mueller2006">Dominic Müller, Manfred Reichert und Joachim Herbst. [http://www.springerlink.com/content/kpm9454157514825/ Flexibility of Data-Driven Process Structures]. In: Business Process Management Workshops Lecture Notes in Computer Science, 2006, Volume 4103/2006, 181-192. ([http://dbis.eprints.uni-ulm.de/111/2/Mueller06-DPM.pdf PDF])</ref>\n<ref name="Mueller2007">Dominic Müller, Manfred Reichert und Joachim Herbst. [http://www.springerlink.com/content/2771670210653747/ Data-Driven Modeling and Coordination of Large Process Structures]. On the Move to Meaningful Internet Systems 2007: CoopIS, DOA, ODBASE, GADA, and IS Lecture Notes in Computer Science, 2007, Volume 4803/2007, 131-149. ([http://dbis.eprints.uni-ulm.de/116/1/Mueller07-CoopIS.pdf PDF])</ref>\n<ref name="Bhattacharya2007">Kamal Bhattacharya, Cagdas Gerede, Richard Hull, Rong Liu, and Jianwen Su. 2007. [http://dl.acm.org/citation.cfm?id=1793141 Towards formal analysis of artifact-centric business process models]. In Proceedings of the 5th international conference on Business process management (BPM\'07), Gustavo Alonso, Peter Dadam, and Michael Rosemann (Eds.). Springer-Verlag, Berlin, Heidelberg, 288-304. (pages 289ff in: [http://alumni.cs.ucsb.edu/~gerede/research/papers/bghls-bpm07-Artifact.pdf PDF])</ref>\n<ref name="Calvanese2009">Diego Calvanese, Giuseppe De Giacomo, Richard Hull und Jianwen Su. [http://www.springerlink.com/content/t58342lj86807111/ Artifact-Centric Workflow Dominance]. Lecture Notes in Computer Science, 2009, Volume 5900/2009, 130-143. ([http://www.dis.uniroma1.it/~degiacom/papers/2009/ICSOC09.pdf PDF])</ref>\n<ref name="Kuenzle2009">Vera Künzle und Manfred Reichert. [http://www.springerlink.com/content/n6448q47g0474242/ Towards Object-Aware Process Management Systems: Issues, Challenges, Benefits]. In: Enterprise, Business-Process and Information Systems Modeling Lecture Notes in Business Information Processing, 2009, Volume 29, Part 1, Part 5, 197-210, DOI: 10.1007/978-3-642-01862-6_17 ([http://dbis.eprints.uni-ulm.de/526/1/BPMDS09_Kuenzle_Reichert.pdf PDF])</ref>\n<ref name="Kuenzle2010">Künzle, Vera and Reichert, Manfred. 2010. [http://dbis.eprints.uni-ulm.de/647/ Herausforderungen bei der Integration von Benutzern in Datenorientierten Prozess-Management-Systemen]. EMISA Forum, 30 (1). pp. 11-28. ISSN 1610-3351 ([http://dbis.eprints.uni-ulm.de/647/2/KuRe10.pdf PDF])</ref>\n<ref name="Wang2005">Wang, J. and A. Kumar. [http://www.springerlink.com/content/79k8v7terwchn5ct/ A Framework for Document-Driven Workflow Systems]. In: Business Process Management. 2005. p. 285-301. ([http://php.scripts.psu.edu/faculty/a/x/axk41/BPM05-jerry-reprint.pdf PDF])</ref>\n<ref name="Kumar2010">Akhil Kumar und Jianrui Wang. [http://www.springerlink.com/content/n218t085521q1347/ A Framework for Designing Resource-Driven Workflows]. In: Handbook on Business Process Management 1, International Handbooks on Information Systems, 2010, Part III, 419-440.</ref>\n<ref name="Neumann2011">Christoph P. Neumann, Peter K. Schwab, Andreas M. Wahl and Richard Lenz. alpha-Adaptive: Evolutionary Workflow Metadata in Distributed Document-Oriented Process Management. In: Proc of the 4th Int\'l Workshop on Process-oriented Information Systems in Healthcare (ProHealth\'11) in conjunction with the 9th Int\'l Conf on Business Process Management (BPM\'11), Clermont-Ferrand, France, August 2011. ([http://www6.informatik.uni-erlangen.de/publications/public/2011/prohealth2011_neumann.pdf PDF])</ref>\n<ref name="Neumann2012">Christoph P. Neumann and Richard Lenz. The alpha-Flow Approach to Inter-Institutional Process Support in Healthcare. International Journal of Knowledge-Based Organizations. IGI Global, 2012.</ref>\n<ref name="DissNeumann2012">Christoph P. Neumann. [http://www.dr.hut-verlag.de/978-3-8439-0919-8.html Distributed Case Handling]. PhD thesis (German \'Dissertation\'). Friedrich-Alexander-Universit&auml;t Erlangen-N&uuml;rnberg. 2012.</ref>\n<ref name="Reijers2003">Hajo Reijers , Jaap Rigter , Wil Van Der Aalst. [http://www.worldscinet.com/ijcis/12/1203/S0218843003000784.html The Case Handling Case]. International Journal of Cooperative Information Systems (IJCIS), Volume: 12, Issue: 3(2003) pp. 365-391. ([http://is.tm.tue.nl/staff/hreijers/H.A.%20Reijers%20Bestanden/chc.pdf PDF])</ref>\n<ref name="Aalst2005">[[Wil M.P. van der Aalst]], [[Mathias Weske]], Dolf Grünbauer. [http://www.sciencedirect.com/science/article/pii/S0169023X04001296 Case handling: a new paradigm for business process support]. In: Data &amp; Knowledge Engineering, Volume 53, Issue 2, May 2005, Pages 129-162, ISSN 0169-023X, 10.1016/j.datak.2004.07.003. ([http://www.imamu.edu.sa/Scientific_selections/abstracts/AbstratctIT1/Case%20handling%20a%20new%20paradigm%20for%20business%20process%20support.pdf PDF])</ref>\n</references>\n\n{{DEFAULTSORT:Content-oriented Workflows}}\n<!--Categories-->\n[[Category:Workflow technology]]\n[[Category:Data management]]']
['Single source of truth', '6831362', '{{Unreferenced|date=December 2009}}\n\nIn [[information systems]] design and theory, \'\'\'single source of truth\'\'\' (\'\'\'SSOT\'\'\'), is the practice of structuring information models and associated [[Database schema|schemata]] such that every data element is stored exactly once (e.g., in no more than a single row of a single table).  Any possible linkages to this data element (possibly in other areas of the relational schema or even in distant [[federated database|federated databases]]) are by [[Reference (computer science)|reference]] only.  Because all other locations of the data just refer back to the primary "source of truth" location, updates to the data element in the primary location propagate to the entire system without the possibility of a duplicate value somewhere being forgotten.\n\nDeployment of an SSOT architecture is becoming increasingly important in enterprise settings where incorrectly linked duplicate or de-normalized data elements (a direct consequence of intentional or unintentional [[denormalization]] of any explicit data model) poses a risk for retrieval of outdated, and therefore incorrect, information.  A common example would be the [[electronic health record]], where it is imperative to accurately validate patient identity against a single referential repository, which serves as the SSOT.  Duplicate representations of data within the enterprise would be implemented by the use of [[pointer (computer programming)|pointer]]s rather than duplicate database tables, rows, or cells.  This ensures that data updates to elements in the authoritative location are comprehensively distributed to all [[federated database]] constituencies in the larger overall enterprise architecture.{{fact|date=July 2012}}\n\nSSOT systems provide data that is authentic, relevant, and referable.<ref>IBM Smarter Planet - Operational risk management for financial services[http://www.ibm.com/smarterplanet/za/en/banking_technology/nextsteps/solution/Z017038Z16405R75.html]</ref>\n\n==Implementation==\nThe "ideal" implementation of SSOT as described above is rarely possible in most enterprises. This is because many organisations have multiple information systems, each of which needs access to data relating to the same entities (e.g., customer). Often these systems are purchased "off-the-shelf" from vendors and cannot be modified in non-trivial ways. Each of these various systems therefore needs to store its own version of common data or entities, and therefore each system must retain its own copy of a record (hence immediately violating the SSOT approach defined above). For example, an ERP (enterprise resource planning) system (such as SAP or Oracle e-Business Suite) may store a customer record; the CRM (customer relationship management) system also needs a copy of the customer record (or part of it) and the warehouse despatch system might also need a copy of some or all of the customer data (e.g., shipping address). In cases where vendors do not support such modifications, it is not always possible to replace these records with pointers to the SSOT.\n\nFor organisations (with more than one information system) wishing to implement a Single Source of Truth (without modifying all but one master system to store pointers to other systems for all entities), three supporting technologies are commonly used:{{fact|date=July 2012}}\n\n*[[Enterprise service bus]] (ESB)\n*[[Master data management]] (MDM)\n*[[Data warehouse]] (DW)\n\n===Enterprise service bus (ESB)===\nAn enterprise service bus (ESB) allows any number of systems in an organisation to receive updates of data that has changed in another system. To implement a Single Source of Truth, a single source system of correct data for any entity must be identified. Changes to this entity (creates, updates, and deletes) are then published via the ESB; other systems which need to retain a copy of that data subscribe to this update, and update their own records accordingly. For any given entity, the master source must be identified (sometimes called the Golden Record). It should be noted that any given system could publish (be the source of truth for) information on a particular entity (e.g., customer) and also subscribe to updates from another system for information on some other entity (e.g., product).{{fact|date=July 2012}}\n\nAn alternative approach is point-to-point data updates, but these become exponentially more expensive to maintain as the number of systems increases, and this approach is increasingly out of favour as an IT architecture.{{fact|date=July 2012}}\n\n===Master data management (MDM)===\nAn MDM system can act as the source of truth for any given entity that might not necessarily have an alternative "source of truth" in another system. Typically the MDM acts as a hub for multiple systems, many of which could allow (be the source of truth for) updates to different aspects of information on a given entity. For example, the CRM system may be the "source of truth" for most aspects of the customer, and is updated by a call centre operator. However, a customer may (for example) also update their address via a customer service web site, with a different back-end database from the CRM system. The MDM application receives updates from multiple sources, acts as a broker to determine which updates are to be regarded as authoritative (the Golden Record) and then syndicates this updated data to all subscribing systems. The MDM application normally requires an ESB to syndicate its data to multiple subscribing systems.<ref>BAYT Job Site - June 2014[http://www.bayt.com/en/specialties/q/7370/what-are-the-top-business-processes-and-applications-that-need-master-data-management/]</ref>\n[[Customer Data Integration]] (CDI), as a common application of Master Data Management, is sometimes abbreviated CDI-MDM.{{fact|date=July 2012}}\n\n===Data warehouse (DW)===\nWhile the primary purpose of a data warehouse is to support reporting and analysis of data that has been combined from multiple sources, the fact that such data has been combined (according to business logic embedded in the [[Extract, transform, load|data transformation and integration processes]]) means that the data warehouse is often used as a \'\'de facto\'\' SSOT. Generally, however, the data available from the data warehouse is not used to update other systems; rather the DW becomes the "single source of truth" for reporting to multiple stakeholders. In this context, the Data Warehouse is more correctly referred to as a "[[single version of the truth]]" since other versions of the truth exist in its operational data sources (no data originates in the DW;  it is simply a reporting mechanism for data loaded from operational systems).{{fact|date=July 2012}}\n\n==See also==\n*[[Don\'t repeat yourself]] (DRY)\n*[[SOLID (object-oriented design)]]\n*[[Database normalization]]\n*[[Single version of the truth]]\n*[[System of record]]\n\n==References==\n{{reflist}}\n\n==External links==\n\n\n{{DEFAULTSORT:Single Source Of Truth}}\n[[Category:Data modeling]]\n[[Category:Database normalization]]\n[[Category:Data management]]']
['Content repository', '18877735', "A '''content  repository''' or '''content store''' is a database of digital content with an associated set of data management, search and access methods allowing application-independent access to the content, rather like a digital library, but with the ability to store and modify content in addition to searching and retrieving. The content repository acts as the storage engine for a larger application such as a [[Content Management System]] or a [[Document Management System]], which adds a [[user interface]] on top of the repository's [[application programming interface]].<ref>[http://openacs.org/doc/acs-content-repository/design.html Content Repository Design], [http://openacs.org/doc/acs-content-repository/ ACS Content Repository], [http://openacs.org/ OpenACS.org].</ref>\n\n==Advantages provided by repositories==\n\n*Common rules for data access allow many applications to work with the same content without interrupting the data.\n*They give out signals when changes happen, letting other applications using the repository know that something has been modified, which enables collaborative data management.\n*Developers can deal with data using programs that are more compatible with the desktop programming environment.\n*The data model is scriptable when users use a content repository.\n\n== Content repository features ==\nA content repository may provide functionality such as:\n* Add/edit/delete content\n* Hierarchy and sort order management\n* Query / search\n* Versioning\n* Access control\n* Import / export\n* Locking\n* Life-cycle management\n* Retention and holding / records management\n\n== Examples ==\n\n* [[Apache Jackrabbit]]\n* ModeShape\n\n==Applications==\n*[[Content management]]\n*[[Document management system|Document management]]\n*[[Digital asset management]]\n*[[Records management]]\n*[[Revision control]]\n*[[Social collaboration]]\n*[[Web content management system|Web content management]]\n\n== Standards and specification ==\n*[[Content repository API for Java]]\n*[[WebDAV]]\n*[[Content Management Interoperability Services]]\n\n== See also ==\n* [[Information repository]]\n* [[Content (media)]]\n\n== References ==\n{{reflist}}\n\n==External links==\n* [http://db-engines.com/en/ranking/content+store DB-Engines Ranking of Content Stores] by popularity, updated monthly\n\n[[Category:Data management]]\n[[Category:Content management systems]]"]
['Small data', '41530851', '\n==Introduction==\nThe term small data did not exist before the word big data which came into use in the 1990\'s. What we once called data is now called small data.  Small data was not made useless by the advent of big data. Most data we see in our life is small data, and it should not be overlooked in any field. \n\nThis article helps to define small data and to give examples in marketing and recruiting to help in understanding.\n\n==Definition==\n\'\'\'Small data\'\'\' (\'\'sm’aē’āll DH(ə)ta\'\') is data that is \'small\' enough for human comprehension.<ref>{{cite news|author=Rufus Pollock |url=https://www.theguardian.com/news/datablog/2013/apr/25/forget-big-data-small-data-revolution |title=Forget big data, small data is the real revolution &#124; News |newspaper=[[The Guardian]] |date= |accessdate=2016-10-02}}</ref><ref>{{cite web|url=http://jwork.org/main/node/18 |title="Small data". Never heard this term? |website=jWork.ORG |date= |accessdate=2016-10-02}}</ref> It is data in a volume and format that makes it accessible, informative and actionable.<ref>{{cite web|url=http://whatis.techtarget.com/definition/small-data |title=What is small data? - Definition from WhatIs.com |website=Whatis.techtarget.com |date=2016-08-18 |accessdate=2016-10-02}}</ref>\n\nThe term "big data" is about machines and "small data" is about people.<ref>{{cite web|author=Eric Lundquist |url=http://www.eweek.com/enterprise-apps/small-data-analysis-the-next-big-thing-advocates-assert.html/ |title=\'Small Data\' Analysis the Next Big Thing, Advocates Assert |website=Eweek.com |date=2013-09-10 |accessdate=2016-10-02}}</ref> This is to say that eye witness observations or five pieces of related data could be small data. Small data is what we used to think of as data. The only way to comprehend [[Big data]] is to reduce the data into small, visually-appealing objects representing various aspects of large data sets (such as\n[[histogram]], [[chart]]s, and scatter plots). So sometimes big data is simplified to be like small data. \n\nA formal definition of small data has been proposed by Allen Bonde, VP of Innovation at [[Actuate Corporation|Actuate]]: "Small data connects people with timely, meaningful insights (derived from big data and/or “local” sources), organized and packaged – often visually – to be accessible, understandable, and actionable for everyday tasks."<ref>{{cite web|url=http://smalldatagroup.com/2013/10/18/defining-small-data/ |title=Defining Small Data |publisher=Small Data Group |date= |accessdate=2016-10-02}}</ref>\n \nAnother definition of \'\'\'small data\'\'\' is:\n* The small set of specific attributes produced by the [[Internet of Things]]. These are typically a small set of sensor data such as temperature, wind speed, vibration and status.<ref>{{cite web|author= |url=http://www.forbes.com/sites/mikekavis/2015/02/25/forget-big-data-small-data-is-driving-the-internet-of-things/#4a72ffad661b |title=Forget Big Data - Small Data Is Driving The Internet Of Things |website=Forbes.com |date= |accessdate=2016-10-02}}</ref>\n\n\n==Some Examples of Uses in Business==\n\n===Marketing===\n\nBonde has written extensively about the topic for Forbes,<ref>{{cite web|author= |url=http://www.forbes.com/sites/markfidelman/2012/10/30/these-smart-social-apps-bring-big-data-down-to-size/ |title=These Smart, Social Apps Bring Big Data Down to Size |website=Forbes.com |date= |accessdate=2016-10-02}}</ref> Direct Marketing News,<ref>{{cite web|url=http://www.dmnews.com/why-small-data-is-the-next-big-thing-for-marketers/article/308376/ |title=Why Small Data Is the Next Big Thing for Marketers - DMN |website=Dmnews.com |date=2013-08-22 |accessdate=2016-10-02}}</ref> CMO.com<ref>{{cite web|last=Bonde |first=Allen |url=http://www.cmo.com/features/articles/2013/11/20/think_small_time_for.html |title=Think Small: Time For Marketers To Move Beyond The Big Data Hype |website=Cmo.com |date=2013-12-12 |accessdate=2016-10-02}}</ref> and other publications.  \n\nAccording to Martin Lindstrom, in his book, [[Small Data: The Tiny Clues that Uncover Huge Trends|Small Data:]] "{In customer research, small data is} Seemingly insignificant behavioral observations containing very specific attributes pointing towards an unmet customer need. Small data is the foundation for break through ideas or completely new ways to turnaround brands."<ref>{{cite web|url=https://www.martinlindstrom.com/small-data/ |title=Small Data - Martin Lindstrom - Bestselling Author |publisher=Martin Lindstrom |date= |accessdate=2016-10-02}}</ref>\n\n==Conclusion==\nSmall Data is what we used to call data. The hype about Big Data should not cause us to look down on Small Data. Small Data\'s practicality and depth of insight is often better than big data.\n\n==References==\n{{Reflist}}\n\n[[Category:Data management]]']
['Head/tail Breaks', '42933069', '{{Multiple issues|\n{{technical|date=June 2014}}\n{{COI|date=July 2014}}\n}}\n\n[[File:Patterns1024Cities2.jpg|thumb|500px|1024 cities that follow exactly Zipf\'s law, which implies that the first largest city is size 1, the second largest city is size 1/2, the third largest city is size 1/3, ... and the smallest city is size 1/1024. The left pattern is produced by head/tail breaks, while the right one by natural breaks, also known as [[Jenks natural breaks optimization]].]]\n\'\'\'Head/tail breaks\'\'\' is a new [[clustering algorithm]] scheme for data with a heavy-tailed distribution such as [[power laws]] and [[lognormal distributions]]. The heavy-tailed distribution can be simply referred to the scaling pattern of far more small things than large ones, or alternatively numerous smallest, a very few largest, and some in between the smallest and largest. The classification is done through dividing things into large (or called the head) and small (or called the tail) things around the arithmetic mean or average, and then recursively going on for the division process for the large things or the head until the notion of far more small things than large ones is no longer valid, or with more or less similar things left only.<ref name="Jiang1">Jiang, Bin (2013a). "Head/tail breaks: A new classification scheme for data with a heavy-tailed distribution", \'\'The Professional Geographer\'\', 65 (3), 482 – 494.</ref> Head/tail breaks is not just for classification, but also for visualization of big data by keeping the head, since the head is self-similar to the whole. Head/tail breaks can be applied not only to vector data such as points, lines and polygons, but also to raster data like digital elevation model (DEM). \n\n==Motivation==\nThe head/tail breaks is mainly motivated by inability of conventional classification methods such as equal intervals, quantiles, geometric progressions, standard deviation, and natural breaks - commonly known as [[Jenks natural breaks optimization]] for revealing the underlying scaling pattern of far more small things than large ones. Note that the notion of far more small things than large one is not only referred to geometric property, but also to topological and semantic properties. In this connection, the notion should be interpreted as far more unpopular (or less-connected) things than popular (or well-connected) ones, or far more meaningless things than meaningful ones.\n\n==Method==\nGiven some variable X that demonstrates a heavy-tailed distribution, there are far more small x than large ones. Take the average of all xi, and obtain the first mean m1. Then calculate the second mean for those xi greater than m1, and obtain m2. In the same recursive way, we can get m3 depending on whether the ending condition of no longer far more small x than large ones is met. For simplicity, we assume there are three means, m1, m2, and m3. This classification leads to four classes: [minimum, m1], (m1, m2], (m2, m3], (m3, maximum]. In general, it can be represented as a recursive function as follows:\n\n     Recursive function \'\'\'Head/tail Breaks\'\'\':\n     Break the input data (around mean or average) into the head and the tail;  \n     // the head for data values greater the mean\n     // the tail for data values less the mean\n     while (head <= 40%):\n         \'\'\'Head/tail Breaks\'\'\'(head);\n     End Function\n\nThe resulting number of classes is referred to as ht-index, an alternative index to [[fractal dimension]] for characterizing complexity of fractals or geographic features: the higher the ht-index, the more complex the fractals.<ref name="Jiang2">Jiang, Bin and Yin Junjun (2014). "Ht-index for quantifying the fractal or scaling structure of geographic features", \'\'Annals of the Association of American Geographers\'\', 104(3), 530–541.</ref> Recently, a more sensitive ht-index, namely CRG-index,<ref>{{Cite journal|title = CRG Index: A more sensitive ht-index for enabling dynamic views of geographic features|url = http://dx.doi.org/10.1080/00330124.2015.1099448|journal = The Professional Geographer|date = 2015-12-09|issn = 0033-0124|pages = 1–13|volume = 0|issue = 0|doi = 10.1080/00330124.2015.1099448|first = Peichao|last = Gao|first2 = Zhao|last2 = Liu|first3 = Meihui|last3 = Xie|first4 = Kun|last4 = Tian|first5 = Gang|last5 = Liu}}</ref> has been developed, and it is able to capture slight changes which ht-index is unable to. Thus while ht-index is an integer, CRG-index is a real number. A PostgreSQL function for calculating ht-index can be found here.<ref>{{Cite journal|title = A PostgreSQL function for calculating the ht-index|url = https://www.researchgate.net/publication/287533541_A_PostgreSQL_function_for_calculating_the_ht-index?channel=doi&linkId=56777e5b08aebcdda0e962fe&showFulltext=true|date = 2015-01-01|doi = 10.13140/RG.2.1.3041.0324|first = Peichao Gao|last = Kun Tian}}</ref>\n\n=== Threshold or its sensitivity ===\nThe criterion to stop the iterative classification process using the head/tail breaks method is that the remaining data (i.e., the head part) are not heavy-tailed, or simply, the head part is no longer a minority (i.e., the proportion of the head part is no longer less than a threshold such as 40%). This threshold is suggested to be 40% by Jiang et al. (2013),<ref name="Jiang3" /> just as the codes above (i.e., head <= 40%). But sometimes a larger threshold, for example 50% or more, can be used, as Jiang and Yin (2014)<ref name="Jiang2" /> noted in another article: "this condition can be relaxed for many geographic features, such as 50 percent or even more". However, all heads\' percentage on average must be smaller than 40% (or 41, 42%), indicating far more small things than large ones. This sensitivity issue deserves further research in the future.\n\n=== Rank-size plot and RA index ===\nA good tool to display the scaling pattern, or the heavy-tailed distribution, is the rank-size plot, which is a scatter plot to display a set of values according to their ranks. With this tool, a new index <ref>{{Cite journal|last=Gao|first=Peichao|last2=Liu|first2=Zhao|last3=Tian|first3=Kun|last4=Liu|first4=Gang|date=2016-03-10|title=Characterizing Trafﬁc Conditions from the Perspective of Spatial-Temporal Heterogeneity|url=http://www.mdpi.com/2220-9964/5/3/34|journal=ISPRS International Journal of Geo-Information|language=en|volume=5|issue=3|pages=34|doi=10.3390/ijgi5030034}}</ref> termed as the ratio of areas (RA) in a rank-size plot was defined to characterize the scaling pattern. The RA index has been successfully used in the estimation of traffic conditions. However, it should be noted that the RA index can only be used as a complementary method to the ht-index, because it is ineffective to capture the scaling structure of geographic features.\n\n==Applications==\nInstead of more or less similar things, there are far more small things than large ones surrounding us. Given the ubiquity of the scaling pattern, head/tail breaks is found to be of use to statistical mapping, map generalization, cognitive mapping and even perception of beauty\n.<ref name="Jiang3">Jiang, Bin, Liu, Xintao and Jia, Tao (2013). "Scaling of geographic space as a universal rule for map generalization", \'\'Annals of the Association of American Geographers\'\', 103(4), 844 – 855.</ref><ref name="Jiang4">Jiang, Bin (2013b). "The image of the city out of the underlying scaling of city artifacts or locations", \'\'Annals of the Association of American Geographers\'\', 103(6), 1552-1566.</ref><ref name="Jiang5">Jiang, Bin and Sui, Daniel (2014). "A new kind of beauty out of the underlying scaling of geographic space", \'\'The Professional Geographer\'\', 66(4), 676–686</ref> It helps visualize big data, since big data are likely to show the scaling property of far more small things than large ones. The visualization strategy is to recursively drop out the tail parts until the head parts are clear or visible enough.<ref name="Jiang6">Jiang, Bin (2015). "Head/tail breaks for visualization of city structure and dynamics", \'\'Cities\'\', 43, 69 - 77.</ref> In addition, it helps delineate cities or natural cities to be more precise from various geographic information such as street networks, social media geolocation data, and nighttime images.\n\n=== Characterizing the imbalance ===\nAs the head/tail breaks method can be used iteratively to obtain head parts of a data set, this method actually captures the underlying hierarchy of the data set. For example, if we divide the array (19, 8, 7, 6, 2, 1, 1, 1, 0) with the head/tail breaks method, we can get two head parts, i.e., the first head part (19, 8, 7, 6) and the second head part (19). These two head parts as well as the original array form a three-level hierarchy:\n\nthe 1st level (19),\n\nthe 2nd level (19, 8, 7, 6), and\n\nthe 3rd level (19, 8, 7, 6, 2, 1, 1, 1, 0).\n\nThe number of levels of the above-mentioned hierarchy is actually a characterization of the imbalance of the example array, and this number of levels has been termed as the ht-index.<ref name="Jiang2" /> With the ht-index, we are able to compare degrees of imbalance of two data sets. For example, the ht-index of the example array (19, 8, 7, 6, 2, 1, 1, 1, 0) is 3, and the ht-index of another array (19, 8, 8, 8, 8, 8, 8, 8, 8) is 2. Therefore, the degree of imbalance of the former array is higher than that of the latter array.\n[[File:Natural_cities_of_Germany,_created_from_points_of_interest.jpg|thumb|250px|right|The left panel pattern contains 50,000 natural cities, which can be put into 7 hierarchical levels. It looks like a hair ball. Instead of showing all the 7 hierarchical levels, we show 4 top levels, by dropping out 3 low levels. Now with the right panel, the scaling pattern of far more small cities than large ones emerges. It is important to note that the right pattern (or the remaining part after dropping out the tails) is self-similar to the whole (or the left pattern). Thus the right pattern reflects the underlying structure of the left one, and enables us to see the whole.]]\n[[File:Headtail breaks of American DEM.jpg|thumb|250px|The scaling pattern of US terrain surface is distorted by the natural breaks, but revealed by the head/tail breaks.]]\n\n=== Delineating natural cities ===\nThe term ‘natural cities’ refers to the human settlements or human activities in general on Earth’s surface that are naturally or objectively defined and delineated from massive geographic information based on head/tail division rule, a non-recursive form of head/tail breaks.<ref name="Jiang7">Jiang, Bin and Miao, Yufan (2015). "The evolution of natural cities from the perspective of location-based social media", \'\'The Professional Geographer\'\', 67(2), 295 - 306.</ref><ref name="Long">Long, Ying (2016). "Redefining Chinese city system with emerging new data", \'\'Applied Geography\'\', 75, 36 - 48.</ref>  Such geographic information could be from various sources, such as massive street junctions <ref name="Long"/> and street ends, a massive number of street blocks, nighttime imagery and social media users’ locations etc. Distinctive from conventional cities, the adjective ‘natural’ could be explained not only by the sources of natural cities, but also by the approach to derive them. Natural cities are derived from a meaningful cutoff averaged from a massive amount of units extracted from geographic information.<ref name="Jiang6"/> Those units vary according to different kinds of geographic information, for example the units could be area units for the street blocks and pixel values for the nighttime images. A \'\'\'[http://www.arcgis.com/home/item.html?id=47b1d6fdd1984a6fae916af389cdc57d natural cities model]\'\'\' has been created using ArcGIS model builder,<ref name="Ren">Ren, Zheng (2016). "Natural cities model in ArcGIS", \'\'http://www.arcgis.com/home/item.html?id=47b1d6fdd1984a6fae916af389cdc57d\'\'.</ref> it follows the same process of deriving natural cities from location-based social media,<ref name="Jiang7"/> namely, building up huge triangular irregular network (TIN) based on the point features (street nodes in this case) and regarding the triangles which are smaller than a mean value as the natural cities.\n\n=== Color rendering DEM ===\nCurrent color renderings for DEM or density map are essentially based on conventional classifications such as natural breaks or equal intervals, so they disproportionately exaggerate high elevations or high densities. As a matter of fact, there are not so many high elevations or high-density locations.<ref name="Jiang8">Jiang, Bin (2015). "Geospatial analysis requires a different way of thinking: The problem of spatial heterogeneity", \'\'GeoJournal\'\', 80(1), 1-13.</ref> It was found that coloring based head/tail breaks is more favorable than those by other classifications <ref name="Wu">Wu, Jou-Hsuan (2015). "Examining the new kind of beauty using the human being as a measuring instrument", \'\'http://www.diva-portal.org/smash/get/diva2:805296/FULLTEXT01.pdf\'\'.</ref>\n\n== Software implementations ==\nThe following implementations are available under [[Free and open-source software|Free/Open Source Software]] licenses.\n* \'\'\'[https://github.com/digmaa/HeadTailBreaks HT calculator]\'\'\': a winform application for obtaining related metrics of head/tail breaks applying on a single data array.\n* \'\'\'[http://jsfiddle.net/mhkeller/5yATK/ HT in JavaScript]\'\'\': a JavaScript implementation for applying head/tail breaks on a single data array.\n* \'\'\'[http://fromto.hig.se/~bjg/axwoman/ HT Mapping tool]\'\'\': a function in the free plug-in Axwoman 6.3 to [[ArcMap]] 10.2 that conducts geo-data symbolization automatically based on the head/tail breaks classification.\n* \'\'\'[https://github.com/chad-m/head_tail_breaks_algorithm HT in Python]\'\'\': Python and JavaScript code for the head/tail breaks algorithm. It\'s works great for choropleth map coloring.\n\n==References==\n{{one author|date=June 2014}}\n{{Reflist}}\n\n==Further reading==\n{{Further reading cleanup|date=June 2014}}\n* Lin, Yue (2013), A comparison study on natural and head/tail breaks involving digital elevation models. http://www.diva-portal.org/smash/get/diva2:658963/FULLTEXT02.pdf\n* Wu, Jou-Hsuan (2015), The mirror of the self test: http://sharon19891101.wix.com/mirror-of-the-self\n\n{{DEFAULTSORT:Head tail Breaks}}\n[[Category:Data management]]\n[[Category:Cartography]]']
['Query Rewriting', '43435003', '{{Orphan|date=August 2014}}\n\n\'\'\'Query Rewriting\'\'\' is a technique used in mediation based [[data integration]] systems for translating the queries formulated over the mediated schema to a query over the various sources by making use of the view definitions.<ref name="refone">{{cite conference | author=[[Alon Y. Halevy]] | title=Answering queries using views: A survey | booktitle=The VLDB Journal | year=2001 | pages=270–294}}</ref> Mediation based data integration system hides from the end user the underlying heterogeneity of the various data providing sources linked to it by providing a uniform query interface in the form of a mediated schema. This schema is also referred to as the global schema whereas the schema of the various data sources is collectively referred to as the local schema. The local schema and the mediated schema are mapped to each other using view definitions. The queries formulated on the mediated schema cannot be directly used to query the sources. Therefore query rewriting translates such a query formulated over the global schema to a query over the various data sources. Examples include bucket algorithm, Minicon algorithm, inverse rules algorithm.<ref name="refone"/> This rewritten query is then evaluated to obtain the query response making use of the data obtained by querying the data sources.\n\n==See also==\n* [[Data integration]]\n* [[Schema Matching]]\n* [[Data Virtualization]]\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Query Rewriting}}\n[[Category:Data management]]']
['Semantic heterogeneity', '43972057', '{{About|semantic differences in data|other uses|Heterogeneity (disambiguation)}}\n\n\'\'\'Semantic heterogeneity\'\'\' is when [[database schema]] or [[Data set|datasets]] for the same domain are developed by independent parties, resulting in differences in meaning and interpretation of data values.<ref>{{cite journal |title=Why your data won\'t mix |author=Alon Halevy|journal=Queue |volume=3 |issue=8 |year=2005 |url=http://queue.acm.org/detail.cfm?id=1103836}}</ref> Beyond [[Data structure|structured data]], the problem of semantic heterogeneity is compounded due to the flexibility of [[semi-structured data]] and various [[Tag (metadata)|tagging]] methods applied to documents or [[unstructured data]]. Semantic heterogeneity is one of the more important sources of differences in [[Heterogeneous database system|heterogeneous datasets]].\n\nYet, for multiple data sources to [[Interoperability|interoperate]] with one another, it is essential to reconcile these [[Semantics|semantic]] differences. Decomposing the various sources of semantic heterogeneities provides a basis for understanding how to map and transform data to overcome these differences.\n\n== Classification ==\n\nOne of the first known classification schemes applied to [[Semantic data model|data semantics]] is from William Kent more than two decades ago.<ref>{{cite conference |title=The many forms of a single fact |author=William Kent |conference=Proceedings of the IEEE COMPCON |date=February 27 – March 3, 1989 |location=San Francisco |number=HPL-SAL-88-8, Hewlett-Packard Laboratories, Oct. 21, 1988 | at=13 pp. |url=http://www.bkent.net/Doc/manyform.htm}}</ref> Kent\'s approach dealt more with structural [[Data mapping|mapping]] issues than differences in meaning, which he pointed to [[Data dictionary|data dictionaries]] as potentially solving.\n\nOne of the most comprehensive classifications is from Pluempitiwiriyawej and Hammer, "Classification Scheme for Semantic and Schematic Heterogeneities in XML Data Sources".<ref>{{cite news |title=A classification scheme for semantic and schematic heterogeneities in XML data sources |author=Charnyote Pluempitiwiriyawej and Joachim Hammer |publisher=University of Florida |at=Technical Report TR00-004 |location=Gainesville, Florida |date=September 2000 |url=https://cise.ufl.edu/tr/DOC/REP-2000-396.pdf}}</ref> They classify heterogeneities into three broad classes:\n\n* \'\'[[Data structure|Structural]]\'\' conflicts arise when the schema of the sources representing related or overlapping data exhibit discrepancies. Structural conflicts can be detected when comparing the underlying schema. The class of structural conflicts includes generalization conflicts, aggregation conflicts, internal path discrepancy, missing items, element ordering, constraint and type mismatch, and naming conflicts between the element types and attribute names.\n* \'\'[[Data domain|Domain]]\'\' conflicts arise when the semantics of the data sources that will be integrated exhibit discrepancies. Domain conflicts can be detected by looking at the information contained in the schema and using knowledge about the underlying data domains. The class of domain conflicts includes schematic discrepancy, scale or unit, precision, and data representation conflicts.\n* \'\'[[Data]]\'\' conflicts refer to discrepancies among similar or related data values across multiple sources. Data conflicts can only be detected by comparing the underlying sources. The class of data conflicts includes ID-value, missing data, incorrect spelling, and naming conflicts between the element contents and the attribute values.\n\nMoreover, mismatches or conflicts can occur between set elements (a "population" mismatch) or attributes (a "description" mismatch).\n\nMichael Bergman expanded upon this schema by adding a fourth major explicit category of language, and also added some examples of each kind of semantic heterogeneity, resulting in about 40 distinct potential categories <ref>{{cite web |title=Sources and classification of semantic heterogeneities |author=M.K. Bergman |website=AI3:::Adaptive Information |date=6 June 2006 |accessdate=28 September 2014 |url=http://www.mkbergman.com/232/sources-and-classification-of-semantic-heterogeneities/}}</ref>\n.<ref>{{cite web |title=Big structure and data interoperability |author=M.K. Bergman |website=AI3:::Adaptive Information |date=12 August 2014 |accessdate=28 September 2014 |url=http://www.mkbergman.com/1782/big-structure-and-data-interoperability/}}</ref> This table shows the combined 40 possible sources of semantic heterogeneities across sources:\n\n{|  style="text-align: left; width: 100%;" border="1" cellpadding="3" cellspacing="0"\n|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Class\n|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Category\n|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Subcategory\n|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Examples\n|-\n| rowspan="8" colspan="1" |\n\'\'\'[[Language]]\'\'\'\n| rowspan="4" colspan="1" |\n[[Character encoding|Encoding]]\n| Ingest Encoding Mismatch\n|\nFor example, [[US-ASCII|ASCII]] \'\'v\'\' [[UTF-8]]\n|-\n| Ingest Encoding Lacking\n| Mis-recognition of tokens because not being parsed with the proper encoding\n|-\n| Query Encoding Mismatch\n| For example, ASCII \'\'v\'\' UTF-8 in search\n|-\n| Query Encoding Lacking\n| Mis-recognition of search tokens because not being parsed with the proper encoding\n|-\n| rowspan="4" colspan="1" | Languages\n| Script Mismatch\n| Variations in how parsers handle, say, stemming, white spaces or hyphens\n|-\n| Parsing / Morphological Analysis Errors (many)\n| Arabic languages (right-to-left) \'\'v\'\' Romance languages (left-to-right)\n|-\n| Syntactical Errors (many)\n|\nAmbiguous sentence references, such as \'\'I\'m glad I\'m a man, and so is Lola\'\' ([[Lola (song)|Lola]] by [[Ray Davies]] and the [[Kinks]])\n|-\n| Semantics Errors (many)\n| River \'\'bank\'\' \'\'v\'\' money \'\'bank\'\' \'\'v\'\' billiards \'\'bank\'\' shot\n|-\n| rowspan="17" colspan="1" | \'\'\'Conceptual\'\'\'\n| rowspan="5" colspan="1" | Naming\n| Case Sensitivity\n| Uppercase \'\'v\'\' lower case \'\'v\'\' Camel case\n|-\n|\n[[Synonym]]s\n| United States \'\'v\'\' USA \'\'v\'\' America \'\'v\'\' Uncle Sam \'\'v\'\' Great Satan\n|-\n|\n[[Acronym]]s\n| United States \'\'v\'\' USA \'\'v\'\' US\n|-\n|\n[[Homonym]]s\n| Such as when the same name refers to more than one concept, such as Name referring to a person \'\'v\'\' Name referring to a book\n|-\n| Misspellings\n| As stated\n|-\n| rowspan="1" colspan="2" | Generalization / Specialization\n| When single items in one schema are related to multiple items in another schema, or vice versa. For example, one schema may refer to "phone" but the other schema has multiple elements such as "home phone", "work phone" and "cell phone"\n|-\n| rowspan="2" colspan="1" | Aggregation\n| Intra-aggregation\n| When the same population is divided differently (such as, Census \'\'v\'\' Federal regions for states, England \'\'v\'\' Great Britain \'\'v\'\' United Kingdom, or full person names \'\'v\'\' first-middle-last)\n|-\n| Inter-aggregation\n| May occur when sums or counts are included as set members\n|-\n| rowspan="1" colspan="2" | Internal Path Discrepancy\n| Can arise from different source-target retrieval paths in two different schemas (for example, hierarchical structures where the elements are different levels of remove)\n|-\n| rowspan="4" colspan="1" | Missing Item\n| Content Discrepancy\n| Differences in set enumerations or including items or not (say, US territories) in a listing of US states\n|-\n| Missing Content\n| Differences in scope coverage between two or more datasets for the same concept\n|-\n| Attribute List Discrepancy\n| Differences in attribute completeness between two or more datasets\n|-\n| Missing Attribute\n| Differences in scope coverage between two or more datasets for the same attribute\n|-\n| rowspan="2" colspan="2" | Item Equivalence\n|\nWhen two types (classes or sets) are asserted as being the same when the scope and reference are not (for example, [[Berlin]] the city \'\'v\'\'  [[States of Germany#Subdivisions|Berlin]] the official city-state)\n|-\n|\nWhen two individuals are asserted as being the same when they are actually distinct (for example, [[John F. Kennedy]] the president \'\'v\'\'  [[USS John F. Kennedy (CV-67)|\'\'John F. Kennedy\'\']] the aircraft carrier)\n|-\n| rowspan="1" colspan="2" | Type Mismatch\n| When the same item is characterized by different types, such as a person being typed as an animal \'\'v\'\' human being \'\'v\'\' person\n|-\n| rowspan="1" colspan="2" | Constraint Mismatch\n| When attributes referring to the same thing have different cardinalities or disjointedness assertions\n|-\n| rowspan="9" colspan="1" |\n\'\'\'[[Domain of discourse|Domain]]\'\'\'\n| rowspan="4" colspan="1" | Schematic Discrepancy\n| Element-value to Element-label Mapping\n| rowspan="4" colspan="1" | One of four errors that may occur when attribute names (say, Hair \'\'v\'\' Fur) may refer to the same attribute, or when same attribute names (say, Hair \'\'v\'\' Hair) may refer to different attribute scopes (say, Hair \'\'v\'\' Fur) or where values for these attributes may be the same but refer to different actual attributes or where values may differ but be for the same attribute and putative value. <br /><br /> Many of the other semantic heterogeneities herein also contribute to schema discrepancies\n|-\n| Attribute-value to Element-label Mapping\n|-\n| Element-value to Attribute-label Mapping\n|-\n| Attribute-value to Attribute-label Mapping\n|-\n| rowspan="2" colspan="1" | Scale or Units\n| Measurement Type\n| Differences, say, in the metric \'\'v\'\' English measurement systems, or currencies\n|-\n| Units\n| Differences, say, in meters \'\'v\'\' centimeters \'\'v\'\' millimeters\n|-\n| rowspan="1" colspan="2" | Precision\n| For example, a value of 4.1 inches in one dataset \'\'v\'\' 4.106 in another dataset\n|-\n| rowspan="2" colspan="1" |\n[[Data representation]]\n| Primitive Data Type\n|\nConfusion often arises in the use of literals \'\'v\'\' [[Uniform resource identifier|URIs]] \'\'v\'\' object types\n|-\n| Data Format\n| Delimiting decimals by period \'\'v\'\' commas; various date formats; using exponents or aggregate units (such as thousands or millions)\n|-\n| rowspan="8" colspan="1" |\n\'\'\'[[Data]]\'\'\'\n| rowspan="5" colspan="1" | Naming\n| Case Sensitivity\n| Uppercase \'\'v\'\' lower case \'\'v\'\' Camel case\n|-\n| Synonyms\n| For example, centimeters \'\'v\'\' cm\n|-\n| Acronyms\n| For example, currency symbols \'\'v\'\' currency names\n|-\n| Homonyms\n| Such as when the same name refers to more than one attribute, such as Name referring to a person \'\'v\'\' Name referring to a book\n|-\n| Misspellings\n| As stated\n|-\n| rowspan="1" colspan="2" | ID Mismatch or Missing ID\n| URIs can be a particular problem here, due to actual mismatches but also use of name spaces or not and truncated URIs\n|-\n| rowspan="1" colspan="2" | Missing Data\n|\nA common problem, more acute with closed world approaches than with [[Open world assumption|open world ones]]\n|-\n| rowspan="1" colspan="2" | Element Ordering\n| Set members can be ordered or unordered, and if ordered, the sequences of individual members or values can differ\n|}\n\nA different approach toward classifying semantics and integration approaches is taken by [[Amit Sheth|Sheth]] et al.<ref>{{cite journal |title=Semantics for the semantic Web: the implicit, the formal and the powerful | author1=Amit P. Sheth|author2=Cartic Ramakrishnan|author3=Christopher Thomas|journal=Int’l Journal on Semantic Web & Information Systems |volume=1 |issue=1 |pages=1–18 |date=2005 |url=http://www.informatik.uni-trier.de/~ley/db/journals/ijswis/ijswis1.html}}</ref> Under their concept, they split semantics into three forms: implicit, formal and powerful. Implicit semantics are what is either largely present or can easily be extracted; formal languages, though relatively scarce, occur in the form of [[Ontology (information science)|ontologies]] or other [[description logic]]s; and powerful (soft) semantics are fuzzy and not limited to rigid set-based assignments. Sheth et al.\'s main point is that [[first-order logic]] (FOL) or description logic is inadequate alone to properly capture the needed semantics.\n\n== Relevant applications ==\n\nBesides data interoperabiity, relevant areas in [[information technology]] that depend on reconciling semantic heterogeneities include [[data mapping]], [[semantic integration]], and [[enterprise information integration]], among many others. From the conceptual to actual data, there are differences in perspective, vocabularies, measures and conventions once any two data sources are brought together. Explicit attention to these semantic heterogeneities is one means to get the information to integrate or interoperate.\n\nA mere twenty years ago, information technology systems expressed and stored data in a multitude of formats and systems. The Internet and Web protocols have done much to overcome these sources of differences. While there is a large number of categories of semantic heterogeneity, these categories are also patterned and can be anticipated and corrected. These patterned sources inform what kind of work must be done to overcome semantic differences where they still reside.\n\n==See also==\n* [[Data integration]]\n* [[Data mapping]]\n* [[Enterprise information integration]]\n* [[Heterogeneous database system]]\n* [[Interoperability]]\n* [[Ontology-based data integration]]\n* [[Schema matching]]\n* [[Semantic integration]]\n* [[Semantic matching]]\n* [[Semantics]]\n\n==References==\n<references/>\n\n==Further reading==\n* [http://wiki.opensemanticframework.org/index.php/Classification_of_Semantic_Heterogeneity Classification of semantic heterogeneity]\n\n[[Category:Data management]]\n[[Category:Interoperability]]\n[[Category:Knowledge management]]\n[[Category:Semantics]]']
['Rtolap', '2878165', '{{multiple issues|\n{{Unreferenced|date=October 2008}}\n{{Original research|date=October 2008}}\n}}\n\n==RTOLAP - Real Time OLAP==\n\nWhilst many [[OLAP]] Servers like [[Microsoft Analysis Services]] store pre-calculating consolidations and calculated elements to achieve rapid response times. A Real Time OLAP Server will calculate the values on the fly, when they are required. \nThe essential characteristic of RTOLAP system is in holding all the data in RAM.\n\nIt is a protocol which analyzes fly values when required. It saves every bit of information in RAM. The calculations are executed in a “right-away” manner which reduces the setback linked with “information outburst” since it only saves information under the RAM size standard.\n\n== Advantages ==\n* Since precalculated values aren\'t stored, the size of a cube in an RTOLAP system is smaller than of an OLAP product which resorts to precalculation. RTOLAP often reduces the problem which may be associated with "Data explosion", by  means of storing less data. \n* RTOLAP essentially performs calculations "just-in-time" by only calculating values when they are needed space can be saved, since in a precalculated system, a great deal of calculations will be stored which may well never be called up.\n* Incremental updates are available once they are loaded, and any modifications to data will flow through the system immediately. With RTOLAP when a change is made, everyone sees the result. This isn\'t a unique characteristic of RTOLAP, since other OLAP systems (e.g. [[SAS Institute]], [[Microsoft Analysis Services]], [[MicroStrategy]]) behave the same way.\n\n== Disadvantages ==\n* Since RTOLAP stores the entire cube in RAM, it doesn\'t scale to the data volumes larger than the RAM size\n* Performance of queries can be slower since the values need to be calculated on the fly instead of being accessed from the precalculated storage\n\n[[Category:Data management]]\n[[Category:Information technology management]]\n[[Category:Online analytical processing]]']
['Data lineage', '44783487', '{{peacock|date=May 2015}}\n\n\'\'\'Data lineage\'\'\' is defined as a data life cycle that includes the data\'s origins and where it moves over time.<ref>http://www.techopedia.com/definition/28040/data-lineage</ref> It describes what happens to data as it goes through diverse processes. It helps provide visibility into the analytics pipeline and simplifies tracing errors back to their sources. It also enables replaying specific portions or inputs of the dataflow for step-wise debugging or regenerating lost output. In fact, database systems have used such information, called data provenance, to address similar validation and debugging challenges already.<ref name="DeSoumyarupa">De, Soumyarupa. (2012). Newt : an architecture for lineage based replay and debugging in DISC systems. UC San Diego: b7355202. Retrieved from: https://escholarship.org/uc/item/3170p7zn</ref>\n\n\'\'\'Data Lineage\'\'\' provides a visual representation to discover the data flow/movement from its source to destination via various changes and hops on its way in the enterprise environment. \n\'\'Data lineage\'\' represents: how the data hops between various data points, how the data gets transformed along the way, how the representation and parameters change, and how the data splits or converges after each hop. Easier representation of the \'\'Data Lineage\'\' can be shown with dots and lines, where dot represents a data container for data point(s) and lines connecting them represents the transformation(s) the data point under goes, between the data containers.\n\n<!-- Deleted image removed: [[File:DataLineage Dots lines.png]] -->\n\nRepresentation of \'\'Data Lineage\'\' broadly depends on scope of the \'\'[[Meta-data management|Metadata Management]]\'\' and reference point of interest. \'\'Data Lineage\'\' provides sources of the data and intermediate data flow hops from the reference point with \'\'\'Backward data lineage\'\'\', leads to the final destination\'s data points and its intermediate data flows with \'\'\'Forward data lineage\'\'\'.  These views can be combined with \'\'\'End to End Lineage\'\'\' for a reference point that provides complete audit trail of that data point of interest from source(s) to its final destination(s). As the data points or hops increases, the complexity of such representation becomes incomprehensible. Thus, the best feature of the data lineage view would be to be able to simplify the view by temporarily \'\'Masking\'\' unwanted peripheral data points. Tools that have the \'\'\'masking\'\'\' feature enables scalability of the view and enhances analysis with best user experience for both Technical and business users alike.\n\n\'\'\'Scope of the data lineage\'\'\' determines the volume of metadata required to represent its data lineage. Usually, [[Data governance|Data Governance]], and [[Data management|Data Management]] determines the scope of the data lineage based on their [[regulation]]s, \'\'enterprise data management strategy\'\', \'\'data impact\'\', \'\'reporting attributes\'\', and \'\'critical [[data element]]s\'\' of the organization.\n\n\'\'Data Lineage\'\' provides the audit trail of the data points at the lowest granular level,but presentation of the lineage may be done at various zoom levels to simplify the vast information, similar to the \'\'analytic web maps\'\'. \'\'Data Lineage\'\' can be visualized at various levels based on the granularity of the view. At a very high level \'\'data lineage\'\' provides what systems the data interacts before it reaches destination. As the granularity increases it goes up to the data point level where it can provide the details of the data point and its historical behavior, attribute properties, and trends and \'\'[[Data quality|Data Quality]]\'\' of the data passed through that specific data point in the \'\'data lineage\'\'.\n\n\'\'[[Data governance|Data Governance]]\'\' plays a key role in metadata management for guidelines, strategies, policies, implementation. \'\'[[Data quality|Data Quality]]\'\', and \'\'[[Master data management|Master Data Management]]\'\' helps in enriching the data lineage with more business value. Even though the final representation of \'\'Data lineage\'\' is provided in one interface but the way the metadata is harvested and exposed to the data lineage \'\'\'[[Graphical user interface|User Interface (UI)]]\'\'\' could be entirely different. Thus, \'\'Data lineage\'\' can be broadly divided into three categories based on the way metadata is harvested:Data lineage involving \'\'software packages for structured data\'\', \'\'Programming Languages\'\', and \'\'Big Data\'\'.\n\n\'\'Data lineage\'\' expects to view at least the technical metadata involving the data points and its various transformations. Along with technical data, \'\'Data Lineage\'\' may enrich the metadata with their corresponding Data Quality results,Reference Data values, [[Data model|Data Models]], [[Glossary of business and management terms|Business Vocabulary]], [[Data steward|People]], [[Program management|Programs]], and [[Enterprise system|Systems]] linked to the data points and transformations. Masking feature in the data lineage visualization allows the tools to incorporate all the enrichments that matter for the specific use case.  \nMetadata normalization may be done in data lineage to represent disparate systems into one common view.\n\n\'\'\'Data provenance\'\'\' documents the inputs, entities, systems, and processes that influence data of interest, in effect providing a historical record of the data and its origins. The generated evidence supports essential forensic activities such as data-dependency analysis, error/compromise detection and recovery, and auditing and compliance analysis. "\'\'\'Lineage\'\'\' is a simple type of \'\'\'why provenance\'\'\'."<ref name="DeSoumyarupa"/>\n\n==Case for Data Lineage==\nThe world of [[big data]] is changing dramatically right before our eyes. Statistics say that Ninety percent (90%) of the world’s data has been created in the last two years alone.<ref>http://newstex.com/2014/07/12/thedataexplosionin2014minutebyminuteinfographic/</ref> This explosion of data has resulted in the ever-growing number of systems and automation at all levels in all sizes of organizations.\n\nToday, distributed systems like Google [[Map Reduce]],<ref>Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on\nlarge clusters. Commun. ACM, 51(1):107–113, January 2008.</ref> Microsoft Dryad,<ref>Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly.\nDryad: distributed data-parallel programs from sequential building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference onComputer\nSystems 2007, EuroSys ’07, pages 59–72, New York, NY, USA, 2007. ACM.</ref> Apache Hadoop <ref>Apache Hadoop. http://hadoop.apache.org.</ref>(an open-source project) and Google Pregel<ref>Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. Pregel: a system for largescale graph processing. In Proceedings of the 2010 international conference on Managementof data, SIGMOD ’10, pages 135–146, New York, NY, USA, 2010. ACM.</ref> provide such platforms for businesses and users. However, even with these systems, [[big data]] analytics can take several hours, days or weeks to run, simply due to the data volumes involved. For example, a ratings prediction algorithm for the Netflix Prize challenge took nearly 20 hours to execute on 50 cores, and a large-scale image processing task to estimate geographic information took 3 days to complete using 400 cores.<ref>Shimin Chen and Steven W. Schlosser. Map-reduce meets wider varieties of\napplications. Technical report, Intel Research, 2008.</ref> "The Large Synoptic Survey Telescope is expected to generate terabytes of data every night and eventually store more than 50 petabytes, while in the bioinformatics sector, the largest genome 12 sequencing houses in the world now store petabytes of data apiece."<ref>The data deluge in genomics. https://www-304.ibm.com/connections/blogs/ibmhealthcare/entry/data overload in genomics3?lang=de, 2010.</ref>\nDue to the humongous size of the [[big data]], there could be features in the data that are not considered in the machine learning algorithm, possibly even outliers. It is very difficult for a data scientist to trace an unknown or an unanticipated result.\n\n===Big Data Debugging===\n\n[[Big data]] analytics is the process of examining large data sets to uncover hidden patterns, unknown correlations, market trends, customer preferences and other useful business information. They apply machine learning algorithms etc. to the data which transform the data. Due to the humongous size of the data, there could be unknown features in the data, possibly even outliers. It is pretty difficult for a data scientist to actually debug an unexpected result.\n\nThe massive scale and unstructured nature of data, the complexity of these analytics pipelines, and long runtimes pose significant manageability and debugging challenges. Even a single error in these analytics can be extremely difficult to identify and remove. While one may debug them by re-running the entire analytics through a debugger for step-wise debugging, this can be expensive due to the amount of time and resources needed. Auditing and data validation are other major problems due to the growing ease of access to relevant data sources for use in experiments, sharing of data between scientific communities and use of third-party data in business enterprises.<ref>Yogesh L. Simmhan, Beth Plale, and Dennis Gannon. A survey of data prove-\nnance in e-science. SIGMOD Rec., 34(3):31–36, September 2005.</ref><ref name="IanFosterJensVockler">Ian Foster, Jens Vockler, Michael Wilde, and Yong Zhao. Chimera: A Virtual Data System for Representing, Querying, and Automating Data Derivation. In 14th International Conference on Scientific and Statistical Database Management, July 2002.</ref><ref name="Benjamim&Luiz">Benjamin H. Sigelman, Luiz Andr Barroso, Mike Burrows, Pat Stephenson, Manoj Plakal, Donald Beaver, Saul Jaspan, and Chandan Shanbhag. Dapper, a large-scale distributed systems tracing infrastructure. Technical report, Google Inc, 2010.</ref><ref name="PeterBuneman">Peter Buneman, Sanjeev Khanna, and Wang Chiew Tan. Data provenance: Some basic issues. In Proceedings of the 20th Conference on Foundations of SoftwareTechnology and Theoretical Computer Science, FST TCS 2000, pages 87–93, London, UK, UK, 2000. Springer-Verlag</ref> These problems will only become larger and more acute as these systems and data continue to grow. As such, more cost-efficient ways of analyzing [[data-intensive computing|data intensive scalable computing]] (DISC) are crucial to their continued effective use.\n\n===Challenges in [[Big Data]] Debugging===\n\n====Massive Scale====\nAccording to an EMC/IDC study:<ref>http://www.emc.com/about/news/press/2012/20121211-01.htm</ref>\n* 2.8ZB of data were created and replicated in 2012,\n* the digital universe will double every two years between now and 2020, and\n* there will be approximately 5.2TB of data for every man, woman and child on earth in 2020.\nWorking with this scale of data has become very challenging.\n\n====Unstructured Data====\nThe phrase [[unstructured data]] usually refers to information that doesn\'t reside in a traditional row-column database. Unstructured data files often include text and multimedia content. Examples include e-mail messages, word processing documents, videos, photos, audio files, presentations, webpages and many other kinds of business documents. Note that while these sorts of files may have an internal structure, they are still considered "unstructured" because the data they contain doesn\'t fit neatly in a database.\nExperts estimate that 80 to 90 percent of the data in any organization is unstructured. And the amount of unstructured data in enterprises is growing significantly often many times faster than structured databases are growing. "[[Big data]] can include both structured and unstructured data, but IDC estimates that 90 percent of [[big data]] is unstructured data."<ref>Webopedia http://www.webopedia.com/TERM/U/unstructured_data.html</ref>\n\n====Long Runtime====\nIn today’s hyper competitive business environment, companies not only have to find and analyze the relevant data they need, they must find it quickly. The challenge is going through the sheer volumes of data and accessing the level of detail needed, all at a high speed. The challenge only grows as the degree of granularity increases. One possible solution is hardware. Some vendors are using increased memory and powerful parallel processing to crunch large volumes of data extremely quickly. Another method is putting data in-memory but using a grid computing approach, where many machines are used to solve a problem. Both approaches allow organizations to explore huge data volumes. Even this level of sophisticated hardware and software, few of the image processing tasks in large scale take a few days to few weeks.<ref>SAS. http://www.sas.com/resources/asset/five-big-data-challenges-article.pdf</ref> Debugging of the data processing is extremely hard due to long run times.\n\n====Complex Platform====\n[[Big Data]] platforms have a very complicated structure. Data is distributed among several machines. Typically the jobs are mapped into several machines and results are later combined by reduce operations. Debugging of a [[big data]] pipeline becomes very challenging because of the very nature of the system. It will not be an easy task for the data scientist to figure out which machine\'s data has the outliers and unknown features causing a particular algorithm to give unexpected results.\n\n====Proposed Solution====\nData provenance or data lineage can be used to make the debugging of [[big data]] pipeline easier. This necessitates the collection of data about data transformations. The below section will explain data provenance in more detail.\n\n==Data Provenance==\nData Provenance provides a historical record of the data and its origins. The provenance of data which is generated by complex transformations such as workflows is of considerable value to scientists. From it, one can ascertain the quality of the data based on its ancestral data and derivations, track back sources of errors, allow automated re-enactment of derivations to update a data, and provide attribution of data sources. Provenance is also essential to the business domain where it can be used to drill down to the source of data in a data warehouse, track the creation of intellectual property, and provide an audit trail for regulatory purposes.\n\nThe use of data provenance is proposed in distributed systems to trace records through a dataflow, replay the dataflow on a subset of its original inputs and debug data flows. To do so, one needs to keep track of the set of inputs to each operator, which were used to derive each of its outputs. Although there are several forms of provenance, such as copy-provenance and how-provenance,<ref name="PeterBuneman" /><ref>Robert Ikeda and Jennifer Widom. Data lineage: A survey. Technical report, Stanford University, 2009.</ref> the information we need is a simple form of \'\'\'why-provenance, or lineage\'\'\', as defined by Cui et al.<ref name="YCui">Y. Cui and J. Widom. Lineage tracing for general data warehouse transformations. VLDB Journal, 12(1), 2003.</ref>\n\n==Lineage Capture==\nIntuitively, for an operator T producing output o, lineage consists of triplets of form {I, T, o}, where I is the set of inputs to T used to derive o. Capturing lineage for each operator T in a dataflow enables users to ask questions such as “Which outputs were produced by an input i on operator T ?” and “Which inputs produced output o in operator T ?”<ref name="DeSoumyarupa"/> A query that finds the inputs deriving an output is called a backward tracing query, while one that finds the outputs produced by an input is called a forward tracing query.<ref name="RobertIkedaHyunjung">Robert Ikeda, Hyunjung Park, and Jennifer Widom. Provenance for generalized map and reduce workflows. In Proc. of CIDR, January 2011.</ref> Backward tracing is useful for debugging, while forward tracing is useful for tracking error propagation.<ref name="RobertIkedaHyunjung" /> Tracing queries also form the basis for replaying an original dataflow.<ref name="IanFosterJensVockler" /><ref name="YCui" /><ref name="RobertIkedaHyunjung" /> However, to efficiently use lineage in a DISC system, we need to be able to capture lineage at multiple levels (or granularities) of operators and data, capture accurate lineage for DISC processing constructs and be able to trace through multiple dataflow stages efficiently.\n\nDISC system consists of several levels of operators and data, and different use cases of lineage can dictate the level at which lineage needs to be captured.  Lineage can be captured at the level of the job, using files and giving lineage tuples of form {IF i, M RJob, OF i }, lineage can also be captured at the level of each task, using records and giving, for example, lineage tuples of form {(k rr, v rr ), map, (k m, v m )}. The first form of lineage is called coarse-grain lineage, while the second form is called fine-grain lineage. Integrating lineage across different granularities enables users to ask questions such as “Which file read by a MapReduce job produced this particular output record?” and can be useful in debugging across different operator and data granularities within a dataflow.<ref name="DeSoumyarupa" />\n[[File:Map Reduce Job -1.png|thumb|center|500px|Map Reduce Job showing containment relationships]]\n\nTo capture end-to-end lineage in a DISC system, we use the Ibis model,<ref>C. Olston and A. Das Sarma. Ibis: A provenance manager for multi-layer\nsystems. In Proc. of CIDR, January 2011.</ref> which introduces the notion of containment hierarchies for operators and data. Specifically, Ibis proposes that an operator can be contained within another and such a relationship between two operators is called \'\'\'operator containment\'\'\'. "Operator containment implies that the contained (or child) operator performs a part of the logical operation of the containing (or parent) operator."<ref name="DeSoumyarupa" /> For example, a MapReduce task is contained in a job. Similar containment relationships exist for data as well, called data containment. Data containment implies that the contained data is a subset of the containing data (superset).\n[[File:Containment Hierarchy.png|thumb|center|500px|Containment Hierarchy]]\n\n==Prescriptive Data Lineage==\n\nThe concept of \'\'\'Prescriptive Data Lineage\'\'\' combines both the logical model (entity) of how that data should flow with the actual lineage for that instance.<ref>http://info.hortonworks.com/rs/549-QAL-086/images/Hadoop-Governance-White-Paper.pdf</ref>\n\nData lineage and provenance typically refers to the way or the steps a dataset came to its current state Data lineage, as well as all copies or derivatives. However, simply looking back at only audit or log correlations to determine lineage from a forensic point of view is flawed for certain data management cases.  For instance, it is impossible to determine with certainty if the route a data workflow took was correct or in compliance without the logic model.\n\nOnly by combining the a logical model with atomic forensic events can proper activities be validated:\n#Authorized copies, joins, or CTAS operations\n#Mapping of processing to the systems that those process are run on\n#Ad-Hoc versus established processing sequences\n\nMany certified compliance reports require provenance of data flow as well as the end state data for a specific instance. With these types of situations, any deviation from the prescribed path need to be accounted for and potentially remediated.<ref>[https://www.sec.gov/info/smallbus/secg/bd-small-entity-compliance-guide.htm SEC Small Entity Compliance Guide]</ref>   This is marks a shift in thinking from purely a look back model  to a framework which is better suited to capture compliance workflows.\n\n==Active vs Lazy Lineage==\nLazy lineage collection typically captures only coarse-grain lineage at run time. These systems incur low capture overheads due to the small amount of lineage they capture. However, to answer fine-grain tracing queries, they must replay the data flow on all (or a large part) of its input and collect fine-grain lineage during the replay. This approach is suitable for forensic systems, where a user wants to debug an observed bad output.\n\nActive collection systems capture entire lineage of the data flow at run time. The kind of lineage they capture may be coarse-grain or fine-grain, but they do\nnot require any further computations on the data flow after its execution. Active fine-grain lineage collection systems incur higher capture overheads than lazy collection systems. However, they enable sophisticated replay and debugging.<ref name="DeSoumyarupa" />\n\n==Actors==\nAn actor is an entity that transforms data; it may be a Dryad vertex, individual map and reduce operators, a MapReduce job, or an entire dataflow pipeline. Actors act as black-boxes and the inputs and outputs of an actor are tapped to capture lineage in the form of associations, where an association is a triplet {i, T, o} that relates an input i with an output o for an actor T . The instrumentation thus captures lineage in a dataflow one actor at a time, piecing it into a set of associations for each actor. The system developer needs to capture the data an actor reads (from other actors) and the data an actor writes (to other actors). For example, a developer can treat the Hadoop Job Tracker as an actor by recording the set of files read and written by each job.\n<ref name="mainPaper">Dionysios Logothetis, Soumyarupa De, and Kenneth Yocum. 2013. Scalable lineage capture for debugging DISC analytics. In Proceedings of the 4th annual Symposium on Cloud Computing (SOCC \'13). ACM, New York, NY, USA, , Article 17 , 15 pages.</ref>\n\n==Associations==\nAssociation is a combination of the inputs, outputs and the operation itself. The operation is represented in terms of a black box also known as the actor. The associations describe the transformations that are applied on the data. The associations are stored in the association tables. Each unique actor is represented by its own association table. An association itself looks like {i, T, o} where i is the set of inputs to the actor T and o is set of outputs given produced by the actor. Associations are the basic units of Data Lineage. Individual associations are later clubbed together to construct the entire history of transformations that were applied to the data.<ref name="DeSoumyarupa"/>\n\n==Architecture==\n[[Big data]] systems scale horizontally i.e. increase capacity by adding new hardware or software entities into the distributed system. The distributed system acts as a single entity in the logical level even though it comprises multiple hardware and software entities. The system should continue to maintain this property after horizontal scaling. An important advantage of horizontal scalability is that it can provide the ability to increase capacity on the fly. The biggest plus point is that horizontal scaling can be done using commodity hardware.\n\nThe horizontal scaling feature of [[Big Data]] systems should be taken into account while creating the architecture of lineage store. This is essential because the lineage store itself should also be able to scale in parallel with the [[Big data]] system. The number of associations and amount of storage required to store lineage will increase with the increase in size and capacity of the system. The architecture of [[Big data]] systems makes the use of a single lineage store not appropriate and impossible to scale. The immediate solution to this problem is to distribute the lineage store itself.<ref name="DeSoumyarupa"/>\n\nThe best case scenario is to use a local lineage store for every machine in the distributed system network. This allows the lineage store also to scale horizontally. In this design, the lineage of data transformations applied to the data on a particular machine is stored on the local lineage store of that specific machine. The lineage store typically stores association tables. Each actor is represented by its own association table. The rows are the associations themselves and columns represent inputs and outputs. This design solves 2 problems. It allows horizontal scaling of the lineage store. If a single centralized lineage store was used, then this information had to be carried over the network, which would cause additional network latency. The network latency is also avoided by the use of a distributed lineage store.<ref name="mainPaper"/>\n\n[[File:Selection 065.png|thumb|center|500px|Architecture of Lineage Systems]]\n\n==Data flow Reconstruction==\nThe information stored in terms of associations needs to be combined by some means to get the data flow of a particular job. In a distributed system a job is broken down into multiple tasks. One or more instances run a particular task. The results produced on these individual machines are later combined together to finish the job. Tasks running on different machines perform multiple transformations on the data in the machine. All the transformations applied to the data on a machines is stored in the local lineage store of that machines. This information needs to be combined together to get the lineage of the entire job. The lineage of the entire job should help the data scientist understand the data flow of the job and he/she can use the data flow to debug the [[big data]] pipeline. The data flow is reconstructed in 3 stages.\n\n===Association tables===\nThe first stage of the data flow reconstruction is the computation of the association tables. The association tables exists for each actor in each local lineage store. The entire association table for an actor can be computed by combining these individual association tables. This is generally done using a series of equality joins based on the actors themselves. In few scenarios the tables might also be joined using inputs as the key. Indexes can also be used to improve the efficiency of a join.The joined tables need to be stored on a single instance or a machine to further continue processing. There are multiple schemes that are used to pick a machine where a join would be computed. The easiest one being the one with minimum CPU load. Space constraints should also be kept in mind while picking the instance where join would happen.\n\n===Association Graph===\nThe second step in data flow reconstruction is computing an association graph from the lineage information. The graph represents the steps in the data flow. The actors act as vertices and the associations act as edges. Each actor T is linked to its upstream and downstream actors in the data flow. An upstream actor of T is one that produced the input of T, while a downstream actor is one that consumes the output of T . Containment relationships are always considered while creating the links. The graph consists of three types of links or edges.\n\n====Explicitly specified links====\nThe simplest link is an explicitly specified link between two actors. These links are explicitly specified in the code of a machine learning algorithm. When an actor is aware of its exact upstream or downstream actor, it can communicate this information to lineage API. This information is later used to link these actors during the tracing query. For example, in the [[MapReduce]] architecture, each map instance knows the exact record reader instance whose output it consumes.<ref name="DeSoumyarupa"/>\n\n====Logically inferred links====\nDevelopers can attach data flow [[archetypes]] to each logical actor. A data flow archetype explains how the children types of an actor type arrange themselves in a data flow. With the help of this information, one can infer a link between each actor of a source type and a destination type. For example, in the [[MapReduce]] architecture, the map actor type is the source for reduce, and vice versa. The system infers this from the data flow archetypes and duly links map instances with reduce instances. However, there may be several [[MapReduce]] jobs in the data flow, and linking all map instances with all reduce instances can create false links. To prevent this, such links are restricted to actor instances contained within a common actor instance of a containing (or parent) actor type. Thus, map and reduce instances are only linked to each other if they belong to the same job.<ref name="DeSoumyarupa"/>\n\n====Implicit links through data set sharing====\nIn distributed systems, sometimes there are implicit links, which are not specified during execution. For example, an implicit link exists between an actor that wrote to a file and another actor that read from it. Such links connect actors which use a common data set for execution. The dataset is the output of the first actor and is the input of the actor following it.<ref name="DeSoumyarupa"/>\n\n===Topological Sorting===\nThe final step in the data flow reconstruction is the [[Topological sorting]] of the association graph. The directed graph created in the previous step is topologically sorted to obtain the order in which the actors have modified the data. This inherit order of the actors defines the data flow of the big data pipeline or task.\n\n==Tracing & Replay==\nThis is the most crucial step in [[Big Data]] debugging. The  captured lineage is combined and processed to obtain the data flow of the pipeline. The data flow helps the data  scientist or a developer to look deeply into the actors and their transformations. This step allows the data scientist to figure out the part of the algorithm that is generating the unexpected output. A [[big data]] pipeline can go wrong in 2 broad ways. The first is a presence of a suspicious actor in the data-flow. The second being the existence of outliers in the data.\n\nThe first case can be debugged by tracing the data-flow. By using lineage and data-flow information together a data scientist can figure out how the inputs are converted into outputs. During the process actors that behave unexpectedly can be caught. Either these actors can be removed from the data flow or they can be augmented by new actors to change the data-flow. The improved data-flow can be replayed to test the validity of it. Debugging faulty actors include recursively performing coarse-grain replay on actors in the data-flow,<ref>Wenchao Zhou, Qiong Fei, Arjun Narayan, Andreas Haeberlen, Boon Thau Loo, and Micah Sherr. Secure network provenance. In Proceedings of 23rd ACM Symposium on Operating System Principles (SOSP), December 2011.</ref> which can be expensive in resources for long dataflows. Another approach is to manually inspect lineage logs to find anomalies,<ref name="Benjamim&Luiz" /><ref>Rodrigo Fonseca, George Porter, Randy H. Katz, Scott Shenker, and Ion Stoica. X-trace: A pervasive network tracing framework. In In Proceedings of NSDI’07, 2007.</ref> which can be tedious and time-consuming across several stages of a data-flow. Furthermore, these approaches work only when the data scientist can discover bad outputs. To debug analytics without known bad outputs, the data scientist need to analyze the data-flow for suspicious behavior in general. However, often, a user may not know the expected normal behavior and cannot specify predicates. This section describes a debugging methodology for retrospectively analyzing lineage to identify faulty actors in a multi-stage data-flow. We believe that sudden changes in an actor’s behavior, such as its average selectivity, processing rate or output size, is characteristic of an anomaly. Lineage can reflect such changes in actor behavior over time and across different actor instances. Thus, mining lineage to identify such changes can be useful in debugging faulty actors in a data-flow.\n[[File:Tracing Anomalous Actors.png|thumb|center|400px|Tracing Anomalous Actors]]\n\nThe second problem i.e. the existence of outliers can also be identified by running the data-flow step wise and looking at the transformed outputs. The data scientist finds a subset of outputs that are not in accordance to the rest of outputs. The inputs which are causing these bad outputs are the outliers in the data. This problem can be solved by removing the set of outliers from the data and replaying the entire data-flow. It can also be solved by modifying the machine learning algorithm by adding, removing or moving actors in the data-flow. The changes in the data-flow are successful if the replayed data-flow does not produce bad outputs.\n[[File:Tracing Outliers in the data.png|thumb|center|400px|Tracing Outliers in the data]]\n\n==Challenges==\nEven though use data lineage is a novel way of debugging of [[big data]] pipelines, the process is not simple. The challenges are scalability of lineage store, fault tolerance of the lineage store, accurate capture of lineage for black box operators and many others. These challenges must be considered carefully and trade offs between them need to be evaluated to make a realistic design for data lineage capture.\n\n===Scalability===\nDISC systems are primarily batch processing systems designed for high throughput. They execute several jobs per analytics, with several tasks per job. The overall number of operators executing at any time in a cluster can range from hundreds to thousands depending on the cluster size. Lineage capture for\nthese systems must be able scale to both large volumes of data and numerous operators to avoid being a bottleneck for the DISC analytics.\n\n===Fault tolerance===\nLineage capture systems must also be fault tolerant to avoid rerunning data flows to capture lineage. At the same time, they must also accommodate failures in the DISC system. To do so, they must be able to identify a failed DISC task and avoid storing duplicate copies of lineage between the partial lineage generated by the failed task and duplicate lineage produced by the restarted task. A lineage system should also be able to gracefully handle multiple instances of local lineage systems going down. This can achieved by storing replicas of lineage associations in multiple machines. The replica can act like a backup in the event of the real copy being lost.\n\n===Black-box operators===\nLineage systems for DISC dataflows must be able to capture accurate lineage across black-box operators to enable fine-grain debugging. Current approaches to this include Prober, which seeks to find the minimal set of inputs that can produce a specified output for a black-box operator by replaying the data-flow several times to deduce the minimal set,<ref>Anish Das Sarma, Alpa Jain, and Philip Bohannon. PROBER: Ad-Hoc Debugging of Extraction and Integration Pipelines. Technical report, Yahoo, April 2010.</ref> and dynamic slicing, as used by Zhang et al.<ref>Mingwu Zhang, Xiangyu Zhang, Xiang Zhang, and Sunil Prabhakar. Tracing lineage beyond relational operators. In Proc. Conference on Very Large Data Bases (VLDB), September 2007.</ref> to capture lineage for [[NoSQL]] operators through binary rewriting to compute dynamic slices. Although producing highly accurate lineage, such techniques can incur significant time overheads for capture or tracing, and it may be preferable to instead trade some accuracy for better performance. Thus, there is a need for a lineage collection system for DISC dataflows that can capture lineage from arbitrary operators with reasonable accuracy, and without significant overheads in capture or tracing.\n\n===Efficient tracing===\nTracing is essential for debugging, during which, a user can issue multiple tracing queries. Thus, it is important that tracing has fast turnaround times. Ikeda et al.<ref name="RobertIkedaHyunjung" /> can perform efficient backward tracing queries for MapReduce dataflows, but are not generic to different DISC systems and do not perform efficient forward queries. Lipstick,<ref>Yael Amsterdamer, Susan B. Davidson, Daniel Deutch, Tova Milo, and Julia Stoyanovich. Putting lipstick on a pig: Enabling database-style workflow provenance. In Proc. of VLDB, August 2011.</ref> a lineage system for Pig,<ref>Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew Tomkins. Pig latin: A not-so-foreign language for data processing. In Proc. of ACM SIGMOD, Vancouver, Canada, June 2008.</ref> while able to perform both backward and forward tracing, is specific to Pig and SQL operators and can only perform coarse-grain tracing for black-box operators. Thus, there is a need for a lineage system that enables efficient forward and backward tracing for generic DISC systems and dataflows with black-box operators.\n\n===Sophisticated replay===\nReplaying only specific inputs or portions of a data-flow is crucial for efficient debugging and simulating what-if scenarios. Ikeda et al. present a methodology for lineage-based refresh, which selectively replays updated inputs to recompute affected outputs.<ref>Robert Ikeda, Semih Salihoglu, and Jennifer Widom. Provenance-based refresh in data-oriented workflows. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM ’11, pages 1659–1668, New York, NY, USA, 2011. ACM.</ref> This is useful during debugging for re-computing outputs when a bad input has been fixed. However, sometimes a user may want to remove the bad input and replay the lineage of outputs previously affected by the error to produce error-free outputs. We call this exclusive replay. Another use of replay in debugging involves replaying bad inputs for step-wise debugging (called selective replay). Current approaches to using lineage in DISC systems do not address these. Thus, there is a need for a lineage system that can perform both exclusive and selective replays to address different debugging needs.\n\n===Anomaly detection===\nOne of the primary debugging concerns in DISC systems is identifying faulty operators. In long dataflows with several hundreds of operators or tasks, manual inspection can be tedious and prohibitive. Even if lineage is used to narrow the subset of operators to examine, the lineage of a single output can still span several operators. There is a need for an inexpensive automated debugging system, which can substantially narrow the set of potentially faulty operators, with reasonable accuracy, to minimize the amount of manual examination required.\n\n==See also==\n<!-- please do not list specific implementations here -->\n* [[Provenance]]\n* [[Big Data]]\n* [[Topological Sorting]]\n* [[Debugging]]\n* [[NoSQL]]\n* [[Scalability]]\n* [[Directed acyclic graph]]\n\n==References==\n{{Reflist|33em}}\n\n[[Category:Data management]]\n[[Category:Distributed computing problems]]\n[[Category:Big data]]']
['Media aggregation platform', '46201390', "A '''Media Aggregation Platform''' or '''Media Aggregation Portal''' (MAP) is an over-the-top service for distributing web-based streaming media content from multiple sources to a large audience. MAPs consist of networks of sources who host their own content which viewers can choose and access directly from a larger variety of content to choose from than a single source can offer.<ref>{{cite web | url=https://medium.com/@bmobley/over-the-top-of-ott-need-a-map-9931096775c2 | title=Over the Top of OTT… Need a MAP? | publisher=[[Medium.com]] | accessdate=23 March 2015}}</ref><ref>{{cite book | title=Building Next-Generation Converged Networks: Theory and Practice | publisher=CRC Press |author1=Al-Sakib Khan Pathan |author2=Muhammad Mostafa Monowar |author3=Zubair Md. Fadlullah | year=2013 | isbn=1466507616}}</ref> The service is used by content providers, looking to extend the reach of their content.\n\nUnlike multichannel video programming distributor ([[MVPD]]) or multiple-system operator (MSO), MAPs rely on the Internet rather than cables or satellite. As more network television channels have moved online in the early 21st century,<ref>{{cite web | url=http://www.exchange4media.com/59377_ott-platforms-to-be-key-growth-area-for-tv-broadcasters.html | title=OTT platforms to be key growth area for TV broadcasters | accessdate=23 March 2015}}</ref> joining web-native channels like [[Netflix]], MAPs aggregates content the way that MSOs and MVPDs have used cable, and to a lesser extent satellite and IPTV infrastructure. There are companies that offer a similar service for free, including [[Yidio]] and TV.com, while others charge a subscription fee like as [[FreeCast Inc]]'s Rabbit TV Plus.<ref>{{cite web | url=http://rabbittvplus.com/ | title=FreeCast Inc Rabbit TV Plus | accessdate=23 March 2015}}</ref> When compared with MSOs and MVPDs, MPAs network have much lower cost due to lack of physical infrastructure. The majority of revenues from their services is retained by the content creators and revenues are from advertisements, [[pay-per-view]], and subscription-based content offerings instead of by licensing and reselling content. MAPs service consumers directly with the content source and they purchase content directly from its source, without the markup added by a middleman.<ref>{{cite web | url=https://www.ncta.com/industry-data | title=NCTA Industry Data | accessdate=23 March 2015}}</ref>\n\n==See also==\n* [[Multichannel video programming distributor|Multichannel Video Programming Distributor (MVPD)]]\n* [[Multiple system operator|Multiple System Operator (MSO)]]\n* [[Internet protocol television|Internet Protocol Television (IPTV)]]\n* [[Over-the-top content|Over-the-top (OTT)]]\n* [[Over-the-air television|Over-the-air (OTA)]]\n* [[Video on demand|Video on demand (VOD)]] \n* [[Broadcast networks|Broadcast Networks]] \n* [[Internet Television]]\n* [[Streaming Media]]\n* [[Pay TV]]\n\n==References==\n{{reflist}}\n\n[[Category:Data management]]"]
['MaPS S.A.', '46489271', "{{Orphan|date=July 2015}}\n\n{{Infobox company\n| name             = MaPS System \n| image            = [[File:MaPS-System Logo.png|250px]]\n| industry         = [[Public limited company|PLC]]\n| founder          = Thierry Muller\n| headquarters     = [[Luxembourg]]\n| area_served      = [[France]], [[Luxembourg]], [[Germany]], [[Switzerland]], [[Belgium]]\n| products         = [[Product Information Management]], [[Digital Asset Management]], [[Master Data Management]] and [[Business Process Management]]\n| homepage         = http://www.maps-system.com/\n}}\n\n'''MaPS S.A.''' is a software editor founded in 2011 by Thierry Muller which is headquartered in [[Luxembourg]]. Its platform, called MaPS System, provides [[Data management|Data Management]] solutions for [[Multichannel Marketing]].\n\n==History and Funding==\n\nIn 1999, the founder, had realized that certain challenges arose with several tools for [[Customer relationship management|Customer Relationship Management]], [[Public relations|Public Relations]] and in particular [[Marketing]] tools set in place. Complex data structures developed difficulties for organizations who lost focus of their dispersed data when wanting to operate and sell in an international and [[Multichannel Marketing|Multichannel]] environment.\n\nThe founder drafted his initial ideas on the topic of [[Multichannel Marketing]] and developed his first version of MaPS System under the agency Prem1um S.A. in 2005, which in combination with the [[Data Management]] solution also provided various Multimedia & Marketing activities.\n\nIn 2011, after being successful, Prem1um S.A. decided to enable the software MaPS System to operate independently under MaPS S.A., as a separate company and editor of the software. The first financial supports were provided by Malta ICI, a Venture Capital firm, and the local partner Chameleon Invest, a seed-capital fund led by Business Angels, who invested €900.000. In a second investment round in 2014 led by Newion Investments, a Venture Capital firm, €1.4 Million were raised, thus amounting to total assets of €2.2 Million.\n\n==Products==\nThe services included in MaPS System range from the data centralization, [[Data Governance]] to an optimized [[Multichannel Marketing]]. The software today features more than 35 modules for [[Master Data Management]], [[Product Information Management]], [[Digital Asset Management]], [[Business Process Management]] including catalogue [[Publishing]] features.\n\n==References==\n* {{Official website | http://www.maps-system.com/ MaPS S.A.}}\n* Newion invests in MaPS System | http://www.newion-investments.com/news/newion-invests-in-maps-system/1]\n* Introducing MaPS System | http://www.siliconluxembourg.lu/introducing-maps-system-a-centralized-information-management-solution/]\n\n[[Category:Data management software]]\n[[Category:Data management]]\n[[Category:Software companies]]\n[[Category:Companies of Luxembourg]]"]
['Cleo (company)', '42965868', '{{Infobox company\n| logo             = [[File:Cleo (company) logo, 2014.png]]\n| name             = Cleo\n| type             = [[Privately held company]]\n| foundation       =  1976\n| location_city    = [[Loves Park, Illinois]]\n| location_country = [[United States of America]]\n| key_people       =\nMahesh Rajasekharan <small>([[Chief executive officer|CEO]])</small><br/>Sumit Garg <small>(President)</small>\n| num_employees    = 200+\n| industry         = [[Managed file transfer]], data integration, [[network management]] and secure file sharing\n| homepage         = {{url|http://cleo.com}}\n}}\n\n\'\'\'Cleo\'\'\' is an [[enterprise software]] company that provides [[electronic data interchange]] (EDI), and application-to-application (A2A), [[business-to-business]] (B2B), and [[big data]] integration services to organizations with [[managed file transfer]] needs. The company, formerly known as Cleo Communications, was founded in 1976. Cleo was acquired by investment firm Globe Equity Partners in 2012. Mahesh Rajasekharan is Cleo\'s [[CEO]], and Sumit Garg serves as Cleo\'s president.<ref>{{cite web|author=Alex Gary |url=http://www.rrstar.com/x1364621329/Private-equity-firm-acquires-Loves-Park-company |title=Private equity firm acquires Loves Park company - Blogs - Rockford Register Star |publisher=Rrstar.com |date= |accessdate=2014-06-05}}</ref>\n\n== Business ==\nCleo originally began as a division of Phone 1 Inc., a voice data gathering systems manufacturer, and built data concentrators and [[terminal emulator]]s — multi-bus computers, modems, and terminals to interface with [[IBM]] mainframes via [[Binary Synchronous Communications|bisynchronous communications]]. The company then began developing [[mainframe]] middleware in the 1980s, and with the rise of the [[Personal computer|PC]], moved into B2B data communications and [[file transfer]] software.<ref>{{cite web|url=https://books.google.com/books?id=JNJWAAAAMAAJ&q=cleo+%22phone+1%22&dq=cleo+%22phone+1%22&hl=en&sa=X&ei=6I65VLiABc6yogTf0IKABA&ved=0CEcQ6AEwBTge |title=Kelly/Grimes IBM PC compatible computer directory - Brian W. Kelly, Dennis J. Grimes - Google Books |publisher=Books.google.com |date=2008-01-28 |accessdate=2015-04-02}}</ref>\n\nCleo\'s portfolio features big data, extreme file transfer, [[data transformation]], person-to-person collaboration, and file sharing solutions,<ref>http://www.channelworld.in/interviews/high-speed-data-transfer-is-more-critical-than-ever%3A-mahesh-rajasekharan%2C-cleo</ref> and its product line includes software for secure file transfer, exchange, and [[Cloud collaboration|collaboration]]; secure email, text, and voice messaging; and others.<ref>[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=204651549 Cleo Communications, Inc.: Private Company Information - Businessweek<!-- Bot generated title -->]</ref>  Cleo products use the [[AS2]] specification and other protocols for connectivity and community management.<ref>{{cite web|author=|url=http://www.filetransferconsulting.com/forresters-managed-file-transfer-good-bad-ugly/ |title=Forrester’s "Managed File Transfer Solutions" – Good, Bad and Ugly |publisher=Filetransferconsulting.com |date=2011-07-14 |accessdate=2014-06-05}}</ref> Cleo VersaLex is the engine behind its software offerings, which include Cleo LexiCom,<ref>[http://www.itjungle.com/fhs/fhs030408-story10.html Four Hundred Stuff-Cleo Updates B2B Communications Software<!-- Bot generated title -->]</ref> Cleo VLTrader,<ref>[http://www.itjungle.com/fhs/fhs021610-story08.html Four Hundred Stuff-Stonebranch Taps Cleo for B2B Expertise<!-- Bot generated title -->]</ref> and Cleo Harmony, which supports the streamlining of [[data integration]].<ref>[http://webcache.googleusercontent.com/search?q=cache:http://lerablog.org/technology/software/ftp-tools-to-help-with-large-file-transfers/ Best FTP Tools for Large File Transfers<!-- Bot generated title -->]{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> The company also developed the Cleo Unify and Cleo Trust secure [[file sharing]] and [[email]] messaging solutions that work independently or in conjunction with Cleo\'s data integration platform.<ref>{{cite web|url=http://www.rrstar.com/article/20150209/News/150209528 |title=Cleo releases new software - Rockford Register Star |publisher=rrstar.com |date=2015-02-09 |accessdate=2015-02-19}}</ref> In 2015, Cleo introduced the Cleo Jetsonic high-speed data transfer software solution.<ref>{{cite web|url=http://www.rrstar.com/article/20150708/NEWS/150709580/-1/json |title=Cleo announces new data solution - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-07-09}}</ref>\n\nThe City of [[Atlanta]] adopted Cleo\'s [[fax]] technology, Cleo Streem, in 2006 to accommodate its communication needs,<ref>http://citycouncil.atlantaga.gov/2013/images/proposed/13R3556.pdf</ref> and the [[U.S. Department of Veterans Affairs]] did the same in 2013 when in need of [[FIPS 140-2]]-compliant technology to protect information.<ref>[http://www.va.gov/TRM/ToolPage.asp?tid=6568 One-VA Technical Reference Model<!-- Bot generated title -->]</ref> Cleo also serves U.S. transportation [[logistics]] company MercuryGate International<ref>{{cite web|url=http://www.rrstar.com/article/20140820/ENTERTAINMENTLIFE/140829886/10487/BUSINESS |title=Loves Park business selected to provide online services - Rockford Register Star |publisher=rrstar.com |date=2014-08-20 |accessdate=2015-02-19}}</ref> as a customer and partners with [[Hortonworks]]<ref>[http://hortonworks.com/blog/secure-reliable-hadoop-data-transfer-option-cleo-mft/ Secure, reliable Hadoop data transfer with Cleo MFT - Hortonworks<!-- Bot generated title -->]</ref> for big data integration and [[Tech Data]] for software distribution.<ref>[http://logistics.cioreview.com/news/cleo-s-data-transfer-solutions-now-available-on-the-tech-data-online-store-nid-2119-cid-33.html Cleo\'s Data Transfer Solutions Now Available on the Tech Data Online Store<!-- Bot generated title -->]</ref> Cleo software also powers the architecture for several major supply chain companies, such as [[JDA Software]] and [[SAP SE|SAP]].<ref>{{cite web|last=Grackin |first=Ann |url=http://searchmanufacturingerp.techtarget.com/tip/Smart-sensors-bring-the-supply-chain-to-life |title=Smart sensors bring the supply chain to life |publisher=Searchmanufacturingerp.techtarget.com |date= |accessdate=2015-07-08}}</ref>\n\nIn 2009, Cleo was added to the [[Gartner]] [[Magic Quadrant]] for managed file transfer.<ref>http://www.servicecatalog.dts.ca.gov/services/sft/docs/MFT_Quad_2009_axway_3183.pdf</ref>\n\n== Expansion ==\nIn June 2014, Cleo opened an office in [[Chicago]] for members of its support and engineering teams.<ref>[http://rockrivertimes.com/2014/07/16/cleo-continues-to-grow-expands-operations-into-chicago-office/ Cleo continues to grow, expands operations into Chicago office | The Rock River Times<!-- Bot generated title -->]</ref> The company in 2014 hired Jorge Rodriguez as senior vice president of product development<ref>{{cite web|author=|url=http://www.marketwatch.com/story/jorge-rodriguez-joins-cleo-as-senior-vice-president-of-product-development-2014-02-04 |title=Jorge Rodriguez Joins Cleo as Senior Vice President of Product Development |publisher=MarketWatch |date=2014-02-04 |accessdate=2015-02-19}}</ref> and John Thielens as vice president of technology.<ref>{{cite web|author=|url=http://www.marketwatch.com/story/john-thielens-joins-cleo-as-vice-president-of-technology-2014-01-31 |title=John Thielens Joins Cleo as Vice President of Technology |publisher=MarketWatch |date=2014-01-31 |accessdate=2015-02-19}}</ref> And in 2015, Cleo hired Dave Brunswick as vice president of solutions for North America.<ref>{{cite web|url=http://www.rrstar.com/article/20150705/NEWS/150709917 |title=Cleo announces new hire - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-07-08}}</ref> Cleo also opened its Center of Innovation product development facility in [[Bengaluru, India]], in 2015.<ref>http://www.deccanherald.com/content/500091/cleo-bengaluru-centre-plans-co.html</ref>\n\nIn 2016, Cleo acquired [[Extol International|EXTOL International]], a [[Pottsville, Pennsylvania|Pottsville, Pa.]]-based business and EDI integration and data transformation company for an undisclosed amount. The Pottsville office will operate under the Cleo name.<ref>http://www.lvb.com/article/20160406/LVB01/160409929/pottsville-tech-firm-acquired-by-illinois-company</ref>\n\n== Certification ==\nCleo regularly submits its products to Drummond Group\'s interoperability software testing for AS2,<ref>[http://www.supplychainbrain.com/content/technology-solutions/supplier-relationship-mgmt/single-article-page/article/cleos-versalex-wins-drummond-certification-for-as2-interoperability/ Cleo\'s VersaLex Wins Drummond Certification for AS2 Interoperability<!-- Bot generated title -->]</ref> AS3<ref>{{cite web|url=http://www.drummondgroup.com/index.php/newsevents/press-releases/341-as3-secure-messaging-products-are-drummond-certified-in-1q14-interoperability-test-event |title=AS3 Secure Messaging Products are Drummond Certified™ in 1Q14 Interoperability Test Event |publisher=Drummond Group |date=2014-02-19 |accessdate=2014-06-05}}</ref> and ebMS 2.0.<ref>{{cite web|url=http://www.drummondgroup.com/index.php/newsevents/press-releases/339-newest-ebms-20-secure-messaging-products-are-drummond-certified |title=Newest ebMS 2.0 Secure Messaging Products are Drummond Certified™ |publisher=Drummond Group |date=2013-09-09 |accessdate=2014-06-05}}</ref>\n\n== Awards ==\nCleo has been given a [[Xerox]] partner of the year award for each of the past five years. The Cleo Streem solution integrates with Xerox multi-function products, providing customers with comprehensive solutions for network fax and interactive messaging needs.<ref>{{cite web|url=http://www.rrstar.com/article/20150330/NEWS/150339927/10447/NEWS |title=Cleo Wins Xerox Partner of the Year Award - News - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-04-02}}</ref>\n\n== References ==\n{{Reflist|3}}\n\n[[Category:EDI software companies]]\n[[Category:Software companies based in Illinois]]\n[[Category:Network management]]\n[[Category:Managed file transfer]]\n[[Category:File transfer protocols]]\n[[Category:Data management]]']
['Big data', '27051151', '{{About|large collections of data|the band|Big Data (band)}}\n[[File:Hilbert InfoGrowth.png|thumb|right|400px|Growth of and digitization of global information-storage capacity<ref>{{cite web|url= http://www.martinhilbert.net/WorldInfoCapacity.html|title= The World’s Technological Capacity to Store, Communicate, and Compute Information|work= MartinHilbert.net|accessdate= 13 April 2016}}</ref>]]\n\n\'\'\'\'\'Big data\'\'\'\'\' is a term for [[data set]]s that are so large or complex that traditional [[data processing]] applications are inadequate to deal with them.  Challenges include [[Data analysis|analysis]], capture, [[data curation]], search, [[Data sharing|sharing]], [[Computer data storage|storage]], [[Data transmission|transfer]], [[Data visualization|visualization]], [[Query language|querying]], updating and [[information privacy]]. The term "big data" often refers simply to the use of [[predictive analytics]], [[user behavior analytics]], or certain other advanced data analytics methods that extract value from data, and seldom to a particular size of data set.<ref>{{Cite book|url= http://link.springer.com/10.1007/978-3-319-21569-3 |title= New Horizons for a Data-Driven Economy – Springer|doi= 10.1007/978-3-319-21569-3}}</ref> "There is little doubt that the quantities of data now available are indeed large, but that’s not the most relevant characteristic of this new data ecosystem."<ref>{{cite journal |last1=boyd |first1=dana |last2=Crawford |first2=Kate |title=Six Provocations for Big Data |journal=Social Science Research Network: A Decade in Internet Time: Symposium on the Dynamics of the Internet and Society |date=September 21, 2011 |doi=10.2139/ssrn.1926431}}</ref>\n\nAnalysis of data sets can find new correlations to "spot business trends, prevent diseases, combat crime and so on".{{r|Economist}} Scientists, business executives, practitioners of medicine, advertising and [[Government database|governments]] alike regularly meet difficulties with large data-sets in areas including [[Web search engine|Internet search]], finance, [[urban informatics]], and [[business informatics]].  Scientists encounter limitations in [[e-Science]] work, including [[meteorology]], [[genomics]],<ref>{{cite journal |title= Community cleverness required |journal= Nature |volume= 455 |issue= 7209 |page= 1 |date= 4 September 2008 |doi= 10.1038/455001a |url= http://www.nature.com/nature/journal/v455/n7209/full/455001a.html}}</ref> [[connectomics]], complex physics simulations, biology and environmental research.<ref>{{cite journal |last1= Reichman |first1= O.J. |last2= Jones |first2= M.B. |last3= Schildhauer |first3= M.P. |title= Challenges and Opportunities of Open Data in Ecology |journal= Science |volume= 331 |issue= 6018 |pages= 703–5 |year= 2011 |doi= 10.1126/science.1197962 |pmid= 21311007 }}</ref>\n\nData sets grow rapidly - in part because they are increasingly gathered by cheap and numerous information-sensing [[mobile device]]s, aerial ([[remote sensing]]), software logs, [[Digital camera|cameras]], microphones, [[radio-frequency identification]] (RFID) readers and [[wireless sensor networks]].<ref>{{cite web |author= Hellerstein, Joe |title= Parallel Programming in the Age of Big Data |date= 9 November 2008 |work= Gigaom Blog |url= http://gigaom.com/2008/11/09/mapreduce-leads-the-way-for-parallel-programming/}}</ref><ref>{{cite book |first1= Toby |last1= Segaran |first2= Jeff |last2= Hammerbacher |title= Beautiful Data: The Stories Behind Elegant Data Solutions |url= https://books.google.com/books?id=zxNglqU1FKgC |year= 2009 |publisher= O\'Reilly Media |isbn= 978-0-596-15711-1 |page= 257}}</ref> The world\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;<ref name="martinhilbert.net">{{cite journal | last1 = Hilbert | first1 = Martin | first2 = Priscila |last2=López | title = The World\'s Technological Capacity to Store, Communicate, and Compute Information | journal = Science | volume = 332 | issue = 6025 | pages = 60–65 | year = 2011 | doi = 10.1126/science.1200970 | pmid = 21310967 | url= http://martinhilbert.net/WorldInfoCapacity.html | ref= harv}}</ref> {{As of|2012|lc=on}}, every day 2.5 [[exabyte]]s (2.5×10<sup>18</sup>) of data are generated.<ref>{{cite web|url= http://www.ibm.com/big-data/us/en/ |title= IBM What is big data? – Bringing big data to the enterprise |publisher= www.ibm.com |accessdate= 2013-08-26}}</ref> One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.<ref>Oracle and FSN, [http://www.fsn.co.uk/channel_bi_bpm_cpm/mastering_big_data_cfo_strategies_to_transform_insight_into_opportunity#.UO2Ac-TTuys "Mastering Big Data: CFO Strategies to Transform Insight into Opportunity"], December 2012</ref>\n\n[[Relational database management system]]s and desktop statistics- and visualization-packages often have difficulty handling big data. The work may require "massively parallel software running on tens, hundreds, or even thousands of servers".<ref>{{cite web |author= Jacobs, A. |title= The Pathologies of Big Data |date= 6 July 2009 |work= ACMQueue |url= http://queue.acm.org/detail.cfm?id=1563874}}</ref> What counts as "big data" varies depending on the capabilities of the users and their tools, and expanding capabilities make big data a moving target. "For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration."<ref>\n{{cite journal \n|last1= Magoulas |first1= Roger \n|last2= Lorica |first2= Ben \n|title= Introduction to Big Data \n|journal= Release 2.0 \n|issue= 11 |date= February 2009 \n|url= http://radar.oreilly.com/r2/release2-0-11.html \n|publisher= O\'Reilly Media \n|location= Sebastopol CA\n}}\n</ref>\n\n== Definition ==\n[[File:Viegas-UserActivityonWikipedia.gif|thumb|Visualization of daily Wikipedia edits created by IBM. At multiple [[terabyte]]s in size, the text and images of Wikipedia are an example of big data.]]\nThe term has been in use since the 1990s, with some giving credit to [[John Mashey]] for coining or at least making it popular.<ref>{{Cite web |title=  Big Data ... and the Next Wave of InfraStress |author= John R. Mashey |date= 25 April 1998 |publisher= Usenix |work= Slides from invited talk |url= http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf |accessdate= 28 September 2016 }}</ref><ref>{{cite web|title=The Origins of ‘Big Data’: An Etymological Detective Story |author=Steve Lohr |date= 1 February 2013 |url=http://bits.blogs.nytimes.com/2013/02/01/the-origins-of-big-data-an-etymological-detective-story/ |publisher= [[New York Times]] |accessdate= 28 September 2016 }}</ref>\nBig data usually includes data sets with sizes beyond the ability of commonly used software tools to [[data acquisition|capture]], [[data curation|curate]], manage, and process data within a tolerable elapsed time.<ref name="Editorial">{{cite journal | last1 = Snijders | first1 = C. | last2 = Matzat | first2 = U. | last3 = Reips | first3 = U.-D. | year = 2012 | title = \'Big Data\': Big gaps of knowledge in the field of Internet | url = http://www.ijis.net/ijis7_1/ijis7_1_editorial.html | journal = International Journal of Internet Science | volume = 7 | issue = | pages = 1–5 }}</ref> Big data "size" is a constantly moving target, {{As of|2012|lc=on}} ranging from a few dozen terabytes to many [[petabyte]]s of data.\nBig data requires a set of techniques and technologies with new forms of integration to reveal insights from datasets that are diverse, complex, and of a massive scale.<ref>{{cite journal | last1 = Ibrahim | first1 =  | last2 = Targio Hashem | first2 = Abaker | last3 = Yaqoob | first3 = Ibrar | last4 = Badrul Anuar | first4 = Nor | last5 = Mokhtar | first5 = Salimah | last6 = Gani | first6 = Abdullah | last7 = Ullah Khan | first7 = Samee | year = 2015 | title = big data" on cloud computing: Review and open research issues | url = | journal = Information Systems | volume = 47 | issue = | pages = 98–115 | doi = 10.1016/j.is.2014.07.006 }}</ref>\n\nIn a 2001 research report<ref>{{cite web |first=Douglas |last=Laney |title=3D Data Management: Controlling Data Volume, Velocity and Variety |url=http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf |publisher=Gartner |accessdate = 6 February 2001}}</ref> and related lectures, [[META Group]] (now [[Gartner]]) analyst [[Doug Laney]] defined data growth challenges and opportunities as being three-dimensional, i.e. increasing [[volume]] (amount of data), [[velocity]] (speed of data in and out), and {{linktext|variety}} (range of data types and sources). Gartner, and now much of the industry, continue to use this "3Vs" model for describing big data.<ref>{{cite web |last=Beyer |first=Mark |title=Gartner Says Solving \'Big Data\' Challenge Involves More Than Just Managing Volumes of Data |url=http://www.gartner.com/it/page.jsp?id=1731916 |publisher=Gartner |accessdate = 13 July 2011| archiveurl= https://web.archive.org/web/20110710043533/http://www.gartner.com/it/page.jsp?id=1731916| archivedate= 10 July 2011 | deadurl= no}}</ref> In 2012, [[Gartner]] updated its definition as follows: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization." Gartner\'s definition of the 3Vs is still widely used, and in agreement with a consensual definition that states that "Big Data represents the Information assets characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value".<ref name="Big Data Definition">{{cite journal | last1 = De Mauro | first1 = Andrea | last2 = Greco | first2 = Marco | last3 = Grimaldi | first3 = Michele | year = 2016 | title = A Formal definition of Big Data based on its essential Features | url = http://www.emeraldinsight.com/doi/abs/10.1108/LR-06-2015-0061 | journal = Library Review | volume = 65| issue = | pages = 122–135 | doi=10.1108/LR-06-2015-0061}}</ref> Additionally, a new V "Veracity" is added by some organizations to describe it,<ref>{{cite web|title=What is Big Data?|url=http://www.villanovau.com/university-online-programs/what-is-big-data/|publisher=[[Villanova University]]}}</ref> revisionism challenged by some industry authorities.<ref>{{cite web|last=Grimes|first=Seth|title=Big Data: Avoid \'Wanna V\' Confusion|url=http://www.informationweek.com/big-data/big-data-analytics/big-data-avoid-wanna-v-confusion/d/d-id/1111077?|publisher=[[InformationWeek]]|accessdate = 5 January 2016}}</ref> The 3Vs have been expanded to other complementary characteristics of big data:<ref name="BD4D">{{cite web |last=Hilbert |first=Martin |title=Big Data for Development: A Review of Promises and Challenges. Development Policy Review. |url=http://www.martinhilbert.net/big-data-for-development |work=martinhilbert.net |accessdate=2015-10-07}}</ref><ref name="WhatIsBigData" />\n* Volume: big data doesn\'t sample; it just observes and tracks what happens\n* Velocity: big data is often available in real-time\n* Variety: big data draws from text, images, audio, video; plus it completes missing pieces through [[data fusion]]\n* [[Machine Learning]]: big data often doesn\'t ask why and simply detects patterns<ref>Mayer-Schönberger, V., & Cukier, K. (2013). Big data: a revolution that will transform how we live, work and think. London: John Murray.</ref>\n* [[Digital footprint]]: big data is often a cost-free byproduct of digital interaction<ref name="WhatIsBigData">{{cite av media|url=https://www.youtube.com/watch?v=XRVIh1h47sA&index=51&list=PLtjBSCvWCU3rNm46D3R85efM0hrzjuAIg|title=DT&SC 7-3: What is Big Data?|date=12 August 2015|publisher=|via=YouTube}}</ref><ref>{{cite web|url=https://canvas.instructure.com/courses/949415|title=Digital Technology & Social Change|publisher=}}</ref>\n\nThe growing maturity of the concept more starkly delineates the difference between big data and [[Business Intelligence]]:<ref>http://www.bigdataparis.com/presentation/mercredi/PDelort.pdf?PHPSESSID=tv7k70pcr3egpi2r6fi3qbjtj6#page=4</ref>\n* Business Intelligence uses [[descriptive statistics]] with data with high information density to measure things, detect trends, etc..\n* Big data uses [[inductive statistics]] and concepts from [[nonlinear system identification]]<ref name="SAB1">Billings S.A. "Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains". Wiley, 2013</ref>  to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density<ref>{{cite web|url=http://www.andsi.fr/tag/dsi-big-data/|title=le Blog ANDSI   » DSI Big Data|publisher=}}</ref> to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.<ref name="SAB1" /><ref>{{cite web|url=http://lecercle.lesechos.fr/entrepreneur/tendances-innovation/221169222/big-data-low-density-data-faible-densite-information-com|title=Les Echos – Big Data car Low-Density Data ? La faible densité en information comme facteur discriminant – Archives|author=Les Echos|date=3 April 2013|work=lesechos.fr}}</ref>\n\n== Characteristics ==\nBig data can be described by the following characteristics:<ref name="BD4D" /><ref name="WhatIsBigData" />\n\n;Volume: The quantity of generated and stored data. The size of the data determines the value and potential insight- and whether it can actually be considered big data or not.\n\n;Variety: The type and nature of the data. This helps people who analyze it to effectively use the resulting insight.\n\n;Velocity: In this context, the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development.\n\n;Variability: Inconsistency of the data set can hamper processes to handle and manage it.\n\n;Veracity: The quality of captured data can vary greatly, affecting accurate analysis.\n\nFactory work and [[Cyber-physical system]]s may have a 6C system:\n* Connection (sensor and networks)\n* Cloud (computing and data on demand)<ref>Wu, D., Liu. X., Hebert, S., Gentzsch, W., Terpenny, J. (2015). Performance Evaluation of Cloud-Based High Performance Computing for Finite Element Analysis. Proceedings of the ASME 2015 International Design Engineering Technical Conference & Computers and Information in Engineering Conference (IDETC/CIE2015), Boston, Massachusetts, U.S.</ref><ref>{{cite journal | last1 = Wu | first1 = D. | last2 = Rosen | first2 = D.W. | last3 = Wang | first3 = L. | last4 = Schaefer | first4 = D. | year = 2015 | title = Cloud-Based Design and Manufacturing: A New Paradigm in Digital Manufacturing and Design Innovation | url = | journal = Computer-Aided Design | volume = 59 | issue = 1| pages = 1–14 | doi = 10.1016/j.cad.2014.07.006 }}</ref>\n* Cyber (model and memory)\n* Content/context (meaning and correlation)\n* Community (sharing and collaboration)\n* Customization (personalization and value)\n\nData must be processed with advanced tools (analytics and algorithms) to reveal meaningful information. For example, to manage a factory one must consider both visible and invisible issues with various components. Information generation algorithms must detect and address invisible issues such as machine degradation, component wear, etc. on the factory floor.<ref name=INDIN2014>{{cite journal|last1=Lee|first1=Jay|last2=Bagheri|first2=Behrad|last3=Kao|first3=Hung-An|title=Recent Advances and Trends of Cyber-Physical Systems and Big Data Analytics in Industrial Informatics|journal=IEEE Int. Conference on Industrial Informatics (INDIN) 2014|date=2014|url=https://www.researchgate.net/profile/Behrad_Bagheri/publication/266375284_Recent_Advances_and_Trends_of_Cyber-Physical_Systems_and_Big_Data_Analytics_in_Industrial_Informatics/links/542dc0100cf27e39fa948a7d?origin=publication_detail}}</ref><ref name=MfgLetters>{{cite journal|last1=Lee|first1=Jay|last2=Lapira|first2=Edzel|last3=Bagheri|first3=Behrad|last4=Kao|first4=Hung-an|title=Recent advances and trends in predictive manufacturing systems in big data environment|journal=Manufacturing Letters|volume=1|issue=1|pages=38–41|doi=10.1016/j.mfglet.2013.09.005|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114}}</ref>\n\n== Architecture ==\n\nIn 2000, Seisint Inc. (now [[LexisNexis|LexisNexis Group]]) developed a C++-based distributed file-sharing framework for data storage and query. The system stores and distributes structured, semi-structured, and [[unstructured data]] across multiple servers. Users can build queries in a C++ [[Dialect (computing)|dialect]] called [[ECL programming language|ECL]]. ECL uses an "apply schema on read" method to infer the structure of stored data when it is queried, instead of when it is stored. In 2004, LexisNexis acquired Seisint Inc.<ref>{{cite web|url=http://www.washingtonpost.com/wp-dyn/articles/A50577-2004Jul14.html|title=LexisNexis To Buy Seisint For $775 Million|publisher=Washington Post|accessdate=15 July 2004}}</ref> and in 2008 acquired [[ChoicePoint|ChoicePoint, Inc.]]<ref>{{cite web|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/02/21/AR2008022100809.html|title=LexisNexis Parent Set to Buy ChoicePoint|publisher=Washington Post|accessdate=22 February 2008}}</ref> and their high-speed parallel processing platform. The two platforms were merged into [[HPCC]] (or High-Performance Computing Cluster) Systems and in 2011, HPCC was open-sourced under the Apache v2.0 License. [[Quantcast File System]] was available about the same time.<ref>{{cite web|url=http://www.datanami.com/2012/10/01/quantcast_opens_exabyte_ready_file_system/|title=Quantcast Opens Exabyte-Ready File System|publisher=www.datanami.com|accessdate=1 October 2012}}</ref>\n\nIn 2004, [[Google]] published a paper on a process called [[MapReduce]] that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data.  With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the Map step). The results are then gathered and delivered (the Reduce step). The framework was very successful,<ref>Bertolucci, Jeff [http://www.informationweek.com/big-data/news/software-platforms/hadoop-from-experiment-to-leading-big-d/240157176 "Hadoop: From Experiment To Leading Big Data Platform"], "Information Week", 2013. Retrieved on 14 November 2013.</ref> so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named [[Apache Hadoop|Hadoop]].<ref>Webster, John. [http://research.google.com/archive/mapreduce-osdi04.pdf "MapReduce: Simplified Data Processing on Large Clusters"], "Search Storage", 2004. Retrieved on 25 March 2013.</ref>\n\n[[MIKE2.0 Methodology|MIKE2.0]] is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled "Big Data Solution Offering".<ref>{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Solution_Offering|title=Big Data Solution Offering|publisher=MIKE2.0|accessdate=8 December 2013}}</ref> The methodology addresses handling big data in terms of useful [[permutation]]s of data sources, [[complexity]] in interrelationships, and difficulty in deleting (or modifying) individual records.<ref>{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Definition|title=Big Data Definition|publisher=MIKE2.0|accessdate=9 March 2013}}</ref>\n\n2012 studies showed that a multiple-layer architecture is one option to address the issues that big data presents. A [[List of file systems#Distributed parallel file systems|distributed parallel]] architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end user by using a front-end application server.<ref>{{cite journal|last=Boja|first=C|author2=Pocovnicu, A |author3=Bătăgan, L. |title=Distributed Parallel Architecture for Big Data|journal=Informatica Economica|year=2012|volume=16|issue=2|pages=116–127}}</ref>\n\nBig data analytics for manufacturing applications is marketed as a 5C architecture (connection, conversion, cyber, cognition, and configuration).<ref>{{cite web|url=http://www.imscenter.net/cyber-physical-platform|title=IMS_CPS — IMS Center|publisher=|accessdate=16 June 2016}}</ref>\n\nThe [[data lake]] allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.<ref>http://www.hcltech.com/sites/default/files/solving_key_businesschallenges_with_big_data_lake_0.pdf</ref><ref>{{ cite web| url=https://secplab.ppgia.pucpr.br/files/papers/2015-0.pdf | title= Method for testing the fault tolerance of MapReduce frameworks | publisher=Computer Networks | year=2015}}</ref>\n\n== Technologies ==\n{{see|Enablers of big data}}\nA 2011 [[McKinsey & Company|McKinsey Global Institute]] report characterizes the main components and ecosystem of big data as follows:<ref name="McKinsey">{{cite journal\n | last1 = Manyika\n | first1 = James\n | first2=Michael |last2=Chui |first3=Jaques |last3=Bughin |first4=Brad |last4=Brown |first5=Richard |last5=Dobbs |first6=Charles |last6=Roxburgh |first7=Angela Hung |last7=Byers\n | title = Big Data: The next frontier for innovation, competition, and productivity\n | publisher = McKinsey Global Institute\n | date = May 2011\n | url =  http://www.mckinsey.com/Insights/MGI/Research/Technology_and_Innovation/Big_data_The_next_frontier_for_innovation\n|accessdate=January 16, 2016\n}}</ref>\n* Techniques for analyzing data, such as [[A/B testing]], [[machine learning]] and [[natural language processing]]\n* Big data technologies, like [[business intelligence]], [[cloud computing]] and databases\n* Visualization, such as charts, graphs and other displays of the data\n\nMultidimensional big data can also be represented as [[tensor]]s, which can be more efficiently handled by tensor-based computation,<ref>{{cite web |title=Future Directions in Tensor-Based Computation and Modeling |date=May 2009|url=http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf}}</ref> such as [[multilinear subspace learning]].<ref name="MSLsurvey">{{cite journal\n |first=Haiping |last=Lu\n |first2=K.N. |last2=Plataniotis\n |first3=A.N. |last3=Venetsanopoulos\n |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf\n |title=A Survey of Multilinear Subspace Learning for Tensor Data\n |journal=Pattern Recognition\n |volume=44 |number=7 |pages=1540–1551 |year=2011\n |doi=10.1016/j.patcog.2011.01.004\n}}</ref> Additional technologies being applied to big data include massively parallel-processing ([[Massive parallel processing|MPP]]) databases, [[search-based application]]s, [[data mining]],<ref>{{cite web|last1=Pllana|first1=Sabri|last2=Janciak|first2=Ivan|last3=Brezany|first3=Peter|last4=Wöhrer|first4=Alexander|title=A Survey of the State of the Art in Data Mining and Integration Query Languages|url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6041580|website=2011 International Conference on Network-Based Information Systems (NBIS 2011)|publisher=IEEE Computer Society|accessdate=2 April 2016}}</ref> [[distributed file system]]s, [[distributed database]]s, [[cloud computing|cloud-based]] infrastructure (applications, storage and computing resources) and the Internet.{{Citation needed|date=September 2011}}\n\nSome but not all [[Massive parallel processing|MPP]] relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the [[RDBMS]].<ref>{{cite web |author=Monash, Curt |title=eBay\'s two enormous data warehouses |date=30 April 2009 |url=http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/}}<br />{{cite web |author=Monash, Curt |title=eBay followup&nbsp;– Greenplum out, Teradata > 10 petabytes, Hadoop has some value, and more |date=6 October 2010 |url=http://www.dbms2.com/2010/10/06/ebay-followup-greenplum-out-teradata-10-petabytes-hadoop-has-some-value-and-more/}}</ref>\n\n[[DARPA]]\'s [[Topological Data Analysis]] program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called [[Ayasdi]].<ref>{{cite web|url=http://www.ayasdi.com/resources/|title=Resources on how Topological Data Analysis is used to analyze big data|publisher=Ayasdi}}</ref>\n\nThe practitioners of big data analytics processes are generally hostile to slower shared storage,<ref>{{cite web |title=Storage area networks need not apply |author=CNET News |date=1 April 2011 |url=http://news.cnet.com/8301-21546_3-20049693-10253464.html}}</ref> preferring direct-attached storage ([[Direct-attached storage|DAS]]) in its various forms from solid state drive ([[Ssd]]) to high capacity [[Serial ATA|SATA]] disk buried inside parallel processing nodes. The perception of shared storage architectures—[[Storage area network]] (SAN) and [[Network-attached storage]] (NAS) —is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.\n\nReal or near-real time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in memory is good—data on spinning disk at the other end of a [[Fiber connector|FC]] [[Storage area network|SAN]] connection is not. The cost of a [[Storage area network|SAN]] at the scale needed for analytics applications is very much higher than other storage techniques.\n\nThere are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners {{As of|2011|lc=on}} did not favour it.<ref>{{cite web |title=How New Analytic Systems will Impact Storage |date=September 2011 |url=http://www.evaluatorgroup.com/document/big-data-how-new-analytic-systems-will-impact-storage-2/}}</ref>\n\n== Applications ==\n[[File:2013-09-11 Bus wrapped with SAP Big Data parked outside IDF13 (9730051783).jpg|thumb|Bus wrapped with [[SAP AG|SAP]] Big data parked outside [[Intel Developer Forum|IDF13]].]]\nBig data has increased the demand of information management specialists so much so that [[Software AG]], [[Oracle Corporation]], [[IBM]], [[Microsoft]], [[SAP AG|SAP]], [[EMC Corporation|EMC]], [[Hewlett-Packard|HP]] and [[Dell]] have spent more than $15&nbsp;billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100&nbsp;billion and was growing at almost 10&nbsp;percent a year: about twice as fast as the software business as a whole.{{r|Economist}}\n\nDeveloped economies increasingly use data-intensive technologies. There are 4.6&nbsp;billion mobile-phone subscriptions worldwide, and between 1&nbsp;billion and 2&nbsp;billion people accessing the internet.{{r|Economist}} Between 1990 and 2005, more than 1&nbsp;billion people worldwide entered the middle class, which means more people became more literate, which in turn lead to information growth. The world\'s effective capacity to exchange information through telecommunication networks was 281 [[petabytes]] in 1986, 471 [[petabytes]] in 1993, 2.2 exabytes in 2000, 65 [[exabytes]] in 2007<ref name="martinhilbert.net"/> and predictions put the amount of internet traffic at 667 exabytes annually by 2014.{{r|Economist}} According to one estimate, one third of the globally stored information is in the form of alphanumeric text and still image data,<ref name="HilbertContent">{{cite web|url=http://www.tandfonline.com/doi/abs/10.1080/01972243.2013.873748|title=An Error Occurred Setting Your User Cookie|publisher=}}</ref> which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).\n\nWhile many vendors offer off-the-shelf solutions for big data, experts recommend the development of in-house solutions custom-tailored to solve the company\'s problem at hand if the company has sufficient technical capabilities.<ref>{{cite web |url=http://www.kdnuggets.com/2014/07/interview-amy-gershkoff-ebay-in-house-BI-tools.html |title=Interview: Amy Gershkoff, Director of Customer Analytics & Insights, eBay on How to Design Custom In-House BI Tools |last1=Rajpurohit |first1=Anmol |date=11 July 2014 |website= KDnuggets|accessdate=2014-07-14|quote=Dr. Amy Gershkoff: "Generally, I find that off-the-shelf business intelligence tools do not meet the needs of clients who want to derive custom insights from their data. Therefore, for medium-to-large organizations with access to strong technical talent, I usually recommend building custom, in-house solutions."}}</ref>\n\n=== Government ===\nThe use and adoption of big data within governmental processes is beneficial and allows efficiencies in terms of cost, productivity, and innovation,<ref>{{cite web|url=http://www.computerworld.com/article/2472667/government-it/the-government-and-big-data--use--problems-and-potential.html |title=The Government and big data: Use, problems and potential |date=21 March 2012 |publisher=Computerworld |access-date=12 September 2016}}</ref> but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. Below are some examples of initiatives the governmental big data space.\n\n==== United States of America ====\n* In 2012, the [[Presidency of Barack Obama|Obama administration]] announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government.<ref name=WH_Big_Data>{{cite web|last=Kalil|first=Tom|title=Big Data is a Big Deal|url=http://www.whitehouse.gov/blog/2012/03/29/big-data-big-deal|publisher=White House|accessdate=26 September 2012}}</ref> The initiative is composed of 84 different big data programs spread across six departments.<ref>{{cite web|last=Executive Office of the President|title=Big Data Across the Federal Government|url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_fact_sheet_final_1.pdf|publisher=White House|accessdate=26 September 2012 |date=March 2012}}</ref>\n* Big data analysis played a large role in [[Barack Obama]]\'s successful [[Barack Obama presidential campaign, 2012|2012 re-election campaign]].<ref name=infoworld_bigdata>{{cite web|last=Lampitt|first=Andrew|title=The real story of how big data analytics helped Obama win|url=http://www.infoworld.com/d/big-data/the-real-story-of-how-big-data-analytics-helped-obama-win-212862|work=[[Infoworld]]|accessdate=31 May 2014}}</ref>\n* The [[United States Federal Government]] owns six of the ten most powerful [[supercomputer]]s in the world.<ref>{{cite web |last=Hoover |first=J. Nicholas |title=Government\'s 10 Most Powerful Supercomputers |url=http://www.informationweek.com/government/enterprise-applications/image-gallery-governments-10-most-powerf/224700271 |work=Information Week |publisher=UBM |accessdate=26 September 2012}}</ref>\n* The [[Utah Data Center]] has been constructed by the United States [[National Security Agency]]. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few [[exabyte]]s.<ref>{{cite news | last=Bamford|first=James|title=The NSA Is Building the Country\'s Biggest Spy Center (Watch What You Say)|url=http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/1|work=Wired Magazine|accessdate=2013-03-18|date=15 March 2012}}</ref><ref>{{cite web|url=http://www.nsa.gov/public_info/press_room/2011/utah_groundbreaking_ceremony.shtml|title=Groundbreaking Ceremony Held for $1.2 Billion Utah Data Center|publisher=National Security Agency Central Security Service|accessdate=2013-03-18}}</ref><ref>{{cite news | last=Hill|first=Kashmir|title=TBlueprints of NSA\'s Ridiculously Expensive Data Center in Utah Suggest It Holds Less Info Than Thought|url=http://www.forbes.com/sites/kashmirhill/2013/07/24/blueprints-of-nsa-data-center-in-utah-suggest-its-storage-capacity-is-less-impressive-than-thought/|work=Forbes|accessdate=2013-10-31}}</ref>\n\n==== India ====\n* Big data analysis was in part responsible for the [[Bharatiya Janata Party|BJP]] to win the [[Indian general election, 2014|Indian General Election 2014]].<ref>{{cite web|url = http://www.livemint.com/Industry/bUQo8xQ3gStSAy5II9lxoK/Are-Indian-companies-making-enough-sense-of-Big-Data.html|title = News: Live Mint|date = 23 June 2014|accessdate = 2014-11-22|website = Are Indian companies making enough sense of Big Data?|publisher = Live Mint}}</ref>\n* The [[Government of India|Indian government]] utilizes numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.<ref>{{cite web|url=http://decipherias.com/currentaffairs/big-data-whats-so-big-about-it/|title=Big Data- What’s so big about it?|date=18 March 2016|publisher=Decipher IAS|access-date=12 September 2016}}</ref>\n\n==== United Kingdom ====\nExamples of uses of big data in public services:\n* Data on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify the considerable delay between the release of any given drug, and a UK-wide adaptation of the [[National Institute for Health and Care Excellence]] guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.<ref>{{cite web|url=https://www.ijedr.org/papers/IJEDR1504022.pdf|title=Survey on Big Data Using Data Mining|date=2015|publisher=International Journal of Engineering Development and Research|access-date=14 September 2016}}</ref>\n* Joining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as \'meals on wheels\'. The connection of data allowed the local authority to avoid any weather related delay.<ref>{{cite web|url=https://www.researchgate.net/publication/297762848_Recent_advances_delivered_by_mobile_cloud_computing_and_Internet_of_Things_for_Big_data_applications_A_Survey|title=Recent advances delivered by Mobile Cloud Computing and Internet of Things for Big Data applications: a survey|date=11 March 2016|publisher=International Journal of Network Management|access-date=14 September 2016}}</ref>\n\n=== International development ===\nResearch on the effective usage of [[information and communication technologies for development]] (also known as [[ICT4D]]) suggests that big data technology can make important contributions but also present unique challenges to [[International development]].<ref>{{cite web|url=http://www.unglobalpulse.org/projects/BigDataforDevelopment|title=White Paper: Big Data for Development: Opportunities & Challenges (2012) – United Nations Global Pulse|publisher=|accessdate=13 April 2016}}</ref><ref>{{cite web|title=WEF (World Economic Forum), & Vital Wave Consulting. (2012). Big Data, Big Impact: New Possibilities for International Development|work= World Economic Forum|accessdate=24 August 2012|url= http://www.weforum.org/reports/big-data-big-impact-new-possibilities-international-development}}</ref> Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, [[economic productivity]], crime, security, and [[natural disaster]] and resource management.<ref name="HilbertBigData2013" /><ref>{{cite web|url=http://blogs.worldbank.org/ic4d/four-ways-to-talk-about-big-data/|title=Elena Kvochko, Four Ways To talk About Big Data (Information Communication Technologies for Development Series)|publisher=worldbank.org|accessdate=2012-05-30}}</ref><ref>{{cite web|title=Daniele Medri: Big Data & Business: An on-going revolution|url=http://www.statisticsviews.com/details/feature/5393251/Big-Data--Business-An-on-going-revolution.html|publisher=Statistics Views |date=21 October 2013}}</ref> Additionally, user-generated data offers new opportunities to give the unheard a voice.<ref>{{cite web|title=Responsible use of data|author=Tobias Knobloch and Julia Manske|work= D+C, Development and Cooperation|date=11 January 2016|url= http://www.dandc.eu/en/article/opportunities-and-risks-user-generated-and-automatically-compiled-data}}</ref> However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.<ref name="HilbertBigData2013" />\n\n=== Manufacturing ===\nBased on TCS 2013 Global Trend Study, improvements in supply planning and product quality provide the greatest benefit of big data for manufacturing.<ref name="TCS Big Data Study – Manufacturing">{{cite web|url=http://sites.tcs.com/big-data-study/manufacturing-big-data-benefits-challenges/# |title=Manufacturing: Big Data Benefits and Challenges |work= TCS Big Data Study|publisher=[[Tata Consultancy Services Limited]] |location=Mumbai, India |accessdate=2014-06-03}}</ref> Big data provides an infrastructure for transparency in manufacturing industry, which is the ability to unravel uncertainties such as inconsistent component performance and availability. Predictive manufacturing as an applicable approach toward near-zero downtime and transparency requires vast amount of data and advanced prediction tools for a systematic process of data into useful information.<ref>{{cite journal|last=Lee|first=Jay|author2=Wu, F. |author3=Zhao, W. |author4=Ghaffari, M. |author5= Liao, L |title=Prognostics and health management design for rotary machinery systems—Reviews, methodology and applications|journal=Mechanical Systems and Signal Processing|date=January 2013|volume=42|issue=1}}</ref> A conceptual framework of predictive manufacturing begins with data acquisition where different type of sensory data is available to acquire such as acoustics, vibration, pressure, current, voltage and controller data. Vast amount of sensory data in addition to historical data construct the big data in manufacturing. The generated big data acts as the input into predictive tools and preventive strategies such as [[Prognostics]] and Health Management (PHM).<ref>{{cite web|url=https://www.phmsociety.org/events/conference/phm/europe/16/tutorials|title=Tutorials|publisher=PHM Society|accessdate=27 September 2016}}</ref><ref>{{cite web|url=https://www.itri.org.tw/eng/Content/MSGPic01/contents.aspx?&SiteID=1&MmmID=620651706136357202&CatID=620653256103620163&MSID=654532365564567545|title=Prognostic and Health Management Technology for MOCVD Equipment|publisher=Industrial Technology Research Institute|accessdate=27 September 2016}}</ref>\n\n==== Cyber-physical models ====\nCurrent PHM implementations mostly use data during the actual usage while analytical algorithms can perform more accurately when more information throughout the machine\'s lifecycle, such as system configuration, physical knowledge and working principles, are included. There is a need to systematically integrate, manage and analyze machinery or process data during different stages of machine life cycle to handle data/information more efficiently and further achieve better transparency of machine health condition for manufacturing industry.\n\nWith such motivation a cyber-physical (coupled) model scheme has been developed. The coupled model is a digital twin of the real machine that operates in the cloud platform and simulates the health condition with an integrated knowledge from both data driven analytical algorithms as well as other available physical knowledge. It can also be described as a 5S systematic approach consisting of sensing, storage, synchronization, synthesis and service. The coupled model first constructs a digital image from the early design stage. System information and physical knowledge are logged during product design, based on which a simulation model is built as a reference for future analysis. Initial parameters may be statistically generalized and they can be tuned using data from testing or the manufacturing process using parameter estimation. After that step, the simulation model can be considered a mirrored image of the real machine—able to continuously record and track machine condition during the later utilization stage. Finally, with the increased connectivity offered by cloud computing technology, the coupled model also provides better accessibility of machine condition for factory managers in cases where physical access to actual equipment or machine data is limited.<ref name="MfgLetters" />\n\n=== Healthcare ===\nBig data analytics has helped healthcare improve by providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries and fragmented point solutions.<ref name="ref135">{{cite journal|doi=10.1016/j.ijrobp.2015.10.060|title=Impending Challenges for the Use of Big Data }}</ref> Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality.<ref>{{cite journal|url=http://doi.acm.org/10.1145/2378016.2378021|title=Data Management Within mHealth Environments: Patient Sensors, Mobile Devices, and Databases|first1=John|last1=O\'Donoghue|first2=John|last2=Herbert|date=1 October 2012|publisher=|volume=4|issue=1|pages=5:1–5:20|accessdate=16 June 2016|via=ACM Digital Library|doi=10.1145/2378016.2378021}}</ref> "Big data very often means `dirty data\' and the fraction of data inaccuracies increases with data volume growth." Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed.<ref name="Mirkes2016">{{cite journal | last1 = Mirkes| first1 =E.M.|last2 = Coats|first2 =T.J.|last3 = Levesley|first3 =J.|last4 = Gorban|first4 = A.N.| title = Handling missing data in large healthcare dataset: A case study of unknown trauma outcomes|url =  https://www.researchgate.net/publication/300400110_Handling_missing_data_in_large_healthcare_dataset_A_case_study_of_unknown_trauma_outcomes| journal = Computers in Biology and Medicine| volume = 75| issue = | pages = 203–216| year = 2016| doi = 10.1016/j.compbiomed.2016.06.004}}</ref> While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use.<ref>{{Cite journal|last=Murdoch|first=Travis B.|last2=Detsky|first2=Allan S.|date=2013-04-03|title=The Inevitable Application of Big Data to Health Care|url=http://jamanetwork.com/journals/jama/article-abstract/1674245|journal=JAMA|language=en|volume=309|issue=13|doi=10.1001/jama.2013.393|issn=0098-7484}}</ref>\n\n=== Education ===\nA [[McKinsey & Company|McKinsey Global Institute]] study found a shortage of 1.5 million highly trained data professionals and managers<ref name="McKinsey"/> and a number of universities<ref>{{cite news\n| url=http://www.forbes.com/sites/jmaureenhenderson/2013/07/30/degrees-in-big-data-fad-or-fast-track-to-career-success/?\n|access-date=2016-02-21\n|newspaper=Forbes\n|title=Degrees in Big Data: Fad or Fast Track to Career Success}}</ref> including [[University of Tennessee]] and [[UC Berkeley]], have created masters programs to meet this demand.  Private bootcamps have also developed programs to meet that demand, including free programs like [[The Data Incubator]] or paid programs like [[General Assembly]].<ref>{{cite news\n|title=NY gets new bootcamp for data scientists: It’s free, but harder to get into than Harvard\n|newspaper=Venture Beat\n|access-date=2016-02-21\n|url=http://venturebeat.com/2014/04/15/ny-gets-new-bootcamp-for-data-scientists-its-free-but-harder-to-get-into-than-harvard/\n}}</ref>\n\n=== Media ===\nTo understand how the media utilises big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that [[wikt:practitioner|practitioners]] in Media and Advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations.  The ultimate aim is to serve, or convey, a message or content that is (statistically speaking) in line with the consumer\'s mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various [[data-mining]] activities.<ref>{{cite journal|last1=Couldry|first1=Nick|last2=Turow|first2=Joseph|title=Advertising, Big Data, and the Clearance of the Public Realm: Marketers’ New Approaches to the Content Subsidy|journal=International Journal of Communication|date=2014|volume=8|pages=1710–1726}}</ref>\n* Targeting of consumers (for advertising by marketers)\n* Data-capture\n* [[Data journalism]]: publishers and journalists use big data tools to provide unique and innovative insights and infographics.\n\n==== Internet of Things (IoT) ====\n{{tone|section|date=September 2016}}\n\n{{Main article|Internet of Things}}\nBig data and the IoT work in conjunction.  From a media perspective, data is the key derivative of device inter-connectivity and allows accurate targeting.  The [[Internet of Things]], with the help of big data, therefore transforms the media industry, companies and even governments, opening up a new era of economic growth and competitiveness. The intersection of people, data and intelligent algorithms have far-reaching impacts on media efficiency. The wealth of data generated allows an elaborate layer on the present targeting mechanisms of the industry.\n\n==== Technology ====\n* [[eBay.com]] uses two data warehouses at 7.5 [[petabytes]] and 40PB as well as a 40PB [[Hadoop]] cluster for search, consumer recommendations, and merchandising.<ref>{{cite web | last=Tay | first=Liz |url=http://www.itnews.com.au/news/inside-ebay8217s-90pb-data-warehouse-342615 | title=Inside eBay’s 90PB data warehouse | publisher=ITNews | accessdate=2016-02-12}}</ref>\n* [[Amazon.com]] handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world\'s three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.<ref>{{cite web|last=Layton |first=Julia |url=http://money.howstuffworks.com/amazon1.htm |title=Amazon Technology |publisher=Money.howstuffworks.com |accessdate=2013-03-05}}</ref>\n* [[Facebook]] handles 50&nbsp;billion photos from its user base.<ref>{{cite web|url=https://www.facebook.com/notes/facebook-engineering/scaling-facebook-to-500-million-users-and-beyond/409881258919 |title=Scaling Facebook to 500 Million Users and Beyond |publisher=Facebook.com |accessdate=2013-07-21}}</ref>\n* As of August 2012, [[Google]] was handling roughly 100&nbsp;billion searches per month.<ref>{{cite web|url=http://searchengineland.com/google-1-trillion-searches-per-year-212940|title=Google Still Doing at Least 1 Trillion Searches Per Year|date=16 January 2015|work=Search Engine Land|accessdate=15 April 2015}}</ref>\n* [[Oracle NoSQL Database]] has been tested to past the 1M ops/sec mark with 8 shards and proceeded to hit 1.2M ops/sec with 10 shards.<ref>{{cite web |last=Lamb |first=Charles |url=https://blogs.oracle.com/charlesLamb/entry/oracle_nosql_database_exceeds_1 |title=Oracle NoSQL Database Exceeds 1 Million Mixed YCSB Ops/Sec}}</ref>\n\n=== Private sector ===\n\n=== Information Technology ===\nEspecially since 2015, big data has come to prominence within [[Business Operations]] as a tool to help employees work more efficiently and streamline the collection and distribution of [[Information Technology]] (IT). The use of big data to attack IT and data collection issues within an enterprise is called [[IT Operations Analytics]] (ITOA).<ref name="ITOA1">{{cite web|last1=Solnik|first1=Ray|title=The Time Has Come: Analytics Delivers for IT Operations|url=http://www.datacenterjournal.com/time-analytics-delivers-operations/|website=Data Center Journal|accessdate=June 21, 2016}}</ref> By applying big data principles into the concepts of [[machine intelligence]] and [[deep computing]], IT departments can predict potential issues and move to provide solutions before the problems even happen.<ref name="ITOA1" /> In this time, ITOA businesses were also beginning to play a major role in [[systems management]] by offering platforms that brought individual [[data silos]] together and generated insights from the whole of the system rather than from isolated pockets of data.\n\n==== Retail ====\n* [[Walmart]] handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data—the equivalent of 167 times the information contained in all the books in the US [[Library of Congress]].{{r|Economist}}\n\n==== Retail banking ====\n* FICO Card Detection System protects accounts worldwide.<ref name="fico.com">{{cite web|url=http://www.fico.com/en/Products/DMApps/Pages/FICO-Falcon-Fraud-Manager.aspx |title=FICO® Falcon® Fraud Manager |publisher=Fico.com |accessdate=2013-07-21}}</ref>\n* The volume of business data worldwide, across all companies, doubles every 1.2 years, according to estimates.<ref name="KnowWPCarey.com">{{cite web|url=http://research.wpcarey.asu.edu/managing-it/ebay-study-how-to-build-trust-and-improve-the-shopping-experience |title=eBay Study: How to Build Trust and Improve the Shopping Experience |publisher=Knowwpcarey.com |date=8 May 2012 |accessdate=2015-12-20}}</ref><ref>[http://www.statista.com/statistics/280444/global-leading-priorities-for-big-data-according-to-business-and-it-executives/ Leading Priorities for Big Data for Business and IT]. eMarketer. October 2013. Retrieved January 2014.</ref>\n\n==== Real estate ====\n* [[Windermere Real Estate]] uses anonymous GPS signals from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.<ref>{{cite news|last=Wingfield |first=Nick |url=http://bits.blogs.nytimes.com/2013/03/12/predicting-commutes-more-accurately-for-would-be-home-buyers/ |title=Predicting Commutes More Accurately for Would-Be Home Buyers – NYTimes.com |publisher=Bits.blogs.nytimes.com |date=12 March 2013 |accessdate=2013-07-21}}</ref>\n\n=== Science ===\nThe [[Large Hadron Collider]] experiments represent about 150 million sensors delivering data 40&nbsp;million times per second. There are nearly 600&nbsp;million collisions per second. After filtering and refraining from recording more than 99.99995%<ref>{{cite web|last1=Alexandru|first1=Dan|title=Prof|url=https://cds.cern.ch/record/1504817/files/CERN-THESIS-2013-004.pdf|website=cds.cern.ch|publisher=CERN|accessdate=24 March 2015}}</ref> of these streams, there are 100 collisions of interest per second.<ref>{{cite web |title=LHC Brochure, English version. A presentation of the largest and the most powerful particle accelerator in the world, the Large Hadron Collider (LHC), which started up in 2008. Its role, characteristics, technologies, etc. are explained for the general public. |url=http://cds.cern.ch/record/1278169?ln=en |work=CERN-Brochure-2010-006-Eng. LHC Brochure, English version. |publisher=CERN |accessdate=20 January 2013}}</ref><ref>{{cite web |title=LHC Guide, English version. A collection of facts and figures about the Large Hadron Collider (LHC) in the form of questions and answers. |url=http://cds.cern.ch/record/1092437?ln=en |work=CERN-Brochure-2008-001-Eng. LHC Guide, English version. |publisher=CERN |accessdate=20 January 2013}}</ref><ref name="nature">{{cite news |title=High-energy physics: Down the petabyte highway |work= Nature |date= 19 January 2011 |first=Geoff |last=Brumfiel |doi= 10.1038/469282a |volume= 469 |pages= 282–83 |url= http://www.nature.com/news/2011/110119/full/469282a.html }}</ref>\n* As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.\n* If all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 [[exabyte]]s per day, before replication. To put the number in perspective, this is equivalent to 500 [[quintillion]] (5×10<sup>20</sup>) bytes per day, almost 200 times more than all the other sources combined in the world.\n\nThe [[Square Kilometre Array]] is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day.<ref>http://www.zurich.ibm.com/pdf/astron/CeBIT%202013%20Background%20DOME.pdf</ref><ref>{{cite web|url=http://arstechnica.com/science/2012/04/future-telescope-array-drives-development-of-exabyte-processing/|title=Future telescope array drives development of exabyte processing|work=Ars Technica|accessdate=15 April 2015}}</ref> It is considered one of the most ambitious scientific projects ever undertaken.<ref>{{cite web|url=http://theconversation.com/australias-bid-for-the-square-kilometre-array-an-insiders-perspective-4891|title=Australia’s bid for the Square Kilometre Array – an insider’s perspective|date=1 February 2012|publisher=[[The Conversation (website)|The Conversation]]|accessdate=27 September 2016}}</ref>\n\n==== Science and research ====\n{{Expand section|date=December 2016}}\n* When the [[Sloan Digital Sky Survey]] (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200&nbsp;GB per night, SDSS has amassed more than 140 terabytes of information.<ref name="Economist">{{cite news |title=Data, data everywhere |url=http://www.economist.com/node/15557443 |newspaper=The Economist |date=25 February 2010 |accessdate=9 December 2012}}</ref> When the [[Large Synoptic Survey Telescope]], successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.{{r|Economist}}\n* Decoding the [[Human Genome Project|human genome]] originally took 10 years to process, now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by [[Moore\'s Law]].<ref>[http://www.oecd.org/sti/ieconomy/Session_3_Delort.pdf#page=6 Delort P., OECD ICCP Technology Foresight Forum, 2012.]</ref>\n* The [[NASA]] Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.<ref>{{cite web|url=http://www.nasa.gov/centers/goddard/news/releases/2010/10-051.html|title=NASA – NASA Goddard Introduces the NASA Center for Climate Simulation|publisher=|accessdate=13 April 2016}}</ref><ref>{{cite web|last=Webster|first=Phil|title=Supercomputing the Climate: NASA\'s Big Data Mission|url=http://www.csc.com/cscworld/publications/81769/81773-supercomputing_the_climate_nasa_s_big_data_mission|work=CSC World|publisher=Computer Sciences Corporation|accessdate=2013-01-18}}</ref>\n* Google\'s DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any \'friction points,\' or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google\'s search server to scale social experiments that would usually take years, instantly.<ref>{{cite web|url=http://www.theglobeandmail.com/life/health-and-fitness/health/these-six-great-neuroscience-ideas-could-make-the-leap-from-lab-to-market/article21681731/|title=These six great neuroscience ideas could make the leap from lab to market|date=20 November 2014|publisher=[[The Globe and Mail]]|accessdate=1 October 2016}}</ref><ref>{{cite web|url=https://cloud.google.com/customers/dnastack/|title=DNAstack tackles massive, complex DNA datasets with Google Genomics|publisher=Google Cloud Platform |accessdate=1 October 2016}}</ref>\n* [[23andme]]\'s [[DNA database]] contains genetic information of over 1,000,000 people worldwide.<ref>{{cite web|title=23andMe - Ancestry|url=https://www.23andme.com/en-int/ancestry/|website=23andme.com|accessdate=29 December 2016}}</ref> The company explores selling the "anonymous aggregated genetic data" to other researchers and pharmaceutical companies for research purposes if patients give their consent.<ref name=verge1>{{cite web|last1=Potenza|first1=Alessandra|title=23andMe wants researchers to use its kits, in a bid to expand its collection of genetic data|url=http://www.theverge.com/2016/7/13/12166960/23andme-genetic-testing-database-genotyping-research|publisher=The Verge|accessdate=29 December 2016|date=13 July 2016}}</ref><ref>{{cite web|title=This Startup Will Sequence Your DNA, So You Can Contribute To Medical Research|url=https://www.fastcompany.com/3066775/innovation-agents/this-startup-will-sequence-your-dna-so-you-can-contribute-to-medical-resea|publisher=Fast Company|accessdate=29 December 2016|date=23 December 2016}}</ref><ref>{{cite web|last1=Seife|first1=Charles|title=23andMe Is Terrifying, but Not for the Reasons the FDA Thinks|url=https://www.scientificamerican.com/article/23andme-is-terrifying-but-not-for-the-reasons-the-fda-thinks/|publisher=Scientific American|accessdate=29 December 2016}}</ref><ref>{{cite web|last1=Zaleski|first1=Andrew|title=This biotech start-up is betting your genes will yield the next wonder drug|url=http://www.cnbc.com/2016/06/22/23andme-thinks-your-genes-are-the-key-to-blockbuster-drugs.html|publisher=CNBC|accessdate=29 December 2016|date=22 June 2016}}</ref><ref>{{cite web|last1=Regalado|first1=Antonio|title=How 23andMe turned your DNA into a $1 billion drug discovery machine|url=https://www.technologyreview.com/s/601506/23andme-sells-data-for-drug-search/|publisher=MIT Technology Review|accessdate=29 December 2016}}</ref> Ahmad Hariri, professor of psychology and neuroscience at [[Duke University]] who has been using 23andMe in his research since 2009 states that the most important aspect of the company\'s new service is that it makes genetic research accessible and relatively cheap for scientists.<ref name=verge1/> A study that identified 15 genome sites linked to depression in 23andMe\'s database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.<ref>{{cite web|title=23andMe reports jump in requests for data in wake of Pfizer depression study {{!}} FierceBiotech|url=http://www.fiercebiotech.com/it/23andme-reports-jump-requests-for-data-wake-pfizer-depression-study|website=fiercebiotech.com|accessdate=29 December 2016}}</ref>\n\n=== Sports ===\nBig data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.<ref>{{cite web|url=http://www.itweb.co.za/index.php?option=com_content&view=article&id=147241|title=Data scientists predict Springbok defeat\n|author=Admire Moyo|work=www.itweb.co.za|accessdate=12 December 2015}}</ref>\nFuture performance of players could be predicted as well. Thus, players\' value and salary is determined by data collected throughout the season.<ref>{{cite web|url=http://www.itweb.co.za/index.php?option=com_content&view=article&id=147852|title= Predictive analytics, big data transform sports\n |author=Regina Pazvakavambwa|work=www.itweb.co.za|accessdate=12 December 2015}}</ref>\n\nThe movie [[Moneyball (film)|\'\'MoneyBall\'\']] demonstrates how big data could be used to scout players and also identify undervalued players.<ref>{{cite web|url=http://www.datacenterknowledge.com/archives/2011/09/23/the-lessons-of-moneyball-for-big-data-analysis/|title= The Lessons of Moneyball for Big Data Analysis|author=Rich Miller|work=www.datecenterknowledge.com|accessdate=12 December 2015}}</ref>\n\nIn Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency. Then, this data is transferred to team headquarters in United Kingdom through fiber optic cables that could carry data at the speed of light.<ref>{{cite web|url=http://www.huffingtonpost.com/dave-ryan/sports-where-big-data-fin_b_8553884.html|title= Sports: Where Big Data Finally Makes Sense |author=Dave Ryan|work=www.huffingtonpost.com|accessdate=12 December 2015}}</ref>\nBased on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.<ref>{{cite web|url=http://www.forbes.com/sites/frankbi/2014/11/13/how-formula-one-teams-are-using-big-data-to-get-the-inside-edge//|title= How Formula One Teams Are Using Big Data To Get The Inside Edge|author=Frank Bi|work=www.forbes.com|accessdate=12 December 2015}}</ref>\n\n== Research activities ==\nEncrypted search and cluster formation in big data was demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at \'\'Tackling the challenges of Big Data\'\' by [[MIT Computer Science and Artificial Intelligence Laboratory]] and Dr. Amir Esmailpour at UNH Research Group investigated the key features of big data as formation of clusters and their interconnections. They focused on the security of big data and the actual orientation of the term towards the presence of different type of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.<ref>{{cite conference |url=http://asee-ne.org/proceedings/2014/Student%20Papers/210.pdf |title=Encrypted Search & Cluster Formation in Big Data |last1=Siwach |first1=Gautam |last2=Esmailpour |first2=Amir |date=March 2014 |year= |conference=ASEE 2014 Zone I Conference |conference-url=http://ubconferences.org/ |location=[[University of Bridgeport]], [[Bridgeport, Connecticut]], US }}</ref>\n\nIn March 2012, The White House announced a national "Big Data Initiative" that consisted of six Federal departments and agencies committing more than $200&nbsp;million to big data research projects.<ref>{{cite web |title=Obama Administration Unveils "Big Data" Initiative:Announces $200 Million In New R&D Investments|publisher=The White House |url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_press_release_final_2.pdf}}</ref>\n\nThe initiative included a National Science Foundation "Expeditions in Computing" grant of $10 million over 5 years to the AMPLab<ref>{{cite web|url=http://amplab.cs.berkeley.edu |title=AMPLab at the University of California, Berkeley |publisher=Amplab.cs.berkeley.edu |accessdate=2013-03-05}}</ref> at the University of California, Berkeley.<ref>{{cite web |title=NSF Leads Federal Efforts in Big Data|date=29 March 2012|publisher=National Science Foundation (NSF) |url=http://www.nsf.gov/news/news_summ.jsp?cntn_id=123607&org=NSF&from=news}}</ref> The AMPLab also received funds from [[DARPA]], and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion<ref>{{cite conference|url=https://amplab.cs.berkeley.edu/publication/scaling-the-mobile-millennium-system-in-the-cloud-2/|author1=Timothy Hunter|date=October 2011|author2=Teodor Moldovan|author3=Matei Zaharia|author4=Justin Ma|author5=Michael Franklin|author6=Pieter Abbeel|author7=Alexandre Bayen|title=Scaling the Mobile Millennium System in the Cloud}}</ref> to fighting cancer.<ref>{{cite news|title=Computer Scientists May Have What It Takes to Help Cure Cancer|author=David Patterson|publisher=The New York Times|date=5 December 2011|url=http://www.nytimes.com/2011/12/06/science/david-patterson-enlist-computer-scientists-in-cancer-fight.html?_r=0}}</ref>\n\nThe White House Big Data Initiative also included a commitment by the  Department of Energy to provide $25 million in funding over 5 years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,<ref>{{cite web|title=Secretary Chu Announces New Institute to Help Scientists Improve Massive Data Set Research on DOE Supercomputers |publisher="energy.gov" |url=http://energy.gov/articles/secretary-chu-announces-new-institute-help-scientists-improve-massive-data-set-research-doe}}</ref> led by the Energy Department’s [[Lawrence Berkeley National Laboratory]]. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the Department\'s supercomputers.\n\nThe U.S. state of [[Massachusetts]] announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions.<ref>{{cite web |title=Governor Patrick announces new initiative to strengthen Massachusetts\' position as a World leader in Big Data |publisher=Commonwealth of Massachusetts |url=http://www.mass.gov/governor/pressoffice/pressreleases/2012/2012530-governor-announces-big-data-initiative.html}}</ref>  The [[Massachusetts Institute of Technology]] hosts the Intel Science and Technology Center for Big Data in the [[MIT Computer Science and Artificial Intelligence Laboratory]], combining government, corporate, and institutional funding and research efforts.<ref>{{cite web|url=http://bigdata.csail.mit.edu/ |title=Big Data @ CSAIL |publisher=Bigdata.csail.mit.edu |date=22 February 2013 |accessdate=2013-03-05}}</ref>\n\nThe European Commission is funding the 2-year-long Big Data Public Private Forum through their [[Seventh Framework Program]] to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for [[Horizon 2020]], their next [[Framework Programmes for Research and Technological Development|framework program]].<ref>{{cite web|url=http://cordis.europa.eu/search/index.cfm?fuseaction=proj.document&PJ_RCN=13267529 |title=Big Data Public Private Forum |publisher=Cordis.europa.eu |date=1 September 2012 |accessdate=2013-03-05}}</ref>\n\nThe British government announced in March 2014 the founding of the [[Alan Turing Institute]], named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyse large data sets.<ref>{{cite news|url=http://www.bbc.co.uk/news/technology-26651179|title=Alan Turing Institute to be set up to research big data|publisher=[[BBC News]]|accessdate=2014-03-19|date=19 March 2014}}</ref>\n\nAt the [[University of Waterloo Stratford Campus]] Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.<ref>{{cite web|url=http://www.betakit.com/event/inspiration-day-at-university-of-waterloo-stratford-campus/|title=Inspiration day at University of Waterloo, Stratford Campus |publisher=betakit.com/|accessdate=2014-02-28}}</ref>\n\nTo make manufacturing more competitive in the United States (and globe), there is a need to integrate more American ingenuity and innovation into manufacturing ; Therefore, National Science Foundation has granted the Industry University cooperative research center for Intelligent Maintenance Systems (IMS) at [[university of Cincinnati]] to focus on developing advanced predictive tools and techniques to be applicable in a big data environment.<ref>{{cite journal|last=Lee|first=Jay|author2=Lapira, Edzel |author3=Bagheri, Behrad |author4= Kao, Hung-An |title=Recent Advances and Trends in Predictive Manufacturing Systems in Big Data Environment|journal=Manufacturing Letters|year=2013|volume=1|issue=1|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114|DOI=10.1016/j.mfglet.2013.09.005 |pages=38–41}}</ref> In May 2013, IMS Center held an industry advisory board meeting focusing on big data where presenters from various industrial companies discussed their concerns, issues and future goals in big data environment.\n\nComputational social sciences&nbsp;– Anyone can use Application Programming Interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences.<ref name=pigdata>{{cite journal|last=Reips|first=Ulf-Dietrich|author2=Matzat, Uwe |title=Mining "Big Data" using Big Data Services |journal=International Journal of Internet Science|year=2014|volume=1|issue=1|pages=1–8 | url=http://www.ijis.net/ijis9_1/ijis9_1_editorial_pre.html}}</ref> Often these APIs are provided for free.<ref name="pigdata" /> [[Tobias Preis]] \'\'et al.\'\' used [[Google Trends]] data to demonstrate that Internet users from countries with a higher per capita gross domestic product (GDP) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviour and real-world economic indicators.<ref>{{cite journal |first1=Tobias |last1=Preis |first2=Helen Susannah |last2=Moat, |first3=H. Eugene |last3=Stanley |first4=Steven R. |last4=Bishop |title=Quantifying the Advantage of Looking Forward |journal=Scientific Reports |volume= 2 |page=350 |year=2012 |doi=10.1038/srep00350 |pmid=22482034 |pmc=3320057}}</ref><ref>{{cite web | url=http://www.newscientist.com/article/dn21678-online-searches-for-future-linked-to-economic-success.html | title=Online searches for future linked to economic success |first=Paul |last=Marks |work=New Scientist | date=5 April 2012 | accessdate=9 April 2012}}</ref><ref>{{cite web | url=http://arstechnica.com/gadgets/news/2012/04/google-trends-reveals-clues-about-the-mentality-of-richer-nations.ars | title=Google Trends reveals clues about the mentality of richer nations |first=Casey |last=Johnston |work=Ars Technica | date=6 April 2012 | accessdate=9 April 2012}}</ref> The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (\'2011\') to the volume of searches for the previous year (\'2009\'), which they call the \'[[future orientation index]]\'.<ref>{{cite web | url = http://www.tobiaspreis.de/bigdata/future_orientation_index.pdf | title = Supplementary Information: The Future Orientation Index is available for download | author = Tobias Preis | date = 24 May 2012 | accessdate = 2012-05-24}}</ref> They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP. The results hint that there may potentially be a relationship between the economic success of a country and the information-seeking behavior of its citizens captured in big data.\n\n[[Tobias Preis]] and his colleagues [[Helen Susannah Moat]] and [[H. Eugene Stanley]] introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends.<ref>{{cite web | url=http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=[[Philip Ball]] | work=Nature | date=26 April 2013 | accessdate=9 August 2013}}</ref> Their analysis of [[Google]] search volume for 98 terms of varying financial relevance, published in \'\'[[Scientific Reports]]\'\',<ref>{{cite journal | author=Tobias Preis, Helen Susannah Moat and H. Eugene Stanley | title=Quantifying Trading Behavior in Financial Markets Using Google Trends | journal=[[Scientific Reports]] | volume= 3 | pages=1684 | year=2013 | doi=10.1038/srep01684 | pmid=23619126 | pmc=3635219}}</ref> suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.<ref>{{cite news | url=http://bits.blogs.nytimes.com/2013/04/26/google-search-terms-can-predict-stock-market-study-finds/ | title= Google Search Terms Can Predict Stock Market, Study Finds | author=Nick Bilton | work=[[New York Times]] | date=26 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite news | url=http://business.time.com/2013/04/26/trouble-with-your-investment-portfolio-google-it/ | title=Trouble With Your Investment Portfolio? Google It! | author=Christopher Matthews | work=[[TIME Magazine]] | date=26 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite web | url= http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=Philip Ball |work=[[Nature (journal)|Nature]] | date=26 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite web | url=http://www.businessweek.com/articles/2013-04-25/big-data-researchers-turn-to-google-to-beat-the-markets | title=\'Big Data\' Researchers Turn to Google to Beat the Markets | author=Bernhard Warner | work=[[Bloomberg Businessweek]] | date=25 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite news | url=http://www.independent.co.uk/news/business/comment/hamish-mcrae/hamish-mcrae-need-a-valuable-handle-on-investor-sentiment-google-it-8590991.html | title=Hamish McRae: Need a valuable handle on investor sentiment? Google it | author=Hamish McRae | work=[[The Independent]] | date=28 April 2013 | accessdate=9 August 2013 | location=London}}</ref><ref>{{cite web | url=http://www.ft.com/intl/cms/s/0/e5d959b8-acf2-11e2-b27f-00144feabdc0.html | title= Google search proves to be new word in stock market prediction | author=Richard Waters | work=[[Financial Times]] | date=25 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite news | url=http://www.forbes.com/sites/davidleinweber/2013/04/26/big-data-gets-bigger-now-google-trends-can-predict-the-market/ | title=Big Data Gets Bigger: Now Google Trends Can Predict The Market | author=David Leinweber | work=[[Forbes]] | date=26 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite news | url=http://www.bbc.co.uk/news/science-environment-22293693 | title=Google searches predict market moves | author=Jason Palmer | work=[[BBC]] | date=25 April 2013 | accessdate=9 August 2013}}</ref>\n\nBig data sets come with algorithmic challenges that previously did not exist. Hence, there is a need to fundamentally change the processing ways.<ref>E. Sejdić, "Adapt current tools for use with big data," \'\'Nature,\'\' vol. vol. 507, no. 7492, pp. 306, Mar. 2014.</ref>\n\nThe Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data.<ref>\nStanford.\n[http://web.stanford.edu/group/mmds/ "MMDS. Workshop on Algorithms for Modern Massive Data Sets"].\n</ref>\n\n=== Sampling big data ===\nAn important research question that can be asked about big data sets is whether you need to look at the full data to draw certain conclusions about the properties of the data or is a sample good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But [[Sampling (statistics)]] enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. For example, there are about 600 million tweets produced every day. Is it necessary to look at all of them to determine the topics that are discussed during the day? Is it necessary to look at all the tweets to determine the sentiment on each of the topics? In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage and controller data are available at short time intervals. To predict down-time it may not be necessary to look at all the data but a sample may be sufficient.  Big Data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data.  With large sets of data points, marketers are able to create and utilize more customized segments of consumers for more strategic targeting.\n\nThere has been some work done in Sampling algorithms for big data. A theoretical formulation for sampling Twitter data has been developed.<ref>{{cite conference |author1=Deepan Palguna |author2=Vikas Joshi |author3=Venkatesan Chakaravarthy |author4=Ravi Kothari |author5=L. V. Subramaniam |last-author-amp=yes | title=Analysis of Sampling Algorithms for Twitter | journal=[[International Joint Conference on Artificial Intelligence]] | year=2015 }}</ref>\n\n== Critique ==\nCritiques of the big data paradigm come in two flavors, those that question the implications of the approach itself, and those that question the way it is currently done.<ref>{{cite journal | doi = 10.1002/joe.21642 | title = Big Data and Business Intelligence: Debunking the Myths | journal = Global Business and Organizational Excellence| volume = 35 | issue = 1 | pages = 23–34 | year = 2015 | last1 = Kimble | first1 = C. | last2 = Milolidakis | first2 = G. }}</ref> One approach to this criticism is the field of [[Critical data studies]].\n\n=== Critiques of the big data paradigm ===\n"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data".<ref name="Editorial" /> In their critique, Snijders, Matzat, and [[Ulf-Dietrich Reips|Reips]] point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at [[Chris Anderson (writer)|Chris Anderson]]\'s assertion that big data will spell the end of theory:<ref>{{cite web|url=http://www.wired.com/science/discoveries/magazine/16-07/pb_theory|title=The End of Theory: The Data Deluge Makes the Scientific Method Obsolete|author=Chris Anderson|date=23 June 2008|work=WIRED}}</ref> focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts.<ref>{{cite news |author=Graham M. |title=Big data and the end of theory? |newspaper=The Guardian |url=https://www.theguardian.com/news/datablog/2012/mar/09/big-data-theory |location=London |date=9 March 2012}}</ref> Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analysed, must be complemented by "big judgment," according to an article in the Harvard Business Review.<ref>{{cite web|title=Good Data Won\'t Guarantee Good Decisions. Harvard Business Review|url=http://hbr.org/2012/04/good-data-wont-guarantee-good-decisions/ar/1|work=Shah, Shvetank; Horne, Andrew; Capellá, Jaime;|publisher=HBR.org|accessdate=8 September 2012}}</ref>\n\nMuch in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably "informed by the world as it was in the past, or, at best, as it currently is".<ref name="HilbertBigData2013">{{cite web|url=http://papers.ssrn.com/abstract=2205145|title=Big Data for Development: From Information- to Knowledge Societies|publisher=}}</ref>  Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past.<ref name="HilbertTEDx">[https://www.youtube.com/watch?v=UXef6yfJZAI Big Data requires Big Visions for Big Change.], Hilbert, M. (2014). London: TEDxUCL, x=independently organized TED talks</ref> If the systems dynamics of the future change (if it is not a [[stationary process]]), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory.<ref name="HilbertTEDx"/>  As a response to this critique it has been suggested to combine big data approaches with computer simulations, such as [[agent-based model]]s<ref name="HilbertBigData2013" /> and [[Complex Systems]]. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms.<ref>{{cite web|url=http://www.theatlantic.com/magazine/archive/2002/04/seeing-around-corners/302471/|title=Seeing Around Corners|author=Jonathan Rauch|date=1 April 2002|work=The Atlantic}}</ref><ref>Epstein, J. M., & Axtell, R. L. (1996). Growing Artificial Societies: Social Science from the Bottom Up. A Bradford Book.</ref> In addition, use of multivariate methods that probe for the latent structure of the data, such as [[factor analysis]] and [[cluster analysis]], have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.\n\nIn health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.<ref>[http://www.bigdataparis.com/documents/Pierre-Delort-INSERM.pdf#page=5 Delort P., Big data in Biosciences, Big Data Paris, 2012]</ref>\nA new postulate is accepted now in biosciences: the information provided by the data in huge volumes ([[omics]]) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation.<ref>{{cite web|url=http://www.cs.cmu.edu/~durand/03-711/2011/Literature/Next-Gen-Genomics-NRG-2010.pdf|title=Next-generation genomics: an integrative approach|date=July 2010|publisher=nature|accessdate=18 October 2016}}</ref><ref>{{cite web|url=https://www.researchgate.net/publication/283298499_BIG_DATA_IN_BIOSCIENCES|title=BIG DATA IN BIOSCIENCES|date=October 2015|publisher=ResearchGate|accessdate=18 October 2016}}</ref> In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor.<ref>{{cite web|url=https://next.ft.com/content/21a6e7d8-b479-11e3-a09a-00144feabdc0|title=Big data: are we making a big mistake?|date=28 March 2014|publisher=Financial Times|accessdate=20 October 2016}}</ref> The search logic is reversed and the limits of induction ("Glory of Science and Philosophy scandal", [[C. D. Broad]], 1926) are to be considered.{{Citation needed|date=April 2015}}\n\n[[Consumer privacy|Privacy]] advocates are concerned about the threat to privacy represented by increasing storage and integration of [[personally identifiable information]]; expert panels have released various policy recommendations to conform practice to expectations of privacy.<ref>{{cite web |first=Paul |last=Ohm |title=Don\'t Build a Database of Ruin |publisher=Harvard Business Review |url=http://blogs.hbr.org/cs/2012/08/dont_build_a_database_of_ruin.html}}</ref><ref>Darwin Bond-Graham, \'\'[http://www.counterpunch.org/2013/12/03/iron-cagebook/ Iron Cagebook – The Logical End of Facebook\'s Patents],\'\' [[Counterpunch.org]], 2013.12.03</ref><ref>Darwin Bond-Graham, \'\'[http://www.counterpunch.org/2013/09/11/inside-the-tech-industrys-startup-conference/  Inside the Tech industry’s Startup Conference],\'\' [[Counterpunch.org]], 2013.09.11</ref>\n\n=== Critiques of big data execution ===\nBig data has been called a "fad" in scientific research and its use was even made fun of as an absurd practice in a satirical example on "pig data".<ref name="pigdata" /> Researcher [[Danah Boyd]] has raised concerns about the use of big data in science neglecting principles such as choosing a [[Sampling (statistics)|representative sample]] by being too concerned about actually handling the huge amounts of data.<ref name="danah">{{cite web | url=http://www.danah.org/papers/talks/2010/WWW2010.html | title=Privacy and Publicity in the Context of Big Data | author=[[danah boyd]] | work=[[World Wide Web Conference|WWW 2010 conference]] | date=29 April 2010 | accessdate = 2011-04-18}}</ref> This approach may lead to results [[Bias (statistics)|bias]] in one way or another. Integration across heterogeneous data resources—some that might be considered big data and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.<ref>{{cite journal |last1=Jones |first1=MB |last2=Schildhauer |first2=MP |last3=Reichman |first3=OJ |last4=Bowers |first4=S |title=The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere |journal=Annual Review of Ecology, Evolution, and Systematics |volume=37 |issue=1 |pages=519–544 |year=2006 |doi=10.1146/annurev.ecolsys.37.091305.110031 |url=http://www.pnamp.org/sites/default/files/Jones2006_AREES.pdf |format=PDF}}</ref>\nIn the provocative article "Critical Questions for Big Data",<ref name="danah2">{{cite journal | doi = 10.1080/1369118X.2012.678878| title = Critical Questions for Big Data| journal = Information, Communication & Society| volume = 15| issue = 5| pages = 662–679| year = 2012| last1 = Boyd | first1 = D. | last2 = Crawford | first2 = K. }}</ref> the authors title big data a part of [[mythology]]: "large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy". Users of big data are often "lost in the sheer volume of numbers", and "working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth".<ref name="danah2" /> Recent developments in BI domain, such as pro-active reporting especially target improvements in usability of big data, through automated [[Filter (software)|filtering]] of non-useful data and correlations.<ref name="Big Decisions White Paper">[http://www.fortewares.com/Administrator/userfiles/Banner/forte-wares--pro-active-reporting_EN.pdf Failure to Launch: From Big Data to Big Decisions], Forte Wares.</ref>\n\nBig data analysis is often shallow compared to analysis of smaller data sets.<ref name="kdnuggets-berchthold">{{cite web|url=http://www.kdnuggets.com/2014/08/interview-michael-berthold-knime-research-big-data-privacy-part2.html|title=Interview: Michael Berthold, KNIME Founder, on Research, Creativity, Big Data, and Privacy, Part 2|date=12 August 2014|author=Gregory Piatetsky|authorlink=Gregory I. Piatetsky-Shapiro|publisher=KDnuggets|accessdate=2014-08-13}}</ref> In many big data projects, there is no large data analysis happening, but the challenge is the [[extract, transform, load]] part of data preprocessing.<ref name="kdnuggets-berchthold" />\n\nBig data is a [[buzzword]] and a "vague term",<ref>{{cite web|last1=Pelt|first1=Mason|title="Big Data" is an over used buzzword and this Twitter bot proves it|url=http://siliconangle.com/blog/2015/10/26/big-data-is-an-over-used-buzzword-and-this-twitter-bot-proves-it/|website=siliconangle.com|publisher=SiliconANGLE|accessdate=4 November 2015}}</ref><ref name="ft-harford">{{cite web |url=http://www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0.html |title=Big data: are we making a big mistake? |last1=Harford |first1=Tim |date=28 March 2014 |website=[[Financial Times]] |publisher=[[Financial Times]] |accessdate=2014-04-07}}</ref> but at the same time an "obsession"<ref name="ft-harford" /> with entrepreneurs, consultants, scientists and the media. Big data showcases such as [[Google Flu Trends]] failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, [[Academy awards]] and election predictions solely based on Twitter were more often off than on target.\nBig data often poses the same challenges as small data; and adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. [[Google Translate]]—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.\nOn the other hand, big data may also introduce new problems, such as the [[multiple comparisons problem]]: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.\nIoannidis argued that "most published research findings are false"<ref name="Ioannidis">{{cite journal | last1 = Ioannidis | first1 = J. P. A. | authorlink1 = John P. A. Ioannidis| title = Why Most Published Research Findings Are False | journal = PLoS Medicine | volume = 2 | issue = 8 | pages = e124 | year = 2005 | pmid = 16060722 | pmc = 1182327 | doi = 10.1371/journal.pmed.0020124}}</ref> due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a "significant" result being actually false grows fast – even more so, when only positive results are published.\n<!-- sorry, this started overlapping with above section more and more... merging is welcome; I already dropped the intended subheadline "Hype cycle and inflated expectations". -->\nFurthermore, big data analytics results are only as good as the model on which they are predicated.  In an example, big data took part in attempting to predict the results of the 2016 U.S. Presidential Election<ref>{{Cite news|url=http://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html|title=How Data Failed Us in Calling an Election|last=Lohr|first=Steve|date=2016-11-10|last2=Singer|first2=Natasha|newspaper=The New York Times|issn=0362-4331|access-date=2016-11-27}}</ref> with varying degrees of success.  Forbes predicted "If you believe in \'\'Big Data\'\' analytics, it’s time to begin planning for a Hillary Clinton presidency and all that entails.".<ref>{{Cite news|url=http://www.forbes.com/sites/jonmarkman/2016/08/08/big-data-and-the-2016-election/#4802f20846d7|title=Big Data And The 2016 Election|last=Markman|first=Jon|newspaper=Forbes|access-date=2016-11-27}}</ref>\n\n== See also ==\n{{portal|Information technology}}\n{{Category see also|LABEL=For a list of companies, and tools, see also|Big data}}\n<!-- NO COMPANIES OR TOOL SPAM HERE. That would be an endless list! "See also" concepts, not linked above. -->\n* [[Big memory]]\n* [[Datafication]]\n* [[Data defined storage]]\n* [[Data journalism]]\n* [[Data lineage]]\n* [[Data philanthropy]]\n* [[Data science]]\n* [[Machine learning]]\n* [[Statistics]]\n* [[Small data]]\n* [[Urban informatics]]\n* [[List of buzzwords]]\n\n== References ==\n{{Reflist|30em}}\n\n==Further reading==\n*{{cite magazine|editors=Peter Kinnaird, Inbal Talgam-Cohen|series=[[XRDS (magazine)|XRDS: Crossroads, The ACM Magazine for Students]]|title=Big Data|issue=19 (1)|date=2012|publisher=[[Association for Computing Machinery]]|issn=1528-4980 |oclc=779657714 |url=http://dl.acm.org/citation.cfm?id=2331042}}\n*{{cite book|title=Mining of massive datasets|author1=[[Jure Leskovec]]|author2=[[Anand Rajaraman]]|author3=[[Jeffrey D. Ullman]]|year=2014|publisher=Cambridge University Press|url=http://mmds.org/|isbn=9781107077232 |oclc=888463433}}\n*{{cite book|author1=[[Viktor Mayer-Schönberger]]|author2=[[Kenneth Cukier]]|title=Big Data: A Revolution that Will Transform how We Live, Work, and Think|date=2013|publisher=Houghton Mifflin Harcourt|isbn=9781299903029 |oclc=828620988}}\n*{{cite web |url=http://www.forbes.com/sites/gilpress/2013/05/09/a-very-short-history-of-big-data |title=A Very Short History Of Big Data |first=Gil |last=Press |work=forbes.com |date=2013-05-09 |accessdate=2016-09-17 |publisher=[[Forbes Magazine]] |location=Jersey City, NJ}}\n\n== External links ==\n*{{Commonsinline}}\n* {{Wiktionary-inline|big data}}\n\n{{Use dmy dates|date=December 2015}}\n{{Authority control}}\n\n[[Category:Big data| ]]\n[[Category:Data management]]\n[[Category:Distributed computing problems]]\n[[Category:Technology forecasting]]\n[[Category:Transaction processing]]']
['Data warehouse automation', '48752218', "'''Data warehouse automation''' or DWA refers to the process of accelerating and automating the [[data warehouse]] development cycles, while assuring quality and consistency. DWA is believed to provide automation of the entire lifecycle of a data warehouse, from source [[Systems analysis|system analysis]] to [[Software testing|testing]] to  [[documentation]]. It helps improve productivity, reduce cost, and improve overall quality.<ref>{{cite web|title=Automate and accelerate your data transformations|url=http://www.attunity.com/products/prepare-data-compose|website=www.attunity.com|publisher=Attunity|accessdate=7 December 2015}}</ref>\n\n==General==\nData warehouse automation primarily focuses on automation of each and every step involved in the lifecycle of a data warehouse, thus reducing the efforts required in managing it.<ref>{{cite web|title=New Buzzword! Data Warehouse Automation|url=http://blogs.jetreports.com/2015/03/05/new-buzzword-data-warehouse-automation/|website=blogs.jetreports.com|publisher=jetreports|accessdate=7 December 2015}}</ref>\nData warehouse automation works on the principles of design patterns. It comprises a central repository of design patterns, which encapsulate architectural standards as well as best practices for data design, data management, data integration, and data usage.<ref>{{cite web|url=https://www.wherescape.com/media/1988/data-warehouse-automation-decision-guide.pdf|title=Data Warehouse Automation - A Decision Guide|website=www.wherescape.com|publisher=David L. Wells, Infocentric LLC|accessdate=7 December 2015}}</ref>\nIn November 2015, an analyst firm has published a guide ''Which Data Warehouse Automation Tool is Right for You?'' covering four of the leading products in the DWA space.<ref>{{cite web|title=Which Data Warehouse Automation Tool is Right for You?|url=http://eckerson.com/register?content=which-data-warehouse-automation-tool-is-right-for-you|website=eckerson.com|publisher= Wayne Eckerson|accessdate=9 December 2015}}</ref> In November 2015, an international software and technology services company engaged in developing ‘agile tools’ for the data integration industry, was named by CIO Review as one of the 20 most promising productivity tools solution providers 2015 <ref>{{cite web|title=CIO Magazine Award - 20 Most promising productivity tools|url=http://analytixds.com/latest_news/analytix-data-services-wins-cio-reviews-2015/|website=www.analtyixds.com|publisher=AnalytiX DS|accessdate=25 November 2016}}</ref>\n\n==Benefits==\nData warehouse automation can provide advantages like source data exploration, warehouse data models, ETL generation, test automation, metadata management, managed deployment, scheduling, change impact analysis and easier maintenance and modification of the data warehouse.<ref>{{cite web|title=Data Warehouse Automation (DWA)?|url=http://www.timextender.com/software/data-warehouse-automation/business-value/|website=timextender.com|publisher=TimeXtender Software 2015|accessdate=7 December 2015}}</ref>\nMore important than the technical features of DWA tools, however, is the ability to deliver projects faster and with less resources.<ref>{{cite web|title=Deliver Faster|url=http://kalido.com/products/kalido-information-engine/deliver-faster/|website=kalido.com|publisher=Magnitude Software|accessdate=9 December 2015}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* {{Dmoz|Computers/Software/Databases/Data_Warehousing/Data_Warehouse_Automation|Data Warehouse Automation}}\n* [http://analytixds.com/wp-content/uploads/2016/07/analytix-data-services-top-20-cio-review.pdf CIO Magazine Award - 20 Most promising productivity tools 'CIO Review', November 10, 2015]\n\n==See also==\n*[[Data warehouse]]\n*[[Data mart]]\n*[[Data warehouse appliance]]\n*[[Data integration]]\n\n{{Data_warehouse}}\n\n[[Category:Data management]]\n[[Category:Data warehousing]]"]
['Category:Data management software', '32164666', '[[Software]], typically proprietary products or open-source projects, with a primary purpose of [[data management]].  [[:Category:Database management systems by license|Database management system software]] could be considered a related category, though those will typically exist for the purpose of managing a [[database]] in a particular structure (i.e. relational, object-oriented). \n[[Category:Data management|Software]]\n[[Category:Application software]]']
['GB & Smith', '48206970', '{{ad|date=August 2016}}\n{{Orphan|date=April 2016}}\n{{Use dmy dates|date=September 2016}}\n{{Infobox company\n| name = GB & Smith\n| logo = GB & Smith.png\n| caption =\n| type = Private\n| traded_as =\n| successor =\n| foundation = 2007\n| founder =  Sebastien Goiffon and Alexandre Biegala\n| defunct =\n| location_city =\n| location_country =\n| location =\n| locations =\n| area_served =\n| key_people =\n| industry = Software\n| products =\n| services =\n| revenue =\n| operating_income =\n| net_income =\n| owner =\n| num_employees = 70\n| parent =\n| divisions =\n| subsid =\n| homepage = {{url|gbandsmith.com}}\n| footnotes =\n| intl =\n}}\n\n\'\'\'GB & Smith\'\'\' is an independent [[software]] editor which provides layer independent matrix-based console allowing instant visual review on any supported [[computing platform]].\n\n== History ==\n\nGB & Smith was founded in 2007 by Sebastien Goiffon and Alexandre Biegala.<ref>{{cite web|title=Price Entrepreneur of the Year 2014: Winners North region|url=http://business.lesechos.fr/entrepreneurs/success-stories/prix-de-l-entrepreneur-de-l-annee-2014-les-laureats-region-nord-103724.php|publisher=[[Les Échos (newspaper)|Les Échos]]|accessdate=20 March 2016}}</ref><ref>{{cite web|title=S\'implanter à Londres ? Les clés du succès selon GB & Smith|url=http://www.lesechos.fr/02/04/2015/lesechos.fr/0204265266931_s-implanter-a-londres---les-cles-du-succes-selon-gb---smith.htm|publisher=[[Les Échos (newspaper)|Les Échos]]|accessdate=19 March 2016}}</ref>\n\nAs of 2016 company is becoming agnostic gradually offering it is security administration solutions for [[Microsoft]], [[Oracle Corporation|Oracle]], [[IBM]], [[Tableau Software|Tableau]].<ref name=les>{{cite web|title=GB & Smith secures corporate data|url=http://www.lesechos.fr/26/12/2013/LesEchos/21592-075-ECH_gb---smith-securise-les-donnees-d-entreprise.htm|publisher=[[Les Échos (newspaper)|Les Échos]]|accessdate=20 March 2016}}</ref>\n\nAlexandre Biegala and Sebastien Goiffon invented a method around [[identity access management]] (IAM) to easily review and administer security rights of various applications and over multiple technologies through a single User interface.<ref name=les /><ref>{{cite web|title=GB & Smith Lille : + 761% de croissance en cinq ans!|url=http://www.lavoixdunord.fr/economie/gb-smith-lille-761-de-croissance-en-cinq-ans-ia0b0n1806557|publisher=[[La Voix du Nord (daily)|La Voix du Nord]]|accessdate=20 March 2016}}</ref>\n\nGbandSmith was granted a patent for a solution on security administration of rights and has become the Security Administration company and known to be a pioneer in Administration Intelligence and real Self-service security administration.<ref>{{cite web|title=US Patent Issued to GB & Smith on Feb. 10 for "Matrix Security Management System for Managing User Accounts and Security Settings"|url=https://www.highbeam.com/doc/1P3-3585409721.html|website=Highbeam.com|accessdate=19 March 2016}}</ref>\n\n[[BusinessObjects]] co-founder Denis Payre joined GB & Smith on 1 April 2016. In 1996, Denis Payre and his partner, Bernard Liautaud were ranked by \'\'Business Week\'\' among the "Best Entrepreneurs", alongside [[Steve Jobs]] and [[Steven Spielberg]].<ref>{{cite web|title=GB&SMITH Announces Denis Payre, Co-Founder of Business Objects, to Join its Board of Directors|url=http://www.bizjournals.com/prnewswire/press_releases/2016/04/01/NE60812|website=[[The Business Journals]]|accessdate=1 April 2016}}</ref>\n\n==Solutions==\n\n=== 365Suite Agnostic Self-Service Security Administration ===\n365Suite is a set of agnostic tools solutions focused on Security administrations such as access rights management, security policy audits and related metadata. 365Suite enables centralizing security administration into a single console managing multiple applications. 365 runs on top of solutions such as Microsoft [[SharePoint]], [[Microsoft Active Directory]], SAP [[SAP BusinessObjects|BO]], [[SAP HANA|Hana]], Oracle [[Oracle Business Intelligence Suite Enterprise Edition|OBIEE,]] [[Oracle Database|ODB]], [[Tableau Software|Tableau]], etc.<ref>{{cite web|title=Le Comparateur assurance remporte le premier prix du Fast50 avec une croissance de + 1 562% en 4 ans|url=http://www.lavoixdunord.fr/economie/le-comparateur-assurance-remporte-le-premier-prix-du-fast50-ia0b0n3165825|publisher=[[La Voix du Nord (daily)|La Voix du Nord]]|accessdate=20 March 2016}}</ref>\n\n365 solutions consists in two solutions:\n* 365View: Single security administration console to operate multiple IT solutions simultaneously (sharepoint, Oarcle BI).\n* 365Eyes: Centralized Metadata repository focused on security administration with ability to operate, monitor, restore and compare metadata from multiple IT solutions.\n\n=== 360Suite ===\n\n360Suite consists in a suite of eight solutions focused around [[SAP SE|SAP]] BusinessObjects:  \n* 360Plus: Backup, incremental backup,Promotion, including ability to restore deleted files.<ref>{{cite web|title=Le Français GB & Smith invente le concept prometteur d\'administration intelligence|url=http://www.channelnews.fr/le-francais-gb-a-smith-invente-le-concept-prometteur-dadministration-intelligence-21842|publisher=ChannelNews|accessdate=19 March 2016}}</ref>\n* 360View: Security administration, via a security matrix crossing Ressources and Users, Bulk updates (UNV to UNX, unbounded documents)<ref>{{cite web|title=GB & Smith, un esprit de conquête et un esprit libre|url=http://www.lopinion.fr/2-decembre-2014/gb-smith-esprit-conquete-esprit-libre-18979|publisher=[[L\'Opinion (newspaper)|L\'Opinion]]|accessdate=19 March 2016}}</ref>\n* 360Cast: Schedule and burst dynamically reports.<ref>{{cite web|title=La Société Ugitech Choisit Les Solutions 360suite De Gb & Smith Pour Administrer Sa Plateforme Sap Businessobjects Bi 4.0|url=http://www.decideo.fr/La-Societe-UGITECH-choisit-les-solutions-360suite-de-GB-SMITH-pour-administrer-sa-plateforme-SAP-BusinessObjects-BI-4-0_a6323.html|publisher=Decideo|accessdate=19 March 2016}}</ref>\n* 360Eyes: Explore and analyze BO [[metadata]] and perform impact analysis.\n* 360Eyes compliance: Compliance to ensure BO compliance.\n* 360Vers: Facilitate and monitor BO versioning.\n* 360Bind: Automate BO Non regression tests. With ability to compare results and pixels from Webi, Deski and Crystal reports.\n* 360Init: Initialize and import your BO security.\n\n== Recognition ==\n\n* 2013-15 [[Deloitte Fast 500#Fast 500 EMEA|Deloitte EMEA technology Fast 500]].<ref>{{cite web|title=Technology Fast 500 EMEA 2013 Ranking|url=http://www2.deloitte.com/content/dam/Deloitte/global/Documents/Technology-Media-Telecommunications/dttl_TMT-Event-Fast-500-2013-winners-ranking.pdf|publisher=[[Deloitte]].com|accessdate=19 March 2016}}</ref><ref>{{cite web|title=2015 Technology Fast 500TM Europe, Middle East & Africa (EMEA) Ranking|url=https://www2.deloitte.com/content/dam/Deloitte/global/Documents/Technology-Media-Telecommunications/gx-deloitte-tmt-emea-fast500-2015-rankings.pdf|publisher=[[Deloitte Fast 500]]|accessdate=19 March 2016}}</ref><ref>{{cite web|title=Technology Fast 50|url=http://www2.deloitte.com/content/dam/Deloitte/fr/Documents/technology%20fast%2050/Deloitte_Palmar%C3%A8s-Fast50_2014.pdf|publisher=[[Deloitte]].com|accessdate=19 March 2016}}</ref>\n* 2014 [[Ernst & Young]] emerging company.<ref>{{cite web|title=Sébastien GOIFFON et Alexandre BIEGALA reçoivent le Prix de l\'Entreprise d\'Avenir de l\'Année 2014 pour le Nord de France|url=http://blog.gbandsmith.com/wp-content/uploads/2014/10/cp_resultats_ceremonie_nord_de_france_2014_gbs2_0.pdf|publisher=[[Ey.com]]|accessdate=19 March 2016}}</ref>\n\n==References==\n{{reflist}}\n\n== External links ==\n*{{Official website|http://www.gbandsmith.com}}\n\n{{DEFAULTSORT:GB and Smith}}\n[[Category:Computer access control]]\n[[Category:Business intelligence companies]]\n[[Category:Identity management]]\n[[Category:Data analysis software]]\n[[Category:Data management]]']
['Category:Clinical data management', '34875542', '[[Category:Clinical research]]\n[[Category:Pharmaceutical industry]]\n[[Category:Data management]]']
['Database normalization', '8640', '\'\'\'Database normalization\'\'\', or simply \'\'\'normalization\'\'\', is the process of organizing the [[column (database)|columns]] (attributes) and [[table (database)|tables]] (relations) of a [[relational database]] to reduce [[data redundancy]] and improve data integrity.\n\nNormalization involves arranging attributes in tables based on [[Dependency theory (database theory)|dependencies]] between attributes, ensuring that the dependencies are properly enforced by database integrity constraints. Normalization is accomplished through applying some formal rules either by a process of synthesis or decomposition. Synthesis creates a normalized database design based on a known set of dependencies. Decomposition takes an existing (insufficiently normalized) database design and improves it based on the known set of dependencies.\n\n[[Edgar F. Codd]], the inventor of the [[relational model]] (RM), introduced the concept of normalization and what we now know as the [[First normal form]] (1NF) in 1970.<ref name="Codd1970">{{cite journal|first=E. F.|last=Codd|authorlink=E.F. Codd|title=A Relational Model of Data for Large Shared Data Banks|journal=[[Communications of the ACM]]|volume=13|issue=6|date=June 1970|pages=377–387|url=http://www.acm.org/classics/nov95/toc.html | doi = 10.1145/362384.362685}}</ref> Codd went on to define the [[Second normal form]] (2NF) and [[Third normal form]] (3NF) in 1971,<ref name="Codd, E.F 1971">Codd, E.F. "Further Normalization of the Data Base Relational Model". (Presented at Courant Computer Science Symposia Series 6, "Data Base Systems", New York City, May 24–25, 1971.) IBM Research Report RJ909 (August 31, 1971). Republished in Randall J. Rustin (ed.), \'\'Data Base Systems: Courant Computer Science Symposia Series 6\'\'. Prentice-Hall, 1972.</ref> and Codd and [[Raymond F. Boyce]] defined the Boyce-Codd Normal Form ([[Boyce–Codd normal form|BCNF]]) in 1974.<ref name="CoddBCNF">Codd, E. F. "Recent Investigations into Relational Data Base Systems". IBM Research Report RJ1385 (April 23, 1974). Republished in \'\'Proc. 1974 Congress\'\' (Stockholm, Sweden, 1974). , N.Y.: North-Holland (1974).</ref> Informally, a relational database table is often described as "normalized" if it meets Third Normal Form.<ref name="DateIntroDBSys">C.J. Date.  \'\'An Introduction to Database Systems\'\'. Addison-Wesley (1999), p. 290</ref>  Most 3NF tables are free of insertion, update, and deletion anomalies.\n\n\n==Objectives==\nA basic objective of the [[first normal form]] defined by Codd in 1970 was to permit data to be queried and manipulated using a "universal data sub-language" grounded in [[first-order logic]].<ref>"The adoption of a relational model of data ... permits the development of a universal data sub-language based on an applied predicate calculus. A first-order predicate calculus suffices if the collection of relations is in first normal form. Such a language would provide a yardstick of linguistic power for all other proposed data languages, and would itself be a strong candidate for embedding (with appropriate syntactic modification) in a variety of host Ianguages (programming, command- or problem-oriented)."  Codd, [http://www.acm.org/classics/nov95/toc.html "A Relational Model of Data for Large Shared Data Banks"], p. 381</ref> ([[SQL]] is an example of such a data sub-language, albeit one that Codd regarded as seriously flawed.)<ref>Codd, E.F.  Chapter 23, "Serious Flaws in SQL", in \'\'The Relational Model for Database Management: Version 2\'\'. Addison-Wesley (1990), pp. 371–389</ref>\n\nThe objectives of normalization beyond 1NF (First Normal Form) were stated as follows by Codd:\n\n{{Quotation|\n# To free the collection of relations from undesirable insertion, update and deletion dependencies;\n# To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus increase the life span of application programs;\n# To make the relational model more informative to users;\n# To make the collection of relations neutral to the query statistics, where these statistics are liable to change as time goes by.\n|E.F. Codd|"Further Normalization of the Data Base Relational Model"<ref>Codd, E.F. "Further Normalization of the Data Base Relational Model", p. 34</ref>}}\n\nThe sections below give details of each of these objectives.\n\n===Free the database of modification anomalies===\n[[File:Update anomaly.svg|280px|thumb|right|An \'\'\'update anomaly\'\'\'. Employee 519 is shown as having different addresses on different records.]]\n[[File:Insertion anomaly.svg|280px|thumb|right|An \'\'\'insertion anomaly\'\'\'. Until the new faculty member, Dr. Newsome, is assigned to teach at least one course, his details cannot be recorded.]]\n[[File:Deletion anomaly.svg|280px|thumb|right|A \'\'\'deletion anomaly\'\'\'. All information about Dr. Giddens is lost if he temporarily ceases to be assigned to any courses.]]\nWhen an attempt is made to modify (update, insert into, or delete from) a table, undesired side-effects may arise in tables that have not been sufficiently normalized. An insufficiently normalized table might have one or more of the following characteristics:\n\n* The same information can be expressed  on multiple rows; therefore updates to the table may result in logical inconsistencies. For example, each record in an "Employees\' Skills" table might contain an Employee ID, Employee Address, and Skill; thus a change of address for a particular employee will potentially need to be applied to multiple records (one for each skill). If the update is not carried through successfully—if, that is, the employee\'s address is updated on some records but not others—then the table is left in an inconsistent state. Specifically, the table provides conflicting answers to the question of what this particular employee\'s address is. This phenomenon is known as an \'\'\'update anomaly\'\'\'.\n* There are circumstances in which certain facts cannot be recorded at all. For example, each record in a "Faculty and Their Courses" table might contain a Faculty ID, Faculty Name, Faculty Hire Date, and Course Code—thus we can record the details of any faculty member who teaches at least one course, but we cannot record the details of a newly hired faculty member who has not yet been assigned to teach any courses except by setting the Course Code to null. This phenomenon is known as an \'\'\'insertion anomaly\'\'\'.\n* Under certain circumstances, deletion of data representing certain facts necessitates deletion of data representing completely different facts. The "Faculty and Their Courses" table described in the previous example suffers from this type of anomaly, for if a faculty member temporarily ceases to be assigned to any courses, we must delete the last of the records on which that faculty member appears, effectively also deleting the faculty member, unless we set the Course Code to null in the record itself.  This phenomenon is known as a \'\'\'deletion anomaly\'\'\'.\n\n===Minimize redesign when extending the database structure===\nWhen a fully normalized database structure is extended to allow it to accommodate new types of data, the pre-existing aspects of the database structure can remain largely or entirely unchanged. As a result, applications interacting with the database are minimally affected.\n\nNormalized tables, and the relationship between one normalized table and another, mirror real-world concepts and their interrelationships.\n\n===Example===\nQuerying and manipulating the data within a data structure that is not normalized, such as the following non-1NF representation of customers, credit card transactions, involves more complexity than is really necessary:\n\n{| class="wikitable"\n! Customer !! Cust. ID !! Transactions\n|-\n| Jones || 1\n|| \n{| class="wikitable"\n! Tr. ID !! Date !! Amount\n|-\n| 12890\n| 14-Oct-2003\n| &minus;87\n|-\n| 12904\n| 15-Oct-2003\n| &minus;50\n|}\n|-\n| Wilkins || 2\n|| \n{| class="wikitable"\n! Tr. ID !! Date !! Amount\n|-\n| 12898\n| 14-Oct-2003\n| &minus;21\n|}\n|-\n| Stevens || 3\n|| \n{| class="wikitable"\n! Tr. ID !! Date !! Amount\n|-\n| 12907\n| 15-Oct-2003\n| &minus;18\n|-\n| 14920\n| 20-Nov-2003\n| &minus;70\n|-\n| 15003\n| 27-Nov-2003\n| &minus;60\n|}\n|}\n<br>\nTo each customer corresponds a \'\'repeating group\'\' of transactions.  The automated evaluation of any query relating to customers\' transactions therefore would broadly involve two stages:\n# Unpacking one or more customers\' groups of transactions allowing the individual transactions in a group to be examined, and\n# Deriving a query result based on the results of the first stage\n\nFor example, in order to find out the monetary sum of all transactions that occurred in October 2003 for all customers, the system would have to know that it must first unpack the \'\'Transactions\'\' group of each customer, then sum the \'\'Amounts\'\' of all transactions thus obtained where the \'\'Date\'\' of the transaction falls in October 2003.\n\nOne of Codd\'s important insights was that this structural complexity could always be removed completely, leading to much greater power and flexibility in the way queries could be formulated (by [[user (computing)|users]] and [[application software|applications]]) and evaluated (by the [[database management system|DBMS]]).  The normalized equivalent of the structure above would look like this:\n\n{| class="wikitable"\n|-\n! Customer !! Cust. ID\n|-\n| Jones || 1\n|-\n| Wilkins || 2\n|-\n| Stevens || 3\n|}\n\n{| class="wikitable"\n|-\n! Cust. ID !! Tr. ID !! Date !! Amount\n|-\n| 1 || 12890 || 14-Oct-2003 || &minus;87\n|-\n| 1 || 12904 || 15-Oct-2003 || &minus;50\n|-\n| 2 || 12898 || 14-Oct-2003 || &minus;21\n|-\n| 3 || 12907 || 15-Oct-2003 || &minus;18\n|-\n| 3 || 14920 || 20-Nov-2003 || &minus;70\n|-\n| 3 || 15003 || 27-Nov-2003 || &minus;60\n|}\n\nIn the modified structure, the keys are {Customer} and {Cust. ID} in the first table, {Cust. ID, Tr ID} in the second table.\n\nNow each row represents an individual credit card transaction, and the DBMS can obtain the answer of interest, simply by finding all rows with a Date falling in October, and summing their Amounts.  The data structure places all of the values on an equal footing, exposing each to the DBMS directly, so each can potentially participate directly in queries; whereas in the previous situation some values were embedded in lower-level structures that had to be handled specially.  Accordingly, the normalized design lends itself to general-purpose query processing, whereas the unnormalized design does not. The normalized version also allows the user to change the customer name in one place and guards against errors that arise if the customer name is misspelled on some records.\n\n==List of Normal Forms==\n* UNF - "[[Denormalization|Unnormalized]] Form"\n* [[First normal form|1NF - First Normal Form]]\n* [[Second normal form|2NF - Second Normal Form]]\n* [[Third normal form|3NF - Third Normal Form]]\n* [[Elementary Key Normal Form|EKNF - Elementary Key Normal Form]]\n* [[Boyce–Codd normal form|BCNF - Boyce–Codd Normal Form]]\n* [[Fourth normal form|4NF - Fourth Normal Form]]\n* [http://researcher.watson.ibm.com/researcher/files/us-fagin/icdt12.pdf ETNF - Essential Tuple Normal Form]\n* [[Fifth normal form|5NF - Fifth Normal Form]]\n* [[Sixth normal form|6NF - Sixth Normal Form]]\n* [[Domain/key normal form|DKNF - Domain/Key Normal Form]]\n\n==See also==\n*[[Refactoring]]\n\n==Notes and references==\n{{reflist|2}}\n{{refbegin}}\n{{refend}}\n\n==Further reading==\n* Date, C. J. (1999), \'\'[http://www.aw-bc.com/catalog/academic/product/0,1144,0321197844,00.html  An Introduction to Database Systems]\'\' (8th ed.). Addison-Wesley Longman. ISBN 0-321-19784-4.\n* Kent, W. (1983) \'\'[http://www.bkent.net/Doc/simple5.htm A Simple Guide to Five Normal Forms in Relational Database Theory]\'\', Communications of the ACM, vol. 26, pp.&nbsp;120–125\n* H.-J. Schek, P. Pistor Data Structures for an Integrated Data Base Management and Information Retrieval System\n\n==External links==\n* [http://databases.about.com/od/specificproducts/a/normalization.htm Database Normalization Basics] by Mike Chapple (About.com)\n* [http://www.databasejournal.com/sqletc/article.php/1428511 Database Normalization Intro], [http://www.databasejournal.com/sqletc/article.php/26861_1474411_1 Part 2]\n* [http://mikehillyer.com/articles/an-introduction-to-database-normalization/ An Introduction to Database Normalization] by Mike Hillyer.\n* [http://phlonx.com/resources/nf3/ A tutorial on the first 3 normal forms] by Fred Coulson\n* [http://www.dbnormalization.com/ DB Normalization Examples]\n* [http://support.microsoft.com/kb/283878 Description of the database normalization basics] by Microsoft\n* [http://www.barrywise.com/2008/01/database-normalization-and-design-techniques/ Database Normalization and Design Techniques] by Barry Wise, recommended reading for the Harvard MIS.\n* [http://www.bkent.net/Doc/simple5.htm A Simple Guide to Five Normal Forms in Relational Database Theory]\n* [http://beginnersbook.com/2015/05/normalization-in-dbms/ Normalization in DBMS by Chaitanya (beginnersbook.com)]\n\n{{Database normalization}}\n{{Database}}\n{{Databases}}\n\n{{DEFAULTSORT:Database Normalization}}\n[[Category:Database normalization| ]]\n[[Category:Database constraints]]\n[[Category:Data management]]\n[[Category:Data modeling]]\n[[Category:Relational algebra]]']
['Imprima iRooms', '51364737', '{{Orphan|date=January 2017}}\n\n{{Infobox company\n| name = Imprima iRooms\n| logo = Imprima-logo-hiRes-300dpi.jpg\n| logo_size = \n| logo_alt = Imprima-logo-hiRes-300dpi.jpg\n| logo_caption = \n| type = [[Private company|Private]]\n| industry = [[Virtual Data Room]], [[Technology]]\n| founded = 1910<br/>2001 <small>(relaunched)</small>\n| founder = \n| hq_location = London, United Kingdom\n| area_served = Worldwide\n| website = {{URL|www.imprima.com}}\n}}\n\'\'\'Imprima iRooms\'\'\' is a [[private company]] headquartered in London. It provides [[Virtual Data Room]] services to organisations worldwide, including the likes of [[Morgan Stanley]], [[HSBC]] and others.<ref name="printweek">{{cite news|last1=Francis|first1=Jo|title=MBO at Imprima print operation {{!}} PrintWeek|url=http://www.printweek.com/print-week/news/1148840/mbo-imprima-print-operation|accessdate=22 August 2016|publisher=[[PrintWeek]]}}</ref> It also has offices in Paris, Frankfurt, Amsterdam and New York.<ref name="growthbusiness">{{cite news|title=The 21st century virtual data room: A how-to guide|url=http://www.growthbusiness.co.uk/growing-a-business/technology-for-business/2471012/the-21st-century-virtual-data-room-a-howto-guide.thtml|accessdate=22 August 2016|publisher=Growth Business UK}}</ref>\n\n==History==\nImprima was founded over 100 years ago and during this time has served the financial sector in a variety of different capacities.<ref>{{cite web|title=Imprima de Bussy Limited: Private Company Information|url=http://www.bloomberg.com/research/stocks/private/snapshot.asp?privcapId=613207|publisher=[[Bloomberg Businessweek]]|accessdate=22 August 2016}}</ref> By 1990, the company was a leading provider of Financial Print solutions, involved in the publication and delivery of sensitive and business-critical communications for their clients.<ref>{{cite web|title=Companies and products...|url=http://www.ukauthority.com/market-report/news/4614/companies-and-products|publisher=UK Authority|accessdate=25 August 2016}}</ref> In doing so, Imprima amassed an impressive customer list featuring some of the world’s most reputable financial advisors, law firms and corporations.<ref name="printweek" /><ref name="cloudnewsdaily">{{cite web|title=Virtual Data Room Providers|url=http://cloudnewsdaily.com/virtual-data-room/|publisher=Cloud News Daily|accessdate=22 August 2016}}</ref>\n\n==iRooms==\nIn 2001, Imprima launched their [[Virtual Data Room]] platform, iRooms.<ref name="teletrader" /> iRooms is used by organisations worldwide for Projects requiring secure online file storage and collaboration. Key use cases include [[Mergers & Acquisitions]] (M&A) activity and [[Real estate transaction|Real estate transactions]].<ref name="Francis">{{cite news|title=Rebrand for Imprima Financial Print|url=http://www.printweek.com/print-week/news/1154511/rebrand-for-imprima-financial-print|accessdate=22 August 2016|work=www.printweek.com|publisher=[[PrintWeek]]}}</ref> In 2012, iRooms software was completely revamped and receives regular upgrades.<ref name="cloudnewsdaily" /><ref name="Francis" />\n\n==New Ownership==\nIn 2014, iRoom was acquired by its current owner, OTM Participation. At that time, Imprima operated two product lines: Financial Print and iRooms (Virtual Data Rooms).<ref name="growthbusiness" /> In November 2014, OTM Participation took the decision to divest away the Financial Print division, whose directors carried out an MBO.<ref name="Francis" /><ref name="teletrader">{{cite web|title=Imprima Adds Multiple Language Interfaces To New iRooms Release|url=http://www.teletrader.com/news/details/6743031?ts=1471881604044|publisher=www.teletrader.com|accessdate=22 August 2016}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:1910 establishments in the United Kingdom]]\n[[Category:Companies based in London]]\n[[Category:Technology companies established in 1910]]\n[[Category:Technology companies of the United Kingdom]]']
['Big memory', '51756257', '\'\'\'Big-memory\'\'\' is a term used to describe server workloads which need to run on machines with a large amount of RAM ([[Random-access memory]]) memory. Some example workloads are databases, in-memory caches, and graph analytics.<ref>{{cite web |url=http://research.cs.wisc.edu/multifacet/papers/isca13_direct_segment.pdf|title=Efficient Virtual Memory for Big Memory Servers|accessdate=2016-09-24 }}</ref>\nOr, more generally, [[Data Science]] and [[Big data]].\n\nSome database systems are designed to run mostly in memory, rarely if ever retrieving data from disk or flash memory. See a [[List of in-memory databases]].\n\nThe performance of big memory systems depends on how the CPU\'s or CPU cores access the memory, via a conventional [[Memory controller]] or via NUMA ( [[Non-uniform memory access]] ). Performance also depends on the size and design of the [[CPU cache]].\n\nPerformance also depends on OS design. The "Huge pages" feature in Linux can improve the efficiency of [[Virtual Memory]].<ref>{{cite web |url=http://lwn.net/Articles/374424/ |title=Huge pages part 1 (Introduction)  |accessdate=2016-09-24 }}</ref> The new "Transparent huge pages" feature in Linux can offer better performance for some big-memory workloads.<ref>{{cite web |url=http://lwn.net/Articles/423584/ |title=Transparent huge pages in 2.6.38 |accessdate=2016-09-24 }}</ref> The "Large-Page Support" in Microsoft Windows enables server applications to establish large-page memory regions which are typically three orders of magnitude larger than the native page size.<ref>{{cite web |url=https://msdn.microsoft.com/en-us/library/windows/desktop/aa366720(v=vs.85).aspx|title=Large-Page Support |accessdate=2016-09-24 }}</ref>\n\n==References==\n{{reflist}}\n\n\n{{database-stub}}\n[[Category:Big data| ]]\n[[Category:Data management]]\n[[Category:Distributed computing problems]]\n[[Category:Technology forecasting]]\n[[Category:Transaction processing]]']
['Data', '18985040', '{{about||data in computer science|Data (computing)|other uses}}\n{{pp-move-indef}}\n[[File:Data types - en.svg|thumb|right|200px|Some of the different types of data.]]\n\'\'\'Data\'\'\' ({{IPAc-en|ˈ|d|eɪ|t|ə}} {{respell|DAY|tə}}, {{IPAc-en|ˈ|d|æ|t|ə}} {{respell|DA|tə}}, or {{IPAc-en|ˈ|d|ɑː|t|ə}} {{respell|DAH|tə}})<ref>The pronunciation {{IPAc-en|ˈ|d|eɪ|t|ə}} {{respell|DAY|tə}} is widespread throughout most varieties of English. The pronunciation {{IPAc-en|ˈ|d|æ|t|ə}} {{respell|DA|tə}} is chiefly [[Hiberno-English|Irish]] and [[American English|North American]]. The pronunciation {{IPAc-en|ˈ|d|ɑː|t|ə}} {{respell|DAH|tə}} is chiefly [[Australian English|Australian]], [[New Zealand English|New Zealand]] and [[South African English|South African]]. Each pronunciation may be realized differently depending on the dialect/language of the speaker.</ref> is a [[set (mathematics)|set]] of values of [[Qualitative data|qualitative]] or [[Quantitative data|quantitative]] variables. An example of qualitative data would be an [[anthropologist]]\'s handwritten notes about her interviews with people of an Indigenous tribe. Pieces of data are individual pieces of [[information]]. While the concept of data is commonly associated with [[scientific research]], data is collected by a huge range of organizations and institutions, including businesses (e.g., sales data, revenue, profits, [[stock price]]), governments (e.g., [[crime rate]]s, [[unemployment rate]]s, [[literacy]] rates) and non-governmental organizations (e.g., censuses of the number of [[homelessness|homeless people]] by non-profit organizations).\n\nData is [[measurement|measured]], [[data reporting|collected and reported]], and [[data analysis|analyzed]], whereupon it can be [[data visualization|visualized]] using graphs, images or other analysis tools. Data as a general [[concept]] refers to the fact that some existing [[information]] or [[knowledge]] is \'\'[[Knowledge representation and reasoning|represented]]\'\' or \'\'[[code]]d\'\' in some form suitable for better usage or [[data processing|processing]]. \'\'[[Raw data]]\'\' ("unprocessed data") is a collection of [[number]]s or [[character (computing)|characters]] before it has been "cleaned" and corrected by researchers. Raw data needs to be corrected to remove [[outlier]]s or obvious instrument or data entry errors (e.g., a thermometer reading from an outdoor Arctic location recording a tropical temperature).  Data processing commonly occurs by stages, and the "processed data" from one stage may be considered the "raw data" of the next stage. [[Field work|Field data]] is raw data that is collected in an uncontrolled "[[in situ]]" environment. [[Experimental data]] is data that is generated within the context of a scientific investigation by observation and recording. Data has been described as the new [[Petroleum|oil]] of the [[digital economy]].<ref>[https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Data Is the New Oil of the Digital Economy]</ref><ref>[https://spotlessdata.com/blog/data-new-oil Data is the new Oil]</ref>\n\n== Etymology and terminology ==\nThe first English use of the word "data" is from the 1640s. Using the word "data" to mean "transmittable and storable computer information" was first done in 1946. The expression "data processing" was first used in 1954.<ref name="eol">http://www.etymonline.com/index.php?term=data</ref>\n\nThe [[Data (word)|Latin word \'\'data\'\']] is the plural of \'\'datum\'\', "(thing) given," neuter past participle of \'\'dare\'\' "to give".<ref name="eol"/>  Data may be used as a plural noun in this sense, with some writers in the 2010s using \'\'datum\'\' in the singular and \'\'data\'\' for plural. In the 2010s, though, in non-specialist, everyday writing, "data" is most commonly used in the singular, as a [[mass noun]] (like "information", "sand" or "rain").<ref>{{cite web|last=Hickey |first=Walt |url=http://fivethirtyeight.com/datalab/elitist-superfluous-or-popular-we-polled-americans-on-the-oxford-comma/ |title=Elitist, Superfluous, Or Popular? We Polled Americans on the Oxford Comma |publisher=FiveThirtyEight |date=2014-06-17 |accessdate=2015-05-04}}</ref>\n\n== Meaning ==\nData, [[information]], [[knowledge]] and [[wisdom]] are closely related concepts, but each has its own role in relation to the other, and each term has its own meaning. Data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion. <ref>{{cite web|title=Joint Publication 2-0, Joint Intelligence|url=http://www.dtic.mil/doctrine/new_pubs/jp2_0.pdf|work=Defense Technical Information Center (DTIC)|publisher=Department of Defense|accessdate=February 22, 2013|pages=GL-11|date=22 June 2007}}</ref> [[Knowledge]] is derived from extensive amounts of experience dealing with information on a subject. For example, the height of [[Mount Everest]] is generally considered data. The height can be recorded precisely with an [[altimeter]] and entered into a database. This data may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to make a decision about the best method to climb it. Using an understanding based on experience climbing mountains to advise persons on the way to reach Mount Everest\'s peak may be seen as "knowledge". Some complement the series "data", "information" and "knowledge" with "wisdom", which would mean the status of a person in possession of a certain "knowledge" who also knows under which circumstances is good to use it.\n\nData is the least abstract concept, information the next least, and knowledge the most abstract.<ref>{{cite web|author=Akash Mitra|year=2011|title=Classifying data for successful modeling|url=http://www.dwbiconcepts.com/data-warehousing/12-data-modelling/101-classifying-data-for-successful-modeling.html}}</ref> Data becomes information by interpretation; e.g., the height of Mount Everest is generally considered "data", a book on Mount Everest geological characteristics may be considered "information", and a climber\'s guidebook containing practical information on the best way to reach Mount Everest\'s peak may be considered "knowledge". "Information" bears a diversity of meanings that ranges from everyday usage to technical use. Generally speaking, the concept of information is closely related to notions of constraint, communication, control, data, form, instruction, knowledge, meaning, mental stimulus, pattern, perception, and representation.<!--given by nupur seth--> Beynon-Davies uses the concept of a [[sign]] to differentiate between data and information; data is a series of symbols, while information occurs when the symbols are used to refer to something.<ref>{{cite book|author=P. Beynon-Davies|year=2002|title=Information Systems: An introduction to  informatics in organisations|publisher=[[Palgrave Macmillan]] |location=Basingstoke, UK|isbn=0-333-96390-3}}</ref><ref>{{cite book|author=P. Beynon-Davies|year=2009|title=Business information systems|publisher=Palgrave |location=Basingstoke, UK|isbn=978-0-230-20368-6}}</ref>\n\nBefore the development of computing devices and machines, only people could collect data and impose patterns on it. Since the development of computing devices and machines, these devices can also collect data. In the 2010s, computers are widely used in many fields to collect data and sort or process it, in disciplines ranging from [[marketing]], analysis of [[social services]] usage by citizens to scientific research. These patterns in data are seen as information which can be used to enhance knowledge. These patterns may be interpreted as "[[truth]]" (though "truth" can be a subjective concept), and may be authorized as aesthetic and ethical criteria in some disciplines or cultures. Events that leave behind perceivable physical or virtual remains can be traced back through data. Marks are no longer considered data once the link between the mark and observation is broken.<ref>{{cite book|author=Sharon Daniel|title=The Database: An Aesthetics of Dignity}}</ref>\n\nMechanical computing devices are classified according to the means by which they represent data. An [[analog computer]] represents a datum as a voltage, distance, position, or other physical quantity. A [[Computer|digital computer]] represents a piece of data as a sequence of symbols drawn from a fixed [[alphabet]]. The most common digital computers use a binary alphabet, that is, an alphabet of two characters, typically denoted "0" and "1". More familiar representations, such as numbers or letters, are then constructed from the binary alphabet. Some special forms of data are distinguished. A [[computer program]] is a collection of data, which can be interpreted as instructions. Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably [[Lisp (programming language)|Lisp]] and similar languages, programs are essentially indistinguishable from other data. It is also useful to distinguish [[metadata]], that is, a description of other data. A similar yet earlier term for metadata is "ancillary data."  The prototypical example of metadata is the library catalog, which is a description of the contents of books.\n\n== In other fields ==\nThough data is also increasingly used in other fields, it has been suggested that the highly interpretive nature of them might be at odds with the ethos of data as "given". Peter Checkland introduced the term \'\'capta\'\' (from the Latin \'\'capered\'\', “to take”) to distinguish between an immense number of possible data and a sub-set of them, to which attention is oriented.<ref>{{cite book | author = P. Checkland and S. Holwell | title = Information, Systems, and Information Systems: Making Sense of the Field. | year = 1998 | publisher = John Wiley & Sons | location = Chichester, West Sussex | isbn = 0-471-95820-4 | pages = 86–89  }}</ref> [[Johanna Drucker]] has argued that since the humanities affirm knowledge production as "situated, partial, and constitutive," using \'\'data\'\' may introduce assumptions that are counterproductive, for example that phenomena are discrete or are observer-independent.<ref>{{cite web\n |author=Johanna Drucker\n |year=2011\n |title=Humanities Approaches to Graphical Display\n |url=http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html\n}}</ref> The term \'\'capta\'\', which emphasizes the act of observation as constitutive, is offered as an alternative to \'\'data\'\' for visual representations in the humanities.\n\n== See also ==\n{{div col|5}}\n* [[Biological data]]\n* [[Data acquisition]]\n* [[Data analysis]]\n* [[Data cable]]\n* [[Dark data]]\n* [[Data domain]]\n* [[Data element]]\n* [[Data farming]]\n* [[Data governance]]\n* [[Data integrity]]\n* [[Data maintenance]]\n* [[Data management]]\n* [[Data mining]]\n* [[Data modeling]]\n* [[Data visualization]]\n* [[Computer data processing]]\n* [[Data publication]]\n* [[Information privacy|Data protection]]\n* [[Data remanence]]\n* [[Data set]]\n* [[Data warehouse]]\n* [[Database]]\n* [[Datasheet]]\n* [[Environmental data rescue]]\n* [[Fieldwork]]\n* [[Metadata]]\n* [[Open data]]\n* [[Scientific data archiving]]\n* [[Statistics]]\n* [[Computer memory]]\n* [[Data structure]]\n* [[Raw Data]]\n* [[Secondary Data]]\n{{div col end}}\n\n== References ==\n{{FOLDOC}}\n{{Reflist}}\n\n== External links ==\n{{Wiktionary}}\n* [http://purl.org/nxg/note/singular-data Data is a singular noun] (a detailed assessment)\n\n{{Statistics}}\n\n[[Category:Computer data| ]]\n[[Category:Data| ]]\n[[Category:Data management]]']
['5 Ways of Conceptualizing Data', '52060631', '{{multiple issues|\n{{essay-like|date=December 2016}}\n{{notability|date=December 2016}}\n{{original research|date=December 2016}}\n}}\n{{Orphan|date=December 2016}}\n\n[[Data]] can be viewed as a measurement of numbers, and characters that are set in a way to understand a certain subject. However, there are many different ways to view data; such as conceptualizing data. These are five ways of conceptualizing data. They all have positive and negative points to each technique. Although they are different, they all bring up questions and concerns with data collection and what happens with the information afterward. Another concern is what is the goal with the data that has been collected depending on the category. The five ways of conceptualizing data are technically, ethically, politically and economically, spatially/ temporal, and philosophically. Typically viewed by critical data scholars, they have all of these ways of viewing data because it is important to see the different ways that data can be viewed and to see if there may be any bias. Not only is it important to see if there is any bias, however, it is also important to understand what all the data will mean in the bigger picture. The way that this is normally done is by understanding raw data, then placing them into categories that will help with the better understanding and creating new knowledge.\n\n==Technically==\nTechnically viewing data concerns the knowledge about the quality of data, if it is reliable, if it is authentic, if it is valid. It is also about knowing how the data is structured, shared, processed, and analyzed.<ref>(Kitchin, p.12)</ref> There are views about the concerns around data such as the representativeness, how it is uncertain, the reliability of it, the chances of any errors, the likelihood of any bias, and around the measuring of the research design and the execution of it.<ref>(Kitchin, p.13)</ref> There are also questions around if this form of scientific technique is going to bring the data that is wanted and needed.<ref>(Kitchin, p.13)</ref> Other reliability concerns go with this technical view about data such as Quixotic reliability, Diachronic reliability, and Synchronic reliability. Quixotic reliability concern is where there is one observation method which produces unvarying measurements.<ref>(Kitchin, p.13-14)</ref> Diachronic reliability is the stability of an observation through time. Lastly, Synchronic reliability is the similarity of observations within the same time period.<ref>(Kitchin,p.14)</ref> With it being technology, there are many different ways that errors could arise, such as, missing data, mistakes, misunderstaning\'s, bias’, and uncertainty.<ref>(Kitchin, p.14)</ref>\n\n==Ethically==\nThe [[ethics|ethical]] view of data is more about the idea of why the data is generated, and what use the data is going to be placed in. There are concerns around how the data will be shared, protected, traded, and to how they are employed”.<ref>(Kitchin, p.15)</ref> This also deals with the issue of sensitivity. Some data is low when it comes to sensitivity, such as the traffic. However, some are a lot higher, such as speaking to survivors of crime.<ref>(Kitchin, p. 15)</ref> With the sensitivity scale, there comes privacy issues, how someone may be treated, and the issue of human rights.<ref>(Kitchin, p. 15)</ref> It is helpful to know that some companies have a data protection act and have privacy laws.<ref>(Kitchin, p.15)</ref> Other components that add to the category of Ethics are the question of equality, fairness, justice, honesty, respect, entitlements, rights, and care of the information that is provided and towards those that give the information.<ref>(Kitchin, p.14)</ref> The honesty, respect and the care of the information can also be misinformed to the subject that is giving the data willingly. Causing ethical concerns for how long the information will be kept, or what the information will be used for. This is an ethical concern in the exchange of the subject and the researcher.<ref>(Jacob)</ref>\n\n==Politically and economically==\n[[Politics|politically]] and [[economic]]ally viewed data is seen to how the data could be viewed or theorized as public goods, intellectual property, political capital, and how they are traded and how they are regulated.<ref>(Kitchin, p.16)</ref> Economically there are many decisions when funding data researching, as well as investing in data researching. Data could be used to manage goals and raise the profits and values to those that invest in it.<ref>(Kitchin, p.15)</ref> Such as the multi-billion-dollar data marketplace, where many companies are trading and using that data to help themselves make a profit. It is positively effecting due to the production of knowledge.<ref>(Kitchin, p. 15)</ref> The more that the company knows about what the people want, and how to market to them, the more that they may profit and gain off of the data, due to them giving what the people want. However, there still is the political side to this. Although the data can make a profit and is economically great, there is also the competition which want to influence opinions and make the data terrain greater.<ref>(Kitchin, p.16)</ref> It is also political because the difference between publicly good data, which is shared with anyone that can have access to it, is much different than business data. This is because business data is wanting to keep the data that they have found and use it to their advantage, such as the “production of knowledge.” <ref>(Kitchin, p.16)</ref> The publicly good data is free to anyone that wants to view it, which would not be helpful in any way to any business strategies or marketing.<ref>(Kitchin, p. 16)</ref>\n\n==Spatially/temporal==\nSpatial and [[temporal]] views data around technical, ethical, political, and economic [[Regime]] with the production of the data.<ref>(Kitchin, p.17)</ref> The way that the terms spatial and temporal can be viewed is around how the data is developed and changed across time and space. Although, depending on the time and where this data is being collected, the process, the analysis, the storage of some information, yet not of others will be different, just due to a time frame and area will be different than others because of the different history that has happened and the different geographical locations. As noted the process of taking in data changes over time, however, they are never sudden changes. These changes happen slowly over time due to different laws that come in place around how data is handled or protected, the different forms of organizing, the improvements around administration, if any new technology has formed, when the methods of data sorting have changed, along with the methods of sorting the data, the geographical statistics that vary and the new techniques of statistics.<ref>(Kitchin, p.17)</ref> Not only does the geographical location change how the assemblage of data is taken is, but it can also be different depending on the person due to how they manage the data, or how they produce it.<ref>(Kitchin, p.17)</ref>\n\nLooking over data temporally can bring forth either questions or patterns depending on what the data is about. An example of this is looking at graphs that have time in them. They present inclines and declines in a pattern about the data over time.<ref>(Whitney)</ref> Spatial data, on the other hand, looks more towards the geographical sense in the data. The information that is gathered could be about the location, the size or the shape of a particular object. A system that uses spatial data is [[GIS]] (Geographical Information System) <ref>(Rouse)</ref>\n\n==Philosophical==\n[[Philosophy]] brings forth views around the areas of epistemology and ontology. In this view of data, there is no interpretations, opinions, importance, or relevance of the data that has been found and processed. The data is simply measured for what it is. Which brings forth to how it is viewed. The data that is viewed philosophically is also viewed in an objective way which means that the data is fixed in some way to prove a specific point. Although the data may be truthful, how the data was provided and how it is placed makes the difference. The data is also viewed in a realist view such as how things truly are. No information is changed, everything is the way that it is and is seen for that.<ref>(Kitchin, p.17-19)</ref> This view also brings up issues around property rights.<ref>(Liu, p.61)</ref> Who would own what and who can have the right to take things.\n\n==References==\n{{Reflist|20em}}\n\n===Works cited===\n{{refbegin}}\n* {{cite book |last=Kitchin |first=Rob |year=2014 |title=The data revolution: Big data, open data, data infrastructures & their consequences |place=London |publisher=Sage |chapter=Conceptualising data |pp=1–26 |chapter-url=http://www.uk.sagepub.com/upm-data/63923_Kitchin_CH1.pdf |format=pdf}}\n* Metcalf, Jacob, Emily F. Keller, and danah boyd. 2016. “Perspectives on Big Data, Ethics, and Society.” Council for Big Data, Ethics, and Society.\n* Rouse, Margaret. (2013). "What Is Spatial Data? - Definition from WhatIs.com." \'\'SearchSQLServer\'\'. TechTarget.\n* Liu, Hong. (2016). "Philosophical Reflections on Data. " Philosophical Reflections on Data. Science Direct.\n* Whitney, Hunter. (2014). "It\'s About Time." It\'s About Time: Visualizing Temporal Data to Reveal Patterns and Stories | UX Magazine. UX Magazine.\n{{refend}}\n\n\n\n[[Category:Data management]]']
['Category:Computer logging', '52734920', '[[Category:Computing]]\n[[Category:Data management]]']
['Rubrik', '52955620', '\'\'\'Rubrik\'\'\' is a privately held cloud data management company headquartered in [[Palo Alto, California|Palo Alto, CA]]. Rubrik offers a data management platform for enterprises in private, public, and hybrid [[Cloud computing|cloud environments]].<ref>{{Cite web|url=http://social.techcrunch.com/2016/08/16/rubrik-snares-61-million-series-c-led-by-khosla-ventures/|title=Rubrik snares $61 million Series C led by Khosla Ventures|last=Miller|first=Ron|website=TechCrunch|access-date=2017-01-23}}</ref>\n\n{{Infobox company\n| name = Rubrik\n| type = Private\n| logo = Rubrik logo.png\n| industry = Cloud Data Management\n| founder =  Bipul Sinha, Arvind Jain, Soham Mazumdar, and Arvind Nithrakashyap\n| hq_location_city = Palo Alto, CA\n| hq_location_country = United States\n| products = Rubrik Cloud Data Management platform\n| website = {{URL|www.rubrik.com/}}\n}}\n\n== History ==\nRubrik was founded in 2014 by Bipul Sinha, Arvind Jain, Soham Mazumdar, and Arvind Nithrakashyap.<ref>{{Cite news|url=http://www.forbes.com/sites/benkepes/2015/03/24/with-an-a-grade-founding-team-and-a-grade-investors-rubrik-launches/#219bf1ea2b6e|title=With An A-Grade Founding Team And A-Grade Investors, Rubrik Launches|last=Kepes|first=Ben|newspaper=Forbes|access-date=2017-01-23}}</ref> It raised $10 million in March 2015,<ref>{{Cite news|url=http://blogs.wsj.com/venturecapital/2015/03/24/rubrik-emerges-with-10-million-for-software-defined-backup/|title=Rubrik Emerges With $10 Million for Software-Defined Backup|last=Gage|first=Deborah|newspaper=WSJ|language=en-US|access-date=2017-01-23}}</ref> followed by a $41 million round that same May. In August 2016, Rubrik raised an additional $61 million in funding led by [[Khosla Ventures]].<ref>{{Cite news|url=http://www.businessinsider.com/rubrik-raises-61-million-khosla-ventures-2016-8|title=This investor turned founder scoffs at funding slowdown: \'real businesses\' can still get money|newspaper=Business Insider|language=en|access-date=2017-01-23}}</ref> Sinha, who is Founding Investor at [[Nutanix]] and a partner at [[Lightspeed Venture Partners]], currently serves as CEO.\n\nIn 2015, Rubrik launched with its r300 series appliances, called "Briks", in two configurations for VMware environments. The appliance includes a console from which users can manage and monitor their data.<ref>{{Cite web|url=http://cormachogan.com/2015/05/26/a-closer-look-at-rubrik/|title=A closer look at Rubrik|date=2015-05-26|website=CormacHogan.com|access-date=2017-01-23}}</ref> In April 2016, it launched its r528 Brik, which is [[FIPS 140-2]] Level 2 certified.<ref>{{Cite web|url=http://www.thepaypers.com/default/rubrik-r528-provides-storage-encryption-and-security/764067-0|title=Rubrik r528 provides storage encryption and security|website=www.thepaypers.com|access-date=2017-01-23}}</ref>\n\nIn April 2016, the company released their Cloud Data Management platform with Rubrik Firefly, which extended capabilities to physical [[SQL]] and [[Linux]] environments, and with Rubrik Edge, a software appliance that extended protection to remote and branch offices. The release also included a software upgrade to utilize [[Erasure code|erasure coding]].<ref>{{Cite web|url=http://www.theregister.co.uk/2016/08/16/rubriks_extra_funding_as_firefly_data_management_flies_out_of_the_coop/|title=Rubrik\'s extra funding as Firefly extended data management flies out of the coop|last=18:13|first=16 Aug 2016 at|last2=tweet_btn()|first2=Chris Mellor|access-date=2017-01-23}}</ref><ref>{{Cite web|url=http://blog.mwpreston.net/2016/08/16/rubrik-firefly-released/|title=Rubrik Firefly - Now with physical, edge, and moar cloud!|date=2016-08-16|website=mwpreston.net|access-date=2017-01-23}}</ref>\n\n== Awards ==\nRubrik was a winner of the 2015 Virtualization Review Editor\'s Choice Awards.<ref>{{Cite web|url=https://virtualizationreview.com/articles/2015/12/01/editors-choice-awards.aspx|title=The 2015 Virtualization Review Editor\'s Choice Awards -|last=Ward|first=By Keith|last2=01/04/2016|website=Virtualization Review|access-date=2017-01-23}}</ref> In 2016, Rubrik was named a [[Gartner]] Cool Vendor in Storage Technologies.<ref>{{Cite web|url=https://www.gartner.com/doc/3290518/cool-vendors-storage-technologies-|title=Cool Vendors in Storage Technologies, 2016|website=www.gartner.com|access-date=2017-01-23}}</ref> Rubrik won “Best security or data protection project” for Best of VMworld Europe User Awards 2016.<ref>{{Cite news|url=http://www.techtarget.com/press-release/techtargets-searchservervirtualization-com-announces-best-vmworld-2016-award-winners/|title=TechTarget’s SearchServerVirtualization.com Announces “Best of VMworld” 2016 Award Winners - TechTarget|newspaper=TechTarget|language=en-US|access-date=2017-01-23}}</ref> In October 2016, Rubrik was selected as one of 25 “Next Billion-Dollar Startups” by [[Forbes]].<ref>{{Cite news|url=http://www.forbes.com/sites/amyfeldman/2016/10/19/next-billion-dollar-startups-2016/#1221eca0554e|title=Next Billion-Dollar Startups 2016|last=Feldman|first=Amy|newspaper=Forbes|access-date=2017-01-23}}</ref>\n\n== References ==\n{{Reflist}}\n\n[[Category:Data management]]\n[[Category:Cloud computing providers]]\n[[Category:Companies based in California]]']
['Category:Video storage', '764184', '[[Category:Video|Storage]]\n[[Category:Electronic documents]]\n[[Category:Information storage]]\n[[Category:Storage media]]']
['Electronic Document Professional', '17463457', 'The \'\'\'EDP\'\'\' (\'\'\'Electronic Document Professional\'\'\') is a professional designation awarded by [[Xplor International]] for participants in the [[electronic document]] industry who have satisfied a number of criteria.\n\nThe EDP is not a measure of specific knowledge, but awardees normally have a broad knowledge of the electronic document industry along with specific knowledge in one or more areas. Thus, the EDP differs from certifications such as the [[CDIA+]] from CompTIA in which the awardee has passed a formal exam. Rather, the EDP is more closely related to the older MIT and LIT designations from [[AIIM]]. Generally, the EDP does show that the awardee has been in the industry for at least 5 years, and has participated in at least 3 major projects showing competence in a number of \'bodies of knowledge\'.\n\nThe EDP program is regulated by the EDP Commission of [[Xplor International]], which is a body of senior professionals in the electronic document industry who set the standards and judge the qualifications of the applicants.\n\n== History ==\n\nThe first EDP \'class\' (as the annual group of awardees is called) was in 1990, when 12 industry professionals were given the award. In 1991, the class was much smaller (only 6), but in 1992 and in all the years following, the number of awardees has generally been double digits, with as many as 25 at one time. The Dutch Chapter of Xplor International has particularly stressed the EDP designation as an essential part of being a professional in the electronic document industry, with the result that in some years, more than ten Dutch members alone were named EDPs, and there are more EDPs per capita in The Netherlands today than in any other country.\n\nIn 2009, Xplor relaunched its certification program, so that there is now a three-level certification process to help employers benchmark their staff.<ref>http://www.printingnews.com/web/online/Industry-News/Xplor-Expands-EDP-Certification/1$10502</ref>\n\nThe EDP and the Master-EDP awards are presented once a year at the annual international conference of Xplor. While the first EDPs were awarded at the 1990 conference in Nashville, Tennessee (USA), the first \'class\' of Master-EDPs was awarded at the association\'s annual event in St. Petersburg, Florida (USA) in March, 2010. At that event, the following industry professionals received the association\'s highest certification:<ref name="m-edp">[http://www.xplor.org/EDP/MEDPlist.cfm]</ref>\n* William Broddy, M-EDP\n* Ernie Crawford, M-EDP\n* Scott Draeger, M-EDP\n* Oscar Dubbeldam, M-EDP\n* William J. "Bill" McCalpin, M-EDP, CDIA, MIT, LIT\n* Walter Riddock, M-EDP, CMDSM\n* Donald Scrima, M-EDP\n\nAt the association\'s next annual event in April, 2011, two more industry professionals will be named M-EDPs:<ref name="m-edp"/>\n* Pat McGrew, M-EDP\n* Carrie Murphy, M-EDP\n\nOn the other hand, the new EDA designation is awarded at the point that the individual\'s application is accepted and verified, throughout the year. Currently (December 2010), there are 48 industry professionals who have received this designation,<ref>http://www.xplor.org/EDP/EDA.cfm?viewpage=EDAlist</ref> nearly all of whom received the designation as a result of attending courses certified by Xplor and taught by [http://www.acadami.org \'\'acadami\'\'], two of whose principals are M-EDPs.\n\n== Levels of certification ==\n\n=== Electronic Document Associate (EDA) ===\nThe EDA designation recognizes electronic document sales, development and support specialists who have shown significantly more knowledge of the industry than someone in another discipline.\nIt requires candidates to be in the industry for 2+ years and have successfully completed 5 days of Xplor Continuing Education Unit (CEU) certified courses, or the equivalent.<ref>http://www.xplor.org/EDP/EDA.cfm</ref>\n\n=== Electronic Document Professional (EDP) ===\nEDPs have clearly shown enough working knowledge of the process to make significant decisions regarding technology or process deployment. For example, management should trust them to lead projects, or support teams.\nTo become certified as an EDP, a candidate must be in the industry for 5+ years, have successfully completed 10 days of Xplor CEU training (or the equivalent), and have shown their working knowledge and experience through 3 work examples.<ref>http://www.xplor.org/EDP/EDP.cfm</ref>\n\n=== Master Electronic Document Professional (M-EDP) ===\nM-EDPs are the recognized experts on specific technologies, processes, or management skills. For example, an M-EDP may have co-developed a composition or print stream transform system. Another might be the expert on print costing, or statement design. By earning their M-EDP, they are clearly recognized as one of the ‘go to’ people in the industry.\n\nTo earn the M-EDP, a candidate must have been in the industry for at least 10 years, have been an EDP for at least 5 years, and be able to prove their area of expertise through published material.<ref>http://www.xplor.org/EDP/MEDP.cfm</ref>\n\n== Designees ==\nIn all, there are more than 500 industry professionals with the EDP designation, including:\n* [http://www.xplorcanada.org/media/eastern/Broddy-Bio-2005.pdf William Broddy, M-EDP] - formerly the Vice Chair of the EDP Commission\n* [http://www.mccalpin.com William J. \'Bill\' McCalpin, M-EDP] - author of \'\'The Document Dilemma\'\' and former General Manager of Xplor International \n* [http://www.acadami.org/about.html Dr. Michael Turton, EDP] - recognized expert in the design of transaction documents and the use of color in transaction documents\n* Scott Kelly, EDP - current Xplor International board member\n* [http://www.crawfordtech.com/ManagementTeam.htm Ernie Crawford, M-EDP] - President of Crawford Technologies\n* [http://www.nautilussolutions.com Stephen Poe, EDP] - Principal at Nautilus Solutions\n* [http://www.gmc.net Scott Draeger, M-EDP] - Vice President of Product at GMC Software\n* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Loic Avenel, EDP] - EMEA Product Marketing Manager at HP Exstream\n* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Olivier DOILLON, EDP] - EMEA South Presales Manager at HP Exstream\n* Roberta McKee-Jackson, EDP - Principal at RSM Consulting and current Vice-Chair of EDP Commission\n* Skip Henk, EDP - Current President/CEO of Xplor International\n* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Kent Lewis, EDP] - Product Manager, HP Exstream\n* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Matt Riley, EDP] - Solution Support Manager, HP Exstream\n* Donald A. Scrima, M-EDP - Current Chair of EDP Commission (2009-20120) and Principal at AFP Education & Consulting\n* [http://www.deberichtenfabriek.nl/wie-wij-zijn/dick-paul-en-rene-joor René M. Joor, EDP] - Current member of the EDP Commission, Partner at De Berichtenfabriek BV, the Netherlands \n* Kenneth H. Pugh, EDP - Senior Output Analyst at Aon Corporation\n* Kathy Rixham, EDP - Print System Administrator at RR Donnelley\n* Kees van de Graaf, EDP - Consultant at PostNL\n* James Shand, EDP - Xplor International member of +30-years, Current President Xplor UK & Ireland and member of the Xplor International Board of Directors\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.xplor.org/ Xplor International Web Page]\n* [http://www.xplor.org/edp/index.cfm Direct Link to Xplor International Certification Program]\n\n[[Category:Electronic documents]]']
['SAFE-BioPharma Association', '2321249', '{{primary sources|date=February 2010}}\n\'\'\'SAFE-BioPharma Association\'\'\' is the non-profit association that created and manages the SAFE-BioPharma [[digital identity]] and [[digital signature]] standard for the global [[pharmaceutical]], [[biotech]] and [[healthcare]] industries. SAFE stands for "Signatures & Authentication For Everyone" (but originally stood for "Secure Access For Everyone"<ref>{{cite web|title=CSI page on SAFE describing original goal and name|url=http://www.csirochester.com/safe.htm}}</ref>).  It was originally created as an initiative of the [[Pharmaceutical Research and Manufacturers of America]] (PhRMA) association to encourage the use of a common digital identity and digital signature standard for the pharmaceutical industry, but is now an independent non-profit association offering such standards services to the government and the entire healthcare industry.\n\nThe SAFE-BioPharma industry standard is used to establish and manage digital identities and to issue and apply digital signatures. It mitigates legal, regulatory and business risk associated with business-to-business and business-to-regulator electronic transactions. It also facilitates interoperability by providing a secure, enforceable, and regulatory-compliant way to verify identities of parties involved in electronic transactions.\n\nSAFE-BioPharma’s vision is to be a catalyst in transforming the biopharmaceutical and healthcare communities to a fully electronic business environment by 2012.\n\n== Certificate authority ==\nThe SAFE-BioPharma digital identity and signature standard operates one of the nation’s leading [[public key infrastructure]] (PKI) certificate authority bridges. These PKI certificate bridges establish an infrastructure for the trusted exchange of confidential information and the reliable authentication of identities over the Internet. By providing a highly secure way to validate, trust, and manage identities of unknown participants in an Internet transaction, the SAFE-BioPharma Bridge [[certificate authority|Certificate Authority]] (SBCA) is essential to helping achieve the speed, efficiency and cost-savings inherent in use of the Internet for business transactions. This technology also improves interoperability across many different systems.\n\n== Cross-certification ==\nThe SAFE-BioPharma Bridge Certificate Authority is cross-certified with the [http://www.cio.gov/fpkia/ Federal Bridge Public Key Infrastructure Architecture] (FPKIA), facilitating the ability of SAFE-BioPharma member companies that meet certain security, technical and operational criteria to leverage the identity credentials of any and all bridge members in the exchange of sensitive and confidential information. In essence, it allows officials in [[Health and Human Services]], the [[Food and Drug Administration]], [[United States Department of Defense|Department of Defense]], and other government agencies to trust the origins of electronic documents received from corporate managers, physicians, clinical researchers, etc. who are credentialed to digitally sign documents with SAFE-BioPharma digital signatures. Additionally, the identities are trusted for authentication access control for sites requiring strong authentication.\n\n== Regulatory acceptance ==\nSAFE-BioPharma has worked closely with the US Food and Drug Administration (FDA), the [[European Medicines Agency]] (EMA) and other global healthcare and regulatory agencies to ensure the digital signatures generated using SAFE-BioPharma certificates meet regulatory requirements and are accepted by these agencies when used on documents that are part of electronic submissions. It thus allows voluminous paper documents used for regulatory compliance to be digitally signed and submitted in electronic form. This improves accuracy, reduces costs, enables electronic search and retrieval and saves energy and natural resources.\n\n== Member organizations ==\nSAFE-BioPharma Association members include [[Abbott Laboratories]] (NYSE: ABT), [[Amgen]] (NASDAQ: AMGN), [[AstraZeneca]] (NYSE: AZN), [[Bristol-Myers Squibb]] (NYSE:BMY), [[GlaxoSmithKline]] (NYSE: GSK), [[Johnson & Johnson]] (NYSE: JNJ), [[Eli Lilly and Company|Eli Lilly]] (NYSE: LLY), [[Merck & Co.|Merck]] (NYSE:MRK), [[National Notary Association]], [[Pfizer]] (NYSE: PFE), Premier Inc.,  and [[Sanofi-Aventis]] (NYSE:SNY).\n\nSAFE-BioPharma is a trademark of the SAFE-BioPharma Association. Any use of this trademark requires approval from the SAFE-BioPharma Association.\n\n==See also==\n* [[Electronic lab notebook]]\n* [[Public key infrastructure|PKI]]\n* [[Title 21 CFR Part 11]]\n* [[Digital signature]]\n* [[Electronic Signatures in Global and National Commerce Act]] (ESIGN, USA)\n* [[European Medicines Agency]] (EMEA)\n* [[Food and Drug Administration]] (FDA)\n* [[Pharmaceutical company]]\n* [[Japan Pharmaceutical Manufacturers Association]] (JPMA)\n* [[Digital signature]]\n* [[Data management]]\n\n==External links==\n* [http://www.safe-biopharma.org SAFE-BioPharma Organization Website]\n\n==References==\n\n{{Reflist}}\n\n{{DEFAULTSORT:Safe-Biopharma Association}}\n[[Category:Public-key cryptography]]\n[[Category:Cryptography companies]]\n[[Category:Electronic documents]]\n[[Category:Non-profit organizations based in New Jersey]]\n[[Category:Pharmaceutical industry trade groups]]\n[[Category:International medical and health organizations]]\n[[Category:Medical and health organizations based in the United States]]']
['Novell Vibe', '25023858', "{{multiple issues |{{Notability|Products|date=April 2012}}\n{{refimprove|date=April 2012}}\n{{advert|date=November 2012}}\n}}\n\n{{Infobox software\n|name                       = Novell Vibe\n|logo                       = \n|screenshot                 = \n|caption                    = \n|collapsible                = \n|author                     = \n|developer                  = [[Novell]]\n|released                   = {{Start date|2008|06|25}} <!-- {{Start date|YYYY|MM|DD}} -->\n|discontinued               = \n|latest release version     = \n|latest release date        = <!-- {{Start date and age|YYYY|MM|DD}} -->\n|latest preview version     = \n|latest preview date        = <!-- {{Start date and age|YYYY|MM|DD}} -->\n|frequently updated         = \n|programming language       = \n|operating system           = \n|platform                   =\n|size                       = \n|language                   = \n|status                     = \n|genre                      = [[Web application]]\n|license                    = Proprietary\n|website                    = [http://www.novell.com/products/vibe/  Novell Vibe]\n}}\n\n'''Novell Vibe''' is a web-based team collaboration platform developed by [[Novell]], and was initially released by Novell in June 2008 under the name of Novell Teaming. Novell Vibe is a collaboration platform that can serve as a knowledge repository, [[document management system]], project collaboration hub, process automation machine, corporate [[intranet]] or [[extranet]]. Users can upload, manage, comment on, and edit content in a secure manner. Supported content includes documents, calendars, discussion forums, wikis, blogs, tasks, and more.\n\nDocument management functionality allows for document versions, approvals, and document life cycle tracking. Users can download and modify pre-built custom web pages and workflows free of charge from the Vibe Resource Library.<ref>{{cite web|url=http://www.novell.com/products/vibe/resource-library/ |title=Novell Vibe Resource Library |publisher=Novell.com |date= |accessdate=2012-04-12}}</ref>\n\n==History==\nNovell Vibe is the result of a merging of two products in November 2010: Novell Teaming and Novell Pulse.<ref>http://www.novell.com/communities/node/12257/great-vibes-paths-merging-novell-teaming-and-novell-pulse</ref>\n\nNovell Teaming began as SiteScape Forum. When Novell Acquired SiteScape in 2008, the name was changed to Novell Teaming.{{citation needed|date=August 2012}}\n\nCreated in 2009, Novell Pulse was a communication tool based on the [[Google Wave Federation Protocol]].<ref>http://www.scala-lang.org/node/6618</ref>\n\n===Server===\nAny combination of Linux and Windows servers can run the Vibe application. Furthermore, MySQL, MS SQL, and Oracle databases are supported.\n\n===End-User operating systems===\nWindows, Linux, Mac, iOS or Android\n\n===Browsers===\nFirefox, Internet Explorer, Safari, or Chrome\n\n===Mobile===\nAny mobile device that has a browser can access the Vibe site. Native iOS <ref>https://itunes.apple.com/us/app/novell-vibe/id476653054?mt=8</ref> and Android <ref>https://play.google.com/store/apps/details?id=com.novell.vibe.android</ref> apps are available for free download in the app stores.\n\n===Microsoft Office integration===\nWord, PowerPoint, and Excel (versions 2013,2010 and 2007) on the Windows operating system are supported.\n\n==Interoperability==\nVibe can be used in conjunction with various other software products, such as [[Novell Access Manager]], [[Novell GroupWise]], [[Skype]], and [[YouTube]]. Novell Vibe integrates with an LDAP directory for authentication.\n\n==Extendability==\nVibe administrators can extend the Vibe software by creating [[software extensions]], remote applications, or [[JAR (file format)]] files that enhance the power and usefulness of the Vibe software to create a custom experience for users.\n\nSoftware extensions enable third-party developers to create abilities which extend an application. Vibe administrators or Vibe developers can create custom extensions (add-ons) to enhance Vibe. For example, you might have an extension that enables Flash video support in Vibe.\n\n===Remote applications===\nA remote application is a program that runs on a remote server and delivers data for use on the Novell Vibe site (such as data from a remote database). For example, Vibe administrators or Vibe developers could set up a remote application for Twitter that displays all of a user's Twitter entries in Vibe.\n\nUnlike creating an extension for Vibe, creating a remote application does not modify the Vibe software.\n\n== Open-source solutions ==\nNot all of these projects implement all of the features Novell Vibe has to offer as well as Vibe is missing some features these products have:\n\n* [[Kablink|Kablink Vibe]] - an open source version of Novell Vibe\n* [[Redmine]]/[[ChiliProject]]\n* [[trac]]\n* [[Feng Office Community Edition]]\n* [[Open Workbench]]\n* [[OpenProj]]\n\n''see also'': [[:Category:Free project management software|Wikipedia category for free project management software]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.novell.com/products/vibe/resource-library/  Novell Vibe product page]\n* [http://kabtim.ru//  Kablink-Vibe]\n{{Novell}}\n\n[[Category:Novell software]]\n[[Category:Proprietary wiki software]]\n[[Category:Electronic documents]]\n[[Category:Instant messaging]]\n[[Category:Online chat]]\n[[Category:Social information processing]]\n[[Category:Groupware]]\n[[Category:Internet Protocol based network software]]\n[[Category:Blog software]]"]
['Xena (software)', '27437313', "{{Other uses|Xena (disambiguation)}}\n\n'''Xena''' is [[open-source software]] for use in [[digital preservation]]. Xena is short for XML Electronic Normalising for Archives.\n\nXena is a [[Java (programming language)|Java]] application developed by the [[National Archives of Australia]]. It is available free of charge under the [[GNU General Public License]].\n\nVersion 6.1.0 was released 31 July 2013. Source code and binaries for Linux, OS X and Windows are available from [[SourceForge]].\n\n==Mode of operation==\nXena attempts to avoid [[digital obsolescence]] by converting files into an openly specified format, such as [[OpenDocument|ODF]] or [[Portable Network Graphics|PNG]]. If the file format is not supported or the Binary Normalisation option is selected, Xena will perform [[ASCII]] [[Base64]] encoding on binary files and wrap the output in XML metadata. The resulting .xena file is plain text, although the content of the data itself is not directly human-readable. The exact original file can be retrieved by stripping the metadata and reversing the Base64 encoding, using an internal viewer.\n\n==Features==\nPlatforms supported by Xena are [[Microsoft Windows]], [[Linux]] and [[Mac OS X]].\n\nXena uses a series of plugins to identify file formats and convert them to an appropriate openly specified format.\n\nXena has an [[application programming interface]] which allows any reasonably skilled Java developer to develop a plugin to cover a new file type.\n\nXena can process individual files or whole directories. When processing a whole directory, it can preserve the original directory structure of the converted records.\n\nXena can create plain text versions of file formats such as [[Tagged Image File Format|TIFF]], [[Microsoft Word|Word]] and [[Portable Document Format|PDF]], with the use of [[Tesseract (software)]].\n\nThe Xena interface or Xena Viewer can be used to view or export a Xena file (extension .xena) in its target file format. These files contain the normalised file as well as any extra information relevant to the normalisation process.\nThe Xena Viewer supports bulk export of Xena files to target file formats.\n\nXena can be used via its [[graphical user interface]] or the [[command line]].\n\nFor Xena to be fully functional, it requires a local installation of the following external software:\n*[[LibreOffice]] suite - to convert office documents to OpenDocument format\n*[[Tesseract (software)|Tesseract]] - to create plain text versions of file formats\n*[[ImageMagick]] - to convert a subset of image files to [[Portable Network Graphics|PNG]]\n*Readpst - to convert [[Microsoft Outlook]] PST files to XML. Readpst is part of the free and open source [http://www.five-ten-sg.com/libpst/ libpst software suite].\n*[[Free Lossless Audio Codec|FLAC]] - to convert audio files to FLAC format. This is also required to play back audio files using Xena.\n\n==Supported file types==\nXena will recognize and process the file types listed below, plus a few others of minor importance. Unsupported file types will automatically undergo binary normalization.\n\nOffice file formats:\n*[[Microsoft Office]] files (including [[Microsoft Office XML formats|MS Office XML]], [[SYLK]] spreadsheets and [[Rich Text Format]]) are converted to the corresponding OpenDocument files\n*[[Microsoft Outlook]] [[Personal Storage Table|PST]] files are parsed for their individual messages, which are converted to XML files and a Xena index file is created\n*[[Microsoft Project]] MPP files are converted to XML\n*[[OpenOffice.org XML]] files (SXC, SXI, SXW) are converted to the corresponding OpenDocument formats\n*[[WordPerfect]] WPD files are converted to OpenDocument ODT\n*[[OpenDocument]] documents (ODT, ODS, ODB, ODP) are preserved unchanged\n*Acrobat PDF files are stored as binaries\n*Mailbox files (MBX) are converted to individual XML files\n\nGraphics:\n*[[BMP file format|BMP]], [[Graphics Interchange Format|GIF]], [[Adobe Photoshop|PSD]], [[PCX]], [[.ras|RAS]], and the [[X Window System]] [[X BitMap|XBM]] and [[X PixMap|XPM]] bitmap files are converted to [[Portable Network Graphics|PNG]]; [[Tagged Image File Format|TIFF]] files additionally get embedded metadata stored in Xena XML. If the [[Tesseract (software)|Tesseract]] [[Optical character recognition|OCR software]] is installed, text will be extracted from TIFF files.\n*OpenDocument Drawings (ODG) and [[Scalable Vector Graphics|SVG]] files are wrapped in Xena XML\n*JPG and PNG files are stored unchanged\n\nArchive Files:\n*Files are extracted from [[File archiver|archives]] ([[ZIP (file format)|ZIP]], [[gzip|GZIP]], [[tar (file format)|TAR/TAR.gz]], [[JAR (file format)|JAR]], [[WAR (Sun file format)|WAR]], Mac binary) and normalised into a separate Xena file. A Xena index file is created, which when opened in the internal Xena viewer will display the files in a table.\n\nAudio files:\n*[[MP3]], [[WAV]], [[AIFF]], and [[Vorbis|OGG]] formats are converted to [[Free Lossless Audio Codec|FLAC]] files.\n\nDatabases:\n*[[SQL]] files are processed as plain text wrapped in XML\n\nOther file types:\n*HTML is converted to XHTML\n*TXT text files are stored as plain text wrapped in XML; CSS files are stored as plain text wrapped in XML\n\n==Reviews==\nAn April 22, 2010 review in Practical e-Records rated Xena at 82/100 points. At present Xena has no target preservation format for video files.<ref>{{cite web |url=http://e-records.chrisprom.com/review-of-xena-normalization-software/ |title=Review of XENA Normalization Software |date=2010-04-22 |accessdate= |archiveurl=http://archive.is/yKw1 |archivedate=2012-07-08}}</ref>\n\n==References==\n<references/>\n\n==External links==\n*[http://xena.sourceforge.net/ Xena on SourceForge]\n*[http://sourceforge.net/apps/mediawiki/xena/index.php?title=Main_Page Xena wiki on SourceForge]\n*[https://web.archive.org/web/20100610095405/http://www.ask-oss.mq.edu.au/index.php?option=com_content&task=view&id=66&Itemid=69 Xena project description at The Australian Service for Knowledge of Open Source Software]\n*[http://www.naa.gov.au/records-management/secure-and-store/e-preservation/at-naa/software.aspx#section1 National Archives of Australia - software]\n\n{{DEFAULTSORT:Xena (Software)}}\n[[Category:Digital preservation]]\n[[Category:Electronic documents]]\n[[Category:Free software programmed in Java (programming language)]]\n[[Category:Binary-to-text encoding formats]]\n[[Category:Mass digitization]]"]
['ViXra', '32834692', '{{lowercase title}}{{Infobox Website\n|name           = viXra\n|logo           = \n|screenshot     = \n|caption        = \n|url            = [http://viXra.org/ viXra.org]\n|alexa          = \n|commercial     = No\n|type           = Science\n|language       = English\n|registration   = \n|owner          = \n|author         = \n|launch date    = \n|current status = Online\n|revenue        = \n|slogan         =\n}}\n\n\'\'\'viXra\'\'\' is an electronic [[e-print]] archive set up by independent physicist Philip Gibbs as an alternative to the dominant [[arXiv]] service operated by [[Cornell University]].\n\n==Description==\nAlthough dominated by physics and mathematics submissions, viXra aims to cover topics across the whole scientific community. It accepts submissions without requiring authors to have an academic affiliation and without any threshold for quality.<ref>"[http://blogs.nature.com/news/2009/07/whats_arxiv_spelled_backwards.html What’s arXiv spelled backwards? A new place to publish]". \'\'Nature News Blog\'\'. 16 July 2009.</ref> The e-prints on viXra are grouped into seven broad categories: physics, mathematics, computational science, biology, chemistry, humanities, and other areas.<ref name="vixra">{{cite web |url=http://vixra.org/ |title=ViXra.org open e-print archive |work=viXra.org | accessdate=22 August 2011}}</ref> Anyone may post anything on viXra, though house rules do prohibit “vulgar, libellous [sic], plagiaristic or dangerously misleading” content.<ref>http://nautil.us/issue/41/selection/what-counts-as-science</ref>\n\nGibbs\' original motivation for starting the archive was to cater for researchers who believed that their preprints had been unfairly rejected or reclassified by the arXiv moderators.<ref name="pw">{{cite journal |title=Fledgling site challenges arXiv server |work=[[Physics World]] |date=15 July 2009 |url= http://physicsworld.com/cws/article/news/39845}}</ref> As of 2013 it had already over 4000 preprints<ref>{{citation|title=A Good Year for viXra|first=Philip E.|last=Gibbs|journal=Prespacetime Journal|volume=4|issue=1|year=2013|pages=87–90|url=http://prespacetime.com/index.php/pst/article/view/482}}.</ref> and in October, 2016 the number had grown to 16,214.<ref>Official site (front page)</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://vixra.org/ Official website]\n\n[[Category:Eprint archives]]\n[[Category:Academic publishing]]\n[[Category:Electronic documents]]\n[[Category:Electronic publishing]]\n[[Category:Online archives]]\n\n\n{{website-stub}}']
['IMail', '30943549', '{{Multiple issues|\n{{refimprove|date=February 2011}}\n{{notability|date=February 2011}}\n{{more footnotes|date=February 2011}}\n{{COI|date=February 2011}}\n}}\n\n{{lowercase title}}\n\n\'\'\'Invisible mail\'\'\', also referred to as \'\'\'iMail\'\'\', \'\'\'i-mail\'\'\' or \'\'\'Bote mail\'\'\', is a method of exchanging [[Digital data|digital]] messages from an author to one or more recipients in a secure and untraceable way. It is an open protocol and its java implementation (I2P-Bote) is free and open source software, licensed under the GPLv3.<ref>http://stats.i2p/cgi-bin/viewmtn/revision/file/cf46b537180b1a5b5740a1e2e85fc049ccc512ef/license.txt</ref>\n\nAs with [[email]], one can send and receive iMails. However, normal [[email]]s are visible to an [[ISP]] and to the administrators of the mail servers providing the service. Https, or secure, connections still allow the server admin to view the content of an email and its related IP number. In invisible mails both the mail\'s content, and the identities (of the sender as well as the receiver) remain unknown to a third party observer or attacker. Furthermore, all iMails are automatically and transparently [[end-to-end principle|end-to-end]] encrypted.\n\nAt present, iMail cannot be sent to regular email accounts. iMail addresses are called iMail destinations. They are much longer than the average email addresses and do not carry the "@" sign nor a domain. They already include the encryption key, so using an iMail destination is not harder than using standard email with [[GNU Privacy Guard|gpg]] encryption. The destination is two in one: the "address" as well as the public key. In contrast to gpg- or pgp-encrypted emails, I2P-Bote also encrypts the mail headers.\n\nI2P-Bote also works as an anonymous or pseudonymous remailer. iMails are sent via the [[I2P]] network, a secure and pseudonymous p2p overlay network on the internet and sender and receiver need not be online at the same time ([[store-and-forward]] model). The entire system is serverless and fully distributed. iMail [[Peer-to-peer|peer]]s accept, forward, store and deliver messages. Neither the users nor their computers are required to be online simultaneously; they need connect only briefly for as long as it takes to send or receive messages.\n\nAn iMail message consists of three components, the message \'\'envelope\'\', the message \'\'header\'\', and the message \'\'body\'\'. The message header contains control information, including, minimally one or more recipient addresses. Usually descriptive information is also added, such as a subject header field and a message submission date/time stamp.\n\niMails can carry international typesets and have small  multi-media content attachments, a process standardized in [[Request for Comments|RFC]] 2045 through 2049. Collectively, these RFCs have come to be called [[Multipurpose Internet Mail Extensions]] (MIME).\n\n==Features==\n* \'\'secure messages\'\': All iMail messages are automatically end-to-end encrypted from the sender to the receiver.\n* \'\'message authentication\'\': All iMail messages that are not sent without any information on the originator are automatically signed and the message\'s integrity and authenticity is checked by the receiver.\n* \'\'anonymous messages\'\': iMails can also be sent without any information about the originator.\n<ref>http://stats.i2p/cgi-bin/viewmtn/revision/file/cf46b537180b1a5b5740a1e2e85fc049ccc512ef/doc/techdoc.txt</ref>\n\n===Attachment size limitations===\n{{Main|Email attachment}}\niMail messages may have one or more attachments. Attachments serve the purpose of delivering binary or text files of unspecified size. In principle there is no technical intrinsic restriction in the I2P-Bote protocol limiting the size or number of attachments. In practice, however, the slow speeds, overheads and data volume due to redundancy limit the viable size of files or the size of an entire message.\n\n===Email spoofing===\n{{Main|Email spoofing}}\n[[Email spoofing]] occurs when the header information of an email is altered to make the message appear to come from a known or trusted source. In the case of iMails, this is countered by [[cryptography|cryptographically]] signing each iMail with its originator\'s key.\n\n===Tracking of sent mails===\nThe I2P-Bote mail service provides no mechanisms for tracking a transmitted message, but a means to verify that it has been delivered, which however does not necessarily mean it has been read.\n\n===Drawbacks===\niMails can only be received or sent via the web interface, there is no implementation of POP3 or SMTP for iMail yet. Furthermore, there are no bridges that allow for sending from I2P-Bote to a standard internet email account or vice versa.\n\n==See also==\n\n===Related services===\n* [[Email]]\n* [[I2P]]\n* [[Data security]]\n* [[Email encryption]]\n* [[Email client]], [[Comparison of email clients]]\n* [[Email hosting service]]\n* [[Internet mail standard]]s\n* [[Mail transfer agent]]\n* [[Mail user agent]]\n* [[Unicode and email]]\n* [[Webmail]]\n* [[Anonymous remailer]]\n* [[Disposable email address]]\n* [[Email encryption]]\n* [[Email tracking]]\n* [[Electronic mailing list]]\n* [[Mailing list archive]]\n\n===Protocols===\n* [[IMAP]]\n* [[POP3]]\n* [[SMTP]]\n* [[UUCP]]\n* [[X400]]\n\n==References==\n{{reflist}}\n* http://i2pbote.i2p/src.zip  (source code)\n* http://i2pbote.i2p/history.txt (history.txt)\n* http://awxcnx.de/handbuch_55.htm (German Privacy Foundation)\n* http://www.unitethecows.com/other-p2p-clients/48940-i2pbote-0-1-2-released.html\n\n==External links==\n{{Wiktionary|iMail|email|outbox}}\n* http://i2pbote.i2p (I2P-internal)\n* http://www.i2p2.de\n<!-- please see http://en.wikipedia.org/wiki/WP:EL before adding links -->\n\n{{Computer-mediated communication}}\n{{Email clients}}\n\n[[Category:Email]]\n[[Category:Internet terminology]]\n[[Category:American inventions]]\n[[Category:Electronic documents]]']
['Zathura (document viewer)', '46505081', '{{notability|Products|date=April 2015}}\n{{Infobox software\n| name                   = Zathura\n| screenshot             = Zathura Screenshot.png\n| caption = Screenshot of zathura viewing a PDF file in Arch Linux.\n| author                 = Moritz Lipp, Sebastian Ramacher\n| developer              = pwmt<ref>{{cite web|url=https://pwmt.org |title=Programs With Movie Titles}}</ref>\n| released               = {{Start date|2009|09|18}}\n| latest release version = 0.3.6\n| latest release date    = {{Release date|2016|04|18}}<ref>{{cite web |url=https://pwmt.org/news/zathura-0-3-6/ |title=ZATHURA 0.3.6 |website=pwmt.org |date=2016-04-18 |accessdate=2016-08-28}}</ref>\n| programming language   = [[C (programming language)|C]]\n| operating system       = [[Unix-like]]\n| status                 = Active\n| genre                  = Document viewer\n| license                = [[Free software license|Free software]]\n| website                = {{URL|pwmt.org/projects/zathura}}\n}}\n\n\'\'\'Zathura\'\'\' is a [[Free software|free]], [[Plug-in (computing)|plugin-based]] [[document viewer]]. Plugins are available for [[Portable Document Format|PDF]] (via [[Poppler_(software)|poppler]] or [[MuPDF]]), [[PostScript]], [[DjVu]], and [[EPUB]]. It was written to be lightweight and controlled with [[Vim (text editor)|vim]]-like keybindings. Zathura\'s customizability makes it well-liked by many Linux users.<ref>{{cite web|url=http://www.maketecheasier.com/8-alternative-pdf-readers-for-your-consideration/|title=MakeTechEasier list of alternative PDF viewers|access-date=24 April 2015}}</ref>\n\nZathura has a mature, well-established codebase and a large development team.<ref>{{cite web|url=https://www.openhub.net/p/zathura-pdf-viewer|title=OpenHUB analysis of Zathura PDF Viewer|access-date=24 April 2015}}</ref> It has official packages available in [[Arch linux]],<ref>{{cite web|url=https://www.archlinux.org/packages/community/x86_64/zathura/|title=Arch Linux zathura package}}</ref>\n[[Debian]],<ref>{{cite web|url=https://packages.debian.org/en/sid/zathura|title=Debian zathura package}}</ref>\n[[Fedora (operating system)|Fedora]],<ref>{{cite web|url=http://pkgs.org/altlinux-sisyphus/classic-x86_64/zathura-devel-0.3.3-alt1.x86_64.rpm.html|title=Fedora zathura package}}</ref>\n[[Gentoo linux|Gentoo]],<ref>{{cite web|url=https://packages.gentoo.org/package/app-text/zathura|title=Gentoo zathura package}}</ref>\n[[Ubuntu (operating system)|Ubuntu]],<ref>{{cite web|url=http://packages.ubuntu.com/precise/zathura|title=Ubuntu zathura package}}</ref>\n[[Source Mage GNU/Linux]],<ref>{{cite web|url=http://download.sourcemage.org/codex/test/doc/zathura/|title=Source Mage zathura package}}</ref>\n[[OpenBSD]],<ref>{{cite web|url=http://openports.se/textproc/zathura|title=OpenBSD zathura package}}</ref>\nand [[Mac OS X]].<ref>{{cite web|url=https://www.macports.org/ports.php?by=name&substr=zathura|title=MacPorts zathura package}}</ref>\n\nZathura was named after the [[Zathura (film)|film]] of the same name.<ref>https://git.pwmt.org/groups/pwmt</ref>\n\n== History ==\n\nDevelopment on Zathura began on 12 August 2009.<ref>{{cite web|url=https://github.com/pwmt/zathura/commit/0eeb457bea2f93983e556d07028c2cfdb49b898c|title=Zathura initial commit}}</ref> On 18 September 2009, version 0.0.1 was announced to the Arch Linux community.<ref>{{cite web|url=https://bbs.archlinux.org/viewtopic.php?id=80458|title=zathura - a document viewer}}</ref>\n\nZathura has been an official Arch Linux package since April 2010.<ref>{{cite web|url=https://projects.archlinux.org/svntogit/community.git/log/trunk?h=packages/zathura|title=Arch Linux package history for Zathura}}</ref> Same year, by the end of July it was imported into Source Mage test grimoire.<ref>{{cite web|url=http://scmweb.sourcemage.org/?p=smgl/grimoire.git;a=commit;h=c0963f5c0a65a0536d21a03a528ffaff4245cce7|title=zathura package in Source Mage}}</ref> It has been an official Debian package since at least 2011, as part of Debian Squeeze.<ref>{{cite web|url=https://packages.debian.org/squeeze/zathura|title=Debian Squeeze package for Zathura}}</ref>\n\n== Features ==\n\nZathura automatically reloads documents. When working in compiled documents such as those written in [[LaTeX]], Zathura will refresh the output whenever compilation takes place. Zathura has the option of enabling [[inverse search]] (using "synctex").<ref>{{cite web|url=https://wiki.math.cmu.edu/iki/wiki/tips/20140310-zathura-fsearch.html|title=LaTeX forward/inverse searches with Zathura}}</ref><ref>{{cite web|url=https://gist.github.com/vext01/16df5bd48019d451e078|title=Vim+Zathura+Synctex}}</ref>\n\nZathura can adjust the document to best-fit or to fit width, and it can rotate pages. It can view pages side-by-side and has a fullscreen mode. Pages can also be recolored to have a black background and white foreground.\n\nZathura can search for text and copy text to the [[X Window selection|primary X selection]]. It supports bookmarks and can open encrypted files.\n\nThe behavior and appearance of Zathura can be customised using a [[configuration file]]. Zathura has the ability to execute external [[List of command-line interpreters#Unix-like systems|shell]] commands. It can be opened in tabs using {{URL|http://tools.suckless.org/tabbed/|tabbed}}.<ref>{{cite web|url=http://taitran.ca/vim/latex/markdown/2015/03/11/vim-latex-and-markdown-preview-scripts.html|title=Vim, Latex and Markdown preview scripts}}</ref>\n\n== See also ==\n{{Portal|Free software}}\n\n* [[List of PDF software]]\n* [[Zathura (film)]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* {{official website|http://pwmt.org/projects/zathura/}}\n* {{URL|https://wiki.archlinux.org/index.php/List_of_applications/Documents#Graphical_2|Arch Linux list of document viewers}}\n\n{{PDF readers}}\n\n[[Category:Free PDF readers]]\n[[Category:PostScript]]\n[[Category:Free software programmed in C]]\n[[Category:Office software that uses GTK+]]\n[[Category:Electronic documents]]']
['Data paper', '42801446', '#REDIRECT [[Data publishing#Paper]]\n\n\n[[Category:Data publishing]]\n[[Category:Electronic documents]]']
['Social Sciences Citation Index', '6853403', "{{ infobox bibliographic database\n| title = Science Citation Index\n| image = \n| caption = \n| producer = [[Thomson Reuters]]\n| country = United States\n| history = \n| languages = \n| providers = \n| cost = \n| disciplines = Social sciences\n| depth = Index & citation indexing \n| formats = \n| temporal = \n| geospatial = \n| number = \n| updates = \n| p_title = \n| p_dates = \n| ISSN = \n| web = http://thomsonreuters.com/en/products-services/scholarly-scientific-research/scholarly-search-and-discovery/social-sciences-citation-index.html\n| titles = http://ip-science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=SS\n}}\nThe '''Social Sciences Citation Index''' ('''SSCI''') is a commercial [[citation index]] product of [[Thomson Reuters]]' Healthcare & Science division. It was developed by the [[Institute for Scientific Information]] from the [[Science Citation Index]].\n\n==Overview==\nThe SSCI citation database covers some 3,000 of the world's leading [[academic journals]] in the [[social sciences]] across more than 50 [[academic discipline|disciplines]].<ref>{{cite web |title=Social Sciences Citation Index |url=http://scientific.thomson.com/products/ssci/ |accessdate=2008-06-11}}</ref> It is made available online through the [[Web of Science]] service for a fee. The database records which articles are cited by other articles.\n\n==Criticism==\n[[Philip Altbach]] has criticised the Social Sciences Citation Index of favouring English-language journals generally and American journals specifically, while greatly underrepresenting journals in non-English languages.<ref>{{cite book |last= Altbach|first=Philip |authorlink= Philip Altbach |year=2005 |article=Academic Challenges: The American Professoriate in Comparative Perspective |title=The Professoriate: Profile of a Profession |url= |location=Dortrecht |publisher=Springer |pages=147–165 |isbn= |author-link= }}</ref>\n\nIn 2004, economists [[Daniel B. Klein]] and Eric Chiang conducted a survey of the Social Sciences Citation Index and identified a bias against free market oriented research. In addition to an ideological bias, Klein and Chiang also identified several methodological deficiencies that encouraged the over-counting of citations, and they argue that the Social Sciences Citation Index does a poor job reflecting the relevance and accuracy of articles.<ref>Daniel Klein and Eric Chiang. [http://econjwatch.org/articles/the-social-science-citation-index-a-black-box-with-an-ideological-bias The Social Science Citation Index: A Black Box—with an Ideological Bias?] ''Econ Journal Watch'', Volume 1, Number 1, April 2004, pp 134–165.</ref>\n\n==See also==\n* [[Arts and Humanities Citation Index]]\n* [[Science Citation Index]]\n\n==References==\n{{reflist}}\n\n==External links==\n*{{Official website|http://ip-science.thomsonreuters.com/mjl/publist_ssci.pdf}}\n*[http://thomsonreuters.com/products_services/science/science_products/a-z/social_sciences_citation_index Introduction to SSCI]\n\n{{Thomson Reuters}}\n\n[[Category:Thomson Reuters]]\n[[Category:Social sciences literature]]\n[[Category:Citation indices]]\n[[Category:Social science journals| ]]\n\n{{database-stub}}\n{{sci-stub}}"]
['Islamic World Science Citation Database', '24783829', "'''Islamic World Science Citation Database''' ('''ISC''') is a [[citation index]] established by the Iranian [[Ministry of Science, Research and Technology]] after it was approved by the [[Organisation of the Islamic Conference]].  It only indexes journals from the [[Islamic world]].\n\nIt was announced in [[Baku]], Azerbaijan during the Fourth Islamic Conference of the Ministers of Higher Education and Scientific Research held in October 2008.<ref>{{cite news | url = http://www.scidev.net/en/science-communication/science-publishing/news/islamic-countries-to-get-own-science-citation-inde.html | title = Islamic countries to get own science citation index | author = Wagdy Sawahel | date = 17 October 2008 | publisher = [[SciDev.Net]] }}</ref>  It is managed by the Islamic World Science Citation Center, located in [[Shiraz]].\n\nIn 2009, ISC partnered with [[Scopus]] that allows ISC's publications to be indexed in Scopus.<ref>{{cite journal | journal = [[Library Connect]] | title = The Islamic World Science Citation Database partnership with Scopus brings greater visibility to Islamic researchers | url = http://libraryconnect.elsevier.com/lcn/0703/lcn070319.html | author = Ahmed Rostom | volume = 7 | issue = 3 | date = August 2009 | issn = 1549-3725 }}</ref>\n\n== References ==\n{{Reflist}}\n\n==See also==\n* [[Academic publishing]]\n* [[List of academic databases and search engines]]\n* [[Impact factor]]\n\n== External links ==\n* {{Official website|http://isc.gov.ir/Default.aspx?lan=en}}\n\n[[Category:Bibliographic databases and indexes]]\n[[Category:Online databases]]\n[[Category:Citation indices]]\n[[Category:Research management]]\n[[Category:Databases in Iran]]\n[[Category:Science and technology in Iran]]\n\n\n{{science-journal-stub}}\n{{islam-stub}}\n{{iran-stub}}"]
['Conference Proceedings Citation Index', '47141591', "{{Incomplete|date=July 2015}}\n{{infobox bibliographic database\n| title = Conference Proceedings Citation Index\n| image = \n| caption = \n| producer = [[Thomson Reuters]]\n| country = United States\n| history = \n| languages = \n| providers = \n| cost = \n| disciplines = \n| depth = \n| formats = \n| temporal = \n| geospatial = \n| number = \n| updates = \n| p_title = \n| p_dates = \n| ISSN = \n| web = http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/\n| titles = \n}}\nThe '''Conference Proceedings Citation Index''' ('''CPCI''') is a [[citation index]] produced by [[Thomson Reuters]] covering [[conference proceedings]].<ref>{{cite web|url=http://wokinfo.com/media/pdf/proceedingswhtpaper.pdf |format=PDF |title=White Paper : Conference Proceddings and Their Impact on Global Research |publisher=Wokinfo.com |accessdate=2015-07-09}}</ref><ref>{{cite web|url=http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/cpciessay/|title=CPCI Essay - IP & Science - Thomson Reuters|author=Thomson Reuters|publisher=Wokinfo.com|accessdate=2015-07-09}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{official website|http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/}}\n\n{{Thomson Reuters}}\n\n[[Category:Citation indices]]\n[[Category:Online databases]]\n[[Category:Thomson Reuters]]\n[[Category:Conference proceedings]]"]
['Emerging Sources Citation Index', '49394017', '{{incomplete|date=January 2014}}\n{{ infobox bibliographic database\n| title = Emerging Sources Citation Index\n| image = \n| caption = \n| producer = [[Thomson Reuters]]\n| country = United States\n| history = 2015-present\n| languages = \n| providers = \n| cost = \n| disciplines = Multidisciplinary\n| depth = \n| formats = \n| temporal = \n| geospatial = Worldwide\n| number =\n| updates = \n| p_title = \n| p_dates = \n| ISSN = \n| web = http://wokinfo.com/products_tools/multidisciplinary/esci/\n| titles = \n}}\nThe \'\'\'Emerging Sources Citation Index\'\'\' is a [[citation index]] produced since 2015 by [[Thomson Reuters]], and now by [[Clarivate Analytics]]. It is accessible through the \'\'[[Web of Science]]\'\'.<ref name=AtoZ>{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2015 |url=http://ip-science.thomsonreuters.com/mjl/}}</ref> The index includes [[academic journal]]s "of regional importance and in emerging scientific fields".<ref>{{cite web |title=ESCI Fact Sheet |url=http://wokinfo.com/media/pdf/ESCI_Fact_Sheet.pdf?utm_source=false&utm_medium=false&utm_campaign=false |publisher=Thomson Reuters |accessdate=28 April 2016 |format=PDF}}</ref> [[Jeffrey Beall]] has stated that, among the databases produced by Thomson Reuters, the Emerging Sources Citation Index is the easiest one to get into and as a result it contains many [[Predatory open access publishing|predatory journals]].<ref name=Beall>{{cite web |last1=Beall |first1=Jeffrey |authorlink1=Jeffrey Beall |title=The TR Master Journal List is not a Journal Whitelist |url=https://scholarlyoa.com/2016/04/28/the-tr-master-journal-list-is-not-a-journal-whitelist/ |website=Scholarly Open Access |publisher=WordPress.com |accessdate=28 April 2016 |date=28 April 2016}}</ref>\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{Official website|http://wokinfo.com/products_tools/multidisciplinary/esci/}}\n{{Thomson Reuters}}\n\n[[Category:Citation indices]]\n[[Category:Online databases]]\n[[Category:Thomson Reuters]]']
['World Radio TV Handbook', '1536556', '{{redirect|WRTH|the radio station|WRTH (FM)}}\nThe \'\'\'\'\'World Radio TV Handbook\'\'\'\'\', also known as \'\'\'\'\'WRTH\'\'\'\'\', is a [[directory (databases)|directory]] of virtually every [[Radio station|radio]] and [[TV station]] on Earth, published yearly. It was started in 1947 by [[Oluf Lund Johansen]] (1891–1975) as the \'\'World Radio Handbook\'\' (WRH).<ref>[http://oz6gh.byethost33.com/lund_johansen.htm O. Lund-Johansen], presented by OZ6GH.</ref> The word "TV" was added to the title in 1965, when [[Jens M. Frost]] (1919–1999) took over as editor.<ref>[http://www.dswci.org/specials/membersofhonour/jens_frost.html DSWCI Member of Honour:  Jens M. Frost]</ref> It had then already included data for [[television broadcasting]] for some years. After the 40th edition in 1986, Frost handed over editorship to [[Andrew G. Sennitt|Andrew G. (Andy) Sennitt]].<ref>[http://www.agsmedia.nl/body_who.html Andy Sennitt], own presentation.</ref>\n\nThe first edition that bears an edition number is the 4th edition, published in 1949. The three previous editions appear to have been:\n* the 1st edition, marked "Winter Ed. 1947" on the cover and completed in November 1947\n* the 2nd edition, marked "1948 (May-November)" on the cover and completed in May 1948\n* the 3rd edition, marked "1948-49" on the cover and completed in November 1948.\n\nSummer Supplements appear to have been issued from 1959 through 1971. From 1959 through 1966 they were called the Summer Supplement. From 1967 through 1971 they were called the Summer Edition.\n\nThrough the 1969 edition, the WRTH indicated the date on which the manuscript was completed.\n\nIssues with covers in Danish are known to have been available for the years 1948 May-November (2d ed.), 1950-51 (5th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), 1952 (6th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), and probably others. The 1952 English ed., which is completely in English, has an extra page with world times and agents, and ads in English which are sometimes different from the ads in the Danish edition. Also, the 1953 ed. mentions the availability of a German edition.\n\n[[Oluf Lund Johansen]] published, in conjunction with [[Libreria Hispanoamericana]] of [[Barcelona]], Spain, a [[softbound]] Spanish-language version of the 1960 WRTH. The book was printed in Spain and called \'\'Guia Mundial de Radio y Television\'\', and carried the WRTH logo at the time as well as all the editorial references contained in the English-language version. \n\nHardbound editions are known to have been available for the years 1963 through 1966, 1968, 1969, and 1975-1978, and probably others.\n\n== Publications ==\n* Gilbert, Sean; Nelson, John; Jacobs, George, [https://books.google.com/books?id=IBu8NHvC4fMC&printsec=frontcover \'\'World Radio TV Handbook 2007\'\'], Watson-Guptill, 2006. ISBN 0-9535864-9-9.\n\n==References==\n{{reflist}}\n\n== External links ==\n* http://www.wrth.com/\n\n[[Category:Radio organizations]]\n[[Category:Television organizations]]\n[[Category:International broadcasting]]\n[[Category:Directories]]\n[[Category:1945 introductions]]']
['The Milepost', '240436', '{{italic title}}\n[[Image:49MilePost.gif|right|thumb|The original 1949 Milepost]]\'\'\'\'\'The Milepost\'\'\'\'\' is an extensive [[guide book]] covering [[Alaska]], the [[Yukon]], the [[Northwest Territories]], and [[British Columbia]].  It was first published in 1949 as a guide about traveling along the [[Alaska Highway]], often locally referred to as "The ALCAN".<ref name="morris">[http://morris.com/divisions/mcc_magazines/the_milepost.shtml \'\'The MILEPOST\'\'] from the website of  [[Morris Communications]]</ref>  It has since expanded to cover all major highways in the northwest corner of [[North America]], including the [[Alaska Marine Highway]].  It is updated annually.\n\n==History==\n<!-- Deleted image removed: [[Image:Milepost2008cover.jpg|right|thumb|The 2008 edition<br />{{deletable image-caption|Sunday, 10 February 2013}}]] -->\'\'The Milepost\'\' is packaged and distributed like a [[book]]  (2008 edition: ISBN 978-189215431-6), but like the [[Yellow Pages]] it includes paid [[advertising]].<ref>[http://www.themilepost.com/media_kit/testimonials.shtml Testimonials from Advertisers from \'\'The MILEPOST\'\' website]</ref> The original 1949 edition was a mere 72 pages, by 2014 it had expanded to 752 pages, detailing every place a traveler might eat, sleep, or just pull off the road for a moment on all of the highways of northwestern North America. In addition to the paid ads, descriptions are provided of interesting hikes or side trip drives near the highways, campgrounds and other public facilities, as well as short histories of most of the settlements on the highways. Newer additions include special sections on selected areas popular with tourists, such as the [[Kenai Peninsula]]. It is also exhaustively cross-indexed and maps and charts are provided so that travelers can determine the total driving distance between any two points covered by the guide.<ref>http://milepost.com/index.php?option=com_content&task=view&id=71&Itemid=62</ref>\n\n==Publishing==\nSince 1997 \'\'The Milepost\'\' has been published by [[Morris Communications]] and currently shares publishing offices with [[Alaska magazine|\'\'Alaska\'\' magazine]].<ref name="morris" /> Beginning in 2009, The Milepost is also available in an interactive digital format or download.<ref>[http://milepost.com/images/media_kit/mp_mediakit_09_email_lr.pdf The Milepost media kit]</ref>\n\n==References==\n<references />\n\n==External links==\n* {{Official website|http://www.themilepost.com}}\n\n{{Morris Communications}}\n\n{{DEFAULTSORT:Milepost, The}}\n[[Category:1949 establishments in Alaska]]\n[[Category:1949 books]]\n[[Category:Books about Alaska]]\n[[Category:Directories]]\n[[Category:Morris Communications]]\n[[Category:Publications established in 1949]]\n[[Category:Roads in Alaska]]\n[[Category:Travel guide books]]']
['Gallia Christiana', '13968535', 'The \'\'\'\'\'Gallia Christiana\'\'\'\'\', a type of work of which there have been several editions, is a documentary catalogue or list, with brief historical notices, of all the Catholic dioceses and abbeys of France from the earliest times, also of their occupants.\n\n== First efforts ==\n\nIn 1621 [[Jean Chenu]], an \'\'[[avocat]]\'\' at the [[Parlement of Paris]], published \'\'Archiepiscoporum et episcoporum Galliæ chronologica historia\'\'. Nearly a third of the bishops are missing, and the episcopal succession as given by Chenu was very incomplete. In 1626, Claude Robert, a priest of [[Langres]], published with the approbation of [[Baronius]], a \'\'Gallia Christiana\'\'. He entered a large number of churches outside of [[Gaul]], and gave a short history of the [[metropolitan see]]s, cathedrals, and abbeys.\n\n== The Samarthani ==\n\nTwo brothers de Sainte-Marthe, Scévole (1571–1650) and Louis (1571–1656), appointed royal historiographers of France in 1620, had assisted Chenu and Robert. At the [[assembly of the French Clergy]] in 1626, a number of prelates commissioned these brothers to compile a more definitive work. They died before the completion of their work, and it was issued in 1656 by the sons of [[Scévole de Sainte-Marthe]], [[Pierre de Sainte-Marthe]] (1618–90), himself historiographer of France, [[Abel de Sainte-Marthe]] (1620–71), theologian, and later general of the [[Oratory (worship)|Oratory]], and [[Nicolas-Charles de Sainte-Marthe]] (1623–62), prior of [[Claunay]]. On 13 September 1656, the Sainte-Marthe brothers were presented to the assembly of the French Clergy, who accepted the dedication of the work on condition that a passage suspected of [[Jansenism]] be suppressed. The work formed four volumes [[in folio]], the first for the [[archdiocese]]s, the second and third for the dioceses, and the fourth for the abbeys, all in alphabetical order.<ref>The title was \'\'Gallia Christiana, qua series omnia archiepiscoporum, episcoporum et abbatum Franciæ vicinarumque ditionum ab origine ecclesiarum ad nostra tempora per quattor tomos deducitur, et probator ex antiquæ fidei manuscriptis Vaticani, regnum, principum tabulariis omnium Galliæ cathedralium et abbatarium\'\'.</ref> It reproduced a large number of manuscripts. Defects and omissions, however, were obvious. The Sainte-Marthe brothers themselves announced in their preface the early appearance of a second edition corrected and enlarged. \n\nAs early as 1660 the Jesuit [[Jean Colomb]] published at Lyons the \'\'Noctes Blancalandanæ\'\', which contains certain additions to the work of the Samarthani, as the brothers and their successors are often called. "The name of Sainte-Marthe", wrote Voltaire, "is one of those of which the country has most reason to be proud." The edition promised by the Sainte-Marthe brothers did not appear.\n\n== Revision by the Maurists ==\n\nIn 1710 the Assembly of the French Clergy offered four thousand livres to [[Denys de Sainte-Marthe]] (1650–1725), a Benedictine monk of the [[Congregation of Saint-Maur]], renowned for his polemics against the Trappist [[Abbé de Rancé]] on the subject of monastic studies, on condition that he should bring the revision of the \'\'Gallia Christiana\'\' to a successful conclusion, that the first volume should appear at the end of four years, and that his Congregation should continue the undertaking after his death. Through his efforts the first volume appeared in 1715, devoted to the ecclesiastical provinces of Albi, Aix, Arles, Avignon, and Auch. In 1720 he produced the second volume dealing with the provinces of Bourges and Bordeaux; and in 1725 the third, which treated Cambrai, Cologne, and Embrun. \n\nAfter his death the Benedictines issued the fourth volume (1728) on Lyons, and the fifth volume (1731) on Mechelen and Mainz. Between 1731 and 1740, on account of the controversies over the Bull \'\'[[Unigenitus]]\'\', Dom [[Félix Hodin]] and Dom [[Etienne Brice]], who were preparing the latter volumes of the \'\'Gallia Christiana\'\', were expelled from [[Saint-Germain-des-Prés]]. They returned to Paris in 1739 and issued the sixth volume, dealing with Narbonne, also (1744) the seventh and eighth volumes on Paris and its [[suffragan see]]s. [[Père Duplessis]] united his efforts with theirs, and the ninth and tenth volumes, both on the [[province of Reims]], appeared in 1751. The eleventh volume (1759) dealing with the [[province of Rouen]] was issued by Père [[Pierre Henri]] and Dom [[Jacques Taschereau]]. In 1770 the twelfth volume on the [[province of Sens]] and [[province of Tarentaise]] appeared, and in 1785 the thirteenth, on the provinces of Toulouse and Trier. \n\nAt the outbreak of the revolution, four volumes were lacking: Tours, Besançon, Utrecht, and Vienne. Barthélemy Hauréau published (in 1856, 1860 and 1865), for the provinces of Tours, Besançon and Vienne, respectively, and according to the Benedictine method, the fourteenth, fifteenth and sixteenth volumes of the \'\'Gallia Christiana\'\'. \n\nThe province of Utrecht alone has no place in this great collection, but this defect has been remedied in part by the \'\'Bullarium Trajectense\'\', edited by [[Gisbert Brom]], and extending from the earliest times to 1378 (The Hague, 1891–96). \n\nThe new \'\'Gallia Christiana\'\', of which volumes I to V and XI to XIII were reprinted by Dom [[Paul Piolin]] between 1870 and 1877, and volumes VI to IX and XII by the publisher H. Welter, places after each metropolitan see its suffragan sees, and after each see the abbeys belonging to it. The original documents, instead of encumbering the body of the articles, are inserted at the end of each diocese under in a section titled \'\'Instrumenta\'\'. This colossal work does great honour to the Benedictines and to the Sainte-Marthe family. "The name of Sainte-Marthe", wrote Voltaire, "is one of those of which the country has most reason to be proud."\n\n== Later works ==\n\nIn 1774 the Abbé [[Hugues du Temps]], vicar-general of Bordeaux, undertook in seven volumes an abridgement of the \'\'Gallia\'\' under the title "Le clergé de France" of which only four volumes appeared. About 1867 [[Honoré Fisquet]] undertook the publication of an episcopal history of France ([http://gallica.bnf.fr/Catalogue/noticesInd/FRBNF34044240.htm]\'\'La France Pontificale\'\'), in which, for the early period, he utilized the \'\'Gallia\'\', at the same time bringing the history of each diocese down to modern times. Twenty-two volumes appeared. \n\n[[Canon Albanès]] projected a complete revision of the \'\'Gallia Christiana\'\', each ecclesiastical province to form a volume. Albanès, who was one of the first scholars to search the Lateran and Vatican libraries, in his efforts to determine the initial years of some episcopal reigns, found occasionally either the acts of election or the Bulls of provision. He hoped in this way to remove certain suppositious bishops who had been introduced to fill gaps in the catalogues, but died in 1897 before the first volume appeared. Through the use of his notes and the efforts of Canon [[Ulysse Chevalier]] three addition volumes of this "Gallia Christiana (novissima)", treating Arles, Aix, and Marseilles, appeared at Montbéliard.\n\n== See also ==\n* [[Jean-Barthélemy Hauréau]]\n\n== References ==\n\n<references/>\n* [[Dreux du Radier]], \'\'Bibliothèque historique et critique du Poitou\'\' (Paris, 1754)\n* \'\'Gallia Christiana\'\', Vol. IV, Préface\n* \'\'Gallia Christiana (novissima)\'\' (Montbéliard, 1899), Préface to the Aix volume\n* [[de Longuemare]], \'\'Une famille d\'auteurs aux seizième, dix-septième et dix-huitième siècles; les Sainte-Marthe\'\' (Paris, 1902)\n* Victor Fouque, \'\'Du "Gallia christiana" et de ses auteurs: étude bibliographique\'\', Paris: E. Tross, 1857. Available on the Bibliothèque nationale\'s [http://gallica.bnf.fr/Catalogue/noticesInd/FRBNF30453708.htm \'\'Gallica\'\'] site.\n\n== External links ==\n* {{CathEncy|url=http://www.newadvent.org/cathen/06350c.htm|title=Gallia Christiana}}\n\n{{Catholic|wstitle=Gallia Christiana}}\n\n[[Category:Directories]]\n[[Category:Religious studies books]]']
["Thacker's Indian Directory", '7719940', "{{italic title}}\n{{Refimprove|date=September 2014}}\n'''''Thacker, Spink & Co.''''' was a well-known [[Kolkata]] publishing company. ''Thacker's Bengal Directory'' was published from 1864 to 1884 and covered the [[Bengal Presidency]] – which included the present day [[Myanmar]] and [[Bangladesh]]. From 1885 the ''Directory'' covered the whole of [[British India]] and was renamed '''''Thacker's Indian Directory'''''.  It was later owned by [[Kameshwar Singh|Maharaja of Darbhanga]].<ref>{{cite book|title=Appendices|date=1982|publisher=India. Second Press Commissior Controller of Publications|pages=266, 343|url=https://books.google.com/books?id=tBwuAAAAMAAJ&q=darbhanga+Thacker+Spink&dq=darbhanga+Thacker+Spink&hl=en&sa=X&ei=Ul0sU5_BH46zrgfjsoCYBw&ved=0CEkQ6AEwBQ}}</ref>   It continued to be published until 1960.\n\nThe directory was essentially an [[almanac]] which listed British and Foreign Merchants and Manufacturers, Commercial Industries, Army, railway and government departments and office holders, European residents, and separately, prominent non-European residents.   Earlier editions of ''Thacker'' had street directories of major cities, such as Kolkata and [[Yangon]], together with the name of the residents of each house.\n\nSimilar directories published included:\n*''Thacker's Bombay Directory'', city and island (together with a directory of the chief industries of Bombay, etc.);\n*Thacker's medical directory of India, Burma and Ceylon;\n*''Thacker's Directory of the Chief Industries of India, Burma and Ceylon''.\n\n== References ==\n<references />\n\n[[Category:Almanacs]]\n[[Category:Directories]]\n[[Category:Books about British India]]\n[[Category:Bengal Presidency]]"]
['Ves Peterburg', '11002640', '{{italic title}}\n\'\'\'\'\'Ves Peterburg\'\'\'\'\' (/vʲesʲ pʲɪtʲɪrˈburg/, Literally translated "\'\'All Petersburg\'\'" or "\'\'The Entire Saint Petersburg\'\'"\n") (Full name in [[cyrillic]] "Ves Petersburg; Adresnaja i spravočnaja kniga g. Petersburga") (often referred to as the \'\'Suvorin directories\'\' from the publisher\'s name) was the title of a series of [[city directory|city directories]] of [[Saint Petersburg]], [[Russia]] published on a yearly basis from 1894 to 1940 by [[Aleksei Sergeevich Suvorin]]. Each volume was anywhere between 500 and 1500 pages long. After changes in the name of the city the directories were called \'\'\'\'\'Ves Petrograd\'\'\'\'\' from 1914 to 1923 and \'\'\'\'\'Ves Leningrad\'\'\'\'\' from 1924 to 1940.\n\nThe directories contained detailed lists of private residents, names of streets and squares across the city with the details of their occupants and owners, government offices, public services and medium and large businesses present in the city.  They are often used by [[genealogists]] for family research in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historian]]s use them to research the [[social histories]] of the city.\n\n== List of residents of St. Petersburg ==\n\nEach directory was written exclusively in Russian Cyrillic only, and contains various sections among which was an alphabetical list of residents in the city. Those listed usually were the head of their respective household and so spouses and minors are not listed.\n\nThe following information can be found:\n*Person\'s surname and first name\n*[[Patronymic]]\n*Street address with apartment number\n*Profession\n*Telephone numbers (only appear sparingly as few private residents could afford a telephone before 1918)\n\n== List of occupants of each building on every street and square ==\n\nA section immediately preceding or following that listing residents in alphabetical order was a directory of all streets, houses and flats with the names of their owners and occupants. In this way readers could determine all those people who lived on a particular street of in a certain apartment block.\n\n== Other sections ==\n\nThe following information can also be found in each directory\n\n*information on the royal family\n*Maps of the city\n*cultural establishments (with interior theatre hall layouts and seating plans)\n*Lists of personnel in state, public and private institutions\n*information on academic institutions of all ranks\n*information on churches and monasteries of St. Petersburg\n*Original commercial advertisements of Russian and foreign companies which had offices in St. Petersburg\n\n== Historical and genealogical value ==\n\nBecause numerous residents emigrated from Saint Petersburg after the [[Russian Revolution of 1917]] and tens of thousands more were either arrested, shot, or sent to the [[gulag]] by the [[Cheka]] and the [[NKVD]] after 1918 the section detailing residents names is especially useful in determining until when a certain person was still living in the city.\n\n== Interruption in the series ==\n\nNo volumes were published in the following years:\n*1918\n*1919\n*1920\n*1921\n\nThis was due to the events of the [[Russian revolution of 1917]] and the subsequent [[Russian civil war]].\n\nThe edition of 1922 was very concise and only contained details of businesses in the city but not residents.\n\n== Termination of series ==\n\nPublication came to a halt after the edition of 1935, coinciding with the time of [[Joseph Stalin]]\'s [[great purge]]s and [[Moscow Trials]]. The only further volumes were issued in 1939 and 1940, but these (like the edition in 1922) only contained details of state run businesses and public and governmental offices, but not residents.\n\n== Availability ==\n\nMany original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the U.S., Europe (including [[The Baltic]], Finland the United Kingdom and Germany) however most only have an incomplete collection. The [[Russian National Library]] in Saint Petersburg has a complete run of all volumes published available.\n\n== Other city directories ==\n\nSuvorin also published city directories for [[Moscow]] under the title \'\'[[Vsia Moskva]]\'\' (All Moscow) for the years 1875 to 1936 and for the whole country under the titles \'\'[[Vsia Rossiia]]\'\' (All Russia) continued under than name \'\'[[Ves SSSR]]\'\' (All USSR) from 1924 to 1931.\n\nSince 1993 a telephone directory under the title "Ves Petersburg" has been published annually by the publishing House Presskom but this is vastly different in content then the original directories and does not list residents.\n\n== Sources ==\n\nhttp://www.encspb.ru/en/article.php?kod=2804017249\nVes Peterburg - http://www.allinform.ru\n\n==See also==\n\n*\'\'[[Vsia Moskva]]\'\'\n*\'\'[[Vsia Rossiia]]\'\'\n\n== External links ==\n*[http://www.nlr.ru Official website of the Russian National Library in Saint Petersburg]\n*[http://surname.litera-ru.ru/ A russian website offering a search engine in cyrillic for some city directories.]\n\n[[Category:Directories]]\n[[Category:History of Saint Petersburg]]\n[[Category:Russian non-fiction books]]\n[[Category:Media in Saint Petersburg]]\n[[Category:1894 books]]']
['Sources (website)', '20263150', '{{Use dmy dates|date=May 2014}}\n{{More footnotes|date=November 2011}}\n\'\'\'Sources\'\'\' is a [[web portal]] for journalists, freelance writers, editors, [[authors]] and [[researchers]], focusing especially on human sources: [[expert]]s and spokespersons who are prepared to answer [[Reporter]]s\' questions or make themselves available for on-air [[interview]]s.\n\n==Structure==\nThe Sources website is built around a [[Controlled vocabulary|controlled-vocabulary]] subject index comprising more than 20,000 topics. This [[Subject indexing|subject index]] is underpinned by an \'Intelligent Search\' system which helps reporters focus their searches by suggesting additional subjects related to their search terms. For example, a search for "cancer" will suggest terms such as "chemotherapy", "melanoma", "oncology", "radiation therapy", "tobacco diseases" and "tumours", as well as topics that actually contain the word "cancer".\n\nEach topic reference links in turn to experts and spokespersons on that topic, with profiles describing their expertise and, where relevant, their approach to the issue, along with their phone numbers and other contact information. Sources includes listings for universities and research institutes, non-profit associations and NGOs, government and public sector bodies, businesses, and individuals including academics, public speakers, and consultants.\n\nThe subject index and the search menus are being translated into French, Spanish and German to make Sources more of an international resource.\n\n==History==\n\n===Print supplement===\nBased in Canada, Sources was founded in 1977 as a print directory for reporters, editors, and story producers. It was first published as a supplement to \'\'Content\'\' magazine, an influential and controversial magazine of journalism criticism. \'\'Content\'\', founded by Dick MacDonald in 1970 and published by [[Barrie Wallace Zwicker|Barrie Zwicker]] after MacDonald\'s death in 1974, frequently took journalists to task for always relying on the same narrow range of sources representing the same conventional points of view for their stories. Zwicker and MacDonald argued in \'\'Content\'\' and in their book \'\'The News: Inside the Canadian Media\'\'<ref>MacDonald, Dick; Zwicker, Barrie. \'\'The News: Inside the Canadian Media\'\'. Deneau. 1982. ISBN 0-88879-053-8</ref> that there was a “terrible sameness” in the media’s coverage of many important issues, and a shutting out of other, potentially valuable, perspectives and sources of information.\n\nZwicker decided to do something about the problem, and in summer 1977, \'\'Content\'\' published its first directory issue, called Sources. Billed as “A Directory of Contacts for Editors and Reporters in Canada”, Sources listed “information officers, public relations officers, media relations and public affairs people, and other contacts for groups, associations, federations, unions, societies, institutions, foundations, industries and companies and federal, provincial and municipal ministries, departments, agencies and boards.”<ref>Sources: A Directory of Contacts for Editors and Reporters in Canada. \'\'Content\'\'. 1977. {{ISSN|0045-835X}}</ref>\n\nExplaining the rationale behind Sources, Zwicker said that “It’s a cliché that every story has two sides. An untrue cliché. Most have several. The reporter’s challenge is digging out all sides. Sources can help.”<ref>Sources 50. 2002. ISBN 0-920299-55-5</ref> From the beginning, Zwicker saw Sources as a public service as well as a tool for journalists. He said that Sources aimed “to help promote a system of information fairness. Communications resources are equivalent to other basic needs – shelter, food, health care, for example. Everyone should have reasonable access to all.”<ref name="ReferenceA">Sources 36. 1995. ISBN 0-920299-24-5</ref> Therefore, he said “we attempt to provide true diversity: access to people in organizations large and small, for-profit and not-for-profit, from low-tech to high-tech, long-established to just-launched.”<ref name="ReferenceA"/>\n\nZwicker told users that “within Sources you will find both mainstream and alternative information. Some may consider alternative as off to one side, not quite up to par, more or less second hand. Here at Sources ‘alternative’ is considered differently, considered as authentic and substantial, even if normally less accessible. The surprises, the jarring notes, the flashes of insight, the ‘odd takes’, the pearls of wisdom, the cries de coeur, the avant garde, tomorrow’s news, the prophesies, the unfiltered, the exciting, the elsewhere-squelched, the memorable, the eccentric, the thought-out-at-length, the unmentionable in polite company, the outrageous, the uncensored ... these are what ‘alternative’ media offer. So far as we can, we will include the alternative with Sources. Sources’ driving philosophy is flat-out informational democracy enabled by user-friendly technology. The assumption is that there is a significant fraction of Canadians who want to use and benefit from such an information resource. The assumption is that a significant fraction of Canadians want to expand their search for solutions, and deepen their understandings, rather than chant conventional wisdoms (however freshly minted) to each other.”<ref name="ReferenceA"/>\n\n===Separate publications===\nAfter a few years, Sources become so big that it could no longer fit into \'\'Content\'\' (the print directory eventually grew to more than 500 pages), and in 1981 it became an independent publication. \'\'Content\'\' itself eventually folded, but Zwicker continued to devote a substantial editorial section in Sources to coverage of topics of interest to journalists, ranging from practical topics such as grammar, style, [[fact-checking]], [[photojournalism]], [[copyright]], fees for freelancers and [[self-publishing]], to feature articles on the state of journalism and the media, to book reviews. From the early 1990s, Sources began to feature articles about online research, notably the regular feature \'Dean\'s Digital World\'<ref>[Dean\'s Digital World – http://www.sources.com/SSR/DeansDigital.htm</ref> by informatics expert Dean Tudor.\n\n===World Wide Web===\n\n====Content====\nSources went on the Internet in 1995 and has been expanding its online portal ever since. It continues to publish a print edition of the directory, primarily for the benefit of freelancers who use it as a source of story ideas, but is now primarily a Web-based resource.\n\nThe Sources website includes not only the Sources directory itself, but a separate government directory, Parliamentary Names & Numbers; a directory of the media, Media Names & Numbers; and The Sources HotLink  [http://www.hotlink.ca (www.hotlink.ca)], which features articles about media relations and public relations. Also on the site is [http://www.sources.com/Fandf/Index.htm Fame and Fortune], a directory of awards, prizes, and scholarships available to writers and journalists, and a portal linked into the online archive of [[Connexions (Information Sharing Services)|Connexions]], a library of documents related to alternatives and social justice.\n\nThe site also houses Sources Select Resources,<ref>Sources Select Resources – http://www.sources.com/SSR.htm</ref> a large library of articles and reviews about journalism and the media, spanning a period of more than 30 years.\n\n====Controversy====\nWhile much of the editorial content has focused on the nitty-gritty of writing, editing and research, Sources has also regularly published articles that have sparked controversy on topics such as censorship and [[media bias]]. One campaign waged by Zwicker and others challenged the [[journalism ethics|ethics]] of journalists accepting free gifts from the people they are supposed to cover. This campaign eventually led Canadian managing editors to agree among themselves that their newspapers would not accept free tickets from travel agencies, resorts, and hotels.\n\nA series of articles by Zwicker on "War, Peace, and the Media"<ref>Zwicker, Barrie. \'\'War, Peace and the Media\'\'. Sources. 1983, 1985</ref> (later collected and published as a booklet) provoked a furor from readers upset by its criticisms of how the media cover [[United States foreign policy|U.S. foreign policy]]. As Zwicker put it in a publisher\'s letter in the next issue, the "reaction ranged from high praise to angry denunciation." The \'\'[[Toronto Sun]]\'\' newspaper devoted three stories to the series. The columnist Claire Hoy was left "trembling with rage" and the editor [[Peter Worthington]] felt "outraged" and a lead editorial denounced Zwicker.\n\nOther controversial articles included one by Wendy Cukier on the public relations battle surrounding proposed [[Gun politics in Canada|gun control]] legislation, which drew the ire of the gun lobby.<ref>Cukier, Wendy. "Anatomy of the Gun Control Debate". Sources. 1996 – http://www.sources.com/SSR/Docs/PNN5-1-GunControl.htm</ref> Ulli Diemer, who succeeded Zwicker as publisher in 1999, came under attack from the [[Fraser Institute]] for his article "Ten Health Care Myths: Understanding Canada’s Medicare Debate”, in which he argued that opponents of [[public health care]] were spreading [[Misinformation|mis-information]] designed to mislead and frighten the public.<ref>Diemer, Ulli. \'Ten Health Care Myths: Understanding Canada’s Medicare Debate’. Sources. 1995. – http://www.diemer.ca/Docs/Diemer-TenHealthCareMyths.htm</ref>\n\n====New resources====\nIn keeping with its mandate of encouraging a wide diversity of points of view in the media, Sources has added extra resources over time to help organizations and individuals to be heard. These include a calendar of events open to the media<ref>Sources Calendar – http://calendar.sources.com</ref> and a [[news release]] service which Sources members can use to distribute their statements and communiques via online posting and [[RSS]]. The releases are also subject indexed and integrated into the overall search structure for information on the Sources site.\n\n==See also==\n* [[Barrie Wallace Zwicker|Barrie Zwicker]]\n\n== Notes ==\n{{reflist|33em}}\n\n==References==\n{{refbegin|33em}}\n* Basch, Reva. \'\'Secrets of the Super Net Searchers: The Reflections, Revelations, and Hard-won Wisdom of 35 of the World’s Top Internet Researchers\'\'. Pemberton Press. 1996. ISBN 0-910965-22-6\n* Berkman, Robert. \'\'The Skeptical Business Searcher: The Information Advisor’s Guide to Evaluating Web Data, Sites and Sources\'\'. Information Today, 2004. ISBN 0-910965-66-8\n* Bonner, Allan. \'\'Media Relations\'\'. Briston House. 2003. ISBN 1-894921-00-3\n* Carney, William Wray. \'\'In the News The Practice of Media Relations in Canada\'\'. University of Alberta Press\', 2002. ISBN 0-88864-382-9\n* Comber, Mary Anne; Mayne, Robert S. \'\'The Newsmongers: How The Media Distort the Political News\'\'. 1987. McClelland & Stewart\n* Cormack, Paul G.; Shewchuk, Murphy (eds.) \'\'The Canadian Writers’ Guide\'\'. 13th Edition. Canadian Authors Association. Fitzhenry & Whiteside, 2003. ISBN 1-55041-740-1\n* Hackett, Robert A.; Gruneau, Richard. \'\'The Missing News: Filters and Blind Spots in Canada’s Press\'\'. Newswatch Canada. Canadian Centre for Policy Alternatives & Garamond Press, 2000\n* Hackett, Robert A. \'\'News and Dissent: The Press and The Politics of Peace in Canada\'\'. 1993. Ablex.\n* Hackett, Robert A.; Zhao, Yuezhi. \'\'Sustaining Democracy? Journalism and the Politics of Objectivity\'\'. Garamond Press. 1998. ISBN 1-55193-013-7\n* Kashmeri, Zuhair. \'\'The Gulf Within: Canadian Arabs, Racism, & The Gulf War\'\'. James Lorimer. 1991\n* MacDonald, Dick; Zwicker, Barrie. \'\'The News: Inside the Canadian Media\'\'. Deneau. 1982. ISBN 0-88879-053-8\n* Mann, Thomas. \'\'The Oxford Guide to Library Research\'\'. Oxford University Press. 1998. ISBN 0-19-512313-1\n* Manson, Katherine; Hackett, Robert; Winter, James; Gutstein, Donald; Gruneau, Richard (eds.) \'\'Blindspots in the News? Project Censored Canada Yearbook\'\'. Project Censored Canada. 1995.\n* McGuire, Mary; Stilborne, Linda; McAdams, Melinda; Hyatt, Laurel. \'\'The Internet Handbook for Writers, Researchers, and Journalists\'\'. Trifolium Books. 1997, 2002. ISBN 1-895579-17-1\n* Miljan, Lydia; Cooper, Barry Cooper. \'\'Hidden Agendas: How Journalists Influence the News\'\'. University of British Columbia Press. 2003. ISBN 0774810203\n* Miller, John. \'\'Yesterday’s News: Why Canada’s Daily Newspapers are Failing Us\'\'. Fernwood Publishing, 1999\n* Ouston, Rick. \'\'Getting the Goods: Information in B.C.: How to Find It, How to Use It\'\'. New Star Books, 1990\n* Patriquin, Larry. \'\'Inventing Tax Rage: Misinformation in the National Post\'\'. Fernwood Publishing, 2004. ISBN 1-55266-146-6\n* Soderlund, Walter C.; Hildebrandt, Kai (eds.) \'\'Canadian Newspaper Ownership in the Era of Convergence: Rediscovering Social Responsibility\'\'. University of Alberta Press. 2005, ISBN 0-88864-439-6\n* Tudor, Dean. \'\'Finding Answers: Approaches to Gathering Information\'\'. McClelland & Stewart Inc., Toronto. 1993.\n* Ward, Stephen J.A. \'\'The Invention of Journalism Ethics: The Path to Objectivity and Beyond\'\'. McGill-Queen’s University Press. 2004. ISBN 0-7735-2810-5\n* Winter, James. \'\'Media Think\'\'. Black Rose Books. 2002. ISBN 1-55164-054-6\n* Zwicker, Barrie. \'\'War, Peace and the Media\'\'. Sources. 1983, 1985\n{{refend}}\n\n==External links==\n* {{official website|http://www.sources.com/}}\n** [http://www.sources.com/SSR.htm Sources Select Resources]\n** [http://www.sources.com/News.htm Sources Select News]\n** [http://calendar.sources.com Sources Calendar]\n** [http://www.sources.com/Fandf/Index.htm Fame & Fortune]\n* [http://www.hotlink.ca The Sources HotLink]\n* [http://www.connexions.org Connexions Information Sharing Services]\n\n{{DEFAULTSORT:Sources (Website)}}\n[[Category:Directories]]\n[[Category:Journalism organizations]]\n[[Category:Knowledge markets]]\n[[Category:Online databases]]\n[[Category:Web directories]]\n[[Category:Websites]]']
["Pigot's Directory", '20147149', "[[File:PigotDirectory1839Kent.jpg|thumb|An example page from Pigot's 1839 directory of Kent, Surrey and Sussex. This page has information on Bromley and Canterbury]]\n'''Pigot's Directory''' was a major British [[Trade directory|directory]] started in 1814 by [[James Pigot]].<ref>{{cite web | url=http://www.hertfordshire-genealogy.co.uk/data/directories/directories-pigot.htm | title=Trade directories in Hertfordshire}}</ref>\n\nPigot's Directories covered England, Scotland, and Wales in the period before official Civil Registration began and are a valuable source of information regarding all major professions, nobility, gentry, clergy, trades and occupations including taverns and public houses and much more are listed. There are even timetables of the coaches and carriers that served a town.\n\nParishes are listed for each area with useful information including the number of inhabitants, a geographical description and the main trades and industries of the area or town.\n\n{{TOC right}}\n\n==List of Pigot’s Trade Directories by date==\n* {{cite book |url=https://books.google.com/books?id=4llGAAAAYAAJ |title= Commercial Directory for 1818-19-20 |location=Manchester |publisher=James Pigot |year=1818 }}\n* {{Citation |publication-place = London |publisher = J. Pigot & Co. |url =https://books.google.com/books?id=zBEHAAAAQAAJ |title = Pigot & Co.'s metropolitan guide & book of reference to every street, court, lane, passage, alley and public building, in the cities of London & Westminster, the borough of Southwark, and their respective suburbs |publication-date = 1824 }}\n* {{cite book |url= https://books.google.com/books?id=hdMHAAAAQAAJ |title= Pigot & Co.'s National Commercial Directory for 1828-9 |location=London |publisher=James Pigot }}\n*{{Citation |url = http://openlibrary.org/books/ia:pigotcosnational1837dire/Pigot_and_Co.'s_national_commercial_directory_for_the_whole_of_Scotland_and_of_the_Isle_of_Man_..._t |title = Pigot and Co.'s National Commercial Directory for the Whole of Scotland and of the Isle of Man, ... Manchester, Liverpool, Leeds, Hull, Birmingham, Sheffiled, Carlisle, and Newcastle-upon-Tyne |publication-date = 1837 |location = London |publisher =J. Pigot & Co. }}\n\n==List of Pigot’s Trade Directories by geographic coverage==\n*[[Bedfordshire]] 1839\n*[[Cambridgeshire]] 1839\n*[[Cambridgeshire]] 1830\n*[[Derbyshire]] 1835\n*[[Durham, England|Durham]] 1834\n*[[Essex]] 1839\n*[[Herefordshire]] 1835\n*[[Hertfordshire]] 1839\n*[[Huntingdonshire]] 1830\n*[[Huntingdonshire]] 1839\n*[[Kent]] 1839\n*[[Leicestershire]] 1835\n*[[Lincolnshire]] 1835\n*[[London]] 1839\n*[[Middlesex]] 1839\n*[[Monmouthshire (historic)|Monmouthshire]] 1835\n*[[Norfolk]] 1839\n*[[North Wales]] 1835\n*[[Northumberland]] 1828\n*[[Northumberland]] 1834\n*[[Nottinghamshire]] 1835\n*[[Rutlandshire]] 1835\n*[[Shropshire]] 1835\n*[[South Wales]] 1835\n*[[Staffordshire]] 1835\n*[[Suffolk]] 1830\n*[[Suffolk]] 1839\n*[[Surrey]] 1839\n*[[Sussex]] 1839\n*[[Sussex]] 1840\n*[[Warwickshire]] 1835\n*[[Worcestershire]] 1835\n\n==References==\n{{reflist}}\n\n==External links==\n* {{citation |title=Historical Directories - England & Wales |publisher=[[University of Leicester]] |location=UK |url=http://specialcollections.le.ac.uk/cdm/landingpage/collection/p16445coll4}}. Includes digitized Pigot's & Slater's directories for England & Wales, various dates\n* {{citation |title=Historical Directories - Scotland |publisher=[[National Library of Scotland]] |location=UK |url=http://www.nls.uk/family-history/directories/post-office/index.cfm?place=Scotland }}. Includes digitized Pigot's & Slater's directories for Scotland, various dates\n\n[[Category:Directories]]\n\n\n{{ref-book-stub}}"]
['Almanach de Gotha', '747726', '{{Multiple issues|\n{{refimprove|date=July 2014}}\n{{primary sources|date=July 2014}}\n}}\n{{italic title}}\n{{Use dmy dates|date=June 2013}}\t\n{{Infobox book series\n| name             = Almanach de Gotha\n| image            =  DeGotha1851.jpg\n| image_caption    = The Almanach de Gotha  1851\n| books            = \n| author           =\n| editors          = \n| title_orig       = \n| translator       = \n| illustrator      = \n| cover_artist     = \n| country          =\n| language         =\n| genre            =\n| discipline       =\n| publisher        = [[Johann Christian Dieterich|J.C. Dieterich]]<br>C.W. Ettinger<br>C.G. Ettinger<br>[[Justus Perthes (publishing company)|Justus Perthes]]<br>Almanach de Gotha, Ltd.\n| pub_date         = 1763-1944<br>1998-\n| english_pub_date = 1998-\n| media_type       = \n| number_of_books  = \n| list_books       = \n| preceded by      = \n| followed by      = \n}}\n\nThe \'\'\'\'\'Almanach de Gotha\'\'\'\'\' was a directory of Europe\'s [[Royal family|royalty]] and higher [[nobility]], also including the major [[government]]al, [[military]] and [[diplomatic corps|diplomatic]] [[corps]], as well as statistical data by country. First published in 1763 by C.W. Ettinger in [[Gotha (town)|Gotha]] at the [[duke|ducal]] [[court]] of [[Frederick III, Duke of Saxe-Gotha-Altenburg|Frederick III]], Duke of [[Saxe-Gotha-Altenburg]], it came to be regarded as an authority in the classification of monarchies and their courts, reigning and former dynasties, princely and ducal families, and the genealogical, biographical and titulary details of Europe\'s highest level of aristocracy. It was published from 1785 annually by [[Justus Perthes (publishing company)|Justus Perthes]] Publishing House in Gotha, until 1944 when the [[Soviet]]s destroyed the \'\'Almanach de Gotha\'s\'\' archives.\n\nIn 1998, a London-based publisher acquired the rights for use of the title of \'\'Almanach de Gotha\'\' from Justus Perthes Verlag Gotha GmbH. Perthes regard the resultant volumes as new works, and not as a continuation of the editions which Perthes had published from 1785 to 1944.<ref name=Perthes>{{Cite web|url=http://www.perthes.de/geschichte_justus_perthes/almanach_de_gotha/almanach_de_gotha_english.html |title=Almanach de Gotha |accessdate=9 June 2008 |publisher=Justus Perthes }}</ref> Two volumes have been printed since 1998, with Volume I containing lists of the sovereign, formerly sovereign and mediatised houses of Europe, and a diplomatic and statistical directory; and Volume II containing lists of the non-sovereign princely and ducal houses of Europe.\n\n==Gotha publication, 1763–1944==\nThe original \'\'Almanach de Gotha\'\' provided detailed facts and statistics on nations of the world, including their [[reign]]ing and formerly reigning houses, those of [[Europe]] being more complete than those of other continents. It also named the highest incumbent [[Great Officer of State (disambiguation)|officers of state]], members of the [[diplomatic corps]], and Europe\'s upper nobility with their families. Although at its most extensive the \'\'Almanach\'\' numbered more than 1200 pages, fewer than half of which were dedicated to monarchical or aristocratic data,<ref name="gotha">Almanach de Gotha. [[Justus Perthes]], Gotha, 1944, pp. 7-12, 131, 169, 363-364, 558, 581-584. French.</ref> it acquired a reputation for the breadth and precision of its information on royalty and nobility compared to other [[almanac]]s.<ref name="diesbach">{{Cite book|title=Secrets of the Gotha|last=de Diesbach|first=Ghislain|authorlink = Ghislain de Diesbach|year=1967|publisher=Chapman & Hall|location=UK|pages=21, 23–24, 28–30}}</ref>\n[[File:London Library book, Gothaisches Genealogisches Taschenbuch der Freiherrlichen Häuser, 1910, Justus Perthes, Gotha.jpg|thumb|[[London Library]]\'s copy of \'\'Gothaisches Genealogisches Taschenbuch der Freiherrlichen Häuser\'\', 1910.]]\nThe \'\'Almanach\'\'\'s publication by [[Justus Perthes]] began at the ducal court of [[Saxe-Coburg and Gotha]] in Germany and, its reigning dynasty was listed first therein well into the 19th century, usually followed by kindred sovereigns of the [[House of Wettin]] and then, in alphabetical order, other families of princely rank, ruling and non-ruling. Although always published in French, other almanacs in French and English were more widely sold internationally. The almanac\'s structure changed and its scope expanded over the years. The second portion, called the \'\'Annuaire diplomatique et statistique\'\' ("Diplomatic and Statistical Yearbook"), provided [[demography|demographic]] and governmental information by nation, similar to other [[almanac]]s. Its first portion, called the \'\'Annuaire généalogique\'\' ("Genealogical Yearbook"), came to consist essentially of three sections: reigning and formerly reigning families, [[mediatization|mediatized families]] and non-sovereign families at least one of whose members bore the title of prince or duke.<ref name="diesbach"/>\n\nThe first section always listed Europe\'s [[sovereignty|sovereign]] houses, whether they ruled as emperor, king, grand duke, duke, prince (or some other title, e.g., [[prince elector]], [[margrave]], [[landgrave]], [[count palatine]] or [[pope]]). Until 1810 these sovereign houses were listed alongside such families and entities as Barbiano-Belgiojoso, Clary, Colloredo, Furstenberg, the Emperor, Genoa, Gonzaga, Hatzfeld, Jablonowski, Kinsky, Ligne, Paar, Radziwill, Starhemberg, Thurn and Taxis, Turkey, Venice and the [[Order of Malta]] and the [[Teutonic Knights]]. In 1812, these entries began to be listed in groups.<ref name="diesbach"/> First were German sovereigns who held the rank of grand duke or prince elector and above (the Duke of Saxe-Gotha was, however, listed here along with, but before, France—see below).\n\nListed next were Germany\'s reigning ducal and princely dynasties under the heading "College of Princes", e.g., [[Hohenzollern]], [[County of Isenburg|Isenburg]], [[Leyen]], [[Liechtenstein]] and the other [[Ernestine duchies|Saxon duchies]]. They were followed by heads of non-German monarchies, i.e. Austria, Brazil, Great Britain, etc. Fourthly were listed non-reigning dukes and princes, whether mediatized or not, including [[Arenberg]], [[House of Croÿ|Croy]], [[Fürstenberg (princely family)|Furstenberg]] alongside [[Batthyany]], [[Jablonowski]], [[Sulkowski]], Porcia and [[Prince of Benevento|Benevento]].\n\nIn 1841 a third group was added to those of the sovereign dynasties and the non-reigning princely and ducal families. It was composed exclusively of the mediatized families of comital rank recognized as belonging, since 1825, to the same historical category and sharing some of the same privileges as reigning dynasties by the various states of the [[German Confederation]]; these families were German with a few exceptions (e.g. [[Bentinck]], [[Van Rechteren|Rechteren-Limpurg]]). The 1815 treaty of the [[Congress of Vienna]] had authorized — and Article 14 of the German Confederation\'s \'\'Bundesakt\'\' (charter) recognized — retention from the [[Holy Roman Empire|German Imperial]] regime of [[Royal intermarriage|equality of birth]] for marital purposes of mediatized families (called \'\'Standesherren\'\') to reigning dynasties.<ref name="diesbach"/> The almanac added a third section consisting exclusively of mediatized families of comital rank.\n\nIn 1877, the mediatized comital families were moved from section III to section II A, where they joined the princely mediatized families. For the first time in the century of its existence, the largely non-German, un-mediatized princely and ducal families of the \'\'Almanach de Gotha\'\' were removed from the same section as other non-reigning families bearing princely titles.<ref name="diesbach"/> While non-mediatized German and Austrian families (e.g. [[Prince Lichnowsky|Lichnowsky]], [[Wrede]]), were likewise relocated from the almanac\'s second to its third section, the second section\'s new preponderance of German families, princely and comital, which were henceforth recognized as possessing the exclusive privilege of inter-marriage with reigning dynasties was salient:<ref name="diesbach"/> Excluded were members of such historically notable families as the [[House of Rohan|Rohan]]s, [[Orsini]]s, [[Duke of Ursel|Ursels]], [[Duke of Norfolk|Norfolks]], [[Czartoryski]]s, [[Galitzine]]s, [[Duc de La Rochefoucauld|La Rochefoucaulds]], [[House of Kinsky|Kinskys]], [[Radziwiłł family|Radziwills]], [[De Mérode|Merodes]], [[Dohna (Disambiguation)#People|Dohnas]] and [[Duke of Alba|Albas]].\n\nAlthough theoretically mediatized families were distinguished from Europe\'s other nobility by the former status of their territories as \'\'[[Imperial State|Reichsstand]]\'\' and their exercise within the Holy Roman Empire of "semi-sovereignty" or [[imperial immediacy]] (\'\'Reichsunmittelbarkeit\'\'), many \'\'Standesherr\'\' families, especially those bearing the [[count|comital]] title, had not been fully recognized as legally possessing immediate status within the Empire prior to its collapse in 1806. No other families whose highest title was count were admitted to any section of the almanac.<ref name="diesbach"/>\n\nMoreover, other [[deposition (politics)|deposed]] European dynasties (e.g. [[House of Arenberg|Arenberg]], [[Ernst-Johann Biron, Prince of Courland|Biron]], [[Dadiani]], [[Boncompagni]]-[[Ludovisi (family)|Ludovisi]], [[Giray dynasty|Giray]], [[House of Murat|Murat]]) did not benefit \'\'vis-a-vis\'\' the almanac from a similar interpretation of their historical status. Many princely or ducal families were listed only in its third, non-dynastic section or were excluded altogether, evoking criticism in the 20th century from such genealogists as [[Cyril Toumanoff]], [[Jean-Engelbert d\'Arenberg|Jean-Engelbert, Duke d\'Arenberg]] and [[William Addams Reitwiesner]],<ref>Fra Cyril Toumanoff, "Genealogical Imperialism" (1985) vol 6 (no 134) (NS) Coat of Arms pp. 145, 147.</ref><ref>Duke and Prince Jean Engelbert d\'[[Arenberg]], "The Lesser Princes of the Holy Roman Empire in the Napoleonic Era" dissertation, Washington, DC, 1950, published as Les Princes du St-Empire à l\'époque napoléonienne (Louvain, 1951) 15ff, quoted in Almanach de Gotha (Almanach de Gotha, London, 1998) pp. 275–286.</ref> the latter commenting that the changes displayed "pan-German triumphalism" and even a "fairly nasty bit of Germanic chauvinism."<ref>{{Cite web|url= http://www.wargs.com/essays/mediatize.html |title= Mediatization |accessdate= 19 April 2011 |last= Reitwiesner |first= William Addams |date=January 1998 |authorlink= William Addams Reitwiesner}}</ref>\n\nEven in the early 19th century the almanac\'s retention of [[deposition (politics)|deposed]] dynasties evoked objections, although not necessarily the desired changes. The elected Emperor [[Napoleon]] protested in writing to his foreign minister, [[Jean-Baptiste Nompère de Champagny|Champagny]]: <blockquote>\'\'Monsieur de Champagny, this year\'s "Almanach de Gotha" is badly done. First comes the Comte de Lille [title used in exile by [[Louis XVIII of France|Louis de Bourbon, Count of Provence]] -- future King Louis XVIII of France], followed by all the princes of the [[Confederation of the Rhine|Confederation]] as if no change has been made in the constitution of Germany; the family of France is named inappropriately therein. Summon the Minister of Gotha, who is to be made to understand that in the next Almanach all of this is to be changed. The House of France must be referred to as in the [French] Imperial Almanac; there must be no further mention of the Comte de Lille, nor of any German prince other than those retained by the Articles of Confederation of the Rhine. You are to insist that the article be transmitted to you prior to publication. If other almanacs are printed in my allies\' realms with inappropriate references to the Bourbons and the House of France, instruct my ministers to make it known that you have taken note, and that this is to be changed by next year.\'\'<ref>{{Cite book|url= https://books.google.com/books?id=ScM3AQAAMAAJ&pg=PA124&dq=Napol%C3%A9on+13275+Champagny&hl=en&sa=X&ei=W7-5U8PHMpHooATfkYCoBg&ved=0CCEQ6AEwAA#v=onepage&q=Napol%C3%A9on%2013275%20Champagny&f=false |title= Correspondance de Napoléon I|volume=XVI |publisher=Imprimerie Impériale|date=1864|location=France|accessdate=6 July 2014}}</ref></blockquote>\n\nThe response of the publishers was to humour Napoleon by producing two editions: one for France, with the recently ennobled, and another which included dynasties deposed since abolition of the [[Holy Roman Empire]]. A merged version, whose first section including recently reigning dynasties but also families which lost sovereignty after the fall of Napoleon in 1815, remained in publication until 1944, and has been replicated in subsequent dynastic compilations (e.g., \'\'Genealogisches Handbuch des Adels, Fürstliche Häuser\'\', \'\'Le Petit Gotha\'\', Ruvigny\'s "Titled Nobility of Europe").\n\nIn 1887 the \'\'Almanach\'\' began to include non-European dynasties in its first section, with the inclusion of one of the ruling families of India.\n\n===World War II and aftermath===\nWhen Soviet troops entered [[Gotha (town)|Gotha]] in 1945, they systematically destroyed all archives of the \'\'Almanach de Gotha\'\'.{{Citation needed|date=September 2010}}\n\nIn 1951 a different publisher, C.A. Starke, began publication of a multi-volume German-language publication entitled the \'\'Genealogisches Handbuch des Adels\'\' ([[:de:Genealogisches Handbuch des Adels|GHdA]]). The publication is divided into subsets; the \'\'Fürstliche Häuser\'\' subset is largely equivalent to the German language \'\'Gothaischer Hofkalender\'\' and its \'\'Fürstlichen Häuser\'\' volume which was also published by Perthes, or sections 1, 2 and 3 of the \'\'Almanach de Gotha\'\'. However, no single volume of the \'\'Fürstliche Häuser\'\' includes all the families included in the \'\'Hofkalender\'\' or \'\'Almanach de Gotha\'\'. It is necessary to use multiple volumes to trace the majority of European royal families.\n\n==London publication, since 1998==\n[[File:2014 Almanach de Gotha Covers.jpg|right|200px|thumb|\'\'Almanach de Gotha\'\', 2014, Volumes I & II]]\n\nIn 1989 the family of [[Justus Perthes]] re-established its right to the use of the name \'\'Almanach de Gotha\'\'. The family then sold these rights in 1995 to a new company, Almanach de Gotha Limited, formed in London.<ref>[https://www.thegazette.co.uk/notice/L-60158-1601227 Notice of Disclaimer]</ref> The new publishers launched with the 182nd edition on 16 March 1998 at [[Claridge\'s Hotel]].<ref>{{Cite web|url= http://www.almanachdegotha.com/site/modern.htm|title=The Modern Gotha |accessdate=30 May 2008 |publisher=Almanach de Gotha |archiveurl=https://web.archive.org/web/20060211154624/http://www.almanachdegotha.com/site/modern.htm |archivedate=11 February 2006}}</ref><ref>Jury, Louise. [http://www.independent.co.uk/news/upper-crust-toasts-aristocrat-studbook-1150076.html Upper crust toasts aristocrat studbook] The Independent (14 March 1998)</ref>  It was written in English instead of French as the editor felt that English was now the language of diplomacy.<ref name=Runciman>{{Cite news|first= Steven |last= Runciman |authorlink=Steven Runciman |title=The first book of kings |url=http://findarticles.com/p/articles/mi_qa3724/is_199805/ai_n8792875 |publisher=[[The Spectator]] |date=2 May 1998 |accessdate=6 June 2008 }}</ref> Charlotte Pike served as editor of the 1998 edition only and John Kennedy as managing director and publisher. The new publishers also revived the Committee of Patrons under the presidency of King [[Juan Carlos I of Spain]] and chairmanship of King [[Michael I of Romania]].<ref>{{Cite web|url= http://www.gotha1763.com/society.html|title=The Société des Amis de l\'Almanach de Gotha |accessdate=1 May 2014 |publisher=Almanach de Gotha}}</ref>\n\nThe London publisher produced a further four editions of volume I (1999, 2000, 2003 and 2004) based on the 1998 edition of volume I which include Europe\'s and South America\'s reigning, formerly reigning, and mediatised princely houses, and a single edition of volume II in 2001 edited by John Kennedy and Ghislain Crassard which include other non-sovereign princely and ducal houses of Europe.<ref name=Hardman>{{Cite news|first=Robert |last=Hardman |title=Family almanac will unmask the noble pretenders |url=http://www.telegraph.co.uk/news/worldnews/europe/germany/1317982/Family-almanac-will-unmask-the-noble-pretenders.html |publisher=[[Daily Telegraph]] |date=19 June 2001 |accessdate=6 June 2008 }}</ref> A review in \'\'[[The Economist]]\'\' criticised the low editorial standards and attacked volume II for a lack of genealogical accuracy.<ref name=review_economist>{{Cite web| title=The Almanach de Gotha -- Gothic horror | url=http://www.economist.com/books/displayStory.cfm?Story_ID=949183 | publisher=[[The Economist]] | date = 24 January 2002 | accessdate=7 October 2007}}</ref> After a gap of eight years a new edition of volume I was published in 2012 under the editorship of John James.<ref>{{Cite web|url=http://www.boydellandbrewer.com/store/viewitem.asp?idproduct=13798 |title=Almanach de Gotha 2012. Volume I, parts I & II |accessdate=8 February 2012 |publisher=[[Boydell & Brewer]] }}</ref> A review in \'\'[[The Times Literary Supplement]]\'\' praised the 2012 volume I for a "punctilious itemization of titles, lineage and heraldry [aiming] for scholarship rather than sensation...Some family legends&nbsp;– such as the Ottoman boast of descent from a grandson of Noah&nbsp;– do not merit inclusion in a work with authoritative aspirations. Most quixotically of all, the title page displays the word \'Annual\', although it has been eight years since the last edition appeared."<ref>{{Cite web|url=http://www.the-tls.co.uk/tls/public/article1139358.ece |title=And dark the Sun and Moon, and the Almanach de Gotha... |accessdate=2013-01-17 |publisher=The Times Literary Supplement |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20130117050945/http://www.the-tls.co.uk/tls/public/article1139358.ece |archivedate=17 January 2013 |df=dmy }}</ref>\n\n==Structure==\nAs it was the practice of the diplomatic corps to employ official titles, adhere to local [[Order of precedence|precedence]] and etiquette, and to tender congratulations and condolences to members of the dynasty of the nation to which they were assigned, the almanac included a \'\'Calendrier des Diplomates\'\' ("Diplomats\' Calendar") section, which detailed major national holidays, anniversaries, ceremonies and royal birthdates.<ref name="gotha"/>\n\nFollowing [[World War I]] and the fall of many [[royal house]]s, fewer regulatory authorities remained to authenticate use of titles; however the \'\'Almanach de Gotha\'\' continued the practice of strict verification of information, requesting certified copies of [[letters patent]], genealogies confirmed by competent authorities, documents, decrees and references for titles claimed.<ref name="gotha"/> Europe\'s middle and lower nobility (families whose principal title ranked below that of prince or duke&nbsp;— except [[German mediatisation|mediatized]] families, listed in a section of their own) were not included in the almanac. Nor were the [[grandee]]s or [[Portuguese dukedoms|ducal families]] of Portugal and Spain (where titles, being habitually transmissible through both male and [[cognatic|female lines]], were often inherited by relatives of non-[[patrilineality|patrilineal]] lineage). Families of some Italian and East European nations (e.g., Russia, Romania), where the princely title was claimed by many, were also incomplete. Yet the reigning, formerly reigning and noble families included in the almanac numbered in the hundreds by the time it ceased publication in 1944.<ref name="gotha"/>\n\nIn 1890 the almanac renamed II A to section II, and II B to section III. Dynasties ruling non-European nations were located in section I B. Families which became extinct were listed for the final time in the year following death of the last member, male or female, and subsequent editions referred readers to that volume.<ref name="gotha"/>\n\nFamilies that ceased to be included for other reasons, such as lack of proof of a family\'s legitimate descendants or discovery that it did not hold a valid princely or ducal title, were henceforth excluded but added, along with dates of previous insertion, to a list following the last section of each \'\'Annuaire Genealogique\'\' (Genealogical Yearbook), which page was entitled \'\'Liste des Maisons authrefois publiees dans la 3e partie de l\'Almanach de Gotha\'\' ("List of Houses formerly published in the 3rd section of the \'\'Almanach de Gotha\'\'.") <ref name="gotha"/>\n\nFrom 1927, the almanac ceased to include all families in each year\'s edition, henceforth rotating entries every few years. Where titles and [[style (manner of address)|style]]s (such as [[Serene Highness]]) had ceased to be recognized by national governments (e.g. Germany, Austria, Czechoslovakia), the almanac provided associated dates and details, but continued to attribute such titles and styles to individuals and families, consistent with its practice since the [[French revolution]]; deposed sovereigns and dynasties continued to be accorded their former titles and rank, but dates of deposition were noted,<ref name="diesbach"/> and titles exclusively associated with sovereignty (e.g. emperor, queen, grand duke, crown princess) were not accorded to those who had not borne them during the monarchy. Titles of [[pretender|pretence]] below sovereign rank were accorded to members of formerly reigning dynasties as reported by heads of their houses, otherwise self-assumed titles were not used. The almanac included an explicit disclaimer announcing that known biographical details, such as birthdates and divorces, would not be suppressed.<ref name="gotha"/>\n\n==See also==\n*[[Burke\'s Peerage]]\n*[[Debrett\'s|Debrett’s Peerage & Baronetage]]\n\n==References==\n{{Reflist|2}}\n\n==Further reading==\n*[[Ghislain de Diesbach|Diesbach, Ghislain de]]. \'\'Secrets of the Gotha\'\'. Meredith Press, 1964.\n\n==External links==\n*[http://gallica.bnf.fr/ Scanned versions of the old almanachs]\n*[https://archive.org/search.php?query=almanach+de+gotha%20AND%20mediatype%3Atexts Almanach de Gotha at Internet Archive]\n*[http://www.gotha1763.com/ Almanach de Gotha]\n\n{{DEFAULTSORT:Almanach De Gotha}}\n[[Category:Biographical dictionaries]]\n[[Category:European nobility]]\n[[Category:Genealogy publications]]\n[[Category:Directories]]\n[[Category:Publications established in 1763]]\n[[Category:Almanacs]]']
['Clergy List', '34259473', 'The \'\'\'Clergy List\'\'\' was a professional directory of the [[Church of England]] which appeared between 1841-1917.  From the start it also covered Wales, together with more limited information relating to Scotland, Ireland, and other churches within the [[Anglican Communion]].\n\n==Background and early contents==\n\nAn opportunity to compile and issue a new directory had been created by the effective disappearance of the earlier [[Clerical Guide or Ecclesiastical Directory]], edited by \'\'\'Richard Gilbert\'\'\', and also by the introduction of the much improved system of the [[Penny Post]]. \n\nThe basic contents of the \'\'\'Clergy List\'\'\'\'s earlier editions was summarised on their title pages: \n*an alphabetical list of the clergy (or at least of those who held benefices)\n*an alphabetical list of the benefices,with their post towns\n*lists of the cathedral establishments\n*benefices arranged under their ecclesiastical divisions\n*lists of ecclesiastical preferments variously under the patronage of the Crown, the bishops, and the deans & chapters, etc.\n\nThe directory was always a bit less expensive than its later rival, [[Crockford\'s Clerical Directory]], but not surprisingly it consequently offered considerably less in the way of biographical detail.  This was especially true in the earlier editions which offered little or no information as to previous appointments, universities attended, or lists of publications by the clergy.\n\n==Publishers and later history==\n\nThe directory was initially published by \'\'\'Charles Cox\'\'\' at the Ecclesiastical Directory Office, [[Southampton Street, London|Southampton Street]], [[Strand, London|Strand]].   Cox – who in 1839 had taken over a periodical called the \'\'\'Ecclesiastical Gazette,\'\'\' originating during the previous year – was able to produce two separate editions during the Clergy List\'s inaugural year of 1841.<ref name="paflin">[http://www.churchtimes.co.uk/content.asp?id=48255] [[Church Times]]: two-part article \'\'Shop-talk and mordant wit\'\', by Christopher Currie & Glyn Paflin, describing the background to \'\'Crockford\'s Clerical Directory\'\'\'s first hundred editions, 6–13 December 2007</ref>  Thereafter it managed to maintain annual publication right up until adverse trading conditions forced its closure as a separate volume in 1917.\n\nCox remained as the Clergy List\'s publisher for many years, but by 1881 the title had been taken over by John Hall of [[Whitehall|Parliament Street]], In 1888 it was further taken over by Hamilton, Adams & Company, of London\'s [[Paternoster Row]].  They had earlier acquired Thomas Bosworth\'s \'\'\'[[Clerical Guide and Ecclesiastical Directory]]\'\'\', merging the two titles in 1889.  During the following year the combined directory was still further transferred to Kelly & Company, the publishers of [[Kelly\'s Directories]].<ref name="paflin" /> \n\nThe later volumes were considerably expanded to include much greater biographical detail – broadly comparable with Crockford – but this was not sufficient to sustain the publication in the longer term.  Over the years the number of pages also increased – ranging from around 300 in 1841 to around 700 by the 1890s. \n\nAfter 1917 the Clergy List finally merged with its long-time rival, \'\'\'Crockford\'s Clerical Directory\'\'\'.  At least as late as 1932 the latter continued to advertise on its preliminary pages that it "incorporated the \'\'\'Clergy List, the Clerical Guide and the Ecclesiastical Directory\'\'\'\'\'".<ref name="paflin" />\n\nIn recent years certain of the earlier editions of the Clergy List (including the first edition <ref>The 1841 first edition of the \'\'Clergy List\'\' may be downloaded free of charge from the Google eBookstore [https://books.google.com/ebooks]</</ref>) have been reissued by various publishers – either on CD-ROM or in scanned format on the World Wide Web.\n\n==References==\n{{reflist}}\n\n[[Category:Directories]]\n[[Category:Church of England]]\n[[Category:Church in Wales]]\n[[Category:Scottish Episcopal Church]]\n[[Category:Anglicanism]]']
['Almanach de Bruxelles (defunct)', '36297187', "The '''''Almanach de Bruxelles''''' is a now defunct [[France|French]] social register that listed [[royal family|royal]] and [[nobility|noble]] [[dynasties]] of [[Europe]]. It was established in 1918 during the [[Second World War]] to compete against the prominent German [[Almanach de Gotha]].<ref>March 17, 1918. [http://query.nytimes.com/mem/archive-free/pdf?res=9E0DEED6143AEF33A25754C1A9659C946996D6CF Almanach de Gotha has a french rival] at ''[[New York Times]]''</ref>\n\n==See also==\n* ''[[Almanach de Gotha]]''\n\n==Sources==\n{{reflist}}\n\n==External links==\n*[http://www.worldcat.org/title/almanach-de-bruxelles-annuaire-genealogique-historique-heraldique-des-maisons-souverains-princieres-et-ducales/oclc/06083750 ''Almanach de Bruxelles'' (1918-] at [[WorldCat]]\n\n[[Category:Genealogy publications]]\n[[Category:Directories]]\n[[Category:Biographical dictionaries]]\n[[Category:Defunct periodicals of France]]\n[[Category:European nobility]]\n[[Category:French royalty]]\n[[Category:1918 establishments in France]]\n[[Category:Publications established in 1918]]\n \n\n{{royal-bio-book-stub}}\n{{bio-dict-stub}}"]
['Association of Directory Publishers', '39023454', 'The \'\'\'Association of Directory Publishers\'\'\' (ADP), is an international [[trade association]] founded in 1898 and is headquartered in [[Traverse City, Michigan]].<ref name=associations2012>{{cite book |title= Encyclopedia of Associations |issn=0071-0202 |volume= 1 |edition=51st |year= 2012 |page= 318 |via=[[Boston Public Library]] Reference & Reader\'s Advisory Department }}<!--|accessdate=April 8, 2013 --></ref> \n\n==About==\nADP is the oldest international trade association serving the Yellow Pages industry.  The Association represents the various interests of its membership which includes publishers of print, online and mobile directories, Certified Marketing Representatives (CMRs), advertising agencies and suppliers to the Yellow Pages and local search industry.\n\nADP represents the $35 billion Yellow Pages industry known as the original "local search engine" that brings buyers to sellers at the exact moment they are ready to buy.\n\nThe Association helps its members expand their businesses by offering them services and tools targeted to assisting them in achieving their clients\' advertising objectives. ADP offers a wide variety of research, marketing and sales materials created with information from leading organization that are developed specifically to help members increase their company\'s bottom line.\n\nADP is a unique Association because of the governance structure of one company, one vote.  Every publisher from the smallest to largest has an equal opportunity to determine the leadership and direction of the Association.  ADP represents member companies of all sizes and from numerous countries.\n\n==History==\nThe group formed in 1898 as the \'\'\'Association of American Directory Publishers,\'\'\' headquartered in New York. It aimed "to improve the [[Reference work|directory]] business."<ref>{{citation |url=https://books.google.com/books?id=gt7UAAAAMAAJ |year=1908 |work=Boyd\'s Directory of Harrisburg |title=(Advertisement for the Association of American Directory Publishers)}}</ref> It changed its name to the \'\'\'Association of North American Directory Publishers\'\'\' in 1919.<ref>{{citation |title=Printers\' Ink |location=NY |date=September 11, 1919 }}</ref><ref>{{cite web |url=http://www.worldcat.org/identities/lccn-no2011-132108 |title=Association of North American Directory Publishers |work=WorldCat |publisher=[[OCLC]] |accessdate=April 5, 2013 }}</ref> It has held annual meetings starting in 1899 and has published the \'\'Directory Bulletin\'\'.<ref>{{citation |title=Directory Bulletin |volume =1 |year=1901 |location=Milwaukee |publisher=Association of American Directory Publishers |url=https://books.google.com/books?id=2zTZAAAAMAAJ }}</ref> Officers have included George W. Overton and [[Ralph Lane Polk]].<ref name=members1921 /> Among the members in the 1920s:<ref name=members1921>{{citation |chapter=Members of Association of North American Directory Publishers |year=1921 |url=https://books.google.com/books?id=qG4UAAAAYAAJ&pg=PA480 |title=Manchester Directory |publisher=Sampson & Murdock Co. }}</ref>\n\n{{Col-begin}}\n{{Col-1-of-3}}\n* Action Pages\n* Atkinson Erie Directory Company\n* Atlanta City Directory Company\n* W.H. Boyd Company\n* Burch Directory Company\n* Caron Directory Company\n* Chicago Directory Company\n* J.W. Clement Company\n* Cleveland Directory Company\n* Connelly Directory Company\n* Fitzgerald Directory Company\n* Gate City Directory Company\n* Hartford Printing Company\n* Henderson Directories Ltd.\n* Hill Directory Company\n{{Col-2-of-3}}\n* C.E. Howe Company\n* Kimball Directory Company\n* Leshnick Directory Company\n* Los Angeles Directory Company\n* John Lovell & Son Ltd.\n* McCoy Directory Company\n* H.A. Manning Company\n* Maritime Directory Company\n* Henry M. Meek Publishing Company\n* Might Directories Ltd.\n* Minneapolis Directory Company\n* Piedmont Directory Company\n* [[R.L. Polk & Company]]\n{{Col-3-of-3}}\n* Polk-Gould Directory Company\n* Polk-Husted Directory Company\n* Polk-McAvoy Directory Company\n* Polk\'s Southern Directory Company\n* Portland Directory Company\n* Price & Lee Company\n* W.L. Richmond\n* Roberts Bros Company\n* Sampson & Murdock Company\n* Soards Directory Company \n* Utica Directory Publishing Company\n* Williams Directory Company<ref>{{Citation |publisher = Williams Directory Co. |publication-place = Cincinnati, Ohio |author = A.V. Williams |url = http://hdl.handle.net/2027/nyp.33433082423645 |title = The development and growth of city directories |publication-date = 1913 }}</ref>\n* John F. Worley Directory Company\n* Wright Directory Company\n{{Col-end}}\n\nIn 1992 the group renamed itself the "Association of Directory Publishers."<ref name=associations2012 />\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* {{citation |title=Pacific Bell fends off feisty competitors seeking confidential Yellow Pages data |work=San Francisco Business Times |date=March 1, 1991 }}\n\n==External links==\n* [http://www.adp.org/ Official website]\n* {{cite web |url=http://www.adp.org/committees |title=Our Industry: Timeline |publisher=Association of Directory Publishers }}\n\n[[Category:Organizations established in 1898]]\n[[Category:1898 establishments in the United States]]\n[[Category:Professional associations based in the United States]]\n[[Category:Publishing organizations]]\n[[Category:Directories]]\n[[Category:Yellow pages]]']
['Women Environmental Artists Directory', '41307188', 'The \'\'\'Women Environmental Artists Directory\'\'\' (WEAD) focuses on promoting environmental and [[Social justice]] art. <ref>{{cite web | url=http://weadartists.org/about-us | title=About Us | publisher=Women Environmental Artists Directory | accessdate=2013-08-12}}</ref> WEAD was founded in 1996 by Jo Hanson, Susan Leibovitz Steinman and Estelle Akamine.<ref>{{cite web | url=http://greenmuseum.org/generic_content.php?ct_id=285 | title=JO HANSON: Pioneering Environmental Artist Dies in San Francisco | publisher=Green Museum | accessdate=2013-08-12 | last=Leibovitz Steinman | first=Susan}}</ref> \n\nWEAD has been listed among the best projects relating to [[Environmental art]],<ref>{{cite web | url=http://www.andrew.cmu.edu/user/md2z/greenarts/artprojects.html | title=Green Arts Web: Artists & Projects | publisher=Carnegie Mellon University | accessdate=2013-08-12}}</ref> and has sponsored a number of exhibits about activist eco art.<ref>{{cite web|title=Earthly Concerns, Activist EcoArt curated by WEAD|url=http://www.usfca.edu/uploadedFiles/Destinations/Library/thacher/archive/Earthly%20Concerns.pdf|publisher=University of San Francisco|accessdate=2013-08-12}}</ref> <ref>{{cite web|title=CONVERGENCE/DIVERGENCE SYMPOSIUM|url=http://www.losmedanos.edu/art/archive.aspx|publisher=Los Medanos College|accessdate=2013-08-12}}</ref> <ref>{{cite web|title=WEAD East I Women and the Environment|url=http://www.kbcc.cuny.edu/artgallery/Pages/ewead.aspx|publisher=Kingsborough Community College|accessdate=2013-08-12}}</ref> \n\nOne of the co-founders, Ms. Steinman, is considered a leader in the eco art field and has participated in roundtables and artists in residences programs,<ref>{{cite web|title=Artist Talk with Susan Steinman|url=http://goddard.edu/news-events/events/artist-talk-susan-steinman|publisher=Goddard College|accessdate=2013-08-12}}</ref> <ref>{{cite web|title=Eco Art Video Salon|url=http://www.berkeleyartcenter.org/programs_Q4-2010.html|publisher=Berkeley Arts Center|accessdate=2013-08-12}}</ref> and is listed in the sculptor directory of the International Sculpture Center.<ref>{{cite web|title=Sculptor Susan Leibovitz Steinman|url=http://www.sculpture.org/portfolio/sculptorPage.php?sculptor_id=1000451|publisher=International Sculpture Center|accessdate=2013-08-12}}</ref>  Another co-founder, Jo Hanson, was instrumental in founding an EPA Artist in Residence Program, which was aimed at educating the public about recycling. Another of the WEAD co-founders, Estelle Akamine, was also one of the artists in residence.<ref>{{cite web|title=Recology’s Artist in Residence|url=http://www.epa.gov/wastes/conserve/smm/web-academy/2011/feb11.htm|publisher=US Environmental Protection Agency|accessdate=2013-08-12}}</ref> Ms. Akamine\'s work has also been featured at the Museum of Craft and Folk Art museum store<ref>{{cite web|title=Museum Store|url=http://www.mocfa.org/store/artists.htm|publisher=Museum of Craft and Folk Art|accessdate=2013-08-12}}</ref> and has lectured at a textile lecture series.<ref>{{cite web|last=Valoma|first=Deborah|title=Textiles Lecture Series Archive|url=https://www.cca.edu/news/2012/08/27/textiles-lecture-archive|publisher=California College for the Arts|accessdate=2013-08-12}}</ref> All three co-founders were featured in a discussion about women artists of the American West whose art was about current social concerns.<ref>{{cite web|last=Cohn|first=Terri|title=Nature, Culture and Public Space|url=http://www.cla.purdue.edu/WAAW/Cohn/index.html|publisher=Purdue University|accessdate=2013-08-12}}</ref> \n\nThe directory lists a wide variety of [[Woman artists]], such as [[Marina DeBris]], a [[trashion]] artist, [[Betty Beaumont]], often called a pioneer of environmental art, and Shai Zakai.\n\nWEAD also published a magazine, which focuses on such topics as dirty water, and the legacy of atomic energy. A recent guest editor was Dr. Elizabeth Dougherty, founder of Wholly H2O, and speaker at events such as Pacific Gas and Electric Company conference on [[Water conservation]]<ref>{{cite web|title=2010 Water Conservation Showcase Speakers Save Water by Going Paperless!|url=http://www.pge.com/pec/water/presentations.shtml|publisher=Pacific Gas and Electric Company|accessdate=2013-08-12}}</ref> and Toulumne County\'s conference on [[greywater]].<ref>{{cite web|title=Greywater in California:  Designing, Managing, Monitoring|url=http://portal.co.tuolumne.ca.us/psp/ps/TUP_HS_ENVIR_HEALTH/ENTP/c/TU_DEPT_MENU.TUOCM_HTML_COMP.GBL?action=U&CONTENT_PNM=EMPLOYEE&CATGID=2651|publisher=TUOLUMNE COUNTY ENVIRONMENTAL HEALTH|accessdate=2013-08-12}}</ref> Linda Weintraub was a contributor to a recent issue of the WEAD magazine. Ms. Weintraub is the author of well known books on art and activism<ref>{{cite web|title=Drop Dead Gorgeous: Beauty and the Aesthetics of Activism|url=http://artsci.ucla.edu/?q=events/art-activism-linda-weintraub|publisher=UCLA Art Sci Center|accessdate=2013-08-12}}</ref> such as "To Life!"<ref>{{cite web|title=To Life! Eco Art in Pursuit of a Sustainable Planet|url=http://www.ucpress.edu/book.php?isbn=9780520273627|publisher=University of California Press|accessdate=2013-08-12}}</ref>  and is an eco art activist.<ref>{{cite web|last=Lambe|first=Claire|title=An Interview with Linda Weintraub\u2009–\u2009Curator of “Dear Mother Nature: Hudson Valley Artists 2012” at The Dorsky|url=http://www.rollmagazine.com/an-interview-with-linda-weintraub-%E2%80%93-curator-of-%E2%80%9Cdear-mother-nature-hudson-valley-artists-2012%E2%80%9D-at-the-dorsky/|publisher=Roll Magazine, Mark Gruber Gallery|accessdate=2013-08-12}}</ref> \n\n==References==\n{{reflist}}\n\n[[Category:1996 introductions]]\n[[Category:Directories]]\n[[Category:Environmental art]]\n[[Category:Women artists]]']
['Address book', '442661', '{{redirect|Address Book|the Apple Inc. software|Address Book (application)}}\n{{unreferenced|date=February 2012}}\n[[File:Address book 1.jpg|thumb|A blank page in a typical paper address book]]\n[[File:Jack L. Warner\'s address book - National Museum of American History - DSC06088.JPG|thumb|[[Jack L. Warner]]\'s address book on display at the [[National Museum of American History]]]]\nAn \'\'\'address book\'\'\' or a \'\'\'name and address book\'\'\' (\'\'\'NAB\'\'\') is a [[book]] or a [[database]] used for storing entries called \'\'\'contacts\'\'\'. Each contact entry usually consists of a few standard [[Field (computer science)|fields]] (for example: first name, last name, company name, [[address (geography)|address]], [[telephone]] number, [[e-mail]] address, [[fax]] number, [[mobile phone]] number). Most such systems store the details in alphabetical order of people\'s names, although in [[paper]]-based address books entries can easily end up out of order as the owner inserts details of more individuals or as people move. Many address books use small [[ring binder]]s that allow adding, removing and shuffling of pages to make room.\n\n== Little black book ==\nA related term that has entered the popular [[lexicon]] is \'\'\'little black book\'\'\' (or simply \'\'\'black book\'\'\'). Such books are used as [[courtship|dating]] guides, listing people who the owner has dated in the past or hopes to in the future, and details of their various relationships. More explicit variations are guides for [[sexual partner]]s. It is unclear how prevalent this is in practice or when it originated, though such books have been mentioned in many pieces of [[popular culture]]. For example, the 1953 film version of \'\'[[Kiss Me, Kate]]\'\' features a musical scene in which [[Howard Keel]]\'s character laments the loss of the social life he enjoyed before marriage, naming numerous female romantic encounters while perusing a miniature black book. More recently, the mid-2000s [[Guinness Brewmasters]] advertising campaign features the "little black book" as an invention of one of the brewmasters.\n\n== Software address book ==\n[[File:X-office-address-book.svg|thumb|A digital address book icon]]\nAddress books can also appear as [[software]] designed for this purpose, such as the [[Address Book (application)|"Address Book"]] application included with [[Apple Inc.]]\'s [[Mac OS X]]. Simple address books have been incorporated into [[e-mail]] software for many years, though more advanced versions have emerged in the 1990s and beyond; and also in [[mobile phone]]s.\n\nA [[personal information manager]] (PIM) integrates an address book, [[calendar]], task list, and sometimes other features.\n\nEntries can be imported and exported from the software in order to transfer them between programs or computers. The common file formats for these operations are:\n* [[LDAP Data Interchange Format|LDIF]] (*.ldif, *.ldi)\n* Tab delimited (*.tab, *.txt)\n* [[Comma-separated values|Comma-separated]] (*.csv)\n* [[vCard]] (*.vcf)\n\nIndividual entries are frequently transferred as [[vCard]]s (*.vcf), which are roughly comparable to physical [[business card]]s. And some software applications like [[Lotus Notes]] and Open Contacts can handle a vCard file containing multiple vCard records.\n\n== Online address book ==\nAn online address book typically enables users to create their own web page (or profile page) which is then indexed by search engines like Google and Yahoo. This in turn enables users to be found by other people via a search of their name and then contacted via their web page containing their personal information. Ability to find people registered with online address books via search engine searches usually varies according to the commonness of the name and the amount of results for the name. Typically users of such systems can synchronize their contact details with other users that they know to ensure that their contact information is kept up to date.\n\n== Network address book ==\nCurrently, most people have many different address books: their email accounts, their mobile phone, and the "friends lists" on their social networks. A network address book allows them to organize and manage all of their address books through a single interface and share their contacts across their different address books and social networks.\n\n== See also ==\n{{colbegin|3}}\n* [[Calendaring software]]\n* [[Contact list]]\n* [[Mobile social address book]]\n* [[Personal information manager]]\n* [[Rolodex]]\n* [[Suvorin directories]]\n* [[Telephone directory]]\n* [[Windows Address Book]]\n{{colend}}\n\n{{Authority control}}\n[[Category:Office equipment]]\n[[Category:Directories]]']
['Category:Web directories', '826434', '{{Cat main|Web directory|List of web directories}}\nA [[web directory]] is a [[directory (databases)|directory]] on the [[World Wide Web]] that specializes in [[hyperlink|linking]] to other [[web site]]s and categorizing those links. This category includes online web directories.\n\n\n[[Category:Websites|Directories]]\n[[Category:Indexes]]\n[[Category:Directories]]\n[[Category:Online services|Directories]]']
['Business directory', '1725756', "{{unreferenced|date=June 2009}}\n\n[[File:PigotDirectory1839Kent.jpg|thumb|An example page from [[Pigot's Directory|Pigot's 1839 directory]] of businesses in the counties of Kent, Surrey and Sussex in England.]]\nA '''business directory''' is a website or [[print media|printed]] listing of [[information]] which lists all businesses within some category. Businesses can be categorized by business, location, activity, or size. Business may be compiled either manually or through an automated online search software.  Online [[yellow pages]] are a type of business directory, as is the traditional [[phone book]].\n\nThe details provided in a business directory varies from business to business. They may include the business name, addresses, telephone numbers, location, type of service or products the business provides, number of employees, the service region and any [[professional association]]s. Some directories include a section for user reviews, comments, and feedback. Business directories in the past would take a printed format but have recently been upgraded to websites due to the advent of the internet.\n\nMany business directories offer complimentary listings in addition to the premium options. There are many business directories and some of these have moved over to the [[internet]] and away from printed format. Whilst not being [[search engine]]s, business directories often have a search facility.\n\n== Formats ==\nBusiness directories can be in either [[hard copy]] or in [[Digital formats|digital format]]. Ease of use and distribution means that many trade directories have digital version.\n\nOnline Business Directories vary in quality and content. There is a balance between professional advertising, value for money and quality of service. Business owners are looking for ROI, web traffic, exposure for their business, plus [[Search engine optimization|SEO]] benefits of [[Backlink|backlinks]].\n\n==See also==\n*[[Web directory]]\n*[[Kelly's Directory]]\n*[[Surplus Record Machinery & Equipment Directory]]\n\n{{DEFAULTSORT:Business Directory}}\n[[Category:Business]]\n[[Category:Directories]]"]
['Directory of Open Access Journals', '2241822', '{{Infobox website\n|name            = Directory of Open Access Journals\n|logo            = DOAJ logo.jpg\n|logocaption     =\n|screenshot      =\n|collapsible     =\n|collapsetext    =\n|caption         =\n|url             = {{URL|https://doaj.org/}}\n|slogan          =\n|commercial      =No\n|type            =\n|registration    =\n|language        =English\n|content license =\n|owner           =\n|author          =\n|launch date     = <!--{{Start date and age|YYYY|MM|DD|df=yes/no}}-->\n|alexa           = 58,591 (as of October 2015)<ref>{{cite web|title=Ranking for Doaj.org|url=http://www.alexa.com/siteinfo/doaj.org|accessdate=2015-10-20|work=[[Alexa.com]]}}</ref>\n|revenue         =\n|current status  =Online\n|footnotes       =\n}}\n\nThe \'\'\'Directory of Open Access Journals\'\'\' (\'\'\'DOAJ\'\'\') is a [[website]] that lists [[open access journal]]s and is maintained by Infrastructure Services for Open Access (IS4OA).<ref>{{cite web |url=http://www.is4oa.org/ |title=Infrastructure Services for Open Access |publisher=Infrastructure Services for Open Access C.I.C. |accessdate=2013-03-05}}</ref> The project defines open access journals as [[scientific journal|scientific]] and [[academic journal|scholarly journal]]s that meet high quality standards by exercising [[peer review]] or editorial quality control and "use a funding model that does not charge readers or their institutions for access."<ref name=aboutdoaj/> The [[Budapest Open Access Initiative]]\'s definition of [[Open access (publishing)|open access]] is used to define required rights given to users, for the journal to be included in the DOAJ, as the rights to "read, download, copy, distribute, print, search, or link to the full texts of these articles".<ref name=aboutdoaj/><ref>The BOAI definition is at "[http://www.earlham.edu/~peters/fos/boaifaq.htm#openaccess Budapest Open Access Initiative: Frequently Asked Questions]".</ref> The aim of DOAJ is to "increase the visibility and ease of use of open access scientific and scholarly journals thereby promoting their increased usage and impact."<ref name=aboutdoaj>{{cite web |url= http://doaj.org/about |title=About |work=Directory of Open Access Journals |accessdate=2015-04-14}}</ref>\n\nAs of March 2015, the database contained records for 10,000 journals.<ref>{{cite web |url= http://sparc.arl.org/blog/doaj-introduces-new-standards |title=Directory of Open Access Journals introduces new standards to help community address quality concerns |first=Caralee |last=Adams |date=5 March 2015 |publisher=SPARC |accessdate=2015-04-14}}</ref> An average of four journals were being added each day in 2012.<ref>{{cite web|title=DOAJ Statistics |url= http://www.doaj.org/doaj?func=newTitles&uiLanguage=en&fromDate=1970-01-01+00:00:00&orderedBy=J.first_added |archiveurl= https://web.archive.org/web/20120404125652/http://www.doaj.org/doaj?func=newTitles&uiLanguage=en&fromDate=1970-01-01+00:00:00&orderedBy=J.first_added |archivedate= 2012-04-04 |accessdate=2013-01-06 |work=Directory of Open Access Journals}}</ref>\n\nIn May 2016, DOAJ announced that they had removed approximately 3,300 journals from their database to provide better reliability on the content listed on it.<ref>{{cite journal |last=Marchitelli|first= Andrea|last2=Galimberti |first2=Paola |last3=Bollini |first3=Andrea|last4=Mitchell |first4=Dominic|date= January 2017 |title=\nHelping journals to improve their publishing standards: a data analysis of DOAJ new criteria effects |url=http://leo.cineca.it/index.php/jlis/article/view/12052|journal= JLIS.it|volume=8 |issue=1 |pages= 39-49|doi=10.4403/jlis.it-12052|access-date=2017-01-22 }}</ref>\n\nThe journals that were removed can reapply as part of an ongoing procedure. <ref>{{Cite web|url=https://doajournals.wordpress.com/2016/05/09/doaj-to-remove-approximately-3300-journals/|title=DOAJ to remove approximately 3300 journals|last=DOAJ|date=2016-05-09|website=News Service|access-date=2016-09-24}}</ref> As of September 2016, the database now contains 9,216 journals. <ref>{{Cite web|url=https://doaj.org/|title=Directory of Open Access Journals|last=DOAJ|access-date=2016-09-24}}</ref>\n\n==History==\nThe [[Open Society Institute]] funded various open access related projects after the Budapest Open Access Initiative; the Directory was one of those projects.<ref>{{cite book|last=Crawford|first=Walt|title=Open access : what you need to know now|publisher=American Library Association|location=Chicago|isbn=9780838911068|page=13}}</ref> The idea for the DOAJ came out of discussions at the first Nordic Conference on\nScholarly Communication in 2002, [[Lund University]] became the organization to set up and maintain the DOAJ.<ref>{{Cite journal | last1 = Hedlund | first1 = T. | last2 = Rabow | first2 = I. | doi = 10.1087/2009303 | title = Scholarly publishing and open access in the Nordic countries | journal = Learned Publishing | volume = 22 | issue = 3 | pages = 177-186| year = 2009 | pmid =  | pmc = }}</ref> It continued to do so  until January 2013, when Infrastructure Services for Open Access (IS4OA) took over.\n\n== See also ==\n* [[List of open-access journals]]\n*[[Open Access Scholarly Publishers Association]]\n\n== References ==\n<references/>\n\n== External links ==\n* {{official website|https://doaj.org/}}\n{{Open access navbox}}\n\n[[Category:Open access (publishing)]]\n[[Category:Open access journals| ]]\n[[Category:Directories]]']
['Lighthouse Directory', '47962147', '#REDIRECT [[Lists of lighthouses and lightvessels]]\n{{R from merge}}\n[[Category:Directories]]']
['VisualRank', '17303714', '\'\'\'VisualRank\'\'\' is a system for [[image retrieval|finding]] and ranking images by analysing and comparing their content, rather than searching image names, Web links or other text.  [[Google]] scientists made their VisualRank work public in a paper describing applying [[PageRank]] to Google image search at the International World Wide Web Conference in [[Beijing]] in 2008.\n<ref name=Jing08>\n{{cite journal\n | author = Yushi Jing and Baluja, S.\n | title = VisualRank: Applying PageRank to Large-Scale Image Search\n | journal = Pattern Analysis and Machine Intelligence, IEEE Transactions on\n | year = 2008\n | volume = 30\n | number = 11\n | pages = 1877–1890\n | ISSN = 0162-8828\n | doi = 10.1109/TPAMI.2008.121}}.\n</ref>\n\n<blockquote>\nWe cast the image-ranking problem into the task of identifying "authority" nodes on an inferred visual similarity graph and propose VisualRank to analyze the visual link structures among images. The images found to be "authorities" are chosen as those that answer the image-queries well. \n</blockquote>\n\n==Methods==\nBoth [[computer vision]] techniques and [[locality-sensitive hashing]] (LSH) are used in the VisualRank [[algorithm]].  Consider an image search initiated by a text query.  An existing search technique based on image metadata and surrounding text is used to retrieve the initial result candidates ([[PageRank]]), which along with other images in the index are clustered in a [[Graph (data structure)|graph]] according to their similarity (which is precomputed).  [[Centrality]] is then measured on the clustering, which will return the most canonical image(s) with respect to the query.  The idea here is that agreement between users of the web about the image and its related concepts will result in those images being deemed more similar.  VisualRank is defined iteratively by <math>VR = S^* \\times VR</math>, where <math>S^*</math> is the image similarity matrix.  As matrices are used, [[eigenvector centrality]] will be the measure applied, with repeated multiplication of <math>VR</math> and <math>S^*</math> producing the [[eigenvector]] we\'re looking for.  Clearly, the image similarity measure is crucial to the performance of VisualRank since it determines the underlying graph structure.\n\nThe main VisualRank system begins with local feature vectors being extracted from images using [[scale-invariant feature transform]] (SIFT).  Local feature descriptors are used instead of color histograms as they allow similarity to be considered between images with potential rotation, scale, and perspective transformations. Locality-sensitive hashing is then applied to these feature vectors using the [[locality-sensitive hashing#methods|p-stable distribution scheme]].  In addition to this, LSH amplification using AND/OR constructions are applied.  As part of the applied scheme, a [[Gaussian distribution]] is used under the [[L2 norm#Euclidean norm|<math>l_2</math> norm]].\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.nytimes.com/2008/04/28/technology/28google.html?adxnnl=1&ref=business&adxnnlx=1210140241-DOwaJr/5AjMPCYJDerw++Q New York Times article]\n*[http://tech.slashdot.org/article.pl?sid=08/04/28/1852254&from=rss Slashdot article]\n[[Category:Internet search]]\n[[Category:Image processing]]']
['Search link optimization', '23265516', '{{orphan|date =August 2009}}\n\n\'\'\'Search Link Optimization\'\'\' (\'\'\'SLO\'\'\') is a process by which internal and external incoming links are optimized for [[search engine]] algorithms to determine the relevance of [[web page]] content.  Relevant [[anchor text]] integration, text that contains keywords for optimizing a web page, is key to this process.\n\n==Inbound links, outbound links, internal links==\n\nInbound and outbound links are those that hyperlink two independent web pages together whereas inbound links would hyperlink domain “A” to domain “B” and outbound links would hyperlink domain “B” to domain “A.”\n\nInbound and outbound links are essential to web page visibility often enhancing web page relevance, ranking, & placement.  There are few instances where inbound links would be discouraged.  Outbound links however should be given sparingly and should only link material to other material of same or similar relevance.  Often, developers will utilize a [[nofollow]] tag used mostly to further optimize hyperlinks by “instructing” search engines not to distribute any [[PageRank]] from the hyperlink.  An example of a nofollow tag might be:\n\n<syntaxhighlight lang="html5">\n<a href="http://www.wikipedia.com" title="Wikipedia Online Encyclopedia" rel="nofollow">Wikipedia, Online Encyclopedia</a>\n</syntaxhighlight>\n\n[[Internal link]]s are those that hyperlink within a single domain. Hyperlinks listed higher within the source code typically gain greater relevance. Some developers use a  practice known as PageRank Sculpting by using the nofollow tag to adjust the flow of PageRank.\n\n==Proper coding of hyperlinks==\n\nThe mere presence of hyperlinks within a web page may not yield desired optimization results. For example, when coding a web page about "blue widgets," anchor text containing links referencing "red widgets" may alter relevance which can result in gain/loss ranking scenario where “blue widgets” gains while “red widgets” actually loses position.  Moreover, another result can be a complete loss of overall web page ranking altogether for either keyword.\n\nA properly coded keyword contains these elements: Relevant anchor text, relevant keyword titling, and compliance-based [[Character encodings in HTML|HTML code]] structures.  Additionally, use of titles that are linked within the [[HTML element|title tag]] is also a recommended practice provided the title tag for the web page has also been properly optimized for the desired keyword(s).  Below is the proper coding of a hyperlink:\n\n<syntaxhighlight lang="html5">\n<a href="http://www.wikipedia.com" title="Wikipedia Online Encyclopedia">Wikipedia, Online Encyclopedia</a>\n</syntaxhighlight>\n\n==External links and references==\nThe footnotes below are given in support of the statements above. Because some facts are proprietary secrets held by private companies and therefore not documented in journals, such facts are reasoned from facts that are public.\n* [http://www.mattcutts.com/blog/pagerank-sculpting/ PageRank Sculpting]\n* [http://googleblog.blogspot.com/2005/01/preventing-comment-spam.html Prevent Comment Spam]\n* [http://www.textlinks2u.com/search_engine_optimization.html Search Engine Optimization]\n\n{{DEFAULTSORT:Search Link Optimization}}\n[[Category:Internet search]]']
['Google (verb)', '375665', '{{cleanup|reason= Substantive entry outdated, summary does not refer to content in body of entry, body reads like dictionary entry for \'ungoogleable\')|date=October 2016}}\n\n{{about|the verb|the use of the verb in cricket|Googly|other uses|Google (disambiguation)}}\n{{redirect|Googled|the book of the same name|Googled: The End of the World as We Know It}}\n{{wiktionary|google}}\nAs a result of the increasing popularity and dominance of the [[Google Search|Google search engine]],<ref>{{cite web |last=Burns |first=Enid |date=June 19, 2007 |url=http://searchenginewatch.com/showPage.html?page=3626208 |title=Top 10 Search Providers, April 2007 |publisher=SearchEngineWatch.com |accessdate=2007-08-11 }}</ref> usage of the [[transitive verb]]<ref>{{cite web|url=http://www.merriam-webster.com/dictionary/google |title=Google - Definition and More from the Free Merriam-Webster Dictionary |publisher=Merriam-webster.com |date= |accessdate=2011-09-19}}</ref> \'\'\'to google\'\'\' (also spelled \'\'\'Google\'\'\') grew ubiquitously. The [[neologism]] commonly refers to searching for information on the [[World Wide Web]], regardless of which [[search engine]] is used.<ref>{{cite web|url=http://www.thelinguafile.com/2013/02/how-google-became-verb.html |title=How Google Became a Verb |publisher=The Lingua File - The Language Blog |date= |accessdate=2013-11-22}}</ref> The [[American Dialect Society]] chose it as the "most useful word of 2002."<ref>{{cite web |date=January 13, 2003 |url=http://www.americandialect.org/index.php/amerdial/2002_words_of_the_y/ |title=2002 Words of the Year |publisher=American Dialect Society |accessdate=2007-08-11 }}</ref> It was added to the \'\'[[Oxford English Dictionary]]\'\' on June 15, 2006,<ref>Bylund, Anders. "[http://www.fool.com/investing/dividends-income/2006/07/05/to-google-or-not-to-google.aspx To Google or Not to Google]." \'\'[[The Motley Fool]].\'\' July 5, 2006. Retrieved on March 28, 2007.</ref> and to the eleventh edition of the \'\'[[Merriam-Webster|Merriam-Webster Collegiate Dictionary]]\'\' in July 2006.<ref>Harris, Scott D. "[http://www.mercurynews.com/mld/mercurynews/business/14985574.htm Dictionary adds verb: to google]." \'\'[[San Jose Mercury News]].\'\' July 7, 2006. Retrieved on July 7, 2006.</ref>\n\n==Etymology==\nThe first recorded usage of \'\'google\'\' used as a [[participle]], thus supposing an [[intransitive verb]], was on July 8, 1998, by [[Google]] co-founder [[Larry Page]] himself, who wrote on a mailing list: "Have fun and keep googling!"<ref>{{cite web |last=Page |first=Larry |authorlink=Larry Page |date=July 8, 1998 |url=http://www.egroups.com/group/google-friends/3.html |title=Google Search Engine: New Features |publisher=Google Friends Mailing List |accessdate=2007-08-06 |archiveurl=https://web.archive.org/web/19991009052012/http://www.egroups.com/group/google-friends/3.html |archivedate=1999-10-09 }}</ref> Its earliest known use (as a transitive verb) on American television was in the "[[Help (Buffy episode)|Help]]" episode of \'\'[[Buffy the Vampire Slayer (TV series)|Buffy the Vampire Slayer]]\'\' (October 15, 2002), when [[Willow Rosenberg|Willow]] asked [[Buffy Summers|Buffy]], "Have you googled her yet?"<ref>{{Cite book |title=Digital Wars: Apple, Google, Microsoft and the Battle for the Internet |last=Arthur |first=Charles |year=2012 |publisher=Kogan Page Publishers |location= |isbn= |page=48 |url=https://books.google.com/books?id=IXiYi-dQenEC&pg=PA48#v=onepage&q&f=false |accessdate=January 2, 2013 }}</ref>\n<!-- Fearing the [[generic trademark|genericizing]] and potential loss of its [[trademark]], Google has discouraged use of the word as a verb, particularly when used as a synonym for general web searching. --> \nOn February 23, 2003,<ref>{{cite web |last=McFedries |first=Paul |date=February 23, 2003 |url=http://listserv.linguistlist.org/cgi-bin/wa?A2=ind0302D&L=ads-l&P=R2450 |title=Google trademark concerns |publisher=American Dialect Society Mailing List |accessdate=2007-08-11 }}</ref> the company sent a [[cease and desist]] letter to [[Paul McFedries]], creator of [[Word Spy]], a website that tracks [[neologism]]s.<ref>Duffy, Jonathan. "[http://news.bbc.co.uk/2/hi/uk_news/3006486.stm Google calls in the \'language police\']." \'\'[[BBC News]].\'\' June 20, 2003. Retrieved on July 7, 2006.</ref> In an article in the \'\'[[Washington Post]]\'\', Frank Ahrens discussed the letter he received from a Google lawyer that demonstrated "appropriate" and "inappropriate" ways to use the verb "google".<ref>{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2006/08/04/AR2006080401536.html|title=So Google Is No Brand X, but What Is \'Genericide\'?|author=Frank Ahrens|date=2006-08-05|accessdate=2006-08-05|publisher=Washington Post}}</ref> It was reported that, in response to this concern, [[lexicographer]]s for the \'\'Merriam-Webster Collegiate Dictionary\'\' lowercased the actual entry for the word, \'\'google\'\', while maintaining the capitalization of the search engine in their definition, "to use the [[Google search|Google search engine]] to seek online information" (a concern which did not deter the Oxford editors from preserving the history of both "cases").<ref>Noon, Chris. "[http://www.forbes.com/2006/07/06/page-brin-google-cx_cn_0706autofacescan01.html Brin, Page See \'Google\' Take Its Place In Dictionary]." \'\'[[Forbes]].\'\' July 6, 2006. Retrieved on July 7, 2006.</ref> On October 25, 2006, Google sent a request to the public requesting that "You should please only use \'Google\' when you’re actually referring to Google Inc. and our services."<ref>{{cite web |last=Krantz |first=Michael |date=October 25, 2006 |url=http://googleblog.blogspot.com/2006/10/do-you-google.html |title=Do you "Google?" |publisher=The Official Google Blog |accessdate=2007-08-11 }}</ref>\n\n==Ungoogleable==\n{{main|Censorship by Google|Deep Web (search indexing)}}\n{{wiktionary|unGoogleable}}\nUngoogleable, (or unGoogleable) is a term for something that cannot be "googled" – i.e. it is a term for something that cannot be found easily using the [[Google Search]] [[web search engine]]. It is increasingly used to mean something that cannot be found using any web search engine.<ref>{{cite news| url=http://www.bbc.co.uk/news/magazine-21956743 | title=Who, What, Why: What is \'ungoogleable\'? | publisher=[[BBC]] |work=[[BBC News Magazine]] | date=27 March 2013 | accessdate=5 April 2013 }}</ref>\n\nIn 2013 the [[Swedish Language Council]] attempted to include the [[Swedish language|Swedish]] version of the word ("\'\'[[:sv:Ogooglebar|ogooglebar]]\'\'") in its list of new words, but Google objected to the definition not being specifically related to Google, and the Council was forced to briefly remove it to avoid a legal confrontation with Google.<ref>{{cite news| url=http://www.bbc.co.uk/news/world-europe-21944834 | title=Google gets ungoogleable off Sweden\'s new word list | first=Sean | last=Fanning | publisher=[[BBC]] | work=[[BBC News]] | date=26 March 2013 | accessdate=5 April 2013 }}</ref><ref>{{cite news| url=http://www.independent.co.uk/news/world/europe/ungoogleable-removed-from-list-of-swedish-words-after-row-over-definition-with-google-8550096.html | title=\'Ungoogleable\' removed from list of Swedish words after row over definition with Google: California based search engine giant asked Swedish to amend definition | first=Rob | last=Williams | newspaper=[[The Independent]] | date=26 March 2013 | accessdate=5 April 2013 }}</ref>\n\n===Causes===\nGoogle Search generally ignores punctuation and [[letter case]] even when using the "quotation" operator to denote exact searches.<ref>[https://support.google.com/websearch/answer/2466433?hl=en Search operators - Search Help]</ref> Thus, Google may not be able to differentiate terms for which punctuation impacts meaning{{--}}for example, "man eating chicken" and "man-eating chicken" (the former meaning a human who is consuming chicken meat and the latter a chicken that eats humans). Because Google treats upper and lower case letters as one and the same, it also is unable to differentiate between the pronoun \'\'[[he]]\'\' and the surname \'\'[[He (surname)|He]]\'\', which, when combined with its disregard for punctuation, could bury results for an obscure person named <code>"Thomas He"</code> among results such as:\n:<q>... Assisted by \'\'\'Thomas, he\'\'\' was able to provide incontrovertible proof of this theory, and in so doing, he gained wide recognition in the medical ...</q><ref>[https://www.google.com/webhp?hl=en&sa=X#hl=en&q=%22Thomas+He%22 "Thomas He" - Google Search]</ref>\n\nThe above also exemplifies how Google\'s [[PageRank]] algorithm, which sorts results by "importance", could also cause something to become ungoogleable: results for those with the 17th most common Chinese surname<ref>{{cite web|url=http://cdn.theatlantic.com/newsroom/img/posts/2013/10/chinassurnames/0886ab335.jpg |title=China\'s Surnames |publisher=Cdn.theatlantic.com |accessdate=2016-07-12}}</ref> are difficult to separate from results containing the 16th [[most common words in English|most common word in English]]. In other words, a specific subject may be ungoogleable because its results are a [[wikt:needle in a haystack|needle in a haystack]] of results for a more "important" term.\n\n==See also==\n{{Portal|Internet}}\n* [[grep#Usage as a verb|grep]]<!--lowercase-->\n* [[Swedish_Language_Council#Controversy|Ogooglebar, Swedish for Ungoogleable]]\n* [[Photo manipulation#Photoshopping|Photoshop (verb)]], a similar neologism referring to digital photo editing\n\n==References==\n{{reflist|30em}}\n\n{{Google Inc.}}\n\n{{DEFAULTSORT:Google (Verb)}}\n[[Category:Google]]\n[[Category:Verbs]]\n[[Category:Internet terminology]]\n[[Category:Internet search]]\n[[Category:Words coined in the 1990s]]\n[[Category:Computer-related introductions in 1998]]\n\n[[ja:Google#派生語]]\n[[ru:Google (компания)#to google]]']
['URL redirection', '636686', '{{Selfref|For redirection on Wikipedia, see [[Wikipedia:Redirect]].}}\n{{Merge from|Rewrite engine|discuss=Talk:URL redirection#Merge Rewrite engine|date=November 2015}}\n{{refimprove|date=December 2015}}\n\'\'\'URL redirection\'\'\', also called \'\'\'URL forwarding\'\'\', is a [[World Wide Web]] technique for making a [[web page]] available under more than one [[Uniform Resource Locator|URL]] address. When a [[web browser]] attempts to open a URL that has been redirected, a page with a different URL is opened. Similarly, domain redirection or domain forwarding is when all pages in a URL [[Domain name|domain]] are redirected to a different domain, as when [http://www.wikipedia.com wikipedia.com] and [http://www.wikipedia.net wikipedia.net] are automatically redirected to [http://www.wikipedia.org wikipedia.org]. URL redirection is done for various reasons: for [[URL shortening]]; to prevent [[link rot|broken links]] when web pages are moved; to allow multiple domain names belonging to the same owner to refer to a single [[website|web site]]; to guide navigation into and out of a website; for privacy protection; and for less innocuous purposes such as [[phishing]] attacks.\n\n== Purposes ==\nThere are several reasons to use URL redirection:\n\n=== Similar domain names ===\nA user might mistype a URL, for example, "example.com" and "exmaple.com". Organizations often register these "misspelled" domains and redirect them to the "correct" location: example.com. The addresses example.com and example.net could both redirect to a single domain, or web page, such as example.org. This technique is often used to "reserve" other [[top-level domain]]s (TLD) with the same name, or make it easier for a true ".edu" or ".net" to redirect to a more recognizable ".com" domain.\n\n=== Moving pages to a new domain ===\nWeb pages may be redirected to a new domain for three reasons:\n* a site might desire, or need, to change its domain name;\n* an author might move his or her individual pages to a new domain;\n* two web sites might merge.\n\nWith URL redirects, incoming links to an outdated URL can be sent to the correct location. These links might be from other sites that have not realized that there is a change or from bookmarks/favorites that users have saved in their browsers. The same applies to [[search engine]]s. They often have the older/outdated domain names and links in their database and will send search users to these old URLs. By using a "moved permanently" redirect to the new URL, visitors will still end up at the correct page. Also, in the next search engine pass, the search engine should detect and use the newer URL.\n\n=== Logging outgoing links ===\nThe access logs of most web servers keep detailed information about where visitors came from and how they browsed the hosted site.  They do not, however, log which links visitors left by.  This is because the visitor\'s browser has no need to communicate with the original server when the visitor clicks on an outgoing link. This information can be captured in several ways.  One way involves URL redirection.  Instead of sending the visitor straight to the other site, links on the site can direct to a URL on the original website\'s domain that automatically redirects to the real target. This technique bears the downside of the delay caused by the additional request to the original website\'s server. As this added request will leave a trace in the server log, revealing exactly which link was followed, it can also be a privacy issue.<ref>\n{{cite journal\n  | title = Google revives redirect snoopery\n  | journal = blog.anta.net\n  | date = 2009-01-29\n  | url = http://blog.anta.net/2009/01/29/509/\n  | issn = 1797-1993\n  | archiveurl=https://web.archive.org/web/20110817024348/http://blog.anta.net/2009/01/29/509/\n  | archivedate=2011-08-17\n}}</ref> The same technique is also used by some corporate websites to implement a statement that the subsequent content is at another site, and therefore not necessarily affiliated with the corporation. In such scenarios, displaying the warning causes an additional delay.\n\n=== Short aliases for long URLs ===\n{{Main article|URL shortening}}\n\nWeb applications often include lengthy descriptive attributes in their URLs which represent data hierarchies, command structures, transaction paths and session information. This practice results in a URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of [[microblogging]] sites. [[URL shortening]] services provide a solution to this problem by redirecting a user to a longer URL from a shorter one.\n\n=== Meaningful, persistent aliases for long or changing URLs ===\n{{See also|Permalink|PURL|Link rot}}\n\nSometimes the URL of a page changes even though the content stays the same. Therefore, URL redirection can help users who have bookmarks. This is routinely done on Wikipedia whenever a page is renamed.\n\n=== Post/Redirect/Get ===\n{{Main article|Post/Redirect/Get}}\n\nPost/Redirect/Get (PRG) is a [[web development]] [[design pattern]] that prevents some duplicate [[form (web)|form]] submissions, creating a more intuitive interface for [[user agent]]s (users).\n\n=== Device targeting and geotargeting ===\n\nRedirects can be effectively used for targeting purposes like [[device targeting]] or [[geotargeting]]. Device targeting has become increasingly important with the rise of mobile clients. There are two approaches to serve mobile users: Make the website [[responsive web design|responsive]] or redirect to a mobile website version. If a mobile website version is offered, users with mobile clients will be automatically forwarded to the corresponding mobile content. For device targeting, client side redirects or non-cacheable server side redirects are used. Geotargeting is the approach to offer localized content and automatically forward the user to a localized version of the requested URL. This is helpful for websites that target audience in more than one location and/or language. Usually server side redirects are used for Geotargeting but client side redirects might be an option as well, depending on requirements.<ref>{{Cite web |url=https://audisto.com/insights/guides/31/ |title=Redirects & SEO - The Total Guide |accessdate=2015-11-29 |publisher=Audisto}}</ref>\n\n=== Manipulating search engines ===\nRedirects have been used to manipulate search engines with unethical intentions, e.g. [[sneaky redirects]] or [[URL hijacking]]. The goal of misleading redirects is to drive search traffic to landing pages, which do not have enough ranking power on their own or which are only remotely or not at all related to the search target. The approach requires a rank for a range of search terms with a number of URLs that would utilize sneaky redirects to forward the searcher to the target page. This method had a revival with the uprise of mobile devices and device targeting. URL hijacking is an off-domain redirect technique<ref>{{cite web|url=https://www.mattcutts.com/blog/seo-advice-discussing-302-redirects/ |title=SEO advice: discussing 302 redirects |date=4 January 2006 |publisher=Matt Cutts, former Head of Google Webspam Team}}</ref> that exploited the nature of the search engine\'s handling for temporary redirects. If a temporary redirect is encountered, search engines have to decide whether they assign the ranking value to the URL that initializes the redirect or to the redirect target URL. The URL that initiates the redirect may be kept to show up in search results, as the redirect indicates a temporary nature. Under certain circumstances it was possible to exploit this behaviour by applying temporary redirects to well ranking URLs, leading to a replacement of the original URL in search results by the URL that initialized the redirect, therefore "stealing" the ranking. This method was usually combined with sneaky redirects to re-target the user stream from the search results to a target page. Search engines have developed efficient technologies to detect these kind of manipulative approaches. Major search engines usually apply harsh ranking penalties on sites that get caught applying techniques like these.<ref>{{cite web|url=https://support.google.com/webmasters/answer/2721217?hl=en |title=Sneaky Redirects |date=3 December 2015 |publisher=Google Webmaster Guidelines}}</ref>\n\n=== Manipulating visitors ===\nURL redirection is sometimes used as a part of [[phishing]] attacks that confuse visitors about which web site they are visiting.<ref>{{cite web|url=https://www.owasp.org/index.php/Unvalidated_Redirects_and_Forwards_Cheat_Sheet |title=Unvalidated Redirects and Forwards Cheat Sheet |date=21 August 2014 |publisher=Open Web Application Security Project (OWASP)}}</ref> Because modern browsers always show the real URL in the address bar, the threat is lessened. However, redirects can also take you to sites that will otherwise attempt to attack in other ways. For example, a redirect might take a user to a site that would attempt to trick them into downloading antivirus software and, ironically, installing a [[trojan horse (computing)|trojan]] of some sort instead.\n\n=== Removing <code>referer</code> information ===\nWhen a link is clicked, the browser sends along in the [[HTTP request]] a field called [[HTTP referer|referer]] which indicates the source of the link. This field is populated with the URL of the current web page, and will end up in the [[server log|logs]] of the server serving the external link. Since sensitive pages may have sensitive URLs (for example, <code><nowiki>http://company.com/plans-for-the-next-release-of-our-product</nowiki></code>), it is not desirable for the <code>referer</code> URL to leave the organization. A redirection page that performs [[Referer#Referrer hiding|referrer hiding]] could be embedded in all external URLs, transforming for example <code><nowiki>http://externalsite.com/page</nowiki></code> into <code><nowiki>http://redirect.company.com/http://externalsite.com/page</nowiki></code>. This technique also eliminates other potentially sensitive information from the referer URL, such as the [[session ID]], and can reduce the chance of [[phishing]] by indicating to the end user that they passed a clear gateway to another site.\n\n== Implementation ==\nSeveral different kinds of response to the browser will result in a redirection.  These vary in whether they affect [[HTTP headers]] or HTML content.  The techniques used typically depend on the role of the person implementing it and their access to different parts of the system.  For example, a web author with no control over the headers might use a [[meta refresh|Refresh meta tag]] whereas a web server administrator redirecting all pages on a site is more likely to use server configuration.\n\n=== Manual redirect ===\nThe simplest technique is to ask the visitor to follow a link to the new page, usually using an HTML anchor like:\n\n<source lang="html4strict">\nPlease follow <a href="http://www.example.com/">this link</a>.\n</source>\n\nThis method is often used as a fall-back&nbsp;— if the browser does not support the automatic redirect, the visitor can still reach the target document by following the link.\n\n=== HTTP status codes 3xx ===\nIn the [[HTTP]] [[Protocol (computing)|protocol]] used by the [[World Wide Web]], a \'\'\'redirect\'\'\' is a response with a [[List of HTTP status codes|status code]] beginning with \'\'3\'\' that causes a browser to display a different page. If a client encounters a redirect, it needs to make a number of decisions how to handle the redirect. Different status codes are used by clients to understand the purpose of the redirect, how to handle caching and which request method to use for the subsequent request.\n\nHTTP/1.1 defines several status codes for redirection (RFC 7231):\n* [[HTTP 300|300 multiple choices]] (e.g. offer different languages)\n* [[HTTP 301|301 moved permanently]]\n* [[HTTP 302|302 found]] (originally "temporary redirect" in HTTP/1.0 and popularly used for CGI scripts; superseded by 303 and 307 in HTTP/1.1 but preserved for backward compatibility)\n* [[HTTP 303|303 see other]] (forces a GET request to the new URL even if original request was POST)\n* [[HTTP 307|307 temporary redirect]] (provides a new URL for the browser to resubmit a GET or POST request)\n* [[HTTP 308|308 permanent redirect]] (provides a new URL for the browser to resubmit a GET or POST request)\n\n==== Redirect status codes and characteristics ====\n\n{| class="wikitable"\n|-\n! HTTP Status Code !! HTTP Version !! Temporary / Permanent !! Cacheable !! Request Method Subsequent Request\n|-\n| 301 || HTTP/1.0 || Permanent || yes || GET / POST may change\n|-\n| 302 || HTTP/1.0 || Temporary || not by default || GET / POST may change\n|-\n| 303 || HTTP/1.1 || Temporary || never || always GET\n|-\n| 307 || HTTP/1.1 || Temporary || not by default || may not change\n|-\n| 308 || HTTP/1.1 || Permanent || by default || may not change\n|-\n|}<ref>{{Cite web |url=https://audisto.com/insights/guides/31/ |title=Redirects & SEO - The Complete Guide |accessdate=2015-11-29 |publisher=Audisto}}</ref>\n\nAll of these status codes require the URL of the redirect target to be given in the Location: header of the HTTP response. The 300 multiple choices will usually list all choices in the body of the message and show the default choice in the Location: header.\n\n(Status codes [[HTTP 304|304 not modified]] and [[HTTP 305|305 use proxy]] are not redirects).\n\n==== Example HTTP response for a 301 redirect ====\n\nA [[HTTP]] response with the 301 "moved permanently" redirect looks like this:\n\n<syntaxhighlight lang="http">\nHTTP/1.1 301 Moved Permanently\nLocation: http://www.example.org/\nContent-Type: text/html\nContent-Length: 174\n\n<html>\n<head>\n<title>Moved</title>\n</head>\n<body>\n<h1>Moved</h1>\n<p>This page has moved to <a href="http://www.example.org/">http://www.example.org/</a>.</p>\n</body>\n</html>\n</syntaxhighlight>\n\n==== Using server-side scripting for redirection ====\nWeb authors producing HTML content can\'t usually create redirects using HTTP headers as these are generated automatically by the web server program when serving an HTML file.  The same is usually true even for programmers writing CGI scripts, though some servers allow scripts to add custom headers (e.g. by enabling "non-parsed-headers").  Many web servers will generate a 3xx status code if a script outputs a "Location:" header line.  For example, in [[PHP]], one can use the "header" function:\n\n<source lang="php">\nheader(\'HTTP/1.1 301 Moved Permanently\');\nheader(\'Location: http://www.example.com/\');\nexit();\n</source>\n\nMore headers may be required to prevent caching.<ref name="php-301-robust-solution">{{cite web|url=http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution/ |title=PHP Redirects: 302 to 301 Rock Solid Robust Solution |publisher=WebSiteFactors.co.uk |archiveurl=https://web.archive.org/web/20121012042703/http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution |archivedate=2012-10-12}}</ref> The programmer must ensure that the headers are output before the body.  This may not fit easily with the natural flow of control through the code.  To help with this, some frameworks for server-side content generation can buffer the body data.  In the [[Active Server Pages|ASP scripting]] language, this can also be accomplished using <code>response.buffer=true</code> and <code>response.redirect <nowiki>"http://www.example.com/"</nowiki></code> HTTP/1.1 allows for either a relative URI reference or an absolute URI reference.<ref>{{cite IETF | title = Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content | rfc = 7231 | section = 7.1.2 | sectionname = Location | page = 68 | editor1 = Roy T. Fielding | editor2 = Julian F. Reschke | year = 2014 | month = June | publisher = [[Internet Engineering Task Force|IETF]]}}</ref> If the URI reference is relative the client computes the required absolute URI reference according to the rules defined in RFC 3986.<ref>{{cite IETF | title = Uniform Resource Identifier (URI): Generic Syntax | rfc = 3986 | section = 5 | sectionname = Reference Resolution | page = 28 | first1 = Tim | last1 = Berners-Lee | author1-link = Tim Berners-Lee | first2 = Roy T. | last2 = Fielding | author2-link = Roy Fielding | first3 = Larry | last3 = Masinter | year = 2005 | month = January | publisher = [[Internet Engineering Task Force|IETF]]}}</ref>\n\n==== Apache mod_rewrite ====\nThe [[Apache HTTP Server]] mod_alias extension can be used to redirect certain requests. Typical configuration directives look like:\n<source lang="apache">\nRedirect permanent /oldpage.html http://www.example.com/newpage.html\nRedirect 301 /oldpage.html http://www.example.com/newpage.html\n</source>\n\nFor more flexible [[URL rewriting]] and redirection, Apache mod_rewrite can be used. E.g., to redirect a requests to a canonical domain name:\n<source lang="apache">\nRewriteEngine on\nRewriteCond %{HTTP_HOST} ^([^.:]+\\.)*oldsite\\.example\\.com\\.?(:[0-9]*)?$ [NC]\nRewriteRule ^(.*)$ http://newsite.example.net/$1 [R=301,L]\n</source>\n\nSuch configuration can be applied to one or all sites on the server through the server configuration files or to a single content directory through a <code>[[.htaccess]]</code> file.\n\n==== nginx rewrite ====\n[[Nginx]] has an integrated http rewrite module,<ref>{{cite web|url=http://nginx.org/r/rewrite |title=Module ngx_http_rewrite_module - rewrite |publisher=nginx.org |date= |accessdate=24 December 2014}}</ref> which can be used to perform advanced URL processing and even web-page generation (with the <tt>return</tt> directive).  A showing example of such advanced use of the rewrite module is [http://mdoc.su/ mdoc.su], which implements a deterministic [[URL shortening]] service entirely with the help of nginx configuration language alone.<ref>{{cite mailing list |date=18 February 2013 |url=http://mailman.nginx.org/pipermail/nginx/2013-February/037592.html |mailinglist=nginx@nginx.org |title=A dynamic web-site written wholly in nginx.conf? Introducing mdoc.su! |first=Constantine A. |last=Murenin |accessdate=24 December 2014}}</ref><ref>{{cite web |url=http://mdoc.su/ |title=mdoc.su — Short manual page URLs for FreeBSD, OpenBSD, NetBSD and DragonFly BSD |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>\n\nFor example, if a request for [http://mdoc.su/DragonFlyBSD/HAMMER.5 <tt>/DragonFlyBSD/HAMMER.5</tt>] were to come along, it would first be redirected internally to <tt>/d/HAMMER.5</tt> with the first rewrite directive below (only affecting the internal state, without any HTTP replies issued to the client just yet), and then with the second rewrite directive, an [[HTTP response]] with a [[HTTP 302|302 Found status code]] would be issued to the client to actually redirect to the external [[Common Gateway Interface|cgi script]] of web-[[man page|man]]:<ref>{{cite web |url=http://nginx.conf.mdoc.su/mdoc.su.nginx.conf |title=mdoc.su.nginx.conf |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>\n<source lang="nginx">\n\tlocation /DragonFly {\n\t\trewrite\t^/DragonFly(BSD)?([,/].*)?$\t/d$2\tlast;\n\t}\n\tlocation /d {\n\t\tset\t$db\t"http://leaf.dragonflybsd.org/cgi/web-man?command=";\n\t\tset\t$ds\t"&section=";\n\t\trewrite\t^/./([^/]+)\\.([1-9])$\t\t$db$1$ds$2\tredirect;\n\t}\n</source>\n\n=== Refresh Meta tag and HTTP refresh header ===\n[[Netscape]] introduced the [[meta refresh]] feature which refreshes a page after a certain amount of time.  This can specify a new URL to replace one page with another.  This is supported by most web browsers.<ref>[http://www.w3schools.com/tags/tag_meta.asp HTML <meta> tag]</ref><ref>[http://web.archive.org/web/20020802170847/http://wp.netscape.com/assist/net_sites/pushpull.html An exploration of dynamic documents]</ref> A timeout of zero seconds effects an immediate redirect. This is treated like a 301 permanent redirect by Google, allowing transfer of PageRank to the target page.<ref>[http://sebastians-pamphlets.com/google-and-yahoo-treat-undelayed-meta-refresh-as-301-redirect/ "Google and Yahoo accept undelayed meta refreshs as 301 redirects"]. Sebastian\'s Pamphlets. 3 September 2007.</ref>\n\nThis is an example of a simple HTML document that uses this technique:\n<source lang="html4strict">\n<html>\n<head>\n<meta http-equiv="Refresh" content="0; url=http://www.example.com/" />\n</head>\n<body>\n<p>Please follow <a href="http://www.example.com/">this link</a>.</p>\n</body>\n</html>\n</source>\n\nThis technique can be used by [[Web designer|web authors]] because the meta tag is contained inside the document itself.  The meta tag must be placed in the "head" section of the HTML file.  The number "0" in this example may be replaced by another number to achieve a delay of that many seconds.  The anchor in the "body" section is for users whose browsers do not support this feature.\n\nThe same effect can be achieved with an HTTP <code>refresh</code> header:\n<source lang="http">\nHTTP/1.1 200 ok\nRefresh: 0; url=http://www.example.com/\nContent-type: text/html\nContent-length: 78\n\nPlease follow <a href="http://www.example.com/">this link</a>.\n</source>\n\nThis response is easier to generate by CGI programs because one does not need to change the default status code.\n\nHere is a simple CGI program that effects this redirect:\n<source lang="perl">\n#!/usr/bin/perl\nprint "Refresh: 0; url=http://www.example.com/\\r\\n";\nprint "Content-type: text/html\\r\\n";\nprint "\\r\\n";\nprint "Please follow <a href=\\"http://www.example.com/\\">this link</a>!"\n</source>\n\nNote: Usually, the HTTP server adds the status line and the Content-length header automatically.\n\nThe [[World Wide Web Consortium|W3C]] discourage the use of meta refresh, since it does not communicate any information about either the original or new resource, to the browser (or [[search engine]]). The W3C\'s [http://www.w3.org/TR/WAI-WEBCONTENT/#tech-no-periodic-refresh Web Content Accessibility Guidelines (7.4)] discourage the creation of auto-refreshing pages, since most web browsers do not allow the user to disable or control the refresh rate.  Some articles that they have written on the issue include [http://www.w3.org/TR/WAI-WEBCONTENT/#gl-movement W3C Web Content Accessibility Guidelines (1.0): Ensure user control of time-sensitive content changes], [http://www.w3.org/QA/Tips/reback Use standard redirects: don\'t break the back button!] and [http://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh Core Techniques for Web Content Accessibility Guidelines 1.0 section 7].\n\n=== JavaScript redirects ===\n[[JavaScript]] can cause a redirect by setting the <code>window.location</code> attribute, e.g.:\n<syntaxhighlight lang="ecmascript">\nwindow.location=\'http://www.example.com/\'\n</syntaxhighlight>\nNormally JavaScript pushes the redirector site\'s [[URL]] to the browser\'s history. It can cause redirect loops when users hit the back button. With the following command you can prevent this type of behaviour.<ref>{{cite web|url=http://insider.zone/tools/client-side-url-redirect-generator/|title=Cross-browser client side URL redirect generator|publisher=Insider Zone}}</ref>\n<syntaxhighlight lang="ecmascript">\nwindow.location.replace(\'http://www.example.com/\')\n</syntaxhighlight>\nHowever, HTTP headers or the refresh meta tag may be preferred for security reasons and because JavaScript will not be executed by some browsers and many [[web crawler]]s.\n\n=== Frame redirects ===\n\nA slightly different effect can be achieved by creating an inline frame:\n\n<source lang="html4strict">\n<iframe height="100%" width="100%" src="http://www.example.com/">\nPlease follow <a href="http://www.example.com/">link</a>.\n</iframe>\n</source>\n\nOne main difference to the above redirect methods is that for a frame redirect, the browser displays the URL of the frame document and not the URL of the target page in the URL bar. This \'\'cloaking\'\' technique may be used so that the reader sees a more memorable URL or to fraudulently conceal a [[phishing]] site as part of [[website spoofing]].<ref>Aaron Emigh (19 January 2005). [http://www.sfbay-infragard.org/Documents/phishing-sfectf-report.pdf "Anti-Phishing Technology"] (PDF). Radix Labs.</ref>\n\nBefore HTML5,<ref>\nhttps://www.w3.org/TR/html5/obsolete.html</ref> the same effect could be done with an [[Framing (World Wide Web)|HTML frame]] that contains the target page:\n<source lang="html4strict">\n<frameset rows="100%">\n  <frame src="http://www.example.com/">\n  <noframes>\n    <body>Please follow <a href="http://www.example.com/">link</a>.</body>\n  </noframes>\n</frameset>\n</source>\n\n=== Redirect chains ===\nOne redirect may lead to another. For example, the URL [http://www.wikipedia.com/wiki/URL_redirection http://www.wikipedia\'\'\'.com\'\'\'/wiki/URL_redirection] (with [[domain name]] in [[.com]]) is first redirected to [[:www:URL redirection|http://www.wikipedia\'\'\'.org\'\'\'/wiki/URL redirection]] (with domain name in [[.org]]), then to the [[HTTPS]] URL [[:www:URL redirection|\'\'\'https:\'\'\'//www.wikipedia.org/wiki/URL redirection]] and finally to the language-specific site https://\'\'\'en\'\'\'.wikipedia.org/wiki/URL redirection. This is unavoidable if the different links in the chain are served by different servers though it should be minimised by \'\'[[rewriting]]\'\' the URL as much as possible on the server before returning it to the browser as a redirect.\n\n=== Redirect loops ===\nSometimes a mistake can cause a page to end up redirecting back to itself, possibly via other pages, leading to an infinite sequence of redirects. Browsers should stop redirecting after a certain number of hops and display an error message.\n\nThe HTTP/1.1 Standard states:<ref name="rfc7231sec6.4">{{cite IETF | title = Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content | rfc = 7231 | section = 6.4 | sectionname = Redirection 3xx | page = 54 | editor1 = Roy T. Fielding | editor1-link = Roy Fielding | editor2 = Julian F. Reschke | year = 2014 | month = June | publisher = [[Internet Engineering Task Force|IETF]]}}</ref>\n<blockquote>\nA client \'\'SHOULD\'\' detect and intervene in cyclical redirections (i.e., "infinite" redirection loops).\n\nNote: An earlier version of this specification recommended a maximum of five redirections ([RFC 2068], Section 10.3).  Content developers need to be aware that some clients might implement such a fixed limitation.\n</blockquote>\nNote that the URLs in the sequence might not repeat, e.g.: http://www.example.com/1 -> http://www.example.com/2 -> http://www.example.com/3 ...\n\n== Services ==\nThere exist services that can perform URL redirection on demand, with no need for technical work or access to the web server your site is hosted on.\n\n=== URL redirection services ===\nA \'\'\'redirect service\'\'\' is an information management system, which provides an internet link that redirects users to the desired content. The typical benefit to the user is the use of a memorable domain name, and a reduction in the length of the URL or web address. A redirecting link can also be used as a permanent address for content that frequently changes hosts, similarly to the [[Domain Name System]]. Hyperlinks involving URL redirection services are frequently used in spam messages directed at blogs and wikis.  Thus, one way to reduce spam is to reject all edits and comments containing hyperlinks to known URL redirection services; however, this will also remove legitimate edits and comments and may not be an effective method to reduce spam.\nRecently, URL redirection services have taken to using [[AJAX]] as an efficient, user friendly method for creating shortened URLs. A major drawback of some URL redirection services is the use of delay pages, or frame based advertising, to generate revenue.\n\n==== History ====\nThe first redirect services took advantage of [[top-level domains]] (TLD) such as "[[.to]]" (Tonga), "[[.at]]" (Austria) and "[[.is]]" (Iceland). Their goal was to make memorable URLs. The first mainstream redirect service was V3.com that boasted 4 million users at its peak in 2000.  V3.com success was attributed to having a wide variety of short memorable domains including "r.im", "go.to", "i.am", "come.to" and "start.at".  V3.com was acquired by FortuneCity.com, a large free web hosting company, in early 1999.<ref>{{cite news| url=http://news.bbc.co.uk/2/hi/technology/6991719.stm | work=BBC News | title=Net gains for tiny Pacific nation | date=2007-09-14 | accessdate=2010-05-27}}</ref> As the sales price of top level domains started falling from $70.00 per year to less than $10.00, use of redirection services declined. With the launch of [[TinyURL]] in 2002 a new kind of redirecting service was born, namely [[URL shortening]]. Their goal was to make long URLs short, to be able to post them on internet forums. Since 2006, with the 140 character limit on the extremely popular [[Twitter]] service, these short URL services have been heavily used.\n\n=== Referrer masking ===\nRedirection services can hide the [[referrer]] by placing an intermediate page between the page the link is on and its destination. Although these are conceptually similar to other URL redirection services, they serve a different purpose, and they rarely attempt to shorten or obfuscate the destination URL (as their only intended side-effect is to hide referrer information and provide a clear gateway between other websites.) This type of redirection is often used to prevent potentially-malicious links from gaining information using the referrer, for example a [[session ID]] in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective [[phishing]]  .\n\nHere is a simplistic example of such a service, written in [[PHP]].\n<source lang="html+php">\n<?php\n$url = htmlspecialchars($_GET[\'url\']);\nheader( \'Refresh: 0; url=http://\'.$url );\n?>\n<!-- Fallback using meta refresh. -->\n<html>\n <head>\n  <title>Redirecting...</title>\n  <meta http-equiv="refresh" content="0;url=http://<?php echo $url; ?>">\n </head>\n <body>\n Attempting to redirect to <a href="http://<?php echo $url; ?>">http://<?php echo $url; ?></a>.\n </body>\n</html>\n</source>\n\nThe above example does not check who called it (e.g. by referrer, although that could be spoofed).  Also, it does not check the url provided.  This means that a malicious person could link to the redirection page using a url parameter of his/her own selection, from any page, which uses the web server\'s resources.\n\n==Security issues==\nURL redirection can be abused by attackers for [[phishing]] attacks, such as [[Open Redirect|open redirect]] and [[Covert Redirect|covert redirect]]. "An open redirect is an application that takes a parameter and redirects a user to the parameter value without any validation."<ref name="Open_Redirect">{{cite web | url=https://www.owasp.org/index.php/Open_redirect | title=Open Redirect |publisher= OWASP |date=16 March 2014 | accessdate=21 December 2014}}</ref> "Covert redirect is an application that takes a parameter and redirects a user to the parameter value WITHOUT SUFFICIENT validation."<ref name="Covert_Redirect">{{cite web | url=http://tetraph.com/covert_redirect/ | title=Covert Redirect |publisher= Tetraph |date=1 May 2014 | accessdate=21 December 2014}}</ref> It was disclosed in May 2014 by a mathematical doctoral student Wang Jing from Nanyang Technological University, Singapore.<ref name="CNET">{{cite web | url=http://www.cnet.com/news/serious-security-flaw-in-oauth-and-openid-discovered/ | title=Serious security flaw in OAuth, OpenID discovered |publisher= CNET |date=2 May 2014 | accessdate=21 December 2014}}</ref>\n\n==See also==\n* [[Link rot]]\n* [[Canonical link element]]\n* [[Canonical meta tag]]\n* [[Domain masking]]\n* [[URL normalization]]\n* [[Semantic URL]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://httpd.apache.org/docs/1.3/urlmapping.html Mapping URLs to Filesystem Locations]\n* [http://www.cs.ucdavis.edu/~hchen/paper/www07.pdf Paper on redirection spam (UC Davis)] (403 Forbidden link)\n* [http://projects.webappsec.org/URL-Redirector-Abuse Security vulnerabilities in URL Redirectors] The Web Application Security Consortium Threat Classification\n\n{{Spamming}}\n\n{{Use dmy dates|date=November 2010}}\n\n{{DEFAULTSORT:Url Redirection}}\n[[Category:Uniform Resource Locator]]\n[[Category:Black hat search engine optimization]]\n[[Category:Internet search]]\n[[Category:Internet terminology]]']
['ZyLAB Technologies', '2744940', '{{advert|date=April 2012}}\n{{Infobox company |\n  name   = ZyLAB |\n  logo   = <!--  Commented out because image was deleted: [[Image:zylab logo.jpg|center]] --> |\n  slogan = "eDiscovery & Information Risk Management" |\n  type   = Private |\n  foundation     = 1983 |\n  location       = [[McLean, Virginia]]<br>[[Amsterdam]] |\n  key_people     = [[Pieter Varkevisser]], president & CEO<br>[[Dr. Johannes C. Scholtes]], chairman & chief strategy officer | Mary Mack, Enterprise Technology Counsel\n  num_employees  = 140 |\n  industry       = [[Software]], eDiscovery and Information Risk Management, Records Management, Email Archiving, SharePoint Archiving |\n  products       = ZyLAB Information Management Platform and various bundles for eDiscovery, email & SharePoint archiving, text-analytics, visualization, contract management, and workflow. |\n\n  homepage       = [http://www.zylab.com/ www.zylab.com]\n}}\n\n\'\'\'ZyLAB\'\'\' is a developer of software for [[Electronic discovery|e-discovery]], information risk management, email management, records, contract, and document management, knowledge management, and workflow. The company is headquartered in [[McLean, Virginia]] and in [[Amsterdam]], [[Netherlands]]. ZyLAB’s most important products are ZyLAB eDiscovery & Production System, the ZyLAB Information Management Platform and bundles that build systems for deployments.\n\n== History ==\nIn 1983 ZyLAB was the first company providing a [[Full text search|full-text]] search program for electronic files stored in the file system of [[IBM PC compatible|IBM-compatible PCs]]. The program was called ZyINDEX. The first version of ZyINDEX was written in [[Pascal (programming language)|Pascal]] and worked on [[MS-DOS]]. Subsequent programs were written in [[C (programming language)|C]], [[C++]] and [[C Sharp (programming language)|C#]] and work on a variety of Microsoft operating systems.\n\nIn 1991, ZyLAB integrated ZyINDEX with an optical character recognition ([[Optical character recognition|OCR]]) program, Calera Wordscan, which was a spin-off from [[Raymond Kurzweil]]’s first OCR implementation. This integration was called ZyIMAGE. ZyIMAGE was the first PC program to include a [[Fuzzy string searching|fuzzy string search]] algorithm to overcome scanning and OCR errors.\n\nIn 1998, the company developed support to full-text search email, including attachments.\n\nIn 2000, ZyLAB embraced the new [[XML]] standard and created a full content management and records management system based on the XML standard and build a full solution for e-discovery, historical archives, records management, document management, email archiving, contract management, and professional back-office solutions.\n\nIn 2003, the company invested in expanding the ZyIMAGE product suite with advanced [[text analytics]], [[text mining]], [[data visualization]], [[computational linguistics]], and [[Machine translation|automatic translation]].\n\n2005: ZyIMAGE Information Access Platform was released, an integrated solution to address information access problems.\n\nPlatforms for ZyIMAGE e-Discovery and legal production, historical archiving, compliance, back-office records management and [[COMINT#COMINT|COMINT]] were launched in 2007.\n\n2010: ZyLAB Information Management Platform was released, an integrated solution to address e-Discovery and information management problems.\n\n==Customers==\nInitial customers of ZyINDEX were organizations such as the [[FBI]] and other law enforcement agencies to investigate electronic data from seized PCs, the [[United States Navy|U.S. Navy]] for on-board manuals, and law firms around the world for [[Electronic discovery|e-Discovery]]. Over the years, ZyLAB received grants from the European Union (DG13).\n\nOther well-known ZyLAB customers were [[O. J. Simpson murder case|O.J. Simpson\'s defense team]], war crime tribunals such as the [[trial of Slobodan Milošević]], the [[Special Court for Sierra Leone]], the [[Extraordinary Chambers in the Courts of Cambodia|UN-AKRT-ECCC Cambodia Khmer Rouge trials]] and the [[International Criminal Tribunal for Rwanda|Rwanda tribunal]]. In 2007, the U.S. [[Executive Office of the President of the United States|Executive Office of the President]] selected ZyLAB for email archiving, basically for its open XML structures, which is endorsed by organizations such as the [[National Archives and Records Administration]]. ZyLAB’s software was used for many other high-profile investigations such as the [[Oklahoma City bombing]].\n\nPublic websites also use the ZyLAB Webserver.\n\n[[Gartner]] positioned ZyLAB in the "Leaders" quadrant in its 2007, 2008 and 2009 Magic Quadrant for Information Access Solutions, gave it a strong positive rating in its 2007, 2008 and 2009 e-Discovery Marketscope and a Positive Rating in its 2007 and 2008 Records Management MarketScope.\n\nZyLAB’s chief strategy officer, Dr. Johannes C. Scholtes, is professor in [[text mining]] at [[Maastricht University|the University of Maastricht]] faculty of Humanities and Sciences and director in the board of AIIM.\n\n==System overview and compatibility==\nAccording to the company’s website it delivers systems for deployments, product bundles and the core components is the ZyLAB Information Management platform include:\n\nSystems:\n*ZyLAB eDiscovery and Production\n*ZyLAB Compliance and Litigation readiness\n*ZyLAB Law Enforcement and Investigations\n*ZyLAB Communications Intelligence\n*ZyLAB Digital Print and Media Archiving\n*ZyLAB Enterprise Information Management\n\nBundles:\n*E-Mail Archiving Bundle\n*Microsoft SharePoint Bundle\n*Analytics Bundle\n*eDiscovery EDRM Processing bundle\n*DoD and Sox Compliant RMA Bundle\n*TIFF Archiving and Production Bundle\n*WebPublishing Bundle\n*Commercial Publishing Bundle\n*Business Process Automation Bundle\n*Development and Integrators Bundle\n*Scanning Bundle\n*Digital Copier Bundle\n*Professional Text Mining\n*Machine translation\n\n===Supported configurations===\n*\'\'\'Server OS\'\'\': Windows 2003, Windows 2008\n*\'\'\'Databases\'\'\': XML, MS SQL Server 2005, MS SQL Server 2008, Oracle 10g, Oracle 11g, mySQL\n*\'\'\'Web Servers\'\'\': IIS\n*\'\'\'Client OS\'\'\': Windows XP, Windows Vista, Windows 7\n*\'\'\'Clustering\'\'\': Support for Active/Passive Failover.\n*\'\'\'Authentication\'\'\': Active Directory, LDAP, XML, NTFS, IBM Tripoli.\n*\'\'\'Virtualization\'\'\': VMware Infrastructure, VMware Workstation, VMware Server, VMware Fusion.\n\n===Languages supported===\n*\'\'\'Unicode\'\'\'. Support for documents in all languages.\n*\'\'\'Internationalization\'\'\'. ZyLAB offers translated products for English, German, French, Dutch, Spanish, Italian, Danish, Swedish, Norwegian, Finnish, Portuguese, Arabic and [[Persian language|Persian]]. In addition to these languages, over 400 languages are supported by ZyLAB\'s recognition and full-text indexing technology, including all Western-European, Eastern European, Baltic, African, Asian and South American languages. ZyLAB\'s technical ability for broad language and character recognition enhances the accuracy of stored information searches and helps diminish the costs incurred by incorrect searches or text correction.\n\n==Zy-IMAGE-nation Annual Conference==\nThe annual Zy-IMAGE-nation Conference is sponsored by ZyLAB. During this conference, seminars and interactive sessions from leading professionals about the advanced technologies and procedural enhancements that are driving new levels of operational efficiency in private and public sectors. The focus of the conference is on technologies that provide integrated capabilities for managing the accumulated knowledge of an organization, especially records and e-mail, as well as other business-critical processes. Related topics to be covered include best practices for e-discovery preparation and implementation, records management, email archiving, and knowledge management.\n\n==See also==\n* [[Electronic discovery|e-Discovery]]\n* [[Optical character recognition|Optical Character Recognition (OCR)]]\n* [[Document Imaging]]\n* [[E-mail archiving|E-mail Archiving]]\n* [[Knowledge Management]]\n* [[Document management system|Document Management (System)]]\n* [[Enterprise content management|Enterprise Content Management]]\n* [[Records management|Records Management]]\n* [[Contract management|Contract Management]]\n* [[Workflow]]\n* [[Text mining|Text Mining]]\n* [[Text analytics|Text Analytics]]\n* [[Machine translation|Automatic Machine Translation]]\n* [[Data visualization|Data Visualization]]\n\n==References==\n{{Reflist}}\n*[http://www.pcmag.com/encyclopedia_term/0,,t=zyindex&i=55248,00.asp Definition of ZyINDEX] in [[PC Magazine|\'\'PCMAG.com\'\']]\'s encyclopedia\n*[http://www.pcmag.com/encyclopedia_term/0,2542,t=ZyIMAGE&i=55247,00.asp Definition of ZyIMAGE] in [[PC Magazine|\'\'PCMAG.com\'\']]\'s encyclopedia\n*[https://web.archive.org/web/20090108050805/http://www.informationweek.com/777/knowledge.htm Review] of ZyImage 3.0 in \'\'[[InformationWeek]]\'\'\n*[http://www.accessmylibrary.com/coms2/summary_0286-9201794_ITM Mac version of ZyINDEX made its debut on Comdex]\n*[http://query.nytimes.com/gst/fullpage.html?res=940DE6DA1730F93AA35751C0A96E948260 Review] of ZyINDEX in the \'\'[[New York Times]]\'\'\n*[https://web.archive.org/web/20090224230803/http://www.computerwoche.de/heftarchiv/1988/26/1155611/ Review] of ZyINDEX on \'\'Computerwoche.de\'\' (article in German)\n*[https://web.archive.org/web/20110717125042/http://www.computerwoche.de/index.cfm?pid=2123&pk=1096333 Review] of ZyIMAGE\'s webserver on \'\'Computerwoche.de\'\' (article in German)\n*[http://nl.newsbank.com/nl-search/we/Archives?p_product=MH&s_site=miami&p_multi=MH&p_theme=realcities&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EB367D56736E685&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date: Review] of ZyINDEX in the \'\'[[Miami Herald]]\'\'\n*[http://www.usdoj.gov/oig/special/0203/chapter3.htm ZyINDEX used in the Investigation of the Belated Production of Documents in the Oklahoma City Bombing Case]\n*[http://www.fcw.com/print/6_31/news/70014-1.html Review]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} of ZyIMAGE on \'\'Federal Computer Week (FCW.com)\'\'\n*Zylab retrieval engine optimized for CD-ROM; Zylab, Progressive Technologies merge," Seybold Report on Desktop Publishing. vol. 8, No. 10, Jun. 6, 1994, p. 40.\n*Knibbe, "ZyImage 2 boosts, OCR, batch duties," InfoWorld, vol. 15, Issue 51, Dec. 20, 1993, p.&nbsp;20.\n*Knibbe, "ZyImage 3.0 will facilitate distribution on CD-ROMs; Boasts integration with WordScan OCR software," InfoWorld, vol. 16, No. 38, Sep. 19, 1994, p.&nbsp;22.\n*Marshall, "Text retrieval alternatives: 10 more ways to pinpoint important information," Infoworld, vol. 14, No. 12, Mar. 23, 1992, pp.&nbsp;88–89.\n*Marshall, "ZyImage adds scanning access to ZyIndex," InfoWorld, vol. 16, No. 15, Apr. 11, 1994, pp.&nbsp;73, 76, and 77.\n*Marshall, "ZyImage is ZyIndex plus a scan interface integrated," InfoWorld. vol. 15, Issue 10, Mar. 8, 1993, p.&nbsp;100.\n*Marshall et al., "ZyIndex for Windows, Version 5.0," InfoWorld, v. 15, n. 21, May 1993, pp.&nbsp;127, 129, 133 and 137.\n*Simon, "ZyImage: A Winning Combination of OCR And Text Indexing," PC Magazine. vol. 12, No. 6, Mar. 30, 1993, p.&nbsp;56.\n*Rooney, "Text-retrieval veterans prepare Windows attack," PC Week, v. 9, n. 24, Jun. 1992, p.&nbsp;46.\n*Rooney, "ZyLab partners with Calera: firms roll out document-image system," PC Week, vol. 10, No. 3, Jan. 25, 1993, p.&nbsp;22.\n*Torgan, "ZyImage: Document Imaging and Retrieval System," PC Magazine. vol. 12, No. 3, Feb. 9, 1993, p.&nbsp;62.\n\n===Gartner reports===\n*Introduction to Investigative Case Management Products (18 April 2007)\n*Hype Cycle for Legal and Regulatory Information Governance, 2007 (16 July 2007)\n*MarketScope for Contract Management, 2007 (16 July 2007)\n*Choosing an E-Discovery Solution in 2007 and 2008 (18 July 2007)\n*Magic Quadrant for Information Access Technology, 2007 (5 September 2007)\n*Magic Quadrant for Information Access Technology, 2008\n*Magic Quadrant for Information Access Technology, 2009\n*The Expanding Enterprise E-Discovery Marketplace (12 November 2007)\n*MarketScope for E-Discovery and Litigation Support Vendors, 2007 (14 December 2007)\n*MarketScope for E-Discovery Product Vendors, 2008\n*MarketScope for E-Discovery Product Vendors, 2009\n*MarketScope for Records Management (20 May 2008)\n*Hype Cycle for Content Management, 2008 (8 July 2008)\n*Using the Electronic Discovery Reference Model to Identify, Collect and Preserve Digital Evidence (11 July 2008)\n*Using the Electronic Discovery Reference Model to Process, Review and Analyze Digital Evidence (11 July 2008)\n*Hype Cycle for Governance, Risk and Compliance Technologies, 2009 (17 July 2009)\n\n==External links==\n*[http://www.zylab.com/ ZyLAB official website]\n*[http://www.edrm.net/ The Electronic Discovery Reference Model (EDRM)]\n*[http://www.aiim.org/ AIIM]\n\n[[Category:Companies established in 1983]]\n[[Category:Software companies of the United States]]\n[[Category:Information retrieval organizations]]']
['European Conference on Information Retrieval', '10328235', "The '''European Conference on Information Retrieval''' (ECIR) is the main \nEuropean research conference for the presentation of new results in the field of [[information retrieval]] (IR).\nIt is organized by the [[Information Retrieval Specialist Group]] of the [[British Computer Society]] (BCS-IRSG).\n      \nThe event started its life as the ''Annual Colloquium on Information Retrieval Research'' in 1978 and was \nheld in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has\nalternated between the United Kingdom and continental Europe. To mark the metamorphosis\nfrom a small informal colloquium to a major event in the IR research calendar, the \nBCS-IRSG later renamed the event to ''European Conference on Information Retrieval''. In recent years,\nECIR has continued to grow and has become the major European forum for the discussion\nof research in the field of Information Retrieval.\n\nSome of the topics dealt with include:\n* IR models, techniques, and algorithms\n* IR applications\n* IR system architectures\n* Test and evaluation methods for IR\n* [[Natural Language Processing]] for IR\n* Distributed IR\n* Multimedia and cross-media IR\n\n==Time and Location==\n\nTraditionally, the ECIR is held in Spring, near the Easter weekend. A list of locations and planned venues are presented below.\n\n* [[Padova, Italy]], 2016 [http://ecir2016.dei.unipd.it/]\n* [[Vienna, Austria]], 2015 [http://www.ecir2015.org/]\n* [[Amsterdam, Netherlands]], 2014 [http://ecir2014.org/]\n* [[Moscow, Russia]], 2013 [http://ecir2013.org/]\n* [[Barcelona, Spain]], 2012 [http://ecir2012.upf.edu/]\n* [[Dublin, Ireland]], 2011 [http://www.ecir2011.dcu.ie/]\n* [[Milton Keynes]], 2010 [http://kmi.open.ac.uk/events/ecir2010/]\n* [[Toulouse]], 2009 [http://ecir09.irit.fr/]\n* [[Glasgow]], 2008 [http://ecir2008.dcs.gla.ac.uk/]\n* [[Rome]], 2007 [http://ecir2007.fub.it/]\n* [[London]], 2006 [http://ecir2006.soi.city.ac.uk/]\n* [[Santiago de Compostela|Santiago]], 2005 [http://www-gsi.dec.usc.es/ecir05/]\n* [[Sunderland, Tyne and Wear|Sunderland]], 2004 [http://ecir04.sunderland.ac.uk/]\n* [[Pisa]], 2003 [http://ecir03.isti.cnr.it/]\n* [[Glasgow]], 2002 [http://irsg.bcs.org/past_ecir.php]*\n* [[Darmstadt]], 2001* (organized by GMD)\n* [[Cambridge]], 2000* (organized by Microsoft Research)\n* [[Glasgow]], 1999*\n* [[Grenoble]], 1998*\n* [[Aberdeen, Scotland|Aberdeen]], 1997*\n* [[Manchester]], 1996*\n* [[Crewe]], 1995* (organized by Manchester Metropolitan University)\n* [[Drymen]], Scotland, 1994* (organized by Strathclyde University)\n* [[Glasgow]], 1993* (organized by Strathclyde University)\n* [[Lancaster, Lancashire|Lancaster]], 1992*\n* [[Lancaster, Lancashire|Lancaster]], 1991*\n* [[Huddersfield]], 1990*\n* [[Huddersfield]], 1989*\n* [[Huddersfield]], 1988*\n* [[Glasgow]], 1987*\n* [[Glasgow]], 1986*\n* [[Bradford]], 1985*\n* [[Bradford]], 1984*\n* [[Sheffield]], 1983*\n* [[Sheffield]], 1982*\n* [[Birmingham]], 1981*\n* [[Leeds]], 1980*\n* [[Leeds]], 1979*\n\n<br /> *as the Annual Colloquium on Information Retrieval Research\n\n==External links==\n* [http://irsg.bcs.org/ecir.php Official page at the website of the British Computer Society]\n\n[[Category:Information retrieval organizations]]\n[[Category:Computer science conferences]]"]
['Conference and Labs of the Evaluation Forum', '27511028', 'The \'\'\'Conference and Labs of the Evaluation Forum\'\'\' (formerly \'\'\'Cross-Language Evaluation Forum\'\'\'), or \'\'\'CLEF\'\'\', is an organization promoting research in multilingual [[information access]] (currently focusing on [[European Commissioner for Multilingualism|European languages]]). Its specific functions are to maintain an underlying framework for testing [[information retrieval]] systems and to create [[digital library|repositories]] of data for researchers to use in developing  comparable [[Technical standard|standards]].<ref name="Peters">{{cite conference | first1 = Carol | last1 = Peters| first2 = Martin | last2 = Braschler | first3 = Khalid | last3 = Choukri | first4 = Julio | last4 = Gonzalo | first5 = Michael | last5 = Kluck | title = The Future of Evaluation for Cross-Language Information Retrieval Systems | conference = Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001 | citeseerx = 10.1.1.109.7647 }}</ref>\nThe organization holds a forum meeting   every September in Europe. Prior to each forum, participants receive a set of challenge tasks. The tasks  are designed to test various aspects of information retrieval systems and encourage their development. Groups of researchers propose and organize campaigns to satisfy those tasks. The results are used as [[benchmark (computing)|benchmarks]] for the state of the art  in the specific areas.,<ref>{{cite journal | url = http://www.springerlink.com/content/l7v0354471u53385/ | title = Special Issue on CLEF | journal = Information Retrieval | volume = 7 | issue = 1–2 | year = 2004 }}</ref><ref>Fredric C. Gey, Noriko Kando, and Carol Peters "Cross-Language Information Retrieval: the way ahead" in \'\'Information Processing & Management\'\'\nvol. 41, no. 3,  p.415-431 May 2005, {{doi|10.1016/j.ipm.2004.06.006}}</ref>\n\nFor example, the 2010 medical retrieval task focuses on retrieval of computed tomography,  MRI, and radiographic images.<ref name="ImageCLEFmed">{{cite web | last = Mueller| first = Henning| authorlink = | coauthors = | title = Medical Retrieval Task| work = | publisher =ImageCLEF - Cross-language image retrieval evaluations | date = 20 May 2010| url =http://www.imageclef.org/2010/medical | format = | doi = | accessdate = 27 May 2010 }}</ref>\n\n==References==\n{{reflist}}\n\n== External links ==\n* [http://www.clef-initiative.eu/ CLEF homepage]\n\n[[Category:Information retrieval organizations]]\n\n\n{{Compu-conference-stub}}']
['Special Interest Group on Information Retrieval', '14109784', '{{Infobox organization\n|name           = ACM Special Interest Group on Information Retrieval\n|image          = sig-information-retrieval-logo.png\n|size           = 140px\n|alt            = ACM SIGIR\n|parent_organization = [[Association for Computing Machinery]]\n|website        = {{URL|sigir.org}}\n}}\n\n\'\'\'SIGIR\'\'\' is the [[Association for Computing Machinery]]\'s \'\'\'Special Interest Group on Information Retrieval\'\'\'. The scope of the group\'s specialty is the theory and application of computers to the acquisition, organization, storage, [[Information retrieval|retrieval]] and distribution of information; emphasis is placed on working with non-numeric information, ranging from natural language to highly structured data bases.\n\n== Conferences ==\nThe annual international SIGIR conference, which began in 1978, is considered the most important in the field of information retrieval. SIGIR also sponsors the annual [[Joint Conference on Digital Libraries]] (JCDL) in association with [[ACM SIGWEB|SIGWEB]], the [[Conference on Information and Knowledge Management]] (CIKM), and the [[International Conference on Web Search and Data Mining]] (WSDM) in association with [[SIGKDD]], [[SIGMOD]], and [[ACM SIGWEB|SIGWEB]].\n\n=== SIGIR conference locations ===\n{| class="wikitable" border="1"\n|-\n! Number\n! Year\n! Location\n|-\n| 22\n| 1999\n| [[Berkeley, California]]\n|-\n| 23\n| 2000\n| [[Athens]]\n|-\n| 24\n| 2001\n| [[New Orleans]]\n|-\n| 25\n| 2002\n| [[Tampere]]\n|-\n| 26\n| 2003\n| [[Toronto]]\n|-\n| 27\n| 2004\n| [[Sheffield]]\n|-\n| 28\n| 2005\n| [[Salvador, Bahia]]\n|-\n| 29\n| 2006\n| [[Seattle]]\n|-\n| 30\n| 2007\n| [[Amsterdam]]\n|-\n| 31\n| 2008\n| [[Singapore]]\n|-\n| 32\n| 2009\n| [[Boston]]\n|-\n| 33\n| 2010\n| [[Geneva]]\n|-\n| 34\n| 2011\n| [[Beijing]]\n|-\n| 35\n| 2012\n| [[Portland, Oregon]]\n|-\n| 36\n| 2013\n| [[Dublin]]\n|-\n| 37\n| 2014\n| [[Gold Coast, Queensland]]\n|-\n| 38\n| 2015\n| [[Santiago]]\n|-\n| 39\n| 2016\n| [[Pisa]]\n|-\n| 40\n| 2017\n| [[Tokyo]]\n|-\n| 41\n| 2018\n| [[Ann Arbor]]\n|}\n\n== Awards ==\nThe group gives out several awards to contributions to the field of information retrieval. The most important award is the [[Gerard Salton Award]] (named after the computer scientist [[Gerard Salton]]), which is awarded every three years to an individual who has made "significant, sustained and continuing contributions to research in information retrieval". Additionally, SIGIR presents a Best Paper Award <ref>{{cite web | url=http://sigir.org/awards/awards.html#bestpaper | title=SIGIR Conference Best Paper Awards | accessdate=2012-08-29 }}</ref> to recognize the highest quality paper at each conference. "Test of time" Award <ref>{{cite web | url=http://sigir.org/awards/test-of-time-awards/ | title=SIGIR Conference Test of Time Awards | accessdate=2015-12-29 }}</ref> is a recent award that is given to a paper that  has had "long-lasting influence, including impact on a subarea of information retrieval research, across subareas of information retrieval research, and outside of the information retrieval research community". This award is selected from a set of full papers presented at the main SIGIR conference 10-12 years before.\n\n==See also==\n* [[Conference on Information and Knowledge Management]]\n\n==References==\n\n{{Reflist}}\n==External links==\n* {{official website|http://www.sigir.org/}}\n\n{{Authority control}}\n\n[[Category:Association for Computing Machinery Special Interest Groups]]\n[[Category:Information retrieval organizations]]']
['Ness Computing', '32567205', '{{Notability|Companies|date=July 2011}}\n\n\'\'\'Ness Computing\'\'\' was a personal search company. It was acquired by OpenTable in March 2014 and was shut down later that year.<ref>{{cite web|last=Lunden|first=Ingrid|title=OpenTable Buys Ness For $17.3M|url=http://techcrunch.com/2014/02/06/opentable-ness/|work=TechCrunch|accessdate=26 March 2014}}</ref> \n\nIt was founded in October 2009 by Corey Reese,<ref>http://www.linkedin.com/in/coreyreese</ref> Paul Twohey,<ref>http://www.linkedin.com/in/twohey</ref> Nikhil Raghavan,<ref>http://www.linkedin.com/in/nikhilraghavan</ref> and Steven Schlansker.<ref>http://www.linkedin.com/in/stevenschlansker</ref> The company was headquartered in Los Altos, California.\n\nNess aimed to help people make decisions about dining, nightlife, entertainment, shopping, music, travel and more. The company referred to its technology as the "Likeness Engine", a combination of a [[recommendation engine]] that used [[machine learning]] to look at data from diverse sources and a traditional [[search engine]] that served up results based on these signals. \n\nThe free Ness Dining App (for iPhone) was referred to as the [[Netflix]] <ref>http://eater.com/archives/2011/08/26/ness-iphone-app-recommends-restaurants-using-likeness-score.php</ref> or [[Pandora Radio|Pandora]] <ref>http://gigaom.com/2011/08/25/ness-restaurant-app/</ref> for restaurants. Based on a user\'s ratings and preferences, the service delivered recommendations for a particular time, location, price range, and cuisine preference. Users could view the menu for a place via SinglePlatform,<ref>http://www.singleplatform.com/</ref> browse [[Instagram]] photos tagged at the restaurant, and make reservations in the app via [[OpenTable]].\n\n==References==\n{{Reflist}}\n\n[[Category:Information retrieval organizations]]\n[[Category:Software companies based in California]]']
['TeLQAS', '21727808', "'''TeLQAS''' (Telecommunication Literature Question Answering System) is an experimental [[question answering]] system developed for answering English questions in the [[telecommunications]] domain.<ref>Mahmoud R. Hejazi, Maryam S. Mirian , Kourosh Neshatian, Azam Jalali, and Bahadorreza Ofoghi, ''A Telecommunication Literature Question/Answering System Benefits from a Text Categorization Mechanism'', International Conference on Information and Knowledge Engineering (IKE2003), July 2003, USA.</ref>\n\n==Architecture==\nTeLQAS includes three main subsystems: an online subsystem, an offline subsystem, and an [[ontology]]. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a [[natural language processing]] component that implements [[part-of-speech tagging]] and simple [[syntactic parsing]]. The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a ''focused [[web crawler]]'' from the web. An ontology server along with its [[API]] is used for knowledge representation.<ref>Kourosh Neshatian and Mahmoud R. Hejazi, ''An Object Oriented Ontology Interface for Information Retrieval Purposes in Telecommunication Domain'', International Symposium on Telecommunication (IST2003).</ref> The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.\n\n==References==\n<references/>\n\n[[Category:Computational linguistics]]\n[[Category:Information retrieval systems]]\n[[Category:Natural language processing software]]"]
['Category:Music search engines', '28073505', '[[Category:Information retrieval systems]]\n[[Category:Music software|Search engines]]\n[[Category:Internet search engines]]\n[[Category:Online music and lyrics databases]]\n[[Category:Music information retrieval]]']
['Find', '1486231', '{{other uses}}\n{{refimprove|date=June 2016}}\n{{lowercase|title=find}} \nIn [[Unix-like]] and some other [[operating system]]s, <code>\'\'\'find\'\'\'</code> is a [[command-line utility]] that [[Search engine (computing)|searches]] one or more [[directory tree]]s of a [[file system]], locates [[Computer file|file]]s based on some [[user (computing)|user]]-specified criteria and applies a user-specified action on each matched file. The possible search criteria include a [[pattern matching|pattern]] to match against the [[filename]] or a time range to match against the modification time or access time of the file. By default, <code>find</code> returns a list of all files below the current [[working directory]].\n\nThe related <code>[[locate (Unix)|locate]]</code> programs use a database of indexed files obtained through <code>find</code> (updated at regular intervals, typically by <code>[[cron]]</code> job) to provide a faster method of searching the entire file system for files by name.\n\n==History==\n<code>find</code> appeared in [[Version 5 Unix]] as part of the [[PWB/UNIX|Programmer\'s Workbench]] project, and was written by Dick Haight alongside \'\'cpio\'\',<ref name="reader">{{cite techreport |first1=M. D. |last1=McIlroy |authorlink1=Doug McIlroy |year=1987 |url=http://www.cs.dartmouth.edu/~doug/reader.pdf |title=A Research Unix reader: annotated excerpts from the Programmer\'s Manual, 1971–1986 |series=CSTR |number=139 |institution=Bell Labs}}</ref> which were designed to be used together.<ref>{{Cite web|title = libarchive/libarchive|url = https://github.com/libarchive/libarchive/wiki/FormatCpio|website = GitHub|accessdate = 2015-10-04}}</ref>\n\n==Find syntax==\n{{expand section|date=August 2008}}\n<source lang="bash">\n$ find [-H] [-L] [-P] path... [expression]\n</source>\nThe three options control how the <code>find</code> command should treat symbolic links. The default behaviour is never to follow symbolic links. This can be explicitly specified using the -P flag. The -L flag will cause the <code>find</code> command to follow symbolic links. The -H flag will only follow symbolic links while processing the command line arguments. These flags are not available with some older versions of <code>find</code>.\n\nAt least one path must precede the expression. <code>find</code> is capable of interpreting [[Wildcard character|wildcards]] internally and commands must be constructed carefully in order to control [[Glob (programming)|shell globbing]].\n\nExpression elements are whitespace-separated and evaluated from left to right. They can contain logical elements such as AND (&#x2011;and or &#x2011;a) and OR (&#x2011;or &#x2011;o) as well as more complex predicates.\n\nThe [[GNU Find Utilities|GNU]] <code>find</code> has a large number of additional features not specified by POSIX.\n\n==POSIX protection from infinite output==\nReal-world file systems often contain looped structures created through the use of [[hard link|hard]] or [[symbolic link|soft links]]. The [[POSIX|POSIX standard]] requires that\n{{Quotation|\nThe <code>find</code> utility shall detect infinite loops; that is, entering a previously visited\ndirectory that is an ancestor of the last file encountered. When it detects an infinite\nloop, <code>find</code> shall write a diagnostic message to standard error and shall either recover\nits position in the hierarchy or terminate.\n}}\n\n==Operators==\nOperators can be used to enhance the expressions of the find command. Operators are listed in order of decreasing precedence:\n* \'\'\'( expr )\'\'\' - forces precedence;\n* \'\'\'! expr\'\'\' - true if expr is false;\n* \'\'\'expr1 expr2\'\'\' (or \'\'\'expr1 -a expr2\'\'\') - AND. expr2 is not evaluated if expr1 is false;\n* \'\'\'expr1 -o expr2\'\'\' - OR. expr2 is not evaluated if expr1 is true.\n\n<source lang="bash">\n$ find . -name \'fileA_*\' -o -name \'fileB_*\'\n</source>\nThis command searches the current working directory tree for files whose names start with "fileA_" or "fileB_".\n\n<source lang="bash">\n$ find . -name \'foo.cpp\' \'!\' -path \'.svn\'\n</source>\nThis command searches the current working directory tree except the subdirectory tree ".svn" for files whose name is "foo.cpp". We quote the <code>!</code> so that it\'s not interpreted by the shell as the history substitution character.\n\n==Type filter explanation==\nVarious type filters are supported by <code>find</code>. They are activated using the configuration switch:\n<source lang="bash">\n$ find -type x\n</source>\nwhere x may be any of:\n* \'\'\'b\'\'\' - [[Device file|block device (buffered)]];\n* \'\'\'c\'\'\' - [[Device file|character device (unbuffered)]];\n* \'\'\'d\'\'\' - \'\'\'[[Directory (computing)|directory]]\'\'\';\n* \'\'\'f\'\'\' - \'\'\'[[regular file]]\'\'\';\n* \'\'\'l\'\'\' - [[symbolic link]]. This is never true if the -L option or the -follow operator is in effect, unless the symbolic link is broken. If you want to search for symbolic links when -L is in effect, use -xtype (though that is a GNU extension);\n* \'\'\'p\'\'\' - [[named pipe]];\n* \'\'\'s\'\'\' - [[Unix domain socket|socket]];\n* \'\'\'D\'\'\' - [[Doors (computing)|door]].\n\nThe configuration switches listed in bold are most commonly used.\n\n==Examples==\n{{howto|section|date=September 2016}}\n\n===From the current working directory===\n<source lang="bash">\n$ find . -name \'my*\'\n</source>\nThis searches the current working directory tree for files whose names start with \'\'my\'\'. The single quotes avoid the [[shell (computing)|shell]] expansion—without them the shell would replace \'\'my*\'\' with the list of files whose names begin with \'\'my\'\' in the current working directory. In newer versions of the program, the directory may be omitted, and it will imply the current working directory.\n\n===Regular files only===\n<source lang="bash">\n$ find . -name \'my*\' -type f\n</source>\nThis limits the results of the above search to only regular files, therefore excluding directories, special files, symbolic links, etc. \'\'my*\'\' is enclosed in single quotes (apostrophes) as otherwise the shell would replace it with the list of files in the current working directory starting with \'\'my\'\'…\n\n===Commands===\nThe previous examples created listings of results because, by default, <code>find</code> executes the <code>-print</code> action. (Note that early versions of the <code>find</code> command had no default action at all; therefore the resulting list of files would be discarded, to the bewilderment of users.)\n\n<source lang="bash">\n$ find . -name \'my*\' -type f -ls\n</source>\nThis prints extended file information.\n\n===Search all directories===\n<source lang="bash">\n$ find / -name myfile -type f -print\n</source>\nThis searches every directory for a regular file whose name is \'\'myfile\'\' and prints it to the screen. It is generally not a good idea to look for files this way. This can take a considerable amount of time, so it is best to specify the directory more precisely. Some operating systems may mount dynamic file systems that are not congenial to <code>find</code>. More complex filenames including characters special to the shell may need to be enclosed in single quotes.\n\n===Search all but one subdirectory tree===\n<source lang="bash">\n$ find / -path excluded_path -prune -o -type f -name myfile -print\n</source>\nThis searches every directory except the subdirectory tree \'\'excluded_path\'\' (full path including the leading /) that is pruned by the <code>-prune</code> action, for a regular file whose name is \'\'myfile\'\'.\n\n===Specify a directory===\n<source lang="bash">\n$ find /home/weedly -name myfile -type f -print\n</source>\nThis searches the \'\'/home/weedly\'\' directory tree for regular files named \'\'myfile\'\'. You should always specify the directory to the deepest level you can remember.\n\n===Search several directories===\n<source lang="bash">\n$ find local /tmp -name mydir -type d -print\n</source>\nThis searches the \'\'local\'\' subdirectory tree of the current working directory and the \'\'/tmp\'\' directory tree for directories named \'\'mydir\'\'.\n\n===Ignore errors===\nIf you\'re doing this as a user other than root, you might want to ignore permission denied (and any other) errors. Since errors are printed to [[stderr]], they can be suppressed by redirecting the output to /dev/null. The following example shows how to do this in the bash shell:\n<source lang="bash">\n$ find / -name myfile -type f -print 2> /dev/null\n</source>\n\nIf you are a [[C shell|csh]] or [[tcsh]] user, you cannot redirect [[stderr]] without redirecting [[stdout]] as well. You can use sh to run the <code>find</code> command to get around this:\n<source lang="bash">\n$ sh -c "find / -name myfile -type f -print 2> /dev/null"\n</source>\nAn alternate method when using [[C shell|csh]] or [[tcsh]] is to pipe the output from [[stdout]] and [[stderr]] into a [[grep]] command. This example shows how to suppress lines that contain permission denied errors.\n<source lang="bash">\n$ find . -name myfile |& grep -v \'Permission denied\'\n</source>\n\n===Find any one of differently named files===\n<source lang="bash">\n$ find . \\( -name \'*jsp\' -o -name \'*java\' \\) -type f -ls\n</source>\nThe <code>-ls</code> operator prints extended information, and the example finds any regular file whose name ends with either \'jsp\' or \'java\'. Note that the parentheses are required. TIn many shells the parentheses must be escaped with a backslash (<code>\\(</code> and <code>\\)</code>) to prevent them from being interpreted as special shell characters. The <code>-ls</code> operator is not available on all versions of <code>find</code>.\n\n===Execute an action===\n<source lang="bash">\n$ find /var/ftp/mp3 -name \'*.mp3\' -type f -exec chmod 644 {} \\;\n</source>\nThis command changes the [[File system permissions|permissions]] of all regular files whose names end with \'\'.mp3\'\' in the directory tree \'\'/var/ftp/mp3\'\'. The action is carried out by specifying the statement <code>-exec [[chmod]] 644 {} \\;</code> in the command. For every regular file whose name ends in <code>.mp3</code>, the command <code>chmod 644 {}</code> is executed replacing <code>{}</code> with the name of the file. The semicolon (backslashed to avoid the shell interpreting it as a command separator) indicates the end of the command. Permission <code>644</code>, usually shown as <code>rw-r--r--</code>, gives the file owner full permission to read and write the file, while other users have read-only access. In some shells, the <code>{}</code> must be quoted. The trailing ";" is customarily quoted with a leading "\\", but could just as effectively be enclosed in single quotes.\n\nNote that the command itself should *not* be quoted; otherwise you get error messages like\n<source lang="console">\nfind: echo "mv ./3bfn rel071204": No such file or directory\n</source>\nwhich means that <code>find</code> is trying to run a file called \'echo "mv ./3bfn rel071204"\' and failing.\n\nIf you will be executing over many results, it is more efficient to use a variant of the exec primary that collects filenames up to ARG_MAX and then executes COMMAND with a list of filenames.\n\n<source lang="bash">\n$ find . -exec COMMAND {} +\n</source>\nThis will ensure that filenames with whitespaces are passed to the executed COMMAND without being split up by the shell.\n\n===Delete files and directories===\nThe <code>-delete</code> action is a GNU extension, and using it turns on <code>-depth</code>. So, if you are testing a find command with <code>-print</code> instead of <code>-delete</code> in order to figure out what will happen before going for it, you need to use <code>-depth -print</code>.\n\nDelete empty files and print the names (note that <code>-empty</code> is a vendor unique extension from GNU <code>find</code> that may not be available in all <code>find</code> implementations):\n<source lang="bash">\n$ find . -empty -delete -print\n</source>\n\nDelete empty regular files:\n<source lang="bash">\n$ find . -type f -empty -delete\n</source>\n\nDelete empty directories:\n<source lang="bash">\n$ find . -type d -empty -delete\n</source>\n\nDelete empty files named \'bad\':\n<source lang="bash">\n$ find . -name bad -empty -delete\n</source>\n\nWarning. — The <code>-delete</code> action should be used with conditions such as <code>-empty</code> or <code>-name</code>:\n<source lang="bash">\n$ find . -delete # this deletes all in .\n</source>\n\n===Search for a string===\nThis command will search all files from the /tmp directory tree for a string:\n<source lang="bash">\n$ find /tmp -type f -exec grep \'search string\' \'{}\' /dev/null \\+\n</source>\nThe <tt>[[/dev/null]]</tt> argument is used to show the name of the file before the text that is found. Without it, only the text found is printed. \nGNU <code>grep</code> can be used on its own to perform this task:\n<source lang="bash">\n$ grep -r \'search string\' /tmp\n</source>\n\nExample of search for "LOG" in jsmith\'s home directory tree:\n<source lang="bash">\n$ find ~jsmith -exec grep LOG \'{}\' /dev/null \\; -print\n/home/jsmith/scripts/errpt.sh:cp $LOG $FIXEDLOGNAME\n/home/jsmith/scripts/errpt.sh:cat $LOG\n/home/jsmith/scripts/title:USER=$LOGNAME\n</source>\n\nExample of search for the string "ERROR" in all XML files in the current working directory tree:\n<source lang="bash">\n$ find . -name "*.xml" -exec grep "ERROR" /dev/null \'{}\' \\+ \n</source>\nThe double quotes (" ") surrounding the search string and single quotes (<nowiki>\' \'</nowiki>) surrounding the braces are optional in this example, but needed to allow spaces and some other special characters in the string. Note with more complex text (notably in most popular shells descended from `sh` and `csh`) single quotes are often the easier choice, since \'\'double quotes do not prevent all special interpretation\'\'. Quoting filenames which have English contractions demonstrates how this can get rather complicated, since a string with an apostrophe in it is easier to protect with double quotes:\n<source lang="bash">\n$ find . -name "file-containing-can\'t" -exec grep "can\'t" \'{}\' \\; -print\n</source>\n\n===Search for all files owned by a user===\n<source lang="bash">\n$ find . -user <userid>\n</source>\n\n===Search in case insensitive mode===\nNote that <code>-iname</code> is not in the standard and may not be supported by all implementations.\n<source lang="bash">\n$ find . -iname \'MyFile*\'\n</source>\n\nIf the <code>-iname</code> switch is not supported on your system then workaround techniques may be possible such as:\n<source lang="bash">\n$ find . -name \'[mM][yY][fF][iI][lL][eE]*\'\n</source>\n\nThis uses [[Perl]] to build the above command for you (though in general this kind of usage is dangerous, since special characters are not properly quoted before being fed into the standard input of `sh`):\n<source lang="bash">\n$ echo \'MyFile*\' | perl -pe \'s/([a-zA-Z])/[\\L\\1\\U\\1]/g;s/(.*)/find . -name \\1/\' | sh\n</source>\n\n===Search files by size===\nSearching files whose size is between 100 kilobytes and 500 kilobytes:\n<source lang="bash">\n$ find . -size +100k -a -size -500k\n</source>\n\nSearching empty files:\n<source lang="bash">\n$ find . -size 0k\n</source>\n\nSearching non-empty files:\n<source lang="bash">\n$ find . ! -size 0k\n</source>\n\n===Search files by name and size ===\n<source lang="bash">\n$ find /usr/src ! \\( -name \'*,v\' -o -name \'.*,v\' \\) \'{}\' \\; -print\n</source>\nThis command will search the /usr/src directory tree. All files that are of the form \'*,v\' and \'.*,v\' are excluded. Important arguments to note are in the [[tooltip]] that is displayed on mouse-over.\n\n<source lang="bash" enclose="div">\nfor file in `find /opt \\( -name error_log -o -name \'access_log\' -o -name \'ssl_engine_log\' -o -name \'rewrite_log\' -o\n -name \'catalina.out\' \\) -size +300000k -a -size -5000000k`; do \n cat /dev/null > $file\ndone\n</source>\nThe units should be one of [bckw], \'b\' means 512-byte blocks, \'c\' means byte, \'k\' means kilobytes and \'w\' means 2-byte words. The size does not count indirect blocks, but it does count blocks in sparse files that are not actually allocated.\n\n==Related utilities==\n* <code>[[locate (Unix)|locate]]</code> is a Unix search tool that searches a prebuilt database of files instead of directory trees of a file system. This is faster than <code>find</code> but less accurate because the database may not be up-to-date.\n* <code>[[grep]]</code> is a command-line utility for searching plain-text data sets for lines matching a regular expression and by default reporting matching lines on [[standard output]].\n* <code>[[tree (Unix)|tree]]</code> is a command-line utility that recursively lists files found in a directory tree, indenting the filenames according to their position in the file hierarchy.\n* [[GNU Find Utilities]] (also known as findutils) is a [[GNU package]] which contains implementations of the tools <code>find</code> and [[xargs]].\n* [[BusyBox]] is a utility that provides several stripped-down Unix tools in a single executable file, intended for embedded operating systems with very limited resources. It also provides a version of <code>find</code>.\n* <code>[[dir (command)|dir]]</code> has the /s option that recursively searches for files or directories.\n\n==See also==\n* [[mdfind]], a similar utility that utilizes metadata for [[Mac OS X]] and [[Darwin (operating system)|Darwin]]\n* [[List of Unix programs]]\n* [[List of DOS commands]]\n* [[Filter (higher-order function)]]\n* [[find (command)]], a DOS and Windows command that is very different from UNIX <code>find</code>\n\n==References==\n{{reflist}}\n\n==External links==\n* {{man|cu|find|SUS|find files}}\n* [https://www.gnu.org/software/findutils/manual/html_mono/find.html Official webpage for GNU find]\n* [http://www.librebyte.net/en/gnulinux/command-find-25-practical-examples/ Command find – 25 practical examples]\n\n{{Unix commands}}\n\n[[Category:Information retrieval systems]]\n[[Category:Standard Unix programs]]\n[[Category:Unix SUS2008 utilities]]']
['Category:Image search', '25810647', '\n\n[[Category:Applications of computer vision]]\n[[Category:Information retrieval systems]]']
['Phynd', '2353165', '\'\'\'Phynd\'\'\' (Find) is a LAN-indexing [[search engine]] used to facilitate [[peer-to-peer]] [[file sharing]] over a [[local-area network]].  It was developed by [[Rensselaer Polytechnic Institute]] student researcher Jesse Jordan to solve various problems experienced by Microsoft browsers and networks when trying to index files within a large network.  \n\nOne of the results of Jordan\'s file indexing exercise was that large numbers of downloaded music files were found on other users\' local systems.  Jordan was relatively unconcerned with the nature of the content he was indexing.  His objective was enabling a network to index all its files without crashing any elements of the network. <ref>Lessig 2004, p. 48</ref>\n\nAlthough Jordan\'s search engine, Phynd, merely indexed public data that users elected to share through an integrated sharing feature in [[Microsoft Windows]], Jordan was sued by [[RIAA]] for copyright infringement. The original Phynd search engine, rpi.phynd.net (defunct), existing years before and months after Jesse\'s lawsuit was shut down by the enormous pressure that the [[RIAA]] in November 2003 brought upon Jordan and his family. The RIAA was demanding $15,000,000 to settle.<ref>Lessig 2004, p. 51</ref>  As a student researcher, Jordan had only modest life savings of approximately $12,000, and his family had only modest assets.  His limited options were to fight the RIAA at enormous personal expense, or to settle. Jordan, chose to settle outside of court for $12,000, his entire life savings from student employment. He subsequently raised $12,005.67 via contributions on a personal web site in July 2003.\n\n==References==\n*Lessig, Lawrence (2004) . "Free Culture" . ISBN 1-59420-006-8 . The Penguin Press . New York \n\n==Notes==\n{{reflist}} \n\n\n==External links==\n* [http://poly.rpi.edu/old/article_view.php3?view=2599&part=1 Phynd server shut down by threat of lawsuit]\n* {{webarchive |date=2013-01-20 |url=http://archive.is/20130120043253/http://news.com.com/2100-1027-995429.html?tag=fd_lede1_hed |title=RIAA sues campus file-swappers}}\n* [http://www.isp-planet.com/news/2003/riaa_030505.html Students to Pay in RIAA Song-Swapping Suit]\n* [http://articles.chicagotribune.com/2003-07-07/news/0307080008_1_recording-industry-donations-settlement $12,005.67: Amount Jesse Jordan, sued by the recording...]\n\n[[Category:Information retrieval systems]]\n\n{{compu-network-stub}}']
['Greenpilot', '26926858', "{{COI|date=April 2010}}\nThe online portal '''Greenpilot''' is a service provided by the German National Library of Medicine, ZB MED.\n\nThe project is funded by the German Research Foundation ([[Deutsche Forschungsgemeinschaft]]) and gets its technical support from  [[Averbis]] Ltd. The portal first went online May 29, 2009 and currently runs in the updated beta version. In the context of the 'Germany - Land of Ideas' (Deutschland - Land der Ideen) initiative under the patronage of the [[President of Germany]] [[Horst Köhler]] the ZB MED was awarded the distinction 'Selected Landmark 2009' (Ausgewählter Ort 2009).<ref>[http://idw-online.de/pages/de/news315583 Pressemitteilung im Informationsdienst Wissenschaft vom 15. Mai 2009 ]</ref>\n\n==Objective==\nThe Greenpilot portal is a [[digital library]] specialised in the fields of Nutritional, Agricultural and Environmental Sciences. It aims to provide researchers in the three fields with a collection of scientific literature which is easy to access and of high quality. Especially the [[gray literature]] is often difficult to find and retrieve for the average user so Greenpilot also aims to make access to these sources easier. The service addresses itself not only to scientists and students but also to the broadly interested public. Greenpilot has been modelled after the corresponding digital library for Medicine, Medpilot,<ref>[http://www.medpilot.de/ Medpilot portal]</ref> also a project of the German National Library of Medicine. The ZB MED has chosen the slogan 'Greenpilot - all about life and science' as a motto. In Greenpilot scientifically relevant databases, library catalogues and websites can be searched by entering a search term and the results are presented in a standardised web interface.\n\n==Technical Background==\nGreenpilot is a search engine based on intuitive search engine technology. The portal's software was developed in the programming language [[Perl]]. The search engine technology is based upon the 'Averbis Search Platform' software developed by the Averbis Ltd. and uses the [[open source]] software [[Lucene]]. Functionally this is an expert search engine which centres around the intelligent semantic connection of search terms by means of a standardised vocabulary. This is made possible by Averbis's MSI software which provides:\n\n* semantic search optimised for the fields of Medicine and Life Sciences\n* a contextual analysis of texts taking synonyms and compounds into account\n* multilingual and cross-language search\n* linking of lay and expert vocabulary\nThe search results are generated from a search index.\n\nAdditionally a [[metasearch]] can be conducted in order to search other databases not contained in the index. This search is based upon individual results from the specific database searched.\n\n==Contents==\nThe Greenpilot portal integrates various scientifically relevant information resources under a uniform search interface. These resources are diverse and encompass national and international expert databases, library catalogues of national libraries with a focus on specific topics, full text documents from [[open access (publishing)|open access]] journals as well as information contained on about one thousand scientifically relevant websites selected for Greenpilot.\nThe following is a list of sources from November 2009:<ref>[http://www.greenpilot.de/beta2/app/misc/help/8cafcf93601eb861aaef86b5ce99ecdc/Datenbanken List of databases in Greenpilot]</ref>\n\n===Library Catalogues===\n* Catalogue of the German National Library of Medicine (ZB MED Nutrition. Environment. Agriculture)\n* Catalogue of the German National Library of Medicine (ZB MED Medicine. Health)\n* Catalogue of the Bonn University Library\n* Library catalogues of scientifically relevant departments within the collective library network (GBV)\n* Catalogue of the Federal Ministry of Food, Agriculture and Consumer Protection (BMELV)\n* Catalogue of the Johann Heinrich von Thünen-Institut (vTI), Federal Research Institute for Rural Areas, Forestry and Fisheries\n* Catalogue of the Julius Kühn-Institut, Federal Research Centre for Cultivated Plants\n* Catalogue of the Friedrich Löffler-Institut, Federal Research Institute for Animal Health\n* Catalogue of the Max Rubner-Institut, Federal Research Institute for Nutrition and Food\n* Catalogue of the Federal Institute for Risk Assessment\n* Catalogue of the Leibniz Institute for Marine Science (IFM-GEOMAR)\n* Catalogue of the Leibniz Institute for Plant Genetics and Crop Plant Research (IPK-Plant Genetics and Crop Plant)\n* Catalogue of the Leibniz Institute for Plant Biochemistry (IPB-Plant Chemistry)\n* Catalogue of the special collection inshore and deep-sea fishery\n* Catalogue of the University of Veterinary Medicine Hannover (TiHo-Veterinary Sciences)\n* Catalogue of the German National Library of Economics (ZBW)\n\n===Bibliographic databases===\n* AGRIS (1975–2008), FAO ( Food and Agriculture Organization of the United Nations)\n* VITIS-VEA, Viticulture and Enology Abstracts\n* Medline (2004–2009)\n* UFORDAT, Environmental Research Database (UBA)\n* ULIDAT, Environmental Literature Database (UBA)\n* ELFIS, International Information System for the Agricultural Sciences and Technology\n\n===Relevant Internet Sources===\n* Reviewed list of [[URL]]s selected by the ZB MED Nutrition. Environment. Agriculture\n* Open Access journals with full text documents\n\n===Metasearch===\n* GetInfo, the knowledge portal for Technical Science provided by the Library for Technical Sciences (TIB) and the professional information centres FIZ Technik Frankfurt, FIZ Karlsruhe and FIZ CHEMIE Berlin.\n* ECONIS, Catalogue of the German National Library of Economics (ZBW).\n\n==Other Features==\n\n===Search and results page===\n* Search and advanced search\n* Context sensitive help function\n* [[Truncation]] and [[Boolean function]]s\n* Personalised refining of search results by filtering for a specific document type, language or database\n* [[Bookmark]]s\n\n===Document ordering===\n* Ordering directly from the results page is made possible by using the document delivery service of the ZB MED or the Electronic Journals Library ([[Elektronische Zeitschriftenbibliothek]]).\n\n===Personalisation===\n* My Greenpilot: a feature requiring the user to sign up for an account. The service is free of charge and offers an overview of ordered documents as well as enabling individual managing of customer data.\n\n==See also==\n*[[List of digital library projects]]\n*[[vascoda]]\n\n==References==\n<references />\n\n==External links==\n* [http://www.greenpilot.de Greenpilot website]\n* [http://www.zbmed.de/home.html?lang=en Website of the German National Library of Medicine, ZB MED]\n* [http://www.land-of-ideas.org Germany - Land of Ideas website]\n\n{{coord missing|Germany}}\n\n[[Category:Libraries in Germany]]\n[[Category:Information retrieval systems]]\n[[Category:Internet search engines]]"]
['Database search engine', '7330158', 'A \'\'\'database search engine\'\'\' is a [[search engine]] that operates on material stored in a digital [[database]].\n\n== Search engines ==\n\nCategories of search engine software include: \n* Web search or full-text search (e.g. [[Lucene]]).\n* [[Database]] or [[structured data]] search (e.g. [[Dieselpoint]]).\n* Mixed or [[enterprise search]] (e.g. [[Google Search Appliance]]). \n\nThe largest online directories, such as [[Google]] and [[Yahoo]], utilize thousands of computers to process billions of website documents using [[web crawlers]] or [[spiders (software)]], returning results for thousands of searches per second. Processing high query volumes requires software to run in a distributed environment with redundancy.\n\n== Components ==\n\nSearching for textual content in [[databases]] or [[structured data]] formats (such as [[XML]] and [[Comma-separated values|CSV]]) presents special challenges and opportunities which specialized search engines resolve. [[Databases]] allow logical queries such as the use of multi-field [[Boolean logic]], while full-text searches do not. "Crawling" (a human by-eye search) is not necessary to find information stored in a database because the data is already structured. [[index (datatbase)|Indexing]] the data allows for faster searches.\n\nDatabase search engines are usually included with major database software products. \n\n== Applications ==\n\nDatabase search technology is used by large public and private entities including government database services, e-commerce companies, online advertising platforms, telecommunications service providers and other consumers with a need to access information in large repositories.\n\n==See also==\n\n*[[Outline of search engines]]\n*[[List of search engines]]\n\n==External links==\n* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]\n\n{{DEFAULTSORT:Search Engine Technology}}\n[[Category:Information retrieval systems]]']
['Precision and recall', '14343887', '[[File:Precisionrecall.svg|thumb|350px|Precision and recall]]\nIn [[pattern recognition]] and [[information retrieval]] with [[binary classification]], \'\'\'precision\'\'\' (also called [[positive predictive value]]) is the fraction of retrieved instances that are relevant, while \'\'\'recall\'\'\' (also known as [[Sensitivity and specificity|sensitivity]]) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of [[relevance]].\n\nSuppose a computer program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program\'s precision is 4/7 while its recall is 4/9.  When a [[Search engine (computing)|search engine]] returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.  So, in this case, precision is "how useful the search results are", and recall is "how complete the results are".\n\nIn [[statistics]], if the [[null hypothesis]] is that all and only the relevant items are retrieved, absence of [[type I and type II errors]] corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative).  The above pattern recognition example contained 7 &minus; 4 = 3 type I errors and 9 &minus; 4 = 5 type II errors.  Precision can be seen as a measure of exactness or \'\'quality\'\', whereas recall is a measure of completeness or \'\'quantity\'\'.\n\nIn simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.\n\n==Introduction==\nIn an information retrieval scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, "relevant" and "not relevant".  In this case, the "relevant" documents are simply those that belong to the "relevant" category.  Recall is defined as the \'\'number of relevant documents\'\' retrieved by a search \'\'divided by the total number of existing relevant documents\'\', while precision is defined as the \'\'number of relevant documents\'\' retrieved by a search \'\'divided by the total number of documents retrieved\'\' by that search.\n\nIn a [[classification (machine learning)|classification]] task, the precision for a class is the \'\'number of true positives\'\' (i.e. the number of items correctly labeled as belonging to the positive class) \'\'divided by the total number of elements labeled as belonging to the positive class\'\' (i.e. the sum of true positives and [[Type I and type II errors|false positives]], which are items incorrectly labeled as belonging to the class).  Recall in this context is defined as the \'\'number of true positives divided by the total number of elements that actually belong to the positive class\'\' (i.e. the sum of true positives and [[Type I and type II errors|false negatives]], which are items which were not labeled as belonging to the positive class but should have been).\n\nIn information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).\n\nIn a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).\n\nOften, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an illustrative example of the tradeoff.  Consider a brain surgeon tasked with removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).\n\nUsually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. \'\'precision at a recall level of 0.75\'\') or both are combined into a single measure. Examples for measures that are a combination of precision and recall are the [[Precision and recall#F-measure|F-measure]] (the weighted [[harmonic mean]] of precision and recall), or the [[Matthews correlation coefficient]], which is a [[geometric mean]] of the chance-corrected variants: the [[regression coefficient]]s [[Informedness]] (DeltaP\') and [[Markedness]] (DeltaP).<ref name="Powers2011" /><ref>{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119 |doi=10.1016/s0911-6044(03)00059-9}}</ref> [[Accuracy and precision#In binary classification|Accuracy]] is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).<ref name="Powers2011"/> Inverse Precision and Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [[Receiver operating characteristic|ROC]] curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.<ref name="Powers2011"/>  The first problem is \'solved\' by using [[Accuracy and precision#In binary classification|Accuracy]] and the second problem is \'solved\' by discounting the chance component and renormalizing to [[Cohen\'s kappa]], but this no longer affords the opportunity to explore tradeoffs graphically. However, [[Informedness]] and [[Markedness]] are Kappa-like renormalizations of Recall and Precision,<ref>{{cite conference |first=David M. W. |last=Powers |date=2012 |title=The Problem with Kappa |booktitle=Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop}}</ref> and their geometric mean [[Matthews correlation coefficient]] thus acts like a debiased F-measure.\n\n== Definition (information retrieval context) ==\n\nIn [[information retrieval]] contexts, precision and recall are defined in terms of a set of \'\'retrieved documents\'\' (e.g. the list of documents produced by a [[web search engine]] for a query) and a set of \'\'relevant documents\'\' (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. [[relevance]]. The measures were defined in {{harvtxt|Perry|Kent|Berry|1955}}.\n\n===Precision===\n\nIn the field of [[information retrieval]], precision is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the query:\n\n<math display="block"> \\text{precision}=\\frac{|\\{\\text{relevant documents}\\}\\cap\\{\\text{retrieved documents}\\}|}{|\\{\\text{retrieved documents}\\}|} </math>\n\nPrecision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called \'\'precision at n\'\' or \'\'P@n\'\'.\n\nFor example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results.\n\nPrecision is also used with [[recall (information retrieval)|recall]], the percent of \'\'all\'\' relevant documents that is returned by the search. The two measures are sometimes used together in the [[F1 Score]] (or f-measure) to provide a single measurement for a system.\n\nNote that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.\n\n===Recall===\n\nRecall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.\n\n<math display="block"> \\text{recall}=\\frac{|\\{\\text{relevant documents}\\}\\cap\\{\\text{retrieved documents}\\}|}{|\\{\\text{relevant documents}\\}|} </math>\n\nFor example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned.\n\nIn binary classification, recall is called [[Sensitivity and specificity#Sensitivity|sensitivity]]. So it can be looked at as the probability that a relevant document is retrieved by the query.\n\nIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\n\n== Definition (classification context) ==\nFor classification tasks, the terms \'\'true positives\'\', \'\'true negatives\'\', \'\'false positives\'\', and \'\'false negatives\'\' (see [[Type I and type II errors]] for definitions) compare the results of the classifier under test with trusted external judgments.  The terms \'\'positive\'\' and \'\'negative\'\' refer to the classifier\'s prediction (sometimes known as the \'\'expectation\'\'), and the terms \'\'true\'\' and \'\'false\'\' refer to whether that prediction corresponds to the external judgment (sometimes known as the \'\'observation\'\').\n\nLet us define an experiment from \'\'P\'\' positive instances and \'\'N\'\' negative instances for some condition. The four outcomes can be formulated in a 2×2 [[contingency table]] or [[confusion matrix]], as follows:\n\n{{DiagnosticTesting_Diagram}}\n{{Confusion matrix terms}}\n\n<!--\n{| border="0" align="center" style="text-align: center; background: #FFFFFF;"\n|+\n!\n! colspan="2" style="background: #ddffdd;"|actual class <br/> (observation)\n|-\n!\n|-----\n|+\n! rowspan="2" style="background: #ffdddd;"|predicted class <br/> (expectation)\n| \'\'\'tp\'\'\' <br> (true positive) <br/> Correct result\n| \'\'\'fp\'\'\' <br> (false positive) <br/> Unexpected result\n|-bgcolor="#EFEFEF"\n| \'\'\'fn\'\'\' <br> (false negative) <br/> Missing result\n| \'\'\'tn\'\'\' <br> (true negative) <br/> Correct absence of result\n|+\n|}\n\n-->\n\nPrecision and recall are then defined as:<ref name="OlsonDelen">Olson, David L.; and Delen, Dursun (2008); \'\'Advanced Data Mining Techniques\'\', Springer, 1st edition (February 1, 2008), page 138, ISBN 3-540-76916-1</ref>\n\n<math display="block">\\text{Precision}=\\frac{tp}{tp+fp} \\, </math>\n\n<math display="block">\\text{Recall}=\\frac{tp}{tp+fn} \\, </math>\n\nRecall in this context is also referred to as the true positive rate or [[Sensitivity and specificity|sensitivity]], and precision is also referred to as [[positive predictive value]] (PPV); other related measures used in classification include true negative rate and [[Accuracy and precision#In binary classification|accuracy]].<ref name="OlsonDelen" /> True negative rate is also called [[Specificity (tests)#Specificity|specificity]].\n\n<math display="block">\\text{True negative rate}=\\frac{tn}{tn+fp} \\, </math>\n\n<math display="block">\\text{Accuracy}=\\frac{tp+tn}{tp+tn+fp+fn} \\, </math>\n\n== Probabilistic interpretation ==\n\nIt is possible to interpret precision and recall not as ratios but as probabilities:\n\n* Precision is the probability that a (randomly selected) retrieved document is relevant.\n* Recall is the probability that a (randomly selected) relevant document is retrieved in a search.\n\nNote that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by \'\'randomly selected retrieved document\'\', we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected.\n\nNote that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation).\n\nAnother interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.\n\n== F-measure ==\n{{main article|F1 score}}\nA measure that combines precision and recall is the [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score:\n\n<math display="block">F = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{ \\mathrm{precision} + \\mathrm{recall}}</math>\n\nThis measure is approximately the average of the two when they are close, and is more generally the [[harmonic mean]], which, for the case of two numbers, coincides with the square of the [[geometric mean]] divided by the [[arithmetic mean]]. There are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric.<ref name="Powers2011" /> This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.\n\nIt is a special case of the general <math>F_\\beta</math> measure (for non-negative real values of&nbsp;<math>\\beta</math>):\n\n<math display="block">F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall} }{ \\beta^2 \\cdot \\mathrm{precision} + \\mathrm{recall}}</math>\n\nTwo other commonly used <math>F</math> measures are the <math>F_2</math> measure, which weights recall higher than precision, and the <math>F_{0.5}</math> measure, which puts more emphasis on precision than recall.\n\nThe F-measure was derived by van Rijsbergen (1979) so that <math>F_\\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen\'s effectiveness measure <math>E_{\\alpha} = 1 - \\frac{1}{\\frac{\\alpha}{P} + \\frac{1-\\alpha}{R}}</math>, the second term being the weighted harmonic mean of precision and recall with weights <math>(\\alpha, 1-\\alpha)</math>.  Their relationship is <math>F_\\beta = 1 - E_{\\alpha}</math> where <math>\\alpha=\\frac{1}{1 + \\beta^2}</math>.\n\n==Limitations as goals==\nThere are other parameters and strategies for performance metric of information retrieval system, such as the area under the precision-recall curve (AUC).<ref>Zygmunt Zając. What you wanted to know about AUC.  http://fastml.com/what-you-wanted-to-know-about-auc/</ref>\n\nFor [[web document]] retrieval, if the user\'s objectives are not clear, the  precision and recall can\'t be optimized. As summarized by Lopresti,<ref>Lopresti, Daniel (2001); [http://www.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti_files/v3_document.htm \'\'WDA 2001 panel\'\']</ref>\n{{quote|[[Browsing]] is a comfortable and powerful paradigm (the [[Serendipity|serendipity effect]]).\n* Search results don\'t have to be very good.\n* Recall?    Not important (as long as you get at least some good hits).\n* Precision? Not important (as long as at least some of the hits on the first page you return are good).}}\n\n==See also==\n* [[Uncertainty coefficient]], also called \'\'proficiency\'\'\n* [[Sensitivity and specificity]]\n\n== References ==\n{{Reflist}}\n{{refbegin}}\n* Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier (1999). \'\'Modern Information Retrieval\'\'. New York, NY: ACM Press, Addison-Wesley, Seiten 75 ff. ISBN 0-201-39829-X\n* Hjørland, Birger (2010); \'\'The foundation of the concept of relevance\'\', Journal of the American Society for Information Science and Technology, 61(2), 217-237\n* Makhoul, John; Kubala, Francis; Schwartz, Richard; and Weischedel, Ralph (1999); [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4637 \'\'Performance measures for information extraction\'\'], in \'\'Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999\'\'\n* {{cite journal |title=Machine literature searching X. Machine language; factors underlying its design and development |year=1955 |doi=10.1002/asi.5090060411}}\n* van Rijsbergen, Cornelis Joost "Keith" (1979); \'\'Information Retrieval\'\', London, GB; Boston, MA: Butterworth, 2nd Edition, ISBN 0-408-70929-4\n{{refend}}\n\n== External links ==\n* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval – C. J. van Rijsbergen 1979]\n* [http://www.text-analytics101.com/2014/10/computing-precision-and-recall-for.html Computing Precision and Recall for a Multi-class Classification Problem]\n\n[[Category:Information retrieval evaluation]]\n[[Category:Information science]]\n[[Category:Bioinformatics]]\n\n[[de:Beurteilung eines Klassifikators#Anwendung im Information Retrieval]]']
["Mooers' law", '11373842', '{{For|the observation regarding integrated circuits|Moore\'s law}}\n{{Refimprove|date=September 2011}}\n\n\'\'\'Mooers\' law\'\'\' is an empirical observation of behavior made by American [[computer scientist]] [[Calvin Mooers]] in 1959. The observation is made in relation to [[information retrieval]] and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.\n\n{{quote|An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.|[[Calvin Mooers]]<ref name="morville">{{cite book|url=https://books.google.com/books?id=xJNLJXXbhusC&printsec=frontcover&dq=isbn:9780596007652&hl=en&sa=X&ei=qvWhT5DfHITs2QX1rNzPCA&ved=0CDAQ6AEwAA#v=onepage&q=mooers\'%20law&f=false |title= Ambient findability |series= O\'Reilly Series. Marketing/Technology & Society |author= Peter Morville |edition= illustrated |publisher= O\'Reilly Media |year= 2005 |page= 44|isbn= 978-0-596-00765-2}}</ref>}}\n\n==Original interpretation==\n\nMooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the user\'s personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.<ref>{{cite web|last=Mooers|first=Calvin|title=Mooers Law, or Why some Retrieval Systems are Used and Others Are not|url=http://findarticles.com/p/articles/mi_qa3633/is_199610/ai_n8749122/|work=Business Library|accessdate=25 October 2011}}</ref>\n\n==Out-of-context interpretation==\n\nThe more commonly used interpretation of Mooers\' law is considered to be a derivation of the [[principle of least effort]] first stated by [[George Kingsley Zipf]]. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker \'gives up\', and the Law is often paraphrased to increase the focus on the retrieval system:\n\n{{quote|The more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.|J. Michael Pemberton}}\n{{quote|Mooers\' Law tells us that information will be used in direct proportion to how easy it is to obtain.|Roger K. Summit <ref name="morville"/>}}\n\nIn this interpretation, "painful and troublesome" comes from \'\'using\'\' the retrieval system.\n\n==References==\n{{reflist}}\n\n*{{cite journal |last=Austin |first=Brice |date=June 2001 |title=Mooers\' Law: In and out of Context |journal=Journal of the American Society for Information Science and Technology |volume=25 |issue=8 |pages=607–609 |url=http://spot.colorado.edu/~norcirc/Mooers.html |accessdate=2007-05-23 |doi=10.1002/asi.1114}}\n\n==External links==\n* [http://special.lib.umn.edu/findaid/xml/cbi00081.xml Calvin N. Mooers Papers, 1930-1992] at the [[Charles Babbage Institute]], University of Minnesota.\n* [http://purl.umn.edu/107510 Oral history interview with Calvin N. Mooers and Charlotte D. Mooers] at the [[Charles Babbage Institute]].  Interview discusses information retrieval and programming language research from World War II through the early 1990s.\n[[Category:Empirical laws]]\n[[Category:Information retrieval evaluation]]']
['Universal IR Evaluation', '26591446', '{{Multiple issues|\n{{refimprove|date=April 2011}}\n{{orphan|date=April 2010}}\n}}\n\nIn [[computer science]], \'\'universal [[Information retrieval evaluation|IR (information retrieval) evaluation]]\'\' aims to develop measures of database retrieval performance that shall be comparable across all information retrieval tasks.\n\n==Measures of "relevance"==\n[[Information retrieval evaluation|IR (information retrieval) evaluation]] begins whenever a user submits a query (search term) to a [[database]]. If the user is able to determine the [[Relevance (information retrieval)|relevance]] of each document in the database (relevant or not relevant), then for each query, the complete set of documents is naturally divided into four distinct (mutually exclusive) subsets: relevant documents that are retrieved, not relevant documents that are retrieved, relevant documents that are not retrieved, and not relevant documents that are not retrieved. These four subsets (of documents) are denoted by the letters a,b,c,d respectively and are called Swets variables, named after their inventor.<ref>Swets, J.A. (1969). Effectiveness of information retrieval methods. \'\'American Documentation, 20\'\'(1), 72-89.</ref>\n\nIn addition to the Swets definitions, four relevance metrics have also been defined: [[Precision (information retrieval)|Precision]] refers to the fraction of relevant documents that are retrieved (a/(a+b)), and [[Precision (information retrieval)|Recall]] refers to the fraction of retrieved documents that are relevant (a/(a+c)). These are the most commonly used and well-known relevance metrics found in the IR evaluation literature. Two less commonly used metrics include the Fallout, i.e., the fraction of not relevant documents that are retrieved (b/(b+d)), and the Miss, which refers to the fraction of relevant documents that are not retrieved (c/(c+d)) during any given search.\n\n==Universal IR evaluation techniques==\nUniversal IR evaluation addresses the mathematical possibilities and relationships among the four relevance metrics Precision, Recall, Fallout and Miss, denoted by P, R, F and M, respectively. One aspect of the problem involves finding a mathematical derivation of a complete set of universal IR evaluation points.<ref>Schatkun, M. (2010). A Second look at Egghe\'s universal IR surface and a simple derivation of a complete set of universal IR evaluation points. \'\'Information Processing & Management, 46\'\'(1), 110-114.</ref> The complete set of 16 points, each one a quadruple of the form (P,R,F,M), describes all the possible universal IR outcomes. For example, many of us have had the experience of querying a database and not retrieving any documents at all. In this case, the Precision would take on the undetermined form 0/0, the Recall and Fallout would both be zero, and the Miss would be any value greater than zero and less than one (assuming a mix of relevant and not relevant documents were in the database, none of which were retrieved). This universal IR evaluation point would thus be denoted by (0/0, 0, 0, M), which represents only one of the 16 possible universal IR outcomes.\n\nThe mathematics of universal IR evaluation is a fairly new subject since the relevance metrics P,R,F,M were not analyzed collectively until recently (within the past decade). A lot of the theoretical groundwork has already been formulated, but new insights in this area await discovery. For a detailed mathematical analysis, a query in the [[ScienceDirect]] database for "universal IR evaluation" retrieves several relevant peer-reviewed papers.\n\n==See also==\n* [[Information retrieval]]\n* [[Web search query]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.sciencedirect.com Science Direct]\n\n{{DEFAULTSORT:Universal Ir Evaluation}}\n[[Category:Databases]]\n[[Category:Information retrieval evaluation]]']
['Divergence-from-randomness model', '1798853', "In the field of [[information retrieval]], '''divergence from randomness''' is one type of [[probabilistic]] model.\n\nTerm weights are computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution.\n\n==External links==\n*[http://terrier.org/docs/v3.5/dfr_description.html Terrier's DFR Web page]\n*[http://ir.dcs.gla.ac.uk/wiki/DivergenceFromRandomness Glasgow IR group Wiki DFR page]\n\n[[Category:Ranking functions]]\n[[Category:Information retrieval techniques]]\n[[Category:Probabilistic models]]\n\n\n{{comp-sci-stub}}"]
['Uncertain inference', '25962276', "'''Uncertain inference''' was first described by [[C. J. van Rijsbergen]]<ref>{{cite | author=C. J. van Rijsbergen | title=A non-classical logic for information retrieval | publisher=The Computer Journal | pages=481–485 | year=1986}}</ref> as a way to formally define a query and document relationship in [[Information retrieval]]. This formalization is a [[logical consequence|logical implication]] with an attached measure of uncertainty.\n\n==Definitions==\nRijsbergen proposes that the measure of [[uncertainty]] of a document ''d'' to a query ''q'' be the probability of its logical implication, i.e.:\n\n:<math>P(d \\to q)</math>\n\nA user's query can be interpreted as a set of assertions about the desired document. It is the system's task to [[inference|infer]], given a particular document, if the query assertions are true. If they are, the document is retrieved.\nIn many cases the contents of documents are not sufficient to assert the queries. A [[knowledge base]] of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as ''plausible inference''. The [[plausibility]] of an inference <math>d \\to q</math> is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.\nSince ''d'' and ''q'' are both generated by users, they are error prone; thus <math>d \\to q</math> is uncertain. This will affect the plausibility of a given query.\n\nBy doing this it accomplishes two things:\n* Separate the processes of revising probabilities from the logic\n* Separate the treatment of relevance from the treatment of requests\n\n[[Multimedia]] documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.\n\nUncertain inference generalizes the notions of [[autoepistemic logic]], where truth values are either known or unknown, and when known, they are true or false.\n\n==Example==\nIf we have a query of the form:\n\n:<math>q = A \\wedge B \\wedge C</math>\n\nwhere A, B and C are query assertions, then for a document D we want the probability:\n\n:<math>P (D \\to (A \\wedge B \\wedge C))</math>\n\nIf we transform this into the [[conditional probability]] <math>P ((A \\wedge B \\wedge C) | D)</math> and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.\n\n==Further work==\nCroft and Krovetz<ref>{{cite | title=Interactive retrieval office documents | url=http://doi.acm.org/10.1145/45410.45435 | author1=W. B. Croft | author2=R. Krovetz | year=1988 }}</ref> applied uncertain inference to an information retrieval system for office documents they called ''OFFICER''. In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.\n\n[[Probabilistic logic network]]s is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.\n\n[[Markov logic network]]s allow uncertain inference to be performed; uncertainties are computed using the [[maximum entropy principle]], in analogy to the way that [[Markov chain]]s describe the uncertainty of [[finite state machine]]s.\n\n== See also ==\n* [[Fuzzy logic]]\n* [[Probabilistic logic]]\n* [[Plausible reasoning]]\n* [[Imprecise probability]]\n\n==References==\n{{reflist}}\n\n[[Category:Fuzzy logic]]\n[[Category:Information retrieval techniques]]\n[[Category:Inference]]"]
['Search suggest drop-down list', '23344134', 'A \'\'\'search suggest drop-down list\'\'\' is a [[Query language|query]] feature used in [[computing]] to show the searcher [[Computer shortcut|shortcut]]s, while the query is typed into a [[text box]]. Before the query is complete, a[[drop-down list]] with the suggested completions appears to provide options to select. The suggested queries then enable the searcher to complete the required search quickly. As a form of [[Autocomplete|autocompletion]], the suggestion list is distinct from [[web browsing history|search history]] in that it attempts to be predictive even when the user is searching for the first time. Data may come from popular searches, sponsors, geographic location or other sources.<ref>{{cite web|url=http://www.thingsontop.com/googles-new-search-suggestions-may-kill-your-website-158.html|title=Google\'s new search suggestions may kill your website|first=Vegard|last=Sandvoid|publisher=Things On Top|date=2008-12-14|accessdate=2016-08-03}}</ref><ref>{{cite web|url=https://diegobasch.com/search-for-obama-on-facebook-and-you-get-romney|title=Search for Obama on Facebook and you get Romney|first=Diego|last=Basch|date=2012-09-19|accessdate=2016-08-03}}</ref><ref name="se-land">{{cite web|url=http://searchengineland.com/how-google-instant-autocomplete-suggestions-work-62592|title=How Google Instant\'s autocomplete suggestions work|first=Danny|last=Sullivan|publisher=Search Engine Land|date=2011-04-06|accessdate=2016-08-03}}</ref> These lists are used by [[operating system]]s, [[web browsers]] and various [[website]]s, particularly [[search engine]]s. Search suggestions are common with a 2014 survey finding that over 80% of [[e-commerce]] websites included them.<ref>{{cite web|url=https://www.smashingmagazine.com/2014/08/the-current-state-of-e-commerce-search/|title=The current state of e-commerce search|first=Christian|last=Holt|publisher=Smash Magazine|date=2014-08-18|accessdate=2016-08-03}}</ref>\n\nThe [[Computer science|computing science]] of [[syntax]] and [[algorithm]]s are used to form search results from a [[database]]. [[Content management system]]s and frequent searches can assist [[Software engineering|software engineers]] in [[Optimization (computer science)|optimizing]] more refined queries with methods of parameters and subroutines. Suggestions can be results for the current query or related queries by words, time and dates, categories and [[Tag (metadata)|tags]]. The suggestion list may be reordered by other options, as [[Enumeration|enumerative]], [[Hierarchical organization|hierarchical]] or [[Faceted classification|faceted]].\n\nAlthough not the first deployment of search suggestions, [[Google Suggest]] is one of the most prominent. Four years before it was considered stable, the feature was developed in 2004 by Google engineer Kevin Gibbs and the name was chosen by [[Marissa Mayer]].<ref>{{cite web|url=http://allthingsd.com/20130823/nearly-a-decade-later-the-autocomplete-origin-story-kevin-gibbs-and-google-suggest/|title=Nearly a Decade Later, the Autocomplete Origin Story: Kevin Gibbs and Google Suggest|first=Liz|last=Gannes|publisher=All Things D|date=2013-08-23|accessdate=2016-08-03}}</ref> Google, and other large search companies, maintain a blacklist that prevents the display of queries that could be interpreted as violating their [[social responsibility]]. Despite this, the company regularly receives complaints that several popular suggestions, or suggestions whose positions have been inflated by [[Internet bot|bots]], should be added to this list.<ref name="se-land" /><ref>{{cite web|url=https://utopiaordystopia.com/2015/02/22/truth-and-prediction-in-the-dataclysm/|title=Truth and Prediction in the Dataclysm|first=Rick|last=Searle|publisher=Utopia or Dystopia|date=2015-02-22|accessdate=2016-08-03}}</ref> The [[Electronic Frontier Foundation]]\'s [[Jillian York]] has criticized [[Apple Computers|Apple]]\'s blacklist for including words that are merely provocative.<ref>{{cite web|url=http://www.thedailybeast.com/articles/2013/07/16/the-apple-kill-list-what-your-iphone-doesn-t-want-you-to-type.html|title=The Apple \'Kill List\': What your iPhone doesn\'t want you to type|first=Michael|last=Keller|publisher=The Daily Beast|date=2013-07-16|accessdate=2016-08-03}}</ref>\n\nOne example of a project using suggested queries to expose societal attitudes was a 2013 ad series called \'\'The Autocomplete Truth\'\' by [[UN Women]]. The campaign showed several gender stereotypes being displayed as popular searches by Google Suggest.<ref>{{cite web|url=http://www.adweek.com/adfreak/after-viral-success-inequality-ads-creators-say-they-will-expand-campaign-153363|title=After viral succes of inequality ads, creators say they will expand campaign|first=David|last=Griner|publisher=Ad Week|date=2013-10-24|accessdate=2016-08-03}}</ref> Another was a story by [[Bad Astronomy]] that revealed a distrustful perspective on scientists in the suggestion box.<ref>{{cite web|url=http://www.slate.com/blogs/bad_astronomy/2013/12/04/search_engine_bias_scientists_are.html|title="Scientists are..."|first=Phil|last=Plait|publisher=Slate|date=2013-12-04|accessdate=2016-08-03}}</ref> Additionally, cases related to [[libel]] laws have posited that suggestions may inspire people to associate specific names with specific alleged crimes when they would not have otherwise.<ref name="japan">{{cite web|url=http://www.tamingthebeast.net/blog/online-world/google-autocomplete-angst.htm|title=Some Folks *Really* Hate Autocomplete|first=Michael|last=Bloch|publisher=Taming The Beast|date=2012-03-27|accessdate=2016-08-03}}</ref><ref>{{cite journal|url=http://ijlit.oxfordjournals.org/content/23/3/261.full|title=Search engine liability for autocomplete suggestions: personality, privacy and the power of the algorithm|first1=Stavroula|last1=Karapapa|first2=Maurizio|last2=Borghi|journal=International Journal of Law and Information Technology|volume=23|pages=261-289|year=2015|accessdate=2016-08-03}}</ref>\n\nSome users have criticized the fact that suggestion-enabled text boxes, unlike the [[web forms]] of static HTML, send data about each keystroke to a central server.<ref>{{cite web|url=http://thekeesh.com/2011/08/who-does-facebook-think-you-are-searching-for/|title=Who does Facebook think you are searching for?|first=Jeremy|last=Keeshin|publisher=The Keesh|date=2011-08-18|accessdate=2016-08-03}}</ref> Such data has the potential to [[keystroke dynamics|identify specific people]]. This has caused at least one [[Mozilla Firefox]] developer to opine that "users mostly dislike search suggestions".<ref>{{cite web|url=https://bugzilla.mozilla.org/show_bug.cgi?id=1189719|title=Recall and display search history within main browser UI|first=Richard|last=Newman|publisher=Mozilla|date=2015-08-25|accessdate=2016-08-03}}</ref> Apart from the privacy debate, some users have expressed negative reception over the usefulness of search autocompletion.<ref name="japan" /><ref>{{cite web|url=http://arnoldit.com/wordpress/2012/09/10/google-autocomplete-is-smart-help-a-hindrance/|title=Google Autocomplete: Is Smart Help A Hindrance?|first=Stephen|last=Arnold|publisher=Beyond Search|date=2012-09-09|accessdate=2016-08-03}}</ref><ref>{{cite web|url=https://www.quora.com/How-do-very-strange-stupid-auto-complete-statements-appear-while-searching-on-Google-Do-people-actually-do-these-kind-of-searches-or-are-they-pun-intended|title=How do very strange/stupid auto-complete statements appear while searching on Google? Do people actually do these kind of searches or are they pun intended?|first=Seshal|last=Jain|publisher=Quora|date=2015-05-02|accessdate=2016-08-03}}</ref> Specifically, the sudden appearance of a suggestion box in some programs has been compared to the behaviour of a [[pop-up ad]].<ref>{{cite web|url=http://martesmartes.blogspot.com/2008/07/disabling-openoffices-stupid.html|title=Disabling Open Office\'s Stupid Autocomplete|first=Jeff|last=Martens|publisher=Martes-Martes|date=2008-07-09|accessdate=2016-08-03}}</ref><ref>{{cite web|url=https://mikesmithers.wordpress.com/2012/01/28/turning-off-code-completion-in-sqldeveloper-a-grumpy-old-man-fights-back/|title=Turning off code completion in SQLDeveloper &mdash; A grumpy old man fights back|first=Mike|last=Smithers|publisher=The Anti-Kyte|date=2012-01-28|accessdate=2016-08-03}}</ref>\n\n==See also==\n*[[Autocomplete]]\n*[[Search engine (computing)]]\n*[[Search box]]\n*[[Search algorithm]]\n*[[Censorship by Google#Search suggestions|Censorship by Google § Search suggestions]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Search Suggest Drop-Down List}}\n[[Category:Information retrieval techniques]]']
['Category:Vector space model', '36475839', '[[Category:Information retrieval techniques]]']
['Statistical semantics', '7271261', '{{linguistics}}\n\'\'\'Statistical semantics\'\'\' is the study of "how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access" {{citation needed|date=July 2012}}<!--([[George Furnas|Furnas]], 2006)--this page has been moved and the new version no longer contains this quotation-->. How can we figure out what words mean, simply by looking at patterns of words in huge collections of text? What are the limits to this approach to understanding words?\n\n==History==\n\nThe term \'\'statistical semantics\'\' was first used by [[Warren Weaver]] in his well-known paper on [[machine translation]].<ref>{{harvnb|Weaver|1955}}</ref> He argued that [[word sense disambiguation]] for machine translation should be based on the [[co-occurrence]] frequency of the context words near a given target word. The underlying assumption that "a word is characterized by the company it keeps" was advocated by [[J. R. Firth|J.R. Firth]].<ref>{{harvnb|Firth|1957}}</ref> This assumption is known in [[linguistics]] as the [[distributional hypothesis]].<ref>{{harvnb|Sahlgren|2008}}</ref> Emile Delavenay defined \'\'statistical semantics\'\' as the "Statistical study of meanings of words and their frequency and order of recurrence."<ref>{{harvnb|Delavenay|1960}}</ref> "[[George Furnas|Furnas]] et al. 1983" is frequently cited as a foundational contribution to statistical semantics.<ref>{{harvnb|Furnas|Landauer|Gomez|Dumais|1983}}</ref>  An early success in the field was [[latent semantic analysis]].\n\n==Applications==\n\nResearch in statistical semantics has resulted in a wide variety of algorithms that use the distributional hypothesis to discover many aspects of [[semantics]], by applying statistical techniques to [[Text corpus|large corpora]]:\n* Measuring the [[Semantic similarity|similarity in word meanings]]<ref>{{harvnb|Lund|Burgess|Atchley|1995}}</ref><ref>{{harvnb|Landauer|Dumais|1997}}</ref><ref>{{harvnb|McDonald|Ramscar|2001}}</ref><ref>{{harvnb|Terra|Clarke|2003}}</ref>\n* Measuring the similarity in word relations <ref>{{harvnb|Turney|2006}}</ref>\n* Modeling [[similarity-based generalization]]<ref>{{harvnb|Yarlett|2008}}</ref>\n* Discovering words with a given relation<ref>{{harvnb|Hearst|1992}}</ref>\n* Classifying relations between words<ref>{{harvnb|Turney|Littman|2005}}</ref>\n* Extracting keywords from documents<ref>{{harvnb|Frank|Paynter|Witten|Gutwin|1999}}</ref><ref>{{harvnb|Turney|2000}}</ref>\n* Measuring the cohesiveness of text<ref>{{harvnb|Turney|2003}}</ref>\n* Discovering the different senses of words<ref>{{harvnb|Pantel|Lin|2002}}</ref>\n* Distinguishing the different senses of words<ref>{{harvnb|Turney|2004}}</ref>\n* Subcognitive aspects of words<ref>{{harvnb|Turney|2001}}</ref>\n* Distinguishing praise from criticism<ref>{{harvnb|Turney|Littman|2003}}</ref>\n\n==Related fields==\n\nStatistical Semantics focuses on the meanings of common words and the relations between common words, unlike [[text mining]], which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical Semantics is a subfield of [[computational semantics]], which is in turn a subfield of [[computational linguistics]] and [[natural language processing]].\n\nMany of the applications of Statistical Semantics (listed above) can also be addressed by [[lexicon]]-based algorithms, instead of the [[text corpus|corpus]]-based algorithms of Statistical Semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.<ref>{{harvnb|Turney|Littman|Bigham|Shnayder|2003}}</ref>\n\n==See also==\n{{Portal|Linguistics}}\n{{div col|3}}\n*[[Co-occurrence]]\n*[[Computational linguistics]]\n*[[Information retrieval]]\n*[[Latent semantic analysis]]\n*[[Latent semantic indexing]]\n*[[Natural language processing]]\n*[[Semantic analytics]]\n*[[Semantic similarity]]\n*[[Text corpus]]\n*[[Text mining]]\n*[[Web mining]]\n{{div col end}}\n\n==References==\n{{reflist|2}}\n\n===Sources===\n{{refbegin}}\n* {{cite book | last = Delavenay | first = Emile | year = 1960 | title = An Introduction to Machine Translation | location = New York, NY | publisher = [[Thames and Hudson]] | oclc = 1001646 | ref = harv }}\n* {{cite journal | last = Firth | first = John R. | authorlink = John Rupert Firth | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = [[Studies in Linguistic Analysis]] | pages = 1–32 | location = Oxford | publisher = [[Philological Society]] | ref = harv }}\n*: Reprinted in {{cite book | editor1-first = F.R. | editor1-last = Palmer | title = Selected Papers of J.R. Firth 1952-1959 | location = London | publisher = Longman | year = 1968 | oclc = 123573912 }}\n* {{cite conference | last1 = Frank | first1 = Eibe | last2 = Paynter | first2 = Gordon W. | last3 = Witten | first3 = Ian H. | last4 = Gutwin | first4 = Carl | last5 = Nevill-Manning | first5 = Craig G. | year = 1999 | title = Domain-specific keyphrase extraction | booktitle = Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence | conference = [[International Joint Conference on Artificial Intelligence|IJCAI-99]] | volume = 2 | pages = 668–673 | location = California | publisher = Morgan Kaufmann | isbn = 1-55860-613-0 | citeseerx = 10.1.1.148.3598 | ref = harv }}\n* {{cite journal | last1 = Furnas | first1 = George W. | authorlink = George Furnas | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | year = 1983 | title = Statistical semantics: Analysis of the potential performance of keyword information systems | url = https://web.archive.org/web/*/http://furnas.people.si.umich.edu/Papers/FurnasEtAl1983_BSTJ_p1753.pdf | journal = [[Bell System Technical Journal]] | volume = 62 | issue = 6 | pages = 1753–1806 | ref = harv | doi=10.1002/j.1538-7305.1983.tb03513.x}}\n* {{cite conference | last = Hearst | first = Marti A. | year = 1992 | title = Automatic Acquisition of Hyponyms from Large Text Corpora | booktitle = Proceedings of the Fourteenth International Conference on Computational Linguistics | conference = [[COLING|COLING \'92]] | pages = 539–545 | location = Nantes, France | url = http://acl.ldc.upenn.edu/C/C92/C92-2082.pdf | doi = 10.3115/992133.992154 | citeseerx = 10.1.1.36.701 | ref = harv }}\n* {{cite journal | last1 = Landauer | first1 = Thomas K. | last2 = Dumais | first2 = Susan T. | year = 1997 | title = A solution to Plato\'s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge | journal = [[Psychological Review]] | volume = 104 | issue = 2 | pages = 211–240 | url = http://lsa.colorado.edu/papers/plato/plato.annote.html | citeseerx = 10.1.1.184.4759 | ref = harv | doi=10.1037/0033-295x.104.2.211}}\n* {{cite conference | last1 = Lund | first1 = Kevin | last2 = Burgess | first2 = Curt | last3 = Atchley | first3 = Ruth Ann | year = 1995 | title = Semantic and associative priming in high-dimensional semantic space | booktitle = Proceedings of the 17th Annual Conference of the Cognitive Science Society | publisher = [[Cognitive Science Society]] | pages = 660–665 | url = http://locutus.ucr.edu/reprintPDFs/lba95csp.pdf | ref = harv }}\n* {{cite conference | last1 = McDonald | first1 = Scott | last2 = Ramscar | first2 = Michael | year = 2001 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611–616 | url = http://homepages.inf.ed.ac.uk/smcdonal/cogsci2001.pdf | citeseerx = 10.1.1.104.7535 | ref = harv }}\n* {{cite conference | last1 = Pantel | first1 = Patrick | last2 = Lin | first2 = Dekang | year = 2002 | title = Discovering word senses from text | booktitle = Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining | isbn = 1-58113-567-X | conference = [[KDD Conference|KDD \'02]] | pages = 613–619 | citeseerx = 10.1.1.12.6771 | doi = 10.1145/775047.775138 | ref = harv }}\n* {{cite journal | last1 = Sahlgren | first1 = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 33–53 | ref = harv}}\n* {{cite conference | last1 = Terra | first1 = Egidio L. | last2 = Clarke | first2 = Charles L. A. | year = 2003 | title = Frequency estimates for statistical word similarity measures | booktitle = Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 | conference = HLT/NAACL 2003 | pages = 244–251 | url = http://acl.ldc.upenn.edu/N/N03/N03-1032.pdf | citeseerx = 10.1.1.12.9041 | doi = 10.3115/1073445.1073477 | ref = harv }}\n* {{cite journal | last = Turney | first = Peter D. |date=May 2000 | title = Learning algorithms for keyphrase extraction | journal = [[Information Retrieval (journal)|Information Retrieval]] | volume = 2 | issue = 4 | pages = 303–336 | arxiv = cs/0212020 | citeseerx = 10.1.1.11.1829 | doi = 10.1023/A:1009976227802 | ref = harv }}\n* {{cite journal | last = Turney | first = Peter D. | year = 2001 | title = Answering subcognitive Turing Test questions: A reply to French | journal = [[Journal of Experimental and Theoretical Artificial Intelligence]] | volume = 13 | issue = 4 | pages = 409–419 | arxiv = cs/0212015 | citeseerx = 10.1.1.12.8734 | ref = harv | doi=10.1080/09528130110100270}}\n* {{cite conference | last = Turney | first = Peter D. | year = 2003 | title = Coherent keyphrase extraction via Web mining | booktitle = Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence | conference = IJCAI-03 | location = Acapulco, Mexico | pages = 434–439 | arxiv = cs/0308033 | citeseerx = 10.1.1.100.3751 | ref = harv }}\n* {{cite conference | last = Turney | first = Peter D. | year = 2004 | title = Word sense disambiguation by Web mining for word co-occurrence probabilities | booktitle = Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | conference = SENSEVAL-3 | location = Barcelona, Spain | pages = 239–242 | arxiv = cs/0407065 | url = http://cogprints.org/3732/ | ref = harv }}\n* {{cite journal | last = Turney | first = Peter D. | year = 2006 | title = Similarity of semantic relations |journal = [[Computational Linguistics (journal)|Computational Linguistics]] | volume = 32 | issue = 3 | pages = 379–416 | arxiv = cs/0608100 | url = http://cogprints.org/5098/ | doi = 10.1162/coli.2006.32.3.379 | citeseerx = 10.1.1.75.8007 | ref = harv }}\n* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. |date=October 2003 | title = Measuring praise and criticism: Inference of semantic orientation from association | journal = [[ACM Transactions on Information Systems]] | volume = 21 | issue = 4 | pages = 315–346 | arxiv = cs/0309034 | url = http://cogprints.org/3164/ | citeseerx = 10.1.1.9.6425 | doi = 10.1145/944012.944013 | ref = harv }}\n* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | year = 2005 | title = Corpus-based Learning of Analogies and Semantic Relations | journal = [[Machine Learning (journal)|Machine Learning]] | volume = 60 | issue = 1–3 | pages = 251–278 | arxiv = cs/0508103 | citeseerx = 10.1.1.90.9819 | doi = 10.1007/s10994-005-0913-1 | url = http://cogprints.org/4518/ | ref = harv }}\n* {{cite conference | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | last3 = Bigham | first3 = Jeffrey | last4 = Shnayder | first4 = Victor | year = 2003 | title = Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems | booktitle = Proceedings of the International Conference on Recent Advances in Natural Language Processing | conference = RANLP-03 | location = [[Borovets]], Bulgaria | pages = 482–489 | arxiv = cs/0309035 | citeseerx = 10.1.1.5.2939 | url = http://cogprints.org/3163/ | ref = harv }}\n* {{cite book | last = Weaver | first = Warren | authorlink = Warren Weaver | year = 1955 | chapter = Translation | chapter-url = http://www.mt-archive.info/Weaver-1949.pdf | editor1-first = W.N. | editor1-last = Locke | editor2-first = D.A. | editor2-last = Booth | title = Machine Translation of Languages | location = [[Cambridge, Massachusetts]] | publisher = [[MIT Press]] | isbn = 0-8371-8434-7 | pages = 15–23 | ref = harv }}\n* {{cite thesis | last = Yarlett | first = Daniel G. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv }}\n{{refend}}\n\n\n{{DEFAULTSORT:Statistical Semantics}}\n[[Category:Artificial intelligence applications]]\n[[Category:Computational linguistics]]\n[[Category:Information retrieval techniques]]\n[[Category:Semantics]]\n[[Category:Statistical natural language processing]]\n[[Category:Applied statistics]]']
['Document clustering', '14663145', '{{Multiple issues|\n{{disputed|date=March 2014}}\n{{more footnotes|date=March 2014}}\n}}\n\n\'\'\'Document clustering\'\'\' (or \'\'\'text clustering\'\'\') is the application of [[cluster analysis]] to textual documents. It has applications in automatic document organization, [[topic (linguistics)|topic]] extraction and fast [[information retrieval]] or filtering.\n\n==Overview==\nDocument clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.\n\nThe application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared to offline applications.\n\nIn general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward\'s method.  By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the [[K-means algorithm]] and its variants. Generally hierarchical algorithms produce more in-depth information for detailed analyses, while algorithms based around variants of the [[K-means algorithm]] are more efficient and provide sufficient information for most purposes.<ref name="manning">Manning, Chris, and Hinrich Schütze, \'\'Foundations of Statistical Natural Language Processing\'\', MIT Press. Cambridge, MA: May 1999.</ref>{{rp|Ch.14}}\n\nThese algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment – each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft – a document’s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters.<ref name="manning"/>{{rp|499}} [[Dimensionality reduction]] methods can be considered a subtype of soft clustering; for documents, these include [[latent semantic indexing]] ([[truncated singular value decomposition]] on term histograms)<ref>http://nlp.stanford.edu/IR-book/pdf/16flat.pdf</ref> and [[topic model]]s.\n\nOther algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering.\n\nGiven a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. [[Cluster labeling|Various methods]] exist for this purpose.\n\n==Clustering in search engines==\nA [[web search engine]] often  returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information.  Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by e.g. open source software such as [[Carrot2]].\n\n==Procedures==\nIn practice, document clustering often takes the following steps:\n \n1. [[Tokenization (lexical analysis)|Tokenization]]\n\nTokenization is the process of parsing text data into smaller units (tokens) such as words and phrases. Commonly used tokenization methods include [[Bag-of-words model]] and [[N-gram model]].\n\n2. [[Stemming]] and [[lemmatization]]\n\nDifferent tokens might carry out similar information (e.g. tokenization and tokenizing). And we can avoid calculating similar information repeatedly by reducing all tokens to its base form using various stemming and lemmatization dictionaries.\n\n3. Removing [[stop words]] and [[punctuation]]\n\nSome tokens are less important than others. For instance, common words such as "the" might not be very helpful for revealing the essential characteristics of a text. So usually it is a good idea to eliminate stop words and punctuation marks before doing further analysis.\n\n4. Computing term frequencies or [[tf-idf]]\n\nAfter pre-processing the text data, we can then proceed to generate features. For document clustering, one of the most common ways to generate features for a document is to calculate the term frequencies of all its tokens. Although not perfect, these frequencies can usually provide some clues about the topic of the document. And sometimes it is also useful to weight the term frequencies by the inverse document frequencies. See [[tf-idf]] for detailed discussions.\n\n5. Clustering\n\nWe can then cluster different documents based on the features we have generated. See the algorithm section in [[cluster analysis]] for different types of clustering methods.\n\n6. Evaluation and visualization\n\nFinally, the clustering models can be assessed by various metrics. And it is sometimes helpful to visualize the results by plotting the clusters into low (two) dimensional space. See [[multidimensional scaling]] as a possible approach.\n\n== Clustering v. Classifying ==\nClustering algorithms in computational text analysis groups documents into what are called subsets or \'\'clusters\'\' where the algorithm\'s goal is to create internally coherent clusters that are distinct from one another.<ref>{{Cite web|url=http://nlp.stanford.edu/IR-book/|title=Introduction to Information Retrieval|website=nlp.stanford.edu|pages=349|access-date=2016-05-03}}</ref> Classification on the other hand, is a form of [[supervised learning]] where the features of the documents are used to predict the "type" of documents.\n\n== References ==\n{{reflist}}\nPublications:\n* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. \'\'Flat Clustering\'\' in <u>Introduction to Information Retrieval.</u> Cambridge University Press. 2008\n* Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [http://eprints.cs.vt.edu/archive/00001000/01/docclust.pdf]\n* Claudio Carpineto, Stanislaw Osiński, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys, Volume 41, Issue 3 (July 2009), Article No. 17, {{ISSN|0360-0300}}\n\n==See also==\n*[[Cluster Analysis]]\n*[[Fuzzy clustering]]\n\n[[Category:Information retrieval techniques]]']
['Fuzzy retrieval', '25935906', '\'\'\'Fuzzy retrieval\'\'\' techniques are based on the [[Extended Boolean model]] and the [[Fuzzy set]] theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the [[Extended Boolean model|P-norms]] algorithm.\n\n==Mixed Min and Max model (MMM)==\n\nIn fuzzy-set theory, an element has a varying degree of membership, say \'\'d<sub>A</sub>\'\', to a given set \'\'A\'\' instead of the traditional membership choice (is an element/is not an element).<br />\nIn MMM<ref>{{citation | last1=Fox | first1=E. A. | author2=S. Sharat | year=1986 | title=A Comparison of Two Methods for Soft Boolean Interpretation in Information Retrieval | publisher=Technical Report TR-86-1, Virginia Tech, Department of Computer Science}}</ref> each index term has a fuzzy set associated with it. A document\'s weight with respect to an index term \'\'A\'\' is considered to be the degree of membership of the document in the fuzzy set associated with \'\'A\'\'. The degree of membership for union and intersection are defined as follows in Fuzzy set theory:<br/>\n:<math>d_{A\\cap B}= min(d_A, d_B)</math>\n:<math>d_{A\\cup B}= max(d_A,d_B)</math>\n\nAccording to this, documents that should be retrieved for a query of the form \'\'A or B\'\', should be in the fuzzy set associated with the union of the two sets \'\'A\'\' and \'\'B\'\'. Similarly, the documents that should be retrieved for a query of the form \'\'A and B\'\', should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the \'\'or\'\' query to be \'\'max(d<sub>A</sub>, d<sub>B</sub>)\'\' and the similarity of the document to the \'\'and\'\' query to be \'\'min(d<sub>A</sub>, d<sub>B</sub>)\'\'. The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the \'\'min\'\' and \'\'max\'\' document weights.\n\nGiven a document \'\'D\'\' with index-term weights \'\'d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>\'\' for terms \'\'A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>n</sub>\'\', and the queries:\n\n\'\'Q<sub>or</sub> = (A<sub>1</sub> or A<sub>2</sub> or ... or A<sub>n</sub>)\'\'<br />\n\'\'Q<sub>and</sub> = (A<sub>1</sub> and A<sub>2</sub> and ... and A<sub>n</sub>)\'\'\n\nthe query-document similarity in the MMM model is computed as follows:\n\n\'\'SlM(Q<sub>or</sub>, D) = C<sub>or1</sub> * max(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>or2</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>)\'\'<br />\n\'\'SlM(Q<sub>and</sub>, D) = C<sub>and1</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>and2</sub> * max(d<sub>A1</sub>, d<sub>A2</sub> ..., d<sub>An</sub>)\'\'\n\nwhere \'\'C<sub>or1</sub>, C<sub>or2</sub>\'\' are "softness" coefficients for the \'\'or\'\' operator, and \'\'C<sub>and1</sub>, C<sub>and2</sub>\'\' are softness coefficients for the \'\'and\'\' operator. Since we would like to give the maximum of the document weights more importance while considering an \'\'or\'\' query and the minimum more importance while considering an \'\'and\'\' query, generally we have \'\'C<sub>or1</sub> > C<sub>or2</sub> and C<sub>and1</sub> > C<sub>and2</sub>\'\'. For simplicity it is generally assumed that \'\'C<sub>or1</sub> = 1 - C<sub>or2</sub>\'\' and \'\'C<sub>and1</sub> = 1 - C<sub>and2</sub>\'\'.\n\nLee and Fox<ref name="leefox">{{citation | last1=Lee | first1=W. C. | author2=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}</ref> experiments indicate that the best performance usually occurs with \'\'C<sub>and1</sub>\'\' in the range [0.5, 0.8] and with \'\'C<sub>or1</sub>\'\' > 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the [[Standard Boolean model]].\n\n==Paice model==\n\nThe Paice model<ref>{{citation | last=Paice | first=C. P. | year=1984 | title=Soft Evaluation of Boolean Search Queries in Information Retrieval Systems | publisher=Information Technology, Res. Dev. Applications, 3(1), 33-42 }}</ref> is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity:\n\n:<math>S(D,Q) = \\sum_{i=1}^n\\frac{r^{i-1}*w_{di}}{\\sum_{j=1}^n r^{j-1}}</math>\n\nwhere \'\'r\'\' is a constant coefficient and \'\'w<sub>di</sub>\'\' is arranged in ascending order for \'\'and\'\' queries and descending order for \'\'or\'\' queries. When n = 2 the Paice model shows the same behavior as the MMM model.\n\nThe experiments of Lee and Fox<ref name="leefox"/> have shown that setting the \'\'r\'\' to 1.0 for \'\'and\'\' queries and 0.7 for \'\'or\'\' queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of \'\'min\'\' or \'\'max\'\' of a set of term weights each time an \'\'and\'\' or \'\'or\'\' clause is considered, which can be done in \'\'O(n)\'\'. The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an \'\'and\'\' clause or an \'\'or\'\' clause is being considered. This requires at least an \'\'0(n log n)\'\' sorting algorithm. A good deal of floating point calculation is needed too.\n\n==Improvements over the Standard Boolean model==\nLee and Fox<ref name="leefox"/> compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement:\n{| class="wikitable"\n|-\n!\n! CISI\n! CACM\n! INSPEC\n|-\n! MMM\n| 68%\n| 109%\n| 195%\n|-\n! Paice\n| 77%\n| 104%\n| 206%\n|}\n\nThese are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three.\n\n==Recent work==\n\nRecently \'\'\'Kang \'\'et al.\'\'\'.<ref>{{citation | title=Fuzzy Information Retrieval Indexed by Concept Identification | url=http://www.springerlink.com/content/ac96v4qf4f8adatp/ | last1=Kang | first1=Bo-Yeong | author2=Dae-Won Kim |author3=Hae-Jung Kim | publisher=Springer Berlin / Heidelberg | year=2005}}</ref> have devised a fuzzy retrieval system indexed by concept identification.\n\nIf we look at documents on a pure [[Tf-idf]] approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document.<br />\nThey report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents.\n\nZadrozny<ref>{{citation | title=Fuzzy information retrieval model revisited | doi=10.1016/j.fss.2009.02.012 | first1=Sławomir | last1=Zadrozny | last2=Nowacka | first2=Katarzyna | year=2009 | publisher=Elsevier North-Holland, Inc.}}</ref> revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by:\n* assuming linguistic terms as importance weights of keywords also in documents\n* taking into account the uncertainty concerning the representation of documents and queries\n* interpreting the linguistic terms in the representation of documents and queries as well as their matching in terms of the Zadeh’s fuzzy logic (calculus of linguistic statements)\n* addressing some pragmatic aspects of the proposed model, notably the techniques of indexing documents and queries\n\nThe proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.\n\n==See also==\n*[[Information retrieval]]\n\n==Further reading==\n* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last1=Fox | first1=E. | author2=S. Betrabet | author3=M. Koushik | author4=W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Fuzzy Retrieval}}\n[[Category:Information retrieval techniques]]']
['Preference learning', '34072838', '\'\'\'Preference learning\'\'\' is a subfield in [[machine learning]] in which the goal is to learn a predictive [[Preference (economics)|preference]] model from observed preference information.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) \'\'Foundations of Machine Learning\'\', The\nMIT Press ISBN 9780262018258.</ref> In the view of [[supervised learning]], preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.\n\nWhile the concept of preference learning has been emerged for some time in many fields such as [[economics]],<ref name="SHOG00" /> it\'s a relatively new topic in [[Artificial Intelligence]] research. Several workshops have been discussing preference learning and related topics in the past decade.<ref name="WEB:WORKSHOP" />\n\n==Tasks==\n\nThe main task in preference learning concerns problems in "[[learning to rank]]". According to different types of preference information observed, the tasks are categorized as three main problems in the book \'\'Preference Learning\'\':<ref name="FURN11" />\n\n===Label ranking===\n\nIn label ranking, the model has an instance space <math>X=\\{x_i\\}\\,\\!</math> and a finite set of labels <math>Y=\\{y_i|i=1,2,\\cdots,k\\}\\,\\!</math>. The preference information is given in the form <math>y_i \\succ_{x} y_j\\,\\!</math> indicating instance <math>x\\,\\!</math> shows preference in <math>y_i\\,\\!</math> rather than <math>y_j\\,\\!</math>. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.\n\nIt was observed some conventional [[Classification in machine learning|classification]] problems can be generalized in the framework of label ranking problem:<ref name="HARP03" /> if a training instance <math>x\\,\\!</math> is labeled as class <math>y_i\\,\\!</math>, it implies that <math>\\forall j \\neq i, y_i \\succ_{x} y_j\\,\\!</math>. In the [[Multi-label classification|multi-label]] case, <math>x\\,\\!</math> is associated with a set of labels <math>L \\subseteq Y\\,\\!</math> and thus the model can extract a set of preference information <math>\\{y_i \\succ_{x} y_j | y_i \\in L, y_j \\in Y\\backslash L\\}\\,\\!</math>. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.\n\n===Instance ranking===\n\nInstance ranking also has the instance space <math>X\\,\\!</math> and label set <math>Y\\,\\!</math>. In this task, labels are defined to have a fixed order <math>y_1 \\succ y_2 \\succ \\cdots \\succ y_k\\,\\!</math> and each instance <math>x_l\\,\\!</math> is associated with a label <math>y_l\\,\\!</math>. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.\n\n===Object ranking===\n\nObject ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form <math>x_i \\succ x_j\\,\\!</math> and the model should find out a ranking order among instances.\n\n==Techniques==\n\nThere are two practical representations of the preference information <math>A \\succ B\\,\\!</math>. One is assigning <math>A\\,\\!</math> and <math>B\\,\\!</math> with two real numbers <math>a\\,\\!</math> and <math>b\\,\\!</math> respectively such that <math>a > b\\,\\!</math>. Another one is assigning a binary value <math>V(A,B) \\in \\{0,1\\}\\,\\!</math> for all pairs <math>(A,B)\\,\\!</math> denoting whether <math>A \\succ B\\,\\!</math> or <math>B \\succ A\\,\\!</math>. Corresponding to these two different representations, there are two different techniques applied to the learning process.\n\n===Utility function===\n\nIf we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called [[utility function]]. For label ranking the mapping is a function <math>f: X \\times Y \\rightarrow \\mathbb{R}\\,\\!</math> such that <math>y_i \\succ_x y_j \\Rightarrow f(x,y_i) > f(x,y_j)\\,\\!</math>. For instance ranking and object ranking, the mapping is a function <math>f: X \\rightarrow \\mathbb{R}\\,\\!</math>.\n\nFinding the utility function is a [[Regression analysis|regression]] learning problem which is well developed in machine learning.\n\n===Preference relations===\n\nThe binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz, Johannes and Hüllermeier proposed this approach in label ranking problem.<ref name="FURN03" /> For object ranking, there is an early approach by Cohen et al.<ref name="COHE98" />\n\nUsing preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.<ref name="FURN03" />\n\n==Uses==\n\nPreference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu\'s survey paper.<ref name="LIU09" />\n\nAnother application of preference learning is [[recommender systems]].<ref name="GEMM09" /> Online store may analyze customer\'s purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user\'s ratings to provide more user preferred contents.\n\n==See also==\n*[[Learning to rank]]\n\n==References==\n\n{{Reflist|\nrefs=\n\n<ref name="SHOG00">{{\ncite journal\n|last       = Shogren\n|first      = Jason F. |author2=List, John A. |author3=Hayes, Dermot J.\n|year       = 2000\n|title      = Preference Learning in Consecutive Experimental Auctions\n|url        = http://econpapers.repec.org/article/oupajagec/v_3a82_3ay_3a2000_3ai_3a4_3ap_3a1016-1021.htm\n|journal    = American Journal of Agricultural Economics\n|volume     = 82\n|pages      = 1016–1021\n|doi=10.1111/0002-9092.00099\n}}</ref>\n\n<ref name="WEB:WORKSHOP">{{\ncite web\n|title      = Preference learning workshops\n|url        = http://www.preference-learning.org/#Workshops\n}}</ref>\n\n<ref name="FURN11">{{\ncite book\n|last       = F&uuml;rnkranz\n|first      = Johannes\n|coauthors  = H&uuml;llermeier, Eyke\n|year       = 2011\n|title      = Preference Learning\n|url        = https://books.google.com/books?id=nc3XcH9XSgYC\n|chapter    = Preference Learning: An Introduction\n|chapterurl = https://books.google.com/books?id=nc3XcH9XSgYC&pg=PA4\n|publisher  = Springer-Verlag New York, Inc.\n|pages      = 3–8\n|isbn       = 978-3-642-14124-9\n}}</ref>\n\n<ref name="HARP03">{{\ncite journal\n|last       = Har-peled\n|first      = Sariel |author2=Roth, Dan |author3=Zimak, Dav\n|year       = 2003\n|title      = Constraint classification for multiclass classification and ranking\n|journal    = In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, NIPS-02\n|pages      = 785–792\n}}</ref>\n\n<ref name="FURN03">{{\ncite journal\n|last       = F&uuml;rnkranz\n|first      = Johannes\n|coauthors  = H&uuml;llermeier, Eyke\n|year       = 2003\n|title      = Pairwise Preference Learning and Ranking\n|journal    = Proceedings of the 14th European Conference on Machine Learning\n|pages      = 145–156\n}}</ref>\n\n<ref name="COHE98">{{\ncite journal\n|last       = Cohen\n|first      = William W. |author2=Schapire, Robert E. |author3=Singer, Yoram\n|year       = 1998\n|title      = Learning to order things\n|url        = http://dl.acm.org/citation.cfm?id=302528.302736\n|journal    = In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems\n|pages      = 451–457\n}}</ref>\n\n<ref name="LIU09">{{\ncite journal\n|last       = Liu\n|first      = Tie-Yan\n|year       = 2009\n|title      = Learning to Rank for Information Retrieval\n|url        = http://dl.acm.org/citation.cfm?id=1618303.1618304\n|journal    = Foundations and Trends in Information Retrieval\n|volume     = 3\n|issue      = 3\n|pages      = 225–331\n|doi        = 10.1561/1500000016\n}}</ref>\n\n<ref name="GEMM09">{{\ncite journal\n|last       = Gemmis\n|first      = Marco De\n|author2=Iaquinta, Leo |author3=Lops, Pasquale |author4=Musto, Cataldo |author5=Narducci, Fedelucio |author6= Semeraro,Giovanni \n|year       = 2009\n|title      = Preference Learning in Recommender Systems\n|url        = http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/preference-learning.pdf#page=45\n|journal    = PREFERENCE LEARNING\n|volume     = 41\n|pages      = 387–407\n|doi=10.1007/978-3-642-14125-6_18\n}}</ref>\n\n}}\n\n==External links==\n*[http://www.preference-learning.org/ Preference Learning site]\n\n[[Category:Information retrieval techniques]]\n[[Category:Machine learning]]']
['Collaborative filtering', '480289', '{{external links|date=November 2013}}\n{{Use dmy dates|date=June 2013}}\n{{Recommender systems}}\n[[File:Collaborative filtering.gif|300px|thumb|\n\nThis image shows an example of predicting of the user\'s rating using [[Collaborative software|collaborative]] filtering. At first, people rate different items (like videos, images, games). After that, the system is making [[prediction]]s about user\'s rating for an item, which the user hasn\'t rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in our case the system has made a prediction, that the active user won\'t like the video.]]\n\n\'\'\'Collaborative filtering\'\'\' (\'\'\'CF\'\'\') is a technique used by [[recommender system]]s.<ref name="handbook">Francesco Ricci and Lior Rokach and Bracha Shapira, [http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf Introduction to Recommender Systems Handbook], Recommender Systems Handbook, Springer, 2011, pp. 1-35</ref> Collaborative filtering has two senses, a narrow one and a more general one.<ref name=recommender>{{cite web|title=Beyond Recommender Systems: Helping People Help Each Other|url=http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf|publisher=Addison-Wesley|accessdate=16 January 2012|page=6|year=2001|last1=Terveen|first1=Loren|last2=Hill|first2=Will|authorlink1=Loren Terveen}}</ref>  \n\nIn the newer, narrower sense, collaborative filtering is a method of making automatic [[prediction]]s (filtering) about the interests of a user by collecting preferences or [[taste (sociology)|taste]] information from [[crowdsourcing|many users]] (collaborating). The underlying assumption of the collaborative filtering approach is that if a person \'\'A\'\' has the same opinion as a person \'\'B\'\' on an issue, A is more likely to have B\'s opinion on a different issue \'\'x\'\' than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for [[television]] tastes could make predictions about which television show a user should like given a partial list of that user\'s tastes (likes or dislikes).<ref>[http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations An integrated approach to TV & VOD Recommendations] {{webarchive |url=https://web.archive.org/web/20120606225352/http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations |date=6 June 2012 }}</ref> Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an [[average]] (non-specific) score for each item of interest, for example based on its number of [[vote]]s.\n\nIn the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc.<ref name="recommender" />  Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications  where the focus is on user data, etc. The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.\n\n==Introduction==\nThe [[internet growth|growth]] of the [[Internet]] has made it much more difficult to effectively [[information extraction|extract useful information]] from all the available [[online information]]. The overwhelming amount of data necessitates  mechanisms for efficient [[information filtering]]. Collaborative filtering is one of the techniques used for dealing with this problem.\n\nThe motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with tastes similar to themselves. Collaborative filtering encompasses techniques for matching people with similar interests and making [[recommender system|recommendations]] on this basis.\n\nCollaborative filtering algorithms often require (1) users\' active participation, (2) an easy way  to represent users\' interests, and (3) algorithms that are able to match people with similar interests.\n\nTypically, the workflow of a collaborative filtering system is:\n# A user expresses his or her preferences by rating items (e.g. books, movies or CDs) of the system. These ratings can be viewed as an approximate representation of the user\'s interest in the corresponding domain.\n# The system matches this user\'s ratings against other users\'  and finds the people with most "similar" tastes.\n# With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user (presumably the absence of rating is often considered as the unfamiliarity of an item)\nA key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.\n\n==Methodology==\n\n[[File:Collaborative Filtering in Recommender Systems.jpg|thumb|Collaborative Filtering in Recommender Systems]]\n\nCollaborative filtering systems have many forms, but many common systems can be reduced to two steps:\n# Look for users who share the same rating patterns with the active user (the user whom the prediction is for).\n# Use the ratings from those like-minded users found in step 1 to calculate a prediction for the active user\nThis falls under the category of user-based collaborative filtering. A specific application of this is the user-based [[K-nearest neighbor algorithm|Nearest Neighbor algorithm]].\n\nAlternatively, [[item-item collaborative filtering|item-based collaborative filtering]] (users who bought x also bought y), proceeds in an item-centric manner:\n# Build an item-item matrix determining relationships between pairs of items\n# Infer the tastes of the current user by examining the matrix and matching that user\'s data\nSee, for example, the [[Slope One]] item-based collaborative filtering family.\n\nAnother form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user\'s behavior in the future, or to predict how a user might like to behave given the chance.  These predictions then have to be filtered through [[business logic]] to determine how they might affect the actions of a business system.  For example, it is not useful to offer to sell somebody a particular album of music if they already have demonstrated that they own that music.\n\nRelying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as [[WWW|web]] search and [[data clustering]].\n\n==Types==\n\n===Memory-based===\nThis approach uses user rating data to compute the similarity between users or items. This is used for making recommendations. This was an early approach used in many commercial systems. It\'s effective and easy to implement. Typical examples of this approach are neighbourhood-based CF and item-based/user-based top-N recommendations. For example, in user based approaches, the value of ratings user \'u\' gives to item \'i\' is calculated as an aggregation of some similar users\' rating of the item:\n:<math>r_{u,i} = \\operatorname{aggr}_{u^\\prime \\in U} r_{u^\\prime, i}</math>\n\nwhere \'U\' denotes the set of top \'N\' users that are most similar to user \'u\' who rated item \'i\'. Some examples of the aggregation function includes:\n:<math>r_{u,i} = \\frac{1}{N}\\sum\\limits_{u^\\prime \\in U}r_{u^\\prime, i}</math>\n:<math>r_{u,i} = k\\sum\\limits_{u^\\prime \\in U}\\operatorname{simil}(u,u^\\prime)r_{u^\\prime, i}</math>\n:<math>r_{u,i} = \\bar{r_u} +  k\\sum\\limits_{u^\\prime \\in U}\\operatorname{simil}(u,u^\\prime)(r_{u^\\prime, i}-\\bar{r_{u^\\prime}} )</math>\n\nwhere k is a normalizing factor defined as <math>k =1/\\sum_{u^\\prime \\in U}|\\operatorname{simil}(u,u^\\prime)| </math>. and <math>\\bar{r_u}</math> is the average rating of user u for all the items rated by u.\n\nThe neighborhood-based algorithm calculates the similarity between two users or items produces a prediction for the user by taking the [[weighted average]] of all the ratings. Similarity computation between items or users is an important part of this approach. Multiple measures, such as [[Pearson product-moment correlation coefficient|Pearson correlation]] and [[Cosine similarity|vector cosine]] based similarity are used for this.\n\nThe Pearson correlation similarity of two users x, y is defined as \n:<math> \\operatorname{simil}(x,y) = \\frac{\\sum\\limits_{i \\in I_{xy}}(r_{x,i}-\\bar{r_x})(r_{y,i}-\\bar{r_y})}{\\sqrt{\\sum\\limits_{i \\in I_{xy}}(r_{x,i}-\\bar{r_x})^2\\sum\\limits_{i \\in I_{xy}}(r_{y,i}-\\bar{r_y})^2}} </math>\n\nwhere I<sub>xy</sub> is the set of items rated by both user x and user y.\n\nThe cosine-based approach defines the cosine-similarity between two users x and y as:<ref name="Breese1999">John S. Breese, David Heckerman, and Carl Kadie, [http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=231&proceeding_id=14 Empirical Analysis of Predictive Algorithms for Collaborative Filtering], 1998 {{webarchive |url=https://web.archive.org/web/20131019134152/http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=231&proceeding_id=14 |date=19 October 2013 }}</ref>\n:<math>\\operatorname{simil}(x,y) = \\cos(\\vec x,\\vec y) = \\frac{\\vec x \\cdot \\vec y}{||\\vec x|| \\times ||\\vec y||} = \\frac{\\sum\\limits_{i \\in I_{xy}}r_{x,i}r_{y,i}}{\\sqrt{\\sum\\limits_{i \\in I_{x}}r_{x,i}^2}\\sqrt{\\sum\\limits_{i \\in I_{y}}r_{y,i}^2}}</math>\n\nThe user based top-N recommendation algorithm uses a similarity-based vector model to identify the k most similar users to an active user. After the k most similar users are found, their corresponding user-item matrices are aggregated to identify the set of items to be recommended. A popular method to find the similar users is the [[Locality-sensitive hashing]], which implements the [[Nearest neighbor search|nearest neighbor mechanism]] in linear time.\n\nThe advantages with this approach include: the explainability of the results, which is an important aspect of recommendation systems; easy creation and use; easy facilitation of new data; content-independence of the items being recommended; good scaling with co-rated items.\n\nThere are also several disadvantages with this approach. Its performance decreases when [[sparsity|data gets sparse]], which occurs frequently with web-related items. This hinders the [[scalability]] of this approach and creates problems with large datasets. Although it can efficiently handle new users because it relies on a [[data structure]], adding new items becomes more complicated since that representation usually relies on a specific [[vector space]]. Adding new items requires inclusion of the new item and the re-insertion of all the elements in the structure.\n\n===Model-based===\nModels are developed using [[data mining]], [[machine learning]] algorithms to find patterns based on training data. These are used to make predictions for real data. There are many model-based CF algorithms. These include [[Bayesian networks]], [[Cluster Analysis|clustering models]], [[Latent Semantic Indexing|latent semantic models]] such as [[singular value decomposition]], [[probabilistic latent semantic analysis]], multiple multiplicative factor, [[latent Dirichlet allocation]] and [[Markov decision process]] based models.<ref name="Suetal2009">Xiaoyuan Su, Taghi M. Khoshgoftaar, [http://www.hindawi.com/journals/aai/2009/421425/ A survey of collaborative filtering techniques], Advances in Artificial Intelligence archive, 2009.</ref>\n\nThis approach has a more holistic goal to uncover latent factors that explain observed ratings.<ref>[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] {{webarchive |url=https://web.archive.org/web/20101023032716/http://research.yahoo.com/pub/2435 |date=23 October 2010 }}</ref> Most of the models are based on creating a classification or clustering technique to identify the user based on the training set. The number of the parameters can be reduced based on types of [[Principal Component Analysis|principal component analysis]].\n\nThere are several advantages with this paradigm. It handles the sparsity better than memory based ones. This helps with scalability with large data sets. It improves the prediction performance. It gives an intuitive rationale for the recommendations.\n\nThe disadvantages with this approach are in the expensive model building. One needs to have a tradeoff between prediction performance and scalability. One can lose useful information due to reduction models. A number of models have difficulty explaining the predictions.\n\n===Hybrid===\nA number of applications combine the memory-based and the model-based CF algorithms. These overcome the limitations of native CF approaches and improve prediction performance. Importantly, they overcome the CF problems such as sparsity and loss of information. However, they have increased complexity and are expensive to implement.<ref>{{cite journal | url = http://www.sciencedirect.com/science/article/pii/S0020025512002587 | doi=10.1016/j.ins.2012.04.012 | volume=208 | title=Kernel-Mapping Recommender system algorithms | journal=Information Sciences | pages=81–104}}\n</ref> Usually most commercial recommender systems are hybrid, for example, the Google news recommender system.<ref>{{cite web|url=http://dl.acm.org/citation.cfm?id=1242610|title=Google news personalization|publisher=}}</ref>\n\n==Application on social web==\nUnlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like [[Reddit]], [[YouTube]], and [[Last.fm]] are typical example of collaborative filtering based media.<ref>[http://www.readwriteweb.com/archives/collaborative_filtering_social_web.php Collaborative Filtering: Lifeblood of The Social Web]</ref>\n\nOne scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of [[Reddit]] as they are "voted up" (rated positively) by the community. As the community becomes larger and more diverse, the promoted stories can better reflect the average interest of the community members.\n\nAnother aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis. The more a given user makes use of the system, the better the recommendations become, as the system gains data to improve its model of that user.\n\n===Problems===\nA collaborative filtering system does not necessarily succeed in automatically matching content to one\'s preferences. Unless the platform achieves unusually good diversity and independence of opinions, one point of view will always dominate another in a particular community. As in the personalized recommendation scenario, the introduction of new users or new items can cause the [[cold start]] problem, as there will be insufficient data on these new entries for the collaborative filtering to work accurately. In order to make appropriate recommendations for a new user, the system must first learn the user\'s preferences by analysing past voting or rating activities. The collaborative filtering system requires a substantial number of users to rate a new item before that item can be recommended.\n\n==Challenges==\n\n===Data sparsity===\nIn practice, many commercial recommender systems are based on large datasets. As a result, the user-item matrix used for collaborative filtering could be extremely large and sparse, which brings about the challenges in the performances of the recommendation.\n\nOne typical problem caused by the data sparsity is the [[cold start]] problem. As collaborative filtering methods recommend items based on users\' past preferences,  new users will need to rate sufficient number of items to enable the system to capture their preferences accurately and thus provides reliable recommendations.\n\nSimilarly,  new items also have the same problem. When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them. The new item problem does not limit the [[Content-based filtering|content-based recommendation]], because the recommendation of an item is based on its discrete set of descriptive qualities rather than its ratings.\n\n===Scalability===\nAs the numbers of users and items grow, traditional CF algorithms will suffer serious scalability problems{{Citation needed|date=April 2013}}. For example, with tens of millions of customers <math>O(M)</math> and millions of items <math>O(N)</math>, a CF algorithm with the complexity of <math>n</math> is already too large. As well, many systems need to react immediately to online requirements and make recommendations for all users regardless of their purchases and ratings history, which demands a higher scalability of a CF system. Large web companies such as Twitter use clusters of machines to scale recommendations for their millions of users, with most computations happening in very large memory machines.<ref name="twitterwtf">Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web</ref>\n\n===Synonyms===\n[[Synonyms]] refers to the tendency of a number of the same or very similar items to have different names or entries. Most recommender systems are unable to discover this latent association and thus treat these products differently.\n\nFor example, the seemingly different items "children movie" and "children film" are actually referring to the same item. Indeed, the degree of variability in descriptive term usage is greater than commonly suspected.{{citation needed|date=September 2013}} The prevalence of synonyms decreases the recommendation performance of CF systems. Topic Modeling (like the [[Latent Dirichlet Allocation]] technique) could solve this by grouping different words belonging to the same topic.{{citation needed|date=September 2013}}\n\n===Gray sheep===\nGray sheep refers to the users whose opinions do not consistently agree or disagree with any group of people and thus do not benefit from collaborative filtering. [[Black sheep]] are the opposite group whose idiosyncratic tastes make recommendations nearly impossible. Although this is a failure of the recommender system, non-electronic recommenders also have great problems in these cases, so black sheep is an acceptable failure.\n\n===Shilling attacks===\nIn a recommendation system where everyone can give the ratings, people may give lots of positive ratings  for their own items and negative ratings for their competitors. It is often necessary for the collaborative filtering systems to introduce precautions to discourage such kind of manipulations.\n\n===Diversity and the long tail===\nCollaborative filters are expected to increase diversity because they help us discover new products. Some algorithms, however, may unintentionally do the opposite. Because collaborative filters recommend products based on past sales or ratings, they cannot usually recommend products with limited historical data. This can create a rich-get-richer effect for popular products, akin to [[positive feedback]]. This bias toward popularity can prevent what are otherwise better consumer-product matches. A [[Wharton School of the University of Pennsylvania|Wharton]] study details this phenomenon along with several ideas that may promote diversity and the "[[long tail]]."<ref>{{cite journal| last1= Fleder | first1= Daniel | first2= Kartik |last2= Hosanagar | title=Blockbuster Culture\'s Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity|journal=Management Science |date=May 2009|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=955984 | doi = 10.1287/mnsc.1080.0974 }}</ref> Several collaborative filtering algorithms have been developed to promote diversity and the "[[long tail]]" by recommending novel, unexpected,<ref>{{cite journal| last1= Adamopoulos | first1= Panagiotis | first2= Alexander |last2= Tuzhilin | title=On Unexpectedness in Recommender Systems: Or How to Better Expect the Unexpected|journal=ACM Transactions on Intelligent Systems and Technology |date=January 2015|url=http://dl.acm.org/citation.cfm?id=2559952 | doi = 10.1145/2559952}}</ref> and serendipitous items.<ref>{{cite journal| last1= Adamopoulos | first1= Panagiotis | title=Beyond rating prediction accuracy: on new perspectives in recommender systems|journal=Proceedings of the 7th ACM conference on Recommender systems |date=October 2013|url=http://dl.acm.org/citation.cfm?id=2508073| doi = 10.1145/2507157.2508073}}</ref>\n\n==Innovations==\n{{Prose|date=May 2012}}\n* New algorithms have been developed for CF as a result of the [[Netflix prize]].\n* Cross-System Collaborative Filtering where user profiles across multiple [[recommender systems]] are combined in a privacy preserving manner.\n* [[Robust collaborative filtering]], where recommendation is stable towards efforts of manipulation. This research area is still active and not completely solved.<ref>{{cite web|url=http://dl.acm.org/citation.cfm?id=1297240 |title=Robust collaborative filtering |doi=10.1145/1297231.1297240 |publisher=Portal.acm.org |date=19 October 2007 |accessdate=2012-05-15}}</ref>\n\n==See also==\n{{div col|3}}\n* [[Attention Profiling Mark-up Language|Attention Profiling Mark-up Language (APML)]]\n* [[Cold start]]\n* [[Collaborative model]]\n* [[Collaborative search engine]]\n* [[Collective intelligence]]\n* [[Customer engagement]]\n* [[Delegative Democracy]], the same principle applied to voting rather than filtering\n* [[Enterprise bookmarking]]\n* [[Firefly (website)]], a defunct website which was based on collaborative filtering\n* [[Filter bubble]]\n* [[Preference elicitation]]\n* [[Recommendation system]]\n* [[Relevance (information retrieval)]]\n* [[Reputation system]]\n* [[Robust collaborative filtering]]\n* [[Similarity search]]\n* [[Slope One]]\n* [[Social translucence]]\n{{div col end}}\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n*[http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf \'\'Beyond Recommender Systems: Helping People Help Each Other\'\'], page 12, 2001\n*[http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf Recommender Systems.] Prem Melville and Vikas Sindhwani. In Encyclopedia of Machine Learning, Claude Sammut and Geoffrey Webb (Eds), Springer, 2010.\n*[http://arxiv.org/abs/1203.4487 Recommender Systems in industrial contexts - PHD thesis (2012) including a comprehensive overview of many collaborative recommender systems]\n*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1423975  Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions]{{dead link|date=June 2016|bot=medic}}{{cbignore|bot=medic}}. Adomavicius, G. and Tuzhilin, A. IEEE Transactions on Knowledge and Data Engineering 06.2005\n*[https://web.archive.org/web/20060527214435/http://ectrl.itc.it/home/laboratory/meeting/download/p5-l_herlocker.pdf Evaluating collaborative filtering recommender systems] ([http://www.doi.org/ DOI]: [http://dx.doi.org/10.1145/963770.963772 10.1145/963770.963772])\n*[http://www.grouplens.org/publications.html GroupLens research papers].\n*[http://www.cs.utexas.edu/users/ml/papers/cbcf-aaai-02.pdf Content-Boosted Collaborative Filtering for Improved Recommendations.] Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), pp.&nbsp;187–192, Edmonton, Canada, July 2002.\n*[http://agents.media.mit.edu/projects.html A collection of past and present "information filtering" projects (including collaborative filtering) at MIT Media Lab]\n*[http://www.ieor.berkeley.edu/~goldberg/pubs/eigentaste.pdf Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.]\n*[http://downloads.hindawi.com/journals/aai/2009/421425.pdf A Survey of Collaborative Filtering Techniques] Su, Xiaoyuan and Khoshgortaar, Taghi. M\n*[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering] Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. International World Wide Web Conference, Proceedings of the 16th international conference on World Wide Web\n*[https://web.archive.org/web/20101023032716/http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] Yehuda Koren, Transactions on Knowledge Discovery from Data (TKDD) (2009)\n*[http://webpages.uncc.edu/~asaric/ISMIS09.pdf Rating Prediction Using Collaborative Filtering]\n*[http://www.cis.upenn.edu/~ungar/CF/ Recommender Systems]\n*[http://www2.sims.berkeley.edu/resources/collab/ Berkeley Collaborative Filtering]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Collaborative Filtering}}\n[[Category:Collaboration]]\n[[Category:Collaborative software]]\n[[Category:Collective intelligence]]\n[[Category:Information retrieval techniques]]\n[[Category:Recommender systems]]\n[[Category:Social information processing]]']
['Subject indexing', '13200719', '\'\'\'Subject indexing\'\'\' is the act of describing or [[document classification|classifying]] a [[document]] by [[index term]]s or other symbols in order to indicate what the document is \'\'\'[[aboutness|about]],\'\'\' to summarize its [[content (media and publishing)|content]] or to increase its [[findability]]. In other words, it is about identifying and describing the \'\'\'[[Subject (documents)|subject]]\'\'\' of documents. Indexes are constructed, separately, on three distinct levels: terms in a document such as a book; objects in a collection such as a library; and documents (such as books and articles) within a field of knowledge.\n\nSubject indexing is used in [[information retrieval]] especially to create [[bibliographic index]]es to retrieve documents on a particular subject. Examples of academic indexing services are [[Zentralblatt MATH]], [[Chemical Abstracts]] and [[PubMed]]. The index terms were mostly assigned by experts but author keywords are also common.\n\nThe process of indexing begins with any analysis of the subject of the document. The indexer must then identify terms which appropriately identify the subject either by extracting words directly from the document or assigning words from a [[controlled vocabulary]].<ref name="Lancaster2003a">F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 6</ref> The terms in the index are then presented in a systematic order.\n\nIndexers must decide how many terms to include and how specific the terms should be. Together this gives a depth of indexing.\n\n== Subject analysis ==\nThe first step in indexing is to decide on the subject matter of the document. In manual indexing, the indexer would consider the subject matter in terms of answer to a set of questions such as "Does the document deal with a specific product, condition or phenomenon?".<ref name="Chowdhury2004">G.G. Chowdhury (2004): "Introduction to modern information retrieval". Third Edition. London, Facet. ISBN 1-85604-480-7. page 71</ref> As the analysis is influenced by the knowledge and experience of the indexer, it follows that two indexers may analyze the content differently and so come up with different index terms. This will impact on the success of retrieval.\n\n=== Automatic vs. manual subject analysis ===\nAutomatic indexing follows set processes of analyzing frequencies of word patterns and comparing results to other documents in order to assign to subject categories. This requires no understanding of the material being indexed. This therefore leads to more uniform indexing but this is at the expense of the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analyzing the full text in depth is costly and time consuming <ref name="Lancaster2003b">F. W. Lancaster (2003): "Indexing and abstracting in theory and practice". Third edition. London, Facet ISBN 1-85604-482-3. page 24</ref> An automated system takes away the time limit and allows the entire document to be analyzed, but also has the option to be directed to particular parts of the document.\n\n== Term selection ==\nThe second stage of indexing involves the translation of the subject analysis into a set of [[keyword (search)|index terms]]. This can involve extracting from the document or assigning from a [[controlled vocabulary]]. With the ability to conduct a [[full text search]] widely available, many people have come to rely on their own expertise in conducting information searches and [[full text search]] has become very popular. Subject indexing and its experts, professional indexers, [[catalogers]], and [[librarians]], remains crucial to information organization and retrieval. These experts understand [[controlled vocabularies]] and are able to find information that cannot be located by [[full text search]]. The cost of expert analysis to create subject indexing is not easily compared to the cost of hardware, software and labor to manufacture a comparable set of full-text, fully searchable materials. With new web applications that allow every user to annotate documents, [[social tagging]] has gained popularity especially in the Web.<ref name="Voss2007">{{cite conference\n|last1=Voss |first1=Jakob\n|title=Tagging, Folksonomy & Co - Renaissance of Manual Indexing?\n|booktitle=Proceedings of the International Symposium of Information Science\n|pages=234–254\n|year=2007\n|arxiv=cs/0701072\n}}</ref>\n\nOne application of indexing, the [[Index (publishing)|book index]], remains relatively unchanged despite the information revolution.\n\n=== Extraction/Derived indexing ===\nExtraction indexing involves taking words directly from the document. It uses [[natural language]] and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A stop-list containing common words (such as "the", "and") would be referred to and such [[stop words]] would be excluded as index terms.\n\nAutomated extraction indexing may lead to loss of meaning of terms by indexing single words as opposed to phrases. Although it is possible to extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases. Automated extraction indexing also has the problem that, even with use of a stop-list to remove common words, some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based on the rest of the database could then be used as an index term, and terms that occur equally frequently throughout will be excluded.\nAnother problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.<ref name="Lamb2008">J. Lamb (2008): \'\'[http://www.indexers.org.uk/index.php?id=463 Human or computer produced indexes?]\'\' [online] Sheffield, Society of Indexers. Accessed 15 January 2009.</ref>\n\n=== Assignment indexing ===\nAn alternative is assignment indexing where index terms are taken from a controlled vocabulary. This has the advantage of controlling for [[synonym]]s as the preferred term is indexed and synonyms or related terms direct the user to the preferred term. This means the user can find articles regardless of the specific term used by the author and saves the user from having to know and check all possible synonyms.<ref name="Tenopir">C. Tenopir (1999): "Human or automated, indexing is important". \'\'Library Journal\'\' \'\'\'124\'\'\'(18) pages 34-38.</ref> It also removes any confusion caused by [[homograph]]s by inclusion of a qualifying term. A third advantage is that it allows the linking of related terms whether they are linked by hierarchy or association, e.g. an index entry for an oral medication may list other oral medications as related terms on the same level of the hierarchy but would also link to broader terms such as treatment. Assignment indexing is used in manual indexing to improve inter-indexer consistency as different indexers will have a controlled set of terms to choose from. Controlled vocabularies do not completely remove inconsistencies as two indexers may still interpret the subject differently.<ref name="Chowdhury2004" />\n\n== Index presentation ==\nThe final phase of indexing is to present the entries in a systematic order. This may involve linking entries. In a pre-coordinated index the indexer determines the order in which terms are linked in an entry by considering how a user may formulate their search. In a post-coordinated index, the entries are presented singly and the user can link the entries through searches, most commonly carried out by computer software. Post-coordination results in a loss of precision in comparison to pre-coordination <ref name="Bodoff1998">D. Bodoff and A. Kambil, (1998): "Partial coordination. I. The best of pre-coordination and post-coordination." \'\'Journal of the American Society for Information Science\'\', \'\'\'49\'\'\'(14), 1254-1269.</ref>\n\n== Depth of Indexing ==\nIndexers must make decisions about what entries should be included and how many entries an index should incorporate. The depth of indexing describes the thoroughness of the indexing process with reference to exhaustivity and specificity <ref name="Cleveland2001">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 105</ref>\n\n=== Exhaustivity ===\nAn exhaustive index is one which lists all possible index terms. Greater exhaustivity gives a higher [[Recall (information retrieval)|recall]], or more likelihood of all the relevant articles being retrieved, however, this occurs at the expense of [[Precision (information retrieval)|precision]]. This means that the user may retrieve a larger number of irrelevant documents or documents which only deal with the subject in little depth. In a manual system a greater level of exhaustivity brings with it a greater cost as more man hours are required. The additional time taken in an automated system would be much less significant. At the other end of the scale, in a selective index only the most important aspects are covered.<ref name="Weinberg1999">B.H. Weinberg (1990): "Exhaustivity of indexes: Books, journals, and electronic full texts; Summary of a workshop presented at the 1999 ASI Annual Conference". \'\'Key Words\'\', \'\'\'7\'\'\'(5), pages 1+.</ref> Recall is reduced in a selective index as if an indexer does not include enough terms, a highly relevant article may be overlooked. Therefore indexers should strive for a balance and consider what the document may be used. They may also have to consider the implications of time and expense.\n\n=== Specificity ===\nThe specificity describes how closely the index terms match the topics they represent <ref name="Anderson1997">J.D. Anderson (1997): \'\'[http://www.niso.org/publications/tr/ Guidelines for indexes and related information retrieval devices]\'\' [online]. Bethesda, Maryland, Niso Press. 10 December 2008.</ref> An index is said to be specific if the indexer uses parallel descriptors to the concept of the document and reflects the concepts precisely.<ref name="Cleveland2001b">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 106</ref> Specificity tends to increase with exhaustivity as the more terms you include, the narrower those terms will be.\n\n==Indexing theory==\n[[Birger Hjørland|Hjørland]] (2011)<ref>Hjørland, Birger (2011). The Importance of Theories of Knowledge: Indexing and Information retrieval as an example. \'\'Journal of the American Society for Information Science and Technology\'\', 62(1,), 72-77.</ref> found that theories of indexing is at the deepest level connected to different theories of knowledge:\n\n\'\'\'Rationalist theories of indexing\'\'\' (such as Ranganathan\'s theory) suggest that subjects are constructed logically from a fundamental set of categories. The basic method of subject analysis is then "analytic-synthetic", to isolate a set of basic categories (=analysis) and then to construct the subject of any given document by combining those categories according to some rules (=synthesis). \'\'\'Empiricist theories of indexing\'\'\' are based on selecting similar documents based on their properties, in particular by applying numerical statistical techniques. \'\'\'Historicist and hermeneutical theories of indexing\'\'\' suggest that the subject of a given document is relative to a given discourse or domain, why the indexing should reflect the need of a particular discourse or domain. According to hermeneutics is a document always written and interpreted from particular horizon. The same is the case with systems of knowledge organization and with all users searching such systems. Any question put to such a system is put from a particular horizon. All those horizons may be more or less in consensus or in conflict. To index a document is to try to contribute to the retrieval of “relevant” documents by knowing about those different horizons. \'\'\'Pragmatic and critical theories of indexing\'\'\' (such as Hjørland, 1997)<ref>Hjørland, B. (1997). Information Seeking and Subject Representation. An Activity-theoretical approach to Information Science. Westport & London: Greenwood Press.</ref> is in agreement with the historicist point of view that subjects are relative to specific discourses but emphasizes that subject analysis should support given goals and values and should consider the consequences of indexing one way or another. These theories believe that indexing cannot be neutral and that it is a wrong goal to try to index in a neutral way. Indexing is an act (and computer based indexing is acting according to the programmers intentions). Acts serve human goals. Libraries and information services also serve human goals, why their indexing should be done in a way that supports these goals as much as possible. At a first glance this looks strange because the goals of libraries and information services is to identify any document or piece of information. Nonetheless is any specific way of indexing always supporting some kind of uses at the expense of other. The documents to be indexed intend to serve some specific purposes in a community. Basically the indexing should intend serving the same purposes. Primary and secondary documents and information services are parts of the same overall social system. In such a system different theories, epistemologies, worldviews etc. may be at play and users need to be able to orient themselves and to navigate among those different views. This calls for a mapping of the different epistemologies in the field and classification of the single document into such a map. Excellent examples of such different paradigms and their consequences for indexing and classification systems are provided in the domain of art by Ørom (2003)<ref>Ørom, Anders (2003). Knowledge Organization in the domain of Art Studies - History, Transition and Conceptual Changes. Knowledge Organization. 30(3/4), 128-143.</ref> and in music by Abrahamsen (2003).<ref>Abrahamsen, Knut T. (2003). Indexing of Musical Genres. An Epistemological Perspective. Knowledge Organization, 30(3/4), 144-169.</ref>\n\nThe core of indexing is, as stated by Rowley & Farrow<ref name=rowley2000>Rowley, J. E. & Farrow, J. (2000). Organizing Knowledge: An Introduction to Managing Access to Information. 3rd. Alderstot: Gower Publishing Company</ref> to evaluate a papers contribution to knowledge and index it accordingly. Or, with the words of Hjørland (1992,<ref>Hjørland, Birger (1992). The Concept of "Subject" in Information Science. Journal of Documentation. 48(2), 172-200. http://iva.dk/bh/Core%20Concepts%20in%20LIS/1992JDOC%5FSubject.PDF</ref> 1997) to index its informative potentials.\n\n"In order to achieve good consistent indexing, the indexer must have a thorough appreciation of the structure of the subject and the nature of the contribution that the document is making to the advancement of knowledge." (Rowley & Farrow, 2000,<ref name=rowley2000/> p.&nbsp;99).\n\n== See also ==\n{{Commons category|Subject indexing}}\n* [[Indexing and abstracting service]]\n* [[Document classification]]\n* [[Metadata]]\n* [[Overcategorization]]\n* [[Thomas of Ireland]], a medieval pioneer in subject indexing\n\n== References ==\n{{reflist}}\n* {{cite book|author=Fugman, Robert|year=1993|title=Subject analysis and indexing. Theoretical foundation and practical advice|place=Frankfurt/Main|publisher=Index Verlag}}\n* {{cite journal|author=Frohmann, B.|year=1990|title=Rules of Indexing: A Critique of [[Mentalism]] in Information Retrieval Theory|journal=Journal of Documentation|volume=46|issue=2|pages=81–101|doi=10.1108/eb026855}}\n\n[[Category:Index (publishing)]]\n[[Category:Information science]]\n[[Category:Information retrieval techniques]]']
['Voice search', '13667706', '\'\'\'Voice search\'\'\', also called voice-enabled search, allows the user to use a voice command to search the Internet, or a portable device.  Currently, voice search is commonly used in (in a narrow sense) "directory assistance", or local search. Examples include [[Google 411]], [[Tellme]] directory assistance and [[Yellowpages.com]]\'s 1-800-YellowPages. \n\nIn a broader definition, voice search include open-domain keyword query on any information on the Internet, for example in [[Google Voice Search]], [[Cortana (software)|Cortana]], [[Siri]] and [[Amazon Echo]]. Given that voice-based systems are interactive, such systems are also called open-domain [[question answering]] systems. \n\nVoice search is often interactive, involving several rounds of interaction that allows a system to ask for clarification. Voice search is a type of [[Dialog systems|dialog system]].\n\n== References ==\n{{No footnotes|date=April 2009}}\n*Ye-Yi Wang, Dong Yu, Yun-Cheng Ju, Alex Acero, An Introduction to Voice Search, IEEE Signal Processing Magazine (Special Issue on Spoken Language Technology), Institute of Electrical and Electronics Engineers, Inc., May 2008\n*J. Sherwani, Dong Yu, Tim Paek, Mary Czerwinski, Yun-Cheng Ju, and Alex Acero. \'VoicePedia: Towards Speech-Based Access to Unstructured Information\', Proceedings of the 8th Annual Conference of the International Communication Association (Interspeech 2007). Antwerp, Belgium, August, 2007\n{{Internet search}}\n\n[[Category:Information retrieval genres]]']
['Temporal information retrieval', '35804330', '\'\'\'Temporal Information Retrieval (T-IR)\'\'\' is an emerging area of research related to the field of [[information retrieval]] (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.\n\nAccording to [[information theory]] science (Metzger, 2007),<ref name="Metzger2007">{{cite journal |last=Metzger |first=Miriam |title=Making Sense of Credibility on the Web: Models for Evaluating Online Information and Recommendations for Future Research |journal=Journal of the American Society for Information Science and Technology |volume=58 |issue=13 |pages=2078–2091 |year =2007 |url=http://dl.acm.org/citation.cfm?id=1315940 |doi=10.1002/asi.20672 }}</ref> timeliness or currency is one of the key five aspects that determine a document’s credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company’s earnings or information on already-happened or invalid predictions.\n\nT-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.\n\nThis page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.\n\n== Temporal dynamics (T-dynamics) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Baeza, Y.\'\'\' (2002). [http://www.dcs.bbk.ac.uk/webDyn2/proceedings/baeza_yates_web_strucutre.pdf/ Web Structure, Dynamics and Page Quality]. In A. Laendar & A. Oliveira (Eds.), \'\'In Lecture Notes in Computer Science - SPIRE2002: 9th International Symposium on String Processing and Information Retrieval\'\' (Vol. 2476/2002, pp.&nbsp;117 – 130). Lisbon, Portugal. September 11–13: Springer Berlin / Heidelberg. || 2002 || SPIRE || T-Dynamics ||\n|-\n|\'\'\'Cho, J., & Garcia-Molina, H.\'\'\' (2003). [http://dl.acm.org/citation.cfm?id=857170 Estimating Frequency of Change]. \'\'In [http://toit.acm.org TOIT: ACM Transactions on Internet Technology]\'\', 3(3), 256 - 290.|| 2003 || TOIT || T-Dynamics ||\n|-\n| \'\'\'Fetterly, D., Manasse, M., Najork, M., & Wiener, J.\'\'\' (2003). [http://dl.acm.org/citation.cfm?id=775246|A Large-Scale Study of the Evolution of Web Pages]]. \'\'In [http://www2003.org/ WWW2003]: Proceedings of the 12th International World Wide Web Conference\'\' (pp.&nbsp;669 – 678). Budapest, Hungary. May 20–24: ACM Press. || 2003 || WWW || T-Dynamics ||\n|-\n| \'\'\'Ntoulas, A., Cho, J., & Olston, C.\'\'\' (2004). [http://dl.acm.org/citation.cfm?id=988674 What\'s New on the Web?: the Evolution of the Web from a Search Engine Perspective]. In [http://www2004.org WWW2004]: Proceedings of the 13th International World Wide Web Conference (pp.&nbsp;1 – 12). New York, NY, United States. May 17–22: ACM Press. || 2004 || WWW || T-Dynamics ||\n|-\n| \'\'\'Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.\'\'\' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 – 142). Paris, France. June 13–18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||\n|-\n| \'\'\'Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]. \'\'In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology\'\', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||\n|-\n| \'\'\'Jones, R., & Diaz, F.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. \'\'In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]\'\', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\n|-\n| \'\'\'Bordino, I., Boldi, P., Donato, D., Santini, M., & Vigna, S.\'\'\' (2008). [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4734022&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4733896%2F4733897%2F04734022.pdf%3Farnumber%3D4734022 Temporal Evolution of the UK Web]. In [http://compbio.cs.uic.edu/adn-icdm08/ ADN2008]: Proceedings of the 1st International Workshop on Analysis of Dynamic Networks associated to [http://icdm08.isti.cnr.it/ ICDM2008]: IEEE International Conference on Data Mining (pp.&nbsp;909 – 918). Pisa, Italy. December 19: IEEE Computer Society Press. || 2008 || ICDM - ADN || T-Dynamics ||\n|-\n| \'\'\'Adar, E., Teevan, J., Dumais, S. T., & Elsas, J. L.\'\'\' (2009). [http://portal.acm.org/citation.cfm?id=1498837 The Web Changes Everything: Understanding the Dynamics of Web Content]. \'\'In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;282 – 291). Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Metzler, D., Jones, R., Peng, F., & Zhang, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. \'\'In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Elsas, J. L., & Dumais, S. T.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. \'\'In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;1 – 10). New York, United States. February 3–06: ACM Press. || 2010 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. \'\'In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference\'\' (pp.&nbsp;1123 – 1124). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || F-IRetrieval ||\n|-\n| \'\'\'Aji, A., Agichtein, E.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2175298.2175332 Deconstructing Interaction Dynamics in Knowledge Sharing Communities]. \'\'In [http://sbp.asu.edu/sbp2010/sbp10.html]: Third International Conference on Social Computing, Behavioral-Cultural Modeling, & Prediction\'\' (pp.&nbsp;273 – 281). Washington DC, United States. March 30–31: Springer-Verlag. || 2010 || SBP || T-Dynamics ||\n|-\n| \'\'\'Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.\'\'\' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]. \'\'In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;167 – 176). Hong Kong, China. February 9–12: ACM Press. || 2011 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Campos, R., Dias, G., & Jorge, A. M.\'\'\' (2011). [http://ceur-ws.org/Vol-707/TWAW2011.pdf What is the Temporal Value of Web Snippets?] \'\'In [http://temporalweb.net/page3/page3.html TWAW 2011]: Proceedings of the 1st International Temporal Web Analytics Workshop associated to [http://www.www2011india.com/ WWW2011]: 20th International World Wide Web Conference\'\'. Hyderabad, India. March 28.: CEUR Workshop Proceedings. || 2011 || WWW - TWAW || T-Dynamics ||\n|-\n| \'\'\'Campos, R., Jorge, A., & Dias, G.\'\'\' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. \'\'In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2011 Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;13 – 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||\n|-\n| \'\'\'Shokouhi, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. In [http://www.sigir2011.org/ SIGIR2011]: \'\'In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information\'\' (pp.&nbsp;1171 – 1172). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || T-Dynamics ||\n|-\n| \'\'\'Dias, G., Campos, R., & Jorge, A.\'\'\' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] \'\'In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\'. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||\n|-\n| \'\'\'Campos, R., Dias, G., & Jorge, A. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. \'\'In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence\'\' (Vol. 7026/2011, pp.&nbsp;581 – 596). Lisboa, Portugal. October 10–13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||\n|-\n| \'\'\'Jatowt, A., & Yeung, C. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1259 – 1264). Glasgow, Scotland, UK. October 24 - 28: ACM Press. || 2011 || CIKM || F-IRetrieval ||\n|-\n| \'\'\'Yeung, C.-m. A., & Jatowt, A.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1231 – 1240). Glasgow, Scotland, UK. October 24–28: ACM Press. || 2011 || CIKM || C-Memory ||\n|-\n| \'\'\'Costa, M., & Silva, M. J., & Couto, F. M.\'\'\' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. \'\'In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference\'\' (pp.&nbsp;757--766). Gold Coast, Australia. July 6–11: ACM Press. || 2014 || SIGIR || T-RModels ||\n|}\n\n== Temporal markup languages (T-MLanguages) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Setzer, A., & Gaizauskas, R.\'\'\' (2000). [ftp://ftp.dcs.shef.ac.uk/home/robertg/papers/lrec00-tempann.pdf Annotating Events and Temporal Information in Newswire Texts]. \'\'In [http://www.xanthi.ilsp.gr/lrec/ LREC2000]: Proceedings of the 2nd International Conference on Language Resources and Evaluation\'\'. Athens, Greece. May 31 - June 2: ELDA. || 2000 || LREC || T-MLanguages ||\n|-\n| \'\'\'Setzer, A.\'\'\' (2001). [http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study]. Sheffield, UK: University of Sheffield. || 2001 || Phd Thesis || T-MLanguages ||\n|-\n| \'\'\'Ferro, L., Mani, I., Sundheim, B., & Wilson, G.\'\'\' (2001). [http://www.timeml.org/site/terqas/readings/MTRAnnotationGuide_v1_02.pdf TIDES Temporal Annotation Guidelines]. Version 1.0.2. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2001 || Technical Report || T-MLanguages ||\n|-\n| \'\'\'Pustejovsky, J., Castaño, J., Ingria, R., Sauri, R., Gaizauskas, R., Setzer, A., et al.\'\'\' (2003). TimeML: Robust Specification of Event and Temporal Expression in Text. \'\'In [http://iwcs.uvt.nl/iwcs5/index.htm IWCS2003]: Proceedings of the 5th International Workshop on Computational Semantics\'\', (pp.&nbsp;28 – 34). Tilburg, Netherlands. January 15–17. || 2003 || IWCS || T-MLanguages ||\n|-\n| \'\'\'Ferro, L., Gerber, L., Mani, I., Sundheim, B., & Wilson, G.\'\'\' (2005). [http://projects.ldc.upenn.edu/ace/docs/English-TIMEX2-Guidelines_v0.1.pdf TIDES 2005 Standard for the Annotation of Temporal Expressions]. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2005 || Technical Report || T-MLanguages ||\n|}\n\n== Temporal taggers (T-taggers) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| [http://www.timeml.org/site/tarsqi/toolkit/manual/ TempEx Module] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] - \'\'\'Mani, I., & Wilson, G.\'\'\' (2000). [[dl.acm.org/citation.cfm?id=1075228|Robust Temporal Processing of News]]. \'\'In [http://www.cse.ust.hk/acl2000/ ACL2000]: Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics\'\' (pp.&nbsp;69 – 76). Hong Kong, China. October 1–8: Association for Computational Linguistics. || 2000 || ACL || T-Taggers ||\n|-\n| [http://www.aktors.org/technologies/annie/ Annie] - [http://gate.ac.uk/download/index.html GATE distribution] - \'\'\'Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V.\'\'\' (2002). [http://eprints.aktors.org/90/01/acl-main.pdf GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications]. \'\'In [http://www.aclweb.org/mirror/acl2002/ ACL2002]: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics\'\' (pp.&nbsp;168 – 175). Philadelphia, PA, United States. July 6–12: Association for Computational Linguistics. || 2002 || ACL || T-Taggers ||\n|-\n| [http://www.timeml.org/site/tarsqi/modules/gutime/download.html GUTime] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] || 2002 ||  || T-Taggers ||\n|-\n| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - \'\'\'Strötgen, J., & Gertz, M.\'\'\' (2010). [http://delivery.acm.org/10.1145/1860000/1859735/p321-strotgen.pdf?ip=188.80.124.88&acc=OPEN&CFID=82473711&CFTOKEN=13661527&__acm__=1337002719_1b05141ffc83e798f400c972756d43ad HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions]. \'\'In [http://semeval2.fbk.eu/semeval2.php SemEval2010]: Proceedings of the 5th International Workshop on Semantic Evaluation associated to [http://acl2010.org/ ACL2010]: 41st Annual Meeting of the Association for Computational Linguistics\'\', (pp.&nbsp;321 – 324). Uppsala, Sweden. July 11–16.|| 2010 || ACL - SemEval || T-Taggers ||\n|-\n| [http://www.timen.org/ TIMEN] \'\'\'Llorens, H., Derczynski, L., Gaizauskas, R. & Saquete, E.\'\'\' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/128_Paper.pdf TIMEN: An Open Temporal Expression Normalisation Resource]. \'\'In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation\'\'. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||\n|-\n| \'\'\'Chang, A., & Manning, C.\'\'\' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf SUTIME: A Library for Recognizing and Normalizing Time Expressions]. \'\'In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation\'\'. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||\n|-\n| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - \'\'\'Strötgen, J., & Gertz, M.\'\'\' (2012). [http://www.springerlink.com/content/64767752451075k8/ Multilingual and cross-domain temporal tagging]. \'\'In [http://www.springerlink.com/content/1574-020x/ LRE]: Language Resources and Evaluation\'\', 1 - 30.|| 2012 || LRE || T-Taggers ||\n|-\n| [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ ManTIME] - \'\'\'Filannino, M., Brown, G. & Nenadic G.\'\'\' (2013). [http://www.aclweb.org/anthology/S/S13/S13-2.pdf#page=89 ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge]. \'\'In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013)\'\', 53 - 57, Atlanta, Georgia, June 14-15, 2013.|| 2013 || ACL - SemEval || T-Taggers || [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ online demo]\n|}\n\n== Temporal indexing (T-indexing) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Alonso, O., & Gertz, M.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. \'\'In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||\n|-\n| \'\'\'Berberich, K., Bedathur, S., Neumann, T., & Weikum, G.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1277831 A Time Machine for Text Search]. \'\'In [http://www.sigir.org/sigir2007 SIGIR 2007]: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;519 – 526). Amsterdam, Netherlands. July 23–27: ACM Press. || 2007 || SIGIR || W-Archives ||\n|-\n| \'\'\'Jin, P., Lian, J., Zhao, X., & Wan, S.\'\'\' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. \'\'In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application\'\' (pp.&nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\n|-\n| \'\'\'Song, S., & JaJa, J.\'\'\' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||\n|-\n| \'\'\'Pasca, M.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. \'\'In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing\'\' (pp.&nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. \'\'In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management\'\'. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||\n|-\n| \'\'\'Arikan, I., Bedathur, S., & Berberich, K.\'\'\' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. \'\'In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining\'\'. Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM ||| T-RModels ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|-\n| \'\'\'Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1871437.1871528 Efficient temporal keyword search over versioned text]. \'\'In [http://www.yorku.ca/cikm10/ CIKM2010]: Proceedings of the 19th ACM international conference on Information and knowledge management\'\', (pp.&nbsp;699-708). Toronto, Canada. October 26-30. ACM Press. || 2010 || CIKM || W-Archives||\n|-\n| \'\'\'Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2009991 Temporal index sharding for space-time efficiency in archive search]. \'\'In [http://www.sigir.org/sigir2011/ SIGIR2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;545-554). Beijing, China. July 24-28. ACM Press. || 2011 || SIGIR || T-Indexing||\n|-\n| \'\'\'Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. \'\'In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;235 – 243). Portland, United States. August 12-16. ACM Press. || 2012 || SIGIR || W-Archives ||\n|}\n\n== Temporal query understanding (TQ-understanding) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.\'\'\' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 – 142). Paris, France. June 13–18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||\n|-\n| \'\'\'Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]]. \'\'In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology\'\', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||\n|-\n| \'\'\'Jones, R., & Diaz, F.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]]. \'\'In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]\'\', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\n|-\n| \'\'\'Dakka, W., Gravano, L., & Ipeirotis, P. G.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]]. \'\'In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1437 – 1438). Napa Valley, California, United States. October 26–30: ACM Press. || 2008 || CIKM || TQ-Understanding ||\n|-\n| \'\'\'Diaz, F.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1498825 Integration of News Content into Web Results]. \'\'In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;182 – 191). Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM || TQ-Understanding ||\n|-\n| \'\'\'Metzler, D., Jones, R., Peng, F., & Zhang, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. \'\'In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'König, A.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1572002 Click-Through Prediction for News Queries]. \'\'In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;347 – 354). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: \'\'In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;11 – 20). New York, United States. February 3–6: ACM Press. || 2010 || WSDM || T-RModels ||\n|-\n| \'\'\'Kanhabua, N., & Nørvåg, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. \'\'In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries\'\'. Glasgow, Scotland. September 6–10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||\n|-\n| \'\'\'Zhang, R., Konda, Y., Dong, A., Kolari, P., Chang, Y., & Zheng, Z.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1870768 Learning Recurrent Event Queries for Web Search]. \'\'In [http://www.lsi.upc.edu/events/emnlp2010/ EMNLP2010]: Proceedings of the Conference on Empiral Methods in Natural Language Processing\'\' (pp.&nbsp;1129 – 1139). Massachusetts, United States. October 9–11: Association for Computational Linguistics. || 2010 || EMNLP || TQ-Understanding ||\n|-\n| \'\'\'Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.\'\'\' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]]. \'\'In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;167 – 176). Hong Kong, China. February 9–12: ACM Press. || 2011 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Campos, R.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010182 Using k-top Retrieved Web Snippets to Date Temporal Implicit Queries based on Web Content Analysis]. \'\'In [http://www.sigir2011.org/%20 SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (p.&nbsp;1325). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Campos, R., Jorge, A., & Dias, G.\'\'\' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. \'\'In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2005 Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;13 – 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||\n|-\n| \'\'\'Shokouhi, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;1171 – 1172). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Campos, R., Dias, G., Jorge, A., & Nunes, C.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2169103&CFID=102654836&CFTOKEN=48651941 Enriching Temporal Query Understanding through Date Identification: How to Tag Implicit Temporal Queries?] \'\'In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference\'\' (pp.&nbsp;41 – 48). Lyon, France. April 17.: ACM - DL. || 2012 || WWW - TWAW || TQ-Understanding ||\n|-\n| \'\'\'Shokouhi, M., & Radinsky, K.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348364 Time-Sensitive Query Auto-Completion]. \'\'In [http://www.sigir.org/sigir2012/ SIGIR 2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;601 – 610). Portland, United States. August 12–16.: ACM Press. || 2012 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Campos, R., Dias, G., Jorge, A., & Nunes, C.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2398567&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 GTE: A Distributional Second-Order Co-Occurrence Approach to Improve the Identification of Top Relevant Dates] \'\'In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;2035 – 2039). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || TQ-Understanding ||\n|-\n| \'\'\'Campos, R., Jorge, A., Dias, G., & Nunes, C.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] \'\'In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,\'\' Vol. 1, (pp.&nbsp;1 – 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||\n|}\n\n== Time-aware retrieval/ranking models (T-RModels) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Li, X., & Croft, B. W.\'\'\' (2003). [http://dl.acm.org/citation.cfm?doid=956863.956951 Time-Based Language Models]. \'\'In CIKM 2003: Proceedings of the 12th International ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;469 – 475). New Orleans, Louisiana, United States. November 2–8: ACM Press. || 2003 || CIKM || T-RModels ||\n|-\n| \'\'\'Sato, N., Uehara, M., & Sakai, Y.\'\'\' (2003). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=1232026&contentType=Conference+Publications Temporal Information Retrieval in Cooperative Search Engine]. \'\'In [http://www.dexa.org/previous/dexa2003/cfp/dexa.html DEXA2003]: Proceedings of the 14th International Workshop on Database and Expert Systems Applications\'\' (pp.&nbsp;215 – 220). Prague, Czech Republic. September 1–5: IEEE. || 2003 || DEXA || T-RModels ||\n|-\n| \'\'\'Berberich, K., Vazirgiannis, M., & Weikum, G.\'\'\' (2005). [http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.im/1150474885&page=record Time-Aware Authority Ranking]. \'\'In [http://www.tandf.co.uk/journals/journal.asp?issn=1542-7951&linktype=44 IM: Internet Mathematics]\'\', 2(3), 301 - 332. || 2005 || IM || T-RModels ||\n|-\n| \'\'\'Cho, J., Roy, S., & Adams, R.\'\'\' (2005). [http://dl.acm.org/citation.cfm?id=1066220 Page Quality: In Search of an Unbiased Web Ranking]. In [http://cimic.rutgers.edu/~sigmod05/ SIGMOD2005]: Proceedings of the International Conference on Management of Data (pp.&nbsp;551 – 562). Baltimore, United States. June 13–16: ACM Press. || 2005 || SIGMOD || T-RModels ||\n|-\n| \'\'\'Perkiö, J., Buntine, W., & Tirri, H.\'\'\' (2005). [http://dl.acm.org/citation.cfm?id=1076171 A Temporally Adaptative Content-Based Relevance Ranking Algorithm]. \'\'In [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;647 – 648). Salvador, Brazil. August 15–16: ACM Press. || 2005 || SIGIR || T-RModels ||\n|-\n| \'\'\'Jones, R., & Diaz, F.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. \'\'In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]\'\', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||\n|-\n| \'\'\'Pasca, M.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. \'\'In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing\'\' (pp.&nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||\n|-\n| \'\'\'Dakka, W., Gravano, L., & Ipeirotis, P. G.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]. \'\'In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1437 – 1438). Napa Valley, California, United States. October 26–30: ACM Press. || 2008 || CIKM || TQ-Understanding ||\n|-\n| \'\'\'Jin, P., Lian, J., Zhao, X., & Wan, S.\'\'\' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. \'\'In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application\'\' (pp.&nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\n|-\n| \'\'\'Arikan, I., Bedathur, S., & Berberich, K.\'\'\' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. \'\'In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining\'\'. Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM ||| T-RModels ||\n|-\n| \'\'\'Zhang, R., Chang, Y., Zheng, Z., Metzler, D., & Nie, J.-y.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1620899 Search Result Re-ranking by Feedback Control Adjustment for Time-sensitive Query]. \'\'In [http://www.naaclhlt2009.org/ NAACL2009]: Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies\'\', (pp.&nbsp;165 – 168). Boulder, Colorado, United States. May 31 - June 5. || 2009 || NAACL || T-RModels ||\n|-\n| \'\'\'Metzler, D., Jones, R., Peng, F., & Zhang, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. \'\'In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. \'\'In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management\'\'. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Elsas, J. L., & Dumais, S. T.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. \'\'In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;1 – 10). New York, United States. February 3–06: ACM Press. || 2010 || WSDM || T-Dynamics ||\n|-\n| \'\'\'Aji, A., Wang, Y., Agichtein, E., Gabrilovich, E.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1871519 Using the Past to Score the Present: Extending Term Weighting Models Through Revision History Analysis] \'\'In [http://www.cikm2010.org/ CIKM 2010]: Proceedings of the 19th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;629 – 638). Toronto, ON, Canada. October 26 - October 30: ACM Press. || 2010 || CIKM || T-RModels ||\n|-\n| \'\'\'Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: \'\'In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;11 – 20). New York, United States. February 3–6: ACM Press. || 2010 || WSDM || T-RModels ||\n|-\n| \'\'\'Berberich, K., Bedathur, S., Alonso, O., & Weikum, G.\'\'\' (2010). [http://www.springerlink.com/content/b193008160713350/ A Language Modeling Approach for Temporal Information Needs]. In C. Gurrin, Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke, et al. (Eds.), \'\'In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://kmi.open.ac.uk/events/ecir2010/ ECIR 2010]: 32nd European Conference on Information Retrieval\'\' (Vol. 5993/2010, pp.&nbsp;13 – 25). Milton Keynes, UK. March 28–31: Springer Berlin / Heidelberg. || 2010 || ECIR || T-RModels ||\n|-\n| \'\'\'Dong, A., Zhang, R., Kolari, P., Jing, B., Diaz, F., Chang, Y., Zheng, Z., & Zha, H.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1772725&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 Time is of the Essence: Improving Recency Ranking Using Twitter Data]. \'\'In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference\'\' (pp.&nbsp;331 – 340). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || T-RModels ||\n|-\n| \'\'\'Inagaki, Y., Sadagopan, N., Dupret, G., Dong, A., Liao, C., Chang, Y., & Zheng, Z.\'\'\' (2010). [http://labs.yahoo.com/files/aaai10_recencyfeature_2.pdf Session Based Click Features for Recency Ranking]. \'\'In [http://www.aaai.org/Conferences/AAAI/aaai10.php AAAI2010]: Proceedings of the 24th AAAI Conference on Artificial Intelligence\'\' (pp.&nbsp;331 – 340). Atlanta, United States. June 11–15: AAAI Press. || 2010 || AAAI || T-RModels ||\n|-\n| \'\'\'Dai, N., & Davison, B.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1835471 Freshness Matters: In Flowers, Food, and Web Authority]. \'\'In [http://www.sigir2010.org/doku.php SIGIR 2010]: Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;114 – 121). Geneve, Switzerland. July 19–23: ACM Press. || 2010 || SIGIR || T-RModels ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|-\n| \'\'\'Kanhabua, N., & Nørvåg, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. \'\'In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries\'\'. Glasgow, Scotland. September 6–10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||\n|-\n| \'\'\'Efron, M., & Golovchinsky, G.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009984 Estimation Methods for Ranking Recent Information]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;495 – 504). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || T-RModels ||\n|-\n| \'\'\'Dai, N., Shokouhi, M., & Davison, B. D.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009933 Learning to Rank for Freshness and Relevance]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;95 – 104). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || T-RModels ||\n|-\n| \'\'\'Kanhabua, N., Blanco, R., & Matthews, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=102654836&CFTOKEN=48651941 Ranking Related News Predictions]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;755 – 764). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||\n|-\n| \'\'\'Chang, P-T., Huang, Y-C., Yang, C-L., Lin, S-D., & Cheng, P-J.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348489 Learning-Based Time-Sensitive Re-Ranking for Web Search]. \'\'In Proceedings of the [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;1101 – 1102). Portland, United States. August 12 - 16. || 2012 || SIGIR || T-RModels ||\n|-\n| \'\'\'Efron, M.\'\'\' (2012). [http://research.microsoft.com/en-us/people/milads/efrontemporalwsv02.pdf Query-Specific Recency Ranking: Survival Analysis for Improved Microblog Retrieval]. \'\'In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval\'\'. Portland, United States. August 16. || 2012 || SIGIR - TAIA || T-RModels ||\n|-\n| \'\'\'Kanhabua, N., & Nørvåg, K.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2398667 Learning to Rank Search Results for Time-Sensitive Queries] \'\'In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;2463 – 2466). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || T-RModels ||\n|-\n| \'\'\'Kim G., and Xing E. P.\'\'\' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. \'\'In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;163 – 172). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || T-IRetrieval ||\n|-\n| \'\'\'Costa, M., & Silva, M. J., & Couto, F. M.\'\'\' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. \'\'In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference\'\' (pp.&nbsp;757--766). Gold Coast, Australia. July 6–11: ACM Press. || 2014 || SIGIR || T-RModels ||\n|}\n\n== Temporal clustering (T-clustering) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.\'\'\' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. \'\'In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]\'\' (pp.&nbsp;165 – 174). Houston, United States. November 27–30: IEEE Press. || 2005 || ICDM - TDM || TDT ||\n|-\n| \'\'\'Alonso, O., & Gertz, M.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. \'\'In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||\n|-\n| \'\'\'Mori, M., Miura, T., & Shioya, I.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. \'\'In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence\'\' (pp.&nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||\n|-\n| \'\'\'Alonso, O., Baeza-Yates, R., & Gertz, M.\'\'\' (2007). Exploratory Search Using Timelines. \'\'In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems\'\'. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\n|-\n| \'\'\'Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. \'\'In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries\'\' (pp.&nbsp;115 – 124). Austin, United States. June 15–19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||\n|-\n| \'\'\'Campos, R., Dias, G., & Jorge, A.\'\'\' (2009). [http://www.ccc.ipt.pt/~ricardo/ficheiros/KDIR2009.pdf Disambiguating Web Search Results By Topic and Temporal Clustering: A Proposal]. In [http://www.kdir.ic3k.org/ KDIR2009]: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, (pp.&nbsp;292 – 296). Funchal - Madeira, Portugal. October 6–8. || 2009 || KDIR || T-Clustering ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. \'\'In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management\'\'. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Jatowt, A., & Yeung, C. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\n|-\n| \'\'\'Campos, R., Jorge, A., Dias, G., & Nunes, C.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] \'\'In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,\'\' Vol. 1, (pp.&nbsp;1 – 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||\n|}\n\n== Temporal text classification (T-classification) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Jong, F., Rode, H., & Hiemstra, D.\'\'\' (2006). [http://doc.utwente.nl/66448/ Temporal Language Models for the Disclosure of Historical Text]. \'\'In [http://www.dans.knaw.nl/en AHC2005]: Proceedings of the XVIth International Conference of the Association for History and Computing\'\' (pp.&nbsp;161 – 168). Amsterdam, Netherlands. September 14–17 || 2005 || AHC || T-Classification ||\n|-\n| \'\'\'Toyoda, M., & Kitsuregawa, M.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1135777.1135815 What\'s Really New on the Web? Identifying New Pages from a Series of Unstable Web Snapshots]. \'\'In [http://www2006.org WWW2006]: Proceedings of the 15th International World Wide Web Conference\'\' (pp.&nbsp;233 – 241). Edinburgh, Scotland. May 23–26: ACM Press. || 2006 || WWW || T-Classification ||\n|-\n| \'\'\'Nunes, S., Ribeiro, C., & David, G.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1316924 Using Neighbors to Date Web Documents]. \'\'In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 9th ACM International Workshop on Web Information and Data Management associated to [[www2.fc.ul.pt/cikm2007|CIKM2007]]: 16th International Conference on Knowledge and Information Management\'\' (pp.&nbsp;129 – 136). Lisboa, Portugal. November 9: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||\n|-\n| \'\'\'Jatowt, A., Kawai, Y., & Tanaka, K.\'\'\' (2007). [http://dl.acm.org/citation.cfm?id=1316925 Detecting Age of Page Content]. \'\'In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 8th International Workshop on Web Information and Data Management associated to [http://www2.fc.ul.pt/cikm2007 CIKM2007]: 16th International Conference on Knowledge and Information Management\'\' (pp.&nbsp;137 – 144). Lisbon. Portugal. November 9.: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||\n|-\n| \'\'\'Kanhabua, N., & Nørvåg, K.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1429902 Improving Temporal Language Models for Determining Time of Non-timestamped Documents]. \'\'In Christensen-Dalsgaard, B., Castelli, D., Jurik, B. A., Lippincott, J. (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2008.org/ ECDL 2008]: 12th European Conference on Research and Advances Technology for Digital Libraries\'\' (Vol. 5173/2008, pp.&nbsp;358 – 370). Aarhus, Denmark. September 14–19: Springer Berlin / Heidelberg. || 2008 || ECDL || T-Classification ||\n|-\n| \'\'\'Jatowt, A., & Yeung, C. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\n|-\n| \'\'\'Strötgen, J., Alonso, O., & Gertz, M.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2169095.2169102&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Identification of Top Relevant Temporal Expressions in Documents]. \'\'In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference\'\' (pp.&nbsp;33 – 40). Lyon, France. April 17: ACM - DL. || 2012 || WWW - TWAW || T-Classification ||\n|-\n| \'\'\'Filannino, M., and Nenadic, G.\'\'\' (2014). [http://www.aclweb.org/anthology/W/W14/W14-4502.pdf Mining temporal footprints from Wikipedia]. \'\'In Proceedings of the First AHA!-Workshop on Information Discovery in Text\'\' (Dublin, Ireland, August 2014), Association for Computational Linguistics and Dublin City University, pp. 7–13. || 2014 || COLING || T-Classification || [http://www.cs.man.ac.uk/~filannim/projects/temporal_footprints/ online demo]\n|}\n\n== Temporal visualization (T-interfaces) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Swan, R., & Allan, J.\'\'\' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. \'\'In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;49 – 56). Athens, Greece. July 24–28: ACM Press. || 2000 || SIGIR || TDT ||\n|-\n| \'\'\'Swan, R., & Jensen, D.\'\'\' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. \'\'In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\'\' (pp.&nbsp;73 – 80). Boston, Massachusetts, United States. August 20–23: ACM Press. || 2000 || KDD - TM || TDT ||\n|-\n| [https://books.google.com/ngrams Google Ngram Viewer] ||  ||  || T-Interfaces ||\n|-\n| \'\'\'Cousins, S., & Kahn, M.\'\'\' (1991). [http://www.sciencedirect.com/science/article/pii/093336579190005V The Visual Display of Temporal Information]. (E. Keravnou, Ed.) \'\'In AIM: Artificial Intelligence in Medicine\'\', 3(6), 341 - 357. || 1991 || AIM || T-Interfaces ||\n|-\n| \'\'\'Karam, G. M.\'\'\' (1994). [http://dl.acm.org/citation.cfm?id=187157 Visualization Using Timelines]. In T. J. Ostrand (Ed.), \'\'ISSTA1994: Proceedings of the International Symposium on Software Testing and Analysis associated to SIGSOFT: ACM Special Interest Group on Software Engineering\'\' (pp.&nbsp;125 – 137). Seattle, Washington, United States. August 17–19: ACM Press. || 1994 || ISSTA || T-Interfaces ||\n|-\n| \'\'\'Plaisant, C., Miiash, B., Rose, A., Widoff, S., & Shneiderman, B.\'\'\' (1996). [http://dl.acm.org/citation.cfm?id=238493 LifeLines: Visualizing Personal Histories]. \'\'In [http://www.sigchi.org/chi96/proceedings/index.htm CHI1996]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems\'\' (pp.&nbsp;221 – 227). Vancouver, British Columbia, Canada. April 13–18: ACM Press. || 1996 || CHI || T-Interfaces ||\n|-\n| \'\'\'Toyoda, M., & Kitsuregawa, M.\'\'\' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. \'\'In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia\'\' (pp.&nbsp;151 – 160). Salzburg, Austria. September 6–9: ACM Press. || 2005 || HT || W-Archives ||\n|-\n| \'\'\'Efendioglu, D., Faschetti, C., & Parr, T.\'\'\' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]. In \'\'D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering\'\' (pp.&nbsp;119 – 120). Palo Alto, California, United States. July 11–14: ACM Press. || 2006 || ICWE || W-Archives ||\n|-\n| \'\'\'Catizone, R., Dalli, A., & Wilks, Y.\'\'\' (2006). [http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf Evaluating Automatically Generated Timelines from the Web]. \'\'In [http://www.lrec-conf.org/lrec2006/ LREC2006]: Proceedings of the 5th International Conference on Language Resources and Evaluation\'\'. Genoa, Italy. May 24–26: ELDA. || 2006 || LREC || T-Interfaces ||\n|-\n| \'\'\'Mori, M., Miura, T., & Shioya, I.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. \'\'In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence\'\' (pp.&nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||\n|-\n| \'\'\'Alonso, O., Baeza-Yates, R., & Gertz, M.\'\'\' (2007). Exploratory Search Using Timelines. \'\'In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems\'\'. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\n|-\n| \'\'\'Jatowt, A., Kawai, Y., & Tanaka, K.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1367497.1367736 Visualizing Historical Content of Web pages]]. \'\'In [http://www2008.org/ WWW2008]: Proceedings of the 17th International World Wide Web Conference\'\' (pp.&nbsp;1221 – 1222). Beijing, China. April 21–25: ACM Press. || 2008 || WWW || W-Archives ||\n|-\n| \'\'\'Nunes, S., Ribeiro, C., & David, G.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1822292 WikiChanges - Exposing Wikipedia Revision Activity]. \'\'In [http://www.wikisym.org/ws2008/ WikiSym2008]: Proceedings of the 4th International Symposium on Wikis\'\'. Porto, Portugal. September 8–10: ACM Press. || 2008 || WikiSym || T-Interfaces ||\n|-\n| \'\'\'Nunes, S., Ribeiro, C., & David, G.\'\'\' (2009). [http://epia2009.web.ua.pt/onlineEdition/601.pdf Improving Web User Experience with Document Activity Sparklines]. \'\'In L. S. Lopes, N. Lau, P. Mariano, & L. Rocha (Ed.), [http://epia2009.web.ua.pt EPIA2009]: Proceedings of the 14th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence\'\', (pp.&nbsp;601 – 604). Aveiro, Portugal. October 12–15. || 2009 || EPIA || T-Interfaces ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|-\n| \'\'\'Khurana, U., Nguyen V., Cheng H., Ahn, J., Chen X., & Shneiderman, B.\'\'\' (2011). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6113166 Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines]. \'\'In [http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6112285]: Proceedings of the IEEE Social Computing\'\', (pp.&nbsp;549 – 554). Boston, United States. || 2011 || SocialCom || T-Interfaces ||\n|}\n\n== Temporal search engines (T-SEngine) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Alonso, O., & Gertz, M.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. \'\'In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||\n|-\n| \'\'\'Alonso, O., Baeza-Yates, R., & Gertz, M.\'\'\' (2007). Exploratory Search Using Timelines. \'\'In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems\'\'. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||\n|-\n| \'\'\'Jin, P., Lian, J., Zhao, X., & Wan, S.\'\'\' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. \'\'In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application\'\' (pp.&nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. \'\'In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management\'\'. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|}\n\n== Temporal question answering (T-QAnswering) ==\n{| class="wikitable sortable"\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Pasca, M.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. \'\'In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing\'\' (pp.&nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||\n|}\n\n== Temporal snippets (T-snippets) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Alonso, O., Baeza-Yates, R., & Gertz, M.\'\'\' (2009). [http://www.wssp.info/2009/WSSP2009AlonsoBaezaYatesGertz.pdf Effectiveness of Temporal Snippets]. \'\'In [http://www.wssp.info/2009.html WSSP2009]: Proceedings of the Workshop on Web Search Result Summarization and Presentation associated to [[www2009.org/|WWW2009]]: 18th International World Wide Web Conference\'\'. Madrid, Spain. April 20–24: ACM Press. || 2009 || WWW - WSSP || T-Snippets ||\n|-\n| \'\'\'Alonso, O., Gertz, M., & Baeza-Yates, R.\'\'\' (2011). [http://www.springerlink.com/content/u78qu8x10h613471/ Enhancing Document Snippets Using Temporal Information]. \'\'In R. Grossi, F. Sebastiani, & F. Silvestri (Eds.), Lecture Notes in Computer Science, [http://spire2011.isti.cnr.it/ SPIRE2011]: 18th International Symposium on String Processing and Information Retrieval\'\' (Vol. 7024, pp.&nbsp;26 – 31). Pisa, Italy. October 17–21.: Springer Berlin / Heidelberg. || 2011 || SPIRE || T-Snippets ||\n|-\n| \'\'\'Svore, K. M., Teevan, J., Dumais, S. T., & Kulkarni, A.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348461 Creating Temporally Dynamic Web Search Snippets]. \'\'In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;1045 – 1046). Portland, United States. August 12-16. ACM Press  || 2012 || SIGIR || T-Snippets ||\n|}\n\n== Future information retrieval (F-IRetrieval) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Baeza-Yates, R.\'\'\' (2005). [http://www.dcs.vein.hu/CIR/cikkek/searching_the_future.pdf Searching the Future]. \'\'In S. Dominich, I. Ounis, & J.-Y. Nie (Ed.), MFIR2005: Proceedings of the Mathematical/Formal Methods in Information Retrieval Workshop associated to [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\'. Salvador, Brazil. August 15–19: ACM Press. || 2005 || SIGIR - MFIR || F-IRetrieval ||\n|-\n| \'\'\'Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.\'\'\' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. \'\'In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries\'\' (pp.&nbsp;115 – 124). Austin, United States. June 15–19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||\n|-\n| \'\'\'Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. \'\'In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication\'\' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||\n|-\n| \'\'\'Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. \'\'In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference\'\' (pp.&nbsp;1123 – 1124). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || F-IRetrieval ||\n|-\n| \'\'\'Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.\'\'\' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. \'\'In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval\'\', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||\n|-\n| \'\'\'Dias, G., Campos, R., & Jorge, A.\'\'\' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] \'\'In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\'. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||\n|-\n| \'\'\'Kanhabua, N., Blanco, R., & Matthews, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=82290723&CFTOKEN=53881602 Ranking Related News Predictions]. \'\'In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;755 – 764). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||\n|-\n| \'\'\'Kanazawa, K., Jatowt, A., & Tanaka, K.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2052362 Improving Retrieval of Future-Related Information in Text Collections]. \'\'In [http://liris.cnrs.fr/~wi-iat11/WI 2011/ WIC2011]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence\'\' (pp.&nbsp;278 – 283). Lyon, France. August 22–27: IEEE Computer Society Press. || 2011 || WIC || F-IRetrieval ||\n|-\n| \'\'\'Campos, R., Dias, G., & Jorge, A. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. \'\'In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence\'\' (Vol. 7026/2011, pp.&nbsp;581 – 596). Lisboa, Portugal. October 10–13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||\n|-\n| \'\'\'Jatowt, A., & Yeung, C. M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||\n|-\n| \'\'\'Weerkamp, W., & Rijke, M.\'\'\' (2012). [http://research.microsoft.com/en-us/people/milads/taia2012-activities.pdf Activity Prediction: A Twitter-based Exploration]. \'\'In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval\'\'. Portland, United States. August 16. || 2012 || SIGIR - TAIA || F-IRetrieval ||\n|-\n| \'\'\'Radinski, K., & Horvitz, E.\'\'\' (2013). [http://dl.acm.org/citation.cfm?id=2433431 Mining the Web to Predict Future Events]. \'\'In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;255 – 264). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || F-IRetrieval ||\n|}\n\n== Temporal image retrieval (T-IRetrieval) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Dias, G., Moreno, J. G., Jatowt, A., & Campos, R.\'\'\' (2012). [http://link.springer.com/content/pdf/10.1007%2F978-3-642-34109-0_21 Temporal Web Image Retrieval]. In Calderón-Benavides, L., González-Caro, C., Chávez, E., Ziviani, N. (Eds.), \'\'In Lecture Notes in Computer Science - [http://catic.unab.edu.co/spire/ SPIRE2012]: 19th International Symposium on String Processing and Information Retrieval\'\' (Vol. 7608/2012, pp.&nbsp;199 – 204). Cartagena de Indias, Colombia. October 21–25: Springer Berlin / Heidelberg. || 2012 || SPIRE || T-IRetrieval ||\n|-\n| \'\'\'Palermo, F., Hays, J., & Efros, A.\'\'\' (2012). [http://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_36 Dating Historical Color Images]. In Fitzgibbon, A., Lazebnik, S., Sato, Y., Schmid, C. (Eds.), \'\'In Lecture Notes in Computer Science - [http://eccv2012.unifi.it/ ECCV2012]: 12th European Conference on Computer Vision\'\' (Vol. 7577/2012, pp.&nbsp;499 – 512). Firenze, Italy. October 07–13: Springer Berlin / Heidelberg. || 2012 || ECCV || T-IRetrieval ||\n|-\n| \'\'\'Kim, G., & Xing, E. P.\'\'\' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. \'\'In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining\'\' (pp.&nbsp;163 – 172). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || T-IRetrieval ||\n|-\n| \'\'\'Martin, P., Doucet, A., & Jurie, F.\'\'\' (2014). [http://dl.acm.org/citation.cfm?id=2578790 Dating Color Images with Ordinal Classification]. \'\'In [http://www.icmr2014.org/ ICMR2014]: Proceedings of International Conference on Multimedia Retrieval\'\' (pp. 447). Glasgow, United Kingdom. April 01-04: ACM Press. || 2014 || ICMR || T-IRetrieval ||\n|}\n\n== Collective memory (C-memory) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Surowiecki, J.\'\'\' (2004). [http://www.amazon.com/The-Wisdom-Crowds-Collective-Economies/dp/0385503865 The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations]. USA: DoubleDay. || 2004 ||  || C-Memory ||\n|-\n| \'\'\'Hall, D., Jurafsky, D., & Manning, C. D.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1613715.1613763 Studying the History of Ideas using Topic Models]. \'\'In [http://conferences.inf.ed.ac.uk/emnlp08 EMNLP 2008]: Proceedings of the Conference on Empirical Methods in Natural Language Processing\'\' (pp.&nbsp;363 – 371). Waikiki, Honolulu, Hawaii. October 25–27: Association for Computational Linguistics. || 2008 || EMNLP || C-Memory ||\n|-\n| \'\'\'Shahaf, D., & Guestrin, C.\'\'\' (2010). [http://dl.acm.org/citation.cfm?id=1835884 Connecting the dots between News Articles]]. In [http://www.sigkdd.org/kdd2010/ KDD2010]: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&nbsp;623 – 632). Washington, United States. July 25–28: ACM Press. || 2010 || KDD || C-Memory ||\n|-\n| \'\'\'Takahashi, Y., Ohshima, H., Yamamoto, M., Iwasaki, H., Oyama, S., & Tanaka, K.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=1995980 Evaluating Significance of Historical Entities based on Tempo-spatial Impacts Analysis using Wikipedia Link Structure]]. \'\'In [http://www.ht2011.org/ HT2011]: Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia\'\' (pp.&nbsp;83 – 92). Eindhoven, Netherlands. June 6–9: ACM Press. || 2011 || HT || C-Memory ||\n|-\n| \'\'\'Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Team, T. G., et al.\'\'\' (2011). [http://www.sciencemag.org/content/331/6014/176 Quantitative Analysis of Culture Using Millions of Digitized Books]. In [http://www.sciencemag.org/ Science], 331(6014), 176 - 182. || 2011 || Science || C-Memory ||\n|-\n| \'\'\'Yeung, C.-m. A., & Jatowt, A.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]]. \'\'In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;1231 – 1240). Glasgow, Scotland, UK. October 24–28: ACM Press. || 2011 || CIKM || C-Memory ||\n|}\n\n== Web archives (W-archives) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| [[List of Web archiving initiatives|List of Web Archive Initiatives]] || 2011 ||  || W-Archives ||\n|-\n| \'\'\'Kahle, B.\'\'\' (1997, 03). [http://www.sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ISSUEID_CHAR=00B8E369-1805-4A27-A331-9D727FEAC21&ARTICLEID_CHAR=00B10B9E-5F13-40B2-AA51-0A4D5C41549 Preserving the Internet]. \'\'In [https://www.scientificamerican.com/sciammag/ Scientific American Magazine]\'\', 276(3), pp.&nbsp;72 – 73. || 1997 || SAM || W-Archives ||\n|-\n| \'\'\'Toyoda, M., & Kitsuregawa, M.\'\'\' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. \'\'In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia\'\' (pp.&nbsp;151 – 160). Salzburg, Austria. September 6–9: ACM Press. || 2005 || HT || W-Archives ||\n|-\n| \'\'\'Efendioglu, D., Faschetti, C., & Parr, T.\'\'\' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]]. In \'\'D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering\'\' (pp.&nbsp;119 – 120). Palo Alto, California, United States. July 11–14: ACM Press. || 2006 || ICWE || W-Archives ||\n|-\n| \'\'\'Jatowt, A., Kawai, Y., Nakamura, S., Kidawara, Y., & Tanaka, K.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1149969 Journey to the Past: Proposal of a Framework for Past Web Browser]. \'\'In HT2006: Proceedings of the 17th Conference on Hypertext and Hypermedia\'\' (pp.&nbsp;135 – 144). Odense, Denmark. August 22–25: ACM Press. || 2006 || HT || W-Archives ||\n|-\n| \'\'\'Adar, E., Dontcheva, M., Fogarty, J., & Weld, D. S.\'\'\' (2008). [http://dl.acm.org/citation.cfm?id=1449756 Zoetrope: Interacting with the Ephemeral Web]]. \'\'In S. B. Cousins, & M. Beaudouin-Lafon (Ed.), [http://www.acm.org/uist/uist2008/ UIST 2008]: Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology\'\' (pp.&nbsp;239 – 248). Monterey, CA, United States. October 19–22: ACM Press. || 2008 || UIST || W-Archives ||\n|-\n| \'\'\'Song, S., & JaJa, J.\'\'\' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||\n|-\n| \'\'\'Gomes, D., Miranda, J., & Costa, M.\'\'\' (2011). [http://dl.acm.org/citation.cfm?id=2042590 A Survey on Web Archiving Initiatives]]. \'\'In [http://www.tpdl2011.org/ TPDL2011]: Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced technology for digital libraries\'\' (pp.&nbsp;408 – 420). Berlin, Germany. September 25–29: Springer-Verlag || 2011 || TPDL || W-Archives ||\n|-\n| \'\'\'Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.\'\'\' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. \'\'In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\', (pp.&nbsp;235 – 243). Portland, United States. August 12-16. ACM Press || 2012 || SIGIR || W-Archives ||\n||\n|-\n| \'\'\'Costa, M., & Silva, M.J.\'\'\' (2012). [http://link.springer.com/chapter/10.1007%2F978-3-642-35063-4_32 Evaluating Web Archive Search Systems]. \'\'In [http://www.wise2012.cs.ucy.ac.cy/ WISE2012]: Proceedings of the 13th International Conference on Web Information System Engineering\'\', (pp.&nbsp;440 - 454). Paphos, Cyprus. November 28-30. Springer-Verlag || 2012 || WISE || W-Archives ||\n|}\n\n== Topic detection and tracking (TDT) ==\n{| class="wikitable sortable"\n|-\n! Reference !! Year !! Conference/Journal !! Main Scope !! Comments\n|-\n| \'\'\'Allan, J., Carbonell, J., Doddington, G., & Yamron, J.\'\'\' (1998). [http://www.cs.pitt.edu/~chang/265/proj10/sisref/1.pdf Topic Detection and Tracking Pilot Study Final Report]. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, (pp.&nbsp;194 – 218). Lansdowne, Virginia, United States. February. || 1998 || Technical Report || TDT ||\n|-\n| \'\'\'Swan, R., & Allan, J.\'\'\' (1999). [http://dl.acm.org/citation.cfm?id=319956 Extracting Significant Time-Varying Features from Text]]. \'\'In [http://cikmconference.org/1999/ CIKM 1999]]: Proceedings of the 8th International ACM Conference on Information and Knowledge Management\'\' (pp.&nbsp;38 – 45). Kansas City, Missouri, United States. November 2–6: ACM Press. || 1999 || CIKM || TDT ||\n|-\n| \'\'\'Swan, R., & Jensen, D.\'\'\' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. \'\'In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\'\' (pp.&nbsp;73 – 80). Boston, Massachusetts, United States. August 20–23: ACM Press. || 2000 || KDD - TM || TDT ||\n|-\n| \'\'\'Swan, R., & Allan, J.\'\'\' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. \'\'In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\'\' (pp.&nbsp;49 – 56). Athens, Greece. July 24–28: ACM Press. || 2000 || SIGIR || TDT ||\n|-\n| \'\'\'Makkonen, J., & Ahonen-Myka, H.\'\'\' (2003). [http://www.springerlink.com/content/a5ev5br7wwh5lvyl/ Utilizing Temporal Information in Topic Detection and Tracking]. \'\'In T. Koch, & I. T. Solvberg (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2003.org/ ECDL 2003]: 7th European Conference on Research and Advances Technology for Digital Libraries\'\' (Vol. 2769/2004, pp.&nbsp;393 – 404). Trondheim, Norway. August 17–22: Springer Berlin / Heidelberg. || 2003 || ECDL || TDT ||\n|-\n| \'\'\'Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.\'\'\' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. \'\'In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]\'\' (pp.&nbsp;165 – 174). Houston, United States. November 27–30: IEEE Press. || 2005 || ICDM - TDM || TDT ||\n|-\n| \'\'\'Mori, M., Miura, T., & Shioya, I.\'\'\' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. \'\'In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence\'\' (pp.&nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||\n|-\n| \'\'\'Kim, P., Myaeng, S.H.\'\'\' (2004). [http://dl.acm.org/citation.cfm?id=1039624 Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking]. \'\'In [http://talip.acm.org/index.htm TALIP]:Journal of ACM Transactions on Asian Language Information Processing\'\' (pp.&nbsp;227 – 242). New York, United States. || 2004 || TALIP || TDT ||\n|}\n\n==References==\n{{reflist}}\n\n[[Category:Information retrieval genres]]']
['Concept search', '17785794', 'A \'\'\'concept search\'\'\' (or \'\'\'conceptual search\'\'\') is an automated [[information retrieval]] method that is used to search electronically stored [[unstructured data|unstructured text]] (for example, [[digital archive]]s, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query.  In other words, the \'\'ideas\'\' expressed in the information retrieved in response to a [[concept]] search query are relevant to the ideas contained in the text of the query.\n\n__TOC__\n\n==Development==\nConcept search techniques were developed because of limitations imposed by classical Boolean [[Search algorithm|keyword search]] technologies when dealing with large, unstructured digital collections of text.  Keyword searches often return results that include many non-relevant items ([[false positive]]s) or that exclude too many relevant items (false negatives) because of the effects of [[synonymy]] and [[polysemy]].  Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.\n\nPolysemy is a major obstacle for all computer systems that attempt to deal with human language.  In English, most frequently used terms have several common meanings.  For example, the word fire can mean: a combustion activity; to terminate employment; to launch, or to excite (as in fire up).  For the 200 most-polysemous terms in English, the typical verb has more than twelve common meanings, or senses.  The typical noun from this set has more than eight common senses.  For the 2000 most-polysemous terms in English, the typical verb has more than eight common senses and the typical noun has more than five.<ref>Bradford, R. B., Word Sense Disambiguation, [[Content Analyst Company]], LLC, U.S. Patent 7415462, 2008.</ref>\n\nIn addition to the problems of polysemous and synonymy, keyword searches can exclude inadvertently [[misspelled]] words as well as the variations on the [[Stemming|stems]] (or roots) of words (for example, strike vs. striking).  Keyword searches are also susceptible to errors introduced by [[optical character recognition]] (OCR) scanning processes, which can introduce [[random error]]s into the text of documents (often referred to as [[noisy text]]) during the scanning process.\n\nA concept search can overcome these challenges by employing [[word sense disambiguation]] (WSD),<ref>R. Navigli, [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ACM Computing Surveys, 41(2), 2009.</ref> and other techniques, to help it derive the actual meanings of the words, and their underlying concepts, rather than by simply matching character strings like keyword search technologies.\n\n==Approaches==\nIn general, [[information retrieval]] research and technology can be divided into two broad categories: semantic and statistical. Information retrieval systems that fall into the semantic category will attempt to implement some degree of syntactic and [[Semantic analysis (machine learning)|semantic analysis]] of the [[natural language]] text that a human user would provide (also see [[computational linguistics]]).  Systems that fall into the statistical category will find results based on statistical measures of how closely they match the query.  However, systems in the semantic category also often rely on statistical methods to help them find and retrieve information.<ref>Greengrass, E., Information Retrieval: A Survey, 2000.</ref>\n\nEfforts to provide information retrieval systems with semantic processing capabilities have basically used three different approaches:\n\n* Auxiliary structures\n* Local [[co-occurrence]] statistics\n* Transform techniques (particularly [[matrix decomposition]]s)\n\n===Auxiliary structures===\nA variety of techniques based on [[artificial intelligence]] (AI) and [[natural language processing]] (NLP) have been applied to semantic processing, and most of them have relied on the use of auxiliary structures such as [[controlled vocabularies]] and [[Ontology (information science)|ontologies]].  Controlled vocabularies (dictionaries and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries.<ref>Dubois, C., The Use of Thesauri in Online Retrieval, Journal of Information Science, 8(2), 1984 March, pp. 63-66.</ref> Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries.  Over the years, additional auxiliary structures of general interest, such as the large synonym sets of [[WordNet]], have been constructed.<ref>Miller, G., Special Issue, [http://www.mit.edu/~6.863/spring2009/readings/5papers.pdf WordNet: An On-line Lexical Database], Intl. Journal of Lexicography, 3(4), 1990.</ref>  It was shown that concept search that is based on auxiliary structures, such as WordNet, can be efficiently implemented by reusing retrieval models and data structures of classical information retrieval.<ref>Fausto Giunchiglia, Uladzimir Kharkevich, and Ilya Zaihrayeu. [http://www.ulakha.com/concept-search-eswc2009.html Concept Search], In Proceedings of European Semantic Web Conference, 2009.</ref>  Later approaches have implemented grammars to expand the range of semantic constructs.  The creation of data models that represent sets of concepts within a specific domain (\'\'domain ontologies\'\'), and which can incorporate the relationships among terms, has also been implemented in recent years.\n\nHandcrafted controlled vocabularies contribute to the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized.  Controlled vocabularies require extensive human input and oversight to keep up with the rapid evolution of language.  They also are not well suited to the growing volumes of unstructured text covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced.  Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.<ref name="Bradford, R. B. 2008">Bradford, R. B., Why LSI? [[Latent Semantic Indexing]] and Information Retrieval, White Paper, [[Content Analyst Company]], LLC, 2008.</ref>\n\n===Local co-occurrence statistics===\nInformation retrieval systems incorporating this approach count the number of times that groups of terms appear together (co-occur) within a [[sliding window]] of terms or sentences (for example, ± 5 sentences or ± 50 words) within a document.  It is based on the idea that words that occur together in similar contexts have similar meanings.  It is local in the sense that the sliding window of terms and sentences used to determine the co-occurrence of terms is relatively small.\n\nThis approach is simple, but it captures only a small portion of the semantic information contained in a collection of text.  At the most basic level, numerous experiments have shown that approximately only ¼ of the information contained in text is local in nature.<ref>Landauer, T., and Dumais, S., A Solution to Plato\'s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge, Psychological Review, 1997, 104(2), pp. 211-240.</ref>   In addition, to be most effective, this method requires prior knowledge about the content of the text, which can be difficult with large, unstructured document collections.<ref name="Bradford, R. B. 2008"/>\n\n===Transform techniques===\nSome of the most powerful approaches to semantic processing are based on the use of mathematical transform techniques.  [[Matrix decomposition]] techniques have been the most successful.  Some widely used matrix decomposition techniques include the following:<ref>Skillicorn, D., Understanding Complex Datasets: Data Mining with Matrix Decompositions, CRC Publishing, 2007.</ref>\n\n* [[Independent component analysis]]\n* Semi-discrete decomposition\n* [[Non-negative matrix factorization]]\n* [[Singular value decomposition]]\n\nMatrix decomposition techniques are data-driven, which avoids many of the drawbacks associated with auxiliary structures.  They are also global in nature, which means they are capable of much more robust information extraction and representation of semantic information than techniques based on local co-occurrence statistics.<ref name="Bradford, R. B. 2008"/>\n\nIndependent component analysis is a technique that creates sparse representations in an automated fashion,<ref>Honkela, T., Hyvarinen, A. and Vayrynen, J. WordICA - Emergence of linguistic representations for words by independent component analysis. Natural Language Engineering, 16(3):277-308, 2010</ref> and the semi-discrete and non-negative matrix approaches sacrifice accuracy of representation in order to reduce computational complexity.<ref name="Bradford, R. B. 2008"/>\n\nSingular value decomposition (SVD) was first applied to text at Bell Labs in the late 1980s. It was used as the foundation for a technique called [[latent semantic indexing]] (LSI) because of its ability to find the semantic meaning that is latent in a collection of text.  At first, the SVD was slow to be adopted because of the resource requirements needed to work with large datasets.  However, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.  LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, Chapter 4, 2004.</ref>\n\n==Uses==\n* \'\'\'[[eDiscovery]]\'\'\' – Concept-based search technologies are increasingly being used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is much more efficient than traditional linear review techniques.  Concept-based searching is becoming accepted as a reliable and efficient search method that is more likely to produce relevant results than keyword or Boolean searches.<ref>Magistrate Judge John M. Facciola of the U.S. District Court for the District of Washington, D.C.\nDisability Rights Council v. Washington Metropolitan Transit Authority, 242 FRD 139 (D. D.C. 2007), citing George L. Paul & Jason R. Baron, "Information Inflation: Can the Legal System Adapt?" 13 Rich. J.L. & Tech. 10 (2007).</ref>\n\n* \'\'\'[[Enterprise Search]] and Enterprise Content Management (ECM)\'\'\' – Concept search technologies are being widely used in enterprise search.  As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis has become essential.  In 2004 the Gartner Group estimated that professionals spend 30 percent of their time searching, retrieving, and managing information.<ref name="Laplanche, R. 2004">Laplanche, R., Delgado, J., Turck, M., Concept Search Technology Goes Beyond Keywords, Information Outlook, July 2004.</ref>  The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.<ref name="Laplanche, R. 2004"/>\n* \'\'\'[[Content-based image retrieval|Content-Based Image Retrieval (CBIR)]]\'\'\' – Content-based approaches are being used for the semantic retrieval of digitized images and video from large visual corpora.  One of the earliest content-based image retrieval systems to address the semantic problem was the ImageScape search engine.  In this system, the user could make direct queries for multiple visual objects such as sky, trees, water, etc. using spatially positioned icons in a WWW index containing more than ten million images and videos using keyframes.  The system used information theory to determine the best features for minimizing uncertainty in the classification.<ref name="Lew, M. S. 2006">Lew, M. S., Sebe, N., Djeraba, C., Jain, R., Content-based Multimedia Information Retrieval: State of the Art and Challenges, ACM Transactions on Multimedia Computing, Communications, and Applications, February 2006.</ref>  The semantic gap is often mentioned in regard to CBIR.  The semantic gap refers to the gap between the information that can be extracted from visual data and the interpretation that the same data have for a user in a given situation.<ref>Datta R., Joshi, D., Li J., Wang, J. Z., [http://infolab.stanford.edu/~wangz/project/imsearch/review/JOUR/datta.pdf Image Retrieval: Ideas, Influences, and Trends of the New Age], ACM Computing Surveys, Vol. 40, No. 2, April 2008.</ref>  The [http://www.liacs.nl/~mir ACM SIGMM Workshop on Multimedia Information Retrieval] is dedicated to studies of CBIR.\n* \'\'\'Multimedia and Publishing\'\'\' – Concept search is used by the multimedia and publishing industries to provide users with access to news, technical information, and subject matter expertise coming from a variety of unstructured sources.  Content-based methods for multimedia information retrieval (MIR) have become especially important when text annotations are missing or incomplete.<ref name="Lew, M. S. 2006"/>\n* \'\'\'Digital Libraries and Archives\'\'\' – Images, videos, music, and text items in digital libraries and digital archives are being made accessible to large groups of users (especially on the Web) through the use of concept search techniques.  For example, the Executive Daily Brief (EDB), a business information monitoring and alerting product developed by EBSCO Publishing, uses concept search technology to provide corporate end users with access to a digital library containing a wide array of business content.  In a similar manner, the [[Music Genome Project]] spawned Pandora, which employs concept searching to spontaneously create individual music libraries or \'\'virtual\'\' radio stations.\n* \'\'\'Genomic Information Retrieval (GIR)\'\'\' – Genomic Information Retrieval (GIR) uses concept search techniques applied to genomic literature databases to overcome the ambiguities of scientific literature.\n* \'\'\'Human Resources Staffing and Recruiting\'\'\' – Many human resources staffing and recruiting organizations have adopted concept search technologies to produce highly relevant resume search results that provide more accurate and relevant candidate resumes than loosely related keyword results.\n\n==Effective searching==\nThe effectiveness of a concept search can depend on a variety of elements including the dataset being searched and the search engine that is used to process queries and display results. However, most concept search engines work best for certain kinds of queries:\n\n* Effective queries are composed of enough text to adequately convey the intended concepts.  Effective queries may include full sentences, paragraphs, or even entire documents.  Queries composed of just a few words are not as likely to return the most relevant results.\n* Effective queries do not include concepts in a query that are not the object of the search.  Including too many unrelated concepts in a query can negatively affect the relevancy of the result items.  For example, searching for information about \'\'boating on the Mississippi River\'\' would be more likely to return relevant results than a search for \'\'boating on the Mississippi River on a rainy day in the middle of the summer in 1967.\'\'\n* Effective queries are expressed in a full-text, natural language style similar in style to the documents being searched.  For example, using queries composed of excerpts from an introductory science textbook would not be as effective for concept searching if the dataset being searched is made up of advanced, college-level science texts.  Substantial queries that better represent the overall concepts, styles, and language of the items for which the query is being conducted are generally more effective.\n\nAs with all search strategies, experienced searchers generally refine their queries through multiple searches, starting with an initial \'\'seed\'\' query to obtain conceptually relevant results that can then be used to compose and/or refine additional queries for increasingly more relevant results.  Depending on the search engine, using query concepts found in result documents can be as easy as selecting a document and performing a \'\'find similar\'\' function.  Changing a query by adding terms and concepts to improve result relevance is called \'\'[[query expansion]]\'\'.<ref>[[Stephen Robertson (computer scientist)|Robertson, S. E.]], [[Karen Spärck Jones|Spärck Jones, K.]], Simple, Proven Approaches to Text Retrieval, Technical Report, University of Cambridge Computer Laboratory, December 1994.</ref> The use of [[ontology (information science)|ontologies]] such as WordNet has been studied to expand queries with conceptually-related words.<ref>Navigli, R., Velardi, P. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. \'\'Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)\'\', in the \'\'14th European Conference on Machine Learning (ECML 2003)\'\', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&nbsp;42–49</ref>\n\n==Relevance feedback==\n[[Relevance feedback]] is a feature that helps users determine if the results returned for their queries meet their information needs.  In other words, relevance is assessed relative to an information need, not a query.  A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.<ref name="Manning, C. D. 2008">Manning, C. D., Raghavan P., Schütze H., Introduction to Information Retrieval, Cambridge University Press, 2008.</ref>   It is a way to involve users in the retrieval process in order to improve the final result set.<ref name="Manning, C. D. 2008"/> Users can refine their queries based on their initial results to improve the quality of their final results.\n\nIn general, concept search relevance refers to the degree of similarity between the concepts expressed in the query and the concepts contained in the results returned for the query.  The more similar the concepts in the results are to the concepts contained in the query, the more relevant the results are considered to be.  Results are usually ranked and sorted by relevance so that the most relevant results are at the top of the list of results and the least relevant results are at the bottom of the list.\n\nRelevance feedback has been shown to be very effective at improving the relevance of results.<ref name="Manning, C. D. 2008"/>   A concept search decreases the risk of missing important result items because all of the items that are related to the concepts in the query will be returned whether or not they contain the same words used in the query.<ref name="Laplanche, R. 2004"/>\n\n[[Ranking]] will continue to be a part of any modern information retrieval system.  However, the problems of heterogeneous data, scale, and non-traditional discourse types reflected in the text, along with the fact that search engines will increasingly be integrated components of complex information management processes, not just stand-alone systems, will require new kinds of system responses to a query.  For example, one of the problems with ranked lists is that they might not reveal relations that exist among some of the result items.<ref name="Callan, J. 2007">Callan, J., Allan, J., Clarke, C. L. A., Dumais, S., Evans, D., A., Sanderson, M., Zhai, C., Meeting of the MINDS: An Information Retrieval Research Agenda, ACM, SIGIR Forum, Vol. 41 No. 2, December 2007.</ref>\n\n==Guidelines for evaluating a concept search engine==\n# Result items should be relevant to the information need expressed by the concepts contained in the query statements, even if the terminology used by the result items is different from the terminology used in the query.\n# Result items should be sorted and ranked by relevance.\n# Relevant result items should be quickly located and displayed.  Even complex queries should return relevant results fairly quickly.\n# Query length should be \'\'non-fixed\'\', i.e., a query can be as long as deemed necessary.  A sentence, a paragraph, or even an entire document can be submitted as a query.\n# A concept query should not require any special or complex syntax.  The concepts contained in the query can be clearly and prominently expressed without using any special rules.\n# Combined queries using concepts, keywords, and metadata should be allowed.\n# Relevant portions of result items should be usable as query text simply by selecting the item and telling the search engine to \'\'find similar\'\' items.\n# Query-ready indexes should be created relatively quickly.\n# The search engine should be capable of performing Federated searches.  Federated searching enables concept queries to be used for simultaneously searching multiple datasources for information, which are then merged, sorted, and displayed in the results.\n# A concept search should not be affected by misspelled words, typographical errors, or OCR scanning errors in either the query text or in the text of the dataset being searched.\n\n==Conferences and forums==\nFormalized search engine evaluation has been ongoing for many years.  For example, the [[Text Retrieval Conference|Text REtrieval Conference (TREC)]] was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today\'s commercial search engines include technology first developed in TREC.<ref>Croft, B., Metzler, D., Strohman, T., Search Engines, Information Retrieval in Practice, Addison Wesley, 2009.</ref>\n\nIn 1997, a Japanese counterpart of TREC was launched, called National Institute of Informatics Test Collection for IR Systems (NTCIR).  NTCIR conducts a series of evaluation workshops for research in information retrieval, question answering, text summarization, etc.  A European series of workshops called the Cross Language Evaluation Forum (CLEF) was started in 2001 to aid research in multilingual information access.  In 2002, the Initiative for the Evaluation of XML Retrieval (INEX) was established for the evaluation of content-oriented XML retrieval systems.\n\nPrecision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user\'s information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.<ref name="Manning, C. D. 2008"/>\n\nAlthough the workshops and publicly available test collections used for search engine testing and evaluation have provided substantial insights into how information is managed and retrieved, the field has only scratched the surface of the challenges people and organizations face in finding, managing, and, using information now that so much information is available.<ref name="Callan, J. 2007"/>   Scientific data about how people use the information tools available to them today is still incomplete because experimental research methodologies haven’t been able to keep up with the rapid pace of change. Many challenges, such as contextualized search, personal information management, information integration, and task support, still need to be addressed.<ref name="Callan, J. 2007"/>\n\n==See also==\n{{div col|3}}\n* [[Approximate string matching]]\n* [[Compound term processing]]\n* [[Concept mining]]\n* [[Information extraction]]\n* [[Latent semantic analysis]]\n* [[Semantic network]]\n* [[Semantic search]]\n* [[Semantic Web]]\n* [[Statistical semantics]]\n* [[Text mining]]\n{{div col end}}\n\n==References==\n{{Reflist|2}}\n\n==External links==\n* [http://trec.nist.gov/ Text Retrieval Conference (TREC)]\n* [http://research.nii.ac.jp/ntcir/ National Institute of Informatics Test Collection for IR Systems (NTCIR)]\n* [http://www.clef-campaign.org/ Cross Language Evaluation Forum (CLEF)]\n* [http://inex.is.informatik.uni-duisburg.de/ Initiative for the Evaluation of XML Retrieval (INEX)]\n\n[[Category:Information retrieval genres]]']
['Cognitive models of information retrieval', '24963841', '{{Orphan|date=September 2012}}\n\n\'\'\'Cognitive models of information retrieval\'\'\' rest on the mix of areas such as [[cognitive science]], [[human-computer interaction]], [[information retrieval]], and  [[library science]]. They describe the relationship between a person\'s cognitive model of the information sought and the organization of this information in an information system.  These models attempt to understand how a person is searching for information so that the database and the search of this database can be designed in such a way as to best serve the user. [[Information retrieval]] may incorporate multiple tasks and cognitive problems, particularly because different people may have different methods for attempting to find this information and expect the information to be in different forms.  Cognitive models of information retrieval may be attempts at something as apparently prosaic as improving search results or may be something more complex, such as attempting to create a database which can be queried with natural language search.\n\n==Berrypicking==\nOne way of understanding how users search for information has been described by [[Marcia Bates]]<ref>[[Marcia Bates]] (1989). "The Design of Browsing and Berrypicking Techniques for the Online Search Interface." https://pages.gseis.ucla.edu/faculty/bates/berrypicking.html</ref> at the [[University of California at Los Angeles]]. Bates argues that "berrypicking" better reflects how users search for information than previous models of information retrieval.  This may be because previous models were strictly linear and did not incorporate cognitive questions.  For instance, one typical model is of a simple linear match between a query and a document.  However, Bates points out that there are simple modifications that can be made to this process.  For instance, Salton has argued that user feedback may help improve the search results.<ref>[[Gerard Salton]] (1968). \'\'Automatic Information and Retrieval\'\' (Computer Science). Dubuque, Iowa: Mcgraw-Hill Inc.</ref>\n\nBates argues that searches are evolving and occur bit by bit.  That is to say, a person constantly changes his or her search terms in response to the results returned from the information retrieval system.  Thus, a simple linear model does not capture the nature of information retrieval because the very act of searching causes feedback which causes the user to modify his or her [[cognitive model]] of the information being searched for.  In addition, information retrieval can be bit by bit.  Bates gives a number of examples.  For instance, a user may look through footnotes and follow these sources.  Or, a user may scan through recent journal articles on the topic.  In each case, the user\'s question may change and thus the search evolves.\n\n==Exploratory Search==\nResearchers in the areas of [[human-computer interaction]] and [[cognitive science]] focus on how people explore for information when interacting with the WWW. This kind of search, sometimes called [[exploratory search]], focuses on how people iteratively refine their search activities and update their internal representations of the search problems.<ref>Qu, Yan & Furnas, George. "Model-driven formative evaluation of exploratory search: A study under a sensemaking framework"</ref> Existing search engines were designed based on traditional library science theories related to retrieval basic facts and simple information through an interface. However, exploratory information retrieval often involves ill-defined search goals and evolving criteria for evaluation of relevance. The interactions between humans and the information system will therefore involve more cognitive activity, and systems that support exploratory search will therefore need to take into account the cognitive complexities involved during the dynamic information retrieval process.\n\n==Natural language searching==\n\nAnother way in which cognitive models of information may help in information retrieval is with natural language searching.  For instance, How Stuff Works imagines a world in which, rather than searching for local movies, reading the reviews, then searching for local Mexican restaurants, and reading their reviews, you will simply type ""I want to see a funny movie and then eat at a good Mexican restaurant. What are my options?" into your browser, and you will receive a useful and relevant response.<ref>Strickland, J. (n.d.). HowStuffWorks "How Web 3.0 Will Work". Howstuffworks "Computer". Retrieved November 4, 2009, from http://computer.howstuffworks.com/web-30.htm</ref>  Although such a thing is not possible today, it represents a holy grail for researchers into cognitive models of information retrieval.  The goal is to somehow program information retrieval programs to respond to natural language searches.  This would require a fuller understanding of how people structure queries.\n\n==Notes==\n{{Reflist}}\n\n[[Category:Information retrieval genres]]\n[[Category:Cognitive modeling]]']
['Audio mining', '14004969', '{{unreferenced|date=January 2012}}\n\'\'\'Audio mining\'\'\' is a technique by which the content of an audio signal can be automatically analysed and searched. It is most commonly used in the field of [[speech recognition|automatic speech recognition]], where the analysis tries to identify any speech within the audio. The audio will typically be processed by a speech recognition system in order to identify word or [[phoneme]] units that are likely to occur in the spoken content. This information may either be used immediately in pre-defined searches for keywords or phrases (a real-time "word spotting" system), or the output of the speech recogniser may be stored in an index file. One or more audio mining index files can then be loaded at a later date in order to run searches for keywords or phrases.\n\nThe results of a search will normally be in terms of hits, which are regions within files that are good matches for the chosen keywords. The user may then be able to listen to the audio corresponding to these hits in order to verify if a correct match was found.\n\nAudio mining systems used in the field of speech recognition are often divided into two groups: those that use [[Large Vocabulary Continuous Speech Recogniser]]s (LVCSR) and those that use phonetic recognition. \n\nMusical audio mining (also known as [[music information retrieval]]) relates to the identification of perceptually important characteristics of a piece of music such as melodic, harmonic or rhythmic structure. Searches can then be carried out to find pieces of music that are similar in terms of their melodic, harmonic and/or rhythmic characteristics.\n\n==See also==\n* [[Speech Analytics]]\n\n\n[[Category:Speech recognition]]\n[[Category:Music information retrieval]]\n[[Category:Information retrieval genres]]\n[[Category:Computational linguistics]]']
['Karen Spärck Jones', '17212387', '{{Infobox scientist\n| name = Karen Spärck Jones\n| image = Karen Spärck.jpg\n| caption = Karen Spärck Jones in 2002\n| birth_date = {{birth date|1935|8|26|df=y}}\n| birth_place = [[Huddersfield]], [[Yorkshire]]\n| death_date = {{death date and age|2007|4|4|1935|8|26|df=y}}\n| death_place = [[Willingham, Cambridgeshire]]\n| residence = United Kingdom\n| nationality = British\n| field = Computer science\n| work_institution = [[University of Cambridge Computer Laboratory]]\n| alma_mater = University of Cambridge\n| doctoral_advisor = Richard Braithwaite<ref name=odnb>{{cite web|title=Jones, Karen Ida Boalth Spärck (1935–2007), Computer Scientist|url=http://www.oxforddnb.com/view/article/98729|work=Oxford Dictionary of National Biography|publisher=Oxford University Press|accessdate=5 October 2014}}</ref>\n| thesis_title = Synonymy and Semantic Classiﬁcation\n| thesis_year = 1964<ref>{{cite book|author=Karen Spärck Jones|title=Synonymy and Semantic Classification (thesis published as a book)|publisher=Edinburgh University Press|series=Edinburgh Information Technology series|volume=1|year=1986}}</ref>\n| doctoral_students = \n| known_for  = work on information retrieval and natural language processing, in particular her probabilistic model of document and text retrieval\n| prizes = ACL Lifetime Achievement Award, BCS Lovelace Medal, ACM-AAAI Allen Newell Award, ACM SIGIR Salton Award, American Society for Information Science and Technology’s Award of Merit\n| religion = \n| spouse = [[Roger Needham]]\n| website = {{URL|http://www.cl.cam.ac.uk/archive/ksj21}}\n}}\n\n\'\'\'Karen Spärck Jones\'\'\' [[Fellow of the British Academy|FBA]] (26 August 1935 – 4 April 2007) was a [[United Kingdom|British]] computer scientist.<ref>{{Cite journal | last1 = Tait | first1 = J. I. | title = Karen Spärck Jones | doi = 10.1162/coli.2007.33.3.289 | journal = Computational Linguistics | volume = 33 | issue = 3 | pages = 289–291 | year = 2007 | pmid =  | pmc = }}</ref><ref>{{Cite journal | last1 = Robertson | first1 = S. | last2 = Tait | first2 = J. | doi = 10.1002/asi.20784 | title = Karen Spärck Jones | journal = Journal of the American Society for Information Science and Technology | volume = 59 | issue = 5 | pages = 852 | year = 2008 | pmid =  | pmc = }}</ref>\n\n==Personal life==\nKaren Ida Boalth Spärck Jones was born in [[Huddersfield]], [[Yorkshire]], [[England]]. Her father was Owen Jones, a lecturer in chemistry, and her mother was Ida Spärck, a [[Norway|Norwegian]] who moved to Britain during [[World War II]].  They left Norway on one of the last boats out after the German invasion in 1940.<ref name="odnb" /> Spärck Jones was educated at a grammar school in Huddersfield and then [[Girton College, Cambridge]] from 1953 to 1956, reading History, with an additional final year in Moral Sciences (philosophy). She briefly became a school teacher, before moving into Computer Science.  During her career in Computer Science, she campaigned hard for more women to enter computing.<ref name="odnb" />   She was married to fellow Cambridge computer scientist [[Roger Needham]] until his death in 2003. She died 4 April 2007 at [[Willingham, Cambridgeshire|Willingham]] in [[Cambridgeshire]].\n\n==Career==\nShe worked at the Cambridge Language Research Unit from the late 1950s,<ref>{{cite web|url=http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/|title=Computer Laboratory obituary}}</ref> then at [[University of Cambridge|Cambridge\'s]] [[Cambridge University Computer Laboratory|Computer Laboratory]] from 1974, and retired in 2002, holding the post of Professor of Computers and Information, which she was awarded in 1999.<ref name="odnb" /> She continued to work in the Computer Laboratory until shortly before her death. Her main research interests, since the late 1950s, were [[natural language processing]] and [[information retrieval]].<ref name="doi10.1108/eb026526">{{Cite journal | last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| doi = 10.1108/eb026526 | title = A Statistical Interpretation of Term Specificity and Its Application in Retrieval | journal = Journal of Documentation | volume = 28 | pages = 11–21 | year = 1972 | url = http://www.emeraldinsight.com/doi/abs/10.1108/eb026526| pmid =  | pmc = }}</ref><ref>{{Cite journal | editor1-last = Tait | editor1-first = John I. | title = Charting a New Course: Natural Language Processing and Information Retrieval, Essays in Honour of Karen Spärck Jones| doi = 10.1007/1-4020-3467-9 | series = The Kluwer International Series on Information Retrieval | volume = 16 | year = 2005 | isbn = 1-4020-3343-5 | pmid =  | pmc = }}</ref> One of her most important contributions was the concept of [[inverse document frequency]] (IDF) weighting in information retrieval, which she introduced in a 1972 paper.<ref name="doi10.1108/eb026526"/><ref name="idf">{{Cite journal | last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| title = Index term weighting | doi = 10.1016/0020-0271(73)90043-0 | journal = Information Storage and Retrieval | volume = 9 | issue = 11 | pages = 619–633 | year = 1973 | pmid =  | pmc = }}</ref> IDF is used in most search engines today, usually as part of the [[tf-idf]] weighting scheme.<ref>{{Cite book | last1 = Maybury | first1 = M. T. | chapter = Karen Spärck Jones and Summarization | doi = 10.1007/1-4020-3467-9_7 | title = Charting a New Course: Natural Language Processing and Information Retrieval | series = The Kluwer International Series on Information Retrieval | volume = 16 | pages = 99–10 | year = 2005 | isbn = 1-4020-3343-5 | pmid =  | pmc = }}</ref>\n\nThere is an annual [[British Computer Society|BCS]] lecture named in her honour.<ref>{{cite web|title=Karen Spärck Jones lecture|url=http://academy.bcs.org/ksj|work=BCS Academy of Computing|publisher=British Computer Society|accessdate=3 October 2013}}</ref>\n\n===Honours===\n* Fellow of the [[British Academy]], of which she was Vice-President in 2000–02\n* Fellow of [[AAAI]]\n* Fellow of [[ECCAI]]\n* President of the [[Association for Computational Linguistics]] in 1994\n\n===Awards===\n* [[Gerard Salton Award]] (1988)\n* [[ASIS&T]] Award of Merit (2002)\n* [[Association for Computational Linguistics|ACL]] Lifetime Achievement Award (2004) <ref>{{cite web|title=ACL Lifetime Achievement Award Recipients|url=http://aclweb.org/aclwiki/index.php?title=ACL_Lifetime_Achievement_Award_Recipients|website=ACL wiki|publisher=[[Association for Computational Linguistics|ACL]]|accessdate=16 August 2014}}</ref>\n* [[British Computer Society|BCS]] [[Lovelace Medal]] (2007)\n* [[ACM - AAAI Allen Newell Award]] (2006)\n\n==Karen Spärck Jones Award==\nTo commemorate her achievements, the Karen Spärck Jones Award was created in 2008 by the [[British Computer Society|BCS]] and its Information Retrieval Specialist Group (BCS IRSG), which is sponsored by [[Microsoft Research]].<ref>[http://irsg.bcs.org/ksjaward.php Microsoft BCS/BCS IRSG Karen Spärck Jones Award An Award to Commemorate Karen Spärck Jones]</ref>\n\nThe recipients are:\n* 2016, [[Jaime Teevan]]\n* 2015, [[Jordan Boyd-Graber]], [[Emine Yilmaz]]\n* 2014, [[Ryen White]]\n* 2013, [[Eugene Agichtein]]\n* 2012, [[Diane Kelly(computer scientist)]]\n* 2011, No award was made\n* 2010, [[Evgeniy Gabrilovich]]\n* 2009, [[Mirella Lapata]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* [http://spectrum.ieee.org/may07/5063 Computer Science, A Woman\'s Work], IEEE Spectrum, May 2007\n\n==External links==\n*[http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/video/ Video: Natural Language and the Information Layer, Karen Spärck Jones, March 2007]\n*[http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/ University of Cambridge obituary]\n*[http://news.independent.co.uk/people/obituaries/article2441969.ece Obituary], \'\'[[The Independent]]\'\', 12 April 2007 {{dead link|date=April 2014}}\n*[http://www.telegraph.co.uk/news/main.jhtml?view=DETAILS&grid=&xml=/news/2007/04/12/db1201.xml  Obituary], \'\'[[The Daily Telegraph]]\'\', 12 April 2007 {{dead link|date=April 2014}}\n*[http://www.timesonline.co.uk/tol/comment/obituaries/article1968942.ece Obituary], \'\'[[The Times]]\'\', 22 June 2007 {{subscription required}}\n\n{{s-start}}\n{{s-ach}}\n{{succession box |\n before=[[Makoto Nagao]] |\n title=ACL Lifetime Achievement Award |\n after=[[Martin Kay]] |\n years=2004}}\n{{s-end}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Sparck Jones, Karen}}\n[[Category:1935 births]]\n[[Category:2007 deaths]]\n[[Category:Alumni of Girton College, Cambridge]]\n[[Category:British computer scientists]]\n[[Category:Women computer scientists]]\n[[Category:Fellows of the British Academy]]\n[[Category:Fellows of the Association for the Advancement of Artificial Intelligence]]\n[[Category:Fellows of Newnham College, Cambridge]]\n[[Category:Fellows of Wolfson College, Cambridge]]\n[[Category:Members of the University of Cambridge Computer Laboratory]]\n[[Category:People from Huddersfield]]\n[[Category:Deaths from cancer in England]]\n[[Category:Information retrieval researchers]]\n[[Category:British women scientists]]\n[[Category:Artificial intelligence researchers]]\n[[Category:20th-century women scientists]]']
['W. Bruce Croft', '24963451', "'''W. Bruce Croft''' is a [[distinguished professor]] of [[computer science]] at the [[University of Massachusetts Amherst]] whose work focuses on [[information retrieval]].<ref>\n{{Cite web\n  | last = Croft\n  | first = W. Bruce\n  | title = Biography\n  | url=http://ciir.cs.umass.edu/personnel/croftbio.pdf\n  | accessdate = November 4, 2009}}</ref>\nHe is the founder of the [[Center for Intelligent Information Retrieval]] and served as the editor-in-chief of [[ACM Transactions on Information Systems]] from 1995 to 2002.  He was also a member of the [[United States National Research Council|National Research Council]] [http://sites.nationalacademies.org/CSTB/index.htm Computer Science and Telecommunications Board] from 2000 to 2003. Since 2015, he is the Dean of the College of Information and Computer Sciences at the University of Massachusetts Amherst. He was Chair of the UMass Amherst Computer Science Department from 2001 to 2007.\n\nBruce Croft formed the [[Center for Intelligent Information Retrieval]] (CIIR) in 1991, since when he and his students have worked with more than 90 industry and government partners on research and technology projects and have produced more than 900 papers. Bruce Croft has made major contributions to most areas of information retrieval, including pioneering work in clustering, passage retrieval, sentence retrieval, and distributed search. One of the most important areas of work for Croft  relates to ranking functions and retrieval models, where he has led the development of one of the major approaches to modeling search: language modelling. In later years, Croft also led the way in the development of feature-based ranking functions. Croft and his research group have also developed a series of search engines: InQuery, the Lemur toolkit, Indri, and Galago. These search engines are open source and offer unique capabilities that are not replicated in other research retrieval platforms source – consequently they are downloaded by hundreds of researchers world wide. As a consequence of his work, Croft is one of the most cited researchers in information retrieval.\n\n==Education==\nCroft earned a bachelor's degree with honors in 1973 and a master's degree in computer science in 1974 from [[Monash University]] in [[Melbourne|Melbourne, Australia]].  He earned his Ph.D in computer science from the [[University of Cambridge]] in 1979 and joined the [[University of Massachusetts Amherst|University of Massachusetts, Amherst]] faculty later that year.\n\n==Honors and awards==\nCroft has received several prestigious awards, including:\n* [[ACM Fellow]] in 1997\n* [[American Society for Information Science and Technology]] Research Award in 2000\n* [[Gerard Salton Award]] (a lifetime achievement award) from ACM SIGIR in 2003\n* [[Tony Kent Strix award|Tony Kent Strix Award]] in 2013\n* IEEE Computer Society Technical Achievement Award in 2014\n* [http://sigir.org/awards/best-student-paper-awards/ Best Student Paper Award] from SIGIR in 1997 and 2005\n* [http://sigir.org/awards/test-of-time-awards/ Test of Time Award] from SIGIR for his papers published in 1990, 1995, 1996, 1998, 2001\n* Many other publications are short-listed as the Best Paper Award in SIGIR and CIKM\n\n==References==\n<references/>\n\n==External links==\n* [http://ciir.cs.umass.edu/personnel/croft.html Faculty homepage]\n\n{{DEFAULTSORT:Croft, W. Bruce}}\n[[Category:American computer scientists]]\n[[Category:Fellows of the Association for Computing Machinery]]\n[[Category:University of Massachusetts Amherst faculty]]\n[[Category:Year of birth missing (living people)]]\n[[Category:Living people]]\n[[Category:Information retrieval researchers]]\n\n\n{{Compu-bio-stub}}"]
['KM programming language', '525334', "{{Infobox programming language\n| name = KM\n| paradigm = [[knowledge representation]]\n| generation =\n| year = \n| designer =\n| developer = \n| latest_release_version = \n| latest_release_date = \n| turing-complete = \n| typing = \n| implementations = \n| dialects = \n| influenced_by = [[KRL (programming language)|KRL]]\n| influenced = \n}}\n\n'''KM''', the '''Knowledge Machine''', is a [[Knowledge frame|frame]]-based language used for [[knowledge representation]] work.\n\nIt has first-order logic semantics, and includes machinery for reasoning, including selection by description, unification, classification, and reasoning about actions. Its origins were the Theo language and [[KRL (programming language)|KRL]], and is implemented in [[Lisp (programming language)|Lisp]].\n\n==External links==\n* [http://www.cs.utexas.edu/users/mfkb/RKF/km.html KM: The Knowledge Machine]. \n* An Ontology editor for the KM language: [http://www.algo.be/ref-projects.htm#KMgen/ KMgen].\n\n[[Category:Declarative programming languages]]\n[[Category:Knowledge representation]]\n[[Category:Common Lisp software]]\n\n\n{{compu-lang-stub}}"]
['Logic form', '1936537', '\'\'\'Logic forms\'\'\' are simple, [[first-order logic]] [[knowledge representation]]s of [[natural language]] sentences formed by the conjunction of concept predicates related through shared arguments. Each noun, verb, adjective, adverb, pronoun, preposition and conjunction generates a predicate. Logic forms can be decorated with [[word sense]]s to [[Word sense disambiguation|disambiguate]] the semantics of the word. There are two types of predicates: events are marked with \'\'e\'\', and entities are marked with \'\'x\'\'. The shared arguments connect the subjects and objects of verbs and prepositions together. Example input/output might look like this:\n Input:  \'\'\'The Earth provides the food we eat every day.\'\'\'\n Output: \'\'\'Earth\'\'\':n_#1(<span style="color:#008800;">x1</span>) \'\'\'provide\'\'\':v_#2(<span style="color:#888800;">e1</span>, <span style="color:#008800;">x1</span>, <span style="color:#880000;">x2</span>) \'\'\'food\'\'\':n_#1(<span style="color:#880000;">x2</span>) \'\'\'we\'\'\'(<span style="color:#000088;">x3</span>) \'\'\'eat\'\'\':v_#1(<span style="color:#880088;">e2</span>, <span style="color:#000088;">x3</span>, <span style="color:#880000;">x2</span>; <span style="color:#008888;">x4</span>) \'\'\'day\'\'\':n_#1(<span style="color:#008888;">x4</span>)\n\nLogic forms are used in some [[natural language processing]] techniques, such as [[question answering]], as well as in [[inference]] both for [[database]] systems and QA systems.\n\n==Evaluations==\n[http://www.senseval.org/ SENSEVAL-3] in 2004 introduced a {{webarchive |url=https://web.archive.org/web/20050902115653/site=http://www.cs.iusb.edu/~vasile/logic/indexLF.html |date=September 2, 2005 |title=Logic Form Identification task }}.\n\n==References==\n*{{cite book | author=Vasile Rus | title=Logic Form for WordNet Glosses  | url=http://www.engr.smu.edu/~vasile/rus02.PhDThesis.ps | publisher=Ph.D. thesis, Southern Methodist University | year=2002 }} <!-- Most information in the article derived from Vasile\'s work -->\n*{{cite journal | author=Vasile Rus and Dan Moldovan | title=High performance logic form transformation | journal=International Journal for Tools with Artificial Intelligence. IEEE Computer Society, IEEE Press |date=September 2002 | volume=11| issue =  3 | pages=437–454 | url=http://www.worldscinet.com/ijait/11/1103/S0218213002000976.html}}\n*{{cite conference | author=Dan Moldovan and Vasile Rus | url=http://engr.smu.edu/~vasile/acl2001.ps | title=Logic Form transformation of wordNet and its Applicability to question answering | booktitle=Proceedings of ACL 2001, Toulouse, France | year=2001 | pages=}}\n*{{cite conference | author=Jerry R. Hobbs | title=Overview of the TACITUS project | booktitle=Computational Linguistics| year=1986 | pages=12(3)}}\n*{{cite conference | author=Vasile Rus | url=http://acl.ldc.upenn.edu/acl2004/senseval/pdf/rus.pdf | title=A First Evaluation of Logic Form Identification Systems | booktitle=SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | year=2004 | pages=|format=PDF}}\n\n[[Category:Natural language processing]]\n[[Category:Computational linguistics]]\n[[Category:Knowledge representation]]\n{{ling-stub}}']
['Guideline execution engine', '2804505', "A '''Guideline Execution Engine''' is a [[computer program]] which can interpret a [[guideline (medical)|clinical guideline]] represented in a computerized format and perform actions towards the user of an [[electronic medical record]].\n\nA Guideline Execution Engine needs to communicate with a host [[Clinical information system]]. [[virtual Medical Record|vMR]] is one possible interface which can be used.\n\nThe engine's main function is to manage instances of executed guidelines of individual patients.\n\nDelivering the inferred engine recommendations or impacts to the host Clinical information system  has to carefully respect current workflow of the clinicians (physicians, nurses, clerks, etc.)\n\n== Architecture of Guideline Execution Engine ==\nThe following modules are generally needed for any engine\n\n* interface to Clinical Information System\n* new guidelines loading module\n* guideline interpreter module\n* clinical events parser\n* alert/recommendations dispatch\n\n== Guideline Interchange Format ==\n\nThe ''Guideline Interchange Format (GLIF)'' is computer representation format for [[clinical guideline]]s.<ref>{{cite web |url=http://mis.hevra.haifa.ac.il/~morpeleg/Intermed/ |title=Guideline Representation Page: GLIF 2.0, 3.4, 3.5 Specifications |work=Stanford University, School of Medicine, InterMed Collaboratory }}</ref> Represented guidelines can be executed using a guideline execution engine.\n\nThe format has several versions as it has been improved. In 2003 GLIF3 was introduced.\n\n== Use of third party workflow engine as a guideline execution engine ==\nSome commercial Electronic Health Record systems use a [[workflow engine]] to execute clinical guidelines. RetroGuide<ref name=eval>{{Cite journal \n| last1 = Huser | first1 = V. \n| last2 = Narus | first2 = S. P. \n| last3 = Rocha | first3 = R. A. \n| doi = 10.1016/j.jbi.2009.06.001 \n| title = Evaluation of a flowchart-based EHR query system: A case study of RetroGuide☆ \n| journal = Journal of Biomedical Informatics \n| volume = 43 \n| issue = 1 \n| pages = 41–50 \n| year = 2010 \n| pmid = 19560553 \n| pmc =2840619 \n}}</ref> and HealthFlow<ref name=hf2010>{{citation|pmc=3079703|title=Implementation of workflow engine technology to deliver basic clinical decision support functionality|journal=BMC Med Res Methodol.|year=2011|volume= 11|page= 43|doi=10.1186/1471-2288-11-43|pmid=21477364|vauthors=Huser V, Rasmussen LV, Oberg R, Starren JB}}</ref>  are examples of such an approach.\n\n== See also ==\n\n*[[Electronic medical record]]\n*[[Clinical practice guideline]]\n*[[Medical algorithm]]\n*[[Arden syntax]]\n*[[Healthcare workflow]]\n*[[Glif]]\n*[[RetroGuide]]\n\n== References ==\n<references/>\n\n== External links ==\n*{{cite journal  |vauthors=Wang D, Peleg M, Tu SW, etal |title=Design and implementation of the GLIF3 guideline execution engine |journal=J Biomed Inform |volume=37 |issue=5 |pages=305–18 |date=October 2004 |pmid=15488745 |doi=10.1016/j.jbi.2004.06.002 |url=http://linkinghub.elsevier.com/retrieve/pii/S1532046404000668}} [http://bmir.stanford.edu/file_asset/index.php/940/BMIR-2004-1008.pdf (PDF)]\n*{{cite journal  |vauthors=Ram P, Berg D, Tu S, etal |title=Executing clinical practice guidelines using the SAGE execution engine |journal=Stud Health Technol Inform |volume=107 |issue=Pt 1 |pages=251–5 |year=2004 |pmid=15360813 }}\n*{{cite journal |vauthors=Tu SW, Campbell J, Musen MA |title=The structure of guideline recommendations: a synthesis |journal=AMIA Annu Symp Proc |volume= |issue= |pages=679–83 |year=2003 |pmid=14728259 |pmc=1480008 }} [http://bmir.stanford.edu/file_asset/index.php/1511/BMIR-2003-0966.pdf (PDF)]\n*{{cite journal |vauthors=Tu SW, Musen MA |title=A flexible approach to guideline modeling |journal=Proc AMIA Symp |volume= |issue= |pages=420–4 |year=1999 |pmid=10566393 |pmc=2232509 }} [http://bmir.stanford.edu/file_asset/index.php/211/BMIR-1999-0789.pdf (PDF)]\n\n[[Category:Health informatics]]\n[[Category:Knowledge representation]]"]
['Philosophy of information', '4522868', '{{Information science}}\nThe \'\'\'philosophy of information\'\'\' (\'\'\'PI\'\'\') is the area of research that studies conceptual issues arising at the intersection of [[computer science]], [[information science]], [[information technology]], and [[philosophy]].\n\nIt includes:\n\n# the critical investigation of the conceptual nature and basic principles of [[information]], including its dynamics, utilisation and sciences\n# the elaboration and application of information-theoretic and computational methodologies to philosophical problems.<ref>Luciano Floridi, [http://www.blackwellpublishing.com/pci/downloads/introduction.pdf "What is the Philosophy of Information?"], \'\'Metaphilosophy\'\', 2002, (33), 1/2.</ref>\n\n==History==\nThe philosophy of information (PI) has evolved from the [[philosophy of artificial intelligence]], [[logic of information]], [[cybernetics]], [[social theory]], [[ethics]] and the study of language and information.\n\n===Logic of information===\nThe [[logic of information]], also known as the \'\'logical theory of information\'\', considers the information content of logical [[sign (semiotics)|sign]]s and expressions along the lines initially developed by [[Charles Sanders Peirce]].\n\n===Cybernetics===\nOne source for the philosophy of information can be found in the technical work of [[Norbert Wiener]], [[Alan Turing]] (though his work has a wholly different origin and theoretical framework), [[William Ross Ashby]], [[Claude Shannon]], [[Warren Weaver]], and many other scientists working on computing and information theory back in the early 1950s. See the main article on [[Cybernetics]].\n\nSome important work on information and communication was done by [[Gregory Bateson]] and his colleagues.\n\n===Study of language and information===\nLater contributions to the field were made by [[Fred Dretske]], [[Jon Barwise]], [[Brian Cantwell Smith]], and others.\n\nThe [[Center for the Study of Language and Information|Center for the Study of Language and Information (CSLI)]] was founded at Stanford University in 1983 by philosophers, computer scientists, linguists, and psychologists, under the direction of [[John Perry (philosopher)|John Perry]] and [[Jon Barwise]].\n\n===P.I.===\nMore recently this field has become known as the philosophy of information. The expression was coined in the 1990s by [[Luciano Floridi]], who has published prolifically in this area with the intention of elaborating a unified and coherent, conceptual frame for the whole subject.{{citation needed|date=April 2015}}\n\n==Definitions of "information"==\n\nThe concept \'\'information\'\' has been defined by several theorists.\n\n===Peirce===\n[[Charles S. Peirce]]\'s theory of information was embedded in his wider theory of symbolic communication he called the \'\'semeiotic\'\', now a major part of [[semiotics]]. For Peirce, information integrates the aspects of [[sign]]s and [[Expression (mathematics)|expressions]] separately covered by the concepts of [[denotation]] and [[extension (semantics)|extension]], on the one hand, and by [[connotation]] and [[comprehension (logic)|comprehension]] on the other.\n\n=== Shannon and Weaver ===\nClaude E. Shannon, for his part, was very cautious: "The word \'information\' has been given different meanings by various writers in the general field of information theory. It is likely that at least a number of these will prove sufficiently useful in certain applications to deserve further study and permanent recognition. It is hardly to be expected that a single concept of information would satisfactorily account for the numerous possible applications of this general field." (Shannon 1993, p.&nbsp;180){{full citation needed|date=April 2015}}. Thus, following Shannon, Weaver supported a tripartite analysis of information in terms of (1) technical problems concerning the quantification of information and dealt with by Shannon\'s theory; (2) semantic problems relating to meaning and truth; and (3) what he called "influential" problems concerning the impact and effectiveness of information on human behaviour, which he thought had to play an equally important role. And these are only two early examples of the problems raised by any analysis of information.\n\nA map of the main senses in which one may speak of information is provided by  [http://plato.stanford.edu/entries/information-semantic/ the Stanford Encyclopedia of Philosophy article]. The previous paragraphs are based on it.\n\n===Bateson===\n[[Gregory Bateson]] defined information as "a difference that makes a difference".<ref>[http://plato.acadiau.ca/courses/educ/reid/papers/PME25-WS4/SEM.html Extract from "Steps to an Ecology of Mind"]</ref> which is based on [[Donald M. MacKay]]: information is a distinction that makes a difference.<ref>The Philosophy of Information.\nLuciano Floridi. Chapter 4. Oxford University Press, USA (March 8, 2011) ASIN: 0199232385 [http://www.amazon.com/Philosophy-Information-Luciano-Floridi/dp/0199232385]</ref>\n\n===Floridi===\nAccording to Luciano Floridi{{citation needed|date=April 2015}}, four kinds of mutually compatible phenomena are commonly referred to as "information": \n*  Information about something (e.g. a train timetable)\n*  Information as something (e.g. DNA, or fingerprints)\n*  Information for something (e.g. algorithms or instructions)\n*  Information in something (e.g. a pattern or a constraint).\n\nThe word "information" is commonly used so metaphorically or so abstractly that the meaning is unclear.\n\n==Philosophical directions==\n\n===Computing and philosophy===\nRecent creative advances and efforts in [[computing]], such as [[semantic web]], [[ontology engineering]], [[knowledge engineering]], and modern [[artificial intelligence]] provide [[philosophy]] with fertile notions, new and evolving subject matters, methodologies, and models for philosophical inquiry.  While [[computer science]] brings new opportunities and challenges to traditional philosophical studies, and changes the ways philosophers understand foundational concepts in philosophy, further major progress in [[computer science]] would only be feasible when philosophy provides sound foundations for areas such as bioinformatics, software engineering, knowledge engineering, and ontologies.\n\nClassical topics in philosophy, namely, [[mind]], [[consciousness]], [[experience]], [[reasoning]], [[knowledge]], [[truth]], [[morality]] and [[creativity]] are rapidly becoming common concerns and foci of investigation in [[computer science]], e.g., in areas such as agent computing, [[software agents]], and intelligent mobile agent technologies.{{citation needed|date=December 2012}}\n\nAccording to Luciano Floridi "<ref>Luciano Floridi, [http://www.philosophyofinformation.net/publications/pdf/oppi.pdf \'\'Open Problems in the Philosophy of Information\'\'] \'\'Metaphilosophy\'\' 35.4, 554-582. Revised version of \'\'The Herbert A. Simon Lecture on Computing and Philosophy\'\' given at Carnegie Mellon University in 2001, with [http://ethics.sandiego.edu/video/CAP/CMU2001/Floridi/index.html RealVideo]</ref> one can think of several ways for applying computational methods towards philosophical matters:\n# Conceptual experiments in silico: As an innovative extension of an ancient tradition of [[thought experiment]], a trend has begun in philosophy to apply computational [[Computer model|modeling]] schemes to questions in [[logic]], [[epistemology]], [[philosophy of science]], [[philosophy of biology]], [[philosophy of mind]], and so on.\n# [[Digital physics#Pancomputationalism or the computational universe theory|Pancomputationalism]]: By this view, computational and informational concepts are considered to be so powerful that given the right level of [[abstraction]], anything in the world could be modeled and represented as a computational system, and any process could be simulated computationally. Then, however, pancomputationalists have the hard task of providing credible answers to the following two questions:\n## how can one avoid blurring all differences among systems?\n## what would it mean for the system under investigation not to be an [[Information system|informational system]] (or a computational system, if computation is the same as information processing)?\n\n===Information and society===\nNumerous philosophers and other thinkers have carried out philosophical studies of the social and cultural aspects of electronically mediated information.\n\n* [[Albert Borgmann]], \'\'Holding onto Reality: The Nature of Information at the Turn of the Millennium\'\' (Chicago University Press, 1999)\n* [[Mark Poster]], \'\'The Mode of Information\'\' (Chicago Press, 1990)\n* [[Luciano Floridi]], "The Informational Nature of Reality", \'\'Fourth International European Conference on Computing and Philosophy\'\' 2006 (Dragvoll Campus, NTNU Norwegian University for Science and Technology, Trondheim, Norway, 22–24 June 2006).\n\n==See also==\n{{col-begin}}\n{{col-break}}\n* [[Barwise prize]]\n* [[Complex system]]\n* [[Digital divide]]\n* [[Digital philosophy]]\n* [[Digital physics]]\n* [[Game theory]]\n* [[Freedom of information]]\n* [[Informatics (academic field)|Informatics]]\n{{col-break}}\n* [[Information]]\n* [[Information art]]\n* [[Information ethics]]\n* [[Information theory]]\n* [[International Association for Computing and Philosophy]]\n* [[Logic of information]]\n{{col-break}}\n* [[Philosophy of artificial intelligence]]\n* [[Philosophy of computer science]]\n* [[Philosophy of technology]]\n* [[Philosophy of thermal and statistical physics]]\n* [[Physical information]]\n* [[Relational quantum mechanics]]\n* [[Social informatics]]\n* [[Statistical mechanics]]\n{{col-end}}\n\n==Notes==\n{{reflist}}\n\n==Further reading==\n*[[Luciano Floridi]], "[http://www.blackwellpublishing.com/pci/downloads/introduction.pdf What is the Philosophy of Information?]" \'\'Metaphilosophy\'\', 33.1/2: 123-145. Reprinted in T.W. Bynum and J.H. Moor (eds.), 2003. \'\'CyberPhilosophy: The Intersection of Philosophy and Computing\'\'. Oxford – New York: Blackwell.\n*-------- (ed.), 2004. \'\'[http://www.blackwellpublishing.com/pci/default.htm The Blackwell Guide to the Philosophy of Computing and Information.]\'\' Oxford - New York: Blackwell.\n*Greco, G.M., Paronitti G., Turilli M., and Floridi L., 2005. \'\'[http://www.wolfson.ox.ac.uk/~floridi/pdf/htdpi.pdf How to Do Philosophy Informationally.]\'\' \'\'Lecture Notes on Artificial Intelligence\'\' 3782, pp.&nbsp;623–634.\n\n== External links ==\n{{Library resources box}}\n*{{cite SEP |url-id=information |title=Information |last=Adriaans |first=Peter |editor-last=Zalta |editor-first=Edward N. ||date=Autumn  2013}}\n*{{cite SEP |url-id=information-semantic |title=Semantic Conceptions of Information |last=Floridi |first=Luciano |editor-last=Zalta |editor-first=Edward N. |date=Spring 2015}}\n*[http://web.comlab.ox.ac.uk/oucl/research/areas/ieg/ IEG site], the Oxford University research group on the philosophy of information.\n*[[Luciano Floridi]], "[https://web.archive.org/web/20060820223325/http://academicfeeds.friwebteknologi.org/index.php?id=28 Where are we in the philosophy of information?]" [[University of Bergen]], [[Norway]]. Podcast dated 21.06.06.\n\n{{Navboxes\n|list=\n{{Philosophy topics}}\n{{philosophy of language}}\n{{philosophy of mind}}\n{{philosophy of science}}\n}}\n\n[[Category:Philosophy by topic|Inf]]\n[[Category:Philosophy of artificial intelligence]]\n[[Category:Knowledge representation]]']
['Simple Knowledge Organization System', '4916685', '\'\'\'Simple Knowledge Organization System\'\'\' (\'\'\'SKOS\'\'\') is a [[W3C recommendation]] designed for representation of [[Thesaurus (information retrieval)|thesauri]], [[classification scheme]]s, [[Taxonomy (general)|taxonomies]], [[Authority control|subject-heading systems]], or any other type of structured [[controlled vocabulary]]. SKOS is part of the [[Semantic Web]] family of standards built upon [[Resource Description Framework|RDF]] and [[RDF Schema|RDFS]], and its main objective is to enable easy publication and use of such vocabularies as [[linked data]].\n\n== History ==\n\n=== DESIRE II project (1997&ndash;2000) ===\n\nThe most direct ancestor to SKOS was the RDF Thesaurus work undertaken in the second phase of the EU DESIRE project <ref name="Desire Project">{{Citation |publication-date=August 7, 2000 |title=Desire: Development of a European Service for Information on Research and Education |publisher=Desire Consortium |url=http://www.desire.org/ |archiveurl=https://web.archive.org/web/20110725230823/http://www.desire.org/ |archivedate=July 25, 2011 }}</ref>{{Citation needed|reason=The Desire Project reference does not appear to directly address the SKOS ancestry statement made here.|date=August 2012}}.  Motivated by the need to improve the user interface and usability of multi-service browsing and searching,<ref name="Desire Deliverable D.36b">{{Citation |title=Desire: Research Deliverables: D3.1 |publisher=Desire Consortium |url=http://www.desire.org/docs/research/deliverables/D3.6/d36b.html |archiveurl=https://web.archive.org/web/20080509135041/http://www.desire.org/html/research/deliverables/D3.6/#d36b |archivedate=May 9, 2008 }}</ref> a basic RDF vocabulary for Thesauri was [http://www.desire.org/results/discovery/rdfthesschema.html produced]. As noted later in the [http://www.w3.org/2001/sw/Europe/plan/workpackages/live/esw-wp-8.html SWAD-Europe workplan], the DESIRE work was adopted and further developed in the SOSIG and LIMBER projects. A version of the DESIRE/SOSIG implementation was described in W3C\'s QL\'98 workshop, motivating early work on RDF rule and query languages: [http://www.w3.org/TandS/QL/QL98/pp/queryservice.html A Query and Inference Service for RDF].<ref>[http://www.w3.org/TandS/QL/QL98/pp/queryservice.html A Query and Inference Service for RDF]</ref>\n\n=== LIMBER (1999&ndash;2001) ===\n\nSKOS built upon the output of the Language Independent Metadata Browsing of European Resources (LIMBER) project funded by the [[European Community]], and part of the [[Information Society Technologies]] programme. In the LIMBER project [[CCLRC]] further developed an [[Resource Description Framework|RDF]]  thesaurus interchange format<ref>[http://journals.tdl.org/jodi/article/viewArticle/34/35 Miller, K. & Matthews, B. (2001). Having the right connections: the LIMBER Project. Journal of Digital Information, 1 (8), 5 February. ]</ref> which was demonstrated on the European Language Social Science Thesaurus ([http://www.cessda.org/results.html?query=elsst ELSST]) at the [[UK Data Archive]] as a multilingual version of the English language Humanities and Social Science Electronic Thesaurus (HASSET) which was planned to be used by the Council of European Social Science Data Archives [http://www.cessda.org/ CESSDA].\n\n=== SWAD-Europe (2002&ndash;2004) ===\n\nSKOS as a distinct initiative began in the SWAD-Europe project, bringing together partners from both DESIRE, SOSIG (ILRT) and LIMBER (CCLRC) who had worked with earlier versions of the schema. It was developed in the Thesaurus Activity Work Package, in the Semantic Web Advanced Development for Europe (SWAD-Europe) project.<ref>[http://www.w3.org/2001/sw/Europe/ SWAD-Europe]</ref> SWAD-Europe was funded by the [[European Community]], and part of the [[Information Society Technologies]] programme. The project was designed to support W3C\'s Semantic Web Activity through research, demonstrators and outreach efforts conducted by the five project partners, [[ERCIM]], the [http://www.ilrt.bris.ac.uk/ ILRT] at [[Bristol University]], [[HP Labs]], [[CCLRC]] and Stilo.<ref>[http://www.stilo.com Stilo Home Page]</ref>\nThe first release of SKOS Core and SKOS Mapping were published at the end of 2003, along with other deliverables on RDF encoding of multilingual thesauri<ref>[http://www.w3c.rl.ac.uk/SWAD/deliverables/8.3.html SWAD-Europe Deliverable 8.3 : RDF Encoding of Multilingual Thesauri]</ref> and thesaurus mapping.<ref>[http://www.w3c.rl.ac.uk/SWAD/deliverables/8.4.html SWAD-Europe Deliverable 8.4 : Inter-Thesaurus Mapping]</ref>\n\n=== Semantic web activity (2004&ndash;2005) ===\n\nFollowing the termination of SWAD-Europe, SKOS effort was supported by the W3C Semantic Web Activity<ref>[http://www.w3.org/2001/sw/ W3C Semantic Web Activity]</ref> in the framework of the Best Practice and Deployment Working Group.<ref>[http://www.w3.org/2004/03/thes-tf/mission W3C Semantic Web Best Practice and Deployment Working Group : Porting Thesauri Task Force]</ref> During this period, focus was put both on consolidation of SKOS Core, and development of practical guidelines for porting and publishing thesauri for the Semantic Web.\n\n=== Development as W3C Recommendation (2006&ndash;2009)===\n\nThe SKOS main published documents — the SKOS Core Guide,<ref>[http://www.w3.org/TR/swbp-skos-core-guide SKOS Core Guide] W3C Working Draft 2 November 2005</ref> the SKOS Core Vocabulary Specification,<ref>[http://www.w3.org/TR/swbp-skos-core-spec SKOS Core Vocabulary Specification] W3C Working Draft 2 November 2005</ref> and the Quick Guide to Publishing a Thesaurus on the Semantic Web<ref>[http://www.w3.org/TR/swbp-thesaurus-pubguide Quick Guide to Publishing a Thesaurus on the Semantic Web] W3C Working Draft 17 May 2005</ref> — were developed through the W3C Working Draft process. Principal editors of SKOS were Alistair Miles,<ref>[http://purl.org/net/aliman Alistair Miles Home Page]</ref> initially Dan Brickley,<ref>[http://danbri.org/ Dan Brickley Home Page]</ref> and Sean Bechhofer.<ref>[http://www.cs.man.ac.uk/~seanb/#me Sean Bechhofer Home Page]</ref>\n\nThe Semantic Web Deployment Working Group,<ref>[http://www.w3.org/2006/07/SWD/ W3C Semantic Web Deployment Working Group]</ref> chartered for two years (May 2006 - April 2008), has put in its charter to push SKOS forward on the [[W3C Recommendation]] track. The roadmap projects SKOS as a Candidate Recommendation by the end of 2007, and as a Proposed Recommendation in the first quarter of 2008. The main issues to solve are determining its precise scope of use, and its articulation with other RDF languages and standards used in libraries (such as [[Dublin Core]]).<ref>[http://isegserv.itd.rl.ac.uk/public/skos/press/dc2006/camera-ready-paper.pdf SKOS: Requirements for Standardization]. The paper by Alistair Miles presented in October 2006 at the International Conference on Dublin Core and Metadata Applications.</ref><ref>[http://purl.org/net/retrieval Retrieval and the Semantic Web, incorporating a Theory of Retrieval Using Structured Vocabularies]. Dissertation on the theory of retrieval using structured vocabularies by Alistair Miles.</ref>\n\n=== Formal release (2009) ===\nOn August 18, 2009, [[W3C]] released the new standard that builds a bridge between the world of knowledge organization systems - including thesauri, classifications, subject headings, taxonomies, and [[folksonomy|folksonomies]] - and the [[linked data]] community, bringing benefits to both. Libraries, museums, newspapers, government portals, enterprises, social networking applications, and other communities that manage large collections of books, historical artifacts, news reports, business glossaries, blog entries, and other items can now use SKOS<ref>[http://www.w3.org/TR/2009/REC-skos-reference-20090818/ Simple Knowledge Organization System (SKOS)]</ref> to leverage the power of linked data.\n\n=== Historical view of components ===\n\nSKOS was originally designed as a modular and extensible family of languages, organized as SKOS Core, SKOS Mapping, and SKOS Extensions, and a Metamodel. The entire specification is now complete within the namespace [http://www.w3.org/2004/02/skos/core# http://www.w3.org/2004/02/skos/core#].\n\n== Overview ==\n\nIn addition to the reference itself, the [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818/ SKOS Primer] (a W3C Working Group Note) summarizes the Simple Knowledge Organization System.\n\nThe SKOS<ref>[http://www.w3.org/TR/skos-reference SKOS Reference]</ref> defines the classes and properties sufficient to represent the common features found in a standard thesaurus. It is based on a concept-centric view of the vocabulary, where primitive objects are not terms, but abstract notions represented by terms. Each SKOS concept is defined as an [[web resource|RDF resource]]. Each concept can have RDF properties attached, including:\n* one or more preferred [[index term]]s (at most one in each natural language)\n* alternative terms or [[synonym]]s\n* definitions and notes, with specification of their language\n\nConcepts can be organized in [[hierarchy|hierarchies]] using broader-narrower relationships, or linked by non-hierarchical (associative) relationships.\nConcepts can be gathered in concept schemes, to provide consistent and structured sets of concepts, representing whole or part of a controlled vocabulary.\n\n=== Element categories ===\n\nThe principal element categories of SKOS are concepts, labels, notations, semantic relations, mapping properties, and collections. The associated concepts are listed in the table below.\n\n{| border="1" class="wikitable"\n|+ SKOS Vocabulary Organized by Theme\n! Concepts\n! Labels & Notation\n! Documentation\n! Semantic Relations\n! Mapping Properties\n! Collections\n|-\n| Concept || prefLabel  || note || broader || broadMatch || Collection \n|-\n| ConceptScheme || altLabel  || changeNote || narrower || narrowMatch || orderedCollection \n|-\n| inScheme || hiddenLabel || definition || related || relatedMatch || member\n|-\n| hasTopConcept || notation  || editorialNote || broaderTransitive || closeMatch || memberList\n|-\n| topConceptOf ||   || example || narrowerTransitive || exactMatch ||  \n|-\n|   ||   || historyNote || semanticRelation || mappingRelation || \n|-\n|   ||   || scopeNote ||  ||  || \n|-\n\n|}\n\n=== Concepts ===\n\nThe SKOS vocabulary is based on concepts. Concepts are the units of thought—ideas, meanings, or objects and events (instances or categories)—which underlie many knowledge organization systems. As such, concepts exist in the mind as abstract entities which are independent of the terms used to label them. In SKOS, a <code>Concept</code> (based on the OWL <code>Class</code>) is used to represent items in a knowledge organization system (terms, ideas, meanings, etc.) or such a system\'s conceptual or organizational structure.\n\nA <code>ConceptScheme</code> is analogous to a vocabulary, thesaurus, or other way of organizing concepts. SKOS does not constrain a concept to be within a particular scheme, nor does it provide any way to declare a complete scheme—there is no way to say the scheme consists only of certain members. A topConcept is (one of) the upper concept(s) in a hierarchical scheme.\n\n=== Labels and notations ===\n\nEach SKOS <code>label</code> is a string of [[Unicode]] characters, optionally with language tags, that are associated with a concept. The <code>prefLabel</code> is the preferred human-readable string (maximum one per language tag), while <code>altLabel</code> can be used for alternative strings, and <code>hiddenLabel</code> can be used for strings that are useful to associate, but not meant for humans to read.\n\nA SKOS <code>notation</code> is similar to a label, but the literal string has a datatype, like integer, float, or date; the datatype can even be made up (see [http://www.w3.org/TR/skos-reference/#L2613 6.5.1 Notations, Typed Literals and Datatypes] in the SKOS Reference). The notation is useful for classification codes and other strings not recognizable as words.\n\n=== Documentation ===\n\nThe Documentation or Note properties provide basic information about SKOS concepts. All the concepts are considered a type of <code>skos:note</code>; they just provide more specific kinds of information. The property <code>definition</code>, for example, should contain a full description of the subject resource.  More specific note types can be defined in a SKOS extension, if desired. A query for <code>&lt;A&gt; skos:note ?</code> will obtain all the notes about &lt;A&gt;, including definitions, examples, and scope, history and change, and editorial documentation.\n\nAny of these SKOS Documentation properties can refer to several object types: a literal (e.g., a string); a resource node that has its own properties; or a reference to another document, for example using a URI. This enables the documentation to have its own [[metadata]], like creator and creation date.\n\nSpecific guidance on SKOS documentation properties can be found in the SKOS Primer [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818/#secdocumentation Documentary Notes].\n\n=== Semantic relations ===\n\nSKOS semantic relations are intended to provide ways to declare relationships between concepts within a concept scheme. While there are no restrictions precluding their use with two concepts from separate schemes, this is discouraged because it is likely to overstate what can be known about the two schemes, and perhaps link them inappropriately.\n\nThe property <code>related</code> simply makes an association relationship between two concepts; no hierarchy or generality relation is implied. The properties <code>broader</code> and <code>narrower</code> are used to assert a direct hierarchical link between two concepts. The meaning may be unexpected; the relation <code>&lt;A&gt; broader &lt;B&gt;</code> means that A has a broader concept called B—hence that B is broader than A. Narrower follows in the same pattern.\n\nWhile the casual reader might expect broader and narrower to be transitive properties, SKOS does not declare them as such. Rather, the properties <code>broaderTransitive</code> and <code>narrowerTransitive</code> are defined as transitive super-properties of broader and narrower. These super-properties are (by convention) not used in declarative SKOS statements. Instead, when a broader or narrower relation is used in a triple, the corresponding transitive super-property also holds; and transitive relations can be inferred (and queried) using these super-properties.\n\n=== Mapping ===\n\nSKOS mapping properties are intended to express matching (exact or fuzzy) of concepts from one concept scheme to another, and by convention are used only to connect concepts from different schemes. The concepts <code>relatedMatch</code>, <code>broadMatch</code>, and <code>narrowMatch</code> are a convenience, with the same meaning as the semantic properties <code>related</code>, <code>broader</code>, and <code>narrower</code>. (See previous section regarding the meanings of broader and narrower.)\n\nThe property relatedMatch makes a simple associative relationship between two concepts. When concepts are so closely related that they can generally be used interchangeably, <code>exactMatch</code> is the appropriate property (exactMatch relations are transitive, unlike any of the other Match relations). The <code>closeMatch</code> property that indicates concepts that only sometimes can be used interchangeably, and so it is not a transitive property.\n\n=== Concept collections ===\n\nThe concept collections (<code>Collection</code>, <code>orderedCollection</code>) are labeled and/or ordered (<code>orderedCollection</code>) groups of SKOS concepts. Collections can be nested, and can have defined URIs or not (which is known as a blank node). Neither a SKOS <code>Concept</code> nor a <code>ConceptScheme</code> may be a Collection, nor vice versa; and SKOS semantic relations can only be used with a Concept (not a Collection). The items in a Collection can not be connected to other SKOS Concepts through the Collection node; individual relations must be defined to each Concept in the Collection.\n\n== Community and participation ==\n\nAll development work is carried out via the mailing list which is a completely open and publicly archived<ref>[http://lists.w3.org/Archives/Public/public-esw-thes/ public-esw-thes@w3.org online archive]. Archives of mailing list used for SKOS development.</ref> mailing list devoted to discussion of issues relating to knowledge organisation systems, information retrieval and the Semantic Web. Anyone may participate informally in the development of SKOS by joining the discussions on public-esw-thes@w3.org - informal participation is warmly welcomed. Anyone who works for a [http://www.w3.org/Consortium/join W3C member] organisation may formally participate in the development process by joining the [http://www.w3.org/2006/07/SWD/ Semantic Web Deployment Working Group] - this entitles individuals to edit specifications and to vote on publication decisions.\n\n== Applications ==\n\n*Some important vocabularies have been migrated into SKOS format and are available in the public domain, including [[EuroVoc]], [[AGROVOC]] and [[GEMET]]. [[Library of Congress Subject Headings]] (LCSH) also support the SKOS format.<ref>[http://id.loc.gov/authorities/about.html About the Library of Congress Authorities]</ref>\n*SKOS has been used as the language for the thesauri used in the [[SWED Environmental Directory]]<ref>[http://www.swed.org.uk/swed Semantic Web Environmental Directory]</ref> developed in the SWAD-Europe project framework.\n*A way to convert thesauri to SKOS,<ref>[http://thesauri.cs.vu.nl/eswc06/ A Method to Convert Thesauri to SKOS]</ref> with examples including the [[Medical Subject Headings|MeSH]] thesaurus, has been outlined by the [[Vrije Universiteit Amsterdam]].\n*Subject classification using [[Darwin Information Typing Architecture|DITA]] and SKOS has been developed by [[IBM]].<ref>[http://www-128.ibm.com/developerworks/xml/library/x-dita10/ Subject classification using [[Darwin Information Typing Architecture|DITA]] and SKOS] by IBM developerWorks.</ref>\n*SKOS is used to represent geographical feature types in the [[GeoNames]] ontology.\n\n== Tools ==\n* [https://github.com/eScienceCenter/ThesauRex ThesauRex] is an open-source, web-based SKOS editor. It is limited to broader/narrower relations among concepts and offers tree-based interaction and with thesauri and drag&drop creation of new thesauri based on a master thesaurus.\n* Mondeca\'s [http://www.mondeca.com/Products/ITM Intelligent Topic Manager] (ITM) is a full-featured SKOS-compliant solution for managing taxonomies, thesauri, and other controlled vocabularies.\n*[http://pactols.frantiq.fr/opentheso/ Opentheso] is an open source web-based thesaurus management system compliant with ISO 25964:2011 and ISO 25964-2:2012 standards (Information and Documentation. Thesauri and Interoperability with other vocabularies). It offers SKOS and csv exports and imports, REST and SOAP web services and manages persistent identifiers (ARK). It has been developed at the French National Center for Scientific Research since 2007. It is currently used by the French archaeological libraries network [http://www.frantiq.fr Frantiq] and by research teams  and by the Hospices Civils de Lyon as a collaborative thesaurus management tool. It can be dowloaded on [https://github.com/frantiq/opentheso github]. \n* [http://openskos.org OpenSKOS] is a web service-based approach to publication, management and use of vocabulary data that can be mapped to SKOS. Its source code is available on [https://github.com/CatchPlus/OpenSKOS GitHub]. It includes [[CRUD]] like [[RESTful]] operations on SKOS concepts and a web-based editor for searching and editing concepts. It was developed by [http://picturae.com Picturae] and funded by the Dutch heritage fond [http://www.catchplus.nl/ CATCHPlus].\n* TemaTres Vocabulary Server<ref>[http://www.vocabularyserver.com/ TemaTres] is a Web tool to manage formal and linguistic representations of knowledge.</ref> is an open source web-based vocabulary server for managing controlled vocabularies, taxonomies and thesauruses. [http://sourceforge.net/projects/tematres Tematres] provides complete export of vocabularies into SKOS-core in addition to Zthes, TopicMaps, MADS, Dublin Core,VDEX, BS 8723, SiteMap, SQL and text.\n* ThManager<ref>[http://thmanager.sourceforge.net/ ThManager] an Open Source Tool for creating and visualizing SKOS RDF vocabularies.</ref> is a [[Java (programming language)|Java]] [[Open-source software|open-source]] application for creating and visualizing SKOS vocabularies.\n* The W3C provides an experimental on-line validation service.<ref>[http://www.w3.org/2004/02/skos/core/validation SKOS Core Validation Service]</ref>\n* SKOS files can also be imported and edited in RDF-OWL editors such as [[Protege (software)|Protégé]] and [[SWOOP]] developed by Maryland Information and Network Dynamics Lab Semantic Web Agents Project [[Mindswap]].<ref>[http://www.mindswap.org/2004/SWOOP/ SWOOP] A Hypermedia-based Featherweight OWL Ontology Editor, developed by [[Mindswap]] - Maryland Information and Network Dynamics Lab Semantic Web Agents Project</ref>\n* SKOS synonyms can be transformed from [[WordNet]] RDF format using an [[XSLT]] style sheet; see [http://www.w3.org/TR/wordnet-rdf W3C RDF]\n* PoolParty<ref>[http://www.poolparty.biz/ PoolParty] is a thesaurus management system and a SKOS editor for the Semantic Web.</ref> is a commercial-quality thesaurus management system and a SKOS editor for the Semantic Web including text analysis functionalities and [[Linked Data]] capabilities.\n* qSKOS<ref>[https://github.com/cmader/qSKOS/ qSKOS] is an open-source tool for SKOS vocabulary quality assessment.</ref> is an open-source tool for performing quality assessment of SKOS vocabularies by checking against a quality issue catalog.\n* SKOSEd<ref>[http://code.google.com/p/skoseditor/ SKOSEd] SKOS plugin for Protege 4</ref> is an open source plug-in for the Protégé 4<ref>[http://www.co-ode.org/downloads/protege-x/ Protégé 4] Protégé 4 OWL editor</ref> [[Web Ontology Language|OWL]] ontology editor that supports authoring SKOS vocabularies. SKOSEd has an accompanying SKOS API<ref>[http://skosapi.sourceforge.net/ SKOS Java API] Java API for SKOS</ref> written in Java that can be used to build SKOS-based applications.\n* Model Futures SKOS Exporter<ref>[http://www.modelfutures.com/software Model Futures Excel SKOS Exporter]</ref> for [[Microsoft Excel]] allows simple vocabularies to be developed as indented Excel spreadsheets and exported as SKOS RDF. BETA version.\n* Lexaurus<ref>[http://www.vocman.com/ Lexaurus] is an enterprise thesaurus management system and multi-format editor.</ref> is an enterprise thesaurus management system and multi-format editor. Its extensive API includes full revision management. SKOS is one of its many supported formats.\n* TopBraid Enterprise Vocabulary Net (EVN) <ref>[http://www.topquadrant.com/solutions/ent_vocab_net.html TopBraid EVN]</ref> is a web-based solution for simplified development and management of interconnected controlled vocabularies. It supports collaboration on defining and linking enterprise vocabularies, taxonomies, thesauri and ontologies used for information integration, customization and search.\n* [http://www.dataharmony.com/products/thesaurus_master.html Thesaurus Master], for creating, developing, and maintaining taxonomies and thesauri, is part of Access Innovations\' [http://www.dataharmony.com/ Data Harmony] knowledge management software line. It offers SKOS-compliant export.\n* [http://www.cognitum.eu/semantics/FluentEditor/ Fluent Editor 2014] - an ontology editor which allows to work and edit directly OWL annotations and SKOS. Annotations will processed also for referenced ontologies as well as imported/exported to OWL/RDF and can be processed on the server.\n* [https://trial.smartlogic.com/S4Trials/ Smartlogic Semaphore Ontology Editor] - a SKOS and SKOS-XL based ontology editor which allows creating models based strictly on the SKOS standards.\n\n== Data ==\nThere are publicly available SKOS data sources.\n* SKOS Datasets wiki<ref>[http://www.w3.org/2001/sw/wiki/SKOS/Datasets SKOS/Datasets]</ref> The W3C recommends using this list of publicly available SKOS data sources. Most data found in this wiki can be used for commercial and research applications.\n\n== Relationships with other standards ==\n\n=== Metamodel ===\nThe SKOS metamodel is broadly compatible with the data model of [[ISO 25964-1]] - Thesauri for Information Retrieval. This data model can be viewed and downloaded from the website for [[ISO 25964]].<ref name="niso.org">[http://www.niso.org/schemas/iso25964 \'\'ISO 25964 – the international standard for thesauri and interoperability with other vocabularies\'\']</ref>\n[[File:Skos metamodel.png|thumb|alt=Alt text|Semantic model of the information elements of SKOS]]\n\n=== SKOS and thesaurus standards ===\nSKOS development has involved experts from both RDF and library community, and SKOS intends to allow easy migration of thesauri defined by standards such as [[NISO]] Z39.19 - 2005<ref>[http://www.niso.org/standards/ NISO Standards] Z39.19 - 2005 : Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies</ref> or [[ISO 25964]].<ref name="niso.org"/>\n\n=== SKOS and other semantic web standards ===\nSKOS is intended to provide a way to make a legacy of concept schemes available to Semantic Web applications, simpler than the more complex ontology language, [[Web Ontology Language|OWL]]. OWL is intended to express complex conceptual structures, which can be used to generate rich metadata and support inference tools. However, constructing useful web ontologies is demanding in terms of expertise, effort, and cost. In many cases, this type of effort might be superfluous or unsuited to requirements, and SKOS might be a better choice. The extensibility of RDF makes possible further incorporation or extension of SKOS vocabularies into more complex vocabularies, including OWL ontologies.\n\n== See also ==\n* [[Glossary]]\n* [[Knowledge representation]]\n* [[Metadata registry]]\n\n== References ==\n{{Reflist|2}}\n\n==External links==\n* [http://www.w3.org/TR/skos-reference/ SKOS Simple Knowledge Organization System Reference]\n* [http://www.w3.org/2004/02/skos/ W3C SKOS Home Page]\n* [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818 W3C Simple Knowledge Organization System Primer]\n* [http://www.idealliance.org/proceedings/xtech05/papers/03-04-01/ Presentation of SKOS at XTech 2005 Conference]\n* [http://www.w3.org/News/2009#item35 W3C Invites Implementations of SKOS (Simple Knowledge Organization System) Reference; Primer Also Published]\n* [http://demo.semantic-web.at:8080/SkosServices/index SKOS Validator and Zthes Converter]\n\n{{Semantic Web}}\n{{W3C standards}}\n\n[[Category:Knowledge representation]]\n[[Category:Semantic Web]]\n[[Category:School of Computer Science, University of Manchester]]']
['Attribute-value system', '7512482', 'An \'\'\'attribute-value system\'\'\' is a basic [[knowledge representation]] framework comprising a table with columns designating "attributes" (also known as "properties", "predicates," "features," "dimensions," "characteristics", "[[Field (computer science)|fields]]", "headers" or "independent variables" depending on the context) and "[[Row (database)|rows]]" designating "objects" (also known as "entities," "instances," "exemplars," "elements", "[[Record (computer science)|records]]" or "dependent variables"). Each table cell therefore designates the value (also known as "state") of a particular attribute of a particular object.\n\n== Example of attribute-value system==\nBelow is a sample attribute-value system. It represents 10 objects (rows) and five features (columns). In this example, the table contains only integer values. In general, an attribute-value system may contain any kind of data, numeric or otherwise. An attribute-value system is distinguished from a simple "feature list" representation in that each feature in an attribute-value system may possess a range of values (e.g., feature <math>P_{1}</math> below, which has domain of {0,1,2}), rather than simply being \'\'present\'\' or \'\'absent\'\' {{Harv|Barsalou|Hale|1993}}.\n\n:{| class="wikitable" style="text-align:center; width:30%" border="1"\n|+ Sample Attribute-Value System\n! Object !! <math>P_{1}</math> !! <math>P_{2}</math> !! <math>P_{3}</math> !! <math>P_{4}</math> !! <math>P_{5}</math>\n|-\n! <math>O_{1}</math>\n| 1 || 2 || 0 || 1 || 1\n|-\n! <math>O_{2}</math>\n| 1 || 2 || 0 || 1 || 1\n|-\n! <math>O_{3}</math>\n| 2 || 0 || 0 || 1 || 0\n|-\n! <math>O_{4}</math>\n| 0 || 0 || 1 || 2 || 1\n|-\n! <math>O_{5}</math>\n| 2 || 1 || 0 || 2 || 1\n|-\n! <math>O_{6}</math>\n| 0 || 0 || 1 || 2 || 2\n|-\n! <math>O_{7}</math>\n| 2 || 0 || 0 || 1 || 0\n|-\n! <math>O_{8}</math>\n| 0 || 1 || 2 || 2 || 1\n|-\n! <math>O_{9}</math>\n| 2 || 1 || 0 || 2 || 2\n|-\n! <math>O_{10}</math>\n| 2 || 0 || 0 || 1 || 0\n|}\n\n== Other terms used for "attribute-value system"==\nAttribute-value systems are pervasive throughout many different literatures, and have been discussed under many different names:\n*\'\'Flat data\'\'\n*\'\'[[Spreadsheet]]\'\'\n*\'\'Attribute-value system\'\' (Ziarko & Shan 1996)\n*\'\'Information system\'\' ([[Zdzislaw Pawlak|Pawlak]] 1981)\n*\'\'Classification system\'\' (Ziarko 1998)\n*\'\'Knowledge representation system\'\' (Wong & Ziarko 1986)\n*\'\'Information table\'\' (Yao & Yao 2002)\n*\'\'Object-predicate table\'\' (Watanabe 1985)\n*\'\'Aristotelian table\'\' (Watanabe 1985)\n*\'\'Simple frames\'\' {{Harv|Barsalou|Hale|1993}}\n*\'\'[[First normal form]]\'\' database\n\n==See also==\n*[[Bayes networks]]\n*[[Entity–attribute–value model]]\n*[[Joint distribution]]\n*[[Knowledge representation]]\n*[[wikibooks:Optimal Classification|Optimal classification]] (in Wikibooks)\n*[[Rough set]]\n*[[Triplestore]]\n\n== References ==\n* {{Cite book\n | last1=Barsalou\n | given1=Lawrence W.\n | surname2=Hale\n | given2=Christopher R.\n | year= 1993\n | chapter=Components of conceptual representation: From feature lists to recursive frames\n | editor=Iven Van Mechelen |editor2=James Hampton |editor3=Ryszard S. Michalski |editor4=Peter Theuns\n | title=Categories and Concepts: Theoretical Views and Inductive Data Analysis\n | pages=97–144\n | edition=\n | publisher=Academic Press\n | place=London\n | url=\n | accessdate=\n | ref=harv\n | postscript=<!--None-->}}\n*{{cite book\n  | last = Pawlak\n  | first = Zdzisław\n  | authorlink = Zdzislaw Pawlak\n  | title = Rough sets: Theoretical Aspects of Reasoning about Data\n  | publisher = Kluwer\n  | year = 1991\n  | location = Dordrecht}}\n*{{cite journal\n  | last = Ziarko\n  | first = Wojciech \n  | last2 = Shan\n  | first2 = Ning\n  | title = A method for computing all maximally general rules in attribute-value systems\n  | journal = Computational Intelligence\n  | volume = 12\n  | issue = 2\n  | pages = 223–234\n  | year = 1996\n  | doi = 10.1111/j.1467-8640.1996.tb00260.x\n  | ref = harv}}\n*{{cite journal\n  | last = Pawlak\n  | first = Zdzisław\n  | last2 = Shan\n  | first2 = Ning\n  | title = Information systems: Theoretical foundations\n  | journal = Information Systems\n  | volume = 6\n  | issue = 3\n  | pages = 205–218\n  | year = 1981\n  | doi = 10.1016/0306-4379(81)90023-5\n  | ref = harv}}\n*{{cite journal\n  | last = Wong\n  | first = S. K. M.\n  | last2 = Ziarko\n  | first2 = Wojciech\n  | last3 = Ye\n  | first3 = R. Li\n  | title = Comparison of rough-set and statistical methods in inductive learning\n  | journal = International Journal of Man-Machine Studies\n  | volume = 24\n  | pages = 53–72\n  | year = 1986\n  | ref = harv}}\n*{{cite conference\n  | first = Yao\n  | last = J. T.\n  |author2=Yao, Y. Y.\n  | title = Induction of classification rules by granular computing\n  | booktitle = Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing (TSCTC\'02)\n  | pages = 331–338\n  | publisher = Springer-Verlag\n  | year = 2002\n  | location = London, UK}}\n*{{cite book\n  | last = Watanabe\n  | first = Satosi\n  | title = Pattern Recognition: Human and Mechanical\n  | publisher = John Wiley & Sons\n  | year = 1985\n  | location = New York}}\n*{{cite conference\n  | first = Wojciech\n  | last = Ziarko\n  | title = Rough sets as a methodology for data mining\n  | booktitle = Rough Sets in Knowledge Discovery 1: Methodology and Applications\n  | pages = 554–576\n  | editor    = Polkowski, Lech |editor2=Skowron, Andrzej\n  | publisher = Physica-Verlag\n  | year = 1998\n  | location = Heidelberg}}\n\n[[Category:Knowledge representation]]\n[[Category:Specific models]]']
['Minimum Information Standards', '7819348', '{{Multiple issues|\n{{confusing|date=January 2010}}\n{{essay-like|date=January 2010}}\n{{lead rewrite|date=January 2010}}\n{{external links|date=September 2012}}\n{{more footnotes|date=January 2010}}\n{{expert-subject|Computational Biology|date=January 2010}}\n}}\n\nThe \'\'\'minimum information standard\'\'\' is a set of guidelines for [[data reporting|reporting]]  [[data]] derived by relevant methods in biosciences. If followed, it ensures that the data can be easily verified, analysed and clearly interpreted by the wider scientific community. Keeping with these recommendations also facilitates the foundation of structuralized databases, public repositories and development of data analysis tools.<ref name="MIFlowCyt: The minimum information about a flow cytometry experiment">{{cite journal|last=Lee|first=Jamie A. |author2=Spidlen, Josef |author3=Boyce, Keith |author4=Cai, Jennifer |author5=Crosbie, Nicholas |author6=Dalphin, Mark |author7=Furlong, Jeff |author8=Gasparetto, Maura |author9=Goldberg, Michael |author10=Goralczyk, Elizabeth M. |author11=Hyun, Bill |author12=Jansen, Kirstin |author13=Kollmann, Tobias |author14=Kong, Megan |author15=Leif, Robert |author16=McWeeney, Shannon |author17=Moloshok, Thomas D. |author18=Moore, Wayne |author19=Nolan, Garry |author20=Nolan, John |author21=Nikolich-Zugich, Janko |author22=Parrish, David |author23=Purcell, Barclay |author24=Qian, Yu |author25=Selvaraj, Biruntha |author26=Smith, Clayton |author27=Tchuvatkina, Olga |author28=Wertheimer, Anne |author29=Wilkinson, Peter |author30=Wilson, Christopher |author31=Wood, James |author32=Zigon, Robert |author33=Scheuermann, Richard H. |author34=Brinkman, Ryan R. |title=MIFlowCyt: The minimum information about a flow cytometry experiment|journal=Cytometry Part A|date=1 October 2008|volume=73A|issue=10|pages=926–930|doi=10.1002/cyto.a.20623|pmid=18752282|pmc=2773297}}</ref><ref name="Minimum information about a microarray experiment (MIAME)—toward standards for microarray data">{{cite journal|last=Brazma|first=Alvis |author2=Hingamp, Pascal |author3=Quackenbush, John |author4=Sherlock, Gavin |author5=Spellman, Paul |author6=Stoeckert, Chris |author7=Aach, John |author8=Ansorge, Wilhelm |author9=Ball, Catherine A. |author10=Causton, Helen C. |author11=Gaasterland, Terry |author12=Glenisson, Patrick |author13=Holstege, Frank C.P. |author14=Kim, Irene F. |author15=Markowitz, Victor |author16=Matese, John C. |author17=Parkinson, Helen |author18=Robinson, Alan |author19=Sarkans, Ugis |author20=Schulze-Kremer, Steffen |author21=Stewart, Jason |author22=Taylor, Ronald |author23=Vilo, Jaak |author24=Vingron, Martin |title=Minimum information about a microarray experiment (MIAME)—toward standards for microarray data|journal=Nature Genetics|date=30 November 2001|volume=29|issue=4|pages=365–371|doi=10.1038/ng1201-365|pmid=11726920}}</ref>\n\nThe individual \'\'\'minimum information standards\'\'\' are brought by the communities of cross-disciplinary specialists focused on the problematic of the specific method used in experimental biology.  The standards then provide specifications what information about the experiments ([[metadata]]) is crucial and important to be reported together with the resultant data to make it comprehensive.<ref name="MIFlowCyt: The minimum information about a flow cytometry experiment"/><ref name="Minimum information about a microarray experiment (MIAME)—toward standards for microarray data"/> The need for this standardization is largely driven by the development of high-throughput experimental methods that provide tremendous amounts of data.  The development of minimum information standards of different methods is since 2008 being harmonized by "Minimum Information about a Biomedical or Biological Investigation" (MIBBI) project.<ref name="Promoting coherent minimum reporting guidelines for biological and biomedical investigations: the MIBBI project">{{cite journal|last=Taylor|first=Chris F|title=Promoting coherent minimum reporting guidelines for biological and biomedical investigations: the MIBBI project|journal=Nature Biotechnology|year=2008|volume=26|pages=889–896|doi=10.1038/nbt.1411}}</ref>\n\n==MI Standards==\n\n===MIAME, gene expression microarray===\nMinimum Information About a Microarray Experiment (MIAME) describes the Minimum Information About a Microarray Experiment that is needed to enable the interpretation of the results of the experiment unambiguously and potentially to reproduce the experiment and is aimed at facilitating the dissemination of data from microarray experiments.\n\nMIAME contains a number of extensions to cover specific biological domains, including MIAME-env, MIAME-nut and MIAME-tox, covering environmental genomics, nutritional genomics and toxogenomics, respectively\n\n===MINI: Minimum Information about a Neuroscience Investigation===\n\n====MINI: Electrophysiology====\n[[Electrophysiology]] is a technology used to study the electrical properties of biological cells and tissues. Electrophysiology typically involves the measurements of voltage change or electric current flow on a wide variety of scales from single ion channel\nproteins to whole tissues. This document is a single module, as part of the Minimum Information about a Neuroscience investigation (MINI) family of reporting guideline\ndocuments, produced by community consultation and continually available for public comment. A MINI module represents the minimum information that should be reported about a dataset to facilitate computational access and analysis to allow a reader to interpret and critically evaluate the processes performed and the conclusions reached, and to support their experimental corroboration. In practice a MINI module comprises a checklist of information that should be provided (for example about the protocols employed) when\na data set is described for publication. The full specification of the MINI module can be found here.<ref>Gibson, Frank, Overton, Paul, Smulders, Tom, Schultz, Simon, Eglen, Stephen, Ingram, Colin, Panzeri, Stefano, Bream, Phil, Sernagor, Evelyne, Cunningham, Mark, Adams, Christopher, Echtermeyer, Christoph, Simonotto, Jennifer, Kaiser, Marcus, Swan, Daniel, Fletcher, Marty, and Lord, Phillip. Minimum Information about a Neuroscience Investigation (MINI) Electrophysiology. Available from Nature Precedings <http://hdl.handle.net/10101/npre.2008.1720.1> (2008)</ref>\n\n===MIARE, RNAi experiment===\nMinimum Information About an RNAi Experiment (MIARE) is a [[data reporting]] guideline which describes the minimum information that should be reported about an RNAi experiment to enable the unambiguous interpretation and reproduction of the results.\n\n===MIACA, cell based assay===\nAdvances in genomics and functional genomics have enabled large-scale analyses of gene and protein function by means of high-throughput cell biological analyses. Thereby, cells in culture can be perturbed in vitro and the induced effects recorded and analyzed. Perturbations can be triggered in several ways, for instance with molecules (siRNAs, expression constructs, small chemical compounds, ligands for receptors, etc.), through environmental stresses (such as temperature shift, serum starvation, oxygen deprivation, etc.), or combinations thereof. The cellular responses to such perturbations are analyzed in order to identify molecular events in the biological processes addressed and understand biological principles.\nWe propose the Minimum Information About a Cellular Assay (MIACA) for reporting a cellular assay, and CA-OM, the modular cellular assay object model, to facilitate exchange of data and accompanying information, and to compare and integrate data that originate from different, albeit complementary approaches, and to elucidate higher order principles. [http://sourceforge.net/project/showfiles.php?group_id=158121 Documents describing MIACA] are available and provide further information as well as the checklist of terms that should be reported.\n\n===MIAPE, proteomic experiments===\nThe Minimum Information About a Proteomic Experiment documents describe information which should be given along with a proteomic experiment. The parent document describes the processes and principles underpinning the development of a series of domain specific documents which now cover all aspects of a MS-based proteomics workflow.\n{{Details|Minimum Information About a Proteomics Experiment }}\n\n===MIMIx, molecular interactions===\nThis document has been developed and maintained by the Molecular Interaction worktrack of the HUPO-PSI (www.psidev.info) and describes the Minimum Information about a Molecular Interaction experiment.\n\n===MIAPAR, protein affinity reagents===\nThe Minimum Information About a Protein Affinity Reagent has been developed and maintained by the Molecular Interaction worktrack of the HUPO-PSI (www.psidev.info)in conjunction with the HUPO Antibody Initiative and a European consortium of binder producers and seeks to encourage users to improve their description of binding reagents, such as antibodies, used in the process of protein identification.\n\n===MIABE, bioactive entities===\nThe Minimum Information About a Bioactive Entity was produced by representatives from both large pharma and academia who are looking to improve the description of usually small molecules which bind to, and potentially modulate the activity of, specific targets in a living organism. This document encompasses drug-like molecules as well as hebicides, pesticides and food additives. It is primarily maintained through the EMBL-EBI Industry program (www.ebi.ac.uk/industry).\n\n===MIGS/MIMS, genome/metagenome sequences===\nThis specification is being developed by the [[Genomic Standards Consortium]]\n\n===MIFlowCyt, flow cytometry===\n\n====Minimum Information about a Flow Cytometry Experiment====\nThe fundamental tenet of any scientific research is that the published results of any study have to be open to independent validation or refutation. The Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) establishes the criteria to record information about the experimental overview, samples, instrumentation and data analysis. It promotes consistent annotation of clinical, biological and technical issues surrounding a flow cytometry experiment by specifying the requirements for data content and by providing a structured framework for capturing information.\n\nMore information can be found at:\n* The Flow Informatics and Computational Cytometry Socienty (FICCS) [http://wiki.ficcs.org/ficcs/MIFlowCyt MIFlowCyt wiki] page.\n* The Bioinformatics Standards for Flow Cytometry [http://flowcyt.sourceforge.net/miflowcyt/ MIFlowCyt web] page.\n\n===MISFISHIE, In Situ Hybridization and Immunohistochemistry Experiments===\n{{Emptysection|date=February 2013}}\n\n===MIAPA, Phylogenetic Analysis===\nCriteria for Minimum Information About a Phylogenetic Analysis were described in 2006. <ref> {{Cite journal | doi = 10.1089/omi.2006.10.231| title = Taking the First Steps towards a Standard for Reporting on Phylogenies: Minimum Information about a Phylogenetic Analysis (MIAPA)| journal = OMICS: A Journal of Integrative Biology| volume = 10| issue = 2| pages = 231| year = 2006| last1 = Leebens-Mack | first1 = J. | last2 = Vision | first2 = T. | last3 = Brenner | first3 = E. | last4 = Bowers | first4 = J. E. | last5 = Cannon | first5 = S. | last6 = Clement | first6 = M. J. | last7 = Cunningham | first7 = C. W. | last8 = Depamphilis | first8 = C. | last9 = Desalle | first9 = R. | last10 = Doyle | first10 = J. J. | last11 = Eisen | first11 = J. A. | last12 = Gu | first12 = X. | last13 = Harshman | first13 = J. | last14 = Jansen | first14 = R. K. | last15 = Kellogg | first15 = E. A. | last16 = Koonin | first16 = E. V. | last17 = Mishler | first17 = B. D. | last18 = Philippe | first18 = H. | last19 = Pires | first19 = J. C. | last20 = Qiu | first20 = Y. L. | last21 = Rhee | first21 = S. Y. | last22 = Sjölander | first22 = K. | last23 = Soltis | first23 = D. E. | last24 = Soltis | first24 = P. S. | authorlink24 = Pamela S. Soltis| last25 = Stevenson | first25 = D. W. | last26 = Wall | first26 = K. | last27 = Warnow | first27 = T. | last28 = Zmasek | first28 = C. }} </ref>\n\n===MIAO, ORF===\n{{Emptysection|date=February 2013}}\n\n===MIAMET, METabolomics experiment===\n{{Emptysection|date=February 2013}}\n\n===MIAFGE, Functional Genomics Experiment===\n{{Emptysection|date=February 2013}}\n\n===MIRIAM, Minimum Information Required in the Annotation of Models===\nThe Minimal Information Required In the Annotation of Models ([[MIRIAM]]), is a set of rules for the curation and annotation of quantitative models of biological systems.\n\n===MIASE, Minimum Information About a Simulation Experiment===\nThe Minimum Information About a Simulation Experiment ([[MIASE]]) is an effort to standardize the description of simulation experiments in the field of systems biology.\n\n===CIMR, Core Information for Metabolomics Reporting===\n\n==External links==\n* [http://mibbi.sourceforge.net/ MIBBI (Minimum Information for Biological and Biomedical Investigations)] A ‘one-stop shop’ for exploring the range of extant projects, foster collaborative development and ultimately promote gradual integration.\n* [http://www.biosharing.org BioSharing catalogue]\n\n==References==\n{{reflist}}\n\n[[Category:Bioinformatics]]\n[[Category:Knowledge representation]]']
['Microformat', '2346998', '{{About||the photographic miniaturization of documents|Microform|details of microformats used on Wikipedia|:Wikipedia:Microformats}}\nA \'\'\'microformat\'\'\' (sometimes abbreviated \'\'\'μF\'\'\') is a [[World Wide Web]]-based approach to semantic markup which uses [[HTML]]/[[XHTML]] tags supported for other purposes to convey additional [[metadata]]<ref>{{cite web |url=http://microformats.org/wiki/existing-classes |work=Microformats.org |title=Class Names Across All Microformats |date=2007-09-23 |accessdate=2008-09-06}}</ref> and other attributes in web pages and other contexts that support (X)HTML, such as [[RSS]]. This approach allows [[software agent|software]] to process information intended for end-users (such as [[Address book|contact information]], [[Geographic coordinate system|geographic coordinates]], calendar events, and similar information) automatically.\n\nAlthough the content of web pages has been capable of some "automated processing" since the inception of the web, such processing is difficult because the [[markup language|markup tags]] used to display information on the web do not describe what the information means.<ref name="Wharton000">{{cite web |title=What’s the Next Big Thing on the Web? It May Be a Small, Simple Thing -- Microformats|work=Knowledge@Wharton |publisher=[[Wharton School of the University of Pennsylvania]] |date=2005-07-27 |url=http://knowledge.wharton.upenn.edu/index.cfm?fa=printArticle&ID=1247}}</ref> Microformats can bridge this gap by attaching [[semantics]], and thereby obviate other, more complicated, methods of automated processing, such as [[natural language processing]] or [[screen scraping]]. The use, adoption and processing of microformats enables data items to be indexed, searched for, saved or cross-referenced, so that information can be reused or combined.<ref name="Wharton000"/>\n\n{{As of | 2013}} microformats allow the encoding and extraction of event details, contact information, social relationships and similar information. Established microformats such as [[hCard]] are published on the web more than alternatives like schema ([[Microdata (HTML)|microdata]]) and [[RDFa]].<ref>{{cite web|url=http://webdatacommons.org/structureddata/index.html#toc2\n |date=2013 |work= section 3.1, "Extraction Results from the November 2013 Common Crawl Corpus" |accessdate=2015-02-21 |title=Web Data Commons – RDFa, Microdata, and Microformat Data Sets}}</ref>{{failed verification|date=November 2016}}\n\n== Background ==\nMicroformats emerged around 2005<ref>The \'\'microformats\'\' is a community-standard maintained by its Wiki, and [http://microformats.org/wiki/index.php?title=Main_Page&dir=prev&action=history the Wiki arrived ~2005].</ref> as part of a grassroots movement to make recognizable data items (such as events, contact details or geographical locations) capable of automated processing by software, as well as directly readable by end-users.<ref name="Wharton000"/><ref>In this context, the definition of "end-user" includes a person reading a web page on a computer screen or mobile device, or an [[assistive technology]] such as a [[screen reader]].</ref> Link-based microformats emerged first. These include vote links that express opinions of the linked page, which search engines can tally into instant polls.<ref name="Khare000">{{cite journal |title=Microformats: The Next (Small) Thing on the Semantic Web? |first=Rohit |last=Khare |journal=[[IEEE Internet Computing]] |volume=10 |issue=1 |pages=68–75 |date=January–February 2006 |publisher=[[IEEE Computer Society]] |url=http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/ic/&toc=comp/mags/ic/2006/01/w1toc.xml&DOI=10.1109/MIC.2006.13 |doi=10.1109/MIC.2006.13 |accessdate=2008-09-06}}\n</ref>\n\n[[CommerceNet]], a nonprofit organization that promotes [[e-commerce]] on the Internet, has helped sponsor and promote the technology and support the microformats community in various ways.<ref name="Khare000"/> CommerceNet also helped co-found the Microformats.org community site.<ref name="Khare000"/>\n\nNeither CommerceNet nor Microformats.org operates as a [[standards body]]. The microformats community functions through an open [[wiki]], a mailing list, and an Internet relay chat ([[Internet Relay Chat|IRC]]) channel.<ref name="Khare000"/> Most of the existing microformats originated at the Microformats.org wiki and the associated mailing list{{citation needed|date=October 2012}} by a process of gathering examples of web-publishing behaviour, then codifying it. Some other microformats (such as [[nofollow|rel=nofollow]] and [[unAPI]]) have been proposed, or developed, elsewhere.\n\n== Technical overview ==\n\nXHTML and HTML standards allow for the embedding and encoding of semantics within the [[HTML element|attributes of markup tags]]. Microformats take advantage of these standards by indicating the presence of metadata using the following attributes:\n\n; <code>class</code>\n: [[Class (computer programming)|Classname]]\n\n; <code>rel</code>\n: relationship, description of the target address in an anchor-element (<code><a href=... rel=...>...</a></code>)\n\n; <code>rev</code>\n: reverse relationship, description of the referenced document (in one case, otherwise deprecated in microformats<ref name="uF-rel-faq">{{cite web |url=http://microformats.org/wiki/rel-faq |title="rel" attribute frequently asked questions |work=Microformats.org |date=2008-08-06 |accessdate=2008-09-06}}</ref>)\n\nFor example, in the text "The birds roosted at <span class="geo"><span class="latitude">52.48</span>, <span class="longitude">-1.89</span></span>" is a pair of numbers which may be understood, from their context, to be a set of [[geographic coordinate system|geographic coordinates]]. With wrapping in [[Span and div|spans]] (or other HTML elements) with specific class names (in this case <code>geo</code>, <code>latitude</code> and <code>longitude</code>, all part of the [[Geo (microformat)|geo microformat]] specification):\n\n <syntaxhighlight lang="xml">The birds roosted at\n   <span class="geo">\n     <span class="latitude">52.48</span>,\n     <span class="longitude">-1.89</span>\n   </span>\n </syntaxhighlight>\n\nsoftware agents can recognize exactly what each value represents and can then perform a variety of tasks such as indexing, locating it on a map and exporting it to a [[GPS]] device.\n\n=== Examples ===\nIn this example, the contact information is presented as follows:\n\n <syntaxhighlight lang="xml">\n <ul>\n   <li>Joe Doe</li>\n   <li>The Example Company</li>\n   <li>604-555-1234</li>\n   <li><a href="http://example.com/">http://example.com/</a></li>\n </ul>\n </syntaxhighlight>\n\nWith hCard microformat markup, that becomes:\n\n <syntaxhighlight lang="xml">\n <ul class="vcard">\n   <li class="fn">Joe Doe</li>\n   <li class="org">The Example Company</li>\n   <li class="tel">604-555-1234</li>\n   <li><a class="url" href="http://example.com/">http://example.com/</a></li>\n </ul>\n </syntaxhighlight>\n\nHere, the formatted name (<code>fn</code>), organisation (<code>org</code>), telephone number (<code>tel</code>) and [[Uniform Resource Locator|web address]] (<code>url</code>) have been identified using specific class names and the whole thing is wrapped in <code>class="vcard"</code>, which indicates that the other classes form an hCard (short for "HTML [[vCard]]") and are not merely coincidentally named. Other, optional, hCard classes also exist. Software, such as browser plug-ins, can now extract the information, and transfer it to other applications, such as an address book.\n\n<div class="noprint"> <!-- ensures that the following "Live" example dies not carry over to printed mirrors -->\n\n<!-- Note "noprint" div started in previous section\n-->=== In-context examples ===\nFor annotated examples of microformats on live pages, see [[HCard#Live example]] and [[Geo (microformat)#Usage]].\n</div>\n\n== Specific microformats ==\nSeveral microformats have been developed to enable semantic markup of particular types of information. However, only hCard and hCalendar have been ratified, the others remaining as drafts:\n\n* [[hAtom]] (superseded by [[h-entry]] and [[h-feed]]) – for marking up [[Atom (standard)|Atom]] feeds from within standard HTML\n* [[hCalendar]] – for events\n* [[hCard]] – for contact information; includes:\n** adr – for postal addresses\n** [[geo (microformat)|geo]] – for geographical coordinates ([[latitude]], [[longitude]])\n* hMedia - for audio/video content<ref>[http://microformats.org/wiki/hmedia hMedia · Microformats Wiki<!-- Bot generated title -->]</ref><ref>[http://sixrevisions.com/web-development/ultimate-guide-to-microformats-reference-and-examples/ Ultimate Guide to Microformats: Reference and Examples<!-- Bot generated title -->]</ref>\n* hAudio – for audio content\n* [[hNews]] -  for news content\n* [[hProduct]] – for products\n* [[hRecipe]] - for recipes and foodstuffs.\n* [[hResume]] – for resumes or [[curriculum vitae|CVs]]\n* [[hReview]] – for reviews\n* rel-[[directory (file systems)|directory]] – for distributed directory creation and inclusion<ref>[http://microformats.org/wiki/rel-directory rel-directory · Microformats Wiki<!-- Bot generated title -->]</ref>\n* rel-enclosure – for multimedia attachments to web pages<ref>[http://microformats.org/wiki/rel-enclosure rel="enclosure" · Microformats Wiki<!-- Bot generated title -->]</ref>\n* rel-license – specification of copyright license<ref>[http://microformats.org/wiki/rel-license rel="license" · Microformats Wiki<!-- Bot generated title -->]</ref>\n* rel-[[nofollow]], an attempt to discourage third-party content spam (e.g. [[spam in blogs]])\n* rel-[[tag (metadata)|tag]] – for decentralized tagging ([[Folksonomy]])<ref>[http://microformats.org/wiki/rel-tag rel="tag" · Microformats Wiki<!-- Bot generated title -->]</ref>\n* [[xFolk]] – for tagged links\n* [[XHTML Friends Network]] (XFN) – for social relationships\n* [[XOXO (microformat)|XOXO]] – for lists and outlines\n\n== Uses ==\nUsing microformats within HTML code provides additional formatting and semantic data that applications can use. For example, applications such as [[web crawler]]s can collect data about on-line resources, or desktop applications such as e-mail clients or scheduling software can compile details. The use of microformats can also facilitate "mash ups" such as exporting all of the geographical locations on a web page into (for example) [[Google Maps]] to visualize them spatially.\n\nSeveral browser extensions, such as [[Operator (extension)|Operator]] for [[Firefox]] and Oomph for [[Internet Explorer]], provide the ability to detect microformats within an HTML document. When hCard or hCalendar are involved, such browser extensions allow microformats to be exported into formats compatible with contact management and calendar utilities, such as [[Microsoft Outlook]]. When dealing with geographical coordinates, they allow the location to be sent to applications such as [[Google Maps]]. [[Yahoo! query language|Yahoo! Query Language]] can be used to extract microformats from web pages.<ref>{{cite web|url=http://developer.yahoo.net/blog/archives/2009/01/wikipedia_w_yql.html|title=Retrieving and displaying data from Wikipedia with YQL|last=Heilman|first=Chris|date=2009-01-19|work=Yahoo Developer Network|publisher=Yahoo|accessdate=2009-01-19}}</ref> On 12 May 2009 [[Google search|Google]] announced that they would be parsing the hCard, hReview and hProduct microformats, and using them to populate search result pages.<ref name="Rich-Snippets">{{cite web|url=http://googlewebmastercentral.blogspot.com/2009/05/introducing-rich-snippets.html|title=Introducing Rich Snippets|last=Goel|first=Kavi|author2=Ramanathan V. Guha |author3=Othar Hansson |date=2009-05-12|work=Google Webmaster Central Blog|publisher=Google|accessdate=2009-05-25}}</ref> They have since extended this to use hCalendar for events<ref name="Google-recipes">{{cite web|url=http://googlewebmastercentral.blogspot.com/2010/04/better-recipes-on-web-introducing.html|title=Better recipes on the web: Introducing recipe rich snippets|last=Gong|first=Jun|author2=Kosuke Suzuki |author3=Yu Watanabe |date=2010-04-13|publisher=Google|accessdate=17 March 2011}}</ref> and hRecipe for cookery recipes.<ref name="Google-recipes" /> Similarly, microformats are also processed by [[Bing (search engine)|Bing]]<ref name="Bing">{{cite web|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/06/02/bing-google-and-yahoo-unite-to-build-the-web-of-objects.aspx|title=Bing Introducing Schema.org: Bing, Google and Yahoo Unite to Build the Web of Objects - Search Blog - Site Blogs - Bing Community|date=2011-06-02|work=[[Bing (search engine)|Bing]]|accessdate=2 June 2011}}</ref> and [[Yahoo!]].<ref name="YSearch">{{cite web|url=http://www.ysearchblog.com/2011/06/02/introducing-schema-org-a-collaboration-on-structured-data|title=Introducing schema.org: A Collaboration on Structured Data|date=2011-06-02|accessdate=2 June 2011}}</ref> Together, these are the world\'s top three search engines.<ref>{{cite web |url=http://gs.statcounter.com/#search_engine-ww-monthly-201010-201012 |title=Top 5 Search Engines from Oct to Dec 10 &#124; StatCounter Global Stats |author= |work= |publisher=StatCounter |accessdate=17 January 2011}}</ref>\n\n[[Microsoft]] said they needed to incorporate Microformats into upcoming projects,<ref>{{cite web|url=http://microformats.org/blog/2006/03/20/bill-gates-at-mix06-we-need-microformats |title=Bill Gates at Mix06 – "We need microformats" |date=2006-03-20 |quote=We need microformats and to get people to agree on them. It is going to bootstrap exchanging data on the Web… …we need them for things like contact cards, events, directions… |accessdate=2008-09-06}}</ref> as did other software companies.\n\nAlex Faaborg summarizes the arguments for putting the responsibility for microformat user interfaces in the web browser rather than making more complicated HTML:<ref>[http://blog.mozilla.com/faaborg/2007/02/04/microformats-part-4-the-user-interface-of-microformat-detection/ Microformats – Part 4: The User Interface of Microformat Detection « Alex Faaborg<!-- Bot generated title -->]</ref>\n* Only the web browser knows what applications are accessible to the user and what the user\'s preferences are\n* It lowers the barrier to entry for web site developers if they only need to do the markup and not handle "appearance" or "action" issues\n* Retains backwards compatibility with web browsers that don\'t support microformats\n* The web browser presents a single point of entry from the web to the user\'s computer, which simplifies security issues\n\n== Evaluation ==\nVarious commentators have offered review and discussion on the design principles and practical aspects of microformats. Microformats have been compared to other approaches that seek to serve the same or similar purpose.<ref name="criticism000">{{cite web |title=Criticism |work=Microformats.org |url=http://microformats.org/wiki?title=criticism&oldid=18478 |date=2007-03-24 |accessdate=2007-08-15}}</ref> From time to time, there is criticism of one, or all, microformats.<ref name="criticism000"/> The spread and use of microformats has been advocated.<ref name="advocacy000">{{cite web |title=Advocacy |work=Microformats.org |url=http://microformats.org/wiki/advocacy |date=2008-08-27 |accessdate=2007-08-15}}</ref><ref name="spread000">{{cite web |title=Spread Microformats |work=Microformats.org |url=http://microformats.org/wiki/spread-microformats |date=2008-08-29 |accessdate= 2007-08-15}} This includes community resources for marketing microformats such as buttons, banners, wallpaper / desktop screens, logo graphics, etc.</ref> [[Opera Software]] CTO and [[Cascading Style Sheets|CSS]] creator [[Håkon Wium Lie]] said in 2005 "We will also see a bunch of microformats being developed, and that’s how the [[Semantic Web|semantic web]] will be built, I believe."<ref name="advocacy001">{{cite web |title=Interview with Håkon Wium Lie |url=http://www.molly.com/2005/03/31/interview-with-hkon-wium-lie/ |first=Molly E. |last=Holzschlag |authorlink=Molly Holzschlag |date=2005-03-31 |work=Molly.com |accessdate=2007-11-18}}</ref> However, in August 2008 Toby Inkster, author of the "Swignition" (formerly "Cognition") microformat parsing service, pointed out that no new microformat specifications had been published since 2005.<ref name="threeyears">{{cite web |title=More than three years |url=http://microformats.org/discuss/mail/microformats-discuss/2008-August/012402.html |work=Microformats.org |first=Toby A. |last=Inkster |date=2008-04-22 |accessdate=2008-08-24}}</ref>\n\n=== Design principles ===\nComputer scientist and entrepreneur, [[Rohit Khare]] stated that \'\'reduce, reuse, and recycle\'\' is "shorthand for several design principles" that motivated the development and practices behind microformats.<ref name="Khare000"/>{{rp|71–72}} These aspects can be summarized as follows:\n\n*Reduce: favor the simplest solutions and focus attention on specific problems;\n*Reuse: work from experience and favor examples of current practice;\n*Recycle: encourage modularity and the ability to embed, valid XHTML can be reused in blog posts, RSS feeds, and anywhere else you can access the web.<ref name="Khare000"/>\n\n=== Accessibility ===\nBecause some microformats make use of title attribute of HTML\'s {{tag|abbr|open}} element to conceal [[machine-readable data]] (particularly date-times and geographical coordinates) in the "[http://microformats.org/wiki/abbr-design-pattern abbr design pattern]", the plain text content of the element is inaccessible to [[screen reader]]s that expand abbreviations.<ref name="ATF">{{cite web |url=http://www.webstandards.org/2007/04/27/haccessibility/ | title=hAccessibility | first=James |last=Craig |publisher= [[Web Standards Project]] |date=2007-04-27 |accessdate=2007-08-16}}</ref> In June 2008 the [[BBC]] announced that it would be dropping use of microformats using the <code>abbr</code> design pattern because of accessibility concerns.<ref name="BBCabbr">{{cite web |url=http://www.bbc.co.uk/blogs/radiolabs/2008/06/removing_microformats_from_bbc.shtml |title=Removing Microformats from bbc.co.uk/programmes |first=Michael |last=Smethurst |publisher= [[BBC]] |date=2008-06-23 |accessdate=2008-08-24}}</ref>\n\n=== Comparison with alternative approaches ===\nMicroformats are not the only solution for providing "more intelligent data" on the web; alternative approaches are used and are under development. For example, the use of [[XML]] markup and standards of the Semantic Web are cited as alternative approaches.<ref name="Khare000"/> Some contrast these with microformats in that they do not necessarily coincide with the design principles of "reduce, reuse, and recycle", at least not to the same extent.<ref name="Khare000"/>\n\nOne advocate of microformats, [[Tantek Çelik]], characterized a problem with alternative approaches: {{cquote|Here\'s a new language we want you to learn, and now you need to output these additional files on your server. It\'s a hassle. (Microformats) lower the barrier to entry.<ref name="Wharton000"/>}}\n\nFor some applications the use of other approaches may be valid. If the type of data to be described does not map to an existing microformat, [[RDFa]] can embed arbitrary vocabularies into HTML, such as for example domain-specific scientific data such as zoological or chemical data for which there is no microformat. Standards such as W3C\'s [[GRDDL]] allow microformats to be converted into data compatible with the Semantic Web.<ref name="King007">{{cite web |title=W3C GRDDL Recommendation Bridges HTML/Microformats and the Semantic Web |work=XML Coverpages |date=2007-09-13 |url=http://xml.coverpages.org/ni2007-09-13-a.html |publisher=[[OASIS (organization)|OASIS]] |accessdate=2007-11-23}}</ref>\n\nAnother advocate of microformats, Ryan King, put the compatibility of microformats with other approaches this way: {{cquote|Microformats provide an easy way for many people to contribute semantic data to the web. With GRDDL all of that data is made available for RDF Semantic Web tools. Microformats and GRDDL can work together to build a better web.<ref name="King007"/>}}\n\n== See also ==\n*[[COinS]]\n*[[Embedded RDF]]\n*[[Intelligent agent]]s\n*[[RDFa Lite]]\n*[[JSON-LD]]\n*[[S5 (file format)]]\n*[[Schema.org]]\n*[[Simple HTML Ontology Extensions]]\n*[[XMDP]]\n\n== Notes ==\n{{Reflist|2}}\n\n== References ==\n{{Refbegin|2}}\n*{{cite book |last=Allsopp |first=John | title=Microformats: Empowering Your Markup for Web 2.0 |date=March 2007 |publisher=[[Apress|Friends of ED]] |isbn=978-1-59059-814-6 |page=368}}\n*{{cite book |last=Orchard |first=Leslie M |title=Hacking RSS and Atom |date=September 2005 |publisher=[[John Wiley & Sons]] |isbn=978-0-7645-9758-9 |page=602}}\n*{{cite book |last=Robbins |first=Jennifer Niederst |authorlink=Jennifer Niederst Robbins |first2=Tantek|last2=Çelik|authorlink2=Tantek Çelik|first3=Derek|last3=Featherstone|first4=Aaron|last4=Gustafson|title=Web Design In A Nutshell |edition=Third |date=February 2006 |publisher=[[O\'Reilly Media]] |isbn=978-0-596-00987-8 |page=826}}\n\n{{Refend}}\n\n== Further reading ==\n* {{cite book |last= Suda |first= Brian |title= Using Microformats |date=September 2006 |publisher= [[O\'Reilly Media]] |isbn=978-0-596-52821-8 |page=45}}\n* Ahmet Soylu, Patrick De Causmaecker, Fridolin Wild [http://www.rintonpress.com/journals/jmmonline.html#v6n1  Ubiquitous Web for Ubiquitous Environments: The Role of Embedded Semantics], article in Journal of Mobile Multimedia, Vol. 6, No.1, pp.&nbsp;26–48, (2010). [https://lirias.kuleuven.be/bitstream/123456789/243944/2/JMM_soylu_et_al_2010.pdf PDF]\n\n== External links ==\n{{Commons category|Microformat screenshots}}\n* [http://microformats.org/ microformats.org]\n* [http://www.digital-web.com/articles/microformats_primer/ Microformats Primer]\n* [http://microformatique.com/optimus/ Optimus] microformats parser and validator\n* [http://blog.mozilla.com/faaborg/2006/12/11/microformats-part-0-introduction A four-part discussion of Microformats, UI issues, and possible presentation in Firefox 3 by Alex Faaborg of Mozilla]\n\n{{Semantic Web}}\n{{Use dmy dates|date=January 2011}}\n\n[[Category:Microformats| ]]\n[[Category:Knowledge representation]]\n[[Category:Semantic HTML]]\n[[Category:Semantic Web]]\n[[Category:Web design]]\n[[Category:Web development]]']
['Frame problem', '11306', 'In [[artificial intelligence]], the \'\'\'frame problem\'\'\' describes an issue with using [[first-order logic]] (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms that simply imply that things in the environment do not change arbitrarily. For example, Hayes describes a "[[block world]]" with rules about stacking blocks together. In a FOL system, additional [[axiom]]s are required to make inferences about the environment (for example, that a block cannot change position unless it\'s physically moved). The frame problem is the problem of finding adequate collections of axioms for a viable description of a robot environment.<ref>{{cite journal|last=Hayes|first=Patrick|title=The Frame Problem and Related Problems in Artificial Intelligence|journal=University of Edinburgh|url=http://aitopics.org/sites/default/files/classic/Webber-Nilsson-Readings/Rdgs-NW-Hayes-FrameProblem.pdf}}</ref>\n\n[[John McCarthy (computer scientist)|John McCarthy]] and [[Patrick J. Hayes]] defined this problem in their 1969 article, \'\'Some Philosophical Problems from the Standpoint of Artificial Intelligence\'\'.  In this paper and many that came after the formal mathematical problem was a starting point for more general discussions of the difficulty of knowledge representation for artificial intelligence. Issues such as how to provide rational default assumptions and what humans consider common sense in a virtual environment.<ref>{{cite journal|last=McCarthy|first=J|author2=P.J. Hayes|title=Some philosophical problems from the standpoint of artificial intelligence|journal=Machine Intelligence|year=1969|volume=4|pages=463–502}}</ref>  Later, the term acquired a broader meaning in [[philosophy]], where it is formulated as the problem of limiting the beliefs that have to be updated in response to actions. In the logical context, actions are typically specified by what they change, with the implicit assumption that everything else (the frame) remains unchanged.\n\n==Description==\nThe frame problem occurs even in very simple domains. A scenario with a door, which can be open or closed, and a light, which can be on or off, is statically represented by two [[proposition]]s <math>\\textit{open}</math> and <math>\\textit{on}</math>. If these conditions can change, they are better represented by two [[Predicate (computer programming)|predicate]]s <math>\\textit{open}(t)</math> and <math>\\textit{on}(t)</math> that depend on time; such predicates are called [[fluent (artificial intelligence)|fluent]]s. A domain in which the door is closed and the light off at time 0, and the door opened at time 1, can be directly represented in logic{{clarify|reason=What kind of logic? If ordinary predicate logic is meant, what is the purpose of the \'true →\' in the 3rd formula? If some other logic (situation calculus?) is meant, it should be stated explicitly here, together with the purpose of the \'true →\' (e.g. some empty action?) in that logic.|date=August 2013}} by the following formulae:\n\n:<math>\\neg \\textit{open}(0)</math>\n:<math>\\neg \\textit{on}(0)</math>\n:<math>\\textit{true} \\rightarrow \\textit{open}(1)</math>\n\nThe first two formulae represent the initial situation; the third formula represents the effect of executing the action of opening the door at time 1. If such an action had preconditions, such as the door being unlocked, it would have been represented by <math>\\neg \\textit{locked}(0) \\rightarrow \\textit{open}(1)</math>. In practice, one would have a predicate <math>\\textit{executeopen}(t)</math> for specifying when an action is executed and a rule <math>\\forall t . \\textit{executeopen}(t) \\wedge \\textit{true} \\rightarrow \\textit{open}(t+1)</math> for specifying the effects of actions.  The article on the [[situation calculus]] gives more details.\n\nWhile the three formulae above are a direct expression in logic of what is known, they do not suffice to correctly draw consequences. While the following conditions (representing the expected situation) are consistent with the three formulae above, they are not the only ones.\n\n:{|\n| <math>\\neg \\textit{open}(0)</math> &nbsp; &nbsp;  || <math>\\textit{open}(1)</math>\n|-\n| <math>\\neg \\textit{on}(0)</math>   || <math>\\neg \\textit{on}(1)</math>\n|}\n\nIndeed, another set of conditions that is consistent with the three formulae above is:\n\n:{|\n| <math>\\neg \\textit{open}(0)</math> &nbsp; &nbsp;  || <math>\\textit{open}(1)</math>\n|-\n| <math>\\neg \\textit{on}(0)</math>   || <math>\\textit{on}(1)</math>\n|}\n\nThe frame problem is that specifying only which conditions are changed by the actions do not allow, in logic, to conclude that all other conditions are not changed. This problem can be solved by adding the so-called “frame axioms”, which explicitly specify that all conditions not affected by actions are not changed while executing that action. For example, since the action executed at time 0 is that of opening the door, a frame axiom would state that the status of the light does not change from time 0 to time 1:\n\n:<math>\\textit{on}(0) \\leftrightarrow \\textit{on}(1)</math>\n\nThe frame problem is that one such frame axiom is necessary for every pair of action and condition such that the action does not affect the condition.{{clarify|reason=Shouldn\'t then the frame axiom be the following modification of the above rule: \'∀t.executeopen(t)→open(t+1)∧(on(t+1)↔on(t))\' ? In contrast, the formula \'on(0)↔on(1)\' seems to be too particular taylored to the \'executeopen(0)\' situation.|date=August 2013}} In other words, the problem is that of formalizing a dynamical domain without explicitly specifying the frame axioms.\n\nThe solution proposed by McCarthy to solve this problem involves assuming that a minimal amount of condition changes have occurred; this solution is formalized using the framework of [[Circumscription (logic)|circumscription]]. The [[Yale shooting problem]], however, shows that this solution is not always correct. Alternative solutions were then proposed, involving predicate completion, fluent occlusion, [[successor state axiom]]s, etc.; they are explained below. By the end of the 1980s, the frame problem as defined by McCarthy and Hayes was solved{{clarify|reason=Mention the (combination of) approach(es) by which the frame problem was solved.|date=August 2013}}. Even after that, however, the term “frame problem” was still used, in part to refer to the same problem but under different settings (e.g., concurrent actions), and in part to refer to the general problem of representing and reasoning with dynamical domains.\n\n== Solutions ==\nThe following solutions depict how the frame problem is solved in various formalisms. The formalisms themselves are not presented in full: what is presented are simplified versions that are sufficient to explain the full solution.\n\n===Fluent occlusion solution===\nThis solution was proposed by [[Erik Sandewall]], who also defined a [[formal language]] for the specification of dynamical domains; therefore, such a domain can be first expressed in this language and then automatically translated into logic. In this article, only the expression in logic is shown, and only in the simplified language with no action names.\n\nThe rationale of this solution is to represent not only the value of conditions over time, but also whether they can be affected by the last executed action. The latter is represented by another condition, called occlusion. A condition is said to be \'\'occluded\'\' in a given time point if an action has been just executed that makes the condition true or false as an effect. Occlusion can be viewed as “permission to change”: if a condition is occluded, it is relieved from obeying the constraint of inertia.\n\nIn the simplified example of the door and the light, occlusion can be formalized by two predicates <math>\\textit{occludeopen}(t)</math> and <math>\\textit{occludeon}(t)</math>. The rationale is that a condition can change value only if the corresponding occlusion predicate is true at the next time point. In turn, the occlusion predicate is true only when an action affecting the condition is executed.\n\n:<math>\\neg \\textit{open}(0)</math>\n:<math>\\neg \\textit{on}(0)</math>\n:<math>\\textit{true} \\rightarrow \\textit{open}(1) \\wedge \\textit{occludeopen}(1)</math>\n:<math>\\forall t . \\neg \\textit{occludeopen}(t) \\rightarrow (\\textit{open}(t-1) \\leftrightarrow \\textit{open}(t))</math>\n:<math>\\forall t . \\neg \\textit{occludeon}(t) \\rightarrow (\\textit{on}(t-1) \\leftrightarrow \\textit{on}(t))</math>\n\nIn general, every action making a condition true or false also makes the corresponding occlusion predicate true. In this case, <math>\\textit{occludeopen}(1)</math> is true, making the antecedent of the fourth formula above false for <math>t=1</math>; therefore, the constraint that <math>\\textit{open}(t-1) \\leftrightarrow \\textit{open}(t)</math> does not hold for <math>t=1</math>. Therefore, <math>\\textit{open}</math> can change value, which is also what is enforced by the third formula.\n\nIn order for this condition to work, occlusion predicates have to be true only when they are made true as an effect of an action. This can be achieved either by [[Circumscription (logic)|circumscription]] or by predicate completion. It is worth noticing that occlusion does not necessarily imply a change: for example, executing the action of opening the door when it was already open (in the formalization above) makes the predicate <math>\\textit{occludeopen}</math> true and makes <math>\\textit{open}</math> true; however, <math>\\textit{open}</math> has not changed value, as it was true already.\n\n===Predicate completion solution===\nThis encoding is similar to the fluent occlusion solution, but the additional predicates denote change, not permission to change. For example, <math>\\textit{changeopen}(t)</math> represents the fact that the predicate <math>\\textit{open}</math> will change from time <math>t</math> to <math>t+1</math>. As a result, a predicate changes if and only if the corresponding change predicate is true. An action results in a change if and only if it makes true a condition that was previously false or vice versa.\n\n:<math>\\neg \\textit{open}(0)</math>\n:<math>\\neg \\textit{on}(0)</math>\n:<math>\\neg \\textit{open}(0) \\wedge \\textit{true} \\rightarrow \\textit{changeopen}(0)</math>\n:<math>\\forall t. \\textit{changeopen}(t) \\leftrightarrow (\\neg \\textit{open}(t) \\leftrightarrow \\textit{open}(t+1))</math>\n:<math>\\forall t. \\textit{changeon}(t) \\leftrightarrow (\\neg \\textit{on}(t) \\leftrightarrow \\textit{on}(t+1))</math>\n\nThe third formula is a different way of saying that opening the door causes the door to be opened. Precisely, it states that opening the door changes the state of the door if it had been previously closed. The last two conditions state that a condition changes value at time <math>t</math> if and only if the corresponding change predicate is true at time <math>t</math>. To complete the solution, the time points in which the change predicates are true have to be as few as possible, and this can be done by applying predicate completion to the rules specifying the effects of actions.\n\n===Successor state axioms solution===\nThe value of a condition after the execution of an action can be determined by\nthe fact that the condition is true if and only if:\n\n# the action makes the condition true; or\n# the condition was previously true and the action does not make it false.\n\nA [[successor state axiom]] is a formalization in logic of these two facts. For\nexample, if <math>\\textit{opendoor}(t)</math> and <math>\\textit{closedoor}(t)</math> are two\nconditions used to denote that the action executed at time <math>t</math> was\nto open or close the door, respectively, the running example is encoded as\nfollows.\n\n: <math>\\neg \\textit{open}(0)</math>\n: <math>\\neg \\textit{on}(0)</math>\n: <math>\\textit{opendoor}(0)</math>\n: <math>\\forall t . \\textit{open}(t+1) \\leftrightarrow \\textit{opendoor}(t) \\vee (\\textit{open}(t) \\wedge \\neg \\textit{closedoor}(t))</math>\n\nThis solution is centered around the value of conditions, rather than the\neffects of actions. In other words, there is an axiom for every condition,\nrather than a formula for every action. Preconditions to actions (which are not\npresent in this example) are formalized by other formulae. The successor state\naxioms are used in the variant to the [[situation calculus]] proposed by\n[[Ray Reiter]].\n\n===Fluent calculus solution===\nThe [[fluent calculus]] is a variant of the situation calculus. It solves the frame problem by using first-order logic\n[[First-order logic#Formation rules|terms]], rather than predicates, to represent the states. Converting\npredicates into terms in first order logic is called [[Reification (knowledge representation)|reification]]; the\nfluent calculus can be seen as a logic in which predicates representing the\nstate of conditions are reified.\n\nThe difference between a predicate and a term in first order logic is that a term is a representation of an object (possibly a complex object composed of other objects), while a predicate represents a condition that can be true or false when evaluated over a given set of terms.\n\nIn the fluent calculus, each possible state is represented by a term obtained by composition of other terms, each one representing the conditions that are true in state. For example, the state in which the door is open and the light is on is represented by the term <math>\\textit{open} \\circ \\textit{on}</math>. It is important to notice that a term is not true or false by itself, as it is an object and not a condition. In other words, the term <math>\\textit{open} \\circ \\textit{on}</math> represent a possible state, and does not by itself mean that this is the current state. A separate condition can be stated to specify that this is actually the state at a given time, e.g., <math>\\textit{state}(\\textit{open} \\circ \\textit{on}, 10)</math> means that this is the state at time <math>10</math>.\n\nThe solution to the frame problem given in the fluent calculus is to specify the effects of actions by stating how a term representing the state changes when the action is executed. For example, the action of opening the door at time 0 is represented by the formula:\n\n: <math>\\textit{state}(s \\circ \\textit{open}, 1) \\leftrightarrow \\textit{state}(s,0)</math>\n\nThe action of closing the door, which makes a condition false instead of true, is represented in a slightly different way:\n\n: <math>\\textit{state}(s, 1) \\leftrightarrow \\textit{state}(s \\circ \\textit{open}, 0)</math>\n\nThis formula works provided that suitable axioms are given about <math>\\textit{state}</math> and <math>\\circ</math>, e.g., a term containing two times the same condition is not a valid state (for example, <math>\\textit{state}(\\textit{open} \\circ s \\circ \\textit{open}, t)</math> is always false for every <math>s</math> and <math>t</math>).\n\n===Event calculus solution===\nThe [[event calculus]] uses terms for representing fluents, like the fluent calculus, but also has axioms constraining the value of fluents, like the successor state axioms. In the event calculus, inertia is enforced by formulae stating that a fluent is true if it has been true at a given previous time point and no action changing it to false has been performed in the meantime. Predicate completion is still needed in the event calculus for obtaining that a fluent is made true only if an action making it true has been performed, but also for obtaining that an action had been performed only if that is explicitly stated.\n\n===Default logic solution===\nThe frame problem can be thought of as the problem of formalizing the principle that, by default, "everything is presumed to remain in the state in which it is" ([[Gottfried Wilhelm Leibniz|Leibniz]], "An Introduction to a Secret Encyclopædia", \'\'c\'\'. 1679).  This default, sometimes called the \'\'commonsense law of inertia\'\', was expressed by [[Raymond Reiter]] in [[default logic]]:\n\n: <math>\\frac{R(x,s)\\; :\\ R(x,\\textit{do}(a,s))}{R(x,\\textit{do}(a,s))}</math>\n\n(if <math>R(x)</math> is true in situation <math>s</math>, and it can be assumed<ref>i.e., no contradicting information is known</ref> that <math>R(x)</math> remains true after executing action <math>a</math>, then we can conclude that <math>R(x)</math> remains true).\n\nSteve Hanks and [[Drew McDermott]] argued, on the basis of their [[Yale shooting problem|Yale shooting]] example, that this solution to the frame problem is unsatisfactory.  Hudson Turner showed, however, that it works correctly in the presence of appropriate additional postulates.\n\n===Answer set programming solution===\nThe counterpart of the default logic solution in the language of [[answer set programming]] is a rule with [[stable model semantics#Strong negation|strong negation]]:\n\n:<math>r(X,T+1) \\leftarrow r(X,T),\\ \\hbox{not }\\sim r(X,T+1)</math>\n\n(if <math>r(X)</math> is true at time <math>T</math>, and it can be assumed that <math>r(X)</math> remains true at time <math>T+1</math>, then we can conclude that <math>r(X)</math> remains true).\n\n===Action description languages===\n[[Action description language]]s elude the frame problem rather than solving it. An action description language is a formal language with a syntax that is specific for describing situations and actions. For example, that the action <math>\\textit{opendoor}</math> makes the door open if not locked is expressed by:\n\n: <math>\\textit{opendoor}</math> causes <math>\\textit{open}</math> if <math>\\neg \\textit{locked}</math>\n\nThe semantics of an action description language depends on what the language can express (concurrent actions, delayed effects, etc.) and is usually based on [[transition system]]s.\n\nSince domains are expressed in these languages rather than directly in logic, the frame problem only arises when a specification given in an action description logic is to be translated into logic. Typically, however, a translation is given from these languages to [[answer set programming]] rather than first-order logic.\n\n==See also==\n* [[Yale shooting problem]]\n* [[Binding problem]]\n* [[Ramification problem]]\n* [[Qualification problem]]\n* [[Common sense]]\n* [[Commonsense reasoning]]\n* [[Defeasible reasoning]]\n* [[Non-monotonic logic]]\n* [[Symbol grounding]]\n* [[Linear logic]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite journal | last1 = Doherty | first1 = P. | last2 = Gustafsson | first2 = J. | last3 = Karlsson | first3 = L. | last4 = Kvarnström | first4 = J. | year = 1998 | title = TAL: Temporal action logics language specification and tutorial | url = http://www.ep.liu.se/ej/etai/1998/009 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 273–306 }}\n* {{cite journal | last1 = Gelfond | first1 = M. | last2 = Lifschitz | first2 = V. | year = 1993 | title = Representing action and change by logic programs | url = | journal = Journal of Logic Programming | volume = 17 | issue = | pages = 301–322 | doi=10.1016/0743-1066(93)90035-f}}\n* {{cite journal | last1 = Gelfond | first1 = M. | last2 = Lifschitz | first2 = V. | year = 1998 | title = Action languages | url = http://www.ep.liu.se/ej/etai/1998/007 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 193–210 }}\n* {{cite journal | last1 = Hanks | first1 = S. | last2 = McDermott | first2 = D. | year = 1987 | title = Nonmonotonic logic and temporal projection | url = | journal = Artificial Intelligence | volume = 33 | issue = 3| pages = 379–412 | doi=10.1016/0004-3702(87)90043-9}}\n* {{cite journal | last1 = Levesque | first1 = H. | authorlink3 = Raymond Reiter | last2 = Pirri | first2 = F. | last3 = Reiter | first3 = R. | year = 1998 | title = Foundations for the situation calculus | url = http://www.ep.liu.se/ej/etai/1998/005 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 159–178 }}\n* {{cite journal | last1 = Liberatore | first1 = P. | year = 1997 | title = The complexity of the language A | url = http://www.ep.liu.se/ej/etai/1997/002 | journal = [[Electronic Transactions on Artificial Intelligence]] | volume = 1 | issue = 1-3| pages = 13–37 }}\n* {{cite journal |first=V. |last=Lifschitz |year=2012 |url=http://www.cs.utexas.edu/~vl/papers/jmc.pdf |title=The frame problem, then and now |publisher=[[University of Texas at Austin]]}} Presented at \'\'Celebration of John McCarthy\'s Accomplishments\'\', [[Stanford University]], March 25, 2012.\n* {{cite journal | last1 = McCarthy | first1 = J. | last2 = Hayes | first2 = P. J. | year = 1969 | title = Some philosophical problems from the standpoint of artificial intelligence | url = http://www-formal.stanford.edu/jmc/mcchay69.html | journal = Machine Intelligence | volume = 4 | issue = | pages = 463–502 }}\n* {{cite journal | last1 = McCarthy | first1 = J. | year = 1986 | title = Applications of circumscription to formalizing common-sense knowledge | url = http://www-formal.stanford.edu/jmc/applications.html | journal = Artificial Intelligence | volume = 28 | issue = | pages = 89–116 | doi=10.1016/0004-3702(86)90032-9}}\n* {{cite journal | last1 = Miller | first1 = R. | last2 = Shanahan | first2 = M. | year = 1999 | title = The event-calculus in classical logic - alternative axiomatizations | url = http://www.ida.liu.se/ext/epa/ej/etai/1999/016/epapage.html | journal = Electronic Transactions on Artificial Intelligence | volume = 3 | issue = 1| pages = 77–105 }}\n* {{cite journal | last1 = Pirri | first1 = F. | last2 = Reiter | first2 = R. | year = 1999 | title = Some contributions to the metatheory of the Situation Calculus | url = | journal = [[Journal of the ACM]] | volume = 46 | issue = 3| pages = 325–361 | doi = 10.1145/316542.316545 }}\n* {{cite journal | last1 = Reiter | first1 = R. | authorlink = Raymond Reiter | year = 1980 | title = A logic for default reasoning | url = | journal = Artificial Intelligence | volume = 13 | issue = | pages = 81–132 | doi=10.1016/0004-3702(80)90014-4}}\n* {{cite book |authorlink=Raymond Reiter |first=Raymond |last=R. |year=1991 |chapter=The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression |editor=Lifschitz, Vladimir |title=Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy |pages=359–380 |publisher=Academic Press |location=New York}}\n* {{cite journal | last1 = Sandewall | first1 = E. | year = 1972 | title = An approach to the Frame Problem and its Implementation | url = | journal = Machine Intelligence | volume = 7 | issue = | pages = 195–204 }}\n* {{cite book |first=E. |last=Sandewall |year=1994 |title=Features and Fluents |volume=(vol. 1) |publisher=Oxford University Press |location=New York |isbn=0-19-853845-6}}\n* {{cite book |first1=E. |last1=Sandewall |first2=Y. |last2=Shoham |year=1995 |chapter=Non-monotonic Temporal Reasoning |editor1=Gabbay, D. M. |editor2=Hogger, C. J. |editor3=Robinson, J. A. |title=Handbook of Logic in Artificial Intelligence and Logic Programming |volume=(vol. 4) |pages=439–498 |publisher=Oxford University Press |isbn=0-19-853791-3}}\n* {{cite journal | last1 = Sandewall | first1 = E. | year = 1998 | title = Cognitive robotics logic and its metatheory: Features and fluents revisited | url = http://www.ep.liu.se/ej/etai/1998/010 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 307–329 }}\n* {{cite book |first=M. |last=Shanahan |year=1997 |title=Solving the frame problem: A mathematical investigation of the common sense law of inertia |publisher=MIT Press}}\n* {{cite journal | last1 = Thielscher | first1 = M. | year = 1998 | title = Introduction to the fluent calculus | url = http://www.ep.liu.se/ej/etai/1998/006 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 179–192 }}\n* {{cite journal | last1 = Toth | first1 = J.A. | year = 1995 | title = Book review. Kenneth M. and Patrick J. Hayes, eds | url = | journal = Reasoning agents in a dynamic world: The frame problem. Artificial Intelligence | volume = 73 | issue = | pages = 323–369 | doi=10.1016/0004-3702(95)90043-8}}\n* {{cite journal | last1 = Turner | first1 = H. | year = 1997 | title = Representing actions in logic programs and default theories: a situation calculus approach | url = http://www.d.umn.edu/~hudson/papers/ralpdt6.pdf | format = PDF | journal = Journal of Logic Programming | volume = 31 | issue = | pages = 245–298 | doi=10.1016/s0743-1066(96)00125-2}}\n\n==External links==\n* {{cite SEP |url-id=frame-problem |title=The Frame Problem}}\n* [http://www-formal.stanford.edu/jmc/mcchay69/mcchay69.html Some Philosophical Problems from the Standpoint of Artificial Intelligence]; the original article of McCarthy and Hayes that proposed the problem.\n\n{{John McCarthy navbox}}\n\n[[Category:Artificial intelligence]]\n[[Category:Knowledge representation]]\n[[Category:Epistemology]]\n[[Category:Logic programming]]\n[[Category:Philosophical problems]]\n[[Category:1969 introductions]]']
['Default logic', '889639', "\n'''Default logic''' is a [[non-monotonic logic]] proposed by [[Raymond Reiter]] to formalize reasoning with default assumptions.\n\nDefault logic can express facts like “by default, something is true”; by contrast, standard logic can only express that something is true or that something is false. This is a problem because reasoning often involves facts that are true in the majority of cases but not always. A classical example is: “birds typically fly”. This rule can be expressed in standard logic either by “all birds fly”, which is inconsistent with the fact that penguins do not fly, or by “all birds that are not penguins and not ostriches and ... fly”, which requires all exceptions to the rule to be specified. Default logic aims at formalizing inference rules like this one without explicitly mentioning all their exceptions.\n\n==Syntax of default logic==\nA default theory is a pair <math>\\langle W, D \\rangle</math>. {{mvar|W}} is a set of logical formulae, called ''the background theory'', that formalize the facts that are known for sure. {{mvar|D}} is a set of ''default rules'', each one being of the form:\n\n: <math>\\frac{\\mathrm{Prerequisite : Justification}_1, \\dots , \\mathrm{Justification}_n}{\\mathrm{Conclusion}}</math>\n\nAccording to this default, if we believe that {{math|Prerequisite}} is true, and each of <math>\\mathrm{Justification}_i</math> is consistent with our current beliefs, we are led to believe that {{math|Conclusion}} is true.\n\nThe logical formulae in {{mvar|W}} and all formulae in a default were originally assumed to be [[first-order logic]] formulae, but they can potentially be formulae in an arbitrary formal logic. The case in which they are formulae in [[propositional logic]] is one of the most studied.\n\n===Examples===\nThe default rule “birds typically fly” is formalized by the following default:\n\n:<math>D = \\left\\{ \\frac{\\mathrm{Bird}(X) : \\mathrm{Flies}(X)}{\\mathrm{Flies}(X)} \\right\\}</math>\n\nThis rule means that, if {{mvar|X}} is a bird, and it can be assumed that it flies, then we can conclude that it flies. A background theory containing some facts about birds is the following one:\n\n:<math>W = \\{ \\mathrm{Bird}(\\mathrm{Condor}), \\mathrm{Bird}(\\mathrm{Penguin}), \\neg \\mathrm{Flies}(\\mathrm{Penguin}), \\mathrm{Flies}(\\mathrm{Bee}) \\}</math>.\n\nAccording to this default rule, a condor flies because the precondition {{math|Bird(Condor)}} is true and the justification {{math|Flies(Condor)}} is not inconsistent with what is currently known. On the contrary, {{math|Bird(Penguin)}} does not allow concluding {{math|Flies(Penguin)}}: even if the precondition of the default {{math|Bird(Penguin)}} is true, the justification {{math|Flies(Penguin)}} is inconsistent with what is known.\nFrom this background theory and this default, {{math|Bird(Bee)}} cannot be concluded because the default rule only allows deriving\n{{math|Flies(''X'')}} from {{math|Bird(''X'')}}, but not vice versa. Deriving the antecedents of an inference rule from the consequences is a form of explanation of the consequences, and is the aim of [[abductive reasoning]].\n\nA common default assumption is that what is not known to be true is believed to be false. This is known as the [[Closed World Assumption]], and is formalized in default logic using a default like the following one for every fact {{mvar|F}}.\n\n: <math>\\frac{:{\\neg}F}{{\\neg}F}</math>\n\nFor example, the computer language [[Prolog]] uses a sort of default assumption when dealing with negation: if a negative atom cannot be proved to be true, then it is assumed to be false.\nNote, however, that Prolog uses the so-called [[negation as failure]]: when the interpreter has to evaluate the atom <math>\\neg F</math>, it tries to prove that {{mvar|F}} is true, and conclude that <math>\\neg F</math> is true if it fails. In default logic, instead, a default having <math>\\neg F</math> as a justification can only be applied if <math>\\neg F</math> is consistent with the current knowledge.\n\n===Restrictions===\n\nA default is categorical or prerequisite-free if it has no prerequisite (or, equivalently, its prerequisite is [[tautology (logic)|tautological]]). A default is normal if it has a single justification that is equivalent to its conclusion. A default is supernormal if it is both categorical and normal. A default is seminormal if all its justifications entail its conclusion. A default theory is called categorical, normal, supernormal, or seminormal if all defaults it contains are categorical, normal, supernormal, or seminormal, respectively.\n\n==Semantics of default logic==\n\nA default rule can be applied to a theory if its precondition is entailed by the theory and its justifications are all '''''consistent with''''' the theory.  The application of a default rule leads to the addition of its consequence to the theory.  Other default rules may then be applied to the resulting theory.  '''When the theory is such that no other default can be applied, the theory is called an extension of the default theory.'''  The default rules may be applied in different order, and this may lead to different extensions. The [[Nixon diamond]] example is a default theory with two extensions:\n\n:<math>\n\\left\\langle\n\\left\\{\n\\frac{\\mathrm{Republican}(X):\\neg \\mathrm{Pacifist}(X)}{\\neg \\mathrm{Pacifist}(X)},\n\\frac{\\mathrm{Quaker}(X):\\mathrm{Pacifist}(X)}{\\mathrm{Pacifist}(X)}\n\\right\\},\n\\left\\{\\mathrm{Republican}(\\mathrm{Nixon}), \\mathrm{Quaker}(\\mathrm{Nixon})\\right\\}\n\\right\\rangle\n</math>\n\nSince [[Richard Nixon|Nixon]] is both a [[American Republican|Republican]] and a [[Quaker]], both defaults can be applied. However, applying the first default leads to the conclusion that Nixon is not a pacifist, which makes the second default not applicable. In the same way, applying the second default we obtain that Nixon is a pacifist, thus making the first default not applicable. This particular default theory has therefore two extensions, one in which {{math|Pacifist(Nixon)}} is true, and one \nin which {{math|Pacifist(Nixon)}} is false. \n\nThe original semantics of default logic was based on the [[Fixed point (mathematics)|fixed point]] of a function. The following is an equivalent algorithmic definition. If a default contains formulae with free variables, it is considered to represent the set of all defaults obtained by giving a value to all these variables. A default <math>\\frac{\\alpha:\\beta_1,\\ldots,\\beta_n}{\\gamma}</math> is applicable to a propositional theory {{mvar|T}} if <math>T \\models \\alpha</math> and\nall theories <math>T \\cup \\{\\beta_i\\}</math> are consistent. The application of this default to {{mvar|T}} leads to the theory <math>T \\cup \\{\\gamma\\}</math>. An extension can be generated by applying the following algorithm:\n\n T=W           /* current theory */\n A=0           /* set of defaults applied so far */\n &nbsp;\n               /* apply a sequence of defaults */\n '''while''' there is a default d that is not in A and is applicable to T\n   add the consequence of d to T\n   add d to A\n &nbsp;\n               /* final consistency check */\n '''if''' \n   for every default d in A\n     T is consistent with all justifications of d\n '''then'''\n   output T\n\nThis algorithm is [[nondeterministic algorithm|non-deterministic]], as several defaults can alternatively be applied to a given theory {{mvar|T}}. In the Nixon diamond example, the application of the first default leads to a theory to which the second default cannot be applied and vice versa. As a result, two extensions are generated: one in which Nixon is a pacifist and one in which Nixon is not a pacifist.\n\nThe final check of consistency of the justifications of all defaults that have been applied implies that some theories do not have any extensions. In particular, this happens whenever this check fails for every possible sequence of applicable defaults. The following default theory has no extension:\n\n:<math>\n\\left\\langle \n\\left\\{\n\\frac{:A(b)}{\\neg A(b)}\n\\right\\},\n\\emptyset\n\\right\\rangle\n</math>\n\nSince <math>A(b)</math> is consistent with the background theory, the default can be applied, thus leading to the conclusion that <math>A(b)</math> is false. This result however undermines the assumption that has been made for applying the first default. Consequently, this theory has no extensions.\n\nIn a normal default theory, all defaults are normal: each default has the form <math>\\frac{\\phi : \\psi}{\\psi}</math>. A normal default theory is guaranteed to have at least one extension. Furthermore, the extensions of a normal default theory are mutually inconsistent, i.e., inconsistent with each other.\n\n===Entailment===\n\nA default theory can have zero, one, or more extensions. [[Entailment]] of a formula from a default theory can be defined in two ways:\n\n; Skeptical : a formula is entailed by a default theory if it is entailed by all its extensions;\n\n; Credulous : a formula is entailed by a default theory if it is entailed by at least one of its extensions.\n\nThus, the Nixon diamond example theory has two extensions, one in which Nixon is a pacifist and one in which he is not a pacifist. Consequently, neither {{math|Pacifist(Nixon)}} nor {{math|&not;Pacifist(Nixon)}} are skeptically entailed, while both of them are credulously entailed. As this example shows, the credulous consequences of a default theory may be inconsistent with each other.\n\n===Alternative default inference rules===\n<!-- these are the alternative default inference rules that are based on the same original syntax of default logic -->\n\nThe following alternative inference rules for default logic are all based on the same syntax as the original system.\n\n; Justified: differs from the original one in that a default is not applied if thereby the set {{mvar|T}} becomes [[inconsistent]] with a justification of an applied default;\n\n; Concise: a default is applied only if its consequence is not already entailed by {{mvar|T}} (the exact definition is more complicated than this one; this is only the main idea behind it);\n\n; Constrained: a default is applied only if the set composed of the background theory, the justifications of all applied defaults, and the consequences of all applied defaults (including this one) is consistent;\n\n; Rational: similar to constrained default logic, but the consequence of the default to add is not considered in the consistency check;\n\n; Cautious: defaults that can be applied but are conflicting with each other (like the ones of the Nixon diamond example) are not applied.\n\nThe justified and constrained versions of the inference rule assign at least an extension to every default theory.\n\n==Variants of default logic==\n<!-- these are the variants of default logic that differ from the original one both in syntax and semantics -->\n\nThe following variants of default logic differ from the original one on both syntax and semantics.\n\n; Assertional variants : An assertion is a pair <math>\\langle p: \\{r_1,\\ldots,r_n\\} \\rangle</math> composed of a formula and a set of formulae. Such a pair indicates that {{mvar|p}} is true while the formulae <math>r_1,\\ldots,r_n</math> have been assumed consistent to prove that {{mvar|p}} is true. An assertional default theory is composed of an assertional theory (a set of assertional formulae) called the background theory and a set of defaults defined as in the original syntax. Whenever a default is applied to an assertional theory, the pair composed of its consequence and its set of justifications is added to the theory. The following semantics use assertional theories:\n\n*Cumulative default logic\n*Commitment to assumptions default logic\n*Quasi-default logic\n\n; Weak extensions : rather than checking whether the preconditions are valid in the theory composed of the background theory and the consequences of the applied defaults, the preconditions are checked for validity in the extension that will be generated; in other words, the algorithm for generating extensions starts by guessing a theory and using it in place of the background theory; what results from the process of extension generation is actually an extension only if it is equivalent to the theory guessed at the beginning. This variant of default logic is related in principle to [[autoepistemic logic]], where a theory <math>\\Box x \\rightarrow x</math> has the model in which {{mvar|x}} is true just because, assuming <math>\\Box x</math> true, the formula <math>\\Box x \\rightarrow x</math> supports the initial assumption.\n\n; Disjunctive default logic : the consequence of a default is a set of formulae instead of a single formula. Whenever the default is applied, at least one of its consequences is nondeterministically chosen and made true.\n\n; Priorities on defaults : the relative priority of defaults can be explicitly specified; among the defaults that are applicable to a theory, only one of the most preferred ones can be applied. Some semantics of default logic do not require priorities to be explicitly specified; rather, more specific defaults (those that are applicable in fewer cases) are preferred over less specific ones.\n\n; Statistical variant : a statistical default is a default with an attached upper bound on its frequency of error; in other words, the default is assumed to be an incorrect inference rule in at most that fraction of times it is applied.\n\n==Translations==\n\nDefault theories can be translated into theories in other logics and vice versa. The following conditions on translations have been considered:\n\n; Consequence-Preserving : the original and the translated theories have the same (propositional) consequences;\n\n; Faithful : this condition only makes sense when translating between two variants of default logic or between default logic and a logic in which a concept similar to extension exists, e.g., models in modal logic; a translation is faithful if there exists a mapping (typically, a bijection) between the extensions (or models) of the original and translated theories;\n\n; Modular : a translation from default logic to another logic is modular if the defaults and the background theory can be translated separately; moreover, the addition of formulae to the background theory only leads to adding the new formulae to the result of the translation;\n\n; Same-Alphabet : the original and translated theories are built on the same alphabet;\n\n; Polynomial : the running time of the translation or the size of the generated theory are required to be polynomial in the size of the original theory.\n\nTranslations are typically required to be faithful or at\nleast consequence-preserving, while the conditions of\nmodularity and same alphabet are sometimes ignored.\n\nThe translatability between propositional default logic and\nthe following logics have been studied:\n\n* classical propositional logic;\n* autoepistemic logic;\n* propositional default logic restricted to seminormal theories;\n* alternative semantics of default logic;\n* circumscription.\n\nTranslations exist or not depending on which conditions are imposed. Translations from propositional default logic to classical propositional logic cannot always generate a polynomially sized propositional theory, unless the [[polynomial hierarchy]] collapses. Translations to autoepistemic logic exists or not depending on whether modularity or the use of the same alphabet is required.\n\n==Complexity==\n\nThe [[Analysis of algorithms|computational complexity]] of the following problems about default logic is known:\n\n; Existence of extensions : deciding whether a propositional default theory has at least one extension is <math>\\Sigma^P_2</math>-complete;\n\n; Skeptical entailment : deciding whether a propositional default theory skeptically entails a [[propositional formula]] is <math>\\Pi^P_2</math>-complete;\n\n; Credulous entailment : deciding whether a propositional default theory credulously entails a propositional formula is <math>\\Sigma^P_2</math>-complete;\n\n; Extension checking : deciding whether a propositional formula is equivalent to an extension of a propositional default theory is <math>\\Delta^{P[log]}_2</math>-complete;\n\n; Model checking : deciding whether a propositional interpretation is a model of an extension of a propositional default theory is <math>\\Sigma^P_2</math>-complete.\n\n==Implementations==\n\nThree systems implementing default logics are \n[ftp://www.cs.engr.uky.edu/cs/manuscripts/deres.ps DeReS],\n[http://www.cs.uni-potsdam.de/wv/xray/ XRay] and\n[http://www.info.univ-angers.fr/pub/stephan/Research/GADEL/GADEL_prolog.html GADeL]\n<!-- algorithms? other implemented systems? -->\n\n==See also==\n* [[Answer set programming]]\n* [[Defeasible logic]]\n* [[Non-monotonic logic]]\n\n==References==\n* G. Antoniou (1999). A tutorial on default logics. ''ACM Computing Surveys'', 31(4):337-359.\n* M. Cadoli, F. M. Donini, P. Liberatore, and M. Schaerf (2000). Space efficiency of propositional knowledge representation formalisms. ''Journal of Artificial Intelligence Research'', 13:1-31.\n* P. Cholewinski, V. Marek, and M. Truszczynski (1996). Default reasoning system DeReS. In ''Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning (KR'96)'', pages 518-528.\n* J. Delgrande and T. Schaub (2003). On the relation between Reiter's default logic and its (major) variants. In ''Seventh European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU 2003)'', pages 452-463.\n* J. P. Delgrande, T. Schaub, and W. K. Jackson (1994). Alternative approaches to default logic. ''Artificial Intelligence'', 70:167-237.\n* G. Gottlob (1992). Complexity results for nonmonotonic logics. ''Journal of Logic and Computation'', 2:397-425.\n* G. Gottlob (1995). Translating default logic into standard autoepistemic logic. ''Journal of the ACM'', 42:711-740.\n* T. Imielinski (1987). Results on translating defaults to circumscription. ''Artificial Intelligence'', 32:131-146.\n* T. Janhunen (1998). On the intertranslatability of autoepistemic, default and priority logics, and parallel circumscription. In ''Proceedings of the Sixth European Workshop on Logics in Artificial Intelligence (JELIA'98)'', pages 216-232.\n* T. Janhunen (2003). Evaluating the effect of semi-normality on the expressiveness of defaults. ''Artificial Intelligence'', 144:233-250.\n* H. E. Kyburg and C-M. Teng (2006). Nonmonotonic Logic and Statistical Inference. ''Computational Intelligence'', 22(1): 26-51.\n* P. Liberatore and M. Schaerf (1998). The complexity of model checking for propositional default logics. In ''Proceedings of the Thirteenth European Conference on Artificial Intelligence (ECAI'98)'', pages 18–22.\n* W. Lukaszewicz (1988). Considerations on default logic: an alternative approach. ''Computational Intelligence'', 4(1):1-16.\n* W. Marek and M. Truszczynski (1993). ''Nonmonotonic Logics: Context-Dependent Reasoning''. Springer.\n* A. Mikitiuk and M. Truszczynski (1995). Constrained and rational default logics. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1509-1517.\n* P. Nicolas, F. Saubion and I. Stéphan (2001). Heuristics for a Default Logic Reasoning System. ''International Journal on Artificial Intelligence Tools'', 10(4):503-523.\n* R. Reiter (1980). A logic for default reasoning. ''Artificial Intelligence'', 13:81-132.\n* T. Schaub, S. Brüning, and P. Nicolas (1996). XRay: A prolog technology theorem prover for default reasoning: A system description. In ''Proceedings of the Thirteenth International Conference on Automated Deduction (CADE'96)'', pages 293-297.\n* G. Wheeler (2004). A resource bounded default logic. In ''Proceedings of the 10th International Workshop on Non-Monotonic Reasoning (NMR-04)'', Whistler, British Columbia, 416-422.\n* G. Wheeler and C. Damasio (2004). An Implementation of Statistical Default Logic. In ''Proceedings of the 9th European Conference on Logics in Artificial Intelligence (JELIA 2004)'', LNCS Series, Springer, pages 121-133.\n\n==External links==\n* Schmidt, Charles F. [http://www.rci.rutgers.edu/~cfs/472_html/Logic_KR/DefaultTheory.html RCI.Rutgers.edu], Default Logic. Retrieved August 10, 2004.\n* Ramsay, Allan (1999). [http://www.ccl.umist.ac.uk/teaching/material/5005/node33.html UMIST.ac.uk], Default Logic. Retrieved August 10, 2004.\n* [http://plato.stanford.edu/entries/reasoning-defeasible/ Stanford.edu], Defeasible reasoning, [[Stanford Encyclopedia of Philosophy]].\n\n[[Category:Logic programming]]\n[[Category:Knowledge representation]]\n[[Category:Logical calculi]]\n[[Category:Non-classical logic]]"]
['Keyword AAA', '13302227', "{{notability|date=August 2016}}\n{{Multiple issues|\n{{Orphan|date=April 2016}}\n{{no footnotes|date=December 2011}}\n}}\n\n'''Keyword AAA''' is a [[thesaurus]] created by the [[State Records Authority of New South Wales]]. It is often used to [[categorize|categorise]] documents in a [[document management system]]. The thesaurus is often implemented in terms of [[ISO 2788]].\n\n==External links==\n* [http://www.records.nsw.gov.au/recordkeeping/resources/keyword-products/keyword-aaa Keyword AAA Overview]\n* [https://www.records.nsw.gov.au/recordkeeping/advice/records-classification/developing-and-implementing-a-keyword-thesaurus Developing and implementing a keyword thesaurus]\n\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]"]
['Region connection calculus', '8489018', '{{no footnotes|date=November 2016}}\nThe \'\'\'region connection calculus\'\'\' (\'\'\'RCC\'\'\') is intended to serve for qualitative spatial representation and [[Spatial-temporal reasoning|reasoning]]. RCC abstractly describes regions (in [[Euclidean space]], or in a [[topological space]]) by their possible relations to each other. RCC8 consists of 8 basic relations that are possible between two regions:\n* disconnected (DC)\n* externally connected (EC)\n* equal (EQ)\n* partially overlapping (PO)\n* tangential proper part (TPP)\n* tangential proper part inverse (TPPi)\n* non-tangential proper part (NTPP)\n* non-tangential proper part inverse (NTPPi)\nFrom these basic relations, combinations can be built. For example, proper part (PP) is the union of TPP and NTPP.\n[[Image:RCC8.jpg]]\n\n==Composition table==\nThe composition table of RCC8 are as follows:\n\n<center>\n{| class="wikitable" style="text-align:center;" border="1"\n! o\n! DC || EC || PO || TPP || NTPP || TPPi || NTPPi || EQ\n|-\n! DC\n| * || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC || DC || DC\n|-\n! EC\n| DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPP,TPPi,EQ || DC,EC,PO,TPP,NTPP || EC,PO,TPP,NTPP || PO,TPP,NTPP || DC,EC || DC || EC\n|-\n! PO\n| DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPPi,NTPPi || * || PO,TPP,NTPP || PO,TPP,NTPP || DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPPi,NTPPi || PO\n|-\n! TPP\n| DC || DC,EC || DC,EC,PO,TPP,NTPP || TPP,NTPP || NTPP || DC,EC,PO,TPP,TPPi,EQ || DC,EC,PO,TPPi,NTPPi || TPP\n|-\n! NTPP\n| DC || DC || DC,EC,PO,TPP,NTPP || NTPP || NTPP || DC,EC,PO,TPP,NTPP|| * || NTPP\n|-\n! TPPi\n| DC,EC,PO,TPPi,NTPPi || EC,PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPP,TPPi,EQ || PO,TPP,NTPP || TPPi,NTPPi || NTPPi || TPPi\n|-\n! NTPPi\n| DC,EC,PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPP,NTPP,TPPi,NTPPi,EQ|| NTPPi || NTPPi || NTPPi\n|-\n! EQ\n|  DC || EC || PO || TPP || NTPP || TPPi || NTPPi || EQ\n|}\n</center>\n\n*  "*" denotes the universal relation.\n\n==Examples==\n\nThe RCC8 calculus is intended for reasoning about spatial configurations. Consider the following example: two houses are connected via a road. Each house is located on an own property. The first house possibly touches the boundary of the property; the second one surely does not. What can we infer about the relation of the second property to the road? \n\nThe spatial configuration can be formalized in RCC8 as the following [[constraint network]]:\n\n house1 DC house2\n house1 {TPP, NTPP} property1\n house1 {DC, EC} property2\n house1 EC road\n house2 { DC, EC } property1\n house2 NTPP property2\n house2 EC road\n property1 { DC, EC } property2\n road { DC, EC, TPP, TPPi, PO, EQ, NTPP, NTPPi } property1\n road { DC, EC, TPP, TPPi, PO, EQ, NTPP, NTPPi } property2\n\nUsing the RCC8 [[composition table]] and the [[path-consistency algorithm]], we can refine the network in the following way:\n road { PO, EC } property1\n road { PO, TPP } property2\n\nThat is, the road either overlaps with the second property, or is even (tangential) part of it.\n\nOther versions of the region connection calculus include RCC5 (with only five basic relations - the distinction whether two regions touch each other are ignored) and RCC23 (which allows reasoning about convexity).\n\n==RCC8 use in GeoSPARQL==\n\nRCC8 has been partially{{Clarify|date=January 2016}} implemented in [[GeoSPARQL]] as described below:\n[[File:Region_Connection_Calculus_8_Relations_and_Open_Geospatial_Consortium_relations.svg|thumb|center|700px|alt=A graphical representation of Region Connection Calculus (RCC: Randell, Cui and Cohn, 1992) and the links to the equivalent naming by the Open Geospatial Consortium (OGC) with their equivalent URIs.|A graphical representation of Region Connection Calculus (RCC: Randell, Cui and Cohn, 1992) and the links to the equivalent naming by the Open Geospatial Consortium (OGC) with their equivalent URIs.]]\n\n==References==\n* Randell, D. A., Cui, Z. and Cohn, A. G.:  [http://wenxion.net/ac/randell92spatial.pdf A spatial logic based on regions and connection], Proc. 3rd Int. Conf. on Knowledge Representation and Reasoning, Morgan Kaufmann, San Mateo, pp. 165–176, 1992.\n* Anthony G. Cohn, Brandon Bennett, John Gooday, Micholas Mark Gotts: Qualitative Spatial Representation and Reasoning with the Region Connection Calculus. GeoInformatica, 1, 275–316, 1997.\n* J. Renz: [http://www.springerlink.com/content/d5g7fcjkd0q2/ Qualitative Spatial Reasoning with Topological Information]. Lecture Notes in Computer Science 2293, Springer Verlag, 2002.\n* T. Dong: [http://www.jstor.org/stable/41217909?seq=1#page_scan_tab_contents A COMMENT ON RCC: FROM RCC TO RCC⁺⁺]. Journal of Philosophical Logic, Vol 34, No. 2, pp. 319--352 \n\n[[Category:Reasoning]]\n[[Category:Knowledge representation]]\n[[Category:Constraint programming]]\n[[Category:Computational topology]]\n[[Category:Logical calculi]]']
['Digital curation', '16702334', '\'\'\'Digital curation\'\'\' is the selection,<ref name="ALA, Scime" >{{Cite web\n  |title=The Content Strategist as Digital Curator\n  |author=Erin Scime\n  |date=8 December 2009\n  |publisher=[[A List Apart]]\n  |url=http://www.alistapart.com/articles/content-strategist-as-digital-curator/\n}}</ref> [[Preservation (library and archival science)|preservation]], maintenance,  collection and [[archiving]] of [[Digital data|digital]] assets.<ref name="paper">{{Cite book | last1 = Rusbridge | first1 = C. | last2 = Buneman | first2 = P. | authorlink2 = Peter Buneman| last3 = Burnhill | first3 = P. | last4 = Giaretta | first4 = D. | last5 = Ross | first5 = S. | last6 = Lyon | first6 = L. | last7 = Atkinson | first7 = M. | authorlink7 = Malcolm Atkinson| chapter = The Digital Curation Centre: A Vision for Digital Curation | doi = 10.1109/LGDI.2005.1612461 | title = 2005 IEEE International Symposium on Mass Storage Systems and Technology | pages = 31 | year = 2005 | url = http://eprints.erpanet.org/82/01/DCC_Vision.pdf| isbn = 0-7803-9228-0 | pmid =  | pmc = }}</ref><ref name="dccdefn">{{cite web |title=What is Digital Curation? |publisher=[[Digital Curation Centre]] |url=http://www.dcc.ac.uk/about/what |accessdate=2008-04-01}}</ref><ref>{{cite web |title=Digital curation |author=Elizabeth Yakel |publisher=Emerald Group Publishing |year=2007 |url=http://www.ingentaconnect.com/content/mcb/164/2007/00000023/00000004/art00003 |accessdate=2008-04-01}}</ref>\nDigital curation establishes, maintains and adds value to repositories of digital data for present and future use.<ref name="dccdefn"/> This is often accomplished by [[archivist]]s, librarians, scientists, historians, and scholars. Enterprises are starting to use digital curation to improve the quality of information and data within their operational and strategic processes.<ref>E. Curry, A. Freitas, and S. O\'Riáin, [http://3roundstones.com/led_book/led-curry-et-al.html "The Role of Community-Driven Data Curation for Enterprises,"] in Linking Enterprise Data, D. Wood, Ed. Boston, MA: Springer US, 2010, pp. 25-47.</ref> Successful digital curation will mitigate digital obsolescence, keeping the information accessible to users indefinitely. \n\nThe term \'\'[[curator|curation]]\'\' in the past commonly referred to museum and library professionals. It has since been applied to interaction with [[social media]] including compiling digital images, web links and movie files.\n\n==Approaches==\n===Create new representation===\nFor some topics, knowledge is embodied in forms that have not been conducive to print, such as how choreography of dance or of the motion of skilled workers or artisans is difficult to encode. New digital approaches such as 3d holograms and other computer-programmed expressions are developing. \n\nFor mathematics, it seems possible for a new common language to be developed that would express mathematical ideas in ways that can be digitally stored, linked, and made accessible. The [[Global Digital Mathematics Library]] is a project to define and develop such a language.\n\n===Convert print resources===\nThe process of converting printed resources into digital collections has been epitomized to some degree by librarians and related specialists. For example,\nThe [[Digital Curation Centre]] is claimed to be a "world leading centre of expertise in digital information curation"<ref name="Digital Curation Centre">{{cite web|last=Digital Curation Centre|title=About the DCC|url=http://www.dcc.ac.uk/about-us|work=Website|publisher=Digital Curation Centre|accessdate=6 March 2013}}</ref> that assists higher education research institutions in such conversions. The DCC, based in the UK, began operations in early 2004 and suggests the following as a general outline of their approach to digital curation:\n\n* Conceptualize: Consider what digital material you will be creating and develop storage options. Take into account websites, publications, email, among other types of digital output.\n* Create: Produce digital material and attach all relevant metadata, typically the more metadata the more accessible the information.\n* Access and use: Determine the level of accessibility for the range of digital material created. Some material may be accessible only by password and other material may be freely accessible to the public.\n* Appraise and select: Consult the mission statement of the institution or private collection and determine what digital data is relevant. There may also be legal guidelines in place that will guide the decision process for a particular collection.\n* Dispose: Discard any digital material that is not deemed necessary to the institution.\n* Ingest: Send digital material to the predetermined storage solution. This may be an archive, repository or other facility.\n* Preservation action: Employ measures to maintain the integrity of the digital material.\n* Reappraise: Reevaluate material to ensure that is it still relevant and is true to its original form.\n* Store: Secure data within the predetermined storage facility.\n* Access and reuse: Routinely check that material is still accessible for the intended audience and that the material has not been compromised through multiple uses.\n* Transform: If desirable or necessary the material may be transferred into a different digital format.\n\n==="Sheer curation"===\n\'\'Sheer curation\'\' is an approach to digital curation where curation activities are quietly integrated into the normal work flow of those creating and managing data and other digital assets. The word sheer is used to emphasize the lightweight and virtually transparent nature of these curation activities. The term \'\'sheer curation\'\' was coined by Alistair Miles in the ImageStore project,<ref>[http://imageweb.zoo.ox.ac.uk/wiki/index.php/The_ImageStore_Project The ImageStore Project - ImageWeb<!-- Bot generated title -->]</ref> and the UK Digital Curation Centre\'s SCARP project.<ref>[http://www.dcc.ac.uk/scarp/ Digital Curation Centre: DCC SCARP Project<!-- Bot generated title -->]</ref> The approach depends on curators having close contact or \'immersion\' in data creators\' working practices. An example is the case study of a neuroimaging research group by Whyte et al., which explored ways of building its digital curation capacity around the apprenticeship style of learning of neuroimaging researchers, through which they share access to datasets and re-use experimental procedures.<ref>Whyte, A., Job, D., Giles, S. and Lawrie, S. (2008) \'[http://www.ijdc.net/index.php/ijdc/article/view/74/53 Meeting Curation Challenges in a Neuroimaging Group]\', The International Journal of Digital Curation Issue 1, Volume 3, 2008</ref>      \n\nSheer curation depends on the hypothesis that good data and digital asset management at the point of creation and primary use is also good practice in preparation for sharing, publication and/or [[long-term preservation]] of these assets. Therefore, sheer curation attempts to identify and promote tools and good practices in local data and digital asset management in specific domains, where those tools and practices add immediate value to the creators and primary users of those assets. Curation can best be supported by identifying existing practices of sharing, stewardship and re-use that add value, and augmenting them in ways that both have short-term benefits, and in the longer term reduce risks to digital assets or provide new opportunities to sustain their long-term accessibility and re-use value.    \n\nThe aim of sheer curation is to establish a solid foundation for other curation activities which may not directly benefit the creators and primary users of digital assets, especially those required to ensure long-term preservation. By providing this foundation, further curation activities may be carried out by specialists at appropriate institutional and organisation levels, whilst causing the minimum of interference to others.\n\nA similar idea is \'\'curation at source\'\' used in the context of Laboratory Information Management Systems [[Laboratory information management system|LIMS]]. This refers more specifically to automatic recording of [[metadata]] or information about data at the point of capture, and has been developed to apply semantic web techniques to integrate laboratory instrumentation and documentation systems.<ref>Frey, J. [http://www.allhands.org.uk/2008/programme/jeremyfrey.cfm \'Sharing and Collaboration\' keynote presentation at UK e-Science All Hands Meeting], 8–11 September 2008, Edinburgh</ref> Sheer curation and curation-at-source can be contrasted with post hoc [[digital preservation]], where a project is initiated to preserve a collection of digital assets that have already been created and are beyond the period of their primary use.\n\n===Channelisation===\n\'\'Channelisation\'\' is curation of digital assets on the web, often by brands and media companies, into continuous flows of content, turning the user experience from a lean-forward interactive medium, to a lean-back passive medium. The curation of content can be done by an independent third party, that selects media from any number of on-demand outlets from across the globe and adds them to a playlist to offer a digital "channel" dedicated to certain subjects, themes, or interests so that the end user would see and/or hear a continuous stream of content.\n\n==Challenges==\n* Storage format evolution and obsolescence<ref name=ijdcpw200711>{{cite web|title=Digital Preservation Theory and Application: Transcontinental Persistent Archives Testbed Activity |publisher=The International Journal of Digital Curation |url=http://www.ijdc.net/./ijdc/article/view/43/50 |author=Paul Watry |date=November 2007 |accessdate=2008-04-01 |deadurl=yes |archiveurl=https://web.archive.org/web/20080315030030/http://www.ijdc.net:80/ijdc/article/view/43/50 |archivedate=2008-03-15 |df= }}</ref> \n* Rate of creation of new data and data sets\n* Maintaining accessibility to data through links and search results\n* Comparability of [[semantic]] and [[ontology|ontological]] definitions of data sets<ref name="ijdcpw200711"/>\n\n===Responses===\n* Specialized research institutions<ref>[http://www.dcc.ac.uk/ Digital Curation Centre]</ref><ref>[http://www.dpconline.org/ Digital Preservation Coalition]</ref>\n* Academic courses\n* Dedicated symposia<ref>[http://www.ils.unc.edu/digccurr2007/ DigCCurr 2007 - an international symposium on Digital Curation, April 18-20, 2007]</ref><ref>[http://stardata.nrf.ac.za/nadicc 1st African Digital Management and Curation Conference and Workshop - Date: 12-13 February 2008]</ref>\n* Peer reviewed technical and industry journals<ref>[http://www.ijdc.net/ International Journal of Digital Curation]</ref>\n\n==See also==\n* [[Biocurator|Biocuration]]\n* [[Curator]]\n* [[Data curation]]\n* [[Data format management]]\n* [[Digital artifactual value]]\n* [[Digital asset management]]\n* [[Digital obsolescence]]\n* [[International Society for Biocuration]]\n\n==References==\n{{reflist|33em}}\n\n==External links==\n*[https://www.youtube.com/watch?v=pbBa6Oam7-w Animations introducing digital preservation and curation]\n*[http://www.alistapart.com/articles/content-strategist-as-digital-curator/ Content Strategist as Digital Curator], A List Apart Journal, December 2009\n*[http://www.dcc.ac.uk/ Digital Curation Centre]\n*[http://journals.tdl.org/jodi/article/view/229/183 Digital Curation and Trusted Repositories: Steps Toward Success]*[http://www.digcur-education.org DigCurV] A project funded by the European Commission to establish a curriculum framework for vocational training in digital curation.\n\n[[Category:Archival science]]\n[[Category:Databases]]\n[[Category:Knowledge representation]]\n[[Category:Digital libraries]]\n[[Category:Digital preservation]]']
['ITools Resourceome', '17727869', "{{lowercase title}}\n[[Image:Biositemap iTools NCBC.png|thumb|right|300px| NCBC iTools]]\n'''iTools'''<ref>{{cite journal|vauthors=Dinov ID, Rubin D, Lorensen W, Dugan J, Ma J, Murphy S, Kirschner B, Bug W, Sherman M, Floratos A, Kennedy D, Jagadish HV, Schmidt J, Athey B, Califano A, Musen M, Altman R, Kikinis R, Kohane I, Delp S, Parker DS, Toga AW | title=iTools: A Framework for Classification, Categorization and Integration of Computational Biology Resources | journal=PLoS ONE |volume=3|issue=5|pages= e2265| doi=10.1371/journal.pone.0002265 | year=2008 |url=http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0002265 | pmid=18509477 | pmc=2386255}}</ref> is a distributed infrastructure for managing, discovery, comparison and integration of computational biology resources. iTools employs [[Biositemap]] technology to retrieve and service meta-data about diverse bioinformatics data services, tools, and web-services. iTools is developed by the [[National Centers for Biomedical Computing]] as part of the [http://nihroadmap.nih.gov/ NIH Road Map Initiative].\n\n==See also==\n* [[Biositemaps]]\n\n==References==\n<references/>\n\n== External links ==\n* [http://iTools.ccb.ucla.edu Interactive iTools Server]\n\n[[Category:Knowledge representation]]\n[[Category:Bioinformatics]]"]
['Darwin Core', '21195116', '\'\'\'Darwin Core\'\'\' (often abbreviated to \'\'\'DwC\'\'\') is an extension of [[Dublin Core]] for [[biodiversity informatics]]. It is meant to provide a stable standard reference for sharing information on biological diversity.<ref>{{cite journal|last=Wieczorek|first=John|author2=D. Bloom |author3=R. Guralnick |author4=S. Blum |author5=M. Döring |author6=R. De Giovanni |author7=T. Robertson |author8=D. Vieglais |title=Darwin Core: An Evolving Community-developed Biodiversity Data Standard.|journal=[[PLoS ONE]] |year=2012|volume=7|issue=1|doi=10.1371/journal.pone.0029715|pmid=22238640|pmc=3253084}}</ref> The terms described in this standard are a part of a larger set of vocabularies and technical specifications under development and maintained by [[Biodiversity Information Standards (TDWG)]] (formerly known as the Taxonomic Databases Working Group (TDWG)).\n\n== Description ==\nThe Darwin Core is a body of standards. It includes a glossary of terms (in other contexts these might be called properties, elements, fields, columns, attributes, or concepts) intended to facilitate the sharing of information about biological diversity by providing reference definitions, examples, and commentaries. The Darwin Core is primarily based on [[taxon|taxa]], their occurrence in nature as documented by observations, specimens, and samples, and related information. Included in the standard are documents describing how these terms are managed, how the set of terms can be extended for new purposes, and how the terms can be used. The \'\'\'Simple Darwin Core\'\'\' <ref name="simpledwc">[http://rs.tdwg.org/dwc/terms/simple/index.htm  The Simple Darwin Core]</ref> is a specification for one particular way to use the terms and to share data about taxa and their occurrences in a simply-structured way. It is likely what is meant if someone were to suggest "formatting your data according to the Darwin Core".\n\nEach \'\'\'term\'\'\' has a definition and commentaries that are meant to promote the consistent use of the terms across applications and disciplines. Evolving commentaries that discuss, refine, expand, or translate the definitions and examples are referred to through links in the Comments attribute of each term. This approach to documentation allows the standard to adapt to new purposes without disrupting existing applications. There is meant to be a clear separation between the terms defined in the standard and the applications that make use of them. For example, though the data types and constraints are not provided in the term definitions, recommendations are made about how to restrict the values where appropriate.\n\nIn practice, Darwin Core decouples the definition and semantics of individual terms from application of these terms in different technologies such as [[XML]], [[Resource Description Framework|RDF]] or simple [[Comma-separated values|CSV]] text files. Darwin Core provides separate guidelines on how to encode the terms as XML<ref name="dwc-xml" >[http://rs.tdwg.org/dwc/terms/guides/xml/index.htm Darwin Core XML Guide]</ref> or text files.<ref name="dwc-text">[http://rs.tdwg.org/dwc/terms/guides/text/index.htm Darwin Core Text Guide]</ref>\n\n== History ==\nDarwin Core was originally created as a [[Z39.50]] profile by the Z39.50 Biology Implementers Group (ZBIG), supported by funding from a USA National Science Foundation award.<ref name="zbig">An Experimental Z39.50 Information Retrieval Protocol Test Bed for Biological Collection and Taxonomic Data, #9811443 [http://nsf.gov/awardsearch/showAward.do?AwardNumber=9811443]</ref>  The name "Darwin Core" was first coined by Allen Allison at the first meeting of the ZBIG held at the University of Kansas in 1998 while commenting on the profile\'s conceptual similarity with Dublin Core. The Darwin Core profile was later expressed as an XML Schema document for use by the Distributed Generic Information Retrieval (DiGIR) protocol. A [[TDWG]] task group was created to revise the Darwin Core, and a ratified metadata standard was officially released on 9 October 2009.\n\nThough ratified as a TDWG/[[Biodiversity Information Standards]] standard since then, Darwin Core has had numerous previous versions in production usage. The published standard contains a history<ref name="history">[http://rs.tdwg.org/dwc/terms/history/index.htm Darwin Core History]</ref> with details of the versions leading to the current standard.\n\n{| class="wikitable" style="text-align:left"\n|+Darwin Core Versions\n|-\n! Name !! Namespace !! Number of terms !! XML Schema !! Date Issued\n|-\n! Darwin Core 1.0\n| Not Applicable || 24 || (Z39.50 GRS-1) || 1998\n|-\n! Darwin Core 1.2 (Classic)\n| http://digir.net/schema/conceptual/darwin/2003/1.0  {{dead link|date=October 2016}} || 46 || [http://digir.net/schema/conceptual/darwin/2003/1.0/darwin2.xsd] || 2001-09-11\n|-\n! Darwin Core 1.21 (MaNIS/HerpNet/ORNIS/FishNet2)\n| http://digir.net/schema/conceptual/darwin/2003/1.0  {{dead link|date=October 2016}} || 63 || [http://digir.net/schema/conceptual/darwin/manis/1.21/darwin2.xsd] || 2003-03-15\n|-\n! Darwin Core OBIS\n| http://www.iobis.org/obis  {{dead link|date=October 2016}} || 27 || [http://iobis.org/obis/obis.xsd] || 2005-07-10\n|-\n! Darwin Core 1.4 (Draft Standard)\n| http://rs.tdwg.org/dwc/dwcore/  {{dead link|date=October 2016}} || 45 || [http://rs.tdwg.org/dwc/tdwg_dw_core.xsd] || 2005-07-10\n|-\n! Darwin Core Terms (properties)\n| http://rs.tdwg.org/dwc/terms/ || 172 || [http://rs.tdwg.org/dwc/xsd/tdwg_dwcterms.xsd] || 2009-10-09\n|-\n|}\n\n== Key Projects Using Darwin Core ==\n* The [[Global Biodiversity Information Facility]] (GBIF)<ref>{{cite web|url=http://www.gbif.org/informatics/standards-and-tools/publishing-data/data-standards/darwin-core-archives/ |title=Darwin Core |publisher=[[Global Biodiversity Information Facility]] |accessdate=April 12, 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/20110412022331/http://www.gbif.org:80/informatics/standards-and-tools/publishing-data/data-standards/darwin-core-archives |archivedate=April 12, 2011 |df= }}</ref>\n* The [[Ocean Biogeographic Information System]] (OBIS)<ref>{{cite web|url=http://www.iobis.org/data/schema-and-metadata|title=Data Schema and metadata|publisher=[[Ocean Biogeographic Information System]]|accessdate=April 12, 2011}}</ref>\n*[http://www.ala.org.au/datastandards.htm The Atlas of Living Australia (ALA)]\n*[http://www3.interscience.wiley.com/cgi-bin/fulltext/120713092/PDFSTART Online Zoological Collections of Australian Museums (OZCAM)]\n*[http://manisnet.org Mammal Networked Information System (MaNIS)]\n*[http://ornisnet.org Ornithological Information System (ORNIS)]\n*[http://www.fishnet2.net/index.html FishNet 2]\n*[http://vertnet.org VertNet]\n*[http://www.canadensys.net/ Canadensys]\n*[http://w3.ufsm.br/herbarioflorestal/nature/site/ Sistema Nature 3.0]\n*[http://eol.org Encyclopedia of Life]\n*[https://www.idigbio.org Integrated Digitized Biocollections (iDigBio)] <ref>{{cite web|title=Data Ingestion Guidance|url=https://www.idigbio.org/wiki/index.php/Data_Ingestion_Guidance|publisher=[[iDigBio]]|accessdate=26 September 2016}}</ref> <ref>{{cite web|title=Getting your data out there: Data publishing & data standards with iDigBio|url=https://www.idigbio.org/content/getting-your-data-out-there-data-publishing-data-standards-idigbio|publisher=[[iDigBio]]|accessdate=26 September 2016}}</ref>\n\n== See also ==\n* [[Darwin Core Archive]]\n* [[Biodiversity Information Standards]] (TDWG)\n* [[Biodiversity]]\n* [[Biodiversity informatics]]\n* [[Metadata standards]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://rs.tdwg.org/dwc/terms/index.htm Darwin Core Quick Reference Guide]\n*[https://github.com/tdwg/dwc/ Darwin Core Development Site]\n*[http://www.tdwg.org/activities/darwincore/ Official Darwin Core Website]\n*[http://www.tdwg.org/fileadmin/subgroups/dwc/exec_summary_dwc.doc Executive Summary of Darwin Core]\n\n[[Category:Bioinformatics]]\n[[Category:Knowledge representation]]\n[[Category:Interoperability]]\n[[Category:Metadata standards]]']
['Upper ontology', '3200382', '{{multiple issues|\n{{more footnotes|date=February 2011}}\n{{essay-like|date=October 2010}}\n}}\n\nIn [[information science]], an \'\'\'upper ontology\'\'\' (also known as a \'\'\'top-level ontology\'\'\' or \'\'\'foundation ontology\'\'\') is an [[Ontology (computer science)|ontology]] (in the sense used in information science) which consists of very general terms (such as "object", "property", "relation") that are common across all domains.  An important function of an upper ontology is to support broad [[semantic interoperability]] among a large number of domain-specific ontologies by providing a common starting point for the formulation of definitions. Terms in the domain ontology are ranked "under" the terms in the upper ontology, and the former stand to the latter in subclass relations.\n\nA number of upper ontologies have been proposed, each with its own proponents. Each upper ontology can be considered as a computational implementation of [[natural philosophy]], which itself is a more empirical method for investigating the topics within the philosophical discipline of [[physical ontology]].\n\n[[Library classification]] systems predate upper ontology systems. Though library classifications organize and categorize knowledge using general concepts that are the same across all knowledge domains, neither system is a replacement for the other.\n\n==Development==\n\nAny standard foundational ontology is likely to be contested among different groups, each with their own idea of "what exists". One factor exacerbating the failure to arrive at a common approach has been the lack of open-source applications that would permit the testing of different ontologies in the same computational environment.  The differences have thus been debated largely on theoretical grounds, or are merely the result of personal preferences. Foundational ontologies can however be compared on the basis of adoption for the purposes of supporting interoperability across domain ontologies.\n\nNo particular upper ontology has yet gained widespread acceptance as a [[de facto]] standard.  Different organizations have attempted to [[#Available ontologies|define standards]] for specific domains.  The \'[[Process Specification Language]]\' (PSL) created by the [[National Institute for Standards and Technology]] (NIST) is one example.\n\nAnother important factor leading to the absence of wide adoption of any existing upper ontology is the complexity. Some upper ontologies -- [[Cyc]] is often cited as an example in this regard -- are very large, ranging up to thousands of elements (classes, relations), with complex interactions among them and with a complexity similar to that of a human [[natural language]], and the learning process can be even longer than for a natural language because of the unfamiliar format and logical rules.  The motivation to overcome this learning barrier is largely absent because of the paucity of publicly accessible examples of use.  As a result, those building domain ontologies for local applications tend to create the simplest possible domain-specific ontology, not related to any upper ontology.  Such domain ontologies may function adequately for the local purpose, but they are very time-consuming to relate accurately to other domain ontologies. \n\nTo solve this problem some genuinely top level ontologies have been developed, which are deliberately designed to have minimal overlap with any domain ontologies. Examples are [[Basic Formal Ontology]] and the [[Domain Ontology for Linguistic and Cognitive Engineering | DOLCE]] (see below).\n\n===Arguments for the infeasibility of an upper ontology===\n{{unreferenced section|date=December 2016}}\nHistorically, many attempts in many societies{{which?|date=December 2016}} have been made to impose or define a single set of concepts as more primal, basic, foundational, authoritative, true or rational than all others. A common objection{{By whom?|date=December 2016}} to such attempts points out that humans lack the sort of  transcendent perspective - or \'\'[[God\'s eye view]]\'\' - that would be required to achieve this goal. Humans are bound by language or culture, and so lack the sort of objective perspective from which to observe the whole terrain of concepts and derive any one standard.\n\nAnother objection is the problem of formulating definitions. Top level ontologies are designed to maximize support for interoperability across a large number of terms. Such ontologies must therefore consist of terms expressing very general concepts, but such concepts are so basic to our understanding that there is no way in which they can be defined, since the very process of definition implies that a less basic (and less well understood) concept is defined in terms of concepts that are more basic and so (ideally) more well understood. Very general concepts can often only be elucidated, for example by means of examples, or paraphrase. \n\n* There is no self-evident way of dividing the world up into [[concept]]s, and certainly no non-controversial one\n* There is no neutral ground that can serve as a means of translating between specialized (or "lower" or "application-specific") ontologies\n* Human [[language]] itself is already an arbitrary approximation of just one among many possible conceptual maps.  To draw any \'\'necessary correlation\'\' between [[English language|English]] words and any number of intellectual concepts we might like to represent in our ontologies is just asking for trouble. ([[WordNet]], for instance, is successful and useful precisely because it does not pretend to be a general-purpose upper ontology; rather, it is a tool for semantic / syntactic / linguistic disambiguation, which is richly embedded in the particulars and peculiarities of the English language.)\n* Any hierarchical or topological representation of concepts must begin from some ontological, [[epistemology|epistemological]], linguistic, cultural, and ultimately pragmatic perspective.  Such pragmatism does not allow for the exclusion of politics between persons or groups, indeed it requires they be considered as perhaps more basic primitives than any that are represented.\n\nThose{{who?|date=December 2016}} who doubt the feasibility of general purpose ontologies are more inclined to ask “what specific purpose do we have in mind for this conceptual map of entities and what practical difference will this ontology make?”  This pragmatic philosophical position surrenders all hope of devising the encoded ontology version of “everything that is the case,” ([[Wittgenstein]], [[Tractatus Logico-Philosophicus]]).\n\nFinally there are objections similar to those against [[artificial intelligence]]{{From whom?|date=December 2016}}.  Technically, the complex concept acquisition and the social / linguistic interactions of human beings suggests any axiomatic foundation of "most basic" concepts must be cognitive, biological or otherwise difficult to characterize since we don\'t have axioms for such systems.  Ethically, any general-purpose ontology could quickly become an actual tyranny by recruiting adherents into a political program designed to propagate it and its funding means, and possibly defend it by violence.  Historically, inconsistent and irrational belief systems have proven capable of commanding obedience to the detriment or harm of persons both inside and outside a society that accepts them.  How much more harmful would a consistent rational one be, were it to contain even one or two basic assumptions incompatible with human life?\n\n===Arguments for the feasibility of an upper ontology===\n{{unreferenced section|date=November 2014}}\nMany of those who doubt the possibility of developing wide agreement on a common upper ontology fall into one of two traps:\n# they assert that there is no possibility of universal agreement on any conceptual scheme; but they argue that a practical common ontology does not need to have universal agreement, it only needs a large enough user community (as is the case for human languages) to make it profitable for developers to use it as a means to general interoperability, and for third-party developer to develop utilities to make it easier to use; and\n# they point out that developers of data schemes find different representations congenial for their local purposes; but they do not demonstrate that these different representation are in fact logically inconsistent.\n\nIn fact, different representations of assertions about the real world (though not philosophical models), if they accurately reflect the world, must be logically consistent, even if they focus on different aspects of the same physical object or phenomenon.  If any two assertions about the real world are logically inconsistent, one or both must be wrong, and that is a topic for experimental investigation, not for ontological representation.  In practice, representations of the real world are created as and known to be approximations to the basic reality, and their use is circumscribed by the limits of error of measurements in any given practical application.  Ontologies are entirely capable of representing approximations, and are also capable of representing situations in which different approximations have different utility.  Objections based on the different ways people perceive things attack a simplistic, impoverished view of ontology.  The objection that there are logically incompatible models of the world are true, but in an upper ontology those different models can be represented as different theories, and the adherents of those theories can use them in preference to other theories, while preserving the logical consistency of the \'\'necessary\'\' assumptions of the upper ontology.  The \'\'necessary\'\' assumptions provide the logical vocabulary with which to specify the meanings of all of the incompatible models.  It has never been demonstrated that incompatible models cannot be properly specified with a common, more basic set of concepts, while there are examples of incompatible theories that can be logically specified with only a few basic concepts.\n\nMany of the objections to upper ontology refer to the problems of life-critical decisions or non-axiomatized  problem areas such as law or medicine or politics that are difficult even for humans to understand.  Some of these objections do not apply to physical objects or standard abstractions that are defined into existence by human beings and closely controlled by them for mutual good, such as standards for electrical power system connections or the signals used in traffic lights.  No single general [[metaphysics]] is required to agree that some such standards are desirable.  For instance, while time and space can be represented many ways, some of these are already used in interoperable artifacts like maps or schedules.\n\nObjections to the feasibility of a common upper ontology also do not take into account the possibility of forging agreement on an ontology that contains all of the \'\'primitive\'\' ontology elements that can be combined to create any number of more specialized concept representations.  Adopting this tactic permits effort to be focused on agreement only on a limited number of ontology elements. By agreeing on the meanings of that inventory of basic concepts, it becomes possible to create and then accurately and automatically interpret an infinite number of concept representations as combinations of the basic ontology elements.  Any domain ontology or database that uses the elements of such an upper ontology to specify the meanings of its terms will be automatically and accurately interoperable with other ontologies that use the upper ontology, even though they may each separately define a large number of domain elements not defined in other ontologies.  In such a case, proper interpretation will require that the logical descriptions of domain-specific elements be transmitted along with any data that is communicated; the data will then be automatically interpretable because the domain element descriptions, based on the upper ontology, will be properly interpretable by any system that can properly use the upper ontology.  In effect elements in different domain ontologies can be *translated* into each other using the common upper ontology. An upper ontology based on such a set of primitive elements can include alternative views, provided that they are logically compatible.  Logically incompatible models can be represented as alternative theories, or represented in a specialized extension to the upper ontology.  The proper use of alternative theories is a piece of knowledge that can itself be represented in an ontology.  Users that develop new domain ontologies and find that there are semantic primitives needed for their domain but missing from the existing common upper ontology can add those new primitives by the accepted procedure, expanding the common upper ontology as necessary.\n\nMost proponents{{Who|date=April 2015}} of an upper ontology argue that several good ones may be created with perhaps different emphasis.  Very few are actually arguing to discover just one within natural language or even an academic field.  Most are simply standardizing some existing communication.  Another view advanced is that there is almost total overlap of the different ways that upper ontologies have been formalized, in the sense that different ontologies focus on a different aspect of the same entities, but the different views are complementary and not contradictory to each other; as a result, an internally consistent ontology that contains all the views, with means of translating the different views into the other, is feasible.  Such an ontology has not thus far been constructed, however, because it would require a large project to develop so as to include all of the alternative views in the separately developed upper ontologies, along with their translations.  The main barrier to construction of such an ontology is not the technical issues, but the reluctance of funding agencies to provide the funds for a large enough consortium of developers and users.\n\nSeveral common arguments against upper ontology can be examined more clearly by separating issues of concept definition (ontology), language (lexicons), and facts (knowledge).  For instance, people have different terms and phrases for the same concept.  However, that does not necessarily mean that those people are referring to different concepts.  They may simply be using different language or idiom.  Formal ontologies typically use linguistic labels to refer to concepts, but the terms that label ontology elements mean no more and no less than what their axioms say they mean.  Labels are similar to variable names in software, evocative rather than definitive.  The proponents of a common upper ontology point out that the meanings of the elements (classes, relations, rules) in an ontology depend only on their [[logical form]], and not on the labels, which are usually chosen merely to make the ontologies more easily usable by their human developers.  In fact, the labels for elements in an ontology need not be words - they could be, for example, images of instances of a particular type, or videos of an action that is represented by a particular type.  It cannot be emphasized too strongly that words are *not* what are represented in an ontology, but entities in the real world, or abstract entities (concepts) in the minds of people.  Words are not equivalent to ontology elements, but words *label* ontology elements.  There can be many words that label a single concept, even in a single language (synonymy), and there can be many concepts labeled by a single word (ambiguity).  Creating the mappings between human language and the elements of an ontology is the province of Natural Language Understanding.  But the ontology itself stands independently as a logical and computational structure.  For this reason, finding agreement on the structure of an ontology is actually easier than developing a controlled vocabulary, because all different interpretations of a word can be included, each *mapped* to the same word in the different terminologies.\n\nA second argument is that people believe different things, and therefore can\'t have the same ontology.  However, people can assign different truth values to a particular assertion while accepting the validity of certain underlying claims, facts, or way of expressing an argument with which they disagree. (Using, for instance, the issue/position/argument form.) This objection to upper ontologies ignores the fact that a single ontology can represent different belief systems, representing them as different belief systems, without taking a position on the validity of either.\n\nEven arguments about the existence of a thing require a certain sharing of a concept, even though its existence in the real world may be disputed.  Separating belief from naming and definition also helps to clarify this issue, and show how concepts can be held in common, even in the face of differing belief.  For instance, [[wiki]] as a medium may permit such confusion but disciplined users can apply [[dispute resolution]] methods to sort out their conflicts.  It is also argued that most people share a common set of "semantic primitives", fundamental concepts, to which they refer when they are trying to explain unfamiliar terms to other people.  An ontology that includes representations of those semantic primitives could in such a case be used to create logical descriptions of any term that a person may wish to define logically.   That ontology would be one form of upper ontology, serving as a logical "interlingua" that can translate ideas in one terminology to its [[logical equivalence|logical equivalent]] in another terminology.\n\nAdvocates{{Who|date=April 2015}} argue that most disagreement about the viability of an upper ontology can be traced to the conflation of ontology, language and knowledge, or too-specialized areas of knowledge: many people, or agents or groups will have areas of their respective internal ontologies that do not overlap.  If they can cooperate and share a conceptual map at all, this may be so very useful that it outweighs any disadvantages that accrue from sharing.  To the degree it becomes harder to share concepts the deeper one probes, the more valuable such sharing tends to get.  If the problem is as basic as opponents of upper ontologies claim, then, it also applies to a group of humans trying to cooperate, who might need machine assistance to communicate easily.\n\nIf nothing else, such ontologies are implied by [[machine translation]], used when people cannot practically communicate.  Whether "upper" or not, these seem likely to proliferate.\n\n==Available upper ontologies==\n\n===Basic Formal Ontology (BFO)===\n{{main|Basic Formal Ontology}}\nThe Basic Formal Ontology (BFO) framework developed by [[Barry Smith (academic and ontologist)|Barry Smith]] and his associates consists of a series of sub-ontologies at different levels of granularity. The ontologies are divided into two varieties: relating to continuant entities such as three-dimensional enduring objects, and occurrent entities (primarily) processes conceived as unfolding in successive phases through time. BFO thus incorporates both three-dimensionalist and four-dimensionalist perspectives on reality within a single framework. Interrelations are defined between the two types of ontologies in a way which gives BFO the facility to deal with both static/spatial and dynamic/temporal features of reality. A continuant domain ontology descending from BFO can be conceived as an inventory of entities existing at a time. Each occurrent ontology can be conceived as an inventory of processes unfolding through a given interval of time. Both BFO itself and each of its extension sub-ontologies can be conceived as a window on a certain portion of reality at a given level of granularity. More than [http://ifomis.uni-saarland.de/bfo/users 200 extension ontologies] of BFO have been created, applying the BFO architecture to different domains through the strategy of downward population. The Cell Ontology, for example, populates downward from BFO by importing the BFO branch terminating with object, and defining a cell as a subkind of object. Other examples of ontologies extending BFO are the [[Ontology for Biomedical Investigations]] (OBI) and the ontologies of the [[OBO Foundry|Open Biomedical Ontologies Foundry]]. In addition to these examples, BFO and extensions are increasingly being use in defense and security domains, for example in the [http://milportal.org AIRS framework]. BFO serves as the upper level of the Sustainable Development Goals (SDG) Interface Ontology developed by the [http://uneplive.unep.org/portal United Nations Environment Programme]. BFO has been documented in the textbook [http://mitpress.mit.edu/building-ontologies Building Ontologies with Basic Formal Ontology], published by MIT Press in 2015.\n\n===BORO===\n{{main|BORO}}\nBusiness Objects Reference Ontology is an upper ontology designed for developing ontological or semantic models for large complex operational applications that consists of a top ontology as well as a process for constructing the ontology.  It is built upon a series of clear [[metaphysical choices]]  to provide a solid (metaphysical) foundation. A key choice was for an [[Extension (metaphysics)|extensional]] (and hence, [[Spacetime|four-dimensional]]) [[ontology]] which provides it a simple [[criteria of identity]]. Elements of it have appeared in a number of standards. For example, the ISO standard, [[ISO 15926]] – Industrial automation systems and integration – was heavily influenced by an early version. The [[IDEAS Group|IDEAS]] (International Defence Enterprise Architecture Specification for exchange) standard is based upon BORO, which in turn was used to develop [[DODAF]] 2.0.\n\n===CIDOC Conceptual Reference Model===\n{{main|CIDOC Conceptual Reference Model}}\nAlthough "CIDOC object-oriented Conceptual Reference Model" (CRM) is a [[Ontology (information science)#Domain ontologies and upper ontologies|domain ontology]], specialised to the purposes of representing cultural heritage, a subset called CRM Core is a generic upper ontology, including:<ref>{{cite web|title=Graphical Representation of core CRM form|url=http://www.cidoc-crm.org/cidoc_core_graphical_representation/graphical_representation.html|publisher=[[CIDOC]]}}</ref><ref>{{cite web|title=Definition of the CIDOC Conceptual Reference Model, Version 5.0.4|url=http://www.cidoc-crm.org/html/5.0.4/cidoc-crm.html#_Toc310250785|publisher=[[CIDOC]]|date=November 2011}}</ref>\n* Space-Time – title/identifier, place, era/period, time-span, relationship to persistent items\n* Events – title/identifier, beginning/ending of existence, participants (people, either individually or in groups), creation/modification of things (physical or conceptional), relationship to persistent items\n* Material Things – title/identifier, place, the information object the material thing carries, part-of relationships, relationship to persistent items\n* Immaterial Things – title/identifier, information objects (propositional or symbolic), conceptional things, part-of relationships\n\nA persistent item is a physical or conceptional item that has a persistent identity recognized within the duration of its existence by its identification rather than by its continuity or by observation. A persistent item is comparable to an endurant.<br/>A propositional object is a set of statements about real or imaginary things.<br/>A symbolic object is a sign/symbol or an aggregation of signs or symbols.\n\n===COSMO===\n[[Common Semantic Model|COSMO]] (COmmon Semantic MOdel, available at http://micra.com/COSMO/COSMO.owl) is an ontology that was initiated as a project of the COSMO working group of the Ontology and taxonomy Coordinating Working Group, with the goal of developing a foundation ontology that can serve to enable broad general [[Semantic Interoperability]]. The current version is an OWL ontology, but a Common-Logic compliant version is anticipated in the future. The ontology and explanatory files are available at the COSMO site. The goal of the COSMO working group was to develop a foundation ontology by a collaborative process that will allow it to represent all of the basic ontology elements that all members feel are needed for their applications. The development of COSMO is fully open, and any comments or suggestions from any sources are welcome. After some discussion and input from members in 2006, the development of the COSMO has been continued primarily by Patrick Cassidy, the chairman of the COSMO Working Group. Contributions and suggestions from any interested party are still welcome and encouraged. Many of the types (OWL classes) in the current COSMO have been taken from the OpenCyc OWL version 0.78, and from the SUMO. Other elements were taken from other ontologies (such as BFO and DOLCE), or developed specifically for COSMO. Development of the COSMO initially focused on including representations of all of the words in the [[Longman Dictionary of Contemporary English]] (LDOCE) controlled [[defining vocabulary]] (2148 words). These words are sufficient to define (linguistically) all of the entries in the LDOCE. It is hypothesized that the ontological representations of the concepts represented by those terms will be sufficient to specify the meanings of any specialized ontology element, thereby serving as a basis for general [[Semantic Interoperability]].  Interoperability via COSMO is enabled by using the COSMO (or an ontology derived from it) as an interlingua by which other domain ontologies can be translated into each other\'s terms and thereby accurately communicate. As new domains are linked into COSMO, additional semantic primitives may be recognized and added to its structure.  The current (January 2016) OWL version of COSMO has over 8000 types (OWL classes), over 1000 relations, and over 3000 restrictions.  The COSMO itself (COSMO.owl) and other related and explanatory files can be obtained at http://micra.com/COSMO.\n\n===Cyc===\n{{main|Cyc}}\nA well-known and quite comprehensive ontology available today is [[Cyc]], a proprietary system under development since 1986, consisting of a foundation ontology and several domain-specific ontologies (called \'\'microtheories\'\'). A subset of that ontology has been released for free under the name [[Cyc#OpenCyc|OpenCyc]], and a more or less unabridged version is made available for free non-commercial use under the name [[Cyc#ResearchCyc|ResearchCyc]].\n\n=== DOLCE ===\nDescriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) is the first module of the WonderWeb foundational ontologies library,<ref>http://www.loa-cnr.it/old/Papers/D18.pdf</ref> developed by Nicola Guarino and his associates at the Laboratory for Applied Ontology (LOA). As implied by its acronym, DOLCE has a clear \'\'cognitive bias\'\', in that it aims at capturing the ontological categories underlying [[natural language]] and human [[common sense]]. DOLCE, however, does not commit to a strictly [[referentialist]] metaphysics related to the intrinsic nature of the world. Rather, the categories it introduces are thought of as cognitive artifacts, which are ultimately depending on human perception, cultural imprints and social conventions. In this sense, they intend to be just \'\'descriptive\'\' (vs \'\'prescriptive\'\') notions, that assist in making already formed conceptualizations explicit.\n\n===General Formal Ontology (GFO)===\n{{main|General formal ontology}}\nThe general formal ontology (GFO), developed by Heinrich Herre and his colleagues of the research group Onto-Med in [[Leipzig]], is a realistic ontology integrating processes and objects. It attempts to include many aspects of recent philosophy, which is reflected both in its taxonomic tree and its axiomatizations. GFO allows for different axiomatizations of its categories (such as the existence of [[atomic time-interval]]s vs. [[dense time]]). The basic principles of GFO are published in the Onto-Med Report Nr. 8 and in "General Formal Ontology (GFO): A Foundational Ontology for Conceptual Modelling".<ref>http://www.onto-med.de/Archiv/ontomed2002/en/publications/scientific-reports/om-report-no8.pdf</ref><ref>http://www.onto-med.de/publications/2010/gfo-basic-principles.pdf</ref>\n\nTwo GFO specialties, among others, are its account of persistence and its time model. Regarding persistence, the distinction between endurants (objects) and perdurants (processes) is made explicit within GFO by the introduction of a special category, a persistent<!-- [sic]?, not persistAnt? -->.<ref>http://www.onto-med.de/en/theories/gfo/part1/node20.html</ref> A persistant<!-- [sic], not persistEnt --> is a special category with the intention that its instances "remain identical" (over time). With respect to time, time intervals are taken as primitive in GFO, and time-points (called "time boundaries") as derived. Moreover, time-points may coincide, which is convenient for modelling instantaneous changes.\n\n===gist===\n\ngist is developed and supported by Semantic Arts.  gist (not an acronym – it means to get the essence of) is a “minimalist upper ontology”.  gist is targeted at enterprise information systems, although it has been applied to healthcare delivery applications.  \nThe major attributes of gist are:\n# it is small (there are 140 classes and 127 properties)\n# it is comprehensive (most enterprises will not find the need to create additional primitive classes, but will find that most of their classes can be defined and derived from gist)\n# it is robust – all the classes descend from 12 primitive classes, which are mostly mutually disjoint.  This aids a great deal in subsequent error detection.  There are 1342 axioms, and it uses almost all of the DL constructs (it is SROIQ(D) )\n# it is concrete – most upper ontologies start with abstract philosophical concepts that users must commit to in order to use the ontology.  Gist starts with concrete classes that most people already do, or reasonably could agree with, such as Person, Organization, Document, Time, UnitOfMeasure and the like) \n# it is unambiguous – ambiguous terms (such as “term”) have been removed as they are often overloaded and confused.  Also terms that frequently have different definitions at different enterprises (such as customer and order) have been removed, also to reduce ambiguity.\n# it is understandable – in addition to being built on concrete, generally understood primitives, it is extremely modular.  The 140 classes are implemented in 18 modular ontologies, each can easily be understood in its entirety, and each imports only the other modules that it needs. \ngist has been used to build Enterprise Ontologies for a number of major commercial and governmental agencies including:  Procter & Gamble, Sentara Healthcare, Washington State Department of Labor & Industries, LexisNexis, Sallie Mae and two major Financial Services firms.\ngist is freely available with a Creative Commons share alike license.  There are 18 small ontologies that make up gist.  Gist can be downloaded all at once by loading or importing gistCore at gist7.   \ngist is actively maintained, and has been in use for 10 years. As of May 2015 it is at version 7.1.1.<ref>{{cite web|url=http://semanticarts.com/gist|title=gist home page|author=Semantic Arts}}</ref>\n\ngist was the subject of a paper exploring how to bridge modeling differences between ontologies <ref>{{cite web|url=http://www.researchgate.net/profile/Anthony_Cohn/publication/221235042_Utility_Ontology_Development_with_Formal_Concept_Analysis/links/0912f50cbb29adba1f000000.pdf#page=163|title=Complexity of Reasoning with Expressive Ontology Mappings|author1=Chiara Ghidini |author2=Luciano Serafini |author3=Segio Tessaris }}</ref>\nIn a paper describing the OQuaRE methodology for evaluating ontologies, the gist unit of measure ontology scored the highest in the manual evaluation against 10 other unit of measure ontologies,  and scored above average in the automated evaluation.  The authors stated "This ontology could easily be tested and validated, its knowledge could be effectively reused and adapted for different specified environments" <ref>{{cite web|url=http://www.acs.org.au/__data/assets/pdf_file/0015/14118/JRPIT43.2.159.pdf|title=OQuaRE: A SQuaRE-based Approach for Evaluating the Quality of Ontologies|author1=Astrid Duque-Ramos  |author2=Jesualdo Tomas Fernandez-Breis |lastauthoramp=yes }}</ref>\n\n===IDEAS===\nThe upper ontology developed by the [[IDEAS Group]] is [[higher-order]], [[extensional]] and [[4D ontology|4D]]. It was developed using the [[BORO Method]]. The IDEAS ontology is not intended for reasoning and inference purposes; its purpose is to be a precise model of business.\n\n===ISO 15926===\n{{main|ISO 15926}}\nISO 15926 is an International Standard for the representation of process plant life-cycle information. This representation is specified by a generic, conceptual data model that is suitable as the basis for implementation in a shared database or data warehouse. The data model is designed to be used in conjunction with reference data: standard instances that represent information common to a number of users, process plants, or both. The support for a specific life-cycle activity depends on the use of appropriate reference data in conjunction with the data model. To enable integration of life-cycle information the model excludes all information constraints that are appropriate only to particular applications within the scope.\nISO 15926-2 defines a generic model with 201 entity types. It has been prepared by Technical Committee ISO/TC 184, Industrial automation systems and integration, Subcommittee SC 4, Industrial data.\n\n===MarineTLO===\nMarineTLO is an upperontology for the marine domain (also applicable to the terrestrial domain), developed by the Information Systems Laboratory at the Institute of Computer Science,\nFoundation for Research and Technology - Hellas ([[FORTH-ICS]]).\nIts purpose is to tackle the need for having integrated sets of facts about marine species,\nand thus to assist research about species and [[biodiversity]].\nIt provides a unified and coherent core model for schema mapping which enables formulating and\nanswering queries which cannot be answered by any individual source.<ref>{{cite web|url=http://www.ics.forth.gr/isl/MarineTLO|title=MarineTLO - A Top Level Ontology for the Marine/Biodiversity Domain|work=forth.gr|accessdate=22 April 2015}}</ref><ref>{{cite journal|author=Tzitzikas, Y. and Alloca, C. and Bekiari, C. and Marketakis, Y. and Fafalios, P. and Doerr, M. and Minadakis, N. and Patkos, T. and Candela, L.|title=Integrating Heterogeneous and Distributed Information about Marine Species through a Top Level Ontology|url=http://link.springer.com/chapter/10.1007/978-3-319-03437-9_29|location=Institute of Computer Science, FORTH-ICS, Greece|publisher=Springer|year=2013|pages=289–301|doi=10.1007/978-3-319-03437-9_29|journal=Communications in Computer and Information Science}}</ref>\n\n===PROTON===\nPROTON (PROTo ONtology) is a basic [[subsumption hierarchy]] which provides coverage of most of the upper-level concepts   necessary for semantic annotation, indexing, and retrieval.{{citation needed|date=November 2014}}\n\n===SUMO (Suggested Upper Merged Ontology)===\n{{main|Suggested Upper Merged Ontology}}\nThe [[Suggested Upper Merged Ontology]] (SUMO) is another comprehensive ontology project.  It includes an [[Standard upper ontology|upper ontology]], created by the [[IEEE]] working group P1600.1 (originally by [[Ian Niles]] and [[Adam Pease]]). It is extended with many domain ontologies and a complete set of links to WordNet. It is open source.\n\n===UMBEL===\n{{main|UMBEL}}\nUpper Mapping and Binding Exchange Layer ([[UMBEL]]) is an ontology of 28,000 reference concepts that maps to a simplified subset of the [[OpenCyc]] ontology, that is intended to provide a way of linking the precise OpenCyc ontology with less formal ontologies.<ref>{{cite web|url=http://www.mkbergman.com/441/the-role-of-umbel-stuck-in-the-middle-with-you/|title=The Role of UMBEL: Stuck in the Middle with You . . .|author=Mike Bergman|accessdate=2010-10-26}}</ref> It also has formal mappings to [[Wikipedia]], [[DBpedia]], [[PROTON Ontology|PROTON]] and [[GeoNames]]. It has been developed and maintained as [[open source]] by Structured Dynamics.\n\n===UFO (Unified Foundational Ontology)===\nThe Unified Foundational Ontology (UFO), developed by Giancarlo Guizzardi and associates, incorporating developments from GFO, DOLCE and the Ontology of Universals underlying OntoClean in a single coherent foundational ontology. The core categories of UFO (UFO-A) have been completely formally characterized in Giancarlo Guizzardi\'s Ph.D. thesis and further extended at the Ontology and Conceptual Modelling Research Group (NEMO) in Brazil with cooperators from Brandenburg University of Technology (Gerd Wagner) and Laboratory for Applied Ontology (LOA). UFO-A has been employed to analyze structural conceptual modeling constructs such as object types and taxonomic relations, associations and relations between associations, roles, properties, datatypes and weak entities, and parthood relations among objects. More recent developments incorporate an ontology of events in UFO (UFO-B), as well as an ontology of social and intentional aspects (UFO-C). The combination of UFO-A, B and C has been used to analyze, redesign and integrate reference conceptual models in a number of complex domains such as, for instance, Enterprise Modeling, Software Engineering, Service Science, Petroleum and Gas, Telecommunications, and Bioinformatics. Another recent development aimed towards a clear account of services and service-related concepts, and provided for a commitment-based account of the notion of service (UFO-S),<ref>Nardi, J. C., Falbo, R. D. A., Almeida, J. P. A., Guizzardi, G., Pires, L. F., van Sinderen, M. J., & Guarino, N. (2013, September). "Towards a commitment-based reference ontology for services". In Enterprise Distributed Object Computing Conference (EDOC), 2013 17th IEEE International (pp. 175-184). IEEE.</ref>\nUFO is the foundational ontology for [[OntoUML]], an ontology modeling language.\n\n===WordNet===\n{{main|WordNet}}\n[[WordNet]], a freely available database originally designed as a [[semantic network]] based on [[psycholinguistic]] principles, was expanded by addition of definitions and is now also viewed as a [[dictionary]]. It qualifies as an upper ontology by including the most general concepts as well as more specialized concepts, related to each other not only by the [[subsumption relation]]s, but by other semantic relations as well, such as part-of and cause. However, unlike Cyc, it has not been formally axiomatized so as to make the logical relations between the concepts precise. It has been widely used in [[Natural language processing]] research.\n\n===YAMATO (Yet Another More Advanced Top Ontology)===\nYAMATO is developed by Riichiro Mizoguchi, formerly at the Institute of Scientific and Industrial Research of the [[University of Osaka]], and now at the [[Japan Advanced Institute of Science and Technology]]. Major features of YAMATO are:\n# an advanced description of quality, attribute, property, and quantity,<ref>http://www.ei.sanken.osaka-u.ac.jp/hozo/onto_library/YAMATO101216.pdf</ref> \n# an ontology of representation,<ref>{{cite journal|url=http://link.springer.com/article/10.1007/BF03040960#page-1|title=Part 3: Advanced course of ontological engineering|work=springer.com|accessdate=22 April 2015|doi=10.1007/BF03040960|volume=22|pages=193–220}}</ref> \n# an advanced description of processes and events,<ref>{{cite journal | doi = 10.3233/AO-2009-0067 }}</ref> \n# the use of a theory of roles.<ref>{{cite journal | last1 = Mizoguchi | first1 = R. | last2 = Sunagawa | first2 = E. | last3 = Kozaki | first3 = K. | last4 = Kitamura | first4 = Y. | year = | title = A Model of Roles within an Ontology Development Tool: Hozo | url = http://iospress.metapress.com/content/w67u25284x0l206v/ | journal = J. of Applied Ontology | volume = 2 | issue = 2| pages = 159–179 }}</ref>\n\nYAMATO has been extensively used for developing other, more applied, ontologies such as a medical ontology,<ref>http://ceur-ws.org/Vol-833/paper9.pdf</ref> an ontology of gene,<ref>http://ceur-ws.org/Vol-897/session1-paper05.pdf</ref> an ontology of learning/instructional theories,<ref>{{cite web|url=http://edont.qee.jp/omnibus/doku.php|title=start    [OMNIBUS project]|date=6 December 2014|work=qee.jp|accessdate=22 April 2015}}</ref> an ontology of sustainability science,<ref>{{cite web|url=http://link.springer.com/article/10.1007%2Fs11625-008-0063-z|title=Toward knowledge structuring of sustainability science based on ontology engineering|work=springer.com|accessdate=22 April 2015}}</ref> and an ontology of the cultural domain.\n\n== Upper/Foundational Ontology tools==\n\n===ONSET===\n{{unreferenced section|date=November 2014}}\nONSET, the foundational ontology selection and explanation tool, assists the domain ontology developer in selecting the most appropriate foundational ontology. The domain ontology developer provides the requirements/answers one or more questions, and ONSET computes the selection of the appropriate foundational ontology and explains why. The current version (v2 of 24 April 2013) includes DOLCE, BFO, GFO, SUMO, YAMATO and GIST.\n\n===ROMULUS===\n{{unreferenced section|date=November 2014}}\nROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO. Features of ROMULUS include:\n# It provides a high-level view of the foundational ontologies with only the most general concepts common to all implemented foundational ontologies. \n# Foundational ontologies in ROMULUS are modularised.\n# Foundational ontology mediation has been performed. This includes alignment, mapping, merging, searchable metadata and an interchangeability method for foundational ontologies. \n# ROMULUS provides detailed taxonomies of each foundational ontology to allow easy browsing of foundational ontologies. \n# ROMULUS allows you to download each foundational ontology module including the integrated foundational ontologies. \n# Searchable metadata of each foundational ontology is available. \n# A comparison of the included foundational ontologies is available.\n\n==See also==\n* [[Authority control]]\n* [[Formal ontology]]\n* [[Foundations of mathematics]]\n* [[Knowledge Organization Systems]]\n* [[Library classification]]\n* [[Ontology (information science)]]\n* [[Physical ontology]]\n* [[Process ontology]]\n* [[Semantic interoperability]]\n* [[Commonsense knowledge]]\n\n==References==\n{{Reflist}}\n\n==External links==\n{{External links|date=April 2015}}\n* [http://www.onto-med.de/ontologies/gfo General Formal Ontology (GFO) homepage]\n* [http://www.loa.istc.cnr.it/ Laboratory of Applied Ontology (LOA) homepage]\n* [http://proton.semanticweb.org/ PROTON Ontology]\n* [http://ontolog.cim3.net/cgi-bin/wiki.pl?UpperOntologySummit Upper Ontology Summit (March 2006)]\n* [http://ontogenesis.knowledgeblog.org/740 What is an upper level ontology?] Knowledge Blog article, 2010.\n* [http://www.ics.forth.gr/isl/MarineTLO/ The MarineTLO ontology] What, Why, Who, Current applications, How to exploit it, Documents and Publications, Provide feedback.\n* [http://www.thezfiles.co.za/ROMULUS/Onset/webonset.html ONSET]\n* [http://www.thezfiles.co.za/ROMULUS/ ROMULUS]\n{{Computable knowledge}}\n\n{{DEFAULTSORT:Upper Ontology (Information Science)}}\n[[Category:Knowledge representation]]\n[[Category:Technical communication]]\n[[Category:Information science]]\n[[Category:Ontology (information science)]]']
['Reification (computer science)', '232423', '{{Other uses|Reification (disambiguation)}}\n\n\'\'\'Reification\'\'\' is the process by which an abstract idea about a [[computer program]] is turned into an explicit [[data model]] or other object created in a [[programming language]]. A computable/addressable object — a resource — is created in a system as a proxy for a non computable/addressable object. By means of reification, something that was previously implicit, unexpressed, and possibly inexpressible is explicitly formulated and made available to conceptual (logical or computational) manipulation. Informally, reification is often referred to as "making something a [[first-class citizen]]" within the scope of a particular system. Some aspect of a system can be reified at \'\'language design time\'\', which is related to [[Reflection (computer science)|reflection]] in programming languages. It can be applied as a stepwise refinement at \'\'system design time\'\'. Reification is one of the most frequently used techniques of [[conceptual analysis]] and [[knowledge representation]].\n\n== Reification and reflective programming languages ==\nIn the context of [[programming language]]s, reification is the process by which a user program or any aspect of a programming language that was implicit in the translated program and the run-time system, are  expressed in the language itself. This process makes it available to the program, which can inspect all these aspects as ordinary [[data]]. In [[Reflection (computer science)|reflective languages]], reification data is causally connected to the related reified aspect such that a modification to one of them affects the other. Therefore, the reification data is always a faithful representation of the related reified aspect. Reification data is often said to be made a [[first class object]]. Reification, at least partially, has been experienced in many languages to date: in early [[Lisp (programming language)|Lisp dialects]] and in current [[Prolog| Prolog dialects]], programs have been treated as data, although the causal connection has often been left to the responsibility of the programmer. In [[Smalltalk]]-80, the compiler from the source text to bytecode has been part of the run-time system since the very first implementations of the language.<ref>J. Malenfant, M. Jacques and F.-N. Demers, [http://www2.parc.com/csl/groups/sda/projects/reflection96/docs/malenfant/ref96/ref96.html A Tutorial on Behavioral Reflection and its Implementation]</ref>\n\n* The [[C (programming language)|C programming language]] reifies the low-level detail of [[memory address]]es.{{paragraph break}}Many programming language designs encapsulate the details of memory allocation in the compiler and the run-time system. In the design of the C programming language, the memory address is reified and is available for direct manipulation by other language constructs. For example, the following code may be used when implementing a memory-mapped device driver. The buffer pointer is a proxy for the memory address 0xB800000.{{paragraph break}}<source lang="c">\n char* buffer = (char*) 0xB800000;\n buffer[0] = 10; \n</source>\n* [[Functional programming languages]] based on [[lambda-calculus]] reify the concept of a procedure abstraction and procedure application in the form of the [[Lambda calculus#Lambda calculus and programming languages|Lambda expression]].\n* The [[Scheme (programming language)|Scheme]] programming language reifies [[continuations]] (approximately, the call stack).\n* In [[C Sharp (programming language)|C#]], reification is used to make [[parametric polymorphism]] implemented as generics as a first-class feature of the language.\n* In the [[Java (programming language)|Java]] programming language, there exist "reifiable types" that are "completely available at run time" (i.e. their information is not erased during compilation).<ref>[http://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.7 The Java Language Specification, section 4.7], Java SE 7 Edition</ref>\n* [[REBOL]] reifies code as data and vice versa.\n* Many languages, such as [[Lisp (programming language)|Lisp]], [[JavaScript]], and [[Curl (programming language)|Curl]], provide an [[eval|<code>eval</code> or <code>evaluate</code> procedure]] that effectively reifies the language interpreter.\n* The [[Logtalk]] framework for [[Prolog]] offers a means to explore reification in the context of [[logic programming]].\n* [[Smalltalk]] and [[Actor model|Actor languages]] permit the reification of blocks and [[message passing|messages]],<ref>{{cite web|url=http://c2.com/cgi/wiki?SmalltalkBlocksAndClosures |title=Smalltalk Blocks And Closures |publisher=C2.com |date=2009-10-15 |accessdate=2010-10-09}}</ref> which are equivalent of lambda expressions in Lisp, and [[thisContext]] which is a reification of the current executing block.\n* [[Homoiconicity|Homoiconic languages]] reify the syntax of the language itself in the form of an [[abstract syntax tree]], typically together with <code>eval</code>.\n\n== Data reification vs. data refinement ==\nData reification ([[stepwise refinement]]) involves finding a more concrete representation of the [[abstract data type]]s used in a [[formal specification]].\n\nData reification is the terminology of the [[Vienna Development Method]] (VDM) that most other people would call data refinement. An example is taking a step towards an implementation by replacing a data representation without a counterpart in the intended implementation language, such as sets, by one that does have a counterpart (such as maps with fixed domains that can be implemented by arrays), or at least one that is closer to having a counterpart, such as sequences. The VDM community prefers the word "reification" over "refinement", as the process has more to do with concretising an idea than with refining it.<ref>[https://www.cs.tcd.ie/FME/original/FAQ/vdm/part13.html Formal Methods Europe, Frequently Asked Questions, part 13].</ref>\n\nFor similar usages, see [[Reification (linguistics)]].\n\n== Reification in conceptual modeling ==\nReification is widely used in [[Conceptual model (computer science)|conceptual modeling]].<ref>Antoni Olivé, Conceptual Modeling of Information Systems, Springer Verlag, 2007.</ref> Reifying a relationship means viewing it as an entity. The purpose of reifying a relationship is to make it explicit, when additional information needs to be added to it. Consider the relationship type \'\'<code>IsMemberOf(member:Person, Committee)</code>\'\'. An instance of \'\'<code>IsMemberOf</code>\'\' is a relationship that represents the fact that a person is a member of a committee. The figure below shows an example population of \'\'<code>IsMemberOf</code>\'\' relationship in tabular form. Person \'\'P1\'\' is a member of committees \'\'C1\'\' and \'\'C2\'\'. Person \'\'P2\'\' is a member of committee \'\'C1\'\' only. [[File:reification example1.png|500px|thumb|Example population of <code>IsMemberOf</code> relationship in tabular form. Person P1 is a member of committees C1 and C2. Person P2 is a member of committee C1 only.]]\n\nThe same fact, however, could also be viewed as an entity. Viewing a relationship as an entity, one can say that the entity reifies the relationship. This is called reification of a relationship. Like any other entity, it must be an instance of an entity type. In the present example, the entity type has been named <code>Membership</code>. For each instance of \'\'<code>IsMemberOf</code>\'\', there is one and only one instance of \'\'<code>Membership</code>\'\', and vice versa. Now, it becomes possible to add more information to the original relationship. As an example, we can express the fact that "person p1 was nominated to be the member of committee c1 by person p2". Reified relationship \'\'<code>Membership</code>\'\' can be used as the source of a new relationship \'\'<code>IsNominatedBy(Membership, Person)</code>\'\'.\n\nFor related usages see [[Reification (knowledge representation)]].\n\n== Reification in Unified Modeling Language (UML) ==\n[[File:reification example2.png|400px|thumb|The UML [[class diagram]] for the Membership example.]] [[Unified Modeling Language|UML]] provides an \'\'association class\'\' construct for defining reified relationship types. The association class is a single model element that is both a kind of association and a kind of a class.<ref>\'\'Unified Modeling Language, UML superstructure\'\', Object Management Group, 2007-11-02.</ref> The association and the entity type that reifies are both the same model element. Note that attributes cannot be reified.\n\n== Reification on Semantic Web ==\n\n=== RDF and OWL ===\nIn [[Semantic Web]] languages, such as [[Resource Description Framework]] (RDF) and [[Web Ontology Language]] (OWL), a statement is a binary relation. It is used to link two individuals or an individual and a value. Applications sometimes need to describe other RDF statements, for instance, to record information like when statements were made, or who made them, which is sometimes called "[[provenance]]" information. As an example, we may want to represent properties of a relation, such as our certainty about it, severity or strength of a relation, relevance of a relation, and so on.\n\nThe example from the conceptual modeling section describes a particular person with <code>URIref person:p1</code>, who is a member of the <code>committee:c1</code>. The RDF triple from that description is\n<source lang="sparql">\n  person:p1   committee:isMemberOf   committee:c1 .\n</source>\nConsider to store two further facts: (i) to record who nominated this particular person to this committee (a statement about the membership itself), and (ii) to record who added the fact to the database (a statement about the statement).\n\nThe first case is a case of classical reification like above in UML: reify the membership and store its attributes and roles etc.:\n\n<source lang="sparql">\n committee:Membership        rdf:type              owl:Class .\n committee:membership12345   rdf:type              committee:Membership .\n committee:membership12345   committee:ofPerson    person:p1 .\n committee:membership12345   committee:inCommittee committee:c1 .\n person:p2                   committee:nominated   committee:membership12345 .  \n</source>\n\nAdditionally, RDF provides a built-in vocabulary intended for describing RDF statements. A description of a statement using this vocabulary is called a reification of the statement. The RDF reification vocabulary consists of the type <code>rdf:Statement</code>, and the properties <code>rdf:subject</code>, <code>rdf:predicate</code>, and <code>rdf:object</code>.<ref name="rdf">{{cite web|url=http://www.w3.org/TR/2004/REC-rdf-primer-20040210/#reification |title=RDF Primer |publisher=W3.org |date= |accessdate=2010-10-09}}</ref>\n\nUsing the reification vocabulary, a reification of the statement about the person\'s membership would be given by assigning the statement a URIref such as <code>committee:membership12345</code> so that describing statements can be written as follows:\n<source lang="sparql">\n committee:membership12345Stat   rdf:type        rdf:Statement .\n committee:membership12345Stat   rdf:subject     person:p1 .\n committee:membership12345Stat   rdf:predicate   committee:isMemberOf . \n committee:membership12345Stat   rdf:object      committee:c1 .\n</source>\nThese statements say that the resource identified by the <code>URIref committee:membership12345Stat</code> is an RDF statement, that the subject of the statement refers to the resource identified by <code>person:p1</code>, the predicate of the statement refers to the resource identified by <code>committee:isMemberOf</code>, and the object of the statement refers to the resource <code>committee:c1</code>. Assuming that the original statement is actually identified by <code>committee:membership12345</code>, it should be clear by comparing the original statement with the reification that the reification actually does describe it. The conventional use of the RDF reification vocabulary always involves describing a statement using four statements in this pattern. Therefore, they are sometimes referred to as the "reification quad".<ref name="rdf"/>\n\nUsing reification according to this convention, we could record the fact that <code>person:p3</code> added the statement to the\ndatabase by\n<source lang="sparql">\n  person:p3    committee:addedToDatabase    committee:membership12345Stat .\n</source>\nIt is important to note that in the conventional use of reification, the subject of the reification triples is assumed to identify a particular instance  of a triple in a particular RDF document, rather than some arbitrary triple having the same subject, predicate, and object. This particular convention is used because reification is intended for expressing properties such as dates of composition and source information, as in the examples given already, and these properties need to be applied to specific instances of triples. \nNote that the described triple <code>(subject predicate object)</code> itself is not implied by such a reification quad (and it is not necessary that it actually exists in the database). This allows also to use this mechanism to express which triples do \'\'not\'\' hold.\n\nThe power of the reification vocabulary in RDF is restricted by the lack of a built-in means for assigning URIrefs to statements, so in order to express "provenance" information of this kind in RDF, one has to use some mechanism (outside of RDF) to assign URIs to individual RDF statements, then make further statements about those individual statements, using their URIs to identify them.<ref name="rdf"/>\n\n=== Reification in Topic Maps ===\nIn an [[Topic Maps|XML Topic Map]] (XTM), only a topic can have a name or play a role in an association. One may use an association to make an assertion about a topic, but one cannot directly make assertions about that assertion. However, it is possible to create a topic that reifies a non-topic construct in a map, thus enabling the association to be named and treated as a topic itself.<ref>[http://www.techquila.com/practical_intro.html Practical Introduction into Topic Maps].</ref>\n\n=== Reification and n-ary relations ===\nIn Semantic Web languages, such as RDF and OWL, a property is a binary relation used to link two individuals or an individual and a value. However, in some cases, the natural and convenient way to represent certain concepts is to use relations to link an individual to more than just one individual or value. These relations are called [[n-ary relations]]. Examples are representing relations among multiple individuals, such as a committee, a person who is a committee member and another person who has nominated the first person to become the committee member, or a buyer, a seller, and an object that was bought when describing a purchase of a book.\n\nA more general approach to reification is to create an explicit new class and n new properties to represent an n-ary relation, making an instance of the relation linking the n individuals an instance of this class. This approach can also be used to represent provenance information and other properties for an individual relation instance.<ref>{{cite web|url=http://www.w3.org/TR/swbp-n-aryRelations/ |title=W3C Defining N-ary relations on Semantic Web |publisher=W3.org |date= |accessdate=2010-10-09}}</ref>\n<source lang="turtle">\n :p1\n      a       :Person ;\n      :has_membership _:membership_12345 .\n _:membership_12345\n      a       :Membership ;\n      :committee :c1;\n      :nominated_by :p2 .\n</source>\n\n=== Reification vs. quotation ===\nIt is also important to note that the reification described here is not the same as "quotation" found in other languages. Instead, the reification describes the relationship between a particular instance of a triple and the resources the triple refers to. The reification can be read intuitively as saying "this RDF triple talks about these things", rather than (as in quotation) "this RDF triple has this form." For instance, in the reification example used in this section, the triple:\n<source lang="sparql">\n  committee:membership12345   rdf:subject   person:p1 .\n</source>\ndescribing the <code>rdf:subject</code> of the original statement says that the subject of the statement is the resource (the person) identified by the URIref <code>person:p1</code>. It does not state that the subject of the statement is the URIref itself (i.e., a string beginning with certain characters), as quotation would.\n\n==See also==\n{{Wiktionary|reification}}\n* [[Denotational semantics]]\n* [[Formal semantics of programming languages]]\n* [[Meta-circular evaluator]]\n* [[Metamodeling]]\n* [[Metaobject]]\n* [[Metaprogramming]]\n* [[Normalization by evaluation]]\n* [[Operational semantics]]\n* [[Reflection (computer science)]]\n* [[Resource Description Framework]]\n* [[Self-interpreter]]\n* [[Topic Maps]]\n\n==References==\n{{reflist}}\n\n<!--Interwikies-->\n\n{{DEFAULTSORT:Reification (Computer Science)}}\n<!--Categories-->\n[[Category:Object-oriented programming]]\n[[Category:Formal methods terminology]]\n[[Category:Knowledge representation]]\n\n[[de:Reifikation#Informatik]]\n[[fr:Réification]]']
['Basic Formal Ontology', '18025074', 'The \'\'\'Basic Formal Ontology\'\'\' ([http://www.ifomis.org/bfo BFO]) is a formal ontological framework developed by [[Barry Smith (ontologist)|Barry Smith]] and his associates that consists in a series of sub-ontologies at different levels of granularity. The ontologies are divided into two varieties: \'\'\'continuant\'\'\' (or snapshot) ontologies, comprehending continuant entities such as three-dimensional enduring objects, and \'\'\'occurrent\'\'\' ontologies, comprehending processes conceived as extended through (or as spanning) time. BFO thus incorporates both three-dimensionalist and four-dimensionalist perspectives on reality within a single framework. Interrelations are defined between the two types of ontologies in a way which gives BFO the facility to deal with both static/spatial and dynamic/temporal features of reality. Each continuant ontology is an inventory of all entities existing at a time. Each occurrent ontology is an inventory (processory) of all the processes unfolding through a given interval of time. Both types of ontology serve as basis for a series of sub-ontologies, each of which can be conceived as a window on a certain portion of reality at a given level of granularity.\n\n==Applications of BFO==\n\nBFO has been adopted as a foundational ontology by many [http://www.ifomis.org/bfo/users projects], principally in the areas of biomedical ontology and security and defense (intelligence) ontology. An example application of BFO can be seen in the [[Ontology for Biomedical Investigations]] (OBI).\n\n==References==\n\n*Arp, R., Smith, B., and Spear, A. D. \'\'[http://mitpress.mit.edu/building-ontologies Building Ontologies with Basic Formal Ontology]\'\', Cambridge, MA: MIT Press, August 2015, xxiv + 220pp.\n* Grenon, P. and Smith, B. (2004) [http://ontology.buffalo.edu/smith/articles/SNAP_SPAN.pdf “SNAP and SPAN: Towards Dynamic Spatial Ontology”], Spatial Cognition and Computation, 4:1, 69-103.\n* Smith, B. and Grenon, P. (2004) [http://ontology.buffalo.edu/smith/articles/cornucopia.pdf “The Cornucopia of Formal-Ontological Relations”], Dialectica, 58:3, 279-296.\n*http://www.ifomis.uni-saarland.de/bfo/\n*https://github.com/bfo-ontology/BFO/wiki\n*http://ncorwiki.buffalo.edu/index.php/Basic_Formal_Ontology_2.0\n==See also==\n\n* [[Formal Ontology]]\n* [[Upper ontology]]\n\n==External links==\n\n* [http://www.ifomis.org/bfo Basic Formal Ontology at IFOMIS]\n*Katherine Munn, Barry Smith (Eds.): [http://www.ontosverlag.com/index.php?page=shop.product_details&flypage=flypage.tpl&product_id=108&category_id=13&option=com_virtuemart&Itemid=1&lang=en \'\'Applied Ontology: An Introduction\'\'], Ontos Verlag.\n*Ludger Jansen: "[http://ontology.buffalo.edu/bfo/Tendencies.pdf Tendencies and other Realizables in Medical Information Sciences]"\n*Fabian Neuhaus, Pierre Grenon, Barry Smith: "[http://ontology.buffalo.edu/bfo/SQU.pdf A Formal Theory of Substances, Qualities, and Universals]"\n*Luc Schneider: "[http://www.ifomis.org/bfo/documents/schneider-fois2010.pdf Revisiting the Ontological Square]"\n*Lars Vogt: "[http://www.biomedcentral.com/1471-2105/11/289 Spatio-structural granularity of biological material entities]"\n*Barry Smith, Werner Ceusters, Bert Klagges, Jacob Köhler, Anand Kumar, Jane Lomax, Chris Mungall, Fabian Neuhaus, Alan Rector and Cornelius Rosse: "[http://genomebiology.com/2005/6/5/R46 Relations in Biomedical Ontologies]", Genome Biology (2005), 6 (5), R46\n*Thomas Bittner, Maureen Donnelly and Barry Smith: "[http://www.acsu.buffalo.edu/~bittner3/Publications_files/Bittner-NA-2006-28.pdf A Spatio-Temporal Ontology for Geographic Information Integration]", International Journal for Geographical Information Science, 23 (6), 2009, 765-798\n* [http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0012258 Realism in Biology]\n* Smith, B. and Ceusters, W. (2010) “[http://iospress.metapress.com/content/1551884412214u67/fulltext.pdf Ontological Realism as a Methodology for Coordinated Evolution of Scientific Ontologies]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}”, Applied Ontology, 5 (2010), 139–188.\n\n\n[[Category:Knowledge representation]]\n[[Category:Information science]]\n[[Category:Ontology]]\n[[Category:Ontology (information science)]]']
['Lumpers and splitters', '558750', '{{Refimprove|date=January 2014}}\n\n\'\'\'Lumpers\'\'\' and \'\'\'splitters\'\'\' are opposing factions in any [[discipline]] that has to [[Categorization|place individual examples into rigorously defined categories]]. The lumper-splitter problem occurs when there is the need to create classifications and assign examples to them, for example schools of [[literature]], [[biology|biological]] [[taxon|taxa]] and so on. A "lumper" is an individual who takes a [[wikt:gestalt|gestalt]] view of a definition, and assigns examples broadly, assuming that differences are not as important as signature similarities. A "splitter" is an individual who takes precise definitions, and creates new categories to classify samples that differ in key ways.\n\n==Origin of the terms==\nThe earliest use of these terms was by [[Charles Darwin]], in a letter to [[J. D. Hooker]] in 1857, "(Those who make many species are the \'splitters,\' and those who make few are the \'lumpers.\')" They were introduced more widely by [[George G. Simpson]] in his 1945 work "The Principles of Classification and a Classification of Mammals." As he put it, "splitters make very small units – their critics say that if they can tell two animals apart, they place them in different genera … and if they cannot tell them apart, they place them in different species. … Lumpers make large units – their critics say that if a [[Carnivora|carnivore]] is neither a dog nor a bear, they call it a cat."<ref>{{Cite journal|last=Simpson|first=George G.|title=The Principles of Classification and a Classification of Mammals|journal=Bulletin of the AMNH|volume=85|page=23|year=1945|publisher=American Museum of Natural History|location=New York}}</ref>\n\nAnother early use can be found in the title of a 1969 paper by the medical geneticist [[Victor McKusick]]: "On lumpers and splitters, or the nosology of genetic disease."<ref>McKusick VA. On lumpers and splitters, or the nosology of genetic disease. Perspect Biol Med. 1969 Winter;12(2):298-312.</ref>\n\nReference to lumpers and splitters also appeared in a debate in 1975 between [[J. H. Hexter]] and [[John Edward Christopher Hill|Christopher Hill]], in the \'\'[[Times Literary Supplement]]\'\'. It followed from Hexter\'s detailed review of Hill\'s book \'\'Change and Continuity in Seventeenth Century England\'\', in which Hill developed [[Max Weber]]\'s argument that the rise of capitalism was facilitated by [[Calvinist]] Puritanism. Hexter objected to Hill\'s \'mining\' of sources to find evidence that supported his theories. Hexter argued that Hill plucked quotations from sources in a way that distorted their meaning. Hexter explained this as a mental habit that he called \'lumping\'. According to him, lumpers rejected differences and chose to emphasize similarities. Any evidence that did not fit their arguments was ignored as aberrant. Splitters, by contrast, emphasised differences, and resisted simple schemes. While lumpers consistently tried to create coherent patterns, splitters preferred incoherent complexity.<ref>{{Cite journal|last=Chase|first=Bob|title=Upstart Antichrist|journal=History Workshop Journal|issue=60|date=Autumn 2005|pages=202-206}}</ref>\n\n==Usage in various fields==\n\n===Biology===\n{{anchor|Lumping and splitting in biology}}\n{{main|Biological classification}}\nThe categorization and naming of a particular species should be regarded as a \'\'hypothesis\'\' about the [[evolution]]ary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be confirmed or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual [[organism]]s later identified as the same species. When two named species are agreed to be of the same species, the older species name is almost always retained dropping the newer species name honoring a convention known as "priority of nomenclature".  This form of lumping is technically called synonymization. Dividing a taxon into multiple, often new, taxa is called splitting. Taxonomists are often referred to as "lumpers" or "splitters" by their colleagues, depending on their personal approach to recognizing differences or commonalities between organisms.\n\n=== History ===\n{{main|Periodization}}\n\nIn history, lumpers are those who tend to create broad definitions that cover large periods of time and many disciplines, whereas splitters want to assign names to tight groups of inter-relationships. Lumping tends to create a more and more unwieldy definition, with members having less and less mutually in common. This can lead to definitions which are little more than conventionalities, or groups which join fundamentally different examples. Splitting often leads to "[[Distinction without a difference|distinctions without difference]]," ornate and fussy categories, and failure to see underlying similarities.\n\nFor example, in the arts, "[[Romanticism|Romantic]]" can refer specifically to a period of [[Germany|German]] poetry roughly from 1780–1810, but would exclude the later work of [[Goethe]], among other writers. In music it can mean every composer from [[Johann Nepomuk Hummel|Hummel]] through [[Sergei Rachmaninoff|Rachmaninoff]], plus many that came after.\n\n=== Software modelling ===\n[[Software engineering]] often proceeds by building models (sometimes known as [[model-driven architecture]]). A lumper is keen to generalize, and produces models with a small number of broadly defined objects. A splitter is reluctant to generalize, and produces models with a large number of narrowly defined objects. Conversion between the two styles is not necessarily symmetrical. For example, if error messages in two narrowly defined classes behave in the same way, the classes can be easily combined. But if some messages in a broad class behave differently, every object in the class must be examined before the class can be split. This illustrates the principle that "splits can be lumped more easily than lumps can be split."<ref name=Pugh2005>{{cite book|last1=Pugh|first1=Ken|title=Prefactoring|date=2005|publisher=O\'Reilly Media|pages=14–15|url=https://books.google.com/books?id=8nykB7qerJYC&pg=PA15#v=onepage&q&f=false|accessdate=2014-10-21}}</ref>\n\n=== Language classification ===\n{{main|Language classification}}\n\nThere is no agreement among [[Historical linguistics|historical linguists]] about what amount of evidence is needed for two languages to be safely classified in the same [[language family]]. For this reason, many language families have had lumper–splitter controversies, including [[Altaic languages|Altaic]], [[Pama–Nyungan languages|Pama–Nyungan]], [[Nilo-Saharan]], and most of the larger [[Classification schemes for indigenous languages of the Americas|families of the Americas]]. At a completely different level, the splitting of a [[mutually intelligible]] [[dialect continuum]] into different languages, or lumping them into one, is also an issue that continually comes up, though the consensus in contemporary linguistics is that there is no completely objective way to settle the question.\n\nSplitters regard the [[comparative method (linguistics)|comparative method]] (meaning not comparison in general, but only reconstruction of a common ancestor or [[protolanguage]]) as the only valid proof of kinship, and consider [[genetic (linguistics)|genetic]] relatedness to be the question of interest. American linguists of recent decades tend to be splitters.\n\nLumpers are more willing to admit techniques like [[mass lexical comparison]] or [[lexicostatistics]], and mass typological comparison, and to tolerate the uncertainty of whether relationships found by these methods are the result of [[linguistic divergence]] (descent from common ancestor) or [[language convergence]] (borrowing). Much long-range comparison work has been from Russian linguists like [[Vladislav Illich-Svitych]] and [[Sergei Starostin]]. In the US, [[Joseph Greenberg|Greenberg]]\'s and [[Merritt Ruhlen|Ruhlen]]\'s work has been met with little acceptance from linguists. Earlier American linguists like [[Morris Swadesh]] and [[Edward Sapir]] also pursued large-scale classifications like [[Classification schemes for indigenous languages of the Americas#Sapir .281929.29: Encyclop.C3.A6dia Britannica|Sapir\'s 1929 scheme for the Americas]], accompanied by controversy similar to that today.<ref>http://www.nostratic.ru/books/(137)ruhlen12.pdf [[Merritt Ruhlen]]: Is Algonquian Amerind?</ref>\n\n=== Liturgical studies ===\n[[Paul F. Bradshaw]] suggests that the same principles of lumping and splitting apply to the study of early Christian [[liturgy]]. Lumpers, who tend to predominate, try to find a single line of texts from the apostolic age to the fourth century (and later). Splitters see many parallel and overlapping strands which intermingle and flow apart so that there is not a single coherent path in development of liturgical texts. Liturgical texts must not be taken solely at face value; often there are hidden agendas in texts.<ref name="bradshaw">Bradshaw, Paul F., \'\'The Search for the Origins of Christian Worship\'\', Oxford University Press, 2002, p. ix. ISBN 0-19-521732-2</ref>\n\nThe Hindu religion is essentially a lumper\'s concept, sometimes also known as [[Smartism]].  Hindu splitters, and individual adherents, often identify themselves as adherents of a religion such as [[Shaivism]], [[Vaishnavism]], or [[Shaktism]] according to which deity they believe to be the supreme creator of the universe.{{Citation needed|date=June 2012}}\n\n===Philosophy===\n[[Freeman Dyson]] has suggested that "observers of the philosophical scene" can be broadly, if over-simplistically, divided into splitters and lumpers, roughly corresponding to [[materialists]], who imagine the world as divided into atoms, and [[Platonists]], who regard the world as made up of ideas.\n\n== See also ==\n* [[Evolutionary biology]]\n* [[Heterarchy]]\n* [[Prototype theory]]\n* [[Sorites paradox]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* [http://www.users.globalnet.co.uk/~rxv/infomgt/abstraction.htm#lumpersplitter Abstraction: Lumpers and Splitters]\n* [http://www.tvtropes.org/pmwiki/pmwiki.php/Main/LumperVsSplitter Lumper Vs. Splitter] on [[TV Tropes|TV Tropes, a wiki dedicated to recurring themes in fiction, metafiction, and real life]]\n\n{{DEFAULTSORT:Lumpers And Splitters}}\n[[Category:Knowledge representation]]\n[[Category:Taxonomy]]']
['OntoCAPE', '26200279', "'''OntoCAPE''' is a large-scale [[ontology (computer science)|ontology]] for the [[Domain knowledge|domain]] of [[Computer-Aided Process Engineering]] (CAPE). It can be downloaded free of charge from the [http://www.avt.rwth-aachen.de/AVT/index.php?id=730&L=1 OntoCAPE Homepage].\n\nOntoCAPE is partitioned into 62 sub-ontologies, which can be used individually or as an integrated suite. \nThe sub-ontologies are organized across different [[abstraction layer]]s, which separate general knowledge from knowledge about particular domains and applications.\n\n* The upper layers have the character of an [[Upper ontology (information science)|upper ontology]], covering general topics such  as mereotopology, systems theory, quantities and units.\n* The lower layers conceptualize the domain of chemical process engineering, covering domain-specific topics such as materials, chemical reactions, or unit operations.\n\n==Further reading==\n* Marquardt et al. (2010). [http://www.springer.com/chemistry/book/978-3-642-04654-4 ''OntoCAPE: A Re-Usable Ontology for Chemical Process Engineering'']. Springer-Verlag, Berlin Heidelberg.\n\n== External links ==\n* [http://www.avt.rwth-aachen.de/AVT/index.php?id=730&L=1 OntoCAPE Homepage]\n\n[[Category:Knowledge representation]]\n[[Category:Ontology (information science)]]"]
['Procedural reasoning system', '8233911', 'In [[artificial intelligence]], a \'\'\'procedural reasoning system\'\'\' (\'\'\'PRS\'\'\') is a framework for constructing real-time [[Reasoning system|reasoning systems]] that can perform complex tasks in dynamic environments. It is based on the notion of a [[rational agent]] or [[intelligent agent]] using the [[belief–desire–intention software model]].\n\nA user application is predominately defined, and provided to a PRS system is a set of \'\'knowledge areas\'\'.  Each knowledge area is a piece of [[procedural knowledge]] that specifies how to do something, e.g., how to navigate down a corridor, or how to plan a path (in contrast with [[robotic architectures]] where the [[computer programmer|programmer]] just provides a model of what the states of the world are and how the agent\'s primitive actions affect them).  Such a program, together with a PRS [[interpreter (computing)|interpreter]], is used to control the agent.\n\nThe interpreter is responsible for maintaining beliefs about the world state, choosing which goals to attempt to achieve next, and choosing which knowledge area to apply in the current situation.  How exactly these operations are performed might depend on domain-specific [[metaknowledge|meta-level]] knowledge areas.  Unlike traditional [[computer planning|AI planning]] systems that generate a complete plan at the beginning, and replan if unexpected things happen, PRS interleaves planning and doing actions in the world.  At any point, the system might only have a partially specified plan for the future.\n\nPRS is based on the [[BDI software agent|BDI]] or belief–desire–intention framework for intelligent agents.  Beliefs consist of what the agent believes to be true about the current state of the world, desires consist of the agent\'s goals, and intentions consist of the agent\'s current plans for achieving those goals.  Furthermore, each of these three components is typically \'\'explicitly\'\' represented somewhere within the memory of the PRS agent at runtime, which is in contrast to purely reactive systems, such as the [[subsumption architecture]].\n\n== History ==\nThe PRS concept was developed by the [[Artificial Intelligence Center]] at [[SRI International]] during the 1980s, by many workers including [[Michael Georgeff]], [[Amy L. Lansky]], and [[François Félix Ingrand]]. Their framework was responsible for exploiting and popularizing the BDI model in software for control of an [[intelligent agent]]. The seminal application of the framework was a fault detection system for the reaction control system of the [[NASA]] [[Space Shuttle Discovery]]. Development on this PRS continued at the [[Australian Artificial Intelligence Institute]] through to the late 1990s, which lead to the development of a [[C++]] implementation and extension called [[distributed multi-agent reasoning system|dMARS]].\n\n== Architecture ==\n[[Image:PRS.gif|thumb|Depiction of the PRS architecture]]\nThe system architecture of SRI\'s PRS includes the following components:\n* \'\'\'Database\'\'\' for beliefs about the world, represented using first order predicate calculus.\n* \'\'\'Goals\'\'\' to be realized by the system as conditions over an interval of time on internal and external state descriptions (desires).\n* \'\'\'Knowledge areas\'\'\' (KAs) or plans that define sequences of low-level actions toward achieving a goal in specific situations.\n* \'\'\'Intentions\'\'\' that include those KAs that have been selected for current and eventual execution.\n* \'\'\'Interpreter\'\'\' or inference mechanism that manages the system.\n\n== Features ==\nSRI\'s PRS was developed for embedded application in dynamic and real-time environments. As such it specifically addressed the limitations of other contemporary control and reasoning architectures like [[expert system]]s and the [[blackboard system]]. The following define the general requirements for the development of their PRS:<ref>\n{{cite journal\n | doi = 10.1109/64.180407\n | last = Ingrand\n | first = F.\n |author2=M. Georgeff |author3=A Rao\n  | title = An architecture for real-time reasoning and system control\n | journal = IEEE Expert: Intelligent Systems and Their Applications\n | volume = 7\n | issue = 6\n | year = 1992\n | pages = 34–44 \n | publisher = IEEE Press\n | url = http://portal.acm.org/citation.cfm?id=629535.629890 }}\n</ref>\n\n* asynchronous event handling\n* guaranteed reaction and response types\n* procedural representation of knowledge\n* handling of multiple problems\n* reactive and goal-directed behavior\n* focus of attention\n* reflective reasoning capabilities\n* continuous embedded operation\n* handling of incomplete or inaccurate data\n* handling of transients\n* modeling delayed feedback\n* operator control\n\n== Applications ==\nThe seminal application of SRI\'s PRS was a monitoring and fault detection system for the reaction control system (RCS) on the NASA space shuttle.<ref>\n{{cite conference\n  | last = Georgeff\n  | first = M. P.\n  |author2=F. F. Ingrand\n  | title = Real-time reasoning: the monitoring and control of spacecraft systems\n  | booktitle = Proceedings of the sixth conference on Artificial intelligence applications\n  | year = 1990\n  | pages = 198–204\n  | url = http://portal.acm.org/citation.cfm?id=96782 }}\n</ref> The RCS provides propulsive forces from a collection of jet thrusters and controls altitude of the space shuttle. A PRS-based fault diagnostic system was developed and tested using a simulator. It included over 100 KAs and over 25 meta level KAs. RCS specific KAs were written by space shuttle mission controllers. It was implemented on the [[Symbolics]] 3600 Series [[LISP]] machine and used multiple communicating instances of PRS. The system maintained over 1000 facts about the RCS, over 650 facts for the forward RCS alone and half of which are updated continuously during the mission. A version of the PRS was used to monitor the reaction control system on the [[NASA]] [[Space Shuttle Discovery]].\n\nPRS was tested on [[Shakey the robot]] including navigational and simulated jet malfunction scenarios based on the space shuttle.<ref>\n{{cite conference\n  | last = Georgeff\n  | first = M. P.\n  |author2=A. L. Lansky\n  | title = Reactive reasoning and planning\n  | booktitle = Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI-87)\n  | year = 1987\n  | pages = 198–204\n  | url = http://www.ai.sri.com/pubs/files/1364.pdf \n  | work = [[Artificial Intelligence Center]]\n  | publisher = [[SRI International]] }}\n</ref> Later applications included a network management monitor called the Interactive Real-time Telecommunications Network Management System (IRTNMS) for [[Telecom Australia]].<ref>\n{{cite conference\n  | last = Rao\n  | first = Anand S.\n  |author2=Michael P. Georgeff \n  | title = Intelligent Real-Time Network Management\n  | booktitle = Australian Artificial Intelligence Institute, Technical Note 15\n  | year = 1991\n  | citeseerx = 10.1.1.48.3297 }}\n</ref>\n\n== Extensions ==\nThe following list the major implementations and extensions of the PRS architecture.<ref>\n{{cite conference\n  | last = Wobcke\n  | first = W. R.\n  | title = Reasoning about BDI Agents from a Programming Languages Perspective\n  | booktitle = Proceedings of the AAAI 2007 Spring Symposium on Intentions in Intelligent Systems\n  | year = 2007\n  | url = http://www.cse.unsw.edu.au/~wobcke/papers/ss.07.pdf }}\n</ref>\n* UM-PRS <ref>[http://www.marcush.net/IRS/irs_downloads.html]</ref>\n* OpenPRS (formerly C-PRS and Propice) <ref>[http://www.laas.fr/~felix/PRS]</ref> <ref>[https://softs.laas.fr/openrobots/wiki/openprs]</ref>\n* [[AgentSpeak]]\n* [[Distributed Multi-Agent Reasoning System]] (dMARS)\n* JAM <ref>[http://www.marcush.net/IRS/irs_downloads.html]</ref>\n* [[JACK Intelligent Agents]]\n* SRI Procedural Agent Realization Kit (SPARK) <ref>[http://www.ai.sri.com/~spark/]</ref>\n* PRS-CL <ref>[http://www.ai.sri.com/~prs/]</ref>\n\n== See also ==\n* [[Distributed multi-agent reasoning system]]\n* [[JACK Intelligent Agents]]\n* [[Belief-desire-intention software model]]\n* [[Intelligent agent]]\n\n== References ==\n{{reflist}}\n\n==Further reading==\n* M.P. Georgeff and A.L. Lansky. "A system for reasoning in dynamic domains: Fault diagnosis on the space shuttle" Technical Note 375, Artificial Intelligence Center, SRI International, 1986.\n* Michael P. Georgeff, Amy L. Lansky, Marcel J. Schoppers. "[http://www.ai.sri.com/pubs/files/579.pdf Reasoning and Planning in Dynamic Domains: An Experiment with a Mobile Robot]" Technical Note 380, Artificial Intelligence Center, SRI International, 1987.\n* M. Georgeff, and A. L. Lansky (1987). [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1457907 Procedural knowledge].  Proceedings of the IEEE 74(10):1383–1398, IEEE Press.\n* Georgeff, Michael P.; Ingrand, Francois Felix. "[http://ntrs.nasa.gov/search.jsp?R=124384&id=4&as=false&or=false&qs=Ns%3DArchiveName%257c0%26N%3D4294823185 Research on procedural reasoning systems]" Final Report – Phase 1, Artificial Intelligence Center, SRI International, 1988.\n* Michael P. Georgeff and François Félix Ingrand "[http://www.laas.fr/~felix/download.php/ijcai89.pdf Decision-Making in an Embedded Reasoning System]" Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, Detroit (Michigan), August 1989.\n* K. L. Myers, [http://www.ai.sri.com/~prs/prs-manual.pdf User Guide for the Procedural Reasoning System] Technical Report, Artificial Intelligence Center, Technical Report, SRI International, Menlo Park, CA, 1997\n* [http://www.sti.nasa.gov/tto/Spinoff2006/ch_2.html A Match Made in Space] Spinoff, NASA, 2006\n\n== External links ==\n* [http://www.ai.sri.com/~prs/ PRS-CL: A Procedural Reasoning System] An extension to PRS maintained by SRI International\n\n[[Category:Knowledge representation]]\n[[Category:Cognitive architecture]]\n[[Category:Agent-based software]]\n[[Category:Multi-agent systems]]\n[[Category:Agent-oriented programming languages]]\n[[Category:Agent-based programming languages]]\n[[Category:SRI International software]]']
['AgMES', '5465589', '{{Refimprove|date=January 2015}}\nThe \'\'\'AgMES\'\'\' (Agricultural Metadata Element set) initiative was developed by the [[Food and Agriculture Organization]] (FAO) of the [[United Nations]] and aims to encompass issues of semantic standards in the domain of agriculture with respect to description, resource discovery, interoperability and data exchange for different types of information resources.\n\nThere are numerous other metadata schemas for different types of information resources. The following list contains a list of a few examples:\n\n* Document-like Information Objects (DLIOs): [[Dublin Core]], Agricultural Metadata Element Set (AgMES)\n* Events: [[VCalendar]]\n* Geographic and Regional Information: Geographic information—Metadata ISO/IEC 11179 Standards<ref>{{cite web|url=http://isotc.iso.org/livelink/livelink/fetch/2000/2489/Ittf_Home/PubliclyAvailableStandards.htm |title=Freely Available Standards |publisher=Isotc.iso.org |date= |accessdate=2013-07-10}}</ref>\n* Persons: [[FOAF (software)|Friend-of-a-friend]] (FOAF), [[vCard]]\n* Plant Production and Protection: Darwin Core (1.0 and 2.0) (DwC)\n\nAgMES as a namespace is designed to include agriculture specific extensions for terms and refinements from established standard metadata namespaces like [[Dublin Core]], AGLS<ref>http://www.naa.gov.au/recordkeeping/gov_online/agls/summary.html</ref> etc. Thus to be used for Document-like Information Objects, for example like publications, articles, books, web sites, papers, etc., it will have to be used in conjunction with the standard namespaces mentioned before.  The AgMES initiative strives to achieve improved interoperability between information resources in agricultural domain by enabling means for exchange of information.\n\nDescribing a DLIO with AgMES means exposing its major characteristics and contents in a standard way that can be reused easily in any information system. The more institutions and organizations in the agricultural domain that use AgMES to describe their DLIOs, the easier it will be to interchange data in between information systems like digital libraries and other repositories of agricultural information.\n\n== Use of AgMES ==\nMetadata on agricultural Document-like Information Objects (DLIOs) can be created and stored in various formats:\n* embedded in a web site (in the manor as with the HTML meta tag)\n* in a separate metadata database\n* in an XML file\n* in an RDF file\n\nAgMES defines elements that can be used to describe a DLIO that can be used together with other metadata standards such as the Dublin Core, the Australian Government Locator Service. A complete list of all elements, refinements and schemes endorsed by AgMES is available from the AgMES website.<ref>{{cite web|url=http://aims.fao.org/standards/agmes/namespace-specification |title=AgMES 1.1 Namespace Specification &#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-10}}</ref>\n\n=== Creating application profiles ===\n[[Application profile]]s are defined as schemas which consist of data elements drawn from one or more namespaces, combined by implementers, and optimized for a particular local application. Application profiles share the following four characteristics:\n* They draw upon existing pool of metadata definition standards to extract suitable application- or requirement oriented elements.\n* An application profile cannot create new elements.\n* Application profiles specify the application specific details such as the schemes or controlled vocabularies. An application profile also contains information such as the format for the element value, cardinality or [[data type]].\n* Lastly, an application profile can refine standardized definitions as long as it is "semantically narrower or more specific". This capability of application profiles caters to situations where a domain specific terminology is needed to replace a more general one.\n\n=== Sample application profiles using AgMES ===\n* The AGRIS Application Profile<ref>{{cite web|url=http://www.fao.org/docrep/008/ae909e/ae909e00.htm |title=The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology |publisher=Fao.org |date= |accessdate=2013-07-10}}</ref> is a standard created specifically to enhance the description, exchange and subsequent retrieval of agricultural Document-like Information Objects (DLIOs). It is a format that allows sharing of information across dispersed bibliographic systems and is based on well-known and accepted metadata standards.\n* The Event Application Profile<ref>{{cite web|url=http://www.fao.org/aims/ap_applied.jsp |title=Agricultural Information Management Standards (AIMS) &#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}</ref> is a standard created to allow members of the Agricultural community to \'know\' about an upcoming event and guide them to the event Web site where they can find further information. The information communicated is thus minimum yet interoperable across domains and organizations.\n\n== AgMES and the semantic web ==\n\nOne of the advantages of the AgMES metadata schema is the ability to link between the [[Data element|metadata element]] and [[Controlled vocabulary|controlled vocabularies]]. The use of controlled vocabulary provides a "known" set of options to the indexer (and the search programmer) as to how the field can be filled out. Often the values may come from a specific thesaurus (e.g. [[AGROVOC]]) or classification schemes (e.g. the AGRIS/CARIS classification scheme) etc.<ref>{{cite web|url=http://www.fao.org/aims/index_en.jsp?callingPage=ag_classifschemes.jsp |title=Agricultural Information Management Standards (AIMS) &#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}</ref>\n\nThanks to the possibility to use controlled vocabularies for metadata elements, the user is provided with the most precise information. In this context, work is also being carried out on exploiting the power of controlled vocabularies expressed as using URIs and machine-understandable semantics.  In this context, FAO is promoting the [[Agricultural Ontology Service]] (AOS) initiative with the objective of expressing more semantics within the traditional thesaurus AGROVOC and build a Concept Server<ref>{{cite web|url=http://www.fao.org/aims/cs.htm |title=Agricultural Information Management Standards (AIMS) &#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}</ref> as a repository from which it will be always possible to extract traditional KOS.\n\n== See also ==\n* [[Agricultural Information Management Standards]]\n* [[AGRIS]]\n* [[AGROVOC]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n* {{Official website|https://web.archive.org/web/20060519100855/http://www.fao.org:80/aims/agmes_intro.jsp }}\n* [http://dublincore.org/ Dublin Core Metadata Initiative]\n* [http://dublincore.org/documents/abstract-model/ Dublin Core Abstract Model]\n* [http://xml.coverpages.org/ni2003-05-12-a.html FAO\'s AgMES Project Releases a New Application Profile for Encoding Metadata.] (\'\'Cover Pages\'\', May 2003)\n* [http://www.fao.org/documents/advanced_s_result.asp?FORM_C=AND&SERIES=339 Agricultural Information and Knowledge Management Papers]\n* [http://www.fao.org/nems/rss/rss_nems_results.asp?owner=615&status=10&dateto=31/12/2006&lang=en&sites=1 RSS feed of news and events]\n* [https://web.archive.org/web/20060617132639/http://www.dgroups.org:80/groups/fao/agstandards/index.cfm?op=dsp_join Agstandards Discussion List]: This is a forum established for discussing metadata standards and the development of multilingual thesauri and ontologies.\n\n{{DEFAULTSORT:Agmes}}\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]\n[[Category:Semantic Web]]\n[[Category:Food and Agriculture Organization]]']
['General Architecture for Text Engineering', '11270152', '{{Infobox software\n| name = GATE\n| screenshot = [[Image:GATE5 main window.png|250px]]\n| caption = GATE Developer v5 main window\n| developer = [http://gate.ac.uk/people GATE research team], [http://www.dcs.shef.ac.uk/ Dept. Computer Science, University of Sheffield]\n| released = {{start date and age |1995}}\n| frequently_updated = yes<!-- Release version update? Don\'t edit this page, just click on the version number! -->\n| programming language = [[Java (programming language)|Java]]\n| operating system = [[Cross-platform]]\n| language = English\n| genre = [[Text mining]] [[Information Extraction]]\n| license = [[LGPL]]\n| website = {{url|http://gate.ac.uk}}\n}}\n\'\'\'General Architecture for Text Engineering\'\'\' or \'\'\'GATE\'\'\' is a [[Java (programming language)|Java]] suite of tools originally developed at the [[University of Sheffield]] beginning in 1995 and now used worldwide by a wide community of scientists, companies, teachers and students for many [[natural language processing]] tasks, including [[information extraction]] in many languages.<ref>Languages mentioned on http://gate.ac.uk/gate/plugins/ include Arabic, Bulgarian, Cebuano, Chinese, French, German, Hindi, Italian, Romanian and Russian.</ref>\n\nGATE has been compared to [[NLTK]], [[R (programming language)|R]] and [[RapidMiner]].<ref>{{cite web|url=http://www.b-eye-network.com/view/9516|title=Open Source Text Analytics by Seth Grimes - BeyeNETWORK|publisher=|accessdate=17 December 2016}}</ref> As well as being widely used in its own right, it forms the basis of the KIM semantic platform.<ref>{{cite journal|url=https://www.cambridge.org/core/journals/natural-language-engineering/article/div-classtitlekim-a-semantic-platform-for-information-extraction-and-retrievaldiv/7249CC61F5AB25CBC7AAE182509DFEDE|title=KIM – a semantic platform for information extraction and retrieval|first1=Borislav|last1=Popov|first2=Atanas|last2=Kiryakov|first3=Damyan|last3=Ognyanoff|first4=Dimitar|last4=Manov|first5=Angel|last5=Kirilov|date=1 September 2004|publisher=|volume=10|issue=3-4|pages=375–392|accessdate=17 December 2016|via=Cambridge Core|doi=10.1017/S135132490400347X}}</ref>\n\nGATE community and research has been involved in several European research projects including [[Transitioning Applications to Ontologies|TAO]], [[SEKT]], NeOn, Media-Campaign, Musing, [[Service-Finder]], LIRICS and [[KnowledgeWeb Project|KnowledgeWeb]], as well as many other projects.\n\nAs of May 28, 2011, 881 people are on the gate-users mailing list at SourceForge.net, and 111,932 downloads from [[SourceForge]] are recorded since the project moved to SourceForge in 2005.<ref>{{cite web|url=http://sourceforge.net/projects/gate/|title=GATE|publisher=|accessdate=17 December 2016}}</ref> The paper "GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications"<ref>[http://gate.ac.uk/sale/acl02/acl-main.pdf "GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications", by Cunningham H., Maynard D., Bontcheva K. and Tablan V. (In proc. of the 40th Anniversary Meeting of the Association for Computational Linguistics, 2002)]</ref> has received over 800 citations in the seven years since publication (according to Google Scholar). Books covering the use of GATE, in addition to the GATE User Guide,<ref>{{cite web|url=http://gate.ac.uk/userguide/|title=GATE.ac.uk  - sale/tao/split.html|publisher=|accessdate=17 December 2016}}</ref> include "Building Search Applications: Lucene, LingPipe, and Gate", by Manu Konchady,<ref>Konchady, Manu. [https://books.google.com/books?id=mcM-OAAACAAJ&dq=Building+Search+Applications:+Lucene,+LingPipe,+and+Gate&hl=en&ei=avbDTczPJITqrQfk1IXQBA&sa=X&oi=book_result&ct=result&resnum=1&ved=0CDEQ6AEwAA Building Search Applications: Lucene, LingPipe, and Gate]. Mustru Publishing. 2008.</ref> and "Introduction to Linguistic Annotation and Text Analytics", by Graham Wilcock.<ref>{{cite web|url=https://books.google.com/books?id=TDQJb1UgVywC&dq=Introduction%20to%20Linguistic%20Annotation%20and%20Text%20Analytics&printsec=frontcover&source=bl&ots=bAF26ZQSTx&sig=TbxZ_-3tRy3IeDBKFofeVN6bAIc&hl=en&ei=vc0gS7PlLo-64QaSgqnfCQ&sa=X&oi=book_result&ct=result&resnum=2&ved=0CBcQ6AEwAQ#v=onepage&q=&f=false|title=Introduction to Linguistic Annotation and Text Analytics|first=Graham|last=Wilcock|date=1 January 2009|publisher=Morgan & Claypool Publishers|accessdate=17 December 2016|via=Google Books}}</ref>\n\n== Features ==\n\nGATE includes an [[information extraction]] system called \'\'\'ANNIE\'\'\' (\'\'\'A Nearly-New Information Extraction System\'\'\') which is a set of modules comprising a [[Lexical analysis|tokenizer]], a [[gazetteer]], a [[Sentence boundary disambiguation|sentence splitter]], a [[Part-of-speech tagging|part of speech tagger]], a [[Named entity recognition|named entities]] transducer and a [[coreference]] tagger. ANNIE can be used as-is to provide basic [[information extraction]] functionality, or provide a starting point for more specific tasks.\n\nLanguages currently handled in GATE include [[English language|English]], [[Mandarin Chinese|Chinese]], [[Arabic]], [[Bulgarian language|Bulgarian]], [[French language|French]], [[German language|German]], [[Hindi]], [[Italian language|Italian]], [[Cebuano language|Cebuano]], [[Romanian language|Romanian]], [[Russian language|Russian]], [[Danish language|Danish]].\n\nPlugins are included for [[machine learning]] with [[Weka (machine learning)|Weka]], RASP, MAXENT, SVM Light, as well as a [[LIBSVM]] integration and an in-house [[perceptron]] implementation, for managing [[Ontology (information science)|ontologies]] like [[WordNet]], for querying [[search engines]] like [[Google]] or [[Yahoo]], for [[part of speech tagging]] with [[Brill tagger|Brill]] or TreeTagger, and many more. Many external plugins are also available, for handling e.g. [[Twitter|tweets]].<ref>{{cite web|url=https://gate.ac.uk/wiki/twitie.html|title=GATE.ac.uk  - wiki/twitie.html|publisher=|accessdate=17 December 2016}}</ref>\n\nGATE accepts input in various formats, such as [[Text file|TXT]], [[HTML]], [[XML]], [[DOC (computing)|Doc]], [[PDF]] documents, and [[Serialization|Java Serial]], [[PostgreSQL]], [[Lucene]], [[Oracle database|Oracle]] Databases with help of [[RDBMS]] storage over [[JDBC]].\n\n[[JAPE (linguistics)|JAPE]] transducers are used within GATE to manipulate annotations on text. Documentation is provided in the GATE User Guide.<ref>{{cite web|url=http://gate.ac.uk/userguide/chap:jape|title=GATE.ac.uk  - sale/tao/splitch8.html|publisher=|accessdate=17 December 2016}}</ref> A tutorial has also been written by Press Association Images.<ref>{{cite web|url=http://realizingsemanticweb.blogspot.com/2009/07/jape-grammar-tutorial.html|title=Realizing Semantic Web: JAPE grammar tutorial|first=Dhavalkumar|last=Thakker|date=17 July 2009|publisher=|accessdate=17 December 2016}}</ref>\n\n== GATE Developer ==\n\n[[Image:GATE5 main window.png|thumb|400px|GATE 5 main window.]]\n\nThe screenshot shows the document viewer used to display a document and its annotations. In pink are <A> hyperlink annotations from an [[Hypertext Markup Language|HTML]] file. The right list is the annotation sets list, and the bottom table is the annotation list. In the center is the annotation editor window.\n\n== GATE Mímir ==\n<!-- re-written to remove any lingering copyright worries -->\n Generate vast quantities of information including; natural language text, semantic annotations, and ontological information. Sometimes the data itself is the end product of an application but often the information would be more useful if it could be efficiently searched. GATE Mimir provides support for indexing and searching the linguistic and semantic information generated by such applications and allows for querying the information using arbitrary combinations of text, structural information, and [[SPARQL]].\n\n==See also==\n{{Portal|Free software}}\n* [[Unstructured Information Management Architecture]] (UIMA)\n* [[OpenNLP]]\n* [[List of natural language processing toolkits]]\n* [[Pheme (project)|Pheme]], a major EU project managed by the GATE group on early detection of false information in social media\n\n==References==\n<references/>\n\n{{DEFAULTSORT:General Architecture For Text Engineering}}\n[[Category:Data mining and machine learning software]]\n[[Category:Free computer libraries]]\n[[Category:Free science software]]\n[[Category:Free software programmed in Java (programming language)]]\n[[Category:Free integrated development environments]]\n[[Category:Knowledge representation]]\n[[Category:Natural language processing toolkits]]\n[[Category:Ontology editors]]']
['Logico-linguistic modeling', '30109665', '\'\'\'Logico-linguistic modeling\'\'\' is a method for building knowledge-based systems with a learning capability using [[Conceptual model|Conceptual Models]] from [[Soft systems methods]], modal predicate logic and the Prolog artificial intelligence language.\n\n== Overview==\nLogico-linguistic modeling is a six stage method developed primarily for building [[knowledge-based systems]] (KBS), but it also has application in manual decision support systems and information source analysis. Logico-linguistic models have a superficial similarity to Sowa\'s<ref>Sowa, John F. (1984), \'\'Conceptual Structures:  Information Processing in Mind and Machine\'\', Addison-Wesley, Reading, MA, USA.</ref> [[Conceptual Graphs]], both use bubble style diagrams, both are concerned with concepts, both can be expressed in logic and both can be used in artificial intelligence. However, logico-linguistic models are very different in both logical form and in their method of construction.\n \nLogico-linguistic modeling was developed in order to solve theoretical problems found in the Soft Systems method for information system design. The main thrust of the research into has been to show how [[Soft Systems Methodology]] (SSM), a method of systems analysis, can be extended into artificial intelligence.\n\n== Background ==\n\nSSM employs three modeling devices i.e. rich pictures, root definitions, and Conceptual Models of human activity systems. The root definitions and conceptual models are built by stakeholders themselves in an iterative debate organized by a facilitator. The strengths of this method lie, firstly, in its flexibility, the fact that it can address any problem situation, and, secondly, in the fact that the solution belongs to the people in the organization and is not imposed by an outside analyst.<ref name =  "source">Gregory, Frank Hutson and Lau, Sui Pong (1999) [http://logicalgregory.jimdo.com/publications/logical-ssm-for-isa/ Logical Soft Systems Modelling for Information Source Analysis - The Case of Hong Kong Telecom], Journal of the Operational Research Society, vol. 50 (2).</ref>\n\nInformation Requirements Analysis (IRA)<ref name="Wilson">Wilson, Brian \'\'Systems: Concepts, Methodologies and Applications\'\', John Wiley & Sons Ltd. 1984, 1990. ISBN 0-471-92716-3</ref>  took the basic SSM method a stage further and showed how the Conceptual Models could be developed into a detailed information system design. IRA calls for the addition of two modeling devices: "Information Categories" which show the required information inputs and outputs from the activities identified in an expanded conceptual model; and the "Maltese Cross" a matrix which shows the inputs and outputs from the information categories and shows where new information processing procedures are required. A completed Maltese Cross is sufficient for the detailed design of a transaction processing system.\n\nThe initial impetus to the development of logico-linguistic modeling was a concern with the theoretical problem of how an information system can have a connection to the physical world.<ref>Gregory, Frank Hutson (1995) [[s:Mapping Information Systems onto the Real World|Mapping Information Systems onto the Real World]]. Working Paper Series No. WP95/01. Dept. of Information Systems, City University of Hong Kong.</ref> This is a problem in both IRA and more established methods (such as [[SSADM]]) because none base their information system design on models of the physical world. IRA designs are based on a notional conceptual model and SSADM is based on models of the movement of documents.\n\nThe solution to these problems provided a formula that was not limited to the design of transaction processing systems but could be used for the design of KBS with learning capability.<ref name="know">Gregory, Frank Hutson (1993) SSM for Knowledge Elicitation & Representation, Warwick Business School Research Paper No. 98 ({{ISSN|0265-5976}}). Later published as Soft Systems Models for Knowledge Elicitation and Representation in Journal of the Operational Research Society (1995) 46, 562-578.</ref>\n\n== The Six Stages of logico-linguistic modeling==\n[[File:Fig 1. SSM model abstracted from Wilson.jpg|thumb|Fig 1. SSM Conceptual Model]]\nThe logico-linguistic modeling method comprises six stages.<ref name="know"/>\n\n=== 1. Systems Analysis ===\n\nIn the first stage logico-linguistic modeling uses SSM for [[systems analysis]]. This stage seeks to structure the problem in the client organization by identifying stakeholders, modelling organizational objectives and discussing possible solutions. At this stage it not assumed that a KBS will be a solution and logico-linguistic modeling often produces solutions that do not require a computerized KBS.\n\n[[Expert systems]] tend to capture the expertise, of individuals in different organizations, on the same topic. By contrast a KBS, produced by logico-linguistic modeling, seeks to capture the expertise of individuals in the same organization on different topics. The emphasis is on the [[elicitation technique|elicitation]] of organizational or group knowledge rather than individual experts. In logico-linguistic modeling the stakeholders become the experts.\n\nThe end point of this stage is an SSM style conceptual models such as figure 1.\n\n=== 2. Language Creation ===\n[[File:Fig 2. Logico-linguistic Model.jpeg|thumb|Fig 2. Logico-linguistic Model]]\n\nAccording to the theory behind logico-linguistic modeling the SSM conceptual model building process is a Wittgensteinian [[language-game]] in which the stakeholders build a language to describe the problem situation.<ref>Gregory, Frank Hutson (1992) [[s:SSM to Information Systems: A Wittengsteinian Approach|SSM to Information Systems: A Wittengsteinian Approach. Warwick Business School Research Paper No. 65.]] With revisions and additions this paper was published in Journal of Information Systems (1993) 3, pp.&nbsp;149–168.</ref> The logico-linguistic model expresses this language as a set of definitions, see figure 2.\n\n=== 3. Knowledge Elicitation===\nAfter the model of the language has been built putative knowledge about the real world can be added by the stakeholders. Traditional SSM conceptual models contain only one logical connective (a necessary condition). In order to represent causal sequences, “[[sufficient condition]]” and “[[necessary and sufficient condition|necessary & sufficient conditions]]” are also required.<ref name="cause2">Gregory, Frank Hutson (1992) [[s:Cause, Effect, Efficiency & Soft Systems Models|Cause, Effect, Efficiency & Soft Systems Models. Warwick Business School Research Paper No. 42]]. Later published in Journal of the Operational Research Society (1993) 44 (4), pp 149-168</ref> In logico-linguistic modeling this deficiency is remedied by two addition types of connective. The outcome of stage three is an empirical model, see figure 3.\n\n=== 4. Knowledge Representation ===\n[[File:Fig 3. Empirical Model.jpeg|thumb|Fig 3. Empirical Model]]\n\nModal predicate logic (a combination of [[modal logic]] and [[predicate logic]]) is used as the formal method of knowledge representation. The connectives from the language model are logically true (indicated by the “\'\'L\'\'” modal operator) and connective added at the knowledge elicitation stage are possibility true (indicated by the “\'\'M\'\'” modal operator). Before proceeding to stage 5, the models are expressed in logical formulae.\n\n=== 5. Computer code ===\n\nFormulae in predicate logic translate easily into the [[Prolog]] artificial intelligence language. The modality is expressed by two different types of Prolog rules. Rules taken from the language creation stage of  model building process are treated as incorrigible. While rules from the knowledge elicitation stage are marked as hypothetical rules. The system is not confined to decision support but has a built in learning capability.\n\n=== 6. Verification ===\n\nA knowledge based system built using this method verifies itself. [[Verification and Validation (software)|Verification]] takes place when the KBS is used by the clients. It is an ongoing process that continues throughout the life of the system. If the stakeholder beliefs about the real world are mistaken this will be brought out by the addition of Prolog facts that conflict with  the hypothetical rules. It operates in accordance to the classic principle of [[falsifiability]] found in the philosophy of science<ref>Gregory, Frank Hutson (1996) "The need for "Scientific" Information Systems" Proceedings of the Americas Conference on Information Systems, Aug 1996, Association for Information Systems, 1996. pp. 534-536.</ref>\n\n== Applications ==\n* \'\'\'Knowledge-based computer systems\'\'\'\nLogico-linguistic modeling has been used to produce fully operational computerized knowledge based systems, such as one for the management of diabetes patients in a hospital out-patients department.<ref>Choi, Mei Yee Sarah (1997) Logico-linguistic Modelling for building a Diabetes Mellitus Patient Management Knowledge Based System. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.</ref>\n\n*\'\'\'Manual decision support\'\'\'\nIn other projects the need to move into Prolog was considered unnecessary because the printed logico-linguistic models provided an easy to use guide to decision making. For example, a system for mortgage loan approval<ref>Lee, Kam Shing Clive (1997) The Development of a Knowledge Based System on Mortgage Loan Approval. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.</ref>\n\n*\'\'\'Information source analysis\'\'\'\nIn some cases a KBS could not be built because the organization did not have all the knowledge needed to support all their activities. In these cases logico-linguistic modeling showed shortcomings in the supply of information and where more was needed. For example, a planning department in a telecoms company<ref name =  "source"/>\n\n== Criticism ==\nWhile logico-linguistic modeling overcomes the problems found in SSM\'s transition from conceptual model to computer code, it does so at the expense of increased stakeholder constructed model complexity. The benefits of this complexity are questionable<ref>Klein, J. H. (1994) Cognitive processes and operational research: a human information processing perspective. Journal of the Operational Research Society. Vol. 45, No. 8.</ref>\nand this modeling method may be much harder to use than other methods.<ref>Klein, J. H. (1995) Over-simplistic cognitive science: A response.  Journal of the Operational Research Society. Vol. 46, No. 4. pp. 275-6.</ref>\n\nThis contention has been exemplified by subsequent research. An attempt by researchers to model buying decisions across twelve companies using logico-linguistic modeling required simplification of the models and removal of the modal elements.<ref>Nakswasdi, Suravut (2004) [http://arrow.unisa.edu.au:8080/vital/access/manager/Repository/unisa:44235 Logical Soft Systems for Modeling Industrial Machinery Buying Decisions in Thailand]. Doctor of Business Administration thesis, University of South Australia.</ref>\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n{{commons category}}\n* Gregory, Frank Hutson  (1993) "[http://wrap.warwick.ac.uk/2888/ A logical analysis of soft systems modelling: implications for information system design and knowledge based system design]\'\'. PhD thesis, University of Warwick.\n\n[[Category:Knowledge representation]]\n[[Category:Systems analysis]]\n[[Category:Modal logic]]']
['Category:Thesauri', '31375917', '{{cat main|Thesaurus}}\n{{Commons category|Thesauri}}\n[[Category:Knowledge representation]]\n[[Category:Reference works]]\n[[Category:Dictionaries by type]]\n[[Category:Information science]]']
['Pinakes', '19391789', '{{italic title}}\n{{hatnote|"Pinakes" may be plural of [[pinax]], a votive tablet that served as a votive object deposited in a sanctuary or burial chamber.}}\n[[Image:Ancientlibraryalex.jpg|thumb|Imaginary depiction of the [[Library of Alexandria]]]]\nThe \'\'\'\'\'Pinakes\'\'\'\'\' ({{lang-grc|Πίνακες}} "tables", plural of {{lang|grc|[[wikt:πίναξ|πίναξ]]}}) was a [[bibliography|bibliographic]] work composed by [[Callimachus]] (310/305–240 BCE) that is popularly considered to be the first [[library catalog]]; its contents were based upon the holdings of the [[Library of Alexandria]] during Callimachus\' tenure there during the third century BCE.<ref>N. Krevans 2002: 173</ref>\n\n==History==\n\nThe Library of Alexandria had been founded by [[Ptolemy I Soter]] about 306 BCE. The first recorded librarian was [[Zenodotus]] of Ephesus. During Zenodotus\' tenure, Callimachus, who was never the head librarian, compiled the \'\'Pinakes\'\', thus becoming the first bibliographer and the scholar who organized the library by authors and subjects about 245 BCE.<ref>Neil Hopkinson, \'\'A Hellenistic Anthology\'\' (CUP, 1988) 83.</ref><ref name ="alexandria3">{{cite web|url= http://www.greekplanet.com.au/forum/lofiversion/index.php/t486.html|title=\nGreek Inventions|accessdate= 2008-09-19}}</ref> His work was 120 volumes long.<ref>Hopkinson</ref>\n\n[[Apollonius of Rhodes]] was the successor to Zenodotus. [[Eratosthenes]] of Cyrene succeeded Apollonius in 235 BCE and compiled his \'\'tetagmenos epi teis megaleis bibliothekeis\'\', the "scheme of the great bookshelves." In 195 BCE [[Aristophanes of Byzantium]] was the librarian and updated the \'\'Pinakes\'\',<ref>Pfeiffer, R. \'\'History of Classical Scholarship from the Beginnings to the End of the Hellenistic Age\'\' (OUP, 1968) 133.</ref> although it is also possible that his work was not a supplement of Callimachus\' \'\'Pinakes\'\' themselves, but an independent polemic against, or commentary upon, their contents.<ref>Slater, W.J. "Grammarians on Handwashing", \'\'Phoenix\'\' 43 (1989) 100&ndash;11, at 102.</ref>\n\n==Description==\n\nThe collection at the Library of Alexandria contained nearly 500,000 [[papyrus]] scrolls, which were grouped together by subject matter and stored in bins.<ref>P.J. Parson, "Libraries", in the \'\'Oxford Classical Dictionary\'\', 3rd ed. (OUP, 1996) describes the evidence for the size of the library\'s holdings thus: "The first Ptolemies (see Ptolemy (1) ) collected ambitiously and systematically; the Alexandrian Library (see ALEXANDRIA (1) ) became legend, and *Callimachus (3)\'s \'\'Pinakes\'\' made its content accessible. There were rivals at *Pella, *Antioch (1) (where *Euphorion (2) was librarian), and especially *Pergamum. Holdings were substantial: if the figures can be trusted, Pergamum held at least 200,000 rolls (Plut. \'\'Ant.\'\' 58. 9), the main library at Alexandria nearly 500,000 (*Tzetzes, \'\'Prolegomena de comoedia\'\' 11a. 2. 10–11 Koster)&mdash;the equivalent, perhaps, of 100,000 modern books."</ref> Each bin carried a label with painted tablets hung above the stored papyri. \'\'Pinakes\'\' was named after these tablets and are a set of index lists. The bins gave bibliographical information for every roll.<ref>Phillips, Heather A., [http://unllib.unl.edu/LPP/phillips.htm "The Great Library of Alexandria?". Library Philosophy and Practice, August 2010]</ref> A typical entry started with a title and also provided the author\'s name, birthplace, father\'s name, any teachers trained under, and educational background. It contained a brief biography of the author and a list of the author\'s publications. The entry had the first line of the work, a summary of its contents, the name of the author, and information about the origin of the roll.<ref name ="alexandria4">{{cite web|url= http://www.greece.org/hec01/www/arts-culture/alexandria/library/library11.htm|title= The Pinakes|accessdate= 2010-05-29}}</ref>\n\nCallimachus\' system divided works into six [[genres]] and five sections of prose: rhetoric, law, epic, tragedy, comedy, lyric poetry, history, medicine, mathematics, natural science and miscellanies. Each category was alphabetized by author.\n\nCallimachus composed two other works that were referred as \'\'pinakes\'\' and were probably somewhat similar in format to the \'\'Pinakes\'\' (of which they "may or may not be subsections"<ref>Nita Krevans, "Callimachus and the Pedestrian Muse," in M.A. harder et al., eds., \'\'Callimachus II\'\' (Hellenistica Groningana 7), 2002, p. [https://books.google.com/books?id=CL4A5I3K-KsC&lpg=PA173&dq=callimachus%20democritus%20catalog&pg=PA173#v=onepage&q&f=false 173] n. 1.</ref>), but were concerned with individual topics. These are listed by the \'\'[[Suda]]\'\' as: \'\'A Chronological Pinax and Description of [[Theatre director#The director in theatre history|Didaskaloi]] from the Beginning\'\' and \'\'Pinax of the Vocabulary and Treatises of [[Democritus]]\'\'.<ref>[http://www.stoa.org/sol-entries/kappa/227 \'\'Suda\'\' On Line]</ref>\n\n==Later bibliographic \'\'pinakes\'\'==\nThe term \'\'pinax\'\' was used for bibliographic catalogs beyond Callimachus. For example, [[Ptolemy-el-Garib]]\'s catalog of [[Aristotle]]\'s writings comes to us with the title \'\'Pinax (catalog) of Aristotle\'s writings\'\'.<ref>[[Ingemar Düring]], \'\'Aristotle in the Ancient Biographical Tradition\'\' (Göteborg 1957), p. 221.</ref>\n\n==Legacy==\nThe \'\'Pinakes\'\' proved indispensable to librarians for centuries. They became a model to use all over the [[Mediterranean]]. Their later influence can be traced to medieval times, even to the Arabic counterpart of the tenth century: [[Ibn al-Nadim]]\'s \'\'Al-Fihrist\'\' ("Index"). Variations on this system were used in libraries until the late 1800s when [[Melvil Dewey]] developed the [[Dewey Decimal Classification]] in 1876, which is still in use today.<ref name ="alexandria4"/>\n\n==Notes==\n{{reflist|35em}}\n\n==Bibliography==\n\n===Texts and translations===\n* The evidence concerning the Pinakes is collected by [[Rudolf Pfeiffer]] (ed.), \'\'Callimachus, vol. I: Fragmenta\'\', Oxford: Clarendon Press 1949, frr. 429-456 (with reference to the most important literature).\n* Witty, F. J. "The Pinakes of Callimachus", \'\'Library Quarterly\'\' 28:1/4 (1958), 132&ndash;36.\n* Witty, F. J. "The Other Pinakes and Reference Works of Callimachus", \'\'Library Quarterly\'\' 43:3 (1973), 237&ndash;44.\n\n===Studies===\n* [[Roger S. Bagnall|Bagnall, R. S.]] [http://archive.nyu.edu/bitstream/2451/28263/2/D172-Alexandria%20Library%20of%20Dreams.pdf "Alexandria: Library of Dreams"], \'\'Proceedings of the American Philosophical Society\'\' 46 (2002) 348&ndash;62.\n* Blum, R. \'\'Kallimachos. The Alexandrian Library and the Origins of Bibliography\'\', trans. H.H. Wellisch (U. Wisconsin, 1991). ISBN 978-0-299-13170-8.\n* Krevans, N. [https://books.google.com/books?id=CL4A5I3K-KsC&lpg=PA173&dq=callimachus%20democritus%20catalog&pg=PA173#v=onepage&q&f=false "Callimachus and the Pedestrian Muse"], in: A. Harder et al. (eds.) \'\'Callimachus II\'\', Hellenistic Groningana 6 (Groningen, 2002) 173&ndash;84.\n* West, M. L. "The Sayings of Democritus", \'\'Classical Review\'\' (1969) 142.\n\n{{coord missing|Egypt}}\n\n{{Callimachus}}\n\n[[Category:Defunct libraries]]\n[[Category:Libraries in Egypt]]\n[[Category:3rd-century BC books]]\n[[Category:History of museums]]\n[[Category:Ptolemaic Alexandria]]\n[[Category:Library cataloging and classification]]\n[[Category:Knowledge representation]]\n[[Category:Bibliographies]]']
['LEAP (programming language)', '911672', '{{Unreferenced|date=November 2007}}\n\'\'\'LEAP\'\'\' is an extension to the [[ALGOL 60]] \'\'\'[[programming language]]\'\'\' which provides an associative memory of triples. The three items in a triple denote the association that an Attribute of an Object has a specific Value.  LEAP was created by Jerome Feldman (University of California Berkeley) and Paul Rovner (MIT Lincoln Lab) in 1967.  LEAP was also implemented in [[SAIL (programming language)|SAIL]].\n\n==References==\n* Feldman, Jerome A. and Paul D. Rovner (Jan, 1968).  "The LEAP language and data structure", MIT Lincoln Laboratory, Lexington, MA.  In abbreviated form, \'\'Proc. 1968 IFIP Congress\'\', Brandon Systems Press, Princeton, NJ.\n* Feldman, Jerome A. and Paul D. Rovner (Aug, 1969).  [http://portal.acm.org/citation.cfm?doid=363196.363204 "An ALGOL-based associative language"], \'\'Communications of the ACM\'\', 12:8, pp 439 - 449.\n* Rovner, Paul D (Dec, 1968).  "The LEAP users manual", MIT Lincoln Laboratory, Lexington, MA.\n* [ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/73/373/CS-TR-73-373.pdf VanLehn, Kurt A. (Jul, 1973).  "SAIL User Manual", Stanford Artificial Intelligence Laboratory, Stanford, CA.]\n\n[[Category:Structured programming languages]]\n[[Category:Procedural programming languages]]\n[[Category:Programming languages created in 1967]]\n[[Category:Knowledge representation]]\n\n\n{{compu-lang-stub}}']
['Library classification', '18328', '{{refimprove|date=March 2012}}\n[[Image:HK Wan Chai Library Inside Bookcase a.jpg|thumb|A library book shelf in [[Hong Kong]] arranged using the [[Dewey Decimal Classification|Dewey classification]]]]\n\nA \'\'\'library classification\'\'\' is a [[system]] by which library resources are arranged according to subject. Library classifications use a notational system that represents the order of topics in the classification and allows items to be stored in that order. Library classification systems group related materials together, typically arranged in a hierarchical tree structure. A different kind of classification system, called a [[faceted classification]] system, is also widely used which allows the assignment of multiple classifications to an object, enabling the classifications to be ordered in multiple ways. The library classification numbers can be considered identifiers for resources but are distinct from the [[International Standard Book Number]] (ISBN) or [[International Standard Serial Number]] (ISSN) system.\n\n== Description ==\nLibrary classification is an aspect of [[library and information science]]. It is distinct from [[taxonomy (general)|scientific classification]] in that it has as its goal to provide a useful ordering of documents rather than a theoretical organization of [[knowledge]].<ref>{{Citation\n | first =Ganesh\n | last =Bhattacharya\n | first2 =S R\n | last2 =Ranganathan\n | author2-link=S R Ranganathan\n | editor-last =Wojciechowski\n | editor-first =Jerzy A.\n | title =From knowledge classification to library classification\n | series =Ottawa Conference on the Conceptual Basis of the Classification of Knowledge, 1971\n | year =1974\n | pages =119–143\n | place =Munich\n | publisher =Verlag Dokumentation\n}}</ref> Although it has the practical purpose of creating a physical ordering of documents, it does generally attempt to adhere to accepted scientific knowledge.<ref>{{cite book\n | last = Bliss\n | first = Henry Evelyn\n | title = The organization of knowledge in libraries\n | publisher = H. W. Wilson\n | location = New Yorka\n | date = 1933\n}}</ref>\n\nLibrary classification is distinct from the application of [[Index term|subject headings]] in that classification organizes knowledge into a systematic order, while subject headings provide access to intellectual materials through vocabulary terms that may or may not be organized as a knowledge system.<ref name=chan >{{Citation\n |publisher = The Scarecrow Press, Inc.\n |isbn = 9780810859449\n |title = Cataloging and classification\n |url = http://openlibrary.org/books/OL9558667M/Cataloging_and_Classification\n |author = Lois Mai Chan\n |edition = Cataloging and Classification\n |publication-date = September 28, 2007\n |id = 0810859440\n }}</ref>\n\n==History==\n\n\nLibrary classifications were preceded by classifications used by bibliographers such as [[Conrad Gessner]]. The earliest library classification schemes organized books in broad subject categories. The increase in available printed materials made such broad classification unworkable, and more granular classifications for library materials had to be developed in the nineteenth century.<ref name=shera>{{cite book|last1=Shera|first1=Jesse H|title=Libraries and the organization of knowledge|date=1965|publisher=Archon Books|location=Hamden, Conn.}}</ref>\n\nAlthough libraries created order within their collections from as early as the fifth century B.C.,<ref name=shera /> the Paris Bookseller\'s classification, developed in 1842 by [[Jacques Charles Brunet]], is generally seen as the first of the modern book classifications. Brunet provided five major classes: theology, jurisprudence, sciences and arts, belles-lettres, and history.<ref name=sayers>{{cite book|last1=Sayers|first1=Berwick|title=An introduction to library classification|date=1918|publisher=H. W. Wilson|location=New York}}</ref>\n\n==Types== \nThere are many standard systems of library classification in use, and many more have been proposed over the years. However, in general, classification systems can be divided into three types depending on how they are used:\n\n* \'\'\'Universal schemes\'\'\' which cover all subjects, for example the [[Dewey Decimal Classification]], [[Universal Decimal Classification]] and [[Library of Congress Classification]]\n* \'\'\'Specific classification schemes\'\'\' which cover particular subjects or types of materials, for example Iconclass, [[British Catalogue of Music Classification]], and [[Dickinson classification]], or the [[NLM Classification]] for medicine. \n* \'\'\'National schemes\'\'\' which are specially created for certain countries, for example the [[Sweden|Swedish]] library classification system, SAB (Sveriges Allmänna Biblioteksförening).\n\nIn terms of functionality, classification systems are often described as:\n\n*\'\'\'[[Enumeration|enumerative]]\'\'\': subject headings are listed alphabetically, with numbers assigned to each heading in alphabetical order.\n*\'\'\'[[Hierarchy|hierarchical]]\'\'\': subjects are divided hierarchically, from most general to most specific.\n*\'\'\'[[Faceted classification|faceted]]\'\'\' or \'\'\'analytico-synthetic\'\'\': subjects are divided into mutually exclusive orthogonal facets.\n\nThere are few completely enumerative systems or faceted systems; most systems are a blend but favouring one type or the other. The most common classification systems, LCC and DDC, are essentially enumerative, though with some hierarchical and faceted elements (more so for DDC), especially at the broadest and most general level. The first true faceted system was the [[Colon classification]] of [[S. R. Ranganathan]].\n\n==Methods or Systems==\n\nClassification types denote the classification or categorization according the form or characteristics or qualities of a classification scheme or schemes. Method and system has similar meaning. Method or methods or system means the classification schemes like Dewey Decimal Classification or Universal Decimal Classification. The types of classification is for identifying and understanding or education or research purposes while classification method means those classification schemes like DDC, UDC.\n \n===English language universal classification systems===\nThe most common systems in [[English language|English]]-speaking countries are:\n* [[Dewey Decimal Classification]] (DDC)\n* [[Library of Congress Classification]] (LCC)\n* [[Colon classification]] (CC)\n* [[Universal Decimal Classification]] (UDC)\n\nOther systems include:\n* [[Harvard-Yenching Classification]], an English classification system for Chinese language materials.\n* V-LIB 1.2 (2008 Vartavan Library Classification for over 700 fields of knowledge, currently sold under license in the UK by Rosecastle Ltd. (see http://rosecastle.atspace.com/index_files/VartavanLibrary.htm)).\n\n===Non-English universal classification systems===\n* A system of book classification for Chinese libraries (Liu\'s Classification) library classification for user\n** [[New Classification Scheme for Chinese Libraries]]\n* [[Nippon Decimal Classification]] (NDC)\n* [[Chinese Library Classification]] (CLC)\n* [[Korean Decimal Classification]] (KDC)\n* Russian [[:ru:Библиотечно-библиографическая классификация|Library-Bibliographical Classification]] (BBK)\n\n===Universal classification systems that rely on synthesis (faceted systems)===\n* [[Bliss bibliographic classification]]\n* [[Colon classification]]\n* [[Cutter Expansive Classification]]\n* [[Universal Decimal Classification]]\n\nNewer classification systems tend to use the principle of synthesis (combining codes from different lists to represent the different attributes of a work) heavily, which is comparatively lacking in LC or DDC.\n\n==The practice of classifying==\n\nLibrary classification is associated with library (descriptive) cataloging under the rubric of \'\'cataloging and classification\'\', sometimes grouped together as \'\'technical services\'\'. The library professional who engages in the process of cataloging and classifying library materials is called a \'\'cataloger\'\' or \'\'catalog librarian\'\'. Library classification systems are one of the two tools used to facilitate [[subject access]].  The other consists of alphabetical indexing languages such as Thesauri and Subject Headings systems.\n\nLibrary classification of a piece of work consists of two steps. Firstly, the "aboutness" of the material is ascertained. Next, a call number (essentially a book\'s address) based on the classification system in use at the particular library will be assigned to the work using the notation of the system.\n\nIt is important to note that unlike subject heading or thesauri where multiple terms can be assigned to the same work, in library classification systems, each work can only be placed in one class. This is due to shelving purposes: A book can have only one physical place. However, in classified catalogs one may have main entries as well as added entries. Most classification systems like the [[Dewey Decimal Classification]] (DDC) and [[Library of Congress Classification]] also add a [[cutter number]] to each work which adds a code for the author of the work.\n\nClassification systems in libraries generally play two roles. Firstly, they facilitate [[subject access]] by allowing the user to find out what works or documents the library has on a certain subject.<ref>{{cite web|url=http://www.iva.dk/bh/lifeboat_ko/concepts/subject_access_points.htm|title=Subject access points|work=iva.dk}}</ref> Secondly, they provide a known location for the information source to be located (e.g. where it is shelved).\n\nUntil the 19th century, most libraries had closed stacks, so the library classification only served to organize the subject [[library catalog|catalog]]. In the 20th century, libraries opened their stacks to the public and started to shelve library material itself according to some library classification to simplify subject browsing.\n\nSome classification systems are more suitable for aiding subject access, rather than for shelf location. For example, [[Universal Decimal Classification]], which uses a complicated notation of pluses and colons, is more difficult to use for the purpose of shelf arrangement but is more expressive compared to DDC in terms of showing relationships between subjects. Similarly [[faceted classification]] schemes are more difficult to use for shelf arrangement, unless the user has knowledge of the citation order.\n\nDepending on the size of the library collection, some libraries might use classification systems solely for one purpose or the other. In extreme cases, a public library with a small collection might just use a classification system for location of resources but might not use a complicated subject classification system. Instead all resources might just be put into a couple of wide classes (travel, crime, magazines etc.). This is known as a "mark and park" classification method, more formally called reader interest classification.<ref>Lynch, Sarah N., and Eugene Mulero. [http://www.nytimes.com/2007/07/14/us/14dewey.html "Dewey? At This Library With a Very Different Outlook, They Don\'t"] \'\'[[The New York Times]]\'\', July 14, 2007.</ref>\n\n== Comparing classification systems ==\nAs a result of differences in notation, history, use of enumeration, hierarchy, and facets, classification systems can differ in the following ways:\n* Type of Notation: Notation can be pure (consisting of only numerals, for example) or mixed (consisting of letters and numerals, or letters, numerals, and other symbols). \n* Expressiveness: This is the degree to which the notation can express relationship between concepts or structure.\n* Whether they support mnemonics: For example, the number 44 in DDC notation often means it concerns some aspect of France. For example, in the Dewey classification 598.0944 concerns "Birds in France", the 09 signifies geographical division, and 44 represents France.\n* Hospitality: The degree to which the system is able to accommodate new subjects.\n* Brevity: The length of the notation to express the same concept.\n* Speed of updates and degree of support: The better classification systems are frequently being reviewed.\n* Consistency \n* Simplicity\n* Usability\n\n== See also ==<!-- PLEASE RESPECT ALPHABETICAL ORDER -->\n{{Wikipedia books}}\n* [[Attribute-value system]]\n* [[Categorization]]\n* [[Document classification]]\n* [[Knowledge organization]]\n* [[Library management]]\n* [[Library of Congress Subject Headings]]\n\n==Notes==\n{{reflist}}\n\n==References==\n{{commons category|Library cataloging and classification}}\n* Chan, Lois Mai. (1994)\'\'Cataloging and Classification: An Introduction\'\', second ed. New York: McGraw-Hill, . ISBN 978-0-07-010506-5, ISBN 978-0-07-113253-4.\n\n{{Library classification systems}}\n{{Computable knowledge}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Library Classification}}\n[[Category:Library cataloging and classification| ]]\n[[Category:Knowledge representation]]']
['Template:InfoMaps', '36485231', '{{Sidebar\n|name     = InfoMaps\n|topimage = [[Image:Screen_Shot_2012-07-19_at_5.56.57_PM.png |165px|Part of "School of Athens" by Raphael (Raffaelo Sanzio, 1483-1520)]]\n|title    = [[Information mapping]]\n|bodyclass = hlist\n|titleclass= navbox-title\n|headingstyle = background:transparent;\n\n|heading1 = Topics & fields\n|content1style = padding-bottom:0.9em;\n|content1 = \n* [[Business decision mapping]]  \n* [[Cognitive map]]  \n* [[Data visualization]]  \n* [[Decision tree]] \n* [[Educational psychology]] \n* [[Educational technology]] \n* [[Graphic communication]] \n* [[Information design]]  \n* [[Information graphics]]  \n* [[Interactive visualization]]  \n* [[Knowledge visualization]] \n* [[Mental model]]  \n* [[Morphological analysis (problem-solving)|Morphological analysis]]  \n* [[Visual analytics]]  \n* [[Visual language]]\n\n|heading2 = Tree-like approaches\n|content2style = padding-bottom:0.9em;\n|content2 = \n* [[Cladistics]]  \n* [[Argument map]] \n* [[Cognitive map]]\n* [[Concept lattice]] \n* [[Concept map]]ping \n* [[Conceptual graph]] \n* [[Dendrogram]]  \n* [[Graph drawing]]  \n* [[Hyperbolic tree]]  \n* [[Layered graph drawing]]  \n* [[Mental model]]  \n* [[Mind map]]ping \n* [[Object-role modeling]] \n* [[Organizational chart]]  \n* [[Radial tree]] \n* [[Semantic network]] \n* [[Sociogram]]  \n* [[Timeline]]   \n* [[Topic Maps]]  \n* [[Tree structure]]   \n\n|heading3 =See also\n|content3style = padding-bottom:0.9em;\n|content3 =\n* [[Diagrammatic reasoning]]\n* [[Entity-relationship model]]\n* [[Geovisualization]]  \n* [[List of concept- and mind-mapping software]]  \n* [[Olog]]  \n* [[Semantic web]]  \n* [[Treemapping]]  \n* [[Wicked problem]]  \n\n|tnavbarstyle = border-top:1px solid #aaa;\n}}<noinclude>\n[[Category:Knowledge representation]]\n</noinclude>']
['Conceptualization (information science)', '38982174', '[[File:Ontological commitments.png|thumb|200px|Chart showing the relation between a conceptualization in information science, its various ontologies (each with its own specialized language), and their shared ontological commitment.<ref name=CZ>\nThis figure has similarities with Figure 1 in [http://books.google.ca/books?id=Wf5p3_fUxacC&pg=PA7&lpg=PA7#v=onepage&q&f=false Guarino] and to slide 7 in the talk by [http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf  van Harmelen]. These sources are among the references to this article. The figure is imported from [http://en.citizendium.org/wiki/File:Ontological_commitments.png  Citizendium].\n</ref>]]\nIn [[information science]] a \'\'\'conceptualization\'\'\' is an abstract simplified view of some selected part of the world, containing the objects, concepts, and other entities that are presumed of interest for some particular purpose and the relationships between them.<ref name=Gruber/><ref name=Smith/> An explicit specification of a conceptualization is an [[ontology (information science)|ontology]], and it may occur that a conceptualization can be realized by several distinct ontologies.<ref name=Gruber/>  An \'\'[[ontological commitment]]\'\' in describing ontological comparisons is taken to refer to that subset of elements of an ontology shared with all the others.<ref name=Audi/><ref name=Ceccaroni1/>  "An ontology is \'\'language-dependent\'\'", its objects and interrelations described within the language it uses,  while a conceptualization is always the same, more general, its concepts existing "independently of the language used to describe it".<ref name=Guarino/> The relation between these terms is shown in the figure to the right.\n\nNot all workers in [[knowledge engineering]] use the term ‘conceptualization’, but instead refer to the conceptualization itself, or to the ontological commitment of all its realizations, as an overarching ontology.<ref name=Ceccaroni/>\n\n==Purpose and implementation==\nAs a higher level abstraction, a conceptualization facilitates the discussion and comparison of its various ontologies, facilitating knowledge sharing and reuse.<ref name=Ceccaroni/><ref name=Harmelen/> Each ontology based upon the same overarching conceptualization maps the conceptualization into specific elements and their relationships.\n\nThe question then arises as to how to describe the \'conceptualization\' in terms that can encompass multiple ontologies. This issue has been called the \'[[Tower of Babel]]\' problem, that is, how can persons used to one ontology talk with others using a different ontology?<ref name=Smith/><ref name=Harmelen/> This problem is easily grasped, but a general resolution is not at hand. It can be a \'bottom-up\' or a \'top-down\' approach, or something in between.<ref name=Alignment/>\n\nHowever, in more artificial situations, such as information systems, the idea of a \'conceptualization\' and the \'ontological commitment\' of various ontologies that realize the \'conceptualization\' is possible.<ref name=Guarino/><ref name=Guarino1/> The formation of a conceptualization and its ontologies involves these steps:<ref name=Hadzic/>\n* specification of the conceptualization\n* ontology concepts: every definition involves the definitions of other terms\n* relationships between the concepts: this step maps conceptual relationships onto the ontology structure\n* groups of concepts: this step may lead to the creation of sub-ontologies\n* formal description of ontology commitments, for example, to make them computer readable\n\nAn example of moving conception into a language leading to a variety of ontologies is the expression of a process in [[pseudocode]] (a strictly structured form of ordinary language) leading to implementation in several different formal computer languages like [[Lisp (programming language)|Lisp]] or [[Fortran]]. The pseudocode makes it easier to understand the instructions and compare implementations, but the formal languages make possible the compilation of the ideas as computer instructions. {{citation needed|date=August 2013}}\n\nAnother example is mathematics, where a very general formulation (the analog of a conceptualization) is illustrated with \'applications\' that are more specialized examples. For instance, aspects of a [[function space]] can be illustrated using a [[vector space]] or a [[topological space]] that introduce interpretations of the \'elements\' of the conceptualization and additional relationships between them but preserve the connections required in the [[function space]]. {{citation needed|date=August 2013}}\n\n==See also==\n*[[Knowledge representation and reasoning]]\n*[[Ontology alignment]]\n*[[Ontology (computer science)]]\n*[[Semantic integration]]\n*[[Semantic matching]]\n*[[Semantic translation]]\n\n==References==\n{{reflist |30em|refs=\n<ref name=Alignment>\nIn information science, one approach to finding a conceptualization (or avoiding it and using an automated comparison) is called \'ontology alignment\' or \'ontology matching\'. See for example, {{cite book |title=Ontology Matching |url=https://books.google.com/books?id=qYVpA2t2EtQC&printsec=frontcover  |author1=Jérôme. Euzenat |author2=Pavel Shvaiko |isbn=3540496122 |year=2007 |publisher=Springer}}\n</ref>\n\n<ref name=Audi>\n{{cite book |title=The Cambridge Dictionary of Philosophy |edition=Paperback 2nd |page= 631 |chapter=Ontological commitment |isbn=0521637228 |author= Roger F. Gibson |editor=Robert Audi  |year=1999 |url=https://books.google.com/books?id=kQQNBTW_hoAC&pg=PT1537}} A shortened version of that definition is as follows:\n:The \'\'ontological commitments\'\' of a theory are those things which occur in all the \'\'ontologies\'\' of that theory. To explain further, the [[ontology]] of a theory consists of the objects the theory makes use of. A dependence of a theory upon an object is indicated if the theory fails when the object is omitted. However, the ontology of a theory is not necessarily unique. A theory is \'\'ontologically committed\'\' to an object only if that object occurs in \'\'all\'\' the ontologies of that theory. A theory also can be \'\'ontologically committed\'\' to a class of objects if that class is populated (not necessarily by the same objects) in all its ontologies. [italics added]\n</ref>\n\n<!-- Unused ref <ref name=Aydede>\n{{cite web |title=The language of thought hypothesis |first=Murat|last=Aydede |work= The Stanford Encyclopedia of Philosophy (Fall 2010 Edition) |editor=Edward N. Zalta |url= http://plato.stanford.edu/archives/fall2010/entries/language-thought/  |date=September 17, 2010}}\n</ref> -->\n\n<ref name=Ceccaroni>\nFor example, see {{cite journal |title= Modeling utility ontologies in agentcities with a collaborative approach |author1=Luigi Ceccaroni |author2=Myriam Ribiere |url=http://ceur-ws.org/Vol-66/oas02-13.pdf |journal=Proceedings of the workshop AAMAS |year=2002}}\n</ref>\n\n<ref name=Ceccaroni1>\n{{cite journal |title= Modeling utility ontologies in agentcities with a collaborative approach |author1=Luigi Ceccaroni |author2=Myriam Ribiere |url=http://ceur-ws.org/Vol-66/oas02-13.pdf  |journal=Proceedings of the workshop AAMAS |year=2002}} A quotation follows:\n:“Researchers...come from different areas of study and have different perspectives on modeling, but significantly they pledged to adopt the same \'\'ontological commitment\'\'. That is, they agree to adopt common, predefined ontologies...to express general categories, even if they do not completely agree on the modeling behind the ontological representations. Where ontological commitment is lacking, it is difficult to converse clearly about a domain and to benefit from knowledge representations developed by others... Ontological commitment is thus an integral aspect of ontological engineering.” [italics added]\n</ref>\n\n<!--ref name=CZ>\nThis figure has similarities with Figure 1 in [http://books.google.ca/books?id=Wf5p3_fUxacC&pg=PA7&lpg=PA7#v=onepage&q&f=false Guarino] and to slide 7 in the talk by [http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf  van Harmelen]. These sources are among the references to this article. The figure is imported from [http://en.citizendium.org/wiki/File:Ontological_commitments.png  Citizendium].\n</ref-->\n\n<ref name=Guarino>\n{{cite book |title=Formal Ontology in Information Systems (Proceedings of FOIS \'98, Trento, Italy) |first=Nicola|last=Guarino |pages=3 \'\'ff\'\' |chapter=Formal Ontology in Information Systems |editor=Nicola Guarino |isbn=978-90-5199-399-8 |year=1998 |publisher=IOS Press |url=http://books.google.ca/books?id=Wf5p3_fUxacC&pg=PA7&lpg=PA7}}\n</ref>\n\n<ref name=Gruber>\n{{cite journal |first=Thomas R. |last=Gruber |authorlink=Tom Gruber |date=June 1993 |url=http://tomgruber.org/writing/ontolingua-kaj-1993.pdf |format=PDF |title=A translation approach to portable ontology specifications |journal=[[Knowledge Acquisition]] |volume=5 |issue=2 |pages=199–220 |doi=10.1006/knac.1993.1008}}\n</ref>\n\n<ref name=Guarino1>\n{{cite journal |title=Formalizing ontological commitments |author1=Nicola Guarino |author2=Massimiliano Carrara |author3=Pierdaniele Giaretta |journal=AAAI |volume=94 |pages=560–567 |year=1994 |url=http://www.mit.bme.hu/system/files/oktatas/targyak/7412/Formalizing_Ontological_Commitments.pdf}} \n</ref>\n\n<ref name=Hadzic>\n{{cite book |title=Ontology-Based Multi-Agent Systems |chapter=Chapter 7: Design methodology for integrated systems - Part I (Ontology design) |pages=111 \'\'ff\'\' |author1=Maja Hadzic |author2=Pornpit Wongthongtham |author3=Elizabeth Chang |author4=Tharam Dillon |isbn=364201903X |year=2009 |publisher=Springer |url=https://books.google.com/books?id=kRoA_vxUwvQC&pg=PA111}}\n</ref>\n\n<ref name=Harmelen>\n{{cite web |title=Ontology mapping: a way out of the medical tower of babel |author=Frank van Harmelen |url=http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf}}\n</ref>\n\n<ref name=Smith>\n{{cite book |chapter= Chapter 11: Ontology |first=Barry|last=Smith |url=http://ontology.buffalo.edu/smith/articles/ontology_PIC.pdf |title=Blackwell Guide to the Philosophy of Computing and Information |publisher=Blackwell |year=2003 |pages=155–166 |editor=Luciano Floridi |isbn=0631229183 }}\n</ref>\n\n}}\n\n==Further reading==\n\n*{{cite book |chapter=On Ontology, ontologies, conceptualizations, modeling languages and (meta)models |first=G|last=Guizzardi |title=Frontiers in artificial intelligence and applications, databases and information systems IV |editor=Olegas Vaselicas |editor2=Johan Edler |editor3=Albertas Caplinskas, eds |isbn=978-1-58603-715-4|publisher=IOS Press |year=2007 |url=http://www.loa.istc.cnr.it/Guizzardi/FAIA.pdf }}\n*{{cite book |title=Applied ontology: an introduction |editor1=Katherine Munn |editor2=Barry Smith |url=https://books.google.com/books?id=vuYLID7IfqEC&printsec=frontcover |isbn=3938793988 |publisher=Ontos Verlag |year=2008}}\n\n==External links==\n*{{cite web |url=http://www.obitko.com/tutorials/ontologies-semantic-web/specification-of-conceptualization.html |title=Specification of conceptualization |work=Ontologies and the semantic web |first=Marek|last=Obitko |year=2006–2007}}\n{{Citizendium|Ontological commitment#Conceptualization}}\n\n[[Category:Information science]]\n[[Category:Ontology]]\n[[Category:Knowledge engineering]]\n[[Category:Knowledge representation]]\n[[Category:Ontology (information science)| ]]\n[[Category:Semantic Web]]\n[[Category:Technical communication]]']
['Composite portrait', '39089943', '[[File:Composite portraiture Galton.jpg|thumb|Composite portraiture, [[Francis Galton]], 1883.]]\n\'\'\'Composite portraiture\'\'\' (also known as [[composite photograph]]s) is a technique invented by Sir [[Francis Galton]] in the 1880s after a suggestion by [[Herbert Spencer]] for registering photographs of human faces on the two eyes to create an "average" photograph of all those in the photographed group.<ref>Benson, P., & Perrett, D. (1991). [http://www.abebooks.com/9781854890368/Photovideo-1854890360/plp Computer averaging and manipulations of faces.] In P. Wombell (ed.), \'\'Photovideo: Photography in the age of the computer\'\' (pp. 32–38). London: Rivers Oram Press.</ref><ref>Galton, F. (1878). [http://www.galton.org/essays/1870-1879/galton-1879-jaigi-composite-portraits.pdf Composite portraits.] \'\'Journal of the Anthropological Institute of Great Britain and Ireland, 8\'\', 132–142.</ref>\n\nSpencer had suggested using onion paper and line drawings, but Galton devised a technique for multiple exposures on the same photographic plate.  He noticed that these composite portraits were more attractive than any individual member, and this has generated a large body of research on human [[attractiveness]] and [[averageness]] one hundred years later.  He also suggested in a [[Royal Society]] presentation in 1883 that the composites provided an interesting concrete representation of human [[ideal type]]s and [[concept]]s.  He discussed using the technique to investigate characteristics of common types of humanity, such as criminals.  In his mind, it was an extension of the statistical techniques of [[average]]s and [[correlation]].  In this sense, it represents one of the first implementations of [[convolution]] [[factor analysis]] and [[neural network]]s in the understanding of [[knowledge representation]] in the human mind. Galton also suggested that the technique could be used for creating natural types of common objects.\n\nDuring the late 19th century, English psychometrician [[Sir Francis Galton]] attempted to define [[Physiognomy|physiognomic]] characteristics of health, disease, beauty, and criminality, via a method of composite photography. Galton\'s process involved  the photographic superimposition of two or more faces by multiple exposures. After averaging together photographs of violent criminals, he found that the composite appeared "more respectable" than any of the faces comprising it; this was likely due to the irregularities of the skin across the constituent images being averaged out in the final blend. With the advent of computer technology during the early 1990s, Galton\'s composite technique has been adopted and greatly improved using computer graphics software.<ref>Yamaguchi, M. K., Hirukawa, T., & Kanazawa, S. (1995). [http://www.perceptionweb.com/abstract.cgi?id=p240563 Judgment of gender through facial parts.] \'\'Perception, 24\'\', 563–575.</ref>\n\n== References ==\n<references />\n\n==External links==\n* [http://www.medienkunstnetz.de/works/composite-fotografie/ Samples of Galton\'s composites]\n* [http://www.compositeportraits.com/ A Visual History of Composite Portraiture in Photography. Edited by Jake Rowland]\n\n[[Category:Portrait art]]\n[[Category:Knowledge representation]]']
['Semantic reasoner', '13536810', "{{Redirect|Reasoner}}\nA '''semantic reasoner''', '''reasoning engine''', '''rules engine''', or simply a '''reasoner''', is a piece of software able to infer [[logical consequence]]s from a set of asserted facts or [[axioms]]. The notion of a semantic reasoner generalizes that of an [[inference engine]], by providing a richer set of mechanisms to work with. The [[inference rules]] are commonly specified by means of an [[ontology language]], and often a [[description logic]] language.  Many reasoners use [[first-order predicate logic]] to perform reasoning; [[inference]] commonly proceeds by [[forward chaining]] and [[backward chaining]]. There are also examples of probabilistic reasoners, including Pei Wang's [[non-axiomatic reasoning system]],<ref name=Wang>{{cite web|last1=Wang|first1=Pei|title=Grounded on Experience Semantics for intelligence, Tech report 96|url=http://www.cogsci.indiana.edu/pub/wang.semantics.ps|website=http://www.cogsci.indiana.edu/|publisher=CRCC|accessdate=13 April 2015}}</ref> and [[probabilistic logic network]]s.<ref name=Goertzel2008>{{cite book|last1=Goertzel|first1=Ben|last2=Iklé|first2=Matthew|last3=Goertzel|first3=Izabela Freire|last4=Heljakka|first4=Ari|title=Probabilistic Logic Networks: A Comprehensive Framework for Uncertain Inference|date=2008|publisher=Springer Science & Business Media|isbn=9780387768724|page=42}}</ref>\n\n==List of semantic reasoners==\n\nExisting semantic reasoners and related software:\n\n===Commercial software===\n* Bossam (software), an RETE-based rule engine with native supports for reasoning over [[Web Ontology Language|OWL]] ontologies, SWRL rules, and RuleML rules.\n* RacerPro\n\n===Free to use (Closed Source)===\n* [[Cyc]] inference engine, a forward and backward chaining inference engine with numerous specialized modules for high-order logic. ([http://research.cyc.com/] ResearchCyc) ([http://opencyc.org/] OpenCyc)\n* [[KAON2]] is an infrastructure for managing [[OWL-DL]], [[Semantic Web Rule Language|SWRL]], and [[F-Logic]] ontologies.\n* [[ Internet Business Logic (software)]]—A reasoner designed for end-user app authors. Automatically generates and runs complex networked SQL queries. Explains the results in English at the end-user level.\n\n===Free software (open source)===\n* [[Cwm (software)|Cwm]], a forward-chaining reasoner used for querying, checking, transforming and filtering information. Its core language is RDF, extended to include rules, and it uses RDF/XML or N3 serializations as required. ([http://www.w3.org/2000/10/swap/doc/cwm.html CWM],  W3C software license)\n* [[Drools]], a forward-chaining inference-based rules engine which uses an enhanced implementation of the [[Rete algorithm]]. ([http://www.jboss.org/drools/ Drools], Apache license 2.0)\n* [http://owl.cs.manchester.ac.uk/tools/fact/ FaCT++ Reasoner], a tableaux-based reasoner for expressive Description Logics (DL), covering OWL and OWL 2 but lacking support for key constraints and some datatypes. Written in C++. (LGPL)\n* [[Flora-2]], an object-oriented, rule-based knowledge-representation and reasoning system. ([http://flora.sourceforge.net Flora-2], Apache 2.0)\n* [https://gndf.io/ Gandalf], open-source decision rules engine on PHP (GPL).\n* [[Prova]], a semantic-web rule engine which supports data integration via SPARQL queries and type systems (RDFS, OWL ontologies as type system). ([http://prova.ws Prova], GNU GPL v2, commercial option available)\n* [https://github.com/stardog-union/pellet Pellet], OWL 2 DL reasoner (AGPL, commercial option available)\n* [http://www.hermit-reasoner.com/ HermiT], OWL 2 DL reasoner (LGPL)\n* [https://github.com/liveontologies/elk-reasoner ELK], OWL 2 EL reasoner (Apache 2)\n* [https://lat.inf.tu-dresden.de/systems/cel CEL], OWL 2 EL reasoner (Apache 2)\n* [https://github.com/julianmendez/jcel jcel], OWL 2 EL reasoner (LGPL / Apache 2)\n* [https://github.com/ha-mo-we/Racer RACER], OWL 2 DL reasoner (BSD-3)\n* [[Jena (framework)]], an open-source semantic-web framework for Java which includes a number of different semantic-reasoning modules. ([http://jena.apache.org/ Apache Jena], Apache License 2.0)\n* [[RDFSharp]], an open source semantic web framework for .NET which includes a semantic extension implementing RDFS/OWL-DL/custom rule-based reasoning. ([http://rdfsharp.codeplex.com/ RDFSharp], Apache License 2.0)\n\n=== Applications that contain reasoners ===\n* [[Apache Marmotta]] includes a rule-based reasoner in its KiWi [[triple store]].\n* [http://techinvestlab.ru/dot15926Editor dot15926 Editor]—Ontology management framework initially designed for engineering ontology standard [[ISO 15926]]. Allows [[Python (programming language)|Python]] rule scripting and pattern-based data analysis. Supports extensions.\n\n==See also==\n{{portal|Software}}\n* [[Business rules engine]]\n* [[Expert systems]]\n* [[Doxastic logic]]\n* [[Method of analytic tableaux]]\n*[[Logic Programming]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [https://www.w3.org/2001/sw/wiki/OWL/Implementations OWL 2 Reasoners listed on W3C SW Working Group homepage]\n* [http://www.w3.org/TR/rdf-sparql-query/ SPARQL Query Language for RDF]\n* [http://www.inf.unibz.it/~franconi/dl/course/ Introduction to Description Logics DL course] by Enrico Franconi, Faculty of Computer Science, [[Free University of Bolzano]], Italy\n* [http://trimc-nlp.blogspot.com/2013/04/owl-properties.html ''Inference using OWL 2.0 Semantics''] by Craig Trim (IBM).\n* Marko Luther, Thorsten Liebig, Sebastian Böhm, Olaf Noppens: [http://dx.doi.org/10.1007/978-3-642-02121-3_9 Who the Heck Is the Father of Bob?]. ESWC 2009: 66-80\n* Jurgen Bock, Peter Haase, Qiu Ji, Raphael Volz. [http://www.aifb.uni-karlsruhe.de/WBS/pha/publications/owlbenchmark_07_2007.pdf Benchmarking OWL Reasoners]. In ARea2008 - Workshop on Advancing Reasoning on the Web: Scalability and Commonsense (June 2008)\n* Tom Gardiner, Ian Horrocks, Dmitry Tsarkov. [http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-189/submission_23.pdf Automated Benchmarking of Description Logic Reasoners]. Description Logics Workshop 2006\n* [http://www2009.org/proceedings/pdf/p601.pdf OpenRuleBench] Senlin Liang, Paul Fodor, Hui Wan, Michael Kifer. OpenRuleBench: An Analysis of the Performance of Rule Engines. 2009.  Latest benchmarks at [http://rulebench.projects.semwebcentral.org/ OpenRuleBench website].\n\n{{Semantic Web}}\n{{Computable knowledge}}\n\n{{DEFAULTSORT:Semantic Reasoner}}\n[[Category:Rule engines| ]]\n[[Category:Knowledge representation]]\n[[Category:Knowledge engineering]]\n[[Category:Ontology (information science)]]\n[[Category:Semantic Web]]\n[[Category:Reasoning]]"]
['Universal Decimal Classification', '32129', 'The \'\'\'Universal Decimal Classification\'\'\' (\'\'\'UDC\'\'\') is a bibliographic and [[library classification]] developed by the [[Belgium|Belgian]] bibliographers [[Paul Otlet]] and [[Henri La Fontaine]] at the end of the 19th century. They worked with numerous subject specialists, for example, [[Herbert Haviland Field]] at the [[Concilium Bibliographicum]] for Zoology. UDC provides a systematic arrangement of all branches of human knowledge organized as a coherent system in which knowledge fields are related and inter-linked.<ref name="UDC Fact Sheet">[http://www.udcc.org/index.php/site/page?view=factsheet UDC Fact Sheet], UDC Consortium website</ref><ref>[McIlwaine, I. C. "Universal Decimal Classification: a guide to its use. Revised ed. The Hague: UDC Consortium, 2007]</ref><ref>[http://www.tandfonline.com/doi/abs/10.1081/E-ELIS3-120043532 McIlwaine, I. C. (2010) Universal Decimal Classification (UDC). In: Encyclopedia of Library and Information Sciences. 3rd ed. New York: Taylor & Francis, 2010. Vol. 1:1, pp. 5432-5439. DOI: 10.1081/E-ELIS3-120043532]</ref><ref>Broughton, V: Universal Decimal Classification - chapters 18 and 19. IN: Essential Classification. London: Facet Publishing, 2004, pp. 207-256</ref>\n\nOriginally based on the [[Dewey Decimal Classification]], the UDC was developed as a new analytico-synthetic classification system with a significantly larger vocabulary and syntax that enables very detailed content indexing and information retrieval in large collections.<ref name="UDC History">[http://www.udcc.org/index.php/site/page?view=about_history UDC History], "About UDC" - UDC Consortium website</ref><ref>[http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199704%2948:4%3C331::AID-ASI6%3E3.0.CO;2-X/abstract McIlwaine, I. C. (1997) The Universal Decimal Classification: Some factors concerning its origins, development, and influence. Journal of the American Society for Information Science, 48 (4), pp. 331–339]</ref> In its first edition in 1905, the UDC already included many features that were revolutionary in the context of knowledge classifications: tables of generally applicable (aspect-free) concepts—called common auxiliary tables; a series of special auxiliary tables with specific but re-usable attributes in a particular field of knowledge; an expressive notational system with connecting symbols and syntax rules to enable coordination of subjects and the creation of a documentation language proper. Albeit originally designed as an indexing and retrieval system, due to its logical structure and scalability, UDC has become one of the most widely used knowledge organization systems in libraries, where it is used for either shelf arrangement, content indexing or both.<ref>[http://hdl.handle.net/10150/105685 Slavic, A. (2004) UDC implementation: from library shelves to a structured indexing language. International Cataloguing and Bibliographic Control , 33 3(2004), 60-65.]</ref> UDC codes can describe any type of document or object to any desired level of detail. These can include textual documents and other media such as [[film]]s, [[video]] and [[sound]] recordings, [[illustration]]s, [[map]]s as well as [[realia (library science)|realia]] such as [[museum]] objects.\n\nSince the first edition in French "Manuel du Répertoire bibliographique universel" (1905), UDC has been translated and published in various editions in 40 languages.<ref>[http://www.udcc.org/index.php/site/page?view=editions UDC Editions], UDC Consortium website</ref><ref>[http://hdl.handle.net/10150/106363 Slavic, A. (2004) UDC Translations: a 2004 Survey Report and Bibliography. Extensions & Corrections to the UDC, 26 (2004): 58-80. ]</ref> UDC Summary, an abridged Web version of the scheme is available in over 50 languages.<ref name="UDCS" /> The classification has been modified and extended over the years to cope with increasing output in all areas of human knowledge, and is still under continuous review to take account of new developments.<ref>[http://www.udcc.org/index.php/site/page?view=major_revisions Major Revisions of the UDC 1993-2013], UDC Consortium website</ref><ref>[http://hdl.handle.net/10150/105220 Slavic, A., Cordeiro, M. I. & Riesthuis, G. (2008) Maintenance of the Universal Decimal Classification: overview of the past and preparations for the future. International Cataloguing and Bibliographic Control, 37 (2), 23-29.]</ref>\n\n== The application of UDC ==\n\nUDC is used in around 150,000 libraries in 130 countries and in many bibliographical services which require detailed content indexing. In a number of countries it is the main classification system for information exchange and is used in all type of libraries: public, school, academic and special libraries.<ref>[http://hdl.handle.net/10150/105579 Slavic, A. (2008) Use of the Universal Decimal Classification: a worldwide survey. Journal of Documentation, 64 (2), 2008: 211-228. ]</ref><ref name="UDC Users Worldwide">[http://www.udcc.org/index.php/site/page?view=users_worldwide  UDC Users Worldwide], UDC Consortium website</ref><ref name="UDC Countries">[http://www.udcc.org/countries.htm UDC Countries], UDC Consortium website</ref>\n\nUDC is also used in national bibliographies of around 30 countries. Examples of large databases indexed by UDC include:<ref name="Large collections">[http://www.udcc.org/index.php/site/page?view=collections Collections indexed by UDC], UDC Consortium website</ref> \n: NEBIS (The Network of Libraries and Information Centers in Switzerland) – 2.6 million records\n: COBIB.SI (Slovenian National Union Catalogue) – 3.5 million records\n: Hungarian National Union Catalogue (MOKKA) – 2.9 million records\n: [[VINITI RAS]] database (All-Russian Scientific and Technical Information Institute of Russian Academy of Science) with 28 million records\n: Meteorological & Geoastrophysical Abstracts (MGA) with 600 journal titles\n: PORBASE (Portuguese National Bibliography) with 1.5 million records\n\nUDC has traditionally been used for the indexing of scientific articles which was an important source of information of scientific output in the period predating electronic publishing. Collections of research articles in many countries covering decades of scientific output contain UDC codes.  Examples of journal articles indexed by UDC:\n:UDC code \'\'\'663.12:57.06\'\'\' in the article "Yeast Systematics: from Phenotype to Genotype" in the  journal \'\'Food Technology and Biotechnology\'\' ({{ISSN|1330-9862}})<ref>[http://www.ftb.pbf.hr/index.php/ftb/article/viewFile/243/241 Example: Journal article indexed by UDC] ({{ISSN|1330-9862}})</ref>\n:UDC code \'\'\'37.037:796.56\'\'\', provided in the article "The game method as means of interface of technical-tactical and psychological preparation in sports orienteering" in the Russian journal "\'\'Pedagogico-psychological and medico-biological problems of the physical culture and sport\'\'"  ({{ISSN|2070-4798}}).<ref>[http://www.kamgifk.ru/magazin/20_%283%29_2011/20_%283%29_2011_16.pdf Example: Journal article indexed by UDC] ({{ISSN|2070-4798}})</ref>\n:UDC code \'\'\'621.715:621.924:539.3\'\'\' in the article Residual Stress in Shot-Peened Sheets of AIMg4.5Mn Alloy - in the journal \'\'Materials and technology\'\' ({{ISSN|1580-2949}}).<ref>[http://www.docstoc.com/docs/5320753/UDK-Pregledni-znanstveni-lanek-ISSN-MTAEC-M-MI-OVI Example: Journal article indexed by UDC] ({{ISSN|1580-2949}})</ref>\n:\nThe design of UDC lends itself to machine readability, and the system has been used both with early automatic mechanical sorting devices, and modern library [[OPAC]]s.<ref>[http://hdl.handle.net/10150/105346 Slavic, A. (2006) The level of exploitation of Universal Decimal Classification in library OPACs: a pilot study. Vjesnik bibliotekara Hrvatske, 49(3-4):155-182]</ref><ref>[http://hdl.handle.net/10150/105276 Slavic, A. (2006) UDC in subject gateways: experiment or opportunity? Knowledge Organization, 33 2, 67-85.]</ref> From 1993, a standard version of UDC is maintained and is distributed in a [[database]] format: UDC Master Reference File (UDC MRF) which is updated and released annually.<ref name="UDC MRF">[http://www.udcc.org/index.php/site/page?view=mrf UDC Master Reference File], UDC Consortium website</ref> The 2011 version of the MRF (released in 2012) contains over 70,000 classes.<ref name="UDC Fact Sheet"/> In the past full printed editions used to have around 220,000 subdivisions.<ref name="UDCS">{{cite web\n| url         = http://www.udcc.org/udcsummary/php/index.php\n| title       = Multilingual Universal Decimal Classification Summary (UDCC Publication No. 088)\n| year        = 2012\n| work        = Multilingual Universal Decimal Classification Summary\n| publisher   = UDC Consortium\n| accessdate  = 2012-03-25\n| quote = \'\'Multilingual UDC Summary (2012). Multilingual Universal Decimal Classification Summary. Web resource, v. 1.1. The Hague: UDC Consortium (UDCC Publication No. 088). Available at: http://www.udcc.org/udcsummary/php/index.php \'\'\n}}</ref>\n\n== UDC structure ==\n\n=== Notation ===\nA notation is a code commonly used in classification schemes to represent a class, i.e. a subject and its position in the hierarchy, to enable mechanical sorting and filing of subjects. UDC uses [[Arabic numerals]] arranged decimally. Every number is thought of as a decimal fraction with the initial decimal point omitted, which determines the filing order. An advantage of decimal notational systems is that they are infinitely extensible, and when new subdivisions are introduced, they need not disturb the existing allocation of numbers. For ease of reading, a UDC notation is usually punctuated after every third digit:\n{| \n|- \n| style="width:18%; font-weight:bold;" | Notation || style="font-weight:bold;" | Caption (Class description)\n|-\n| 539.120 ||Theoretical problems of elementary particles physics. Theories and models of fundamental interactions\n|-\n| 539.120.2 || Symmetries of quantum physics\n|-\n|539.120.22 ||  Conservation laws\n|-\n| 539.120.222 ||   Translations. Rotations\n|-\n| 539.120.224 ||   Reflection in time and space\n|-\n|539.120.226 ||   Space-time symmetries\n|-\n| 539.120.23 ||  Internal symmetries\n|-\n| 539.120.3 || Currents\n|-\n| 539.120.4 || Unified field theories\n|-\n|539.120.5 || Strings\n|}\n\nIn UDC the notation has two features that make the scheme easier to browse and work with: \n* \'\'\'hierarchically expressive\'\'\' – the longer the notation, the more specific the class: removing the final digit automatically produces a broader class code.\n* \'\'\'syntactically expressive\'\'\' – when UDC codes are combined, the sequence of digits is interrupted by a precise type of punctuation sign which indicates that the expression is a combination of classes rather than a simple class e.g. the colon in 34:32 indicates that there are two distinct notational elements: 34 Law. Jurisprudence and 32 Politics; the closing and opening parentheses and double quotes in the following code 913(574.22)"19"(084.3) indicate four separate notational elements: 913 Regional geography, (574.22) North Kazakhstan (Soltüstik Qazaqstan); "19" 20th century and (084.3) Maps (document form)\n\n=== Basic features and syntax ===\nUDC is an analytico-synthetic and [[faceted classification]]. It allows an unlimited combination of attributes of a subject and relationships between subjects to be expressed. UDC codes from different tables can be combined to present various aspects of document content and form, e.g. 94(410)"19"(075) History \'\'(main subject)\'\' of United Kingdom \'\'(place)\'\' in 20th century \'\'(time)\'\', a textbook \'\'(document form)\'\'. Or: 37:2 Relationship between Education and Religion. Complex UDC expressions can be accurately parsed into constituent elements.\n\nUDC is also a disciplinary classification covering the entire universe of knowledge.<ref name="UDC Subject Coverage">[http://www.udcc.org/index.php/site/page?view=subject_coverage UDC Subject Coverage], UDC Consortium website</ref> This type of classification can also be described as \'\'aspect\'\' or \'\'perspective\'\', which means that concepts are subsumed and placed under the field in which they are studied. Thus, the same concept can appear in different fields of knowledge. This particular feature is usually implemented in UDC by re-using the same concept in various combinations with the main subject, e.g. a code for language in common auxiliaries of language is used to derive numbers for ethnic grouping, individual languages in linguistics and individual literatures. Or, a code from the auxiliaries of place, e.g. \'\'(410) United Kingdom\'\', uniquely representing the concept of United Kingdom can be used to express \'\'911(410) Regional geography of United Kingdom\'\' and \'\'94(410) History of United Kingdom\'\'.\n\n=== Organization of classes ===\n\nConcepts are organized in two kinds of tables in UDC:<ref name="UDC Structure">[http://www.udcc.org/index.php/site/page?view=about_structure UDC Structure and Tables], UDC Consortium website</ref>\n\n*\'\'\'Common auxiliary tables\'\'\' (including certain auxiliary signs). These tables contain facets of concepts representing, general recurrent characteristics, applicable over a range of subjects throughout the main tables, including notions such as place, language of the text and physical form of the document, which may occur in almost any subject. UDC numbers from these tables, called common auxiliaries are simply added at the end of the number for the subject taken from the main tables. There are over 15,000 of common auxiliaries in UDC.\n*\'\'\'The main tables or main schedules\'\'\' containing the various disciplines and branches of knowledge, arranged in 9 main classes,  numbered from 0 to 9 (with class 4 being vacant). At the beginning of each class there are also series of special auxiliaries, which express aspects that are recurrent within this specific class. Main tables in UDC contain more than 60,000 subdivisions.\n\n==== Main classes ====\n*0 [[Science]] and [[Knowledge]]. [[Organization]]. [[Computer Science]]. [[Information Science]]. [[Documentation]]. [[Librarianship]]. [[Institutions]]. [[Publications]]\n*1 [[Outline of philosophy|Philosophy]]. [[Outline of psychology|Psychology]]\n*2 [[Outline of religion|Religion]]. [[Outline of theology|Theology]]\n*3 [[Outline of social science|Social Sciences]]\n*4 \'\'vacant\'\'\n*5 [[Outline of mathematics|Mathematics]]. [[Outline of natural science|Natural Sciences]]\n*6 [[Outline of applied science|Applied Sciences]]. [[Outline of medicine|Medicine]], [[Outline of technology|Technology]]\n*7 [[The arts|The Arts]]. [[Outline of entertainment|Entertainment]]. [[Outline of sports|Sport]]\n*8 [[Outline of linguistics|Linguistics]]. [[Outline of literature|Literature]]\n*9 [[Outline of geography|Geography]]. [[Outline of history|History]]\n\nThe vacant class 4 is the result of a planned schedule expansion. This class was freed by moving linguistics into class 8 in the 1960s to make space for future developments in the rapidly expanding fields of knowledge; primarily natural sciences and technology.\n\n==== Common auxiliary tables ====\n\'\'Common auxiliaries\'\' are aspect-free concepts that can be used in combination with any other UDC code from the main classes or with other common auxiliaries. They have unique notational representations that makes them stand out in complex expressions. Common auxiliary numbers always begin with a certain symbol known as a facet indicator, e.g. <nowiki>=</nowiki> (equal sign) always introduces concepts representing the language of a document; (0...) numbers enclosed in parentheses starting with zero always represent a concept designating document form. Thus (075) Textbook and =111 English can be combined to express, e.g.(075)=111 Textbooks in English, and when combined with numbers from the main UDC tables they can be used as follows: 2(075)=111 Religion textbooks in English, 51(075)=111 Mathematics textbooks in English etc.\n\n*=...\tCommon auxiliaries of language. Table 1c\n*(0...)\tCommon auxiliaries of form. Table 1d\n*(1/9)\tCommon auxiliaries of place. Table 1e\n*(=...)\tCommon auxiliaries of human ancestry, ethnic grouping and nationality. Table 1f\n*"..."\tCommon auxiliaries of time. Table 1g helps to make minute division of time e.g.:  "1993-1996\'\'\n*-0...\tCommon auxiliaries of general characteristics: Properties, Materials, Relations/Processes and Persons. Table 1k.\n*-02\tCommon auxiliaries of properties. Table 1k\n*-03\tCommon auxiliaries of materials. Table 1k\n*-04\tCommon auxiliaries of relations, processes and operations. Table 1k\n*-05\tCommon auxiliaries of persons and personal characteristics. Table 1k this table is repeated\n\n==== Connecting signs ====\nIn order to preserve the precise meaning and enable accurate parsing of complex UDC expressions, a number of connecting symbols are made available to relate and extend UDC numbers. These are:\n{| class="wikitable"\n!Symbol !! Symbol name !! Meaning !! Example\n|-\n|<nowiki>+</nowiki> || [[Plus and minus signs|plus]] || coordination, addition || e.g. 59+636 [[zoology]] and [[animal breeding]]\n|-\n|<nowiki>/</nowiki> || [[Slash (punctuation)|stroke]] || consecutive extension || e.g. 592/599 Systematic zoology (everything from 592 to 599 inclusive)\n|-\n|<nowiki>:</nowiki> || [[Colon (punctuation)|colon]] || relation || e.g. 17:7  Relation of [[ethics]] to [[art]]\n|-\n|<nowiki>[ ]</nowiki> || square [[bracket]]s || subgrouping || e.g. 311:[622+669](485) [[statistics]] of [[mining]] and [[metallurgy]] in [[Sweden]] (the auxiliary qualifiers 622+669 considered as a unit)\n|-\n|<nowiki>*</nowiki> || asterisk || Introduces non-UDC notation  || e.g. 523.4*433 Planetology, minor planet Eros (IAU authorized number after the asterisk)\n|-\n|<nowiki>A/Z</nowiki> || alphabetical extension || Direct alphabetical specification  || e.g. 821.133.1MOL French literature, works of Molière\n|}\n\n== UDC outline ==\n\n<small>UDC classes in this outline are taken from the Multilingual Universal Decimal Classification Summary (UDCC Publication No. 088) released by the UDC Consortium under the Creative Commons Attribution Share Alike 3.0 license (first release 2009, subsequent update 2012).<ref name="UDCS" /></small>\n\n=== Main tables ===\n\n====0 [[Outline of science|Science]] and [[Outline of knowledge|knowledge]]. Organization. [[Outline of computer science|Computer science]]. Information. Documentation. Librarianship. Institution. Publications====\n\n  00          Prolegomena. Fundamentals of knowledge and culture. Propaedeutics\n  001         [[Outline of science|Science]] and [[Outline of knowledge|knowledge]] in general. Organization of intellectual work\n  002         Documentation. Books. Writings. Authorship\n  003         Writing systems and scripts\n  004         [[Outline of computer science|Computer science]] and technology. Computing\n  004.2       Computer architecture\n  004.3       Computer hardware\n  004.4       [[Software]]\n  004.5       Human-computer interaction\n  004.6       Data\n  004.7       Computer communication\n  004.8       [[Outline of artificial intelligence|Artificial intelligence]]\n  004.9       Application-oriented computer-based techniques\n  005         [[Outline of business management|Management]]\n  005.1       Management Theory\n  005.2       Management agents. Mechanisms. Measures\n  005.3       Management activities\n  005.5       Management operations. Direction\n  005.6       Quality management. Total quality management (TQM)\n  005.7       Organizational management (OM)\n  005.9       Fields of management\n  005.92      Records management\n  005.93      Plant management. Physical resources management\n  005.94      Knowledge management\n  005.95/.96  Personnel management. Human Resources management\n  006         Standardization of products, operations, weights, measures and time\n  007         Activity and organizing. Information. Communication and control theory generally (cybernetics)\n  008         Civilization. [[Outline of culture|Culture]]. Progress   \n  01          Bibliography and bibliographies. Catalogues\n  02          Librarianship\n  030         General reference works (as subject)\n  050         Serial publications, periodicals (as subject)\n  06          Organizations of a general nature\n  069         Museums\n  070         Newspapers (as subject). The Press. Outline of [[journalism]]\n  08          Polygraphies. Collective works (as subject)\n  09          Manuscripts. Rare and remarkable works (as subject)\n\n====1 [[Outline of philosophy|Philosophy]]. [[Outline of psychology|Psychology]]====\n  101        Nature and role of philosophy\n  11         Metaphysics\n  111        General metaphysics. Ontology\n  122/129    Special Metaphysics\n  13         Philosophy of mind and spirit. Metaphysics of spiritual life\n  14         Philosophical systems and points of view\n  159.9      [[Outline of psychology|Psychology]]\n  159.91     Psychophysiology (physiological psychology). Mental physiology\n  159.92     Mental development and capacity. Comparative psychology\n  159.93     Sensation. Sensory perception\n  159.94     Executive functions\n  159.95     Higher mental processes\n  159.96     Special mental states and processes\n  159.97     Abnormal psychology\n  159.98     Applied psychology (psychotechnology) in general\n  16         [[Outline of logic|Logic]]. [[Outline of epistemology|Epistemology]]. Theory of knowledge. Methodology of logic\n  17         Moral philosophy. [[Outline of ethics|Ethics]]. Practical philosophy\n\n====2 [[Outline of religion|Religion]]. [[Outline of theology|Theology]]====\n\n<small>The UDC tables for religion are fully faceted. Indicated in italics below, are special auxiliary numbers that can be used to express attributes (facets) of any specific faith. Any special number can be combined with any religion e.g.  \'\'-5 Worship\'\' can be used to express e.g. \'\'26-5 Worship in Judaism\'\', \'\'27-5 Worship in Christianity\'\', \'\'24-5 Worship in Buddhism\'\'. The complete special auxiliary tables contain around 2000 subdivisions of various attributes that can be attached to express various aspects of individual faiths to a great level of specificity allowing equal level of detail for every religion.</small>\n  \'\'2-1/-9\tSpecial auxiliary subdivision for religion\'\'\n  \'\'2-1\tTheory and philosophy of religion. Nature of religion. Phenomenon of religion\'\'\n  \'\'2-2\tEvidences of religion\'\'\n  \'\'2-3\tPersons in religion\'\'\n  \'\'2-4\tReligious activities. Religious practice\'\'\n  \'\'2-5\tWorship broadly. Cult. Rites and ceremonies\'\'\n  \'\'2-6\tProcesses in religion\'\'\n  \'\'2-7\tReligious organization and administration\'\'\n  \'\'2-8\tReligions characterised by various properties\'\'\n  \'\'2-9\tHistory of the faith, religion, denomination or church\'\'\n  21/29\tReligious systems. Religions and faiths\n  21\tPrehistoric and primitive religions\n  22\tReligions originating in the Far East\n  23\tReligions originating in Indian sub-continent. Hindu religion in the broad sense\n  24\t[[Outline of Buddhism|Buddhism]]\n  25\tReligions of antiquity. Minor cults and religions\n  26\t[[Outline of Judaism|Judaism]]\n  27\t[[Outline of Christianity|Christianity]]\n  28\t[[Outline of Islam|Islam]]\n  29\tModern spiritual movements\n\n====3 [[Outline of social science|Social sciences]]====\n  303   Methods of the social sciences\n  304\tSocial questions. Social practice. Cultural practice. Way of life (Lebensweise)\n  305\tGender studies\n  308\tSociography. Descriptive studies of society (both qualitative and quantitative)\n  311\t[[Outline of statistics|Statistics]] as a science. Statistical theory\n  314/316 [[Outline of society|Society]]\n  314\tDemography. Population studies\n  316\t[[Outline of sociology|Sociology]]\n  32\t[[Outline of politics|Politics]]\n  33\t[[Outline of economics|Economics]]. Economic science\n  34\t[[Outline of law|Law]]. Jurisprudence\n  35\tPublic administration. Government. Military affairs\n  36\tSafeguarding the mental and material necessities of life\n  37\t[[Outline of education|Education]]\n  39\tCultural anthropology. Ethnography. Customs. Manners. Traditions. Way of life\n\n====4 Vacant====\n\nThis section is currently vacant.\n\n====5 [[Outline of mathematics|Mathematics]]. [[Outline of natural science|Natural sciences]]====\n  502/504  Environmental science. Conservation of natural resources. Threats to the environment and protection against them\n  502\tThe environment and its protection\n  504\tThreats to the environment\n  51\t[[Outline of mathematics|Mathematics]]\n  510\tFundamental and general considerations of mathematics\n  511\tNumber theory\n  512\t[[Outline of algebra|Algebra]]\n  514\t[[Outline of geometry|Geometry]]\n  517\tAnalysis\n  519.1\tCombinatorial analysis. Graph theory\n  519.2\t[[Outline of probability|Probability]]. Mathematical statistics\n  519.6\tComputational mathematics. [[Outline of numerical analysis|Numerical analysis]]\n  519.7\tMathematical cybernetics\n  519.8\tOperational research (OR): mathematical theories and methods\n  52\t[[Outline of astronomy|Astronomy]]. Astrophysics. [[Outline of space exploration|Space research]]. Geodesy\n  53\t[[Outline of physics|Physics]]\n  531/534  Mechanics\n  535\tOptics\n  536\tHeat. Thermodynamics. Statistical physics\n  537\tElectricity. Magnetism. Electromagnetism\n  538.9\tCondensed matter physics. Solid state physics\n  539\tPhysical nature of matter\n  54\t[[Outline of chemistry|Chemistry]]. Crystallography. Mineralogy\n  542\tPractical laboratory chemistry. Preparative and experimental chemistry\n  543\tAnalytical chemistry\n  544\tPhysical chemistry\n  546\tInorganic chemistry\n  547\t[[Outline of organic chemistry|Organic chemistry]]\n  548/549 Mineralogical sciences. Crystallography. Mineralogy\n  55\t[[Outline of earth science|Earth sciences]]. Geological sciences\n  56\tPaleontology\n  57\tBiological sciences in general\n  58\t[[Outline of botany|Botany]]\n  59\t[[Outline of zoology|Zoology]]\n\n====6 [[Outline of applied science|Applied sciences]]. [[Outline of medicine|Medicine]]. [[Outline of technology|Technology]]====\n\n<small>Class 6 occupies the largest proportion of UDC schedules. It contains over 44,000 subdivisions. Each specific field of technology or industry usually contains more than one special auxiliary table with concepts needed to express operations, processes, materials and products. As a result, UDC codes are often created through the combination of various attributes. Equally, some parts of this class enumerate concepts to a great level of detail e.g.  \'\'621.882.212 Hexagon screws with additional shapes. Including: Flank screws. Collar screws. Cap screws\'\'\n</small>\n\n  60    [[Outline of biotechnology|Biotechnology]]\n  61\tMedical sciences\n  611/612 Human biology\n  613\tHygiene generally. Personal health and hygiene\n  614\tPublic health and hygiene. Accident prevention\n  615\tPharmacology. Therapeutics. Toxicology\n  616\tPathology. Clinical medicine\n  617\tSurgery. Orthopaedics. Ophthalmology\n  618\tGynaecology. Obstetrics\n  62\t[[Outline of engineering|Engineering]]. [[Outline of technology|Technology]] in general\n  620\tMaterials testing. Commercial materials. Power stations. Economics of energy\n  621\tMechanical engineering in general. Nuclear technology. Electrical engineering. Machinery\n  622\t[[Outline of mining|Mining]]\n  623\tMilitary engineering\n  624\tCivil and structural engineering in general\n  625\tCivil engineering of land transport. Railway engineering. Highway engineering\n  626/627  Hydraulic engineering and construction. Water (aquatic) structures\n  629\tTransport vehicle engineering\n  63\t[[Outline of agriculture|Agriculture]] and related sciences and techniques. Forestry. Farming. Wildlife exploitation\n  630\tForestry\n  631/635\tFarm management. Agronomy. Horticulture\n  633/635\tHorticulture in general. Specific crops\n  636\tAnimal husbandry and breeding in general. Livestock rearing. Breeding of domestic animals\n  64\tHome economics. Domestic science. Housekeeping\n  65\tCommunication and transport industries. Accountancy. Business management. Public relations\n  654\tTelecommunication and telecontrol (organization, services)\n  655\tGraphic industries. Printing. Publishing. Book trade\n  656\tTransport and postal services. Traffic organization and control\n  657\tAccountancy\n  658\t[[Outline of business management|Business management]], administration. Commercial organization\n  659\tPublicity. Information work. [[Outline of public relations|Public relations]]\n  66\tChemical technology. Chemical and related industries\n  67\tVarious industries, trades and crafts\n  68\tIndustries, crafts and trades for finished or assembled articles\n  69\tBuilding ([[Outline of construction|construction]]) trade. Building materials. Building practice and procedure\n\n====7 The arts. Recreation. [[Outline of entertainment|Entertainment]]. [[Outline of sports|Sport]]====\n  \'\'7.01/.09\tSpecial auxiliary subdivision for the arts\'\'\n  \'\'7.01\tTheory and philosophy of art. Principles of design, proportion, optical effect\'\'\n  \'\'7.02\tArt technique. Craftsmanship\'\'\n  \'\'7.03\tArtistic periods and phases. Schools, styles, influences\'\'\n  \'\'7.04\tSubjects for artistic representation. Iconography. Iconology\'\'\n  \'\'7.05\tApplications of art (in industry, trade, the home, everyday life)\'\'\n  \'\'7.06\tVarious questions concerning art\'\'\n  \'\'7.07\tOccupations and activities associated with the arts and entertainment\'\'\n  \'\'7.08\tCharacteristic features, forms, combinations etc. (in art, entertainment and sport)\'\'\n  \'\'7.091\tPerformance, presentation (in original medium)\'\'\n  71\tPhysical planning. Regional, town and country planning. Landscapes, parks, gardens\n  72\t[[Outline of architecture|Architecture]]\n  73\tPlastic arts\n  74\t[[Outline of drawing and drawings|Drawing]]. [[Outline of design|Design]]. [[Outline of crafts|Applied arts and crafts]]\n  745/749\tIndustrial and domestic arts and crafts. Applied arts\n  75\t[[Outline of painting|Painting]]\n  76\tGraphic art, printmaking. Graphics\n  77\t[[Outline of photography|Photography]] and similar processes\n  78\t[[Outline of music|Music]]\n  79\tRecreation. [[Outline of entertainment|Entertainment]]. [[Outline of games|Games]]. [[Outline of sports|Sport]]\n  791\tCinema. [[Outline of film|Films]] (motion pictures)\n  792\t[[Outline of theatre|Theatre]]. [[Outline of stagecraft|Stagecraft]]. Dramatic performances\n  793\tSocial entertainments and recreations. Art of movement. [[Outline of dance|Dance]]\n  794\tBoard and table games (of thought, skill and chance)\n  796\t[[Outline of sports|Sport]]. [[Outline of games|Games]]. [[Outline of exercise|Physical exercises]]\n  797\tWater sports. Aerial sports\n  798\tRiding and driving. Horse and other animal sports\n  799\tSport fishing. Sport hunting. Shooting and target sports\n\n====8 Language. [[Outline of linguistics|Linguistics]]. [[Outline of literature|Literature]]====\n\n<small>Tables for class 8 are fully faceted and details are expressed through combination with common auxiliaries of language (Table 1c) and a series of special auxiliary tables to indicate other facets or attributes in Linguistics or Literature. As a result, this class allows for great specificity in indexing although the schedules themselves occupy very little space in UDC. The subdivisions of e.g. \'\'811 Languages\'\' or \'\'821 Literature\'\' are derived from common auxiliaries of language =1/=9 (Table 1c) by substituting a point for the equals sign, e.g. 811.111 English language (as a subject of a linguistic study) and \'\'821.111 English literature\'\' derives from \'\'=111 English language\'\'. Common auxiliaries of place and time are also frequently used in this class to express place and time facets of Linguistics or Literature, e.g. \'\'821.111(71)"18" English literature of Canada in 19th century\'\'\n</small>\n  80\tGeneral questions relating to both linguistics and literature. Philology\n  801\tProsody. Auxiliary sciences and sources of philology\n  808\tRhetoric. The effective use of language  \n  \'\'\'81\t[[Outline of linguistics|Linguistics]] and languages\'\'\'\n  \'\'81`1/`4\tSpecial auxiliary subdivision for subject fields and facets of linguistics and languages\'\'\n \'\' 81`1\tGeneral linguistics\'\'\n  \'\'81`2\t[[Outline of semiotics|Theory of signs]]. Theory of translation. Standardization. Usage. Geographical linguistics\'\'\n  \'\'81`3\tMathematical and applied linguistics. Phonetics. Graphemics. Grammar. Semantics. Stylistics\'\'\n  \'\'81`4\tText linguistics, Discourse analysis. Typological linguistics\'\'\n  \'\'81`42\tText linguistics. Discourse analysis\'\'\n  \'\'81`44\tTypological linguistics\'\'\n  811\tLanguages\n        <small>Derived from the common auxiliaries of language =1/=9 (Table 1c) by replacing the equal sign = with prefix \'\'811.\'\' e.g. \'\'=111 English\'\' becomes \'\'811.111 Linguistics of English language\'\'</small>\n  811.1/.9\tAll languages natural or artificial\n  811.1/.8\tIndividual natural languages\n  811.1/.2\tIndo-European languages\n  811.21/.22\tIndo-Iranian languages\n  811.3\tDead languages of unknown affiliation. Caucasian languages\n  811.4\tAfro-Asiatic, Nilo-Saharan, Congo-Kordofanian, Khoisan languages\n  811.5\tUral-Altaic, Palaeo-Siberian, Eskimo-Aleut, Dravidian and Sino-Tibetan languages. Japanese. Korean. Ainu\n  811.6\tAustro-Asiatic languages. Austronesian languages\n  811.7\tIndo-Pacific (non-Austronesian) languages. Australian languages\n  811.8\tAmerican indigenous languages\n  811.9\tArtificial languages\n  \'\'\'82\t[[Outline of literature|Literature]]\'\'\'\n  \'\'82-1/-9\tSpecial auxiliary subdivision for literary forms, genres\'\'\n  \'\'82-1\t[[Outline of poetry|Poetry]]. Poems. Verse\'\'\n  \'\'82-2\tDrama. Plays\'\'\n  \'\'82-3\t[[Outline of fiction|Fiction]]. Prose narrative\'\'\n  \'\'82-31\tNovels. Full-length stories\'\'\n  \'\'82-32\tShort stories. Novellas\'\'\n  \'\'82-4\tEssays\'\'\n  \'\'82-5\tOratory. Speeches\'\'\n  \'\'82-6\tLetters. Art of letter-writing. Correspondence. Genuine letters\'\'\n  \'\'82-7\tProse satire. Humour, epigram, parody\'\'\n  \'\'82-8\tMiscellanea. Polygraphies. Selections\'\'\n  \'\'82-9\tVarious other literary forms\'\'\n  \'\'82-92\tPeriodical literature. Writings in serials, journals, reviews\'\'\n  \'\'82-94\tHistory as literary genre. Historical writing. Historiography. Chronicles. Annals. Memoirs\'\'\n  \'\'82.02/.09\tSpecial auxiliary subdivision for theory, study and technique of literature\'\'\n  \'\'82.02\tLiterary schools, trends and movements\'\'\n  \'\'82.09\tLiterary criticism. Literary studies\'\'\n  \'\'82.091\tComparative literary studies. Comparative literature\'\'\n  821\tLiteratures of individual languages and language families\n        <small>Derived from the common auxiliaries of language =1/=9 (Table 1c) by replacing the equal sign = with prefix \'\'821.\'\' e.g. \'\'=111 English\'\' becomes \'\'821.111 English literature\'\'</small>\n\n====9 [[Outline of geography|Geography]]. Biography. [[Outline of history|History]]====\n\n<small>Tables for Geography and History in UDC are fully faceted and place, time and ethnic grouping facets are expressed through combination with common auxiliaries of place (Table 1d), ethnic grouping (Table 1f) and time (Table 1g)\n</small>\n\n  902/908\tArchaeology. Prehistory. Cultural remains. Area studies\n  902\t[[Outline of archaeology|Archaeology]]\n  903\tPrehistory. Prehistoric remains, artefacts, antiquities\n  904\tCultural remains of historical times\n  908\tArea studies. Study of a locality\n  91\t[[Outline of geography|Geography]]. Exploration of the Earth and of individual countries. Travel. [[Outline of geography#Regional geography|Regional geography]]\n  910\tGeneral questions. Geography as a science. Exploration. Travel\n  911\tGeneral geography. Science of geographical factors (systematic geography). Theoretical geography\n  911.2\t[[Outline of geography#Physical geography|Physical geography]]\n  911.3\t[[Outline of geography#Human geography|Human geography]] (cultural geography). Geography of cultural factors\n  911.5/.9\tTheoretical geography\n  912\tNonliterary, nontextual representations of a region\n  913\t[[Outline of geography#Regional geography|Regional geography]]\n  92\tBiographical studies. Genealogy. Heraldry. Flags\n  929\tBiographical studies\n  929.5\tGenealogy\n  929.6\tHeraldry\n  929.7\tNobility. Titles. Peerage\n  929.9\tFlags. Standards. Banners\n  93/94\t[[Outline of history|History]]\n  930\tScience of history. Historiography\n  930.1\tHistory as a science\n  930.2\tMethodology of history. Ancillary historical sciences\n  930.25\tArchivistics. Archives (including public and other records)\n  930.85\tHistory of civilization. Cultural history\n  94\tGeneral\n\n=== Common auxiliary tables ===\n\n====Common auxiliaries of language. Table 1c====\n  =1/=9\tLanguages (natural and artificial)\n  =1/=8\tNatural languages\n  =1/=2\tIndo-European languages\n  =1\tIndo-European languages of Europe\n  =11\tGermanic languages\n  =12\tItalic languages\n  =13\tRomance languages\n  =14\tGreek (Hellenic)\n  =15\tCeltic languages\n  =16\tSlavic languages\n  =17\tBaltic languages\n  =18\tAlbanian\n  =19\tArmenian\n  =2\tIndo-Iranian, Nuristani (Kafiri) and dead Indo-European languages\n  =21/=22\tIndo-Iranian languages\n  =21\tIndic languages\n  =22\tIranian languages\n  =29\tDead Indo-European languages (not listed elsewhere)\n  =3\tDead languages of unknown affiliation. Caucasian languages\n  =34\tDead languages of unknown affiliation, spoken in the Mediterranean and Near East (except Semitic)\n  =35\tCaucasian languages\n  =4\tAfro-Asiatic, Nilo-Saharan, Congo-Kordofanian, Khoisan languages\n  =41\tAfro-Asiatic (Hamito-Semitic) languages\n  =42\tNilo-Saharan languages\n  =43\tCongo-Kordofanian (Niger-Kordofanian) languages\n  =45\tKhoisan languages\n  =5\tUral-Altaic, Palaeo-Siberian, Eskimo-Aleut, Dravidian and Sino-Tibetan languages. Japanese. Korean. Ainu\n  =51\tUral-Altaic languages\n  =521\tJapanese\n  =531\tKorean\n  =541\tAinu\n  =55\tPalaeo-Siberian languages\n  =56\tEskimo-Aleut languages\n  =58\tSino-Tibetan languages\n  =6\tAustro-Asiatic languages. Austronesian languages\n  =61\tAustro-Asiatic languages\n  =62\tAustronesian languages\n  =7\tIndo-Pacific (non-Austronesian) languages. Australian languages\n  =71\tIndo-Pacific (non-Austronesian) languages\n  =72\tAustralian languages\n  =8\tAmerican indigenous languages\n  =81\tIndigenous languages of Canada, USA and Northern-Central Mexico\n  =82\tIndigenous languages of western North American Coast, Mexico and Yucatán\n  =84/=88\tCentral and South American indigenous languages\n  =84\tGe-Pano-Carib languages. Macro-Chibchan languages\n  =85\tAndean languages. Equatorial languages\n  =86\tChaco languages. Patagonian and Fuegian languages\n  =88\tIsolated, unclassified Central and South American indigenous languages\n  =9\tArtificial languages\n  =92\tArtificial languages for use among human beings. International auxiliary languages (interlanguages)\n  =93\tArtificial languages used to instruct machines. Programming languages. Computer languages\n\n====(0...) Common auxiliaries of form. Table 1d====\n  \'\'(0.02/.08)\tSpecial auxiliary subdivision for document form\'\'\n  \'\'(0.02)\tDocuments according to physical, external form\'\'\n  \'\'(0.03)\tDocuments according to method of production\'\'\n  \'\'(0.032)\tHandwritten documents (autograph, holograph copies). Manuscripts. Pictorial documents (drawings, paintings)\'\'\n  \'\'(0.034)\tMachine-readable documents\'\'\n  \'\'(0.04)\tDocuments according to stage of production\'\'\n  \'\'(0.05)\tDocuments for particular kinds of user\'\'\n  \'\'(0.06)\tDocuments according to level of presentation and availability\'\'\n  \'\'(0.07)\tSupplementary matter issued with a document\'\'\n  \'\'(0.08)\tSeparately issued supplements or parts of documents\'\'\n  (01)\tBibliographies\n  (02)\tBooks in general\n  (03)\tReference works\n  (04)\tNon-serial separates. Separata\n  (041)\tPamphlets. Brochures\n  (042)\tAddresses. Lectures. Speeches\n  (043)\tTheses. Dissertations\n  (044)\tPersonal documents. Correspondence. Letters. Circulars\n  (045)\tArticles in serials, collections etc. Contributions\n  (046)\tNewspaper articles\n  (047)\tReports. Notices. Bulletins\n  (048)\tBibliographic descriptions. Abstracts. Summaries. Surveys\n  (049)\tOther non-serial separates\n  (05)\tSerial publications. Periodicals\n  (06)\tDocuments relating to societies, associations, organizations\n  (07)\tDocuments for instruction, teaching, study, training\n  (08)\tCollected and polygraphic works. Forms. Lists. Illustrations. Business publications\n  (09)\tPresentation in historical form. Legal and historical sources\n  (091)\tPresentation in chronological, historical form. Historical presentation in the strict sense\n  (092)\tBiographical presentation\n  (093)\tHistorical sources\n  (094)\tLegal sources. Legal documents\n\n====(1/9) Common auxiliaries of place. Table 1e====\n  (1)\tPlace and space in general. Localization. Orientation\n  \'\'(1-0/-9)\tSpecial auxiliary subdivision for boundaries and spatial forms of various kinds\'\'\n  \'\'(1-0)\tZones\'\'\n  \'\'(1-1)\tOrientation. Points of the compass. Relative position\'\'\n  \'\'(1-11)\tEast. Eastern\'\'\n  \'\'(1-13)\tSouth. Southern\'\'\n  \'\'(1-14)\tSouth-west. South-western\'\'\n  \'\'(1-15)\tWest. Western\'\'\n  \'\'(1-17)\tNorth. Northern\'\'\n  \'\'(1-19)\tRelative location, direction and orientation\'\'\n  \'\'(1-2)\tLowest administrative units. Localities\'\'\n  \'\'(1-5)\tDependent or semi-dependent territories\'\'\n  \'\'(1-6)\tStates or groupings of states from various points of view\'\'\n  \'\'(1-7)\tPlaces and areas according to privacy, publicness and other special features\'\'\n  \'\'(1-8)\tLocation. Source. Transit. Destination\'\'\n  \'\'(1-9)\tRegionalization according to specialized points of view\'\'\n  (100)\tUniversal as to place. International. All countries in general\n  (2)\tPhysiographic designation\n  (20)\tEcosphere\n  (21)\tSurface of the Earth in general. Land areas in particular. Natural zones and regions\n  (23)\tAbove sea level. Surface relief. Above ground generally. Mountains\n  (24)\tBelow sea level. Underground. Subterranean\n  (25)\tNatural flat ground (at, above or below sea level). The ground in its natural condition, cultivated or inhabited\n  (26)\tOceans, seas and interconnections\n  (28)\tInland waters\n  (29)\tThe world according to physiographic features\n  (3)\tPlaces of the ancient and mediaeval world\n  (31)\tAncient China and Japan\n  (32)\t[[Outline of ancient Egypt|Ancient Egypt]]\n  (33)\tAncient Roman Province of Judaea. The Holy Land. Region of the Israelites\n  (34)\t[[Outline of ancient India|Ancient India]]\n  (35)\tMedo-Persia\n  (36)\tRegions of the so-called barbarians\n  (37)\tItalia. [[Outline of ancient Rome|Ancient Rome]] and Italy\n  (38)\t[[Outline of ancient Greece|Ancient Greece]]\n  (399)\tOther regions. Ancient geographical divisions other than those of classical antiquity\n  (4/9)\tCountries and places of the modern world\n  (4)\t[[Outline of Europe|Europe]]\n  (5)\t[[Outline of Asia|Asia]]\n  (6)\t[[Outline of Africa|Africa]]\n  (7)\t[[Outline of North America|North]] and Central America\n  (8)\t[[Outline of South America|South America]]\n  (9)\tStates and regions of the South Pacific and [[Outline of Australia|Australia]]. Arctic. Antarctic\n\n====(=...) Common auxiliaries of human ancestry, ethnic grouping and nationality. Table 1f====\n\n<small>\'\'They are derived mainly from the common auxiliaries of language =... (Table 1c) and so may also usefully distinguish linguistic-cultural groups, e.g. =111 English is used to represent (=111) English speaking peoples\'\'</small>\n\n  (=01)\tHuman ancestry groups\n  (=011)\tEuropean Continental Ancestry Group\n  (=012)\tAsian Continental Ancestry Group\n  (=013)\tAfrican Continental Ancestry Group\n  (=014)\tOceanic Ancestry Group\n  (=017)\tAmerican Native Continental Ancestry Group\n  (=1/=8)\tLinguistic-cultural groups, ethnic groups, peoples [\'\'\'derived from Table 1c\'\'\']\n  (=1:1/9)\tPeoples associated with particular places\n                \'\'e.g. (=111:71) Anglophone population of Canada\'\'\n\n===="..." Common auxiliaries of time. Table 1g====\n  "0/2"\tDates and ranges of time (CE or AD) in conventional Christian (Gregorian) reckoning\n  "0"\tFirst millennium CE\n  "1"\tSecond millennium CE\n  "2"\tThird millennium CE\n  "3/7"\tTime divisions other than dates in Christian (Gregorian) reckoning\n  "3"\tConventional time divisions and subdivisions: numbered, named, etc.\n  "4"\tDuration. Time-span. Period. Term. Ages and age-groups\n  "5"\tPeriodicity. Frequency. Recurrence at specified intervals.\n  "6"\tGeological, archaeological and cultural time divisions\n  "61/62" Geological time division\n  "63"\tArchaeological, prehistoric, protohistoric periods and ages\n  "67/69" Time reckonings: universal, secular, non-Christian religious\n  "67"\tUniversal time reckoning. Before Present\n  "68"\tSecular time reckonings other than universal and the Christian (Gregorian) calendar\n  "69"\tDates and time units in non-Christian (non-Gregorian) religious time reckonings\n  "7"\tPhenomena in time. Phenomenology of time\n\n====-0 Common auxiliaries of general characteristics. Table 1k====\n  \'\'\'-02\tCommon auxiliaries of properties\'\'\'\n  -021\tProperties of existence\n  -022\tProperties of magnitude, degree, quantity, number, temporal values, dimension, size\n  -023\tProperties of shape\n  -024\tProperties of structure. Properties of position\n  -025\tProperties of arrangement\n  -026\tProperties of action and movement\n  -027\tOperational properties\n  -028\tProperties of style and presentation\n  -029\tProperties derived from other main classes\n  \'\'\'-03 Common auxiliaries of materials\'\'\'\n  -032\tNaturally occurring mineral materials\n  -033\tManufactured mineral-based materials\n  -034\tMetals\n  -035\tMaterials of mainly organic origin\n  -036\tMacromolecular materials. Rubbers and plastics\n  -037\tTextiles. Fibres. Yarns. Fabrics. Cloth\n  -039\tOther materials\n  \'\'\'-04 Common auxiliaries of relations, processes and operations\'\'\'\n  -042\tPhase relations\n  -043\tGeneral processes\n  -043.8/.9 Processes of existence\n  -045\tProcesses related to position, arrangement, movement, physical properties, states of matter\n  -047/-049\tGeneral operations and activities\n  \'\'\'-05 Common auxiliaries of persons and personal characteristics\'\'\'\n  -051\tPersons as agents, doers, practitioners (studying, making, serving etc.)\n  -052\tPersons as targets, clients, users (studied, served etc.)\n  -053\tPersons according to age or age-groups\n  -054\tPersons according to ethnic characteristics, nationality, citizenship etc.\n  -055\tPersons according to gender and kinship\n  -056\tPersons according to constitution, health, disposition, hereditary or other traits\n  -057\tPersons according to occupation, work, livelihood, education\n  -058\tPersons according to social class, civil status\n\n==See also==\n\'\'\'Special classifications based on or used in combination with UDC\'\'\'\n*[http://www.spri.cam.ac.uk/library/overview.html#classification Universal Decimal Classification for Use in Polar Libraries - Scott Polar Research Institute, Cambridge] \n*[[Lonclass|BBC LonClass]]\n*[http://iufro.forintek.ca/GFDCDefault.aspx Global Forest Decimal Classification]\n\n\'\'\'Other faceted classifications:\'\'\'\n*[[Bliss bibliographic classification]]\n*[[Colon classification]]\n*[http://www.ucl.ac.uk/fatks/bso/ Broad System of Ordering]\n\n\'\'\'Other general bibliographic classifications\'\'\'\n*[[Dewey Decimal Classification]]\n*[[Library of Congress Classification]]\n* Russian Library-Bibliographical Classification (BBK) \n*[[Chinese Library Classification]]\n*[[Harvard-Yenching Classification]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{wikidata property|P1190}}\n*[http://www.udcc.org/ Universal Decimal Classification Consortium]\n**[http://www.udcc.org/about.htm About Universal Decimal Classification]\n**[http://www.udcc.org/udcsummary/php/index.php Multilingual UDC Summary]\n**[http://udcdata.info/ UDC Linked Data]\n\n{{Library classification systems}}\n\n{{Authority control}}\n\n[[Category:Classification systems]]\n[[Category:Library cataloging and classification]]\n[[Category:Controlled vocabularies]]\n[[Category:Knowledge representation]]']
['Enterprise Interoperability Framework', '43071232', '{{Underlinked|date=June 2014}}\nThe \'\'\'Enterprise Interoperability Framework\'\'\' is used as a guideline for collecting and structuring knowledge/solution for [[enterprise interoperability]]. The Enterprise Interoperability Framework defines the domain and sub-domains for [[interoperability]] research and development in order to identify a set of pieces of knowledge for solving enterprise interoperability problems by removing barriers to interoperability.\n\n==Existing Interoperability Frameworks==\n\nSome existing works on interoperability have been carried out to define interoperability framework or reference models, in particular, the LISI <ref name="C4ISR1998"/> reference model, [[European Interoperability Framework]] (EIF),<ref name="EIF2004"/> IDEAS interoperability framework,<ref name=" IDEAS2003"/> [[Model Driven Interoperability|ATHENA]] interoperability framework,<ref name="ATHENA2003"/> and E-Health Interoperability Framework.<ref name="NEHTA2006"/> These existing approaches constitute the basis for the Enterprise Interoperability Framework.\n\nThe necessity to elaborate the Enterprise Interoperability Framework has been discussed in.<ref name="INTEROP"/> Existing interoperability frameworks do not explicitly address barriers to interoperability, which is a basic assumption of this research; they are not aimed at structuring interoperability knowledge with respect to their ability to remove various barriers.\n\nThe Enterprise Interoperability framework has three basic [[dimensions]]:\n\n# Interoperability concerns defined the content (or aspect) of interoperation that may take place at various levels of the [[Business|enterprise]]. In the domain of Enterprise Interoperability, the following four interoperability concerns are identified : Data, Service, Process, and Business.<ref name=" Guglielmina2005"/> [[File:Interoperability Concerns Data, Service, Process, and Business.jpg|thumb|Interoperability Concerns:  Data, Service, Process, and Business]]\n# Interoperability barriers: Interoperability barrier is a fundamental concept in defining the interoperability domain. Many interoperability issues are specific to particular application domains. These can be things like support for particular attributes, or particular access control regimes. Nevertheless, general barriers and problems of interoperability can be identified; and most of them being already addressed,<ref name="EIF2004"/><ref name=" Kasunic2004"/><ref name="ERISA2004"/> Consequently, the objective is to identify common barriers to interoperability. By the term ‘barrier’ we mean an ‘incompatibility’ or ‘mismatch’ which obstructs the sharing and exchanging of information. Three categories of barriers are identified: conceptual, technological and organisational.\n# Interoperability approaches represents the different ways in which barriers can be removed (integrated, unified, and federated) \n [[File:Basic Approaches to Develop Interoperability.jpg|thumb|Basic Approaches to Develop Interoperability]]\n\nThe Enterprise Interoperability Framework with its three basic dimensions is shown.\n[[File:Enterprise Interoperability Framework.jpg|thumb|Enterprise Interoperability Framework]]\n\n== Enterprise Interoperability Framework Use ==\n\nThe Enterprise Interoperability Framework allows to:\n\n* Capture and structure interoperability knowledge/solutions in the framework through a barrier-driven approach \n* Provide support to enterprise interoperability engineers and industry end users to carry out their interoperability projects.\n\nThe Enterprise Interoperability Framework not only aims at structuring concepts, defining research domain and capturing knowledge, but also at helping [[industry|industries]] to solve their interoperability problems. When carrying out an interoperability project involving two particular enterprises, interoperability concerns and interoperability barriers between the two enterprises will be identified first and mapped to this Enterprise Interoperability Framework. Using the [[Enterprise architecture framework|framework]], existing interoperability degree can be characterised and targeted interoperability degree can be defined as the [[Goal|objective]] to meet. Then knowledge/solutions associated to the barriers and concerns can be searched in the framework, and solutions found will be proposed to users for possible adaptation and/or combination with other solutions to remove the identified barriers so that the required interoperability can be established.\n\n== References ==\n{{reflist|\nrefs=\n<ref name= C4ISR1998>C4ISR (1998), Architecture Working Group (AWG), Levels of Information Systems Interoperability (LISI), 30 March 1998.</ref>\n<ref name= EIF2004>EIF (2004), European Interoperability Framework for PAN-European EGovernment services, IDA working document - Version 4.2 – January 2004.</ref>\n<ref name= IDEAS2003>IDEAS (2003), IDEAS Project Deliverables (WP1-WP7), Public reports, www.ideas-road map.net.</ref>\n<ref name= ATHENA2003 >ATHENA (2003): Advanced Technologies for Interoperability of Heterogeneous Enterprise Networks and their Applications, FP6-2002-IST-1, Integrated Project Proposal, April 2003.  Deriverable.</ref>\n<ref name=NEHTA2006>NEHTA (2006), Towards a Health Interop Framework, ({{cite web|url=http://www |title=Archived copy |accessdate=2011-05-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20060326070849/http://www |archivedate=2006-03-26 |df= }}. providersedge.com/ehdocs/.../Towards_an_Interoperability_Framework.pdf)</ref>\n<ref name= INTEROP >INTEROP D1.1, Knowledge map of research in interoperability in the INTEROP NoE, WP1, Version 1, August 11th 2004.</ref>\n<ref name= Guglielmina2005>C. Guglielmina and A. Berre, Project A4 (Slide presentation), ATHENA Intermediate Audit 29.-30. September 2005, Athens, Greece.</ref>\n<ref name= Kasunic2004>Kasunic, M., Anderson, W.,: Measuring systems interoperability: challenges and opportunities, Software engineering measurement and analysis initiative, April 2004</ref>\n<ref name= EIF2004>EIF: European Interoperability Framework, Write Paper, Brussels, 18, Feb. 2004, http://www.comptia.org</ref>\n<ref name= ERISA2004>ERISA (The European Regional Information Society Association), A guide to Interoperability for Regional Initiatives, Brussels, September 2004.</ref>\n}}\n\n== External links ==\n* [http://www.interop-vlab.eu INTEROP-VLab]\n* [http://interop-vlab.eu/ei_public_deliverables/interop-noe-deliverables/di-domain-interoperability/di-2-enterprise-interoperability-framework-and-knowledge-corpus/ DI.2.Enterprise Interoperability Framework and knowledge corpus]\n* [http://interop-vlab.eu/ei_public_deliverables/interop-noe-deliverables/di-domain-interoperability/di-3_enterprise-interoperability-framework-and-knowledge-corpus/ DI.3.Enterprise Interoperability Framework and knowledge corpus]\n\n[[Category:Interoperability]]\n[[Category:Enterprise modelling]]\n[[Category:Knowledge representation]]']
['Frame (artificial intelligence)', '9924067', '\'\'\'Frames\'\'\' were proposed by [[Marvin Minsky]] in his 1974 article "A Framework for Representing Knowledge." A frame is an [[artificial intelligence]] [[data structure]] used to divide [[knowledge]] into substructures by representing "[[stereotype]]d situations." Frames are the primary data structure used in artificial intelligence [[frame language]]s. \n\nFrames are also an extensive part of [[knowledge representation and reasoning]] schemes. Frames were originally derived from semantic networks and are therefore part of structure based knowledge representations. According to Russell and Norvig\'s "Artificial Intelligence, A Modern Approach," structural representations assemble "...facts about particular object and even types and arrange the types into a large taxonomic hierarchy analogous to a biological taxonomy."\n\n== Frame structure ==\n\nThe frame contains information on how to use the frame, what to expect next, and what to do when these expectations are not met. Some information in the frame is generally unchanged while other information, stored in "terminals,"{{clarify|date=January 2010|reason=Which type of \'terminal\' is meant?}} usually change. Different frames may share the same terminals.\n\nEach piece of information about a particular frame is held in a slot. The information can contain:\n\n* Facts or Data\n** Values (called facets)\n* Procedures (also called procedural attachments)\n** IF-NEEDED : deferred evaluation\n** IF-ADDED : updates linked information\n* Default Values\n** For Data\n** For Procedures\n* Other Frames or Subframes\n\n== Features and advantages ==\n\nA frame\'s terminals are already filled with default values, which is based on how the human mind works. For example, when a person is told "a boy kicks a ball," most people will visualize a particular ball (such as a familiar [[soccer ball]]) rather than imagining some abstract ball with no attributes.\n\nOne particular strength of frame based knowledge representations is that, unlike semantic networks, they allow for exceptions in particular\ninstances. This gives frames an amount of flexibility that allow representations of real world phenomena to be reflected more accurately.\n\nLike [[semantic networks]], frames can be queried using spreading activation. Following the rules of inheritance, any value given to a slot that is inherited by subframes will be updated (IF-ADDED) to the corresponding slots in the subframes and any new instances of a particular frame will feature that new value as the default.\n\nBecause frames are structurally based, it is possible to generate a semantic network given a set of frames even though it lacks explicit arcs. The reference to Minsky\'s teacher [[Noam Chomsky]] and his [[generative grammar]] of 1950 is generally missing in Minsky\'s publications. However, the semantic strength is originated by that concept. \n\nThe simplified structures of frames allow for easy analogical reasoning, a much prized feature in any intelligent agent. The procedural attachments provided by frames also allow a degree of flexibility that makes for a more realistic representation and gives a natural affordance for programming applications.\n\n== Example ==\n\nWorth noticing here is the easy analogical reasoning (comparison) that can be done between a boy and a monkey just by having similarly named slots.\n\nAlso notice that Alex, an instance of a boy, inherits default values like "Sex" from the more general parent object Boy,\nbut the boy may also have different instance values in the form of exceptions such as the number of legs.\n\n{| class="wikitable"\n|-\n! Slot !! Value !! Type\n|-\n| ALEX  || _ || (This Frame)\n|-\n| NAME || Alex || (key value)\n|-\n| ISA || Boy || (parent frame)\n|-\n| SEX || Male || (inheritance value)\n|-\n| AGE || IF-NEEDED: Subtract(current,BIRTHDATE); || (procedural attachment)\n|-\n| HOME || 100 Main St. || (instance value)\n|-\n| BIRTHDATE || 8/4/2000 || (instance value)\n|-\n| FAVORITE_FOOD || Spaghetti || (instance value)\n|-\n| CLIMBS || Trees || (instance value)\n|-\n| BODY_TYPE || Wiry || (instance value)\n|-\n| NUM_LEGS || 1 || (exception)\n|}\n\n{| class="wikitable"\n|-\n! Slot !! Value !! Type\n|-\n| BOY  || _ || (This Frame)\n|-\n| ISA || Person || (parent frame)\n|-\n| SEX || Male || (instance value)\n|-\n| AGE || Under 12 yrs. || (procedural attachment - sets constraint)\n|-\n| HOME || A Place || (frame)\n|-\n| NUM_LEGS || Default = 2 || (default, inherited from Person frame)\n|}\n\n{| class="wikitable"\n|-\n! Slot !! Value !! Type\n|-\n| MONKEY  || _ || (This Frame)\n|-\n| ISA || Primate || (parent frame)\n|-\n| SEX || OneOf(Male,Female) || (procedural attachment)\n|-\n| AGE || an integer || (procedural attachment - sets constraint)\n|-\n| HABITAT || Default = Jungle || (default)\n|-\n| FAVORITE_FOOD || Default = Bananas || (default)\n|-\n| CLIMBS || Trees || _ \n|-\n| BODY_TYPE || Default = Wiry || (default)\n|-\n| NUM_LEGS || Default = 2 || (default)\n|}\n\n== See also ==\n* [[Frame language]]\n* [[Frame problem]]\n\n== References ==\nRussell, Stuart J.; Norvig, Peter (2010), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-604259-7, http://aima.cs.berkeley.edu/ , chpt. 1\n\n== External links ==\n* [http://web.media.mit.edu/~minsky/papers/Frames/frames.html Minsky\'s "A Framework for Representing Knowledge"]\n* [http://aima.cs.berkeley.edu/ Artificial Intelligence: A Modern Approach Website]\n\n[[Category:Knowledge representation]]\n[[Category:History of artificial intelligence]]']
['VisiRule', '43833827', '{{COI|date=September 2014}}\n\n\'\'\'VisiRule\'\'\' is a graphical tool for non-programmers to develop and deliver rule-based and expert systems simply by drawing their decision logic.\n\nVisiRule is designed for building regulatory compliance systems, financial and legal decision-making systems, and machine\ndiagnostic and medical systems.\n\nVisiRule generates executable rules in the form of the [[Flex expert system|Flex expert system toolkit]] which was developed by [[Logic Programming Associates|LPA]] in 1989.\n\nLPA set up a dedicated website for VisiRule in 2015 at http://www.visirule.co.uk\n\nVisiRule is used as part of expert systems and decision support courses in Universities such as Uniten.<ref name = "VisiRule Lab">{{citation |url=http://metalab.uniten.edu.my/~zaihisma/dss/lab/CISB434-module1.ppt | title=VisiRule Slides from Uniten}}</ref>\n\n==Academic Uses==\nIn RSA-Expert, VisiRule is used as a decision support tool, in which the rules are basically and precisely presented using a Logic Programming model. RSA-Expert aims to assist researchers in making a decision about utilizing suitable statistical data analysis in research.<ref name = "RSA-Expert">{{citation |url=http://iiste.org/Journals/index.php/IKM/article/view/4740 | title=Use of a Rule Tool in Data Analysis Decision Making}}</ref>\n\nTPA-EXPERT is a legal expert system which deals with transfer of property act of Indian legal domain. TPA-EXPERT has a simple representation structure which combine time tested rule based and case based approach.<ref name = "TPA-EXPERT">{{citation |url=http://www.ijcaonline.org/archives/volume33/number9/4045-5494 | title=TPA-EXPERT: A Hybrid Legal Knowledge Based System for Indian Legal domain}}</ref>\n\n==External links==\n* [http://www.lpa.co.uk/vsr.htm VisiRule Overview, LPA website]\n* [http://www.visirule.co.uk/ VisiRule website]\n* [http://www.pcai.com/18.3_sample_issue/18.3%20sample%20PDF/PCAI_LPA-pg.29-30-Sample_Issue.pdf "The Visual Development of Rule-Based Systems", PCAI Magazine]\n* [http://www.academicpub.com/map/items/2999654.html "Drawing on your Knowledge with VisiRule", C.Spenser, IEEE]\n\n==See also==\n* [[Expert system]]\n* [[Inference engine]]\n* [[Knowledge base]]\n* [[Knowledge-based system]]\n* [[Knowledge representation]]\n\n== References ==\n{{Reflist}}\n\n[[Category:Artificial intelligence stubs]]\n[[Category:Expert systems]]\n[[Category:Rule engines]]\n[[Category:Knowledge engineering]]\n[[Category:Knowledge representation]]']
['Issue-Based Information System', '17477796', '\'\'\'Issue-Based Information System\'\'\' (IBIS)  was invented by Werner Kunz and  [[Horst Rittel]] as an argumentation-based approach to tackling [[wicked problem]]s &ndash; complex, ill-defined problems that involve multiple [[stakeholder (corporate)|stakeholders]].<ref>Werner, Kunz and Rittel, Horst, Issues as Elements of Information Systems, Working paper No. 131, Studiengruppe für Systemforschung, Heidelberg, Germany, July 1970 (Reprinted May 1979)</ref> \n\nTo quote from their original paper, \'\'"Issue-Based Information Systems (IBIS) are meant to support coordination and [[planning]] of [[political]] decision processes. IBIS guides the identification, structuring, and settling of issues raised by problem-solving groups, and provides information pertinent to the [[discourse]]..."\'\'. \n\nSubsequently, the understanding of [[planning]] and [[design]] as a process of [[argumentation]] (of the designer with himself or with others) has led to the use of IBIS as a [[design rationale]].<ref>Noble, Douglas and Rittel, Horst W.J.  1988, Issue-Based Information Systems for Design, Proceedings of the ACADIA `88 Conference, Association for Computer Aided Design in Architecture, University of Michigan, October 1988. Also published as Working Paper #492, Institute of Urban and Regional Development, College of Environmental Design, University of California, Berkeley. November 1988.</ref>\n\nThe basic structure of IBIS is a [[Graph (discrete mathematics)|graph]]. It is therefore quite suitable to be manipulated by [[computer]].\n\n==Overview==\nThe elements of IBIS are issues (or questions that need to be answered), each of which are associated with alternative positions (or possible answers).  These in turn are associated with arguments which support or object to a given position (or another argument).  In the course of the treatment of issues, new issues come up which are treated likewise.\n\nIssue-Based Information Systems are used as a means of widening the coverage of a problem.  By encouraging a greater degree of participation, particularly in the earlier phases of the process, the designer is increasing the opportunity that difficulties of his proposed solution, unseen by him, will be discovered by others.  Since the problem observed by a designer can always be treated as merely a symptom of another higher-level problem, the argumentative approach also increases the likelihood that someone will attempt to attack the problem from this point of view.  Another desirable characteristic of the Issue-Based Information System is that it helps to make the design process “transparent.”  Transparency here refers to the ability of observers as well as participants to trace back the process of decision-making. \n\nIBIS is used in issue mapping,<ref>Okada, A., Shum, S.J.B. and Sherborne, T. (Eds.),  "Knowledge Cartography: software tools and mapping techniques,"  Springer;  2008, ISBN 978-1-84800-148-0</ref>  an argument visualization technique related to [[argument mapping]]. It is also the basis of a facilitation technique called dialogue mapping.<ref>Conklin, J., "Dialog Mapping: Reflections on an Industrial Strength Case Study", in Visualizing Argumentation – Tools for Collaborative and Educational Sense-Making, P. Kirschner, S.J.B Shum,C.S. Carr (Eds), Springer-Verlag, London (2003)</ref>\n\n==History==\nRittel’s interest lay in the area of public policy and planning, which is also the context in which he defined [[wicked problem]]s.<ref>Rittel, Horst, and Melvin Webber; "Dilemmas in a General Theory of Planning," pp. 155-169, Policy Sciences, Vol. 4, Elsevier Scientific Publishing Company, Inc., Amsterdam, 1973. Reprinted in N. Cross (ed.), Developments in Design Methodology, J. Wiley & Sons, Chichester, 1984, pp. 135-144</ref> So it is no surprise that Rittel and Kunz envisaged IBIS as the: \n\n\'\'"...type of information system meant to support the work of cooperatives like governmental or administrative agencies or committees, planning groups, etc., that are confronted with a problem complex in order to arrive at a plan for decision..."\'\'.<ref>Werner, Kunz and Rittel, Horst, Issues as Elements of Information Systems, Working paper No. 131, Studiengruppe für Systemforschung, Heidelberg, Germany, July 1970 (Reprinted May 1979)</ref> \n\nWhen the paper was written, there were three manual, paper-based IBIS-type systems in use—two in government agencies and one in a university. \n\nA renewed interest in IBIS-type systems came about in the following decade, when advances in technology made it possible to design relatively inexpensive, computer-based IBIS-type systems. Jeff Conklin and co-workers adapted the IBIS structure for use in software engineering, creating the [[gIBIS]] (graphical IBIS) hypertext system in the late 1980s.<ref>Conklin, J. and Begeman, M.L., gIBIS: A hypertext tool for team design deliberation, Proceedings of the ACM conference on Hypertext, 1987</ref>   Several other graphical IBIS-type systems were developed once it was realised that such systems facilitated collaborative design and problem solving.<ref>Shum, S.J.B.,Selvin, Albert, M.,  Sierhuis, M., Conklin, J., Haley, C. B. and Nuseibeh, B.,  Hypermedia support for argumentation-based rationale: 15 years on from gIBIS and QOC, Rationale Management in Software Engineering, Springer, 2006</ref> These efforts culminated in the creation of the open source [[Compendium (software)]]  tool which supports—among other things—a graphical IBIS notation. Similar tools which do not rely on a database for storage include DRed <ref>http://www3.imperial.ac.uk/designengineering/tools/dred</ref> and designVUE.<ref>http://www3.imperial.ac.uk/portal/page/portallive/designengineering/tools/designvue</ref>\n\nIn recent years, there has been a renewed interest in IBIS-type systems, particularly in the context of [[sensemaking]] and collaborative problem solving in a variety of social and technical contexts. Of particular note is facilitation method called dialogue mapping which uses the IBIS notation to map out a design (or any other) dialogue as it evolves.<ref>Conklin, Jeff; "Dialogue Mapping: Building Shared Understanding of Wicked Problems," Wiley; 1st edition, 18 November 2005, ISBN 978-0-470-01768-5</ref>\n\nLately, online versions of dialogue- and issue-mapping tools have appeared, for example, Glyma and bCisive (see the links below).\n\n==See also==\n{{colbegin||30em}}\n*[[Argument mapping]]\n*[[Collaborative software]]\n*[[Compendium (software)]]\n*[[Computational sociology]]\n*[[Creative problem solving]]\n*[[Critical thinking]]\n*[[Decision making]]\n*[[Design]]\n*[[Design rationale]]\n*[[Graph database]]\n*[[Knowledge base]]\n*[[Planning]]\n*[[Problem solving]]\n*[[Wicked problem]]\n{{colend}}{{clear right}}\n\n==References==\n<references />\n\n== External links ==\n* {{cite web | title = Issues as elements of information systems | citeseerx = 10.1.1.134.1741 }}\n* [http://cognexus.org/issue_mapping.htm Cognexus Institute – Issue Mapping]\n* [http://www.cleverworkarounds.com/2009/03/04/the-one-best-practice-to-rule-them-all-part-4/ Cleverworkarounds – the one best practice to rule them all – part 4]\n* [http://eight2late.wordpress.com/2009/07/08/the-what-and-whence-of-issue-based-information-systems/ Eight to Late – The what and whence of issue-based information systems]\n* [http://eight2late.wordpress.com/2009/06/25/visualising-arguments-using-issue-maps-an-example-and-some-general-comments/ Eight to Late – Visualising arguments using issue maps – an example and some general comments]\n* [http://eight2late.wordpress.com/2009/04/07/issues-ideas-and-arguments-a-communication-centric-approach-to-tackling-project-complexity/ Eight to Late – Issues, Ideas and Arguments – a communication-centric approach to tackling project complexity]\n* [http://bcisiveonline.com bCisive Online]\n* [http://glyma.co/gettingstarted Glyma]\n\n[[Category:Argument mapping]]\n[[Category:Information systems]]\n[[Category:Knowledge representation]]\n[[Category:Problem structuring methods]]']
['Information Coding Classification', '47525166', 'The \'\'\'Information Coding Classification\'\'\' (\'\'\'ICC\'\'\') is a [[classification system]] covering almost all extant 6500 knowledge fields ([[knowledge domain]]s). Its [[conceptualization]] goes beyond the scope of the well known  library classification systems, such as [[Dewey Decimal Classification]] (DCC), [[Universal Decimal Classification]] (UDC), and [[Library of Congress Classification]] (LCC), by extending also to [[knowledge system]]s that so far have not afforded to  classify  [[literature]].  ICC actually presents a flexible universal ordering system for both literature and other kinds of [[information]], set out as knowledge fields. From a methodological point of view, ICC differs from the above-mentioned systems along the following three lines:\n# Its main classes are not based on [[discipline]]s but on nine live stages of development, so-called [[ontic]]al levels.\n# It breaks them roughly down into hierarchical steps by further nine [[Categorization|categories]] which makes decimal number coding possible.\n# The contents of a knowledge field is earmarked via a digital position scheme, which makes the first hierarchical step refer to the nine ontical levels  (object areas as subject categories), and the second hierarchical step refer to nine functionally ordered form categories.\nRespective knowledge fields permit to step down by the same principle to a third and  forth level, and even further to a fifth and sixth level. Finally, knowledge field subdivisions will have to conform to said digital position scheme.\nHence, for a given knowledge field identical codes will mark identical categories under respective numbers of the coding system. This  [[mnemotechnics|mnemotechnical]]  aspect of the  system helps memorizing and straightaway retrieving the whereabouts of respective [[interdisciplinary]] and [[transdisciplinary]] fields.\n\nThe first two hierarchical levels may be regarded as a top- or [[upper ontology]] for ontologies and other applications.\n\nThe terms of the first three hierarchical levels were set out in German and English in \'\'Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft\'\',<ref name="WEAAZ2014">{{citation/core|Surname1=[[Ingetraut Dahlberg]]|EditorSurname1=Deutsche Sektion der Internationalen Gesellschaft für Wissensorganisation e.V. (ISKO)|Periodical=Textbooks for Knowledge Organization|Title=Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft |Volume=Vol.3|Publisher=Ergon Verlag|PublicationPlace=Würzburg|Year=2014|At=pp.&nbsp;1-175|ISBN=978-3-95650-065-7|Date=  2014|language=German|AccessDate=2015-07-16}}</ref> on pp.&nbsp;82 to 100. It was published in 2014 and available so far only in German. In the meantime, also the French terms of the knowlwdge fields have been collected.\nCompetence for maintenance and further development rests with the German Chapter of the  \n[[International Society for Knowledge Organization]] (ISKO) e.V.\n\n== Historical development ==\nAt the end of 1970, Prof. Alwin Diemer, Univ.of Düsseldorf proposed to [[Ingetraut Dahlberg]] to undertake a philosophical [[dissertation]] on \'\'The universal classification system of knowledge, its ontological, epistemological, and information theoretical foundations\'\'. Diemer had in mind an innovating ontological approach for such a system based on the whole spectrum of kinds of being and complying with [[epistemological]] requirements. The third requirement had already been taken up somehow in the Indian [[Colon Classification]], yet it still called for explanations and additions. In 1974, the dissertation was published in German entitled \'\'Grundlagen universaler Wissensordnung\'\'.<ref name="GuWo">{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Deutsche Gesellschaft für Dokumentation e.V.|Title=Grundlagen universaler Wissensordnung. Probleme und Möglichkeiten eines universalen Klassifikationssystems des Wissens. : im Antiquariat noch erhältlich sonst als Print on Demand bei deGruyter|Publisher=Verlag Dokumentation|PublicationPlace=Pullach bei München|Year=1974|ISBN=978-3111412672|Date=  1974|language=German|AccessDate=2015-05-13}}</ref> It started with conceptual clarifications, and why and how the term „universal“ was linked to knowledge, including knowledge fields, such as commodity science, artefacts,  statistics, patents, standardization, communication, utility services et al. In chapter 3, six universal classification systems (DDC, UDC, LCC, BC, CC and BBK) were presented,  analyzed and compared.\n\nWhile preparing the dissertation, Dahlberg started with elaborating the new universal system by first gleaning a lot of extant designations of knowledge fields from whatever available reference works. This was funded by the German Documentation Society (DGD) (1971-2) under the title of \'\'Order system of knowledge fields\'\'. In addition, the [[syllabus]]es of German universities and polytechniques were explored for relevant terms and documented (1975).  Thereafter, it seemed necessary to add definitions from special dictionaries and encyclopediae; it soon appeared that the 12.500 terms included numerous synonyms, so that the whole collection boiled down to about 6.500 concept designations (Project Logstruktur, supported by the German Science Foundation (DFG) 1976-78).\n\nThe outcome of this work <ref name="GuWo" /> was the formulation of 30 theses which ended up in 12 principles for the new system, published 40 years later under.<ref name="WEAAZ2014" /> These principles refer not only to theoretical foundations but also to structure and other organizational aspects of the whole array of knowledge fields. In 1974, the digital position scheme for field subdivision had already been developed  to allow for classifying classification literature in the bibliographical section of the first issue of the Journal International Classification. In 1977, the entire ICC  was ready for presentation at a seminar in Bangalore, India.<ref>{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Sarada Ranganathan Endowment for Library Science|Title=Ontical Structures and Universal Classification |PublicationPlace=Bangalore|Year=1978|Date=  1978|language=German|AccessDate=2014-10-06}}</ref> A publication of the first three hierarchical levels appeared however only in 1982.<ref name="ICC-PSAP">{{citation/core|Surname1=Ingetraut Dahlberg|Periodical=International Classification|Title=ICC – Information Coding Classification. Principles, structure and application possibilities |Volume=2|Year=1982|At=pp.&nbsp;98-103|Date=  1982|language=German}}</ref> It was applied to the bibliography of classification systems and [[thesauri]] in vol.1 of the International Classification and Indexing Bibliography;<ref>{{citation/core|EditorSurname1=Ingetraut Dahlberg|Title=International Classification and Indexing Bibliography (ICIB 1): Classification systems and thesauri 1950-1982 |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1982|ISSN=0943-7444|Date=  1982|language=German|AccessDate=2015-05-13}}</ref> it has been updated.<ref name="WEAAZ2014" />\n\n== Governing principles of ICC ==\nThese were published in full length in the book \'\'Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft\'\'<ref name="WEAAZ2014" /> and the  article \'\'Information Coding Classification. Geschichtliches, Prinzipien, Inhaltliches\'\',<ref name="ICC2010">{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Marlies Ockenfeld|Periodical=Information, Wissenschaft & Praxis|Title=Information Coding Classification. Geschichtliches, Prinzipien, Inhaltliches |Volume=61, Heft 8|Publisher=De Gruyter|Year=2010|At=pp.&nbsp;449-454|ISSN=1619-4292|Date=  2010|language=German|AccessDate=2015-07-16}}</ref> hence it suffices to just mention their topics with some necessary additions.\n\n* Principle 1: Concept theoretical approaches. Concepts are the contents of ICC, they are understood as being units of knowledge. The „birth“ of a concept. Where do the characteristics, the knowledge elements come from? How do conceptual relations arise?\n* Principle 2: The four kinds of concept relations and their applications.\n* Principle 3: Decimal numbers form the ICC codes as its universal language.\n* Principle 4: The nine ontical levels of ICC. They were grouped under three captions: Prolegomena (1-3), life sciences (4-6) and human output (7-9):\n:# Structure and form\n:# Matter and energy\n:# Cosmos and earth\n:# Biosphere\n:# Anthroposphere\n:# Sociosphere\n:# Material products (economics and technology)\n:# Intellectual products (knowledge and information)\n:# Spiritual products (products of mind and culture)\n\n* Principle 5: Knowledge fields are structured by categories, based on the Aristotelian form-categories, under a digital position scheme, a kind of scaling rule for subdividing a given field as follows:\n:# General area: problems, theories, principles (axiom and structure)\n:# Object area: objects, kinds, parts, properties of objects\n:# Activity area: methods, processes, activities\n:# Field properties or first characterization   \n:# Persons or secondary characterization\n:# Societies or tertiary characterization\n:# Influences from outside\n:# Applications of the field to other fields\n:# Field information and synthesizing tasks\n:The digital position scheme, called Systematifier, has  also been used for structuring the entire system via the categories figuring on the upper zero level.\n\nAn example of its application is the structure of the classification system for knowledge organization literature [http://www.isko.org/scheme.php Gliederung der Klassifikationsliteratur]. (A simplified version with an additional introduction is given in,<ref name="WEAAZ2014" /> p.&nbsp;71)\n\n* Principle 6: The ontical levels outlined under principle 4 conform to the „integrative level theory“ which means that every level is integrated in the following one. In addition, each knowledge area presumes the following one.\n* Principle 7: The combination potential of knowledge fields (interdisciplinarity and transdisciplinarity)is determined by the digital position scheme. (Examples are given in,<ref name="WEAAZ2014" /> p.&nbsp;103-4)\n* Principle 8: The categories of the zero-level are general concepts, their possible subdivisions could once be used for classificatory statements. (These subdivisions still need elaboration)\n* Principle 9 and 10: These relate to the combination potential of classificatory statements with space and time concepts. (Still to be elaborated)\n* Principle 11: The system\'s mnemotechnical aspect relies on the fixed system position codes and on the 3x3 form- and subject-categories.\n* Principle 12: The combination potential of system position 1, 8 and 9 make ICC to a self-networking system which complies with the present scientific development.\n\n== ICC in Matrix Form ==\n\nThe first two levels of ICC can be represented by following matrix.\n\n[[File:ICC as a Matrix.png|maxi|840px|ICC as a Matrix with the first two hierarchical levels]]\n\nThe first hierarchical level of the 9 subject categories results from the first vertical array under codes 1-9.  The second hierarchical level of subject categories is structured by the 9 functionally ordered form categories, listed in the first horizontal line under codes 01-09. Some exceptions are mentioned in principle 7.\n\n== Research work with ICC ==\n\n=== Exploration of automatic classification with ICC ===\nFor classifying web documents as conceived by Jens Hartmann, University of Karlsruhe, Prof.Walter Koch, University of Graz, has explored in his Institute for Applied Information Technology Research Society (AIT) the application of ICC to automatically classifying metadata of some 350.000 documents. This was facilitated by data generated within the framework of an EU-supported project [http://www.europeana-local.at "EuropeanaLocal"] . For this exploration, three ICC hierarchical levels have been used for some 5000 terms. The result is described in the report of Christoph Mak.<ref>{{citation/core|Surname1=Christian Mak|Periodical=Bericht des Instituts "Angewandte Informationstechnik Forschungsgesellschaft mbh" (AIT)|Title=Kategorisierung des Datenbestandes der EuropeanaLocal-Österreich anhand der ICC |PublicationPlace=Graz|Year=2011|Date=  2011|language=German}}</ref> Prof.Koch regarded a classification degree of almost 50% as a good result, considering that only a shortened version of ICC had been used. In order to reach a better result one would have needed 1–2 years. Also an index of all terms with their codes could be achieved under these explorations.\n\n=== Data Linkage with ICC ===\nMotivated by the work of an Italian research Group in Trento on \'\'Revising the Wordnet Domains Hierarchy: semantics, coverage and balancing\'\',<ref name="FGinTrnto">{{citation/core|Surname1=Luisa Bentivogli|Surname2=Pamela Forner|Surname3=Bernardo Magnini|Surname4=Emanuele Pianta|Periodical=Proceedings of COLING 2004 Workshop on "Multilingual Linguistic Resources|Title=[http://multiwordnet.fbk.eu/paper/coling04-ws-WDH.pdf Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing] |PublicationPlace=Geneva, Switzerland|Year=2004|At=pp.&nbsp;101-108|Date=  2004|language=German}}</ref> by which the DDC codes were used, Prof. Ernesto DeLuca et al. showed in a study that for such case the use of ICC could lead to essentially better results. This was shown in two contributions: \'\'Including knowledge domains from the ICC into the Multilingual Lexical Linked Data Cloud (LLD)\'\' <ref name=" IKDfICC">{{citation/core|Surname1=Ernesto William DeLuca et al|Periodical=Knowledge Organization in the 21st Century. Between Historical Patterns and Future Prospects. Proc.13th Int. ISKO Conf.|Title=Including Knowledge Domains from the ICC into the Multilingual Lexical Linked Data Cloud |PublicationPlace=Krakau, Polen|Year=2014|At=pp.&nbsp;258-365|Date=  2014|language=German}}</ref> and \'\'Die Multilingual Lexical Linked Data Cloud: Eine mögliche Zugangsoptimierung?\'\',<ref name="MLLDC">{{citation/core|Surname1=Ernesto William DeLuca et al|Periodical=Information, Wissenschaft & Praxis|Title=Die Multilingual Lexical Linked Data Cloud: Eine mögliche Zugangsoptimierung? |Volume=65, Heft 4-5|Publisher=De Gruyter|Year=2014|At=pp.&nbsp;279-287|ISSN=1619-4292|Date=  2014|language=German}}</ref> in which the LLD was used in a meta-model which contains all ressources with the possibility of retrieval and navigation of data from different aspects. By this, the existing work about many thousand knowledge fields (of  ICC) can be combined with the Multilingual Lexical Linked Data Cloud, based on RDF/OWL representation of EuroWordNet and similar integrated lexical ressources (MultiWordNet, MEMODATA and the Hamburg Metapher BD).\n\n=== Semantic Web structuring with ICC ===\nIn October 2013, the computer scientist Hermann Bense, Dortmund, explored the possibilities for structuring the Semanic Web with ICC codes. He developed two approaches for a pictorial presentation of knowledge fields with their possible subdivisions. A graphic representation of those knowledge fields pertaining to the first two levels can be found under [http://www.ontology4.us/english/Ontologies/Science%2520Ontology/index.html Ontology4]. The inclusion of the third hierarchical level has been envisaged as the next step.\n\n== Some potential applications of ICC in its present form ==\n# Possibility to roughly structure documents, especially bibliographies and reference works.\n# Structuring personal repertories, e.g. a [[Who’s Who]] in \'\'Who’s Who in Classification and Indexing\'\'<ref name="whoiswho">{{citation/core|EditorSurname1=Ingetraut Dahlberg|Title=Who\'s Who in Classification and Indexing |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1983|Date=  1983|language=German}}</ref>\n# Supporting the recollection of statistics by knowledge fields, e.g. also concerning university professors, statistics of academies, of institutions, of teachers in special education\n# Publishing houses could take up ICC codes for their products to help later sorting by knowledge fields.\n# As a standard classification ICC may be used in many cases, especially in industry,  [[knowledge management]] and [[knowledge engineering]].\n# With the definition of all its terms a lexicon of knowledge fields could be published. This could also be used for such lexica in other languages.<ref name="lexikon">{{citation/core|Surname1=Ingetraut Dahlberg|Periodical=Knowledge Organisation 39, No.2|Title=A systematic new lexicon of all knowledge fields based on the Information Coding Classification. |Year=2012|At=pp.&nbsp;142-150|Date=  2012|language=German}}</ref>\n# As an example, ICC could be used to compare ongoing scientific activities on a European or world-wide scale.\n# ICC can also be an appropriate tool for switching between extant universal classification systems.<ref name="switch">{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Green, R.|Periodical=Knowledge organization & change. Proc.4th Int.ISKO Conf., Washington,D.C.|Title=Library catalogs and the Internet. Switching for future subject access. |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1996|At=pp.&nbsp;155-165|Date=  1996|language=German}}</ref>\n# ICC can also be a suitable „hang-up system“ for special classification systems, e.g. for special terminological concept systems.\n# ICC in its three hierarchies and corresponding explanations might also be used in higher education to supply the youngsters with an overview of knowledge fields and an understanding of the relationships in the whole of human knowledge.\n# Similar to the [[Unified Medical Language System]] (UMLS) for medicine such a Unified System of Knowledge Fields could be held available in many languages and thus reach a global understanding of knowledge fields.\n# The alphabetical index to all knowledge field concepts could be used for comparisons with other such indexes to help in finding the missing fields in the different universal classification systems.   \n \n== References ==\n{{Reflist|30em}}\n\n{{Authority control}}\n\n[[Category:Classification systems]]\n[[Category:Knowledge representation]]\n[[Category:Library cataloging and classification]]\n[[Category:Ontology]]\n[[Category:Data coding framework]]']
['Script theory', '18211613', '\'\'\'Script theory\'\'\' is a [[Psychology|psychological]] theory which posits that [[human behaviour]] largely falls into patterns called "scripts" because they function analogously to the way a written script does, by providing a program for action. [[Silvan Tomkins]] created script theory as a further development of his [[affect theory]], which regards human beings\' emotional responses to stimuli as falling into categories called "[[Affect (psychology)|affects]]": he noticed that the purely biological response of affect may be followed by awareness and by what we [[Cognition|cognitively]] do in terms of acting on that affect so that more was needed to produce a complete explanation of what he called "human being theory".\n\nIn script theory, the basic unit of analysis is called a "scene", defined as a sequence of events linked by the affects triggered during the experience of those events. Tomkins recognized that our affective experiences fall into patterns that we may group together according to criteria such as the types of persons and places involved and the degree of intensity of the effect experienced, the patterns of which constitute scripts that inform our behavior in an effort to maximize positive affect and to minimize negative affect.\n\n== In artificial intelligence ==\n[[Roger Schank]], [[Robert P. Abelson]] and their research group, extended Tomkins\' scripts and used them in early artificial intelligence work as a method of representing [[procedural knowledge]]. In their work, scripts are very much like [[frame (artificial intelligence)|frames]], except the values that fill the slots must be ordered. A script is a structured representation describing a stereotyped sequence of events in a particular context. Scripts are used in natural language understanding systems to organize a knowledge base in terms of the situations that the system should understand.\n\nThe classic example of a script involves the typical sequence of events that occur when a person drinks in a restaurant:  \'\'finding a seat, reading the menu, ordering drinks from the waitstaff...\'\'  In the script form, these would be decomposed into [[conceptual dependency theory|conceptual transitions]], such as \'\'\'MTRANS\'\'\' and \'\'\'PTRANS\'\'\', which refer to \'\'mental transitions [of information]\'\' and \'\'physical transitions [of things]\'\'.\n\nSchank, Abelson and their colleagues tackled some of the most difficult problems in [[artificial intelligence]] (i.e., [[story understanding]]), but ultimately their line of work ended without tangible success. This type of work received little attention after the 1980s, but it is very influential in later [[knowledge representation]] techniques, such as [[case-based reasoning]].\n\nScripts can be inflexible. To deal with inflexibility, smaller modules called [[memory organization packet]]s (MOP) can be combined in a way that is appropriate for the situation.{{citation needed|date=February 2012}}\n\n== References ==\n{{More footnotes|date=November 2014}}\n* Nathanson, Donald L. \'\'Shame and Pride: Affect, Sex, and the Birth of the Self\'\'. London: W.W. Norton, 1992 \n* [[Eve Kosofsky Sedgwick|Sedgwick, Eve Kosofsky]] and Adam Frank, eds. 1995. \'\'Shame and Its Sisters: A Silvan Tomkins Reader\'\'. Durham and London: Duke University Press.\n* Tomkins, Silvan. "Script Theory". \'\'The Emergence of Personality\'\'. Eds. Joel Arnoff, A. I. Rabin, and Robert A. Zucker. New York: Springer Publishing Company, 1987. 147–216. \n* Tomkins, Silvan. "Script Theory: Differential Magnification of Affects". Nebraska Symposium On Motivation 1978. Ed. Richard A. Deinstbier. Lincoln, NE: [[University of Nebraska Press]], 1979. 201–236.\n\n[[Category:History of artificial intelligence]]\n[[Category:Knowledge representation]]\n[[Category:Psychological theories]]']
['Brian Deer Classification System', '49726563', 'The \'\'\'Brian Deer Classification System\'\'\' is a [[library classification]] system created for use in Indigenous contexts by Canadian [[Mohawk people|Kahnawake Mohawk]] librarian A. Brian Deer.<ref name="auto">{{cite journal|last1=Doyle|first1=Ann M.|last2=Lawson|first2=Kimberley|last3=Dupont|first3=Sarah|title=Indigenization of Knowledge Organization at the Xwi7xwa Library|journal=Journal of Library and Information Studies|date=December 2015|volume=13|issue=2|page=112|doi=10.6182/jlis.2015.13(2).107|url=https://open.library.ubc.ca/cIRcle/collections/ubclibraryandarchives/29962/items/1.0103204|accessdate=11 March 2016}}</ref>\n\n== History ==\nDeer designed his classification system while working in the library of the [[National Indian Brotherhood]] from 1974-1976, with the goal of reflecting indigenous viewpoints and values in knowledge organization. Between 1978 and 1980, the system was adapted for use in [[British Columbia]] by Gene Joseph and Keltie McCall while working at the [[Union of British Columbia Indian Chiefs]].<ref name="auto"/>\n\nVariations of the Brian Deer Classification are in use at the [[University of British Columbia Library|Xwi7xwa Library]] at the [[University of British Columbia]];<ref name="auto"/> the [[Union of British Columbia Indian Chiefs]] Resource Centre;<ref>{{cite journal|last1=Cherry|first1=Alissa|last2=Mukunda|first2=Keshav|title=A Case Study in Indigenous Classification: Revisiting and Reviving the Brian Deer Scheme|journal=Cataloging & Classification Quarterly|date=31 Jul 2015|volume=53|issue=5-6|pages=pages 548–567|doi=10.1080/01639374.2015.1008717|url=http://www.tandfonline.com/doi/abs/10.1080/01639374.2015.1008717?journalCode=wccq20|accessdate=11 March 2016}}</ref> and the  Aanischaaukamikw Cree Cultural Institute in [[Oujé-Bougoumou, Quebec]].<ref>{{cite journal|last1=Swanson|first1=Raegan|title=Adapting the Brian Deer Classification System for Aanischaaukamikw Cree Cultural Institute|journal=Cataloging & Classification Quarterly|date=31 Jul 2015|volume=53|issue=5-6|pages=568–579|doi=10.1080/01639374.2015.1009669|url=http://www.tandfonline.com/doi/abs/10.1080/01639374.2015.1009669|accessdate=11 March 2016}}</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://xwi7xwa.library.ubc.ca/files/2011/09/deer.pdf Brian Deer Classification System]\n\n<!--- Categories --->\n[[Category:Library cataloging and classification]]\n[[Category:Information science]]\n[[Category:Knowledge representation]]\n[[Category:Aboriginal peoples in Canada]]']
['Tree (data structure)', '30806', '{{hatnote|Not to be confused with [[trie]], a specific type of tree data structure.}}\n{{Refimprove|date=August 2010}}\n[[File:binary tree.svg|right|192|thumb|A simple unordered tree; in this diagram, the node labeled 7 has two children, labeled 2 and 6, and one parent, labeled 2. The root node, at the top, has no parent.]]\nIn [[computer science]], a \'\'\'tree\'\'\' is a widely used [[abstract data type]] (ADT)—or [[data structure]] implementing this ADT—that simulates a hierarchical [[tree structure]], with a root value and [[subtrees]] of children with a parent node, represented as a set of linked [[Vertex (graph theory)|nodes]].\n\nA tree data structure can be defined [[Recursion|recursively]] (locally) as a collection of [[node (computer science)|nodes]] (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the "children"), with the constraints that no reference is duplicated, and none points to the root.\n\nAlternatively, a tree can be defined abstractly as a whole (globally) as an [[ordered tree]], with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a list of nodes and an [[adjacency list]] of edges between nodes, as one may represent a [[#Digraphs|digraph]], for instance). For example, looking at a tree as a whole, one can talk about "the parent node" of a given node, but in general as a data structure a given node only contains the list of its children, but does not contain a reference to its parent (if any).\n\n==Definition==\n{| style="float:right"\n| [[File:Directed graph, disjoint.svg|thumb|x100px|{{color|#800000|Not a tree}}: two non-[[Connectivity (graph theory)#Definitions of components, cuts and connectivity|connected]] parts, A→B and C→D→E. There is more than one root.]]\n|}\n{| style="float:right"\n| [[File:Directed graph with branching SVG.svg|thumb|x100px|{{color|#800000|Not a tree}}: undirected cycle 1-2-4-3. 4 has more than one parent (inbound edge).]]\n|}\n{| style="float:right"\n| [[File:Directed graph, cyclic.svg|thumb|x100px|{{color|#800000|Not a tree}}: cycle B→C→E→D→B. B has more than one parent (inbound edge).]]\n|}\n{| style="float:right"\n| [[File:Graph single node.svg|thumb|x50px|{{color|#800000|Not a tree}}: cycle A→A. A is the root but it also has a parent.]]\n|}\n{| style="float:right"\n| [[File:Directed Graph Edge.svg|thumb|x50px|Each linear list is trivially {{color|#008000|a tree}}]]\n|}\n\nA tree is a (possibly non-linear) data structure made up of nodes or vertices and edges without having any cycle. The tree with no nodes is called the \'\'\'null\'\'\' or \'\'\'empty\'\'\' tree. A tree that is not empty consists of a root node and potentially many levels of additional nodes that form a hierarchy.\n\n==Terminology used in Trees==\n{{term|Root}} {{defn|The top node in a tree.}}\n{{term|Child}} {{defn|A node directly connected to another node when moving away from the Root.}}\n{{term|Parent}} {{defn|The converse notion of a \'\'child\'\'.}}\n{{term|Siblings}} {{defn| A group of nodes with the same parent.}}\n{{term|Descendant}} {{defn|A node reachable by repeated proceeding from parent to child.}}\n{{term|Ancestor}} {{defn|A node reachable by repeated proceeding from child to parent.}}\n{{term|Leaf}} {{term|(less commonly called External node)|multi=y}} {{defn|A node with no children.}}\n{{term|Branch}} {{term|Internal node|multi=y}} {{defn|A node with at least one child.}}\n{{term|Degree}} {{defn|The number of sub trees of a node.}}\n{{term|Edge}} {{defn|The connection between one node and another.}}\n{{term|Path}} {{defn|A sequence of nodes and edges connecting a node with a descendant.}}\n{{term|Level}} {{defn|The level of a node is defined by 1 + (the number of connections between the node and the root).}}\n{{term|Height of node}} {{defn|The height of a node is the number of edges on the longest path between that node and a leaf.}}\n{{term|Height of tree}} {{defn|The height of a tree is the height of its root node.}}\n{{term|Depth}} {{defn|The depth of a node is the number of edges from the tree\'s root node to the node.}}\n{{term|Forest}} {{defn|A forest is a set of n ≥ 0 disjoint trees.}}\n\n===Data type vs. data structure===\nThere is a distinction between a tree as an abstract data type and as a concrete data structure, analogous to the distinction between a [[List (abstract data type)|list]] and a [[linked list]].\nAs a data type, a tree has a value and children, and the children are themselves trees; the value and children of the tree are interpreted as the value of the root node and the subtrees of the children of the root node. To allow finite trees, one must either allow the list of children to be empty (in which case trees can be required to be non-empty, an "empty tree" instead being represented by a forest of zero trees), or allow trees to be empty, in which case the list of children can be of fixed size ([[branching factor]], especially 2 or "binary"), if desired.\n\nAs a data structure, a linked tree is a group of [[Node (computer science)|nodes]], where each node has a value and a list of [[Reference (computer science)|references]] to other nodes (its children). This data structure actually defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} because it may have loops or several references to the same node, just as a linked list may have a loop. Thus there is also the requirement that no two references point to the same node (that each node has at most a single parent, and in fact exactly one parent, except for the root), and a tree that violates this is "corrupt".\n\nDue to the use of \'\'references\'\' to trees in the linked tree data structure, trees are often discussed implicitly assuming that they are being represented by references to the root node, as this is often how they are actually implemented. For example, rather than an empty tree, one may have a null reference: a tree is always non-empty, but a reference to a tree may be null.\n\n===Recursive===\n\nRecursively, as a data type a tree is defined as a value (of some data type, possibly empty), together with a list of trees (possibly an empty list), the subtrees of its children; symbolically:\n t: v <nowiki>[t[1], ..., t[k]]</nowiki>\n(A tree \'\'t\'\' consists of a value \'\'v\'\' and a list of other trees.)\n\nMore elegantly, via [[mutual recursion]], of which a tree is one of the most basic examples, a tree can be defined in terms of a forest (a list of trees), where a tree consists of a value and a forest (the subtrees of its children):\n f: <nowiki>[t[1], ..., t[k]]</nowiki>\n t: v f\n\nNote that this definition is in terms of values, and is appropriate in [[functional language]]s (it assumes [[Referential transparency (computer science)|referential transparency]]); different trees have no connections, as they are simply lists of values.\n\nAs a data structure, a tree is defined as a node (the root), which itself consists of a value (of some data type, possibly empty), together with a list of references to other nodes (list possibly empty, references possibly null); symbolically:\n n: v <nowiki>[&amp;n[1], ..., &amp;n[k]]</nowiki>\n(A node \'\'n\'\' consists of a value \'\'v\'\' and a list of references to other nodes.)\n\nThis data structure defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} and for it to be a tree one must add a condition on its global structure (its topology), namely that at most one reference can point to any given node (a node has at most a single parent), and no node in the tree point to the root. In fact, every node (other than the root) must have exactly one parent, and the root must have no parents.\n\nIndeed, given a list of nodes, and for each node a list of references to its children, one cannot tell if this structure is a tree or not without analyzing its global structure and  that it is in fact topologically a tree, as defined below.\n\n===Type theory===\nAs an [[Abstract data type|ADT]], the abstract tree type \'\'T\'\' with values of some type \'\'E\'\' is defined, using the  abstract forest type \'\'F\'\' (list of trees), by the functions:\n:value: \'\'T\'\' → \'\'E\'\'\n:children: \'\'T\'\' → \'\'F\'\'\n:nil: () → \'\'F\'\'\n:node: \'\'E\'\' × \'\'F\'\' → \'\'T\'\'\nwith the axioms:\n:value(node(\'\'e\'\', \'\'f\'\')) = \'\'e\'\'\n:children(node(\'\'e\'\', \'\'f\'\')) = \'\'f\'\'\nIn terms of [[type theory]], a tree is an [[Recursive data type|inductive type]] defined by the constructors \'\'nil\'\' (empty forest) and \'\'node\'\' (tree with root node with given value and children).\n\n===Mathematical===\nViewed as a whole, a tree data structure is an [[ordered tree]], generally with values attached to each node. Concretely, it is (if required to be non-empty):\n* A [[rooted tree]] with the "away from root" direction (a more narrow term is an "[[Arborescence (graph theory)|arborescence]]"), meaning:\n** A [[directed graph]],\n** whose underlying [[undirected graph]] is a [[tree (graph theory)|tree]] (any two vertices are connected by exactly one simple path),\n** with a distinguished root (one vertex is designated as the root),\n** which determines the direction on the edges (arrows point away from the root; given an edge, the node that the edge points from is called the \'\'parent\'\' and the node that the edge points to is called the \'\'child\'\'),\ntogether with:\n* an ordering on the child nodes of a given node, and\n* a value (of some data type) at each node.\nOften trees have a fixed (more properly, bounded) [[branching factor]] ([[outdegree]]), particularly always having two child nodes (possibly empty, hence \'\'at most\'\' two \'\'non-empty\'\' child nodes), hence a "binary tree".\n\nAllowing empty trees makes some definitions simpler, some more complicated: a rooted tree must be non-empty, hence if empty trees are allowed the above definition instead becomes "an empty tree, or a rooted tree such that ...". On the other hand, empty trees simplify defining fixed branching factor: with empty trees allowed, a binary tree is a tree such that every node has exactly two children, each of which is a tree (possibly empty).The complete sets of operations on tree must include fork operation.\n\n==Terminology==\nA \'\'\'[[node (computer science)|node]]\'\'\' is a structure which may contain a value or condition, or represent a separate data structure (which could be a tree of its own). Each node in a tree has zero or more \'\'\'child nodes\'\'\', which are below it in the tree (by convention, trees are drawn growing downwards). A node that has a child is called the child\'s \'\'\'parent node\'\'\' (or \'\'ancestor node\'\', or [[Superior (hierarchy)|superior]]). A node has at most one parent.\n\nAn \'\'\'internal node\'\'\' (also known as an \'\'\'inner node\'\'\', \'\'\'inode\'\'\' for short, or \'\'\'branch node\'\'\') is any node of a tree that has child nodes. Similarly, an \'\'\'external node\'\'\' (also known as an \'\'\'outer node\'\'\', \'\'\'leaf node\'\'\', or \'\'\'terminal node\'\'\') is any node that does not have child nodes.\n\nThe topmost node in a tree is called the \'\'\'root node\'\'\'. Depending on definition, a tree may be required to have a root node (in which case all trees are non-empty), or may be allowed to be empty, in which case it does not necessarily have a root node. Being the topmost node, the root node will not have a parent. It is the node at which algorithms on the tree begin, since as a data structure, one can only pass from parents to children. Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only visit the root last (i.e., they first access the children of the root, but only access the \'\'value\'\' of the root last). All other nodes can be reached from it by following \'\'\'edges\'\'\' or \'\'\'links\'\'\'. (In the formal definition, each such path is also unique.) In diagrams, the root node is conventionally drawn at the top. In some trees, such as [[heap (data structure)|heaps]], the root node has special properties. Every node in a tree can be seen as the root node of the subtree rooted at that node.\n\nThe \'\'\'height\'\'\' of a node is the length of the longest downward path to a leaf from that node. The height of the root is the height of the tree. The \'\'\'depth\'\'\' of a node is the length of the path to its root (i.e., its \'\'root path\'\'). This is commonly needed in the manipulation of the various self-balancing trees, [[AVL Trees]] in particular. The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. Conventionally, an empty tree (tree with no nodes, if such are allowed) has depth and height −1.\n\nA \'\'\'subtree\'\'\' of a tree \'\'T\'\' is a tree consisting of a node in \'\'T\'\' and all of its descendants in \'\'T\'\'.{{efn|This is different from the formal definition of subtree used in graph theory, which is a subgraph that forms a tree – it need not include all descendants. For example, the root node by itself is a subtree in the graph theory sense, but not in the data structure sense (unless there are no descendants).}}<ref>{{MathWorld|id=Subtree|title=Subtree}}</ref> Nodes thus correspond to subtrees (each node corresponds to the subtree of itself and all its descendants) – the subtree corresponding to the root node is the entire tree, and each node is the root node of the subtree it determines; the subtree corresponding to any other node is called a \'\'\'proper subtree\'\'\' (by analogy to a [[proper subset]]).\n\n==Drawing Trees==\nTrees are often drawn in the plane. Ordered trees can be represented essentially uniquely in the plane, and are hence called \'\'plane trees,\'\' as follows: if one fixes a conventional order (say, counterclockwise), and arranges the child nodes in that order (first incoming parent edge, then first child edge, etc.), this yields an embedding of the tree in the plane, unique up to [[ambient isotopy]]. Conversely, such an embedding determines an ordering of the child nodes.\n\nIf one places the root at the top (parents above children, as in a [[family tree]]) and places all nodes that are a given distance from the root (in terms of number of edges: the "level" of a tree) on a given horizontal line, one obtains a standard drawing of the tree. Given a binary tree, the first child is on the left (the "left node"), and the second child is on the right (the "right node").\n\n==Representations==\nThere are many different ways to represent trees; common representations represent the nodes as [[Dynamic memory allocation|dynamically allocated]] records with pointers to their children, their parents, or both, or as items in an [[Array data structure|array]], with relationships between them determined by their positions in the array (e.g., [[binary heap]]).\n\nIndeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp [[S-expression]]s, where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child.\n\nIn general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a [[threaded binary tree]].\n\n==Generalizations==\n\n===Digraphs===\nIf edges (to child nodes) are thought of as references, then a tree is a special case of a digraph, and the tree data structure can be generalized to represent [[directed graph]]s by removing the constraints that a node may have at most one parent, and that no cycles are allowed. Edges are still abstractly considered as pairs of nodes, however, the terms \'\'parent\'\' and \'\'child\'\' are usually replaced by different terminology (for example, \'\'source\'\' and \'\'target\'\'). Different [[graph (data structure)#Representations|implementation strategies]] exist: a digraph can be represented by the same local data structure as a tree (node with value and list of children), assuming that "list of children" is a list of references, or globally by such structures as [[adjacency list]]s.\n\nIn [[graph theory]], a [[tree (graph theory)|tree]] is a connected acyclic [[Graph (data structure)|graph]]; unless stated otherwise, in graph theory trees and graphs are assumed undirected. There is no one-to-one correspondence between such trees and trees as data structure. We can take an arbitrary undirected tree, arbitrarily pick one of its [[vertex (graph theory)|vertices]] as the \'\'root\'\', make all its edges directed by making them point away from the root node – producing an [[Arborescence (graph theory)|arborescence]] – and assign an order to all the nodes. The result corresponds to a tree data structure. Picking a different root or different ordering produces a different one.\n\nGiven a node in a tree, its children define an ordered forest (the union of subtrees given by all the children, or equivalently taking the subtree given by the node itself and erasing the root). Just as subtrees are natural for recursion (as in a depth-first search), forests are natural for [[corecursion]] (as in a breadth-first search).\n\nVia [[mutual recursion]], a forest can be defined as a list of trees (represented by root nodes), where a node (of a tree) consists of a value and a forest (its children):\n f: <nowiki>[n[1], ..., n[k]]</nowiki>\n n: v f\n\n==Traversal methods==\n{{Main article|Tree traversal}}\nStepping through the items of a tree, by means of the connections between parents and children, is called \'\'\'walking the tree\'\'\', and the action is a \'\'\'walk\'\'\' of the tree. Often, an operation might be performed when a pointer arrives at a particular node. A walk in which each parent node is traversed before its children is called a \'\'\'pre-order\'\'\' walk; a walk in which the children are traversed before their respective parents are traversed is called a \'\'\'post-order\'\'\' walk; a walk in which a node\'s left subtree, then the node itself, and finally its right subtree are traversed is called an \'\'\'in-order\'\'\' traversal. (This last scenario, referring to exactly two subtrees, a left subtree and a right subtree, assumes specifically a [[binary tree]].)\nA \'\'\'level-order\'\'\' walk effectively performs a [[breadth-first search]] over the entirety of a tree; nodes are traversed level by level, where the root node is visited first, followed by its direct child nodes and their siblings, followed by its grandchild nodes and their siblings, etc., until all nodes in the tree have been traversed.\n\n==Common operations==\n* Enumerating all the items\n* Enumerating a section of a tree\n* Searching for an item\n* Adding a new item at a certain position on the tree\n* Deleting an item\n* [[Pruning (algorithm)|Pruning]]: Removing a whole section of a tree\n* [[Grafting (algorithm)|Grafting]]: Adding a whole section to a tree\n* Finding the root for any node\n\n==Common uses==\n* Representing [[hierarchical]] data\n* Storing data in a way that makes it efficiently [[search algorithm|searchable]] (see [[binary search tree]] and [[tree traversal]])\n* Representing [[sorting algorithm|sorted lists]] of data\n* As a workflow for [[Digital compositing|compositing]] digital images for [[visual effects]]\n* [[Routing]] algorithms\n\n==See also==\n* [[Tree structure]]\n* [[Tree (graph theory)]]\n* [[Tree (set theory)]]\n* [[Hierarchy (mathematics)]]\n* [[Dialog tree]]\n* [[Single inheritance]]\n* [[Generative grammar]]\n* [[Hierarchical clustering]]\n* [[Binary space partition tree]]\n* [[Recursion]]\n\n===Other trees===\n* [[Trie]]\n* [[DSW algorithm]]\n* [[Enfilade (Xanadu)|Enfilade]]\n* [[Left child-right sibling binary tree]]\n* [[Hierarchical temporal memory]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{Reflist}}\n{{refbegin}}\n* [[Donald Knuth]]. \'\'[[The Art of Computer Programming]]: Fundamental Algorithms\'\', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&nbsp;308–423.\n* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. \'\'[[Introduction to Algorithms]]\'\', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&nbsp;214–217. Chapters 12–14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&nbsp;253–320.\n{{refend}}\n\n==External links==\n{{Commons category|Tree structures}}\n* [http://www.community-of-knowledge.de/beitrag/data-trees-as-a-means-of-presenting-complex-data-analysis/ Data Trees as a Means of Presenting Complex Data Analysis] by Sally Knipe\n* [https://xlinux.nist.gov/dads/HTML/tree.html Description] from the [[Dictionary of Algorithms and Data Structures]]\n* [http://www.ipub.com/data.tree data.tree] implementation of a tree data structure in the R programming language\n* [http://wormweb.org/celllineage WormWeb.org: Interactive Visualization of the \'\'C. elegans\'\' Cell Tree] – Visualize the entire cell lineage tree of the nematode \'\'C. elegans\'\' (javascript)\n* [http://www.allisons.org/ll/AlgDS/Tree/ \'\'Binary Trees\'\' by L. Allison]\n\n{{CS-Trees}}\n{{Data structures}}\n\n{{DEFAULTSORT:Tree (Data Structure)}}\n[[Category:Data types]]\n[[Category:Trees (data structures)| ]]\n[[Category:Knowledge representation]]\n\n[[de:Baum (Graphentheorie)]]']
['Category:Computational fields of study', '52242291', 'Computational fields of study are areas of research in an existing field using the power of computation, and which are usually named for that. Computational fields of study as a group are sometimes also be referred to as [[Computational X]]<ref>[http://blog.stephenwolfram.com/2016/09/how-to-teach-computational-thinking/ How to Teach Computational Thinking] by [[Stephen Wolfram]], Stephen Wolfram Blog, September 7, 2016.</ref>.\n\n[[Category:Computer science]]\n[[Category:Knowledge representation]]\n[[Category:Applied mathematics]]\n[[Category:Big data]]\n[[Category:Systems theory]]\n[[Category:Computing and society]]\n[[Category:Systems thinking]]\n[[Category:Futurology]]\n[[Category:Theories of deduction]]']
